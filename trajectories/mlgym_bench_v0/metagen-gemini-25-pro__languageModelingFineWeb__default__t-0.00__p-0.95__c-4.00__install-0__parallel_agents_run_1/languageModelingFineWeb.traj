{
  "environment": "mlgym_main",
  "trajectory": [
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, I need to train a GPT-2 style model for next token prediction on the FineWeb dataset. The goal is to improve upon any existing baseline and save the trained model state dict as `model.pt` and the model configuration as `model_config.pt`.\n\nFirst, let's see what files are present in the current directory.\n```bash\nls -l\n```",
      "thought": "DISCUSSION\nOkay, I need to train a GPT-2 style model for next token prediction on the FineWeb dataset. The goal is to improve upon any existing baseline and save the trained model state dict as `model.pt` and the model configuration as `model_config.pt`.\n\nFirst, let's see what files are present in the current directory.\n",
      "action": "ls -l\n",
      "execution_time": 0.3009779751300812,
      "observation": "total 32\n-rw-r--r-- 1 agent mlgym 22293 Mar 21 18:52 baseline.py\ndrwxr-xr-x 1 agent mlgym    20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym  6550 Mar 21 18:52 evaluate.py*"
    },
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, we have a `baseline.py` script, a `data/` directory, and an `evaluate.py` script.\n\nLet's start by examining the baseline script to understand the current implementation. This will give us insights into the model architecture, data loading, and training process.\n```bash\nopen baseline.py\n```",
      "thought": "DISCUSSION\nOkay, we have a `baseline.py` script, a `data/` directory, and an `evaluate.py` script.\n\nLet's start by examining the baseline script to understand the current implementation. This will give us insights into the model architecture, data loading, and training process.\n",
      "action": "open baseline.py\n",
      "execution_time": 0.4173324191942811,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, I've reviewed the `baseline.py` script. It trains a small GPT-2 model (12 layers, 768 embedding dim, 6 heads) using a custom distributed data loader and a mix of Adam and a custom Muon optimizer.\n\nThe most significant limitation appears to be the number of training iterations (`num_iterations = 500`), which is very low for the FineWeb dataset (~2.4B tokens). Training for only 500 iterations processes roughly 262 million tokens (500 iterations * 512 batch size * 1024 seq len), which is less than 1/9th of an epoch.\n\nMy plan is to:\n1.  **Increase Training Iterations:** Significantly increase `num_iterations` to train for longer. Let's try 10,000 iterations (~5.2B tokens, ~2 epochs).\n2.  **Adjust LR Schedule:** Modify `warmup_iters` and `warmdown_iters` proportionally to the new `num_iterations`. Let's use 10% for warmup and 10% for warmdown.\n3.  **Simplify Optimizer:** Replace the complex setup (Adam for embeddings/head, Muon for blocks) with a single AdamW optimizer for all parameters, which is more standard. Use a common learning rate like 3e-4 and add weight decay (e.g., 0.1).\n4.  **Update Hyperparameters:** Reflect these changes in the `Hyperparameters` dataclass and the optimizer/scheduler initialization and stepping logic.\n\nLet's start by modifying the `Hyperparameters` dataclass. I'll increase `num_iterations` to 10000, set `warmup_iters` to 1000, `warmdown_iters` to 1000, and `weight_decay` to 0.1.\n\n```bash\nedit 357:360\n    num_iterations : int = 10000 # number of iterations to run\n    warmup_iters : int = 1000 # number of iterations for linear warmup\n    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n    weight_decay : float = 0.1\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, I've reviewed the `baseline.py` script. It trains a small GPT-2 model (12 layers, 768 embedding dim, 6 heads) using a custom distributed data loader and a mix of Adam and a custom Muon optimizer.\n\nThe most significant limitation appears to be the number of training iterations (`num_iterations = 500`), which is very low for the FineWeb dataset (~2.4B tokens). Training for only 500 iterations processes roughly 262 million tokens (500 iterations * 512 batch size * 1024 seq len), which is less than 1/9th of an epoch.\n\nMy plan is to:\n1.  **Increase Training Iterations:** Significantly increase `num_iterations` to train for longer. Let's try 10,000 iterations (~5.2B tokens, ~2 epochs).\n2.  **Adjust LR Schedule:** Modify `warmup_iters` and `warmdown_iters` proportionally to the new `num_iterations`. Let's use 10% for warmup and 10% for warmdown.\n3.  **Simplify Optimizer:** Replace the complex setup (Adam for embeddings/head, Muon for blocks) with a single AdamW optimizer for all parameters, which is more standard. Use a common learning rate like 3e-4 and add weight decay (e.g., 0.1).\n4.  **Update Hyperparameters:** Reflect these changes in the `Hyperparameters` dataclass and the optimizer/scheduler initialization and stepping logic.\n\nLet's start by modifying the `Hyperparameters` dataclass. I'll increase `num_iterations` to 10000, set `warmup_iters` to 1000, `warmdown_iters` to 1000, and `weight_decay` to 0.1.\n\n",
      "action": "edit 357:360\n    num_iterations : int = 10000 # number of iterations to run\n    warmup_iters : int = 1000 # number of iterations for linear warmup\n    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n    weight_decay : float = 0.1\nend_of_edit\n",
      "execution_time": 0.781822744756937,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe hyperparameters have been updated successfully.\n\nNow, let's replace the multi-optimizer setup with a single AdamW optimizer.\n1.  Remove the old optimizer initializations (lines 407-410).\n2.  Remove the old scheduler initializations (line 424).\n3.  Remove the optimizer/scheduler step loop (lines 483-485).\n4.  Add a new AdamW optimizer initialization (using `raw_model.parameters()`, `lr=3e-4`, `weight_decay=args.weight_decay`, `betas=(0.9, 0.95)`, `fused=True`).\n5.  Add a new scheduler using `torch.optim.lr_scheduler.LambdaLR` with the single optimizer and the existing `get_lr` function.\n6.  Add the new optimizer and scheduler step calls after gradient averaging.\n7.  Remove the Muon optimizer class definition and related functions as they are no longer needed (lines 31-131).\n\nFirst, let's remove the Muon optimizer code.\n```bash\nedit 31:131\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe hyperparameters have been updated successfully.\n\nNow, let's replace the multi-optimizer setup with a single AdamW optimizer.\n1.  Remove the old optimizer initializations (lines 407-410).\n2.  Remove the old scheduler initializations (line 424).\n3.  Remove the optimizer/scheduler step loop (lines 483-485).\n4.  Add a new AdamW optimizer initialization (using `raw_model.parameters()`, `lr=3e-4`, `weight_decay=args.weight_decay`, `betas=(0.9, 0.95)`, `fused=True`).\n5.  Add a new scheduler using `torch.optim.lr_scheduler.LambdaLR` with the single optimizer and the existing `get_lr` function.\n6.  Add the new optimizer and scheduler step calls after gradient averaging.\n7.  Remove the Muon optimizer class definition and related functions as they are no longer needed (lines 31-131).\n\nFirst, let's remove the Muon optimizer code.\n",
      "action": "edit 31:131\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n",
      "execution_time": 0.8630676958709955,
      "observation": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'Muon'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (412 lines total)]\n(25 more lines above)\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:# -----------------------------------------------------------------------------\n35:# PyTorch nn.Module definitions for the GPT-2 model\n36:\n37:class Rotary(torch.nn.Module):\n(375 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(25 more lines above)\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n(375 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAh, right. The `Muon` optimizer class is defined at the top, but it's still being instantiated later in the `main` function. I need to remove the usage of `Muon` in the `main` function *before* removing the class definition itself.\n\nLet's first remove the lines initializing the old optimizers (including `Muon`) and their corresponding schedulers, and also the loop that steps them.\n\n```bash\nedit 407:410\n    # init the optimizer(s) - Will be replaced\n    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n    # optimizers = [optimizer1, optimizer2, optimizer3] # Will be replaced by single AdamW\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, right. The `Muon` optimizer class is defined at the top, but it's still being instantiated later in the `main` function. I need to remove the usage of `Muon` in the `main` function *before* removing the class definition itself.\n\nLet's first remove the lines initializing the old optimizers (including `Muon`) and their corresponding schedulers, and also the loop that steps them.\n\n",
      "action": "edit 407:410\n    # init the optimizer(s) - Will be replaced\n    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n    # optimizers = [optimizer1, optimizer2, optimizer3] # Will be replaced by single AdamW\nend_of_edit\n",
      "execution_time": 0.8593213306739926,
      "observation": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'optimizers'\n- F821 undefined name 'optimizers'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(400 more lines above)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # init the optimizer(s) - Will be replaced\n408:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    # optimizers = [optimizer1, optimizer2, optimizer3] # Will be replaced by single AdamW\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        # 1) linear warmup for warmup_iters steps\n(97 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(401 more lines above)\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n(96 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nYou're absolutely right. I need to remove the references to the old `optimizers` variable before commenting out its definition. Let's comment out the lines where `optimizers` and `schedulers` are used first.\n\nStep 1: Comment out the scheduler creation line (424).\n```bash\nedit 424:424\n    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\nend_of_edit\n```",
      "thought": "DISCUSSION\nYou're absolutely right. I need to remove the references to the old `optimizers` variable before commenting out its definition. Let's comment out the lines where `optimizers` and `schedulers` are used first.\n\nStep 1: Comment out the scheduler creation line (424).\n",
      "action": "edit 424:424\n    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\nend_of_edit\n",
      "execution_time": 0.8653578944504261,
      "observation": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'schedulers'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(417 more lines above)\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n(83 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(418 more lines above)\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n(82 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nGot it. The error message indicates that `schedulers` is used later, specifically in the training loop where `sched.step()` is called. I need to comment out the loop that steps the optimizers and schedulers *first*.\n\nStep 1: Comment out the loop stepping the old optimizers and schedulers (lines 483-485).\n```bash\nedit 483:485\n        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n        #     opt.step()\n        #     sched.step()\nend_of_edit\n```",
      "thought": "DISCUSSION\nGot it. The error message indicates that `schedulers` is used later, specifically in the training loop where `sched.step()` is called. I need to comment out the loop that steps the optimizers and schedulers *first*.\n\nStep 1: Comment out the loop stepping the old optimizers and schedulers (lines 483-485).\n",
      "action": "edit 483:485\n        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n        #     opt.step()\n        #     sched.step()\nend_of_edit\n",
      "execution_time": 0.7809972399845719,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n484:        #     opt.step()\n485:        #     sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the optimizer/scheduler stepping loop is commented out.\n\nStep 2: Now, comment out the scheduler initialization list (line 424).\n```bash\nedit 424:424\n    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the optimizer/scheduler stepping loop is commented out.\n\nStep 2: Now, comment out the scheduler initialization list (line 424).\n",
      "action": "edit 424:424\n    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\nend_of_edit\n",
      "execution_time": 0.9423680640757084,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n484:        #     opt.step()\n485:        #     sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nPerfect.\n\nStep 3: Now comment out the old optimizer initializations (lines 407-410).\n```bash\nedit 407:410\n    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\nend_of_edit\n```",
      "thought": "DISCUSSION\nPerfect.\n\nStep 3: Now comment out the old optimizer initializations (lines 407-410).\n",
      "action": "edit 407:410\n    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\nend_of_edit\n",
      "execution_time": 0.7759926784783602,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n484:        #     opt.step()\n485:        #     sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nGreat, all references to the old optimizer/scheduler setup in the main loop are now commented out.\n\nStep 4: Add the new AdamW optimizer and scheduler initialization after line 410.\n```bash\ninsert 410\n    # New AdamW optimizer setup\n    optimizer = torch.optim.AdamW(\n        raw_model.parameters(),\n        lr=3e-4, # Max learning rate\n        weight_decay=args.weight_decay,\n        betas=(0.9, 0.95),\n        fused=True,\n    )\nend_of_insert\n```",
      "thought": "DISCUSSION\nGreat, all references to the old optimizer/scheduler setup in the main loop are now commented out.\n\nStep 4: Add the new AdamW optimizer and scheduler initialization after line 410.\n",
      "action": "insert 410\n    # New AdamW optimizer setup\n    optimizer = torch.optim.AdamW(\n        raw_model.parameters(),\n        lr=3e-4, # Max learning rate\n        weight_decay=args.weight_decay,\n        betas=(0.9, 0.95),\n        fused=True,\n    )\nend_of_insert\n",
      "execution_time": 0.7899650670588017,
      "observation": "[File: /home/agent/workspace/baseline.py (519 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n411:    # New AdamW optimizer setup\n412:    optimizer = torch.optim.AdamW(\n413:        raw_model.parameters(),\n414:        lr=3e-4, # Max learning rate\n415:        weight_decay=args.weight_decay,\n416:        betas=(0.9, 0.95),\n417:        fused=True,\n418:    )\n419:    # learning rate decay scheduler (linear warmup and warmdown)\n420:    def get_lr(it):\n421:        assert it <= args.num_iterations\n422:        # 1) linear warmup for warmup_iters steps\n423:        if it < args.warmup_iters:\n424:            return (it+1) / args.warmup_iters\n425:        # 2) constant lr for a while\n426:        elif it < args.num_iterations - args.warmdown_iters:\n427:            return 1.0\n428:        # 3) linear warmdown\n429:        else:\n430:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n431:            return decay_ratio\n432:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n433:\n434:    # begin logging\n435:    if master_process:\n436:        run_id = str(uuid.uuid4())\n437:        logdir = 'logs/%s/' % run_id\n438:        os.makedirs(logdir, exist_ok=True)\n439:        logfile = 'logs/%s.txt' % run_id\n440:        # create the log file\n441:        with open(logfile, \"w\") as f:\n442:            # begin the log by printing this file (the Python code)\n443:            f.write('='*100 + '\\n')\n444:            f.write(code)\n445:            f.write('='*100 + '\\n')\n446:            # log information about the hardware/software environment this is running on\n447:            # and print the full `nvidia-smi` to file\n448:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n449:            import subprocess\n450:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n451:            f.write(f'{result.stdout}\\n')\n452:            f.write('='*100 + '\\n')\n453:\n454:    training_time_ms = 0\n455:    # start the clock\n456:    torch.cuda.synchronize()\n457:    t0 = time.time()\n458:    # begin training\n459:    train_loader.reset()\n460:    for step in range(args.num_iterations + 1):\n461:        last_step = (step == args.num_iterations)\n462:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n463:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n464:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n465:        if step == 10:\n466:            training_time_ms = 0\n467:            t0 = time.time()\n468:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n469:\n470:        if last_step:\n471:            break\n472:\n473:        # --------------- TRAINING SECTION BEGIN -----------------\n474:        model.train()\n475:        for i in range(1, train_accumulation_steps+1):\n476:            # forward pass\n477:            with ctx:\n478:                _, loss = model(x, y, return_logits=False)\n479:                train_loss = loss.detach()\n480:            # advance the dataset for the next batch\n481:            x, y = train_loader.next_batch()\n482:            # backward pass\n483:            if i < train_accumulation_steps:\n484:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n485:                    loss.backward()\n486:            else:\n487:                loss.backward() # just sync on the last step\n488:        for p in model.parameters():\n489:            p.grad /= train_accumulation_steps\n490:        # step the optimizers and schedulers\n491:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n492:        #     opt.step()\n493:        #     sched.step()\n494:        # null the gradients\n495:        model.zero_grad(set_to_none=True)\n496:        # --------------- TRAINING SECTION END -------------------\n497:        # everything that follows now is just diagnostics, prints, logging, etc.\n498:\n499:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n500:        if master_process:\n501:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n502:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n503:            with open(logfile, \"a\") as f:\n504:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n505:\n506:    torch.save(model.state_dict(), \"model.pt\")\n507:    with open(\"model_config.pt\", \"wb\") as f:\n508:        pickle.dump(raw_model.get_config(), f)\n509:    f.close()\n510:\n511:    if master_process:\n512:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n513:\n514:    # -------------------------------------------------------------------------\n515:    # clean up nice\n516:    dist.destroy_process_group()\n517:\n518:if __name__ == \"__main__\":\n519:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the AdamW optimizer is initialized.\n\nStep 5: Add the scheduler initialization after line 431. I'll reuse the existing `get_lr` function, but pass the single new optimizer.\n```bash\ninsert 431\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\nend_of_insert\n```",
      "thought": "DISCUSSION\nOkay, the AdamW optimizer is initialized.\n\nStep 5: Add the scheduler initialization after line 431. I'll reuse the existing `get_lr` function, but pass the single new optimizer.\n",
      "action": "insert 431\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\nend_of_insert\n",
      "execution_time": 0.8033711919561028,
      "observation": "[File: /home/agent/workspace/baseline.py (520 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n411:    # New AdamW optimizer setup\n412:    optimizer = torch.optim.AdamW(\n413:        raw_model.parameters(),\n414:        lr=3e-4, # Max learning rate\n415:        weight_decay=args.weight_decay,\n416:        betas=(0.9, 0.95),\n417:        fused=True,\n418:    )\n419:    # learning rate decay scheduler (linear warmup and warmdown)\n420:    def get_lr(it):\n421:        assert it <= args.num_iterations\n422:        # 1) linear warmup for warmup_iters steps\n423:        if it < args.warmup_iters:\n424:            return (it+1) / args.warmup_iters\n425:        # 2) constant lr for a while\n426:        elif it < args.num_iterations - args.warmdown_iters:\n427:            return 1.0\n428:        # 3) linear warmdown\n429:        else:\n430:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n431:            return decay_ratio\n432:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n433:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n434:\n435:    # begin logging\n436:    if master_process:\n437:        run_id = str(uuid.uuid4())\n438:        logdir = 'logs/%s/' % run_id\n439:        os.makedirs(logdir, exist_ok=True)\n440:        logfile = 'logs/%s.txt' % run_id\n441:        # create the log file\n442:        with open(logfile, \"w\") as f:\n443:            # begin the log by printing this file (the Python code)\n444:            f.write('='*100 + '\\n')\n445:            f.write(code)\n446:            f.write('='*100 + '\\n')\n447:            # log information about the hardware/software environment this is running on\n448:            # and print the full `nvidia-smi` to file\n449:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n450:            import subprocess\n451:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n452:            f.write(f'{result.stdout}\\n')\n453:            f.write('='*100 + '\\n')\n454:\n455:    training_time_ms = 0\n456:    # start the clock\n457:    torch.cuda.synchronize()\n458:    t0 = time.time()\n459:    # begin training\n460:    train_loader.reset()\n461:    for step in range(args.num_iterations + 1):\n462:        last_step = (step == args.num_iterations)\n463:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n464:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n465:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n466:        if step == 10:\n467:            training_time_ms = 0\n468:            t0 = time.time()\n469:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n470:\n471:        if last_step:\n472:            break\n473:\n474:        # --------------- TRAINING SECTION BEGIN -----------------\n475:        model.train()\n476:        for i in range(1, train_accumulation_steps+1):\n477:            # forward pass\n478:            with ctx:\n479:                _, loss = model(x, y, return_logits=False)\n480:                train_loss = loss.detach()\n481:            # advance the dataset for the next batch\n482:            x, y = train_loader.next_batch()\n483:            # backward pass\n484:            if i < train_accumulation_steps:\n485:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n486:                    loss.backward()\n487:            else:\n488:                loss.backward() # just sync on the last step\n489:        for p in model.parameters():\n490:            p.grad /= train_accumulation_steps\n491:        # step the optimizers and schedulers\n492:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n493:        #     opt.step()\n494:        #     sched.step()\n495:        # null the gradients\n496:        model.zero_grad(set_to_none=True)\n497:        # --------------- TRAINING SECTION END -------------------\n498:        # everything that follows now is just diagnostics, prints, logging, etc.\n499:\n500:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n501:        if master_process:\n502:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n503:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n504:            with open(logfile, \"a\") as f:\n505:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n506:\n507:    torch.save(model.state_dict(), \"model.pt\")\n508:    with open(\"model_config.pt\", \"wb\") as f:\n509:        pickle.dump(raw_model.get_config(), f)\n510:    f.close()\n511:\n512:    if master_process:\n513:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n514:\n515:    # -------------------------------------------------------------------------\n516:    # clean up nice\n517:    dist.destroy_process_group()\n518:\n519:if __name__ == \"__main__\":\n520:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the scheduler is initialized.\n\nStep 6: Add the new optimizer and scheduler step calls in the training loop after gradient averaging (after line 490).\n```bash\ninsert 490\n        # Step the new optimizer and scheduler\n        optimizer.step()\n        scheduler.step()\nend_of_insert\n```",
      "thought": "DISCUSSION\nOkay, the scheduler is initialized.\n\nStep 6: Add the new optimizer and scheduler step calls in the training loop after gradient averaging (after line 490).\n",
      "action": "insert 490\n        # Step the new optimizer and scheduler\n        optimizer.step()\n        scheduler.step()\nend_of_insert\n",
      "execution_time": 0.8318955982103944,
      "observation": "[File: /home/agent/workspace/baseline.py (523 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n411:    # New AdamW optimizer setup\n412:    optimizer = torch.optim.AdamW(\n413:        raw_model.parameters(),\n414:        lr=3e-4, # Max learning rate\n415:        weight_decay=args.weight_decay,\n416:        betas=(0.9, 0.95),\n417:        fused=True,\n418:    )\n419:    # learning rate decay scheduler (linear warmup and warmdown)\n420:    def get_lr(it):\n421:        assert it <= args.num_iterations\n422:        # 1) linear warmup for warmup_iters steps\n423:        if it < args.warmup_iters:\n424:            return (it+1) / args.warmup_iters\n425:        # 2) constant lr for a while\n426:        elif it < args.num_iterations - args.warmdown_iters:\n427:            return 1.0\n428:        # 3) linear warmdown\n429:        else:\n430:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n431:            return decay_ratio\n432:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n433:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n434:\n435:    # begin logging\n436:    if master_process:\n437:        run_id = str(uuid.uuid4())\n438:        logdir = 'logs/%s/' % run_id\n439:        os.makedirs(logdir, exist_ok=True)\n440:        logfile = 'logs/%s.txt' % run_id\n441:        # create the log file\n442:        with open(logfile, \"w\") as f:\n443:            # begin the log by printing this file (the Python code)\n444:            f.write('='*100 + '\\n')\n445:            f.write(code)\n446:            f.write('='*100 + '\\n')\n447:            # log information about the hardware/software environment this is running on\n448:            # and print the full `nvidia-smi` to file\n449:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n450:            import subprocess\n451:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n452:            f.write(f'{result.stdout}\\n')\n453:            f.write('='*100 + '\\n')\n454:\n455:    training_time_ms = 0\n456:    # start the clock\n457:    torch.cuda.synchronize()\n458:    t0 = time.time()\n459:    # begin training\n460:    train_loader.reset()\n461:    for step in range(args.num_iterations + 1):\n462:        last_step = (step == args.num_iterations)\n463:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n464:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n465:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n466:        if step == 10:\n467:            training_time_ms = 0\n468:            t0 = time.time()\n469:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n470:\n471:        if last_step:\n472:            break\n473:\n474:        # --------------- TRAINING SECTION BEGIN -----------------\n475:        model.train()\n476:        for i in range(1, train_accumulation_steps+1):\n477:            # forward pass\n478:            with ctx:\n479:                _, loss = model(x, y, return_logits=False)\n480:                train_loss = loss.detach()\n481:            # advance the dataset for the next batch\n482:            x, y = train_loader.next_batch()\n483:            # backward pass\n484:            if i < train_accumulation_steps:\n485:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n486:                    loss.backward()\n487:            else:\n488:                loss.backward() # just sync on the last step\n489:        for p in model.parameters():\n490:            p.grad /= train_accumulation_steps\n491:        # Step the new optimizer and scheduler\n492:        optimizer.step()\n493:        scheduler.step()\n494:        # step the optimizers and schedulers\n495:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n496:        #     opt.step()\n497:        #     sched.step()\n498:        # null the gradients\n499:        model.zero_grad(set_to_none=True)\n500:        # --------------- TRAINING SECTION END -------------------\n501:        # everything that follows now is just diagnostics, prints, logging, etc.\n502:\n503:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n504:        if master_process:\n505:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n506:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n507:            with open(logfile, \"a\") as f:\n508:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n509:\n510:    torch.save(model.state_dict(), \"model.pt\")\n511:    with open(\"model_config.pt\", \"wb\") as f:\n512:        pickle.dump(raw_model.get_config(), f)\n513:    f.close()\n514:\n515:    if master_process:\n516:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n517:\n518:    # -------------------------------------------------------------------------\n519:    # clean up nice\n520:    dist.destroy_process_group()\n521:\n522:if __name__ == \"__main__\":\n523:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nLooks correct. The new optimizer and scheduler steps are inserted at the right place, before zeroing the gradients.\n\nStep 7: Finally, remove the Muon optimizer class definition and related functions, as they are no longer needed (lines 31-131).\n```bash\nedit 31:131\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n```",
      "thought": "DISCUSSION\nLooks correct. The new optimizer and scheduler steps are inserted at the right place, before zeroing the gradients.\n\nStep 7: Finally, remove the Muon optimizer class definition and related functions, as they are no longer needed (lines 31-131).\n",
      "action": "edit 31:131\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n",
      "execution_time": 0.7610982367768884,
      "observation": "[File: /home/agent/workspace/baseline.py (424 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:# -----------------------------------------------------------------------------\n35:# PyTorch nn.Module definitions for the GPT-2 model\n36:\n37:class Rotary(torch.nn.Module):\n38:\n39:    def __init__(self, dim, base=10000):\n40:        super().__init__()\n41:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n42:        self.seq_len_cached = None\n43:        self.cos_cached = None\n44:        self.sin_cached = None\n45:\n46:    def forward(self, x):\n47:        seq_len = x.shape[1]\n48:        if seq_len != self.seq_len_cached:\n49:            self.seq_len_cached = seq_len\n50:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n51:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n52:            self.cos_cached = freqs.cos().bfloat16()\n53:            self.sin_cached = freqs.sin().bfloat16()\n54:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n55:\n56:def apply_rotary_emb(x, cos, sin):\n57:    assert x.ndim == 4 # multihead attention\n58:    d = x.shape[3]//2\n59:    x1 = x[..., :d]\n60:    x2 = x[..., d:]\n61:    y1 = x1 * cos + x2 * sin\n62:    y2 = x1 * (-sin) + x2 * cos\n63:    return torch.cat([y1, y2], 3).type_as(x)\n64:\n65:class CausalSelfAttention(nn.Module):\n66:\n67:    def __init__(self, config):\n68:        super().__init__()\n69:        self.n_head = config.n_head\n70:        self.n_embd = config.n_embd\n71:        self.head_dim = self.n_embd // self.n_head\n72:        assert self.n_embd % self.n_head == 0\n73:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n74:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n76:        # output projection\n77:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n78:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n79:        self.rotary = Rotary(self.head_dim)\n80:\n81:    def forward(self, x):\n82:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n83:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n84:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n85:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n86:        cos, sin = self.rotary(q)\n87:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n88:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n89:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n90:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n91:        y = self.c_proj(y)\n92:        return y\n93:\n94:class MLP(nn.Module):\n95:\n96:    def __init__(self, config):\n97:        super().__init__()\n98:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n99:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n100:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n101:\n102:    def forward(self, x):\n103:        x = self.c_fc(x)\n104:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n105:        x = self.c_proj(x)\n106:        return x\n107:\n108:class Block(nn.Module):\n109:\n110:    def __init__(self, config):\n111:        super().__init__()\n112:        self.attn = CausalSelfAttention(config)\n113:        self.mlp = MLP(config)\n114:\n115:    def forward(self, x):\n116:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n117:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n118:        return x\n119:\n120:# -----------------------------------------------------------------------------\n121:# The main GPT-2 model\n122:\n123:@dataclass\n124:class ModelConfig:\n125:    vocab_size : int = 50304\n126:    n_layer : int = 12\n127:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n128:    n_embd : int = 768\n129:\n130:class Model(nn.Module):\n131:\n132:    def __init__(self, config):\n133:        super().__init__()\n134:        self.config = config\n135:\n136:        self.transformer = nn.ModuleDict(dict(\n137:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n138:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n139:        ))\n140:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n141:        self.lm_head.weight.data.zero_() # @Grad62304977\n142:\n143:    def forward(self, idx, targets=None, return_logits=True):\n144:\n145:        # forward the GPT model itself\n146:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n147:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n148:        for block in self.transformer.h:\n149:            x = block(x)\n150:        x = F.rms_norm(x, (x.size(-1),))\n151:\n152:        if targets is not None:\n153:            # if we are given some desired targets also calculate the loss\n154:            logits = self.lm_head(x)\n155:            logits = logits.float() # use tf32/fp32 for logits\n156:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n157:        else:\n158:            # inference-time mini-optimization: only forward the lm_head on the very last position\n159:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n160:            logits = logits.float() # use tf32/fp32 for logits\n161:            loss = None\n162:\n163:        # there are performance reasons why not returning logits is prudent, if not needed\n164:        if not return_logits:\n165:            logits = None\n166:\n167:        return logits, loss\n168:    \n169:    def get_config(self):\n170:        return self.config\n171:\n172:# -----------------------------------------------------------------------------\n173:# Our own simple Distributed Data Loader\n174:\n175:def _peek_data_shard(filename):\n176:    # only reads the header, returns header data\n177:    with open(filename, \"rb\") as f:\n178:        # first read the header, which is 256 int32 integers (4 bytes each)\n179:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n180:    if header[0] != 20240520:\n181:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n182:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n183:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n184:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n185:        exit(1)\n186:    assert header[1] == 1, \"unsupported version\"\n187:    ntok = header[2] # number of tokens (claimed)\n188:    return ntok # for now just return the number of tokens\n189:\n190:def _load_data_shard(filename):\n191:    with open(filename, \"rb\") as f:\n192:        # first read the header, which is 256 int32 integers (4 bytes each)\n193:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n194:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n195:        assert header[1] == 1, \"unsupported version\"\n196:        ntok = header[2] # number of tokens (claimed)\n197:        # the rest of it are tokens, stored as uint16\n198:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n199:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n200:    return tokens\n201:\n202:class DistributedDataLoader:\n203:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n204:        self.process_rank = process_rank\n205:        self.num_processes = num_processes\n206:        self.B = B\n207:        self.T = T\n208:\n209:        # glob files that match the pattern\n210:        self.files = sorted(glob.glob(filename_pattern))\n211:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n212:\n213:        # load and validate all data shards, count number of tokens in total\n214:        ntok_total = 0\n215:        for fname in self.files:\n216:            shard_ntok = _peek_data_shard(fname)\n217:            assert shard_ntok >= num_processes * B * T + 1\n218:            ntok_total += int(shard_ntok)\n219:        self.ntok_total = ntok_total\n220:\n221:        # kick things off\n222:        self.reset()\n223:\n224:    def reset(self):\n225:        self.current_shard = 0\n226:        self.current_position = self.process_rank * self.B * self.T\n227:        self.tokens = _load_data_shard(self.files[self.current_shard])\n228:\n229:    def advance(self): # advance to next data shard\n230:        self.current_shard = (self.current_shard + 1) % len(self.files)\n231:        self.current_position = self.process_rank * self.B * self.T\n232:        self.tokens = _load_data_shard(self.files[self.current_shard])\n233:\n234:    def next_batch(self):\n235:        B = self.B\n236:        T = self.T\n237:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n238:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n239:        x = (buf[:-1]).view(B, T) # inputs\n240:        y = (buf[1:]).view(B, T) # targets\n241:        # advance current position and load next shard if necessary\n242:        self.current_position += B * T * self.num_processes\n243:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n244:            self.advance()\n245:        return x.cuda(), y.cuda()\n246:\n247:# -----------------------------------------------------------------------------\n248:# int main\n249:\n250:@dataclass\n251:class Hyperparameters:\n252:    # data hyperparams\n253:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n254:    # optimization hyperparams\n255:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n256:    device_batch_size : int = 64 # batch size, in sequences, per device\n257:    sequence_length : int = 1024 # sequence length, in tokens\n258:    num_iterations : int = 10000 # number of iterations to run\n259:    warmup_iters : int = 1000 # number of iterations for linear warmup\n260:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n261:    weight_decay : float = 0.1\n262:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n263:\n264:\n265:def main():\n266:    with open(sys.argv[0]) as f:\n267:        code = f.read() # read the code of this file ASAP, for logging\n268:    args = Hyperparameters()\n269:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n270:\n271:    # set up DDP (distributed data parallel). torchrun sets this env variable\n272:    assert torch.cuda.is_available()\n273:    dist.init_process_group(backend='nccl')\n274:    ddp_rank = int(os.environ['RANK'])\n275:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n276:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n277:    device = f'cuda:{ddp_local_rank}'\n278:    torch.cuda.set_device(device)\n279:    print(f\"using device: {device}\")\n280:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n281:\n282:    # convenience variables\n283:    B, T = args.device_batch_size, args.sequence_length\n284:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n285:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n286:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n287:\n288:    # load tokens\n289:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n290:    if master_process:\n291:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n292:    x, y = train_loader.next_batch()\n293:\n294:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n295:    # this originates from Karpathy's experiments.\n296:    num_vocab = 50304\n297:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n298:    model = model.cuda()\n299:    if hasattr(config, \"coordinate_descent_tuning\"):\n300:        config.coordinate_descent_tuning = True # suggested by @Chillee\n301:    model = torch.compile(model)\n302:    # here we wrap model into DDP container\n303:    model = DDP(model, device_ids=[ddp_local_rank])\n304:    raw_model = model.module # always contains the \"raw\" unwrapped model\n305:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n306:\n307:    # init the optimizer(s)\n308:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n309:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n310:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n311:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n312:    # New AdamW optimizer setup\n313:    optimizer = torch.optim.AdamW(\n314:        raw_model.parameters(),\n315:        lr=3e-4, # Max learning rate\n316:        weight_decay=args.weight_decay,\n317:        betas=(0.9, 0.95),\n318:        fused=True,\n319:    )\n320:    # learning rate decay scheduler (linear warmup and warmdown)\n321:    def get_lr(it):\n322:        assert it <= args.num_iterations\n323:        # 1) linear warmup for warmup_iters steps\n324:        if it < args.warmup_iters:\n325:            return (it+1) / args.warmup_iters\n326:        # 2) constant lr for a while\n327:        elif it < args.num_iterations - args.warmdown_iters:\n328:            return 1.0\n329:        # 3) linear warmdown\n330:        else:\n331:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n332:            return decay_ratio\n333:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n334:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n335:\n336:    # begin logging\n337:    if master_process:\n338:        run_id = str(uuid.uuid4())\n339:        logdir = 'logs/%s/' % run_id\n340:        os.makedirs(logdir, exist_ok=True)\n341:        logfile = 'logs/%s.txt' % run_id\n342:        # create the log file\n343:        with open(logfile, \"w\") as f:\n344:            # begin the log by printing this file (the Python code)\n345:            f.write('='*100 + '\\n')\n346:            f.write(code)\n347:            f.write('='*100 + '\\n')\n348:            # log information about the hardware/software environment this is running on\n349:            # and print the full `nvidia-smi` to file\n350:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n351:            import subprocess\n352:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n353:            f.write(f'{result.stdout}\\n')\n354:            f.write('='*100 + '\\n')\n355:\n356:    training_time_ms = 0\n357:    # start the clock\n358:    torch.cuda.synchronize()\n359:    t0 = time.time()\n360:    # begin training\n361:    train_loader.reset()\n362:    for step in range(args.num_iterations + 1):\n363:        last_step = (step == args.num_iterations)\n364:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n365:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n366:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n367:        if step == 10:\n368:            training_time_ms = 0\n369:            t0 = time.time()\n370:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n371:\n372:        if last_step:\n373:            break\n374:\n375:        # --------------- TRAINING SECTION BEGIN -----------------\n376:        model.train()\n377:        for i in range(1, train_accumulation_steps+1):\n378:            # forward pass\n379:            with ctx:\n380:                _, loss = model(x, y, return_logits=False)\n381:                train_loss = loss.detach()\n382:            # advance the dataset for the next batch\n383:            x, y = train_loader.next_batch()\n384:            # backward pass\n385:            if i < train_accumulation_steps:\n386:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n387:                    loss.backward()\n388:            else:\n389:                loss.backward() # just sync on the last step\n390:        for p in model.parameters():\n391:            p.grad /= train_accumulation_steps\n392:        # Step the new optimizer and scheduler\n393:        optimizer.step()\n394:        scheduler.step()\n395:        # step the optimizers and schedulers\n396:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n397:        #     opt.step()\n398:        #     sched.step()\n399:        # null the gradients\n400:        model.zero_grad(set_to_none=True)\n401:        # --------------- TRAINING SECTION END -------------------\n402:        # everything that follows now is just diagnostics, prints, logging, etc.\n403:\n404:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n405:        if master_process:\n406:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n407:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n408:            with open(logfile, \"a\") as f:\n409:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n410:\n411:    torch.save(model.state_dict(), \"model.pt\")\n412:    with open(\"model_config.pt\", \"wb\") as f:\n413:        pickle.dump(raw_model.get_config(), f)\n414:    f.close()\n415:\n416:    if master_process:\n417:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n418:\n419:    # -------------------------------------------------------------------------\n420:    # clean up nice\n421:    dist.destroy_process_group()\n422:\n423:if __name__ == \"__main__\":\n424:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the Muon optimizer code has been removed. There are some redundant comment lines left over (lines 30-35). Let's clean those up.\n```bash\nedit 30:35\n\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the Muon optimizer code has been removed. There are some redundant comment lines left over (lines 30-35). Let's clean those up.\n",
      "action": "edit 30:35\n\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n",
      "execution_time": 0.8031464302912354,
      "observation": "[File: /home/agent/workspace/baseline.py (421 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:# The main GPT-2 model\n119:\n120:@dataclass\n121:class ModelConfig:\n122:    vocab_size : int = 50304\n123:    n_layer : int = 12\n124:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n125:    n_embd : int = 768\n126:\n127:class Model(nn.Module):\n128:\n129:    def __init__(self, config):\n130:        super().__init__()\n131:        self.config = config\n132:\n133:        self.transformer = nn.ModuleDict(dict(\n134:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n135:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n136:        ))\n137:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n138:        self.lm_head.weight.data.zero_() # @Grad62304977\n139:\n140:    def forward(self, idx, targets=None, return_logits=True):\n141:\n142:        # forward the GPT model itself\n143:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n144:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n145:        for block in self.transformer.h:\n146:            x = block(x)\n147:        x = F.rms_norm(x, (x.size(-1),))\n148:\n149:        if targets is not None:\n150:            # if we are given some desired targets also calculate the loss\n151:            logits = self.lm_head(x)\n152:            logits = logits.float() # use tf32/fp32 for logits\n153:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n154:        else:\n155:            # inference-time mini-optimization: only forward the lm_head on the very last position\n156:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n157:            logits = logits.float() # use tf32/fp32 for logits\n158:            loss = None\n159:\n160:        # there are performance reasons why not returning logits is prudent, if not needed\n161:        if not return_logits:\n162:            logits = None\n163:\n164:        return logits, loss\n165:    \n166:    def get_config(self):\n167:        return self.config\n168:\n169:# -----------------------------------------------------------------------------\n170:# Our own simple Distributed Data Loader\n171:\n172:def _peek_data_shard(filename):\n173:    # only reads the header, returns header data\n174:    with open(filename, \"rb\") as f:\n175:        # first read the header, which is 256 int32 integers (4 bytes each)\n176:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n177:    if header[0] != 20240520:\n178:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n179:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n180:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n181:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n182:        exit(1)\n183:    assert header[1] == 1, \"unsupported version\"\n184:    ntok = header[2] # number of tokens (claimed)\n185:    return ntok # for now just return the number of tokens\n186:\n187:def _load_data_shard(filename):\n188:    with open(filename, \"rb\") as f:\n189:        # first read the header, which is 256 int32 integers (4 bytes each)\n190:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n191:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n192:        assert header[1] == 1, \"unsupported version\"\n193:        ntok = header[2] # number of tokens (claimed)\n194:        # the rest of it are tokens, stored as uint16\n195:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n196:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n197:    return tokens\n198:\n199:class DistributedDataLoader:\n200:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n201:        self.process_rank = process_rank\n202:        self.num_processes = num_processes\n203:        self.B = B\n204:        self.T = T\n205:\n206:        # glob files that match the pattern\n207:        self.files = sorted(glob.glob(filename_pattern))\n208:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n209:\n210:        # load and validate all data shards, count number of tokens in total\n211:        ntok_total = 0\n212:        for fname in self.files:\n213:            shard_ntok = _peek_data_shard(fname)\n214:            assert shard_ntok >= num_processes * B * T + 1\n215:            ntok_total += int(shard_ntok)\n216:        self.ntok_total = ntok_total\n217:\n218:        # kick things off\n219:        self.reset()\n220:\n221:    def reset(self):\n222:        self.current_shard = 0\n223:        self.current_position = self.process_rank * self.B * self.T\n224:        self.tokens = _load_data_shard(self.files[self.current_shard])\n225:\n226:    def advance(self): # advance to next data shard\n227:        self.current_shard = (self.current_shard + 1) % len(self.files)\n228:        self.current_position = self.process_rank * self.B * self.T\n229:        self.tokens = _load_data_shard(self.files[self.current_shard])\n230:\n231:    def next_batch(self):\n232:        B = self.B\n233:        T = self.T\n234:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n235:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n236:        x = (buf[:-1]).view(B, T) # inputs\n237:        y = (buf[1:]).view(B, T) # targets\n238:        # advance current position and load next shard if necessary\n239:        self.current_position += B * T * self.num_processes\n240:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n241:            self.advance()\n242:        return x.cuda(), y.cuda()\n243:\n244:# -----------------------------------------------------------------------------\n245:# int main\n246:\n247:@dataclass\n248:class Hyperparameters:\n249:    # data hyperparams\n250:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n251:    # optimization hyperparams\n252:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n253:    device_batch_size : int = 64 # batch size, in sequences, per device\n254:    sequence_length : int = 1024 # sequence length, in tokens\n255:    num_iterations : int = 10000 # number of iterations to run\n256:    warmup_iters : int = 1000 # number of iterations for linear warmup\n257:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n258:    weight_decay : float = 0.1\n259:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n260:\n261:\n262:def main():\n263:    with open(sys.argv[0]) as f:\n264:        code = f.read() # read the code of this file ASAP, for logging\n265:    args = Hyperparameters()\n266:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n267:\n268:    # set up DDP (distributed data parallel). torchrun sets this env variable\n269:    assert torch.cuda.is_available()\n270:    dist.init_process_group(backend='nccl')\n271:    ddp_rank = int(os.environ['RANK'])\n272:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n273:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n274:    device = f'cuda:{ddp_local_rank}'\n275:    torch.cuda.set_device(device)\n276:    print(f\"using device: {device}\")\n277:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n278:\n279:    # convenience variables\n280:    B, T = args.device_batch_size, args.sequence_length\n281:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n282:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n283:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n284:\n285:    # load tokens\n286:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n287:    if master_process:\n288:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n289:    x, y = train_loader.next_batch()\n290:\n291:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n292:    # this originates from Karpathy's experiments.\n293:    num_vocab = 50304\n294:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n295:    model = model.cuda()\n296:    if hasattr(config, \"coordinate_descent_tuning\"):\n297:        config.coordinate_descent_tuning = True # suggested by @Chillee\n298:    model = torch.compile(model)\n299:    # here we wrap model into DDP container\n300:    model = DDP(model, device_ids=[ddp_local_rank])\n301:    raw_model = model.module # always contains the \"raw\" unwrapped model\n302:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n303:\n304:    # init the optimizer(s)\n305:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n306:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n307:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n308:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n309:    # New AdamW optimizer setup\n310:    optimizer = torch.optim.AdamW(\n311:        raw_model.parameters(),\n312:        lr=3e-4, # Max learning rate\n313:        weight_decay=args.weight_decay,\n314:        betas=(0.9, 0.95),\n315:        fused=True,\n316:    )\n317:    # learning rate decay scheduler (linear warmup and warmdown)\n318:    def get_lr(it):\n319:        assert it <= args.num_iterations\n320:        # 1) linear warmup for warmup_iters steps\n321:        if it < args.warmup_iters:\n322:            return (it+1) / args.warmup_iters\n323:        # 2) constant lr for a while\n324:        elif it < args.num_iterations - args.warmdown_iters:\n325:            return 1.0\n326:        # 3) linear warmdown\n327:        else:\n328:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n329:            return decay_ratio\n330:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n331:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n332:\n333:    # begin logging\n334:    if master_process:\n335:        run_id = str(uuid.uuid4())\n336:        logdir = 'logs/%s/' % run_id\n337:        os.makedirs(logdir, exist_ok=True)\n338:        logfile = 'logs/%s.txt' % run_id\n339:        # create the log file\n340:        with open(logfile, \"w\") as f:\n341:            # begin the log by printing this file (the Python code)\n342:            f.write('='*100 + '\\n')\n343:            f.write(code)\n344:            f.write('='*100 + '\\n')\n345:            # log information about the hardware/software environment this is running on\n346:            # and print the full `nvidia-smi` to file\n347:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n348:            import subprocess\n349:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n350:            f.write(f'{result.stdout}\\n')\n351:            f.write('='*100 + '\\n')\n352:\n353:    training_time_ms = 0\n354:    # start the clock\n355:    torch.cuda.synchronize()\n356:    t0 = time.time()\n357:    # begin training\n358:    train_loader.reset()\n359:    for step in range(args.num_iterations + 1):\n360:        last_step = (step == args.num_iterations)\n361:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n362:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n363:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n364:        if step == 10:\n365:            training_time_ms = 0\n366:            t0 = time.time()\n367:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n368:\n369:        if last_step:\n370:            break\n371:\n372:        # --------------- TRAINING SECTION BEGIN -----------------\n373:        model.train()\n374:        for i in range(1, train_accumulation_steps+1):\n375:            # forward pass\n376:            with ctx:\n377:                _, loss = model(x, y, return_logits=False)\n378:                train_loss = loss.detach()\n379:            # advance the dataset for the next batch\n380:            x, y = train_loader.next_batch()\n381:            # backward pass\n382:            if i < train_accumulation_steps:\n383:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n384:                    loss.backward()\n385:            else:\n386:                loss.backward() # just sync on the last step\n387:        for p in model.parameters():\n388:            p.grad /= train_accumulation_steps\n389:        # Step the new optimizer and scheduler\n390:        optimizer.step()\n391:        scheduler.step()\n392:        # step the optimizers and schedulers\n393:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n394:        #     opt.step()\n395:        #     sched.step()\n396:        # null the gradients\n397:        model.zero_grad(set_to_none=True)\n398:        # --------------- TRAINING SECTION END -------------------\n399:        # everything that follows now is just diagnostics, prints, logging, etc.\n400:\n401:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n402:        if master_process:\n403:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n404:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n405:            with open(logfile, \"a\") as f:\n406:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n407:\n408:    torch.save(model.state_dict(), \"model.pt\")\n409:    with open(\"model_config.pt\", \"wb\") as f:\n410:        pickle.dump(raw_model.get_config(), f)\n411:    f.close()\n412:\n413:    if master_process:\n414:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n415:\n416:    # -------------------------------------------------------------------------\n417:    # clean up nice\n418:    dist.destroy_process_group()\n419:\n420:if __name__ == \"__main__\":\n421:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe script looks much cleaner now. The main changes were:\n- Increased `num_iterations` from 500 to 10000.\n- Adjusted `warmup_iters` and `warmdown_iters` to 1000 each.\n- Replaced the complex multi-optimizer setup with a single AdamW optimizer with `lr=3e-4`, `weight_decay=0.1`, `betas=(0.9, 0.95)`.\n- Updated the scheduler to work with the new optimizer.\n- Removed the unused Muon optimizer code.\n\nNow, I'm ready to run the training. The script uses `torchrun`, so I need to determine the number of available GPUs first.\n```bash\nnvidia-smi --query-gpu=count --format=csv,noheader\n```",
      "thought": "DISCUSSION\nThe script looks much cleaner now. The main changes were:\n- Increased `num_iterations` from 500 to 10000.\n- Adjusted `warmup_iters` and `warmdown_iters` to 1000 each.\n- Replaced the complex multi-optimizer setup with a single AdamW optimizer with `lr=3e-4`, `weight_decay=0.1`, `betas=(0.9, 0.95)`.\n- Updated the scheduler to work with the new optimizer.\n- Removed the unused Muon optimizer code.\n\nNow, I'm ready to run the training. The script uses `torchrun`, so I need to determine the number of available GPUs first.\n",
      "action": "nvidia-smi --query-gpu=count --format=csv,noheader\n",
      "execution_time": 0.3007946228608489,
      "observation": "2\n2"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nIt seems the command outputted the number twice. There are 2 GPUs available. I'll use `torchrun --nproc_per_node=2` to launch the script. Training might take a while with 10000 iterations.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nIt seems the command outputted the number twice. There are 2 GPUs available. I'll use `torchrun --nproc_per_node=2` to launch the script. Training might take a while with 10000 iterations.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 4.892889183014631,
      "observation": "W0405 11:21:42.609000 768 site-packages/torch/distributed/run.py:793] \nW0405 11:21:42.609000 768 site-packages/torch/distributed/run.py:793] *****************************************\nW0405 11:21:42.609000 768 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0405 11:21:42.609000 768 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 421, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 286, in main\n[rank1]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 208, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 421, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 286, in main\n[rank0]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 208, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\n[rank0]:[W405 11:21:45.035800547 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0405 11:21:45.448000 768 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 835 closing signal SIGTERM\nE0405 11:21:45.562000 768 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 834) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-05_11:21:45\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 834)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe training failed because the script couldn't find the data files at the expected location: `./data/fineweb10B/fineweb_train_*.bin`.\n\nThe `AssertionError` comes from the `DistributedDataLoader` which uses `glob.glob` to find the training shards. This means either the directory `data/fineweb10B` doesn't exist, or it doesn't contain files matching `fineweb_train_*.bin`.\n\nLet's check the contents of the `data` directory to see what's inside.\n```bash\nls -l data/\n```",
      "thought": "DISCUSSION\nThe training failed because the script couldn't find the data files at the expected location: `./data/fineweb10B/fineweb_train_*.bin`.\n\nThe `AssertionError` comes from the `DistributedDataLoader` which uses `glob.glob` to find the training shards. This means either the directory `data/fineweb10B` doesn't exist, or it doesn't contain files matching `fineweb_train_*.bin`.\n\nLet's check the contents of the `data` directory to see what's inside.\n",
      "action": "ls -l data/\n",
      "execution_time": 0.3007391085848212,
      "observation": "total 0\ndr-xr-xr-x 1 agent mlgym 908 Mar 26 02:59 fineweb108/"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAh, the directory name is `fineweb108/`, not `fineweb10B/`. That's a simple typo in the script's `Hyperparameters`.\n\nLet's correct the `input_bin` path in the `Hyperparameters` dataclass (line 250).\n```bash\nedit 250:250\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, the directory name is `fineweb108/`, not `fineweb10B/`. That's a simple typo in the script's `Hyperparameters`.\n\nLet's correct the `input_bin` path in the `Hyperparameters` dataclass (line 250).\n",
      "action": "edit 250:250\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "execution_time": 0.9444626299664378,
      "observation": "[File: /home/agent/workspace/baseline.py (421 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:# The main GPT-2 model\n119:\n120:@dataclass\n121:class ModelConfig:\n122:    vocab_size : int = 50304\n123:    n_layer : int = 12\n124:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n125:    n_embd : int = 768\n126:\n127:class Model(nn.Module):\n128:\n129:    def __init__(self, config):\n130:        super().__init__()\n131:        self.config = config\n132:\n133:        self.transformer = nn.ModuleDict(dict(\n134:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n135:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n136:        ))\n137:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n138:        self.lm_head.weight.data.zero_() # @Grad62304977\n139:\n140:    def forward(self, idx, targets=None, return_logits=True):\n141:\n142:        # forward the GPT model itself\n143:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n144:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n145:        for block in self.transformer.h:\n146:            x = block(x)\n147:        x = F.rms_norm(x, (x.size(-1),))\n148:\n149:        if targets is not None:\n150:            # if we are given some desired targets also calculate the loss\n151:            logits = self.lm_head(x)\n152:            logits = logits.float() # use tf32/fp32 for logits\n153:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n154:        else:\n155:            # inference-time mini-optimization: only forward the lm_head on the very last position\n156:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n157:            logits = logits.float() # use tf32/fp32 for logits\n158:            loss = None\n159:\n160:        # there are performance reasons why not returning logits is prudent, if not needed\n161:        if not return_logits:\n162:            logits = None\n163:\n164:        return logits, loss\n165:    \n166:    def get_config(self):\n167:        return self.config\n168:\n169:# -----------------------------------------------------------------------------\n170:# Our own simple Distributed Data Loader\n171:\n172:def _peek_data_shard(filename):\n173:    # only reads the header, returns header data\n174:    with open(filename, \"rb\") as f:\n175:        # first read the header, which is 256 int32 integers (4 bytes each)\n176:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n177:    if header[0] != 20240520:\n178:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n179:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n180:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n181:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n182:        exit(1)\n183:    assert header[1] == 1, \"unsupported version\"\n184:    ntok = header[2] # number of tokens (claimed)\n185:    return ntok # for now just return the number of tokens\n186:\n187:def _load_data_shard(filename):\n188:    with open(filename, \"rb\") as f:\n189:        # first read the header, which is 256 int32 integers (4 bytes each)\n190:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n191:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n192:        assert header[1] == 1, \"unsupported version\"\n193:        ntok = header[2] # number of tokens (claimed)\n194:        # the rest of it are tokens, stored as uint16\n195:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n196:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n197:    return tokens\n198:\n199:class DistributedDataLoader:\n200:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n201:        self.process_rank = process_rank\n202:        self.num_processes = num_processes\n203:        self.B = B\n204:        self.T = T\n205:\n206:        # glob files that match the pattern\n207:        self.files = sorted(glob.glob(filename_pattern))\n208:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n209:\n210:        # load and validate all data shards, count number of tokens in total\n211:        ntok_total = 0\n212:        for fname in self.files:\n213:            shard_ntok = _peek_data_shard(fname)\n214:            assert shard_ntok >= num_processes * B * T + 1\n215:            ntok_total += int(shard_ntok)\n216:        self.ntok_total = ntok_total\n217:\n218:        # kick things off\n219:        self.reset()\n220:\n221:    def reset(self):\n222:        self.current_shard = 0\n223:        self.current_position = self.process_rank * self.B * self.T\n224:        self.tokens = _load_data_shard(self.files[self.current_shard])\n225:\n226:    def advance(self): # advance to next data shard\n227:        self.current_shard = (self.current_shard + 1) % len(self.files)\n228:        self.current_position = self.process_rank * self.B * self.T\n229:        self.tokens = _load_data_shard(self.files[self.current_shard])\n230:\n231:    def next_batch(self):\n232:        B = self.B\n233:        T = self.T\n234:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n235:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n236:        x = (buf[:-1]).view(B, T) # inputs\n237:        y = (buf[1:]).view(B, T) # targets\n238:        # advance current position and load next shard if necessary\n239:        self.current_position += B * T * self.num_processes\n240:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n241:            self.advance()\n242:        return x.cuda(), y.cuda()\n243:\n244:# -----------------------------------------------------------------------------\n245:# int main\n246:\n247:@dataclass\n248:class Hyperparameters:\n249:    # data hyperparams\n250:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n251:    # optimization hyperparams\n252:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n253:    device_batch_size : int = 64 # batch size, in sequences, per device\n254:    sequence_length : int = 1024 # sequence length, in tokens\n255:    num_iterations : int = 10000 # number of iterations to run\n256:    warmup_iters : int = 1000 # number of iterations for linear warmup\n257:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n258:    weight_decay : float = 0.1\n259:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n260:\n261:\n262:def main():\n263:    with open(sys.argv[0]) as f:\n264:        code = f.read() # read the code of this file ASAP, for logging\n265:    args = Hyperparameters()\n266:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n267:\n268:    # set up DDP (distributed data parallel). torchrun sets this env variable\n269:    assert torch.cuda.is_available()\n270:    dist.init_process_group(backend='nccl')\n271:    ddp_rank = int(os.environ['RANK'])\n272:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n273:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n274:    device = f'cuda:{ddp_local_rank}'\n275:    torch.cuda.set_device(device)\n276:    print(f\"using device: {device}\")\n277:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n278:\n279:    # convenience variables\n280:    B, T = args.device_batch_size, args.sequence_length\n281:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n282:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n283:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n284:\n285:    # load tokens\n286:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n287:    if master_process:\n288:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n289:    x, y = train_loader.next_batch()\n290:\n291:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n292:    # this originates from Karpathy's experiments.\n293:    num_vocab = 50304\n294:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n295:    model = model.cuda()\n296:    if hasattr(config, \"coordinate_descent_tuning\"):\n297:        config.coordinate_descent_tuning = True # suggested by @Chillee\n298:    model = torch.compile(model)\n299:    # here we wrap model into DDP container\n300:    model = DDP(model, device_ids=[ddp_local_rank])\n301:    raw_model = model.module # always contains the \"raw\" unwrapped model\n302:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n303:\n304:    # init the optimizer(s)\n305:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n306:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n307:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n308:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n309:    # New AdamW optimizer setup\n310:    optimizer = torch.optim.AdamW(\n311:        raw_model.parameters(),\n312:        lr=3e-4, # Max learning rate\n313:        weight_decay=args.weight_decay,\n314:        betas=(0.9, 0.95),\n315:        fused=True,\n316:    )\n317:    # learning rate decay scheduler (linear warmup and warmdown)\n318:    def get_lr(it):\n319:        assert it <= args.num_iterations\n320:        # 1) linear warmup for warmup_iters steps\n321:        if it < args.warmup_iters:\n322:            return (it+1) / args.warmup_iters\n323:        # 2) constant lr for a while\n324:        elif it < args.num_iterations - args.warmdown_iters:\n325:            return 1.0\n326:        # 3) linear warmdown\n327:        else:\n328:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n329:            return decay_ratio\n330:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n331:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n332:\n333:    # begin logging\n334:    if master_process:\n335:        run_id = str(uuid.uuid4())\n336:        logdir = 'logs/%s/' % run_id\n337:        os.makedirs(logdir, exist_ok=True)\n338:        logfile = 'logs/%s.txt' % run_id\n339:        # create the log file\n340:        with open(logfile, \"w\") as f:\n341:            # begin the log by printing this file (the Python code)\n342:            f.write('='*100 + '\\n')\n343:            f.write(code)\n344:            f.write('='*100 + '\\n')\n345:            # log information about the hardware/software environment this is running on\n346:            # and print the full `nvidia-smi` to file\n347:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n348:            import subprocess\n349:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n350:            f.write(f'{result.stdout}\\n')\n351:            f.write('='*100 + '\\n')\n352:\n353:    training_time_ms = 0\n354:    # start the clock\n355:    torch.cuda.synchronize()\n356:    t0 = time.time()\n357:    # begin training\n358:    train_loader.reset()\n359:    for step in range(args.num_iterations + 1):\n360:        last_step = (step == args.num_iterations)\n361:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n362:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n363:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n364:        if step == 10:\n365:            training_time_ms = 0\n366:            t0 = time.time()\n367:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n368:\n369:        if last_step:\n370:            break\n371:\n372:        # --------------- TRAINING SECTION BEGIN -----------------\n373:        model.train()\n374:        for i in range(1, train_accumulation_steps+1):\n375:            # forward pass\n376:            with ctx:\n377:                _, loss = model(x, y, return_logits=False)\n378:                train_loss = loss.detach()\n379:            # advance the dataset for the next batch\n380:            x, y = train_loader.next_batch()\n381:            # backward pass\n382:            if i < train_accumulation_steps:\n383:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n384:                    loss.backward()\n385:            else:\n386:                loss.backward() # just sync on the last step\n387:        for p in model.parameters():\n388:            p.grad /= train_accumulation_steps\n389:        # Step the new optimizer and scheduler\n390:        optimizer.step()\n391:        scheduler.step()\n392:        # step the optimizers and schedulers\n393:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n394:        #     opt.step()\n395:        #     sched.step()\n396:        # null the gradients\n397:        model.zero_grad(set_to_none=True)\n398:        # --------------- TRAINING SECTION END -------------------\n399:        # everything that follows now is just diagnostics, prints, logging, etc.\n400:\n401:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n402:        if master_process:\n403:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n404:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n405:            with open(logfile, \"a\") as f:\n406:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n407:\n408:    torch.save(model.state_dict(), \"model.pt\")\n409:    with open(\"model_config.pt\", \"wb\") as f:\n410:        pickle.dump(raw_model.get_config(), f)\n411:    f.close()\n412:\n413:    if master_process:\n414:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n415:\n416:    # -------------------------------------------------------------------------\n417:    # clean up nice\n418:    dist.destroy_process_group()\n419:\n420:if __name__ == \"__main__\":\n421:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe data path has been corrected to `fineweb108`. Let's try running the training script again.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThe data path has been corrected to `fineweb108`. Let's try running the training script again.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 1781.8383913449943,
      "observation": "W0405 11:22:01.150000 905 site-packages/torch/distributed/run.py:793] \nW0405 11:22:01.150000 905 site-packages/torch/distributed/run.py:793] *****************************************\nW0405 11:22:01.150000 905 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0405 11:22:01.150000 905 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nusing device: cuda:1\nstep:1/10000 train_loss:10.8258 train_time:58187ms step_avg:nanms\nstep:2/10000 train_loss:10.8258 train_time:69616ms step_avg:nanms\nstep:3/10000 train_loss:10.8257 train_time:69682ms step_avg:nanms\nstep:4/10000 train_loss:10.8257 train_time:69847ms step_avg:nanms\nstep:5/10000 train_loss:10.8255 train_time:70014ms step_avg:nanms\nstep:6/10000 train_loss:10.8253 train_time:70182ms step_avg:nanms\nstep:7/10000 train_loss:10.8250 train_time:70346ms step_avg:nanms\nstep:8/10000 train_loss:10.8247 train_time:70513ms step_avg:nanms\nstep:9/10000 train_loss:10.8243 train_time:70688ms step_avg:nanms\nstep:10/10000 train_loss:10.8239 train_time:70855ms step_avg:nanms\nstep:11/10000 train_loss:10.8234 train_time:65ms step_avg:nanms\nstep:12/10000 train_loss:10.8227 train_time:231ms step_avg:nanms\nstep:13/10000 train_loss:10.8219 train_time:403ms step_avg:134.25ms\nstep:14/10000 train_loss:10.8208 train_time:570ms step_avg:142.45ms\nstep:15/10000 train_loss:10.8197 train_time:739ms step_avg:147.70ms\nstep:16/10000 train_loss:10.8180 train_time:903ms step_avg:150.49ms\nstep:17/10000 train_loss:10.8169 train_time:1071ms step_avg:153.01ms\nstep:18/10000 train_loss:10.8145 train_time:1238ms step_avg:154.70ms\nstep:19/10000 train_loss:10.8119 train_time:1405ms step_avg:156.11ms\nstep:20/10000 train_loss:10.8092 train_time:1573ms step_avg:157.34ms\nstep:21/10000 train_loss:10.8058 train_time:1743ms step_avg:158.48ms\nstep:22/10000 train_loss:10.8013 train_time:1911ms step_avg:159.27ms\nstep:23/10000 train_loss:10.7969 train_time:2081ms step_avg:160.10ms\nstep:24/10000 train_loss:10.7920 train_time:2248ms step_avg:160.58ms\nstep:25/10000 train_loss:10.7867 train_time:2417ms step_avg:161.13ms\nstep:26/10000 train_loss:10.7829 train_time:2585ms step_avg:161.55ms\nstep:27/10000 train_loss:10.7733 train_time:2755ms step_avg:162.07ms\nstep:28/10000 train_loss:10.7662 train_time:2922ms step_avg:162.34ms\nstep:29/10000 train_loss:10.7569 train_time:3088ms step_avg:162.55ms\nstep:30/10000 train_loss:10.7500 train_time:3255ms step_avg:162.75ms\nstep:31/10000 train_loss:10.7384 train_time:3422ms step_avg:162.94ms\nstep:32/10000 train_loss:10.7251 train_time:3589ms step_avg:163.13ms\nstep:33/10000 train_loss:10.7192 train_time:3759ms step_avg:163.42ms\nstep:34/10000 train_loss:10.7067 train_time:3926ms step_avg:163.57ms\nstep:35/10000 train_loss:10.6962 train_time:4091ms step_avg:163.62ms\nstep:36/10000 train_loss:10.6863 train_time:4256ms step_avg:163.69ms\nstep:37/10000 train_loss:10.6705 train_time:4422ms step_avg:163.78ms\nstep:38/10000 train_loss:10.6609 train_time:4588ms step_avg:163.86ms\nstep:39/10000 train_loss:10.6467 train_time:4760ms step_avg:164.14ms\nstep:40/10000 train_loss:10.6361 train_time:4926ms step_avg:164.21ms\nstep:41/10000 train_loss:10.6237 train_time:5093ms step_avg:164.29ms\nstep:42/10000 train_loss:10.6083 train_time:5260ms step_avg:164.37ms\nstep:43/10000 train_loss:10.5940 train_time:5425ms step_avg:164.40ms\nstep:44/10000 train_loss:10.5801 train_time:5590ms step_avg:164.43ms\nstep:45/10000 train_loss:10.5610 train_time:5761ms step_avg:164.60ms\nstep:46/10000 train_loss:10.5472 train_time:5928ms step_avg:164.67ms\nstep:47/10000 train_loss:10.5366 train_time:6095ms step_avg:164.73ms\nstep:48/10000 train_loss:10.5206 train_time:6259ms step_avg:164.72ms\nstep:49/10000 train_loss:10.5054 train_time:6426ms step_avg:164.78ms\nstep:50/10000 train_loss:10.4898 train_time:6595ms step_avg:164.88ms\nstep:51/10000 train_loss:10.4853 train_time:6763ms step_avg:164.95ms\nstep:52/10000 train_loss:10.4564 train_time:6928ms step_avg:164.95ms\nstep:53/10000 train_loss:10.4459 train_time:7097ms step_avg:165.04ms\nstep:54/10000 train_loss:10.4247 train_time:7266ms step_avg:165.13ms\nstep:55/10000 train_loss:10.3941 train_time:7433ms step_avg:165.18ms\nstep:56/10000 train_loss:10.3902 train_time:7603ms step_avg:165.28ms\nstep:57/10000 train_loss:10.3717 train_time:7771ms step_avg:165.35ms\nstep:58/10000 train_loss:10.3555 train_time:7941ms step_avg:165.44ms\nstep:59/10000 train_loss:10.3365 train_time:8109ms step_avg:165.50ms\nstep:60/10000 train_loss:10.3256 train_time:8276ms step_avg:165.53ms\nstep:61/10000 train_loss:10.3095 train_time:8445ms step_avg:165.59ms\nstep:62/10000 train_loss:10.2865 train_time:8611ms step_avg:165.60ms\nstep:63/10000 train_loss:10.2643 train_time:8780ms step_avg:165.65ms\nstep:64/10000 train_loss:10.2437 train_time:8947ms step_avg:165.69ms\nstep:65/10000 train_loss:10.2466 train_time:9115ms step_avg:165.73ms\nstep:66/10000 train_loss:10.2246 train_time:9278ms step_avg:165.69ms\nstep:67/10000 train_loss:10.1781 train_time:9443ms step_avg:165.67ms\nstep:68/10000 train_loss:10.1662 train_time:9612ms step_avg:165.73ms\nstep:69/10000 train_loss:10.1576 train_time:9777ms step_avg:165.72ms\nstep:70/10000 train_loss:10.1373 train_time:9944ms step_avg:165.73ms\nstep:71/10000 train_loss:10.1276 train_time:10112ms step_avg:165.77ms\nstep:72/10000 train_loss:10.1453 train_time:10278ms step_avg:165.77ms\nstep:73/10000 train_loss:10.0778 train_time:10442ms step_avg:165.75ms\nstep:74/10000 train_loss:10.0507 train_time:10610ms step_avg:165.79ms\nstep:75/10000 train_loss:10.0577 train_time:10779ms step_avg:165.83ms\nstep:76/10000 train_loss:10.0162 train_time:10948ms step_avg:165.88ms\nstep:77/10000 train_loss:9.9929 train_time:11115ms step_avg:165.89ms\nstep:78/10000 train_loss:9.9442 train_time:11280ms step_avg:165.89ms\nstep:79/10000 train_loss:9.9599 train_time:11448ms step_avg:165.91ms\nstep:80/10000 train_loss:9.9566 train_time:11615ms step_avg:165.93ms\nstep:81/10000 train_loss:9.9067 train_time:11784ms step_avg:165.97ms\nstep:82/10000 train_loss:9.9186 train_time:11951ms step_avg:165.98ms\nstep:83/10000 train_loss:9.8642 train_time:12118ms step_avg:166.01ms\nstep:84/10000 train_loss:9.8384 train_time:12283ms step_avg:165.99ms\nstep:85/10000 train_loss:9.8524 train_time:12448ms step_avg:165.97ms\nstep:86/10000 train_loss:9.8724 train_time:12615ms step_avg:165.99ms\nstep:87/10000 train_loss:9.8215 train_time:12784ms step_avg:166.02ms\nstep:88/10000 train_loss:9.7628 train_time:12951ms step_avg:166.04ms\nstep:89/10000 train_loss:9.7468 train_time:13119ms step_avg:166.06ms\nstep:90/10000 train_loss:9.7395 train_time:13284ms step_avg:166.04ms\nstep:91/10000 train_loss:9.7169 train_time:13450ms step_avg:166.05ms\nstep:92/10000 train_loss:9.6740 train_time:13616ms step_avg:166.05ms\nstep:93/10000 train_loss:9.6391 train_time:13783ms step_avg:166.06ms\nstep:94/10000 train_loss:9.6638 train_time:13949ms step_avg:166.06ms\nstep:95/10000 train_loss:9.6241 train_time:14115ms step_avg:166.05ms\nstep:96/10000 train_loss:9.5974 train_time:14281ms step_avg:166.05ms\nstep:97/10000 train_loss:9.5609 train_time:14446ms step_avg:166.04ms\nstep:98/10000 train_loss:9.5467 train_time:14613ms step_avg:166.05ms\nstep:99/10000 train_loss:9.5048 train_time:14783ms step_avg:166.11ms\nstep:100/10000 train_loss:9.4662 train_time:14951ms step_avg:166.12ms\nstep:101/10000 train_loss:9.4789 train_time:15119ms step_avg:166.14ms\nstep:102/10000 train_loss:9.4379 train_time:15285ms step_avg:166.14ms\nstep:103/10000 train_loss:9.4368 train_time:15452ms step_avg:166.15ms\nstep:104/10000 train_loss:9.4268 train_time:15621ms step_avg:166.18ms\nstep:105/10000 train_loss:9.3905 train_time:15788ms step_avg:166.19ms\nstep:106/10000 train_loss:9.4114 train_time:15956ms step_avg:166.21ms\nstep:107/10000 train_loss:9.3594 train_time:16123ms step_avg:166.22ms\nstep:108/10000 train_loss:9.3250 train_time:16292ms step_avg:166.25ms\nstep:109/10000 train_loss:9.3342 train_time:16462ms step_avg:166.28ms\nstep:110/10000 train_loss:9.2295 train_time:16629ms step_avg:166.29ms\nstep:111/10000 train_loss:9.2496 train_time:16795ms step_avg:166.29ms\nstep:112/10000 train_loss:9.2576 train_time:16961ms step_avg:166.28ms\nstep:113/10000 train_loss:9.2485 train_time:17126ms step_avg:166.28ms\nstep:114/10000 train_loss:9.2123 train_time:17292ms step_avg:166.27ms\nstep:115/10000 train_loss:9.1947 train_time:17460ms step_avg:166.28ms\nstep:116/10000 train_loss:9.1418 train_time:17629ms step_avg:166.31ms\nstep:117/10000 train_loss:9.1199 train_time:17797ms step_avg:166.33ms\nstep:118/10000 train_loss:9.1061 train_time:17964ms step_avg:166.33ms\nstep:119/10000 train_loss:9.1124 train_time:18128ms step_avg:166.31ms\nstep:120/10000 train_loss:9.0947 train_time:18294ms step_avg:166.31ms\nstep:121/10000 train_loss:9.0352 train_time:18462ms step_avg:166.33ms\nstep:122/10000 train_loss:8.9998 train_time:18630ms step_avg:166.34ms\nstep:123/10000 train_loss:8.9820 train_time:18797ms step_avg:166.34ms\nstep:124/10000 train_loss:9.1255 train_time:18964ms step_avg:166.35ms\nstep:125/10000 train_loss:8.9571 train_time:19128ms step_avg:166.33ms\nstep:126/10000 train_loss:8.8639 train_time:19297ms step_avg:166.36ms\nstep:127/10000 train_loss:8.9042 train_time:19463ms step_avg:166.35ms\nstep:128/10000 train_loss:8.8907 train_time:19628ms step_avg:166.34ms\nstep:129/10000 train_loss:8.9119 train_time:19795ms step_avg:166.34ms\nstep:130/10000 train_loss:8.9511 train_time:19962ms step_avg:166.35ms\nstep:131/10000 train_loss:8.8363 train_time:20128ms step_avg:166.35ms\nstep:132/10000 train_loss:8.7903 train_time:20294ms step_avg:166.34ms\nstep:133/10000 train_loss:8.7930 train_time:20460ms step_avg:166.34ms\nstep:134/10000 train_loss:8.7665 train_time:20626ms step_avg:166.34ms\nstep:135/10000 train_loss:8.7693 train_time:20795ms step_avg:166.36ms\nstep:136/10000 train_loss:8.7126 train_time:20961ms step_avg:166.36ms\nstep:137/10000 train_loss:8.6994 train_time:21127ms step_avg:166.36ms\nstep:138/10000 train_loss:8.7590 train_time:21295ms step_avg:166.36ms\nstep:139/10000 train_loss:8.6623 train_time:21459ms step_avg:166.35ms\nstep:140/10000 train_loss:8.6985 train_time:21628ms step_avg:166.37ms\nstep:141/10000 train_loss:8.6165 train_time:21795ms step_avg:166.38ms\nstep:142/10000 train_loss:8.6876 train_time:21961ms step_avg:166.37ms\nstep:143/10000 train_loss:8.5715 train_time:22125ms step_avg:166.36ms\nstep:144/10000 train_loss:8.6398 train_time:22292ms step_avg:166.36ms\nstep:145/10000 train_loss:8.5854 train_time:22461ms step_avg:166.38ms\nstep:146/10000 train_loss:8.5394 train_time:22629ms step_avg:166.39ms\nstep:147/10000 train_loss:8.4559 train_time:22798ms step_avg:166.41ms\nstep:148/10000 train_loss:8.5206 train_time:22965ms step_avg:166.41ms\nstep:149/10000 train_loss:8.3594 train_time:23134ms step_avg:166.43ms\nstep:150/10000 train_loss:8.4247 train_time:23301ms step_avg:166.44ms\nstep:151/10000 train_loss:8.4716 train_time:23471ms step_avg:166.46ms\nstep:152/10000 train_loss:8.4228 train_time:23639ms step_avg:166.47ms\nstep:153/10000 train_loss:8.5074 train_time:23809ms step_avg:166.50ms\nstep:154/10000 train_loss:8.4176 train_time:23972ms step_avg:166.47ms\nstep:155/10000 train_loss:8.3024 train_time:24141ms step_avg:166.49ms\nstep:156/10000 train_loss:8.3885 train_time:24306ms step_avg:166.48ms\nstep:157/10000 train_loss:8.3717 train_time:24474ms step_avg:166.49ms\nstep:158/10000 train_loss:8.2864 train_time:24645ms step_avg:166.52ms\nstep:159/10000 train_loss:8.3146 train_time:24813ms step_avg:166.53ms\nstep:160/10000 train_loss:8.2299 train_time:24981ms step_avg:166.54ms\nstep:161/10000 train_loss:8.3074 train_time:25149ms step_avg:166.55ms\nstep:162/10000 train_loss:8.2358 train_time:25317ms step_avg:166.56ms\nstep:163/10000 train_loss:8.1615 train_time:25483ms step_avg:166.55ms\nstep:164/10000 train_loss:8.2305 train_time:25652ms step_avg:166.57ms\nstep:165/10000 train_loss:8.1453 train_time:25820ms step_avg:166.58ms\nstep:166/10000 train_loss:8.1063 train_time:25985ms step_avg:166.57ms\nstep:167/10000 train_loss:8.0659 train_time:26155ms step_avg:166.59ms\nstep:168/10000 train_loss:8.1540 train_time:26326ms step_avg:166.62ms\nstep:169/10000 train_loss:8.2223 train_time:26496ms step_avg:166.64ms\nstep:170/10000 train_loss:8.0874 train_time:26664ms step_avg:166.65ms\nstep:171/10000 train_loss:8.2121 train_time:26833ms step_avg:166.66ms\nstep:172/10000 train_loss:8.0946 train_time:27000ms step_avg:166.66ms\nstep:173/10000 train_loss:8.0925 train_time:27168ms step_avg:166.67ms\nstep:174/10000 train_loss:8.0526 train_time:27337ms step_avg:166.69ms\nstep:175/10000 train_loss:8.0023 train_time:27503ms step_avg:166.69ms\nstep:176/10000 train_loss:8.0202 train_time:27671ms step_avg:166.69ms\nstep:177/10000 train_loss:7.9986 train_time:27840ms step_avg:166.70ms\nstep:178/10000 train_loss:8.1132 train_time:28006ms step_avg:166.70ms\nstep:179/10000 train_loss:8.0905 train_time:28171ms step_avg:166.69ms\nstep:180/10000 train_loss:7.9346 train_time:28338ms step_avg:166.69ms\nstep:181/10000 train_loss:7.9026 train_time:28505ms step_avg:166.70ms\nstep:182/10000 train_loss:7.9845 train_time:28674ms step_avg:166.71ms\nstep:183/10000 train_loss:7.9359 train_time:28845ms step_avg:166.73ms\nstep:184/10000 train_loss:7.9084 train_time:29012ms step_avg:166.74ms\nstep:185/10000 train_loss:7.9590 train_time:29182ms step_avg:166.75ms\nstep:186/10000 train_loss:7.8302 train_time:29352ms step_avg:166.77ms\nstep:187/10000 train_loss:8.0111 train_time:29520ms step_avg:166.78ms\nstep:188/10000 train_loss:7.8398 train_time:29686ms step_avg:166.78ms\nstep:189/10000 train_loss:7.9427 train_time:29856ms step_avg:166.79ms\nstep:190/10000 train_loss:7.9542 train_time:30023ms step_avg:166.80ms\nstep:191/10000 train_loss:7.8261 train_time:30190ms step_avg:166.80ms\nstep:192/10000 train_loss:7.9067 train_time:30359ms step_avg:166.81ms\nstep:193/10000 train_loss:7.7897 train_time:30528ms step_avg:166.82ms\nstep:194/10000 train_loss:7.8232 train_time:30703ms step_avg:166.87ms\nstep:195/10000 train_loss:7.8049 train_time:30864ms step_avg:166.83ms\nstep:196/10000 train_loss:7.6681 train_time:31033ms step_avg:166.84ms\nstep:197/10000 train_loss:7.7980 train_time:31203ms step_avg:166.86ms\nstep:198/10000 train_loss:7.8840 train_time:31372ms step_avg:166.87ms\nstep:199/10000 train_loss:7.7462 train_time:31539ms step_avg:166.87ms\nstep:200/10000 train_loss:7.7686 train_time:31706ms step_avg:166.87ms\nstep:201/10000 train_loss:7.7451 train_time:31874ms step_avg:166.88ms\nstep:202/10000 train_loss:7.7666 train_time:32044ms step_avg:166.90ms\nstep:203/10000 train_loss:7.4963 train_time:32212ms step_avg:166.90ms\nstep:204/10000 train_loss:7.8131 train_time:32382ms step_avg:166.92ms\nstep:205/10000 train_loss:7.7289 train_time:32550ms step_avg:166.93ms\nstep:206/10000 train_loss:7.6306 train_time:32718ms step_avg:166.93ms\nstep:207/10000 train_loss:7.6296 train_time:32886ms step_avg:166.94ms\nstep:208/10000 train_loss:7.7572 train_time:33053ms step_avg:166.93ms\nstep:209/10000 train_loss:7.7191 train_time:33221ms step_avg:166.94ms\nstep:210/10000 train_loss:7.7078 train_time:33389ms step_avg:166.95ms\nstep:211/10000 train_loss:7.7047 train_time:33559ms step_avg:166.96ms\nstep:212/10000 train_loss:7.6963 train_time:33726ms step_avg:166.96ms\nstep:213/10000 train_loss:7.8143 train_time:33894ms step_avg:166.97ms\nstep:214/10000 train_loss:7.6375 train_time:34063ms step_avg:166.97ms\nstep:215/10000 train_loss:7.6354 train_time:34229ms step_avg:166.97ms\nstep:216/10000 train_loss:7.6793 train_time:34397ms step_avg:166.98ms\nstep:217/10000 train_loss:7.5229 train_time:34566ms step_avg:166.99ms\nstep:218/10000 train_loss:7.8361 train_time:34734ms step_avg:166.99ms\nstep:219/10000 train_loss:7.7099 train_time:34902ms step_avg:166.99ms\nstep:220/10000 train_loss:7.6065 train_time:35070ms step_avg:167.00ms\nstep:221/10000 train_loss:7.6132 train_time:35239ms step_avg:167.01ms\nstep:222/10000 train_loss:7.6160 train_time:35413ms step_avg:167.04ms\nstep:223/10000 train_loss:7.5672 train_time:35585ms step_avg:167.06ms\nstep:224/10000 train_loss:7.5580 train_time:35753ms step_avg:167.07ms\nstep:225/10000 train_loss:7.6061 train_time:35920ms step_avg:167.07ms\nstep:226/10000 train_loss:7.5200 train_time:36085ms step_avg:167.06ms\nstep:227/10000 train_loss:7.6737 train_time:36254ms step_avg:167.07ms\nstep:228/10000 train_loss:7.5745 train_time:36424ms step_avg:167.08ms\nstep:229/10000 train_loss:7.6067 train_time:36593ms step_avg:167.09ms\nstep:230/10000 train_loss:7.6394 train_time:36762ms step_avg:167.10ms\nstep:231/10000 train_loss:7.7118 train_time:36931ms step_avg:167.11ms\nstep:232/10000 train_loss:7.5725 train_time:37101ms step_avg:167.12ms\nstep:233/10000 train_loss:7.5057 train_time:37270ms step_avg:167.13ms\nstep:234/10000 train_loss:7.5424 train_time:37440ms step_avg:167.14ms\nstep:235/10000 train_loss:7.5780 train_time:37610ms step_avg:167.16ms\nstep:236/10000 train_loss:7.6337 train_time:37778ms step_avg:167.16ms\nstep:237/10000 train_loss:7.6051 train_time:37944ms step_avg:167.16ms\nstep:238/10000 train_loss:7.5887 train_time:38114ms step_avg:167.17ms\nstep:239/10000 train_loss:7.6406 train_time:38282ms step_avg:167.17ms\nstep:240/10000 train_loss:7.5015 train_time:38448ms step_avg:167.17ms\nstep:241/10000 train_loss:7.6204 train_time:38615ms step_avg:167.17ms\nstep:242/10000 train_loss:7.4702 train_time:38786ms step_avg:167.18ms\nstep:243/10000 train_loss:7.4723 train_time:38956ms step_avg:167.19ms\nstep:244/10000 train_loss:7.5216 train_time:39125ms step_avg:167.20ms\nstep:245/10000 train_loss:7.5417 train_time:39293ms step_avg:167.20ms\nstep:246/10000 train_loss:7.5334 train_time:39461ms step_avg:167.21ms\nstep:247/10000 train_loss:7.4484 train_time:39629ms step_avg:167.21ms\nstep:248/10000 train_loss:7.5540 train_time:39798ms step_avg:167.22ms\nstep:249/10000 train_loss:7.6389 train_time:39967ms step_avg:167.23ms\nstep:250/10000 train_loss:7.4639 train_time:40139ms step_avg:167.25ms\nstep:251/10000 train_loss:7.4686 train_time:40312ms step_avg:167.27ms\nstep:252/10000 train_loss:7.4763 train_time:40476ms step_avg:167.26ms\nstep:253/10000 train_loss:7.5593 train_time:40642ms step_avg:167.25ms\nstep:254/10000 train_loss:8.2568 train_time:40813ms step_avg:167.27ms\nstep:255/10000 train_loss:7.4853 train_time:40983ms step_avg:167.28ms\nstep:256/10000 train_loss:7.4326 train_time:41152ms step_avg:167.28ms\nstep:257/10000 train_loss:7.5519 train_time:41321ms step_avg:167.29ms\nstep:258/10000 train_loss:7.3595 train_time:41492ms step_avg:167.30ms\nstep:259/10000 train_loss:7.5035 train_time:41661ms step_avg:167.31ms\nstep:260/10000 train_loss:7.2991 train_time:41829ms step_avg:167.32ms\nstep:261/10000 train_loss:7.4638 train_time:41998ms step_avg:167.32ms\nstep:262/10000 train_loss:7.4112 train_time:42168ms step_avg:167.33ms\nstep:263/10000 train_loss:7.3786 train_time:42337ms step_avg:167.34ms\nstep:264/10000 train_loss:7.2964 train_time:42506ms step_avg:167.35ms\nstep:265/10000 train_loss:7.4579 train_time:42673ms step_avg:167.34ms\nstep:266/10000 train_loss:7.3409 train_time:42844ms step_avg:167.36ms\nstep:267/10000 train_loss:7.3692 train_time:43013ms step_avg:167.37ms\nstep:268/10000 train_loss:7.4883 train_time:43183ms step_avg:167.38ms\nstep:269/10000 train_loss:7.4737 train_time:43354ms step_avg:167.39ms\nstep:270/10000 train_loss:7.5778 train_time:43524ms step_avg:167.40ms\nstep:271/10000 train_loss:7.2181 train_time:43693ms step_avg:167.41ms\nstep:272/10000 train_loss:7.4487 train_time:43863ms step_avg:167.42ms\nstep:273/10000 train_loss:7.5158 train_time:44031ms step_avg:167.42ms\nstep:274/10000 train_loss:7.2445 train_time:44204ms step_avg:167.44ms\nstep:275/10000 train_loss:7.2852 train_time:44375ms step_avg:167.45ms\nstep:276/10000 train_loss:7.4682 train_time:44545ms step_avg:167.46ms\nstep:277/10000 train_loss:7.3742 train_time:44714ms step_avg:167.47ms\nstep:278/10000 train_loss:7.1799 train_time:44885ms step_avg:167.48ms\nstep:279/10000 train_loss:7.3561 train_time:45053ms step_avg:167.48ms\nstep:280/10000 train_loss:7.2933 train_time:45225ms step_avg:167.50ms\nstep:281/10000 train_loss:7.4282 train_time:45393ms step_avg:167.50ms\nstep:282/10000 train_loss:7.4696 train_time:45563ms step_avg:167.51ms\nstep:283/10000 train_loss:7.2973 train_time:45733ms step_avg:167.52ms\nstep:284/10000 train_loss:7.4449 train_time:45905ms step_avg:167.54ms\nstep:285/10000 train_loss:7.5393 train_time:46074ms step_avg:167.54ms\nstep:286/10000 train_loss:7.3573 train_time:46245ms step_avg:167.56ms\nstep:287/10000 train_loss:7.4436 train_time:46415ms step_avg:167.56ms\nstep:288/10000 train_loss:7.3172 train_time:46586ms step_avg:167.58ms\nstep:289/10000 train_loss:7.2529 train_time:46756ms step_avg:167.58ms\nstep:290/10000 train_loss:7.2787 train_time:46925ms step_avg:167.59ms\nstep:291/10000 train_loss:7.3820 train_time:47096ms step_avg:167.60ms\nstep:292/10000 train_loss:7.3582 train_time:47265ms step_avg:167.61ms\nstep:293/10000 train_loss:7.3113 train_time:47432ms step_avg:167.60ms\nstep:294/10000 train_loss:7.3102 train_time:47601ms step_avg:167.61ms\nstep:295/10000 train_loss:7.1259 train_time:47771ms step_avg:167.62ms\nstep:296/10000 train_loss:7.1119 train_time:47941ms step_avg:167.63ms\nstep:297/10000 train_loss:7.3042 train_time:48109ms step_avg:167.63ms\nstep:298/10000 train_loss:7.3442 train_time:48282ms step_avg:167.64ms\nstep:299/10000 train_loss:7.1793 train_time:48449ms step_avg:167.64ms\nstep:300/10000 train_loss:7.1981 train_time:48619ms step_avg:167.65ms\nstep:301/10000 train_loss:7.2623 train_time:48787ms step_avg:167.65ms\nstep:302/10000 train_loss:7.2474 train_time:48959ms step_avg:167.67ms\nstep:303/10000 train_loss:7.0725 train_time:49128ms step_avg:167.67ms\nstep:304/10000 train_loss:7.1548 train_time:49298ms step_avg:167.68ms\nstep:305/10000 train_loss:7.2084 train_time:49468ms step_avg:167.69ms\nstep:306/10000 train_loss:7.2803 train_time:49639ms step_avg:167.70ms\nstep:307/10000 train_loss:7.2788 train_time:49808ms step_avg:167.70ms\nstep:308/10000 train_loss:7.3000 train_time:49980ms step_avg:167.72ms\nstep:309/10000 train_loss:7.1843 train_time:50148ms step_avg:167.72ms\nstep:310/10000 train_loss:7.2388 train_time:50318ms step_avg:167.73ms\nstep:311/10000 train_loss:7.2047 train_time:50488ms step_avg:167.73ms\nstep:312/10000 train_loss:7.1404 train_time:50658ms step_avg:167.74ms\nstep:313/10000 train_loss:7.2225 train_time:50828ms step_avg:167.75ms\nstep:314/10000 train_loss:7.2768 train_time:50998ms step_avg:167.76ms\nstep:315/10000 train_loss:7.0549 train_time:51165ms step_avg:167.75ms\nstep:316/10000 train_loss:6.8123 train_time:51337ms step_avg:167.77ms\nstep:317/10000 train_loss:7.1432 train_time:51506ms step_avg:167.77ms\nstep:318/10000 train_loss:7.2466 train_time:51676ms step_avg:167.78ms\nstep:319/10000 train_loss:7.0085 train_time:51846ms step_avg:167.79ms\nstep:320/10000 train_loss:7.1264 train_time:52016ms step_avg:167.80ms\nstep:321/10000 train_loss:7.1333 train_time:52186ms step_avg:167.80ms\nstep:322/10000 train_loss:7.0246 train_time:52357ms step_avg:167.81ms\nstep:323/10000 train_loss:7.1226 train_time:52528ms step_avg:167.82ms\nstep:324/10000 train_loss:7.2396 train_time:52699ms step_avg:167.83ms\nstep:325/10000 train_loss:7.1082 train_time:52869ms step_avg:167.84ms\nstep:326/10000 train_loss:7.1739 train_time:53040ms step_avg:167.85ms\nstep:327/10000 train_loss:7.1258 train_time:53208ms step_avg:167.85ms\nstep:328/10000 train_loss:7.0837 train_time:53378ms step_avg:167.85ms\nstep:329/10000 train_loss:7.3934 train_time:53546ms step_avg:167.86ms\nstep:330/10000 train_loss:7.1682 train_time:53717ms step_avg:167.86ms\nstep:331/10000 train_loss:6.9654 train_time:53884ms step_avg:167.86ms\nstep:332/10000 train_loss:7.1457 train_time:54055ms step_avg:167.87ms\nstep:333/10000 train_loss:7.1964 train_time:54227ms step_avg:167.89ms\nstep:334/10000 train_loss:7.0287 train_time:54399ms step_avg:167.90ms\nstep:335/10000 train_loss:7.0399 train_time:54568ms step_avg:167.90ms\nstep:336/10000 train_loss:7.0303 train_time:54739ms step_avg:167.91ms\nstep:337/10000 train_loss:7.1525 train_time:54908ms step_avg:167.91ms\nstep:338/10000 train_loss:6.9022 train_time:55076ms step_avg:167.92ms\nstep:339/10000 train_loss:6.9481 train_time:55245ms step_avg:167.92ms\nstep:340/10000 train_loss:7.1018 train_time:55416ms step_avg:167.93ms\nstep:341/10000 train_loss:7.0443 train_time:55586ms step_avg:167.93ms\nstep:342/10000 train_loss:7.0824 train_time:55757ms step_avg:167.94ms\nstep:343/10000 train_loss:7.0366 train_time:55927ms step_avg:167.95ms\nstep:344/10000 train_loss:7.0169 train_time:56098ms step_avg:167.96ms\nstep:345/10000 train_loss:6.9989 train_time:56267ms step_avg:167.96ms\nstep:346/10000 train_loss:7.0127 train_time:56440ms step_avg:167.98ms\nstep:347/10000 train_loss:7.0184 train_time:56610ms step_avg:167.98ms\nstep:348/10000 train_loss:7.0587 train_time:56781ms step_avg:167.99ms\nstep:349/10000 train_loss:6.9592 train_time:56951ms step_avg:168.00ms\nstep:350/10000 train_loss:7.1499 train_time:57121ms step_avg:168.00ms\nstep:351/10000 train_loss:6.9058 train_time:57290ms step_avg:168.01ms\nstep:352/10000 train_loss:6.9129 train_time:57462ms step_avg:168.02ms\nstep:353/10000 train_loss:6.8182 train_time:57632ms step_avg:168.02ms\nstep:354/10000 train_loss:7.0655 train_time:57803ms step_avg:168.03ms\nstep:355/10000 train_loss:6.9352 train_time:57971ms step_avg:168.03ms\nstep:356/10000 train_loss:6.8937 train_time:58142ms step_avg:168.04ms\nstep:357/10000 train_loss:6.6851 train_time:58310ms step_avg:168.04ms\nstep:358/10000 train_loss:6.9790 train_time:58480ms step_avg:168.05ms\nstep:359/10000 train_loss:7.0287 train_time:58648ms step_avg:168.05ms\nstep:360/10000 train_loss:6.7616 train_time:58821ms step_avg:168.06ms\nstep:361/10000 train_loss:6.8191 train_time:58988ms step_avg:168.06ms\nstep:362/10000 train_loss:6.9359 train_time:59160ms step_avg:168.07ms\nstep:363/10000 train_loss:6.8775 train_time:59330ms step_avg:168.07ms\nstep:364/10000 train_loss:6.9500 train_time:59501ms step_avg:168.08ms\nstep:365/10000 train_loss:6.9134 train_time:59669ms step_avg:168.08ms\nstep:366/10000 train_loss:7.0560 train_time:59840ms step_avg:168.09ms\nstep:367/10000 train_loss:7.1302 train_time:60011ms step_avg:168.10ms\nstep:368/10000 train_loss:6.8704 train_time:60179ms step_avg:168.10ms\nstep:369/10000 train_loss:7.0689 train_time:60348ms step_avg:168.10ms\nstep:370/10000 train_loss:6.8913 train_time:60519ms step_avg:168.11ms\nstep:371/10000 train_loss:6.9620 train_time:60688ms step_avg:168.11ms\nstep:372/10000 train_loss:7.1501 train_time:60859ms step_avg:168.12ms\nstep:373/10000 train_loss:6.8472 train_time:61029ms step_avg:168.12ms\nstep:374/10000 train_loss:6.8587 train_time:61200ms step_avg:168.13ms\nstep:375/10000 train_loss:6.8438 train_time:61372ms step_avg:168.14ms\nstep:376/10000 train_loss:6.9669 train_time:61540ms step_avg:168.14ms\nstep:377/10000 train_loss:6.7645 train_time:61710ms step_avg:168.15ms\nstep:378/10000 train_loss:6.8635 train_time:61881ms step_avg:168.16ms\nstep:379/10000 train_loss:6.8679 train_time:62049ms step_avg:168.15ms\nstep:380/10000 train_loss:6.7290 train_time:62218ms step_avg:168.16ms\nstep:381/10000 train_loss:6.9695 train_time:62387ms step_avg:168.16ms\nstep:382/10000 train_loss:6.7211 train_time:62560ms step_avg:168.17ms\nstep:383/10000 train_loss:6.9533 train_time:62728ms step_avg:168.17ms\nstep:384/10000 train_loss:6.8411 train_time:62898ms step_avg:168.18ms\nstep:385/10000 train_loss:6.7023 train_time:63068ms step_avg:168.18ms\nstep:386/10000 train_loss:6.8399 train_time:63239ms step_avg:168.19ms\nstep:387/10000 train_loss:6.7791 train_time:63408ms step_avg:168.19ms\nstep:388/10000 train_loss:6.8876 train_time:63578ms step_avg:168.20ms\nstep:389/10000 train_loss:6.7788 train_time:63751ms step_avg:168.21ms\nstep:390/10000 train_loss:6.6868 train_time:63922ms step_avg:168.22ms\nstep:391/10000 train_loss:6.8651 train_time:64090ms step_avg:168.22ms\nstep:392/10000 train_loss:6.6932 train_time:64262ms step_avg:168.23ms\nstep:393/10000 train_loss:6.7410 train_time:64432ms step_avg:168.23ms\nstep:394/10000 train_loss:6.8726 train_time:64602ms step_avg:168.23ms\nstep:395/10000 train_loss:6.6526 train_time:64773ms step_avg:168.24ms\nstep:396/10000 train_loss:6.6615 train_time:64943ms step_avg:168.25ms\nstep:397/10000 train_loss:6.8125 train_time:65113ms step_avg:168.25ms\nstep:398/10000 train_loss:6.8048 train_time:65282ms step_avg:168.25ms\nstep:399/10000 train_loss:6.6620 train_time:65452ms step_avg:168.26ms\nstep:400/10000 train_loss:6.6425 train_time:65625ms step_avg:168.27ms\nstep:401/10000 train_loss:6.5177 train_time:65795ms step_avg:168.27ms\nstep:402/10000 train_loss:6.6970 train_time:65966ms step_avg:168.28ms\nstep:403/10000 train_loss:6.9328 train_time:66142ms step_avg:168.30ms\nstep:404/10000 train_loss:6.5564 train_time:66306ms step_avg:168.29ms\nstep:405/10000 train_loss:6.7998 train_time:66477ms step_avg:168.30ms\nstep:406/10000 train_loss:6.7690 train_time:66648ms step_avg:168.30ms\nstep:407/10000 train_loss:6.7258 train_time:66819ms step_avg:168.31ms\nstep:408/10000 train_loss:6.6874 train_time:66986ms step_avg:168.31ms\nstep:409/10000 train_loss:6.8648 train_time:67155ms step_avg:168.31ms\nstep:410/10000 train_loss:6.8110 train_time:67324ms step_avg:168.31ms\nstep:411/10000 train_loss:6.7880 train_time:67495ms step_avg:168.32ms\nstep:412/10000 train_loss:6.5079 train_time:67664ms step_avg:168.32ms\nstep:413/10000 train_loss:6.7214 train_time:67837ms step_avg:168.33ms\nstep:414/10000 train_loss:6.6028 train_time:68006ms step_avg:168.33ms\nstep:415/10000 train_loss:7.2109 train_time:68176ms step_avg:168.34ms\nstep:416/10000 train_loss:6.6984 train_time:68345ms step_avg:168.34ms\nstep:417/10000 train_loss:6.4837 train_time:68515ms step_avg:168.34ms\nstep:418/10000 train_loss:6.7472 train_time:68685ms step_avg:168.35ms\nstep:419/10000 train_loss:6.7550 train_time:68860ms step_avg:168.36ms\nstep:420/10000 train_loss:6.7296 train_time:69027ms step_avg:168.36ms\nstep:421/10000 train_loss:6.5965 train_time:69197ms step_avg:168.36ms\nstep:422/10000 train_loss:6.9654 train_time:69366ms step_avg:168.36ms\nstep:423/10000 train_loss:6.8252 train_time:69538ms step_avg:168.37ms\nstep:424/10000 train_loss:6.8711 train_time:69705ms step_avg:168.37ms\nstep:425/10000 train_loss:6.6871 train_time:69878ms step_avg:168.38ms\nstep:426/10000 train_loss:6.6987 train_time:70048ms step_avg:168.39ms\nstep:427/10000 train_loss:6.8529 train_time:70220ms step_avg:168.39ms\nstep:428/10000 train_loss:6.6170 train_time:70389ms step_avg:168.39ms\nstep:429/10000 train_loss:6.6915 train_time:70560ms step_avg:168.40ms\nstep:430/10000 train_loss:6.3602 train_time:70730ms step_avg:168.41ms\nstep:431/10000 train_loss:6.5525 train_time:70900ms step_avg:168.41ms\nstep:432/10000 train_loss:6.5320 train_time:71070ms step_avg:168.41ms\nstep:433/10000 train_loss:6.6896 train_time:71241ms step_avg:168.42ms\nstep:434/10000 train_loss:6.6525 train_time:71410ms step_avg:168.42ms\nstep:435/10000 train_loss:6.5476 train_time:71581ms step_avg:168.43ms\nstep:436/10000 train_loss:6.7381 train_time:71752ms step_avg:168.43ms\nstep:437/10000 train_loss:6.5958 train_time:71922ms step_avg:168.44ms\nstep:438/10000 train_loss:6.6320 train_time:72091ms step_avg:168.44ms\nstep:439/10000 train_loss:6.6065 train_time:72264ms step_avg:168.45ms\nstep:440/10000 train_loss:6.7156 train_time:72433ms step_avg:168.45ms\nstep:441/10000 train_loss:6.7151 train_time:72601ms step_avg:168.45ms\nstep:442/10000 train_loss:6.6319 train_time:72771ms step_avg:168.45ms\nstep:443/10000 train_loss:6.5641 train_time:72942ms step_avg:168.46ms\nstep:444/10000 train_loss:6.5882 train_time:73114ms step_avg:168.47ms\nstep:445/10000 train_loss:6.6348 train_time:73286ms step_avg:168.47ms\nstep:446/10000 train_loss:6.6311 train_time:73455ms step_avg:168.48ms\nstep:447/10000 train_loss:6.7024 train_time:73622ms step_avg:168.47ms\nstep:448/10000 train_loss:6.5984 train_time:73790ms step_avg:168.47ms\nstep:449/10000 train_loss:6.6501 train_time:73965ms step_avg:168.48ms\nstep:450/10000 train_loss:6.6197 train_time:74137ms step_avg:168.49ms\nstep:451/10000 train_loss:6.6539 train_time:74307ms step_avg:168.50ms\nstep:452/10000 train_loss:6.3810 train_time:74478ms step_avg:168.50ms\nstep:453/10000 train_loss:6.7334 train_time:74648ms step_avg:168.51ms\nstep:454/10000 train_loss:6.4230 train_time:74818ms step_avg:168.51ms\nstep:455/10000 train_loss:6.5764 train_time:74986ms step_avg:168.51ms\nstep:456/10000 train_loss:6.6508 train_time:75160ms step_avg:168.52ms\nstep:457/10000 train_loss:6.5255 train_time:75327ms step_avg:168.52ms\nstep:458/10000 train_loss:6.6755 train_time:75498ms step_avg:168.52ms\nstep:459/10000 train_loss:6.6873 train_time:75667ms step_avg:168.52ms\nstep:460/10000 train_loss:6.5881 train_time:75838ms step_avg:168.53ms\nstep:461/10000 train_loss:6.5405 train_time:76009ms step_avg:168.53ms\nstep:462/10000 train_loss:6.4653 train_time:76180ms step_avg:168.54ms\nstep:463/10000 train_loss:6.6149 train_time:76350ms step_avg:168.54ms\nstep:464/10000 train_loss:6.7045 train_time:76520ms step_avg:168.55ms\nstep:465/10000 train_loss:6.6119 train_time:76689ms step_avg:168.55ms\nstep:466/10000 train_loss:6.4772 train_time:76860ms step_avg:168.55ms\nstep:467/10000 train_loss:6.6446 train_time:77028ms step_avg:168.55ms\nstep:468/10000 train_loss:6.4902 train_time:77200ms step_avg:168.56ms\nstep:469/10000 train_loss:6.5290 train_time:77370ms step_avg:168.56ms\nstep:470/10000 train_loss:6.4978 train_time:77542ms step_avg:168.57ms\nstep:471/10000 train_loss:6.6125 train_time:77711ms step_avg:168.57ms\nstep:472/10000 train_loss:6.5048 train_time:77884ms step_avg:168.58ms\nstep:473/10000 train_loss:6.3988 train_time:78055ms step_avg:168.59ms\nstep:474/10000 train_loss:6.6073 train_time:78227ms step_avg:168.59ms\nstep:475/10000 train_loss:6.5602 train_time:78397ms step_avg:168.60ms\nstep:476/10000 train_loss:6.5770 train_time:78568ms step_avg:168.60ms\nstep:477/10000 train_loss:6.6216 train_time:78739ms step_avg:168.61ms\nstep:478/10000 train_loss:6.5364 train_time:78907ms step_avg:168.60ms\nstep:479/10000 train_loss:6.4828 train_time:79085ms step_avg:168.62ms\nstep:480/10000 train_loss:6.6127 train_time:79247ms step_avg:168.61ms\nstep:481/10000 train_loss:6.4573 train_time:79418ms step_avg:168.62ms\nstep:482/10000 train_loss:6.4123 train_time:79587ms step_avg:168.62ms\nstep:483/10000 train_loss:6.4896 train_time:79755ms step_avg:168.62ms\nstep:484/10000 train_loss:6.5274 train_time:79923ms step_avg:168.61ms\nstep:485/10000 train_loss:6.5514 train_time:80094ms step_avg:168.62ms\nstep:486/10000 train_loss:6.4261 train_time:80266ms step_avg:168.63ms\nstep:487/10000 train_loss:6.4290 train_time:80437ms step_avg:168.63ms\nstep:488/10000 train_loss:6.4388 train_time:80608ms step_avg:168.64ms\nstep:489/10000 train_loss:6.3047 train_time:80774ms step_avg:168.63ms\nstep:490/10000 train_loss:6.3258 train_time:80942ms step_avg:168.63ms\nstep:491/10000 train_loss:6.4424 train_time:81113ms step_avg:168.63ms\nstep:492/10000 train_loss:6.5604 train_time:81285ms step_avg:168.64ms\nstep:493/10000 train_loss:6.5491 train_time:81457ms step_avg:168.65ms\nstep:494/10000 train_loss:6.6433 train_time:81626ms step_avg:168.65ms\nstep:495/10000 train_loss:6.3473 train_time:81795ms step_avg:168.65ms\nstep:496/10000 train_loss:6.4277 train_time:81965ms step_avg:168.65ms\nstep:497/10000 train_loss:6.4566 train_time:82134ms step_avg:168.65ms\nstep:498/10000 train_loss:6.4104 train_time:82306ms step_avg:168.66ms\nstep:499/10000 train_loss:6.4683 train_time:82480ms step_avg:168.67ms\nstep:500/10000 train_loss:6.2488 train_time:82647ms step_avg:168.67ms\nstep:501/10000 train_loss:6.3707 train_time:82818ms step_avg:168.67ms\nstep:502/10000 train_loss:6.4739 train_time:82993ms step_avg:168.69ms\nstep:503/10000 train_loss:6.3393 train_time:83166ms step_avg:168.69ms\nstep:504/10000 train_loss:6.3795 train_time:83337ms step_avg:168.70ms\nstep:505/10000 train_loss:6.4447 train_time:83511ms step_avg:168.71ms\nstep:506/10000 train_loss:6.5727 train_time:83680ms step_avg:168.71ms\nstep:507/10000 train_loss:6.6073 train_time:83851ms step_avg:168.72ms\nstep:508/10000 train_loss:6.4582 train_time:84022ms step_avg:168.72ms\nstep:509/10000 train_loss:6.5986 train_time:84193ms step_avg:168.72ms\nstep:510/10000 train_loss:6.3719 train_time:84362ms step_avg:168.72ms\nstep:511/10000 train_loss:6.3884 train_time:84536ms step_avg:168.73ms\nstep:512/10000 train_loss:6.4900 train_time:84704ms step_avg:168.73ms\nstep:513/10000 train_loss:6.4351 train_time:84872ms step_avg:168.73ms\nstep:514/10000 train_loss:6.5960 train_time:85039ms step_avg:168.73ms\nstep:515/10000 train_loss:6.3844 train_time:85208ms step_avg:168.73ms\nstep:516/10000 train_loss:6.4561 train_time:85380ms step_avg:168.73ms\nstep:517/10000 train_loss:6.3068 train_time:85548ms step_avg:168.73ms\nstep:518/10000 train_loss:6.3999 train_time:85718ms step_avg:168.74ms\nstep:519/10000 train_loss:6.2037 train_time:85887ms step_avg:168.74ms\nstep:520/10000 train_loss:6.5331 train_time:86058ms step_avg:168.74ms\nstep:521/10000 train_loss:6.2795 train_time:86227ms step_avg:168.74ms\nstep:522/10000 train_loss:6.5101 train_time:86399ms step_avg:168.75ms\nstep:523/10000 train_loss:6.3356 train_time:86568ms step_avg:168.75ms\nstep:524/10000 train_loss:6.5665 train_time:86744ms step_avg:168.76ms\nstep:525/10000 train_loss:6.7174 train_time:86908ms step_avg:168.75ms\nstep:526/10000 train_loss:6.3513 train_time:87078ms step_avg:168.76ms\nstep:527/10000 train_loss:6.3516 train_time:87247ms step_avg:168.76ms\nstep:528/10000 train_loss:6.3043 train_time:87420ms step_avg:168.76ms\nstep:529/10000 train_loss:6.3346 train_time:87590ms step_avg:168.77ms\nstep:530/10000 train_loss:6.3758 train_time:87761ms step_avg:168.77ms\nstep:531/10000 train_loss:6.3629 train_time:87931ms step_avg:168.77ms\nstep:532/10000 train_loss:6.2835 train_time:88102ms step_avg:168.78ms\nstep:533/10000 train_loss:6.3251 train_time:88270ms step_avg:168.78ms\nstep:534/10000 train_loss:6.3709 train_time:88441ms step_avg:168.78ms\nstep:535/10000 train_loss:6.3214 train_time:88611ms step_avg:168.78ms\nstep:536/10000 train_loss:6.3444 train_time:88782ms step_avg:168.79ms\nstep:537/10000 train_loss:6.4914 train_time:88951ms step_avg:168.79ms\nstep:538/10000 train_loss:6.3377 train_time:89121ms step_avg:168.79ms\nstep:539/10000 train_loss:6.4394 train_time:89290ms step_avg:168.79ms\nstep:540/10000 train_loss:6.2752 train_time:89461ms step_avg:168.79ms\nstep:541/10000 train_loss:6.2014 train_time:89631ms step_avg:168.80ms\nstep:542/10000 train_loss:6.2024 train_time:89804ms step_avg:168.80ms\nstep:543/10000 train_loss:6.3954 train_time:89973ms step_avg:168.80ms\nstep:544/10000 train_loss:6.2192 train_time:90146ms step_avg:168.81ms\nstep:545/10000 train_loss:6.3048 train_time:90318ms step_avg:168.82ms\nstep:546/10000 train_loss:6.2900 train_time:90487ms step_avg:168.82ms\nstep:547/10000 train_loss:6.1857 train_time:90657ms step_avg:168.82ms\nstep:548/10000 train_loss:6.3341 train_time:90826ms step_avg:168.82ms\nstep:549/10000 train_loss:6.2435 train_time:90998ms step_avg:168.83ms\nstep:550/10000 train_loss:6.3128 train_time:91168ms step_avg:168.83ms\nstep:551/10000 train_loss:6.3179 train_time:91339ms step_avg:168.83ms\nstep:552/10000 train_loss:6.3003 train_time:91508ms step_avg:168.83ms\nstep:553/10000 train_loss:6.2241 train_time:91678ms step_avg:168.84ms\nstep:554/10000 train_loss:6.3439 train_time:91847ms step_avg:168.84ms\nstep:555/10000 train_loss:6.2054 train_time:92017ms step_avg:168.84ms\nstep:556/10000 train_loss:6.2599 train_time:92187ms step_avg:168.84ms\nstep:557/10000 train_loss:6.2888 train_time:92358ms step_avg:168.84ms\nstep:558/10000 train_loss:6.2408 train_time:92529ms step_avg:168.85ms\nstep:559/10000 train_loss:6.2462 train_time:92700ms step_avg:168.85ms\nstep:560/10000 train_loss:6.1565 train_time:92868ms step_avg:168.85ms\nstep:561/10000 train_loss:6.4076 train_time:93039ms step_avg:168.85ms\nstep:562/10000 train_loss:6.2027 train_time:93208ms step_avg:168.86ms\nstep:563/10000 train_loss:6.0967 train_time:93379ms step_avg:168.86ms\nstep:564/10000 train_loss:6.1854 train_time:93548ms step_avg:168.86ms\nstep:565/10000 train_loss:6.3853 train_time:93719ms step_avg:168.86ms\nstep:566/10000 train_loss:6.3394 train_time:93888ms step_avg:168.86ms\nstep:567/10000 train_loss:6.4779 train_time:94058ms step_avg:168.86ms\nstep:568/10000 train_loss:6.1435 train_time:94228ms step_avg:168.87ms\nstep:569/10000 train_loss:6.3676 train_time:94400ms step_avg:168.87ms\nstep:570/10000 train_loss:6.0644 train_time:94571ms step_avg:168.88ms\nstep:571/10000 train_loss:6.2416 train_time:94742ms step_avg:168.88ms\nstep:572/10000 train_loss:6.3869 train_time:94911ms step_avg:168.88ms\nstep:573/10000 train_loss:6.3176 train_time:95081ms step_avg:168.88ms\nstep:574/10000 train_loss:6.2552 train_time:95249ms step_avg:168.88ms\nstep:575/10000 train_loss:6.1423 train_time:95420ms step_avg:168.89ms\nstep:576/10000 train_loss:6.6669 train_time:95589ms step_avg:168.89ms\nstep:577/10000 train_loss:6.2072 train_time:95761ms step_avg:168.89ms\nstep:578/10000 train_loss:6.1888 train_time:95931ms step_avg:168.89ms\nstep:579/10000 train_loss:6.2518 train_time:96102ms step_avg:168.90ms\nstep:580/10000 train_loss:6.2017 train_time:96271ms step_avg:168.90ms\nstep:581/10000 train_loss:6.2617 train_time:96441ms step_avg:168.90ms\nstep:582/10000 train_loss:6.2254 train_time:96612ms step_avg:168.90ms\nstep:583/10000 train_loss:6.2537 train_time:96783ms step_avg:168.91ms\nstep:584/10000 train_loss:6.2903 train_time:96951ms step_avg:168.90ms\nstep:585/10000 train_loss:6.0920 train_time:97121ms step_avg:168.91ms\nstep:586/10000 train_loss:6.2150 train_time:97291ms step_avg:168.91ms\nstep:587/10000 train_loss:6.1503 train_time:97463ms step_avg:168.91ms\nstep:588/10000 train_loss:6.2285 train_time:97635ms step_avg:168.92ms\nstep:589/10000 train_loss:6.1331 train_time:97804ms step_avg:168.92ms\nstep:590/10000 train_loss:6.1725 train_time:97972ms step_avg:168.92ms\nstep:591/10000 train_loss:6.1568 train_time:98143ms step_avg:168.92ms\nstep:592/10000 train_loss:5.9639 train_time:98314ms step_avg:168.93ms\nstep:593/10000 train_loss:6.2207 train_time:98488ms step_avg:168.93ms\nstep:594/10000 train_loss:6.3059 train_time:98659ms step_avg:168.94ms\nstep:595/10000 train_loss:6.0638 train_time:98829ms step_avg:168.94ms\nstep:596/10000 train_loss:6.4167 train_time:99001ms step_avg:168.94ms\nstep:597/10000 train_loss:6.2410 train_time:99171ms step_avg:168.94ms\nstep:598/10000 train_loss:6.3111 train_time:99344ms step_avg:168.95ms\nstep:599/10000 train_loss:6.2860 train_time:99513ms step_avg:168.95ms\nstep:600/10000 train_loss:6.3419 train_time:99685ms step_avg:168.96ms\nstep:601/10000 train_loss:6.2753 train_time:99853ms step_avg:168.96ms\nstep:602/10000 train_loss:6.2075 train_time:100025ms step_avg:168.96ms\nstep:603/10000 train_loss:6.3178 train_time:100195ms step_avg:168.96ms\nstep:604/10000 train_loss:6.1003 train_time:100366ms step_avg:168.97ms\nstep:605/10000 train_loss:6.0923 train_time:100537ms step_avg:168.97ms\nstep:606/10000 train_loss:6.1698 train_time:100707ms step_avg:168.97ms\nstep:607/10000 train_loss:6.1482 train_time:100877ms step_avg:168.97ms\nstep:608/10000 train_loss:6.2569 train_time:101046ms step_avg:168.97ms\nstep:609/10000 train_loss:6.1344 train_time:101217ms step_avg:168.98ms\nstep:610/10000 train_loss:6.1976 train_time:101386ms step_avg:168.98ms\nstep:611/10000 train_loss:6.3180 train_time:101558ms step_avg:168.98ms\nstep:612/10000 train_loss:6.0746 train_time:101727ms step_avg:168.98ms\nstep:613/10000 train_loss:5.9059 train_time:101897ms step_avg:168.98ms\nstep:614/10000 train_loss:6.1178 train_time:102067ms step_avg:168.99ms\nstep:615/10000 train_loss:6.1345 train_time:102237ms step_avg:168.99ms\nstep:616/10000 train_loss:6.0795 train_time:102415ms step_avg:169.00ms\nstep:617/10000 train_loss:6.1868 train_time:102582ms step_avg:169.00ms\nstep:618/10000 train_loss:6.2240 train_time:102750ms step_avg:169.00ms\nstep:619/10000 train_loss:6.2684 train_time:102920ms step_avg:169.00ms\nstep:620/10000 train_loss:6.0752 train_time:103089ms step_avg:169.00ms\nstep:621/10000 train_loss:6.1919 train_time:103260ms step_avg:169.00ms\nstep:622/10000 train_loss:6.1199 train_time:103429ms step_avg:169.00ms\nstep:623/10000 train_loss:6.0882 train_time:103599ms step_avg:169.00ms\nstep:624/10000 train_loss:6.1815 train_time:103770ms step_avg:169.01ms\nstep:625/10000 train_loss:6.2572 train_time:103941ms step_avg:169.01ms\nstep:626/10000 train_loss:6.2888 train_time:104111ms step_avg:169.01ms\nstep:627/10000 train_loss:6.0334 train_time:104281ms step_avg:169.01ms\nstep:628/10000 train_loss:6.1209 train_time:104451ms step_avg:169.01ms\nstep:629/10000 train_loss:6.1415 train_time:104622ms step_avg:169.02ms\nstep:630/10000 train_loss:6.0435 train_time:104792ms step_avg:169.02ms\nstep:631/10000 train_loss:6.0194 train_time:104962ms step_avg:169.02ms\nstep:632/10000 train_loss:6.1796 train_time:105131ms step_avg:169.02ms\nstep:633/10000 train_loss:6.0358 train_time:105301ms step_avg:169.02ms\nstep:634/10000 train_loss:6.1138 train_time:105470ms step_avg:169.02ms\nstep:635/10000 train_loss:6.2280 train_time:105643ms step_avg:169.03ms\nstep:636/10000 train_loss:6.0340 train_time:105812ms step_avg:169.03ms\nstep:637/10000 train_loss:5.9950 train_time:105984ms step_avg:169.03ms\nstep:638/10000 train_loss:6.0582 train_time:106154ms step_avg:169.04ms\nstep:639/10000 train_loss:6.2869 train_time:106325ms step_avg:169.04ms\nstep:640/10000 train_loss:6.0540 train_time:106497ms step_avg:169.04ms\nstep:641/10000 train_loss:6.0411 train_time:106667ms step_avg:169.05ms\nstep:642/10000 train_loss:6.0593 train_time:106840ms step_avg:169.05ms\nstep:643/10000 train_loss:6.0806 train_time:107010ms step_avg:169.05ms\nstep:644/10000 train_loss:6.1073 train_time:107180ms step_avg:169.05ms\nstep:645/10000 train_loss:5.9218 train_time:107350ms step_avg:169.06ms\nstep:646/10000 train_loss:6.1389 train_time:107521ms step_avg:169.06ms\nstep:647/10000 train_loss:6.1419 train_time:107691ms step_avg:169.06ms\nstep:648/10000 train_loss:6.1221 train_time:107863ms step_avg:169.06ms\nstep:649/10000 train_loss:6.0335 train_time:108031ms step_avg:169.06ms\nstep:650/10000 train_loss:6.1288 train_time:108201ms step_avg:169.06ms\nstep:651/10000 train_loss:6.3008 train_time:108370ms step_avg:169.06ms\nstep:652/10000 train_loss:6.0201 train_time:108541ms step_avg:169.07ms\nstep:653/10000 train_loss:5.9885 train_time:108711ms step_avg:169.07ms\nstep:654/10000 train_loss:5.9582 train_time:108884ms step_avg:169.07ms\nstep:655/10000 train_loss:6.1265 train_time:109053ms step_avg:169.07ms\nstep:656/10000 train_loss:5.9316 train_time:109225ms step_avg:169.08ms\nstep:657/10000 train_loss:5.9297 train_time:109395ms step_avg:169.08ms\nstep:658/10000 train_loss:6.0897 train_time:109567ms step_avg:169.08ms\nstep:659/10000 train_loss:6.0934 train_time:109737ms step_avg:169.09ms\nstep:660/10000 train_loss:6.0538 train_time:109908ms step_avg:169.09ms\nstep:661/10000 train_loss:5.8645 train_time:110080ms step_avg:169.09ms\nstep:662/10000 train_loss:6.2346 train_time:110251ms step_avg:169.10ms\nstep:663/10000 train_loss:5.9947 train_time:110420ms step_avg:169.10ms\nstep:664/10000 train_loss:6.1529 train_time:110588ms step_avg:169.10ms\nstep:665/10000 train_loss:6.0986 train_time:110759ms step_avg:169.10ms\nstep:666/10000 train_loss:6.0121 train_time:110927ms step_avg:169.10ms\nstep:667/10000 train_loss:6.0206 train_time:111099ms step_avg:169.10ms\nstep:668/10000 train_loss:5.9314 train_time:111268ms step_avg:169.10ms\nstep:669/10000 train_loss:5.7876 train_time:111439ms step_avg:169.10ms\nstep:670/10000 train_loss:6.0488 train_time:111608ms step_avg:169.10ms\nstep:671/10000 train_loss:6.0927 train_time:111781ms step_avg:169.11ms\nstep:672/10000 train_loss:6.0108 train_time:111947ms step_avg:169.10ms\nstep:673/10000 train_loss:5.9563 train_time:112118ms step_avg:169.11ms\nstep:674/10000 train_loss:5.9387 train_time:112287ms step_avg:169.11ms\nstep:675/10000 train_loss:5.9098 train_time:112458ms step_avg:169.11ms\nstep:676/10000 train_loss:6.0254 train_time:112626ms step_avg:169.11ms\nstep:677/10000 train_loss:6.0062 train_time:112802ms step_avg:169.12ms\nstep:678/10000 train_loss:6.0398 train_time:112970ms step_avg:169.12ms\nstep:679/10000 train_loss:6.0502 train_time:113141ms step_avg:169.12ms\nstep:680/10000 train_loss:5.9999 train_time:113311ms step_avg:169.12ms\nstep:681/10000 train_loss:5.9922 train_time:113482ms step_avg:169.12ms\nstep:682/10000 train_loss:5.9876 train_time:113650ms step_avg:169.12ms\nstep:683/10000 train_loss:6.1611 train_time:113820ms step_avg:169.12ms\nstep:684/10000 train_loss:5.2970 train_time:113990ms step_avg:169.12ms\nstep:685/10000 train_loss:5.9163 train_time:114161ms step_avg:169.13ms\nstep:686/10000 train_loss:5.9471 train_time:114331ms step_avg:169.13ms\nstep:687/10000 train_loss:6.1273 train_time:114504ms step_avg:169.13ms\nstep:688/10000 train_loss:5.9373 train_time:114673ms step_avg:169.13ms\nstep:689/10000 train_loss:6.1976 train_time:114842ms step_avg:169.13ms\nstep:690/10000 train_loss:6.0348 train_time:115013ms step_avg:169.14ms\nstep:691/10000 train_loss:6.0725 train_time:115185ms step_avg:169.14ms\nstep:692/10000 train_loss:5.8661 train_time:115354ms step_avg:169.14ms\nstep:693/10000 train_loss:5.8513 train_time:115524ms step_avg:169.14ms\nstep:694/10000 train_loss:6.1166 train_time:115695ms step_avg:169.15ms\nstep:695/10000 train_loss:5.8585 train_time:115864ms step_avg:169.14ms\nstep:696/10000 train_loss:6.0722 train_time:116036ms step_avg:169.15ms\nstep:697/10000 train_loss:6.0199 train_time:116208ms step_avg:169.15ms\nstep:698/10000 train_loss:6.0406 train_time:116378ms step_avg:169.15ms\nstep:699/10000 train_loss:5.8900 train_time:116547ms step_avg:169.15ms\nstep:700/10000 train_loss:6.1145 train_time:116718ms step_avg:169.16ms\nstep:701/10000 train_loss:6.1656 train_time:116887ms step_avg:169.16ms\nstep:702/10000 train_loss:5.8737 train_time:117058ms step_avg:169.16ms\nstep:703/10000 train_loss:6.0358 train_time:117227ms step_avg:169.16ms\nstep:704/10000 train_loss:5.9680 train_time:117398ms step_avg:169.16ms\nstep:705/10000 train_loss:5.9137 train_time:117569ms step_avg:169.16ms\nstep:706/10000 train_loss:5.8402 train_time:117741ms step_avg:169.17ms\nstep:707/10000 train_loss:5.7907 train_time:117911ms step_avg:169.17ms\nstep:708/10000 train_loss:6.1568 train_time:118084ms step_avg:169.17ms\nstep:709/10000 train_loss:5.9387 train_time:118255ms step_avg:169.18ms\nstep:710/10000 train_loss:5.8322 train_time:118427ms step_avg:169.18ms\nstep:711/10000 train_loss:5.7381 train_time:118597ms step_avg:169.18ms\nstep:712/10000 train_loss:5.8779 train_time:118767ms step_avg:169.18ms\nstep:713/10000 train_loss:6.1893 train_time:118938ms step_avg:169.19ms\nstep:714/10000 train_loss:6.0095 train_time:119108ms step_avg:169.19ms\nstep:715/10000 train_loss:6.0558 train_time:119280ms step_avg:169.19ms\nstep:716/10000 train_loss:5.8901 train_time:119449ms step_avg:169.19ms\nstep:717/10000 train_loss:5.8890 train_time:119622ms step_avg:169.20ms\nstep:718/10000 train_loss:5.9166 train_time:119791ms step_avg:169.20ms\nstep:719/10000 train_loss:6.0312 train_time:119962ms step_avg:169.20ms\nstep:720/10000 train_loss:5.9650 train_time:120131ms step_avg:169.20ms\nstep:721/10000 train_loss:5.9716 train_time:120304ms step_avg:169.20ms\nstep:722/10000 train_loss:5.9633 train_time:120472ms step_avg:169.20ms\nstep:723/10000 train_loss:5.9279 train_time:120644ms step_avg:169.21ms\nstep:724/10000 train_loss:6.0448 train_time:120814ms step_avg:169.21ms\nstep:725/10000 train_loss:5.7870 train_time:120986ms step_avg:169.21ms\nstep:726/10000 train_loss:5.9474 train_time:121158ms step_avg:169.21ms\nstep:727/10000 train_loss:5.8947 train_time:121329ms step_avg:169.22ms\nstep:728/10000 train_loss:5.7332 train_time:121498ms step_avg:169.22ms\nstep:729/10000 train_loss:5.9991 train_time:121666ms step_avg:169.22ms\nstep:730/10000 train_loss:5.8322 train_time:121842ms step_avg:169.23ms\nstep:731/10000 train_loss:5.7592 train_time:122008ms step_avg:169.22ms\nstep:732/10000 train_loss:5.9850 train_time:122179ms step_avg:169.22ms\nstep:733/10000 train_loss:5.9091 train_time:122346ms step_avg:169.22ms\nstep:734/10000 train_loss:5.8048 train_time:122516ms step_avg:169.22ms\nstep:735/10000 train_loss:5.8156 train_time:122685ms step_avg:169.22ms\nstep:736/10000 train_loss:6.1307 train_time:122855ms step_avg:169.22ms\nstep:737/10000 train_loss:5.7791 train_time:123027ms step_avg:169.22ms\nstep:738/10000 train_loss:5.9773 train_time:123198ms step_avg:169.23ms\nstep:739/10000 train_loss:5.7357 train_time:123367ms step_avg:169.23ms\nstep:740/10000 train_loss:5.8667 train_time:123538ms step_avg:169.23ms\nstep:741/10000 train_loss:5.8514 train_time:123705ms step_avg:169.23ms\nstep:742/10000 train_loss:5.7912 train_time:123873ms step_avg:169.23ms\nstep:743/10000 train_loss:5.8280 train_time:124045ms step_avg:169.23ms\nstep:744/10000 train_loss:5.9076 train_time:124215ms step_avg:169.23ms\nstep:745/10000 train_loss:5.7710 train_time:124386ms step_avg:169.23ms\nstep:746/10000 train_loss:6.1591 train_time:124556ms step_avg:169.23ms\nstep:747/10000 train_loss:5.8944 train_time:124726ms step_avg:169.23ms\nstep:748/10000 train_loss:5.9177 train_time:124896ms step_avg:169.24ms\nstep:749/10000 train_loss:6.0384 train_time:125067ms step_avg:169.24ms\nstep:750/10000 train_loss:5.8620 train_time:125238ms step_avg:169.24ms\nstep:751/10000 train_loss:5.7677 train_time:125414ms step_avg:169.25ms\nstep:752/10000 train_loss:5.9793 train_time:125582ms step_avg:169.25ms\nstep:753/10000 train_loss:5.8996 train_time:125751ms step_avg:169.25ms\nstep:754/10000 train_loss:5.8010 train_time:125919ms step_avg:169.25ms\nstep:755/10000 train_loss:5.9318 train_time:126086ms step_avg:169.24ms\nstep:756/10000 train_loss:5.4083 train_time:126258ms step_avg:169.25ms\nstep:757/10000 train_loss:5.3073 train_time:126428ms step_avg:169.25ms\nstep:758/10000 train_loss:6.0077 train_time:126603ms step_avg:169.26ms\nstep:759/10000 train_loss:5.8886 train_time:126777ms step_avg:169.26ms\nstep:760/10000 train_loss:5.7759 train_time:126944ms step_avg:169.26ms\nstep:761/10000 train_loss:5.7154 train_time:127114ms step_avg:169.26ms\nstep:762/10000 train_loss:5.9818 train_time:127515ms step_avg:169.57ms\nstep:763/10000 train_loss:5.8402 train_time:127674ms step_avg:169.55ms\nstep:764/10000 train_loss:5.7729 train_time:127840ms step_avg:169.55ms\nstep:765/10000 train_loss:5.6996 train_time:128008ms step_avg:169.55ms\nstep:766/10000 train_loss:5.9463 train_time:128176ms step_avg:169.54ms\nstep:767/10000 train_loss:5.8321 train_time:128344ms step_avg:169.54ms\nstep:768/10000 train_loss:5.7383 train_time:128525ms step_avg:169.56ms\nstep:769/10000 train_loss:5.8956 train_time:128698ms step_avg:169.56ms\nstep:770/10000 train_loss:5.8330 train_time:128867ms step_avg:169.56ms\nstep:771/10000 train_loss:5.9090 train_time:129036ms step_avg:169.56ms\nstep:772/10000 train_loss:5.8429 train_time:129204ms step_avg:169.56ms\nstep:773/10000 train_loss:5.6153 train_time:129373ms step_avg:169.56ms\nstep:774/10000 train_loss:5.8469 train_time:129546ms step_avg:169.56ms\nstep:775/10000 train_loss:5.8885 train_time:129718ms step_avg:169.57ms\nstep:776/10000 train_loss:5.8614 train_time:129887ms step_avg:169.57ms\nstep:777/10000 train_loss:5.9117 train_time:130058ms step_avg:169.57ms\nstep:778/10000 train_loss:5.8401 train_time:130228ms step_avg:169.57ms\nstep:779/10000 train_loss:5.9480 train_time:130399ms step_avg:169.57ms\nstep:780/10000 train_loss:6.0650 train_time:130570ms step_avg:169.57ms\nstep:781/10000 train_loss:5.7858 train_time:130743ms step_avg:169.58ms\nstep:782/10000 train_loss:5.8020 train_time:130913ms step_avg:169.58ms\nstep:783/10000 train_loss:5.7664 train_time:131083ms step_avg:169.58ms\nstep:784/10000 train_loss:5.9267 train_time:131253ms step_avg:169.58ms\nstep:785/10000 train_loss:5.7158 train_time:131424ms step_avg:169.58ms\nstep:786/10000 train_loss:5.8980 train_time:131595ms step_avg:169.58ms\nstep:787/10000 train_loss:5.8551 train_time:131765ms step_avg:169.58ms\nstep:788/10000 train_loss:5.7471 train_time:131935ms step_avg:169.58ms\nstep:789/10000 train_loss:5.6970 train_time:132103ms step_avg:169.58ms\nstep:790/10000 train_loss:5.6954 train_time:132275ms step_avg:169.58ms\nstep:791/10000 train_loss:5.9914 train_time:132446ms step_avg:169.59ms\nstep:792/10000 train_loss:5.7427 train_time:132620ms step_avg:169.59ms\nstep:793/10000 train_loss:5.6800 train_time:132789ms step_avg:169.59ms\nstep:794/10000 train_loss:5.9039 train_time:132961ms step_avg:169.59ms\nstep:795/10000 train_loss:5.7769 train_time:133128ms step_avg:169.59ms\nstep:796/10000 train_loss:5.7180 train_time:133299ms step_avg:169.59ms\nstep:797/10000 train_loss:5.6999 train_time:133468ms step_avg:169.59ms\nstep:798/10000 train_loss:5.7939 train_time:133641ms step_avg:169.60ms\nstep:799/10000 train_loss:5.9894 train_time:133811ms step_avg:169.60ms\nstep:800/10000 train_loss:5.7839 train_time:133981ms step_avg:169.60ms\nstep:801/10000 train_loss:5.6975 train_time:134149ms step_avg:169.59ms\nstep:802/10000 train_loss:6.4363 train_time:134318ms step_avg:169.59ms\nstep:803/10000 train_loss:5.9255 train_time:134487ms step_avg:169.59ms\nstep:804/10000 train_loss:5.7698 train_time:134659ms step_avg:169.60ms\nstep:805/10000 train_loss:5.8819 train_time:134829ms step_avg:169.60ms\nstep:806/10000 train_loss:5.9746 train_time:134998ms step_avg:169.60ms\nstep:807/10000 train_loss:5.7399 train_time:135167ms step_avg:169.60ms\nstep:808/10000 train_loss:5.8761 train_time:135339ms step_avg:169.60ms\nstep:809/10000 train_loss:5.9271 train_time:135508ms step_avg:169.60ms\nstep:810/10000 train_loss:5.7625 train_time:135679ms step_avg:169.60ms\nstep:811/10000 train_loss:5.7149 train_time:135848ms step_avg:169.60ms\nstep:812/10000 train_loss:5.8400 train_time:136020ms step_avg:169.60ms\nstep:813/10000 train_loss:5.9493 train_time:136189ms step_avg:169.60ms\nstep:814/10000 train_loss:5.7271 train_time:136360ms step_avg:169.60ms\nstep:815/10000 train_loss:5.8049 train_time:136530ms step_avg:169.60ms\nstep:816/10000 train_loss:5.7144 train_time:136701ms step_avg:169.60ms\nstep:817/10000 train_loss:5.7114 train_time:136872ms step_avg:169.61ms\nstep:818/10000 train_loss:5.7279 train_time:137044ms step_avg:169.61ms\nstep:819/10000 train_loss:5.8172 train_time:137213ms step_avg:169.61ms\nstep:820/10000 train_loss:5.9371 train_time:137386ms step_avg:169.61ms\nstep:821/10000 train_loss:5.7715 train_time:137555ms step_avg:169.61ms\nstep:822/10000 train_loss:5.7266 train_time:137724ms step_avg:169.61ms\nstep:823/10000 train_loss:5.7643 train_time:137892ms step_avg:169.61ms\nstep:824/10000 train_loss:5.5972 train_time:138062ms step_avg:169.61ms\nstep:825/10000 train_loss:5.7212 train_time:138234ms step_avg:169.61ms\nstep:826/10000 train_loss:5.8209 train_time:138404ms step_avg:169.61ms\nstep:827/10000 train_loss:5.8373 train_time:138575ms step_avg:169.61ms\nstep:828/10000 train_loss:5.7496 train_time:138747ms step_avg:169.62ms\nstep:829/10000 train_loss:5.6320 train_time:138917ms step_avg:169.62ms\nstep:830/10000 train_loss:5.6275 train_time:139086ms step_avg:169.62ms\nstep:831/10000 train_loss:5.6482 train_time:139257ms step_avg:169.62ms\nstep:832/10000 train_loss:5.7153 train_time:139428ms step_avg:169.62ms\nstep:833/10000 train_loss:5.7781 train_time:139601ms step_avg:169.62ms\nstep:834/10000 train_loss:5.7408 train_time:139768ms step_avg:169.62ms\nstep:835/10000 train_loss:5.4698 train_time:139939ms step_avg:169.62ms\nstep:836/10000 train_loss:5.8617 train_time:140109ms step_avg:169.62ms\nstep:837/10000 train_loss:5.6951 train_time:140279ms step_avg:169.62ms\nstep:838/10000 train_loss:5.6514 train_time:140448ms step_avg:169.62ms\nstep:839/10000 train_loss:5.7666 train_time:140620ms step_avg:169.63ms\nstep:840/10000 train_loss:5.6868 train_time:140790ms step_avg:169.63ms\nstep:841/10000 train_loss:5.8226 train_time:140962ms step_avg:169.63ms\nstep:842/10000 train_loss:5.6876 train_time:141132ms step_avg:169.63ms\nstep:843/10000 train_loss:5.7827 train_time:141303ms step_avg:169.63ms\nstep:844/10000 train_loss:5.6467 train_time:141473ms step_avg:169.63ms\nstep:845/10000 train_loss:5.7361 train_time:141645ms step_avg:169.63ms\nstep:846/10000 train_loss:5.6546 train_time:141817ms step_avg:169.64ms\nstep:847/10000 train_loss:5.7462 train_time:141986ms step_avg:169.64ms\nstep:848/10000 train_loss:5.9467 train_time:142156ms step_avg:169.64ms\nstep:849/10000 train_loss:5.6738 train_time:142324ms step_avg:169.64ms\nstep:850/10000 train_loss:5.6328 train_time:142491ms step_avg:169.63ms\nstep:851/10000 train_loss:5.9518 train_time:142663ms step_avg:169.64ms\nstep:852/10000 train_loss:5.6015 train_time:142834ms step_avg:169.64ms\nstep:853/10000 train_loss:5.7700 train_time:143004ms step_avg:169.64ms\nstep:854/10000 train_loss:5.7150 train_time:143174ms step_avg:169.64ms\nstep:855/10000 train_loss:5.8469 train_time:143347ms step_avg:169.64ms\nstep:856/10000 train_loss:5.7240 train_time:143518ms step_avg:169.64ms\nstep:857/10000 train_loss:5.5440 train_time:143689ms step_avg:169.64ms\nstep:858/10000 train_loss:5.7248 train_time:143860ms step_avg:169.65ms\nstep:859/10000 train_loss:5.8581 train_time:144030ms step_avg:169.65ms\nstep:860/10000 train_loss:5.6335 train_time:144201ms step_avg:169.65ms\nstep:861/10000 train_loss:5.9516 train_time:144371ms step_avg:169.65ms\nstep:862/10000 train_loss:5.6058 train_time:144542ms step_avg:169.65ms\nstep:863/10000 train_loss:5.7272 train_time:144711ms step_avg:169.65ms\nstep:864/10000 train_loss:5.7512 train_time:144882ms step_avg:169.65ms\nstep:865/10000 train_loss:5.5735 train_time:145052ms step_avg:169.65ms\nstep:866/10000 train_loss:5.7142 train_time:145222ms step_avg:169.65ms\nstep:867/10000 train_loss:5.5474 train_time:145392ms step_avg:169.65ms\nstep:868/10000 train_loss:5.6691 train_time:145564ms step_avg:169.65ms\nstep:869/10000 train_loss:5.5632 train_time:145734ms step_avg:169.66ms\nstep:870/10000 train_loss:5.6409 train_time:145903ms step_avg:169.66ms\nstep:871/10000 train_loss:5.6693 train_time:146074ms step_avg:169.66ms\nstep:872/10000 train_loss:5.5849 train_time:146246ms step_avg:169.66ms\nstep:873/10000 train_loss:5.5146 train_time:146415ms step_avg:169.66ms\nstep:874/10000 train_loss:5.4451 train_time:146584ms step_avg:169.66ms\nstep:875/10000 train_loss:5.7203 train_time:146754ms step_avg:169.66ms\nstep:876/10000 train_loss:5.6382 train_time:146923ms step_avg:169.66ms\nstep:877/10000 train_loss:5.7194 train_time:147094ms step_avg:169.66ms\nstep:878/10000 train_loss:5.5977 train_time:147265ms step_avg:169.66ms\nstep:879/10000 train_loss:5.4414 train_time:147436ms step_avg:169.66ms\nstep:880/10000 train_loss:5.6120 train_time:147606ms step_avg:169.66ms\nstep:881/10000 train_loss:5.5612 train_time:147777ms step_avg:169.66ms\nstep:882/10000 train_loss:5.7308 train_time:147945ms step_avg:169.66ms\nstep:883/10000 train_loss:5.5802 train_time:148117ms step_avg:169.66ms\nstep:884/10000 train_loss:5.7307 train_time:148288ms step_avg:169.67ms\nstep:885/10000 train_loss:5.5689 train_time:148462ms step_avg:169.67ms\nstep:886/10000 train_loss:5.6955 train_time:148632ms step_avg:169.67ms\nstep:887/10000 train_loss:5.6063 train_time:148803ms step_avg:169.67ms\nstep:888/10000 train_loss:5.7586 train_time:148971ms step_avg:169.67ms\nstep:889/10000 train_loss:5.7033 train_time:149144ms step_avg:169.67ms\nstep:890/10000 train_loss:5.6592 train_time:149314ms step_avg:169.68ms\nstep:891/10000 train_loss:5.5903 train_time:149487ms step_avg:169.68ms\nstep:892/10000 train_loss:5.7529 train_time:149657ms step_avg:169.68ms\nstep:893/10000 train_loss:5.6733 train_time:149828ms step_avg:169.68ms\nstep:894/10000 train_loss:5.6274 train_time:149998ms step_avg:169.68ms\nstep:895/10000 train_loss:5.7539 train_time:150166ms step_avg:169.68ms\nstep:896/10000 train_loss:5.7190 train_time:150338ms step_avg:169.68ms\nstep:897/10000 train_loss:5.5210 train_time:150506ms step_avg:169.68ms\nstep:898/10000 train_loss:5.6552 train_time:150676ms step_avg:169.68ms\nstep:899/10000 train_loss:5.7519 train_time:150844ms step_avg:169.68ms\nstep:900/10000 train_loss:5.3915 train_time:151013ms step_avg:169.68ms\nstep:901/10000 train_loss:5.6392 train_time:151185ms step_avg:169.68ms\nstep:902/10000 train_loss:5.5095 train_time:151358ms step_avg:169.68ms\nstep:903/10000 train_loss:5.7031 train_time:151526ms step_avg:169.68ms\nstep:904/10000 train_loss:5.5256 train_time:151697ms step_avg:169.68ms\nstep:905/10000 train_loss:5.7219 train_time:151868ms step_avg:169.68ms\nstep:906/10000 train_loss:5.5115 train_time:152038ms step_avg:169.69ms\nstep:907/10000 train_loss:5.5567 train_time:152206ms step_avg:169.68ms\nstep:908/10000 train_loss:5.4858 train_time:152377ms step_avg:169.68ms\nstep:909/10000 train_loss:5.6558 train_time:152546ms step_avg:169.68ms\nstep:910/10000 train_loss:5.5558 train_time:152715ms step_avg:169.68ms\nstep:911/10000 train_loss:5.7175 train_time:152883ms step_avg:169.68ms\nstep:912/10000 train_loss:5.6454 train_time:153055ms step_avg:169.68ms\nstep:913/10000 train_loss:5.6490 train_time:153225ms step_avg:169.68ms\nstep:914/10000 train_loss:5.6745 train_time:153396ms step_avg:169.69ms\nstep:915/10000 train_loss:5.6410 train_time:153564ms step_avg:169.68ms\nstep:916/10000 train_loss:5.5833 train_time:153734ms step_avg:169.68ms\nstep:917/10000 train_loss:5.6021 train_time:153901ms step_avg:169.68ms\nstep:918/10000 train_loss:5.6151 train_time:154073ms step_avg:169.68ms\nstep:919/10000 train_loss:5.8182 train_time:154244ms step_avg:169.69ms\nstep:920/10000 train_loss:5.6998 train_time:154417ms step_avg:169.69ms\nstep:921/10000 train_loss:5.6446 train_time:154589ms step_avg:169.69ms\nstep:922/10000 train_loss:5.5826 train_time:154761ms step_avg:169.69ms\nstep:923/10000 train_loss:5.4451 train_time:154931ms step_avg:169.69ms\nstep:924/10000 train_loss:5.5826 train_time:155103ms step_avg:169.70ms\nstep:925/10000 train_loss:5.4133 train_time:155274ms step_avg:169.70ms\nstep:926/10000 train_loss:5.7047 train_time:155447ms step_avg:169.70ms\nstep:927/10000 train_loss:5.5093 train_time:155620ms step_avg:169.71ms\nstep:928/10000 train_loss:5.4143 train_time:155792ms step_avg:169.71ms\nstep:929/10000 train_loss:5.6595 train_time:155958ms step_avg:169.70ms\nstep:930/10000 train_loss:5.4778 train_time:156126ms step_avg:169.70ms\nstep:931/10000 train_loss:5.5254 train_time:156298ms step_avg:169.70ms\nstep:932/10000 train_loss:5.6844 train_time:156469ms step_avg:169.71ms\nstep:933/10000 train_loss:5.7057 train_time:156641ms step_avg:169.71ms\nstep:934/10000 train_loss:5.4966 train_time:156811ms step_avg:169.71ms\nstep:935/10000 train_loss:5.5365 train_time:156983ms step_avg:169.71ms\nstep:936/10000 train_loss:5.5180 train_time:157152ms step_avg:169.71ms\nstep:937/10000 train_loss:5.5714 train_time:157322ms step_avg:169.71ms\nstep:938/10000 train_loss:5.7371 train_time:157493ms step_avg:169.71ms\nstep:939/10000 train_loss:5.7401 train_time:157666ms step_avg:169.72ms\nstep:940/10000 train_loss:5.3562 train_time:157839ms step_avg:169.72ms\nstep:941/10000 train_loss:5.7265 train_time:158009ms step_avg:169.72ms\nstep:942/10000 train_loss:5.5906 train_time:158178ms step_avg:169.72ms\nstep:943/10000 train_loss:5.4399 train_time:158347ms step_avg:169.72ms\nstep:944/10000 train_loss:5.7523 train_time:158518ms step_avg:169.72ms\nstep:945/10000 train_loss:5.4121 train_time:158687ms step_avg:169.72ms\nstep:946/10000 train_loss:5.7316 train_time:158860ms step_avg:169.72ms\nstep:947/10000 train_loss:5.6787 train_time:159029ms step_avg:169.72ms\nstep:948/10000 train_loss:5.5291 train_time:159200ms step_avg:169.72ms\nstep:949/10000 train_loss:5.2988 train_time:159368ms step_avg:169.72ms\nstep:950/10000 train_loss:5.6312 train_time:159538ms step_avg:169.72ms\nstep:951/10000 train_loss:5.6124 train_time:159707ms step_avg:169.72ms\nstep:952/10000 train_loss:5.4973 train_time:159878ms step_avg:169.72ms\nstep:953/10000 train_loss:5.7332 train_time:160049ms step_avg:169.72ms\nstep:954/10000 train_loss:5.5021 train_time:160219ms step_avg:169.72ms\nstep:955/10000 train_loss:5.5838 train_time:160390ms step_avg:169.72ms\nstep:956/10000 train_loss:5.6351 train_time:160563ms step_avg:169.73ms\nstep:957/10000 train_loss:5.4276 train_time:160732ms step_avg:169.73ms\nstep:958/10000 train_loss:5.5805 train_time:160902ms step_avg:169.73ms\nstep:959/10000 train_loss:5.4246 train_time:161070ms step_avg:169.73ms\nstep:960/10000 train_loss:5.6414 train_time:161240ms step_avg:169.73ms\nstep:961/10000 train_loss:5.4204 train_time:161407ms step_avg:169.72ms\nstep:962/10000 train_loss:5.8388 train_time:161579ms step_avg:169.73ms\nstep:963/10000 train_loss:5.6323 train_time:161749ms step_avg:169.73ms\nstep:964/10000 train_loss:5.5198 train_time:161920ms step_avg:169.73ms\nstep:965/10000 train_loss:5.6219 train_time:162088ms step_avg:169.73ms\nstep:966/10000 train_loss:5.5050 train_time:162258ms step_avg:169.73ms\nstep:967/10000 train_loss:5.6343 train_time:162428ms step_avg:169.73ms\nstep:968/10000 train_loss:5.6999 train_time:162599ms step_avg:169.73ms\nstep:969/10000 train_loss:5.6477 train_time:162769ms step_avg:169.73ms\nstep:970/10000 train_loss:5.8625 train_time:162943ms step_avg:169.73ms\nstep:971/10000 train_loss:5.6475 train_time:163113ms step_avg:169.73ms\nstep:972/10000 train_loss:5.4743 train_time:163287ms step_avg:169.74ms\nstep:973/10000 train_loss:5.4149 train_time:163455ms step_avg:169.74ms\nstep:974/10000 train_loss:5.4347 train_time:163626ms step_avg:169.74ms\nstep:975/10000 train_loss:5.4129 train_time:163798ms step_avg:169.74ms\nstep:976/10000 train_loss:5.4416 train_time:163967ms step_avg:169.74ms\nstep:977/10000 train_loss:5.5580 train_time:164136ms step_avg:169.74ms\nstep:978/10000 train_loss:5.4919 train_time:164305ms step_avg:169.74ms\nstep:979/10000 train_loss:5.6466 train_time:164476ms step_avg:169.74ms\nstep:980/10000 train_loss:5.5318 train_time:164642ms step_avg:169.73ms\nstep:981/10000 train_loss:5.5445 train_time:164810ms step_avg:169.73ms\nstep:982/10000 train_loss:5.5194 train_time:164982ms step_avg:169.73ms\nstep:983/10000 train_loss:5.5528 train_time:165151ms step_avg:169.73ms\nstep:984/10000 train_loss:5.5087 train_time:165320ms step_avg:169.73ms\nstep:985/10000 train_loss:5.4620 train_time:165489ms step_avg:169.73ms\nstep:986/10000 train_loss:5.5109 train_time:165661ms step_avg:169.73ms\nstep:987/10000 train_loss:5.6526 train_time:165830ms step_avg:169.73ms\nstep:988/10000 train_loss:5.4778 train_time:166001ms step_avg:169.74ms\nstep:989/10000 train_loss:5.2952 train_time:166170ms step_avg:169.73ms\nstep:990/10000 train_loss:5.5482 train_time:166341ms step_avg:169.74ms\nstep:991/10000 train_loss:5.5151 train_time:166511ms step_avg:169.74ms\nstep:992/10000 train_loss:5.7066 train_time:166683ms step_avg:169.74ms\nstep:993/10000 train_loss:5.4356 train_time:166852ms step_avg:169.74ms\nstep:994/10000 train_loss:5.3565 train_time:167024ms step_avg:169.74ms\nstep:995/10000 train_loss:5.6073 train_time:167192ms step_avg:169.74ms\nstep:996/10000 train_loss:5.3853 train_time:167360ms step_avg:169.74ms\nstep:997/10000 train_loss:5.5392 train_time:167530ms step_avg:169.74ms\nstep:998/10000 train_loss:5.5417 train_time:167702ms step_avg:169.74ms\nstep:999/10000 train_loss:5.4557 train_time:167873ms step_avg:169.74ms\nstep:1000/10000 train_loss:5.3967 train_time:168044ms step_avg:169.74ms\nstep:1001/10000 train_loss:5.4464 train_time:168216ms step_avg:169.74ms\nstep:1002/10000 train_loss:5.4620 train_time:168387ms step_avg:169.75ms\nstep:1003/10000 train_loss:5.2475 train_time:168557ms step_avg:169.75ms\nstep:1004/10000 train_loss:5.5405 train_time:168728ms step_avg:169.75ms\nstep:1005/10000 train_loss:5.5685 train_time:168899ms step_avg:169.75ms\nstep:1006/10000 train_loss:5.3493 train_time:169075ms step_avg:169.75ms\nstep:1007/10000 train_loss:5.7162 train_time:169238ms step_avg:169.75ms\nstep:1008/10000 train_loss:5.4517 train_time:169407ms step_avg:169.75ms\nstep:1009/10000 train_loss:5.4440 train_time:169579ms step_avg:169.75ms\nstep:1010/10000 train_loss:5.3165 train_time:169749ms step_avg:169.75ms\nstep:1011/10000 train_loss:5.4909 train_time:169920ms step_avg:169.75ms\nstep:1012/10000 train_loss:5.4328 train_time:170088ms step_avg:169.75ms\nstep:1013/10000 train_loss:5.3507 train_time:170258ms step_avg:169.75ms\nstep:1014/10000 train_loss:5.4266 train_time:170427ms step_avg:169.75ms\nstep:1015/10000 train_loss:5.4410 train_time:170599ms step_avg:169.75ms\nstep:1016/10000 train_loss:5.4505 train_time:170768ms step_avg:169.75ms\nstep:1017/10000 train_loss:5.4051 train_time:170939ms step_avg:169.75ms\nstep:1018/10000 train_loss:5.4623 train_time:171108ms step_avg:169.75ms\nstep:1019/10000 train_loss:5.2891 train_time:171277ms step_avg:169.75ms\nstep:1020/10000 train_loss:5.5137 train_time:171446ms step_avg:169.75ms\nstep:1021/10000 train_loss:5.5881 train_time:171618ms step_avg:169.75ms\nstep:1022/10000 train_loss:5.3127 train_time:171786ms step_avg:169.75ms\nstep:1023/10000 train_loss:5.4635 train_time:171957ms step_avg:169.75ms\nstep:1024/10000 train_loss:5.5693 train_time:172127ms step_avg:169.75ms\nstep:1025/10000 train_loss:5.4481 train_time:172299ms step_avg:169.75ms\nstep:1026/10000 train_loss:5.4569 train_time:172467ms step_avg:169.75ms\nstep:1027/10000 train_loss:5.3197 train_time:172636ms step_avg:169.75ms\nstep:1028/10000 train_loss:5.4897 train_time:172806ms step_avg:169.75ms\nstep:1029/10000 train_loss:5.4547 train_time:172976ms step_avg:169.75ms\nstep:1030/10000 train_loss:5.6044 train_time:173145ms step_avg:169.75ms\nstep:1031/10000 train_loss:5.4015 train_time:173316ms step_avg:169.75ms\nstep:1032/10000 train_loss:5.3539 train_time:173485ms step_avg:169.75ms\nstep:1033/10000 train_loss:5.6661 train_time:173654ms step_avg:169.75ms\nstep:1034/10000 train_loss:5.2931 train_time:173824ms step_avg:169.75ms\nstep:1035/10000 train_loss:5.3319 train_time:173994ms step_avg:169.75ms\nstep:1036/10000 train_loss:5.3749 train_time:174167ms step_avg:169.75ms\nstep:1037/10000 train_loss:5.3380 train_time:174337ms step_avg:169.75ms\nstep:1038/10000 train_loss:5.4218 train_time:174507ms step_avg:169.75ms\nstep:1039/10000 train_loss:5.3704 train_time:174678ms step_avg:169.76ms\nstep:1040/10000 train_loss:5.4163 train_time:174846ms step_avg:169.75ms\nstep:1041/10000 train_loss:5.5033 train_time:175015ms step_avg:169.75ms\nstep:1042/10000 train_loss:5.3088 train_time:175187ms step_avg:169.76ms\nstep:1043/10000 train_loss:5.4795 train_time:175361ms step_avg:169.76ms\nstep:1044/10000 train_loss:5.4540 train_time:175529ms step_avg:169.76ms\nstep:1045/10000 train_loss:5.2079 train_time:175699ms step_avg:169.76ms\nstep:1046/10000 train_loss:5.3545 train_time:175867ms step_avg:169.76ms\nstep:1047/10000 train_loss:5.2682 train_time:176036ms step_avg:169.76ms\nstep:1048/10000 train_loss:5.4195 train_time:176208ms step_avg:169.76ms\nstep:1049/10000 train_loss:5.5996 train_time:176380ms step_avg:169.76ms\nstep:1050/10000 train_loss:5.4375 train_time:176550ms step_avg:169.76ms\nstep:1051/10000 train_loss:5.4976 train_time:176719ms step_avg:169.76ms\nstep:1052/10000 train_loss:5.3721 train_time:176888ms step_avg:169.76ms\nstep:1053/10000 train_loss:5.4440 train_time:177060ms step_avg:169.76ms\nstep:1054/10000 train_loss:5.4800 train_time:177231ms step_avg:169.76ms\nstep:1055/10000 train_loss:5.3949 train_time:177404ms step_avg:169.76ms\nstep:1056/10000 train_loss:5.3599 train_time:177573ms step_avg:169.76ms\nstep:1057/10000 train_loss:5.4610 train_time:177742ms step_avg:169.76ms\nstep:1058/10000 train_loss:5.8349 train_time:177910ms step_avg:169.76ms\nstep:1059/10000 train_loss:5.3308 train_time:178080ms step_avg:169.76ms\nstep:1060/10000 train_loss:5.2157 train_time:178248ms step_avg:169.76ms\nstep:1061/10000 train_loss:5.1435 train_time:178417ms step_avg:169.76ms\nstep:1062/10000 train_loss:5.4316 train_time:178587ms step_avg:169.76ms\nstep:1063/10000 train_loss:5.5777 train_time:178758ms step_avg:169.76ms\nstep:1064/10000 train_loss:5.2823 train_time:178927ms step_avg:169.76ms\nstep:1065/10000 train_loss:5.2718 train_time:179096ms step_avg:169.76ms\nstep:1066/10000 train_loss:5.4901 train_time:179264ms step_avg:169.76ms\nstep:1067/10000 train_loss:5.4935 train_time:179435ms step_avg:169.76ms\nstep:1068/10000 train_loss:5.3529 train_time:179604ms step_avg:169.76ms\nstep:1069/10000 train_loss:5.1998 train_time:179776ms step_avg:169.76ms\nstep:1070/10000 train_loss:5.3088 train_time:179945ms step_avg:169.76ms\nstep:1071/10000 train_loss:5.4159 train_time:180116ms step_avg:169.76ms\nstep:1072/10000 train_loss:5.2276 train_time:180285ms step_avg:169.76ms\nstep:1073/10000 train_loss:5.4487 train_time:180454ms step_avg:169.76ms\nstep:1074/10000 train_loss:5.2503 train_time:180624ms step_avg:169.76ms\nstep:1075/10000 train_loss:5.2038 train_time:180794ms step_avg:169.76ms\nstep:1076/10000 train_loss:5.3095 train_time:180965ms step_avg:169.76ms\nstep:1077/10000 train_loss:5.3777 train_time:181136ms step_avg:169.76ms\nstep:1078/10000 train_loss:5.4128 train_time:181305ms step_avg:169.76ms\nstep:1079/10000 train_loss:5.2581 train_time:181477ms step_avg:169.76ms\nstep:1080/10000 train_loss:5.4599 train_time:181646ms step_avg:169.76ms\nstep:1081/10000 train_loss:5.1964 train_time:181816ms step_avg:169.76ms\nstep:1082/10000 train_loss:5.4455 train_time:181983ms step_avg:169.76ms\nstep:1083/10000 train_loss:5.3605 train_time:182152ms step_avg:169.76ms\nstep:1084/10000 train_loss:5.4146 train_time:182323ms step_avg:169.76ms\nstep:1085/10000 train_loss:5.4262 train_time:182494ms step_avg:169.76ms\nstep:1086/10000 train_loss:5.3929 train_time:182664ms step_avg:169.76ms\nstep:1087/10000 train_loss:5.3847 train_time:182837ms step_avg:169.77ms\nstep:1088/10000 train_loss:5.2050 train_time:183006ms step_avg:169.76ms\nstep:1089/10000 train_loss:5.2836 train_time:183177ms step_avg:169.77ms\nstep:1090/10000 train_loss:5.2473 train_time:183349ms step_avg:169.77ms\nstep:1091/10000 train_loss:5.3818 train_time:183519ms step_avg:169.77ms\nstep:1092/10000 train_loss:5.2691 train_time:183687ms step_avg:169.77ms\nstep:1093/10000 train_loss:5.3747 train_time:183855ms step_avg:169.76ms\nstep:1094/10000 train_loss:5.5883 train_time:184023ms step_avg:169.76ms\nstep:1095/10000 train_loss:5.2543 train_time:184194ms step_avg:169.76ms\nstep:1096/10000 train_loss:5.4361 train_time:184362ms step_avg:169.76ms\nstep:1097/10000 train_loss:5.4289 train_time:184533ms step_avg:169.76ms\nstep:1098/10000 train_loss:5.2321 train_time:184703ms step_avg:169.76ms\nstep:1099/10000 train_loss:5.1629 train_time:184875ms step_avg:169.77ms\nstep:1100/10000 train_loss:5.4051 train_time:185047ms step_avg:169.77ms\nstep:1101/10000 train_loss:5.1737 train_time:185217ms step_avg:169.77ms\nstep:1102/10000 train_loss:5.3896 train_time:185388ms step_avg:169.77ms\nstep:1103/10000 train_loss:5.3591 train_time:185557ms step_avg:169.77ms\nstep:1104/10000 train_loss:5.6114 train_time:185726ms step_avg:169.77ms\nstep:1105/10000 train_loss:5.3237 train_time:185896ms step_avg:169.77ms\nstep:1106/10000 train_loss:5.4006 train_time:186064ms step_avg:169.77ms\nstep:1107/10000 train_loss:5.2422 train_time:186234ms step_avg:169.77ms\nstep:1108/10000 train_loss:5.3435 train_time:186406ms step_avg:169.77ms\nstep:1109/10000 train_loss:5.3394 train_time:186576ms step_avg:169.77ms\nstep:1110/10000 train_loss:5.2297 train_time:186743ms step_avg:169.77ms\nstep:1111/10000 train_loss:5.3141 train_time:186912ms step_avg:169.77ms\nstep:1112/10000 train_loss:5.5234 train_time:187082ms step_avg:169.77ms\nstep:1113/10000 train_loss:5.2192 train_time:187250ms step_avg:169.76ms\nstep:1114/10000 train_loss:5.5232 train_time:187427ms step_avg:169.77ms\nstep:1115/10000 train_loss:5.2732 train_time:187596ms step_avg:169.77ms\nstep:1116/10000 train_loss:5.3276 train_time:187764ms step_avg:169.77ms\nstep:1117/10000 train_loss:5.2134 train_time:187934ms step_avg:169.77ms\nstep:1118/10000 train_loss:5.3274 train_time:188108ms step_avg:169.77ms\nstep:1119/10000 train_loss:5.3526 train_time:188274ms step_avg:169.77ms\nstep:1120/10000 train_loss:5.2203 train_time:188445ms step_avg:169.77ms\nstep:1121/10000 train_loss:5.4266 train_time:188618ms step_avg:169.77ms\nstep:1122/10000 train_loss:5.2702 train_time:188787ms step_avg:169.77ms\nstep:1123/10000 train_loss:5.1984 train_time:188956ms step_avg:169.77ms\nstep:1124/10000 train_loss:5.3165 train_time:189126ms step_avg:169.77ms\nstep:1125/10000 train_loss:5.2765 train_time:189296ms step_avg:169.77ms\nstep:1126/10000 train_loss:5.3116 train_time:189465ms step_avg:169.77ms\nstep:1127/10000 train_loss:5.3514 train_time:189637ms step_avg:169.77ms\nstep:1128/10000 train_loss:5.4979 train_time:189806ms step_avg:169.77ms\nstep:1129/10000 train_loss:5.3268 train_time:189978ms step_avg:169.77ms\nstep:1130/10000 train_loss:5.4102 train_time:190146ms step_avg:169.77ms\nstep:1131/10000 train_loss:5.2783 train_time:190318ms step_avg:169.78ms\nstep:1132/10000 train_loss:5.2419 train_time:190488ms step_avg:169.78ms\nstep:1133/10000 train_loss:5.2503 train_time:190661ms step_avg:169.78ms\nstep:1134/10000 train_loss:5.3306 train_time:190830ms step_avg:169.78ms\nstep:1135/10000 train_loss:5.2900 train_time:190999ms step_avg:169.78ms\nstep:1136/10000 train_loss:5.3964 train_time:191168ms step_avg:169.78ms\nstep:1137/10000 train_loss:5.1530 train_time:191338ms step_avg:169.78ms\nstep:1138/10000 train_loss:5.2176 train_time:191504ms step_avg:169.77ms\nstep:1139/10000 train_loss:5.3997 train_time:191675ms step_avg:169.77ms\nstep:1140/10000 train_loss:5.3560 train_time:191847ms step_avg:169.78ms\nstep:1141/10000 train_loss:5.3563 train_time:192022ms step_avg:169.78ms\nstep:1142/10000 train_loss:5.3333 train_time:192190ms step_avg:169.78ms\nstep:1143/10000 train_loss:5.2864 train_time:192358ms step_avg:169.78ms\nstep:1144/10000 train_loss:5.4116 train_time:192526ms step_avg:169.78ms\nstep:1145/10000 train_loss:5.3199 train_time:192697ms step_avg:169.78ms\nstep:1146/10000 train_loss:5.2662 train_time:192867ms step_avg:169.78ms\nstep:1147/10000 train_loss:5.2752 train_time:193038ms step_avg:169.78ms\nstep:1148/10000 train_loss:5.3003 train_time:193206ms step_avg:169.78ms\nstep:1149/10000 train_loss:5.2701 train_time:193376ms step_avg:169.78ms\nstep:1150/10000 train_loss:5.2999 train_time:193545ms step_avg:169.78ms\nstep:1151/10000 train_loss:5.2271 train_time:193717ms step_avg:169.78ms\nstep:1152/10000 train_loss:5.3156 train_time:193886ms step_avg:169.78ms\nstep:1153/10000 train_loss:5.3012 train_time:194057ms step_avg:169.78ms\nstep:1154/10000 train_loss:5.4343 train_time:194227ms step_avg:169.78ms\nstep:1155/10000 train_loss:5.3181 train_time:194396ms step_avg:169.78ms\nstep:1156/10000 train_loss:5.1582 train_time:194565ms step_avg:169.78ms\nstep:1157/10000 train_loss:5.0074 train_time:194736ms step_avg:169.78ms\nstep:1158/10000 train_loss:5.4052 train_time:194905ms step_avg:169.78ms\nstep:1159/10000 train_loss:5.1587 train_time:195076ms step_avg:169.78ms\nstep:1160/10000 train_loss:5.1822 train_time:195244ms step_avg:169.78ms\nstep:1161/10000 train_loss:5.3974 train_time:195415ms step_avg:169.78ms\nstep:1162/10000 train_loss:5.2256 train_time:195584ms step_avg:169.78ms\nstep:1163/10000 train_loss:5.2525 train_time:195756ms step_avg:169.78ms\nstep:1164/10000 train_loss:5.3510 train_time:195924ms step_avg:169.78ms\nstep:1165/10000 train_loss:5.2565 train_time:196095ms step_avg:169.78ms\nstep:1166/10000 train_loss:5.1601 train_time:196266ms step_avg:169.78ms\nstep:1167/10000 train_loss:5.2840 train_time:196435ms step_avg:169.78ms\nstep:1168/10000 train_loss:5.1986 train_time:196604ms step_avg:169.78ms\nstep:1169/10000 train_loss:5.3138 train_time:196778ms step_avg:169.78ms\nstep:1170/10000 train_loss:5.3338 train_time:196946ms step_avg:169.78ms\nstep:1171/10000 train_loss:5.2418 train_time:197117ms step_avg:169.78ms\nstep:1172/10000 train_loss:5.2453 train_time:197284ms step_avg:169.78ms\nstep:1173/10000 train_loss:5.3001 train_time:197456ms step_avg:169.78ms\nstep:1174/10000 train_loss:5.1571 train_time:197624ms step_avg:169.78ms\nstep:1175/10000 train_loss:5.1298 train_time:197793ms step_avg:169.78ms\nstep:1176/10000 train_loss:5.2074 train_time:197961ms step_avg:169.78ms\nstep:1177/10000 train_loss:5.4705 train_time:198131ms step_avg:169.78ms\nstep:1178/10000 train_loss:5.3123 train_time:198298ms step_avg:169.78ms\nstep:1179/10000 train_loss:5.4230 train_time:198467ms step_avg:169.78ms\nstep:1180/10000 train_loss:5.1280 train_time:198637ms step_avg:169.78ms\nstep:1181/10000 train_loss:5.0112 train_time:198805ms step_avg:169.77ms\nstep:1182/10000 train_loss:5.3790 train_time:198975ms step_avg:169.77ms\nstep:1183/10000 train_loss:5.3665 train_time:199146ms step_avg:169.77ms\nstep:1184/10000 train_loss:5.0770 train_time:199317ms step_avg:169.78ms\nstep:1185/10000 train_loss:5.2676 train_time:199487ms step_avg:169.78ms\nstep:1186/10000 train_loss:5.1862 train_time:199657ms step_avg:169.78ms\nstep:1187/10000 train_loss:5.3446 train_time:199828ms step_avg:169.78ms\nstep:1188/10000 train_loss:5.1312 train_time:199999ms step_avg:169.78ms\nstep:1189/10000 train_loss:5.4316 train_time:200169ms step_avg:169.78ms\nstep:1190/10000 train_loss:5.3741 train_time:200340ms step_avg:169.78ms\nstep:1191/10000 train_loss:5.2378 train_time:200509ms step_avg:169.78ms\nstep:1192/10000 train_loss:5.2115 train_time:200679ms step_avg:169.78ms\nstep:1193/10000 train_loss:5.3623 train_time:200848ms step_avg:169.78ms\nstep:1194/10000 train_loss:5.2003 train_time:201021ms step_avg:169.78ms\nstep:1195/10000 train_loss:5.1802 train_time:201190ms step_avg:169.78ms\nstep:1196/10000 train_loss:5.1192 train_time:201362ms step_avg:169.78ms\nstep:1197/10000 train_loss:5.1911 train_time:201531ms step_avg:169.78ms\nstep:1198/10000 train_loss:5.4808 train_time:201701ms step_avg:169.78ms\nstep:1199/10000 train_loss:5.0982 train_time:201870ms step_avg:169.78ms\nstep:1200/10000 train_loss:5.3273 train_time:202040ms step_avg:169.78ms\nstep:1201/10000 train_loss:5.2969 train_time:202210ms step_avg:169.78ms\nstep:1202/10000 train_loss:5.2007 train_time:202382ms step_avg:169.78ms\nstep:1203/10000 train_loss:5.1707 train_time:202550ms step_avg:169.78ms\nstep:1204/10000 train_loss:5.2517 train_time:202718ms step_avg:169.78ms\nstep:1205/10000 train_loss:5.1736 train_time:202888ms step_avg:169.78ms\nstep:1206/10000 train_loss:5.1980 train_time:203061ms step_avg:169.78ms\nstep:1207/10000 train_loss:5.2638 train_time:203234ms step_avg:169.79ms\nstep:1208/10000 train_loss:5.1608 train_time:203407ms step_avg:169.79ms\nstep:1209/10000 train_loss:5.2824 train_time:203575ms step_avg:169.79ms\nstep:1210/10000 train_loss:5.2032 train_time:203743ms step_avg:169.79ms\nstep:1211/10000 train_loss:5.2129 train_time:203912ms step_avg:169.78ms\nstep:1212/10000 train_loss:5.2572 train_time:204081ms step_avg:169.78ms\nstep:1213/10000 train_loss:5.1884 train_time:204251ms step_avg:169.78ms\nstep:1214/10000 train_loss:5.2629 train_time:204421ms step_avg:169.78ms\nstep:1215/10000 train_loss:5.1439 train_time:204590ms step_avg:169.78ms\nstep:1216/10000 train_loss:5.2225 train_time:204761ms step_avg:169.79ms\nstep:1217/10000 train_loss:5.2305 train_time:204930ms step_avg:169.78ms\nstep:1218/10000 train_loss:5.2071 train_time:205101ms step_avg:169.79ms\nstep:1219/10000 train_loss:5.2359 train_time:205270ms step_avg:169.79ms\nstep:1220/10000 train_loss:5.6340 train_time:205442ms step_avg:169.79ms\nstep:1221/10000 train_loss:5.0486 train_time:205611ms step_avg:169.79ms\nstep:1222/10000 train_loss:5.2992 train_time:205788ms step_avg:169.79ms\nstep:1223/10000 train_loss:4.9330 train_time:205952ms step_avg:169.79ms\nstep:1224/10000 train_loss:5.1626 train_time:206123ms step_avg:169.79ms\nstep:1225/10000 train_loss:5.1358 train_time:206294ms step_avg:169.79ms\nstep:1226/10000 train_loss:5.2157 train_time:206466ms step_avg:169.79ms\nstep:1227/10000 train_loss:5.3050 train_time:206637ms step_avg:169.79ms\nstep:1228/10000 train_loss:5.0716 train_time:206808ms step_avg:169.79ms\nstep:1229/10000 train_loss:5.2246 train_time:206978ms step_avg:169.79ms\nstep:1230/10000 train_loss:5.1906 train_time:207147ms step_avg:169.79ms\nstep:1231/10000 train_loss:5.2200 train_time:207318ms step_avg:169.79ms\nstep:1232/10000 train_loss:5.3020 train_time:207489ms step_avg:169.79ms\nstep:1233/10000 train_loss:5.0526 train_time:207658ms step_avg:169.79ms\nstep:1234/10000 train_loss:5.3505 train_time:207827ms step_avg:169.79ms\nstep:1235/10000 train_loss:5.0927 train_time:207997ms step_avg:169.79ms\nstep:1236/10000 train_loss:5.0230 train_time:208167ms step_avg:169.79ms\nstep:1237/10000 train_loss:5.0846 train_time:208338ms step_avg:169.79ms\nstep:1238/10000 train_loss:5.1482 train_time:208508ms step_avg:169.79ms\nstep:1239/10000 train_loss:5.3596 train_time:208680ms step_avg:169.80ms\nstep:1240/10000 train_loss:5.2745 train_time:208850ms step_avg:169.80ms\nstep:1241/10000 train_loss:5.2362 train_time:209022ms step_avg:169.80ms\nstep:1242/10000 train_loss:5.2114 train_time:209192ms step_avg:169.80ms\nstep:1243/10000 train_loss:5.2869 train_time:209361ms step_avg:169.80ms\nstep:1244/10000 train_loss:5.2492 train_time:209532ms step_avg:169.80ms\nstep:1245/10000 train_loss:5.2323 train_time:209701ms step_avg:169.80ms\nstep:1246/10000 train_loss:5.1934 train_time:209869ms step_avg:169.80ms\nstep:1247/10000 train_loss:5.3041 train_time:210038ms step_avg:169.80ms\nstep:1248/10000 train_loss:5.1419 train_time:210207ms step_avg:169.80ms\nstep:1249/10000 train_loss:5.1845 train_time:210378ms step_avg:169.80ms\nstep:1250/10000 train_loss:5.0721 train_time:210546ms step_avg:169.80ms\nstep:1251/10000 train_loss:5.1587 train_time:210717ms step_avg:169.80ms\nstep:1252/10000 train_loss:5.3477 train_time:210886ms step_avg:169.80ms\nstep:1253/10000 train_loss:5.2632 train_time:211058ms step_avg:169.80ms\nstep:1254/10000 train_loss:5.0953 train_time:211227ms step_avg:169.80ms\nstep:1255/10000 train_loss:5.2744 train_time:211401ms step_avg:169.80ms\nstep:1256/10000 train_loss:5.3850 train_time:211572ms step_avg:169.80ms\nstep:1257/10000 train_loss:5.1420 train_time:211743ms step_avg:169.80ms\nstep:1258/10000 train_loss:5.2910 train_time:211911ms step_avg:169.80ms\nstep:1259/10000 train_loss:5.1985 train_time:212082ms step_avg:169.80ms\nstep:1260/10000 train_loss:5.2308 train_time:212252ms step_avg:169.80ms\nstep:1261/10000 train_loss:5.2676 train_time:212424ms step_avg:169.80ms\nstep:1262/10000 train_loss:4.8892 train_time:212596ms step_avg:169.80ms\nstep:1263/10000 train_loss:5.2621 train_time:212767ms step_avg:169.81ms\nstep:1264/10000 train_loss:5.1458 train_time:212936ms step_avg:169.81ms\nstep:1265/10000 train_loss:5.2115 train_time:213105ms step_avg:169.80ms\nstep:1266/10000 train_loss:5.1779 train_time:213275ms step_avg:169.81ms\nstep:1267/10000 train_loss:5.1246 train_time:213448ms step_avg:169.81ms\nstep:1268/10000 train_loss:5.1475 train_time:213619ms step_avg:169.81ms\nstep:1269/10000 train_loss:5.0396 train_time:213788ms step_avg:169.81ms\nstep:1270/10000 train_loss:5.1536 train_time:213962ms step_avg:169.81ms\nstep:1271/10000 train_loss:5.1111 train_time:214126ms step_avg:169.81ms\nstep:1272/10000 train_loss:5.0791 train_time:214295ms step_avg:169.81ms\nstep:1273/10000 train_loss:4.9759 train_time:214464ms step_avg:169.81ms\nstep:1274/10000 train_loss:5.0424 train_time:214636ms step_avg:169.81ms\nstep:1275/10000 train_loss:5.0943 train_time:214806ms step_avg:169.81ms\nstep:1276/10000 train_loss:5.0300 train_time:214975ms step_avg:169.81ms\nstep:1277/10000 train_loss:5.2799 train_time:215143ms step_avg:169.81ms\nstep:1278/10000 train_loss:5.1318 train_time:215315ms step_avg:169.81ms\nstep:1279/10000 train_loss:5.2064 train_time:215481ms step_avg:169.80ms\nstep:1280/10000 train_loss:5.0873 train_time:215649ms step_avg:169.80ms\nstep:1281/10000 train_loss:5.2226 train_time:215820ms step_avg:169.80ms\nstep:1282/10000 train_loss:5.0459 train_time:215993ms step_avg:169.81ms\nstep:1283/10000 train_loss:5.1364 train_time:216165ms step_avg:169.81ms\nstep:1284/10000 train_loss:5.1465 train_time:216334ms step_avg:169.81ms\nstep:1285/10000 train_loss:5.0087 train_time:216503ms step_avg:169.81ms\nstep:1286/10000 train_loss:5.1733 train_time:216673ms step_avg:169.81ms\nstep:1287/10000 train_loss:5.2310 train_time:216843ms step_avg:169.81ms\nstep:1288/10000 train_loss:5.0953 train_time:217013ms step_avg:169.81ms\nstep:1289/10000 train_loss:5.1568 train_time:217184ms step_avg:169.81ms\nstep:1290/10000 train_loss:5.0452 train_time:217353ms step_avg:169.81ms\nstep:1291/10000 train_loss:5.1161 train_time:217523ms step_avg:169.81ms\nstep:1292/10000 train_loss:5.1967 train_time:217691ms step_avg:169.81ms\nstep:1293/10000 train_loss:5.1583 train_time:217861ms step_avg:169.81ms\nstep:1294/10000 train_loss:5.1539 train_time:218029ms step_avg:169.80ms\nstep:1295/10000 train_loss:5.1850 train_time:218201ms step_avg:169.81ms\nstep:1296/10000 train_loss:5.2818 train_time:218370ms step_avg:169.81ms\nstep:1297/10000 train_loss:5.0709 train_time:218538ms step_avg:169.80ms\nstep:1298/10000 train_loss:5.1183 train_time:218706ms step_avg:169.80ms\nstep:1299/10000 train_loss:5.1296 train_time:218884ms step_avg:169.81ms\nstep:1300/10000 train_loss:5.2648 train_time:219048ms step_avg:169.80ms\nstep:1301/10000 train_loss:5.2233 train_time:219220ms step_avg:169.81ms\nstep:1302/10000 train_loss:5.0552 train_time:219385ms step_avg:169.80ms\nstep:1303/10000 train_loss:5.2650 train_time:219560ms step_avg:169.81ms\nstep:1304/10000 train_loss:5.1269 train_time:219730ms step_avg:169.81ms\nstep:1305/10000 train_loss:5.1446 train_time:219899ms step_avg:169.81ms\nstep:1306/10000 train_loss:5.1054 train_time:220069ms step_avg:169.81ms\nstep:1307/10000 train_loss:5.2302 train_time:220240ms step_avg:169.81ms\nstep:1308/10000 train_loss:5.5626 train_time:220407ms step_avg:169.81ms\nstep:1309/10000 train_loss:4.9739 train_time:220578ms step_avg:169.81ms\nstep:1310/10000 train_loss:5.1745 train_time:220748ms step_avg:169.81ms\nstep:1311/10000 train_loss:5.2568 train_time:220920ms step_avg:169.81ms\nstep:1312/10000 train_loss:5.2678 train_time:221087ms step_avg:169.81ms\nstep:1313/10000 train_loss:5.2264 train_time:221257ms step_avg:169.81ms\nstep:1314/10000 train_loss:5.2315 train_time:221426ms step_avg:169.81ms\nstep:1315/10000 train_loss:5.1777 train_time:221597ms step_avg:169.81ms\nstep:1316/10000 train_loss:5.1154 train_time:221768ms step_avg:169.81ms\nstep:1317/10000 train_loss:5.1417 train_time:221937ms step_avg:169.81ms\nstep:1318/10000 train_loss:5.0933 train_time:222107ms step_avg:169.81ms\nstep:1319/10000 train_loss:5.3351 train_time:222277ms step_avg:169.81ms\nstep:1320/10000 train_loss:5.1001 train_time:222452ms step_avg:169.81ms\nstep:1321/10000 train_loss:5.1967 train_time:222619ms step_avg:169.81ms\nstep:1322/10000 train_loss:5.0925 train_time:222785ms step_avg:169.81ms\nstep:1323/10000 train_loss:5.0551 train_time:222955ms step_avg:169.81ms\nstep:1324/10000 train_loss:5.1484 train_time:223125ms step_avg:169.81ms\nstep:1325/10000 train_loss:5.0352 train_time:223294ms step_avg:169.81ms\nstep:1326/10000 train_loss:5.6154 train_time:223465ms step_avg:169.81ms\nstep:1327/10000 train_loss:5.0477 train_time:223639ms step_avg:169.81ms\nstep:1328/10000 train_loss:5.0544 train_time:223806ms step_avg:169.81ms\nstep:1329/10000 train_loss:5.0201 train_time:223977ms step_avg:169.81ms\nstep:1330/10000 train_loss:5.0257 train_time:224149ms step_avg:169.81ms\nstep:1331/10000 train_loss:4.9895 train_time:224317ms step_avg:169.81ms\nstep:1332/10000 train_loss:5.0882 train_time:224486ms step_avg:169.81ms\nstep:1333/10000 train_loss:5.0232 train_time:224660ms step_avg:169.81ms\nstep:1334/10000 train_loss:5.0184 train_time:224826ms step_avg:169.81ms\nstep:1335/10000 train_loss:4.9950 train_time:224997ms step_avg:169.81ms\nstep:1336/10000 train_loss:5.0155 train_time:225166ms step_avg:169.81ms\nstep:1337/10000 train_loss:5.0080 train_time:225340ms step_avg:169.81ms\nstep:1338/10000 train_loss:5.1026 train_time:225507ms step_avg:169.81ms\nstep:1339/10000 train_loss:5.1291 train_time:225677ms step_avg:169.81ms\nstep:1340/10000 train_loss:5.2419 train_time:225846ms step_avg:169.81ms\nstep:1341/10000 train_loss:5.1084 train_time:226016ms step_avg:169.81ms\nstep:1342/10000 train_loss:5.1379 train_time:226186ms step_avg:169.81ms\nstep:1343/10000 train_loss:5.0083 train_time:226358ms step_avg:169.81ms\nstep:1344/10000 train_loss:5.1555 train_time:226526ms step_avg:169.81ms\nstep:1345/10000 train_loss:4.9493 train_time:226695ms step_avg:169.81ms\nstep:1346/10000 train_loss:5.0717 train_time:226864ms step_avg:169.81ms\nstep:1347/10000 train_loss:4.9697 train_time:227034ms step_avg:169.81ms\nstep:1348/10000 train_loss:5.6516 train_time:227206ms step_avg:169.81ms\nstep:1349/10000 train_loss:6.2686 train_time:227379ms step_avg:169.81ms\nstep:1350/10000 train_loss:5.1189 train_time:227547ms step_avg:169.81ms\nstep:1351/10000 train_loss:5.2453 train_time:227718ms step_avg:169.81ms\nstep:1352/10000 train_loss:5.1245 train_time:227890ms step_avg:169.81ms\nstep:1353/10000 train_loss:5.1307 train_time:228062ms step_avg:169.82ms\nstep:1354/10000 train_loss:5.0549 train_time:228234ms step_avg:169.82ms\nstep:1355/10000 train_loss:5.2227 train_time:228406ms step_avg:169.82ms\nstep:1356/10000 train_loss:5.1457 train_time:228576ms step_avg:169.82ms\nstep:1357/10000 train_loss:5.0345 train_time:228746ms step_avg:169.82ms\nstep:1358/10000 train_loss:5.0349 train_time:228916ms step_avg:169.82ms\nstep:1359/10000 train_loss:5.0750 train_time:229086ms step_avg:169.82ms\nstep:1360/10000 train_loss:5.0249 train_time:229257ms step_avg:169.82ms\nstep:1361/10000 train_loss:5.0602 train_time:229427ms step_avg:169.82ms\nstep:1362/10000 train_loss:4.9567 train_time:229598ms step_avg:169.82ms\nstep:1363/10000 train_loss:4.9866 train_time:229767ms step_avg:169.82ms\nstep:1364/10000 train_loss:5.0132 train_time:229937ms step_avg:169.82ms\nstep:1365/10000 train_loss:5.1291 train_time:230106ms step_avg:169.82ms\nstep:1366/10000 train_loss:5.0488 train_time:230277ms step_avg:169.82ms\nstep:1367/10000 train_loss:5.0019 train_time:230446ms step_avg:169.82ms\nstep:1368/10000 train_loss:4.9947 train_time:230618ms step_avg:169.82ms\nstep:1369/10000 train_loss:5.0637 train_time:230786ms step_avg:169.82ms\nstep:1370/10000 train_loss:4.9956 train_time:230957ms step_avg:169.82ms\nstep:1371/10000 train_loss:5.0622 train_time:231124ms step_avg:169.82ms\nstep:1372/10000 train_loss:5.0689 train_time:231293ms step_avg:169.82ms\nstep:1373/10000 train_loss:4.9805 train_time:231461ms step_avg:169.82ms\nstep:1374/10000 train_loss:5.1746 train_time:231631ms step_avg:169.82ms\nstep:1375/10000 train_loss:5.0642 train_time:231799ms step_avg:169.82ms\nstep:1376/10000 train_loss:5.1306 train_time:231967ms step_avg:169.81ms\nstep:1377/10000 train_loss:4.8012 train_time:232138ms step_avg:169.82ms\nstep:1378/10000 train_loss:5.0206 train_time:232308ms step_avg:169.82ms\nstep:1379/10000 train_loss:5.1701 train_time:232480ms step_avg:169.82ms\nstep:1380/10000 train_loss:5.0622 train_time:232650ms step_avg:169.82ms\nstep:1381/10000 train_loss:4.9846 train_time:232823ms step_avg:169.82ms\nstep:1382/10000 train_loss:4.9959 train_time:232992ms step_avg:169.82ms\nstep:1383/10000 train_loss:4.9739 train_time:233161ms step_avg:169.82ms\nstep:1384/10000 train_loss:5.0086 train_time:233331ms step_avg:169.82ms\nstep:1385/10000 train_loss:4.9342 train_time:233502ms step_avg:169.82ms\nstep:1386/10000 train_loss:5.0988 train_time:233671ms step_avg:169.82ms\nstep:1387/10000 train_loss:4.9620 train_time:233842ms step_avg:169.82ms\nstep:1388/10000 train_loss:5.0799 train_time:234011ms step_avg:169.82ms\nstep:1389/10000 train_loss:5.0923 train_time:234180ms step_avg:169.82ms\nstep:1390/10000 train_loss:4.9685 train_time:234348ms step_avg:169.82ms\nstep:1391/10000 train_loss:5.0053 train_time:234520ms step_avg:169.82ms\nstep:1392/10000 train_loss:5.0148 train_time:234688ms step_avg:169.82ms\nstep:1393/10000 train_loss:5.0801 train_time:234859ms step_avg:169.82ms\nstep:1394/10000 train_loss:5.0681 train_time:235028ms step_avg:169.82ms\nstep:1395/10000 train_loss:4.9131 train_time:235200ms step_avg:169.82ms\nstep:1396/10000 train_loss:4.9255 train_time:235368ms step_avg:169.82ms\nstep:1397/10000 train_loss:4.7717 train_time:235538ms step_avg:169.82ms\nstep:1398/10000 train_loss:5.0402 train_time:235706ms step_avg:169.82ms\nstep:1399/10000 train_loss:4.9119 train_time:235879ms step_avg:169.82ms\nstep:1400/10000 train_loss:4.5601 train_time:236048ms step_avg:169.82ms\nstep:1401/10000 train_loss:4.9988 train_time:236220ms step_avg:169.82ms\nstep:1402/10000 train_loss:5.1874 train_time:236388ms step_avg:169.82ms\nstep:1403/10000 train_loss:4.9432 train_time:236559ms step_avg:169.82ms\nstep:1404/10000 train_loss:4.8876 train_time:236728ms step_avg:169.82ms\nstep:1405/10000 train_loss:5.0549 train_time:236898ms step_avg:169.82ms\nstep:1406/10000 train_loss:4.9400 train_time:237067ms step_avg:169.82ms\nstep:1407/10000 train_loss:5.0367 train_time:237238ms step_avg:169.82ms\nstep:1408/10000 train_loss:5.2917 train_time:237407ms step_avg:169.82ms\nstep:1409/10000 train_loss:4.9548 train_time:237577ms step_avg:169.82ms\nstep:1410/10000 train_loss:4.9867 train_time:237744ms step_avg:169.82ms\nstep:1411/10000 train_loss:4.9541 train_time:237913ms step_avg:169.82ms\nstep:1412/10000 train_loss:4.8652 train_time:238080ms step_avg:169.81ms\nstep:1413/10000 train_loss:5.0236 train_time:238251ms step_avg:169.82ms\nstep:1414/10000 train_loss:4.9988 train_time:238423ms step_avg:169.82ms\nstep:1415/10000 train_loss:4.9996 train_time:238594ms step_avg:169.82ms\nstep:1416/10000 train_loss:5.0273 train_time:238763ms step_avg:169.82ms\nstep:1417/10000 train_loss:4.8831 train_time:238940ms step_avg:169.82ms\nstep:1418/10000 train_loss:4.9465 train_time:239113ms step_avg:169.82ms\nstep:1419/10000 train_loss:4.8919 train_time:239279ms step_avg:169.82ms\nstep:1420/10000 train_loss:5.0018 train_time:239448ms step_avg:169.82ms\nstep:1421/10000 train_loss:5.0401 train_time:239619ms step_avg:169.82ms\nstep:1422/10000 train_loss:5.0544 train_time:239788ms step_avg:169.82ms\nstep:1423/10000 train_loss:5.0447 train_time:239959ms step_avg:169.82ms\nstep:1424/10000 train_loss:5.0665 train_time:240132ms step_avg:169.82ms\nstep:1425/10000 train_loss:4.9134 train_time:240301ms step_avg:169.82ms\nstep:1426/10000 train_loss:4.9418 train_time:240474ms step_avg:169.83ms\nstep:1427/10000 train_loss:4.8239 train_time:240643ms step_avg:169.83ms\nstep:1428/10000 train_loss:5.1477 train_time:240815ms step_avg:169.83ms\nstep:1429/10000 train_loss:4.9744 train_time:240985ms step_avg:169.83ms\nstep:1430/10000 train_loss:5.0253 train_time:241149ms step_avg:169.82ms\nstep:1431/10000 train_loss:5.2216 train_time:241312ms step_avg:169.82ms\nstep:1432/10000 train_loss:4.9743 train_time:241483ms step_avg:169.82ms\nstep:1433/10000 train_loss:4.8658 train_time:241652ms step_avg:169.82ms\nstep:1434/10000 train_loss:5.0422 train_time:241822ms step_avg:169.82ms\nstep:1435/10000 train_loss:5.1144 train_time:241994ms step_avg:169.82ms\nstep:1436/10000 train_loss:5.2810 train_time:242165ms step_avg:169.82ms\nstep:1437/10000 train_loss:4.9785 train_time:242333ms step_avg:169.82ms\nstep:1438/10000 train_loss:4.9206 train_time:242503ms step_avg:169.82ms\nstep:1439/10000 train_loss:5.1965 train_time:242675ms step_avg:169.82ms\nstep:1440/10000 train_loss:4.7987 train_time:242845ms step_avg:169.82ms\nstep:1441/10000 train_loss:4.9382 train_time:243014ms step_avg:169.82ms\nstep:1442/10000 train_loss:5.2507 train_time:243184ms step_avg:169.82ms\nstep:1443/10000 train_loss:5.0967 train_time:243361ms step_avg:169.83ms\nstep:1444/10000 train_loss:5.1808 train_time:243532ms step_avg:169.83ms\nstep:1445/10000 train_loss:5.0081 train_time:243703ms step_avg:169.83ms\nstep:1446/10000 train_loss:5.1359 train_time:243868ms step_avg:169.82ms\nstep:1447/10000 train_loss:4.8127 train_time:244039ms step_avg:169.83ms\nstep:1448/10000 train_loss:5.1419 train_time:244206ms step_avg:169.82ms\nstep:1449/10000 train_loss:5.0547 train_time:244378ms step_avg:169.82ms\nstep:1450/10000 train_loss:5.0454 train_time:244552ms step_avg:169.83ms\nstep:1451/10000 train_loss:5.0826 train_time:244725ms step_avg:169.83ms\nstep:1452/10000 train_loss:4.9496 train_time:244893ms step_avg:169.83ms\nstep:1453/10000 train_loss:4.9557 train_time:245064ms step_avg:169.83ms\nstep:1454/10000 train_loss:4.9233 train_time:245233ms step_avg:169.83ms\nstep:1455/10000 train_loss:4.9446 train_time:245398ms step_avg:169.83ms\nstep:1456/10000 train_loss:4.9141 train_time:245567ms step_avg:169.82ms\nstep:1457/10000 train_loss:5.6065 train_time:245737ms step_avg:169.83ms\nstep:1458/10000 train_loss:5.1041 train_time:245907ms step_avg:169.82ms\nstep:1459/10000 train_loss:4.9828 train_time:246077ms step_avg:169.83ms\nstep:1460/10000 train_loss:5.0393 train_time:246248ms step_avg:169.83ms\nstep:1461/10000 train_loss:4.9725 train_time:246418ms step_avg:169.83ms\nstep:1462/10000 train_loss:5.0021 train_time:246587ms step_avg:169.83ms\nstep:1463/10000 train_loss:5.0777 train_time:246761ms step_avg:169.83ms\nstep:1464/10000 train_loss:4.9587 train_time:246929ms step_avg:169.83ms\nstep:1465/10000 train_loss:4.8919 train_time:247101ms step_avg:169.83ms\nstep:1466/10000 train_loss:4.9641 train_time:247271ms step_avg:169.83ms\nstep:1467/10000 train_loss:4.9852 train_time:247439ms step_avg:169.83ms\nstep:1468/10000 train_loss:4.9497 train_time:247608ms step_avg:169.83ms\nstep:1469/10000 train_loss:4.9330 train_time:247778ms step_avg:169.83ms\nstep:1470/10000 train_loss:4.9479 train_time:247946ms step_avg:169.83ms\nstep:1471/10000 train_loss:4.9546 train_time:248117ms step_avg:169.83ms\nstep:1472/10000 train_loss:4.9509 train_time:248285ms step_avg:169.83ms\nstep:1473/10000 train_loss:4.9144 train_time:248456ms step_avg:169.83ms\nstep:1474/10000 train_loss:5.0250 train_time:248625ms step_avg:169.83ms\nstep:1475/10000 train_loss:4.9523 train_time:248797ms step_avg:169.83ms\nstep:1476/10000 train_loss:4.8219 train_time:248966ms step_avg:169.83ms\nstep:1477/10000 train_loss:4.9792 train_time:249139ms step_avg:169.83ms\nstep:1478/10000 train_loss:4.9235 train_time:249308ms step_avg:169.83ms\nstep:1479/10000 train_loss:4.8774 train_time:249481ms step_avg:169.83ms\nstep:1480/10000 train_loss:4.9728 train_time:249649ms step_avg:169.83ms\nstep:1481/10000 train_loss:4.9349 train_time:249818ms step_avg:169.83ms\nstep:1482/10000 train_loss:4.7965 train_time:249986ms step_avg:169.83ms\nstep:1483/10000 train_loss:5.0469 train_time:250157ms step_avg:169.83ms\nstep:1484/10000 train_loss:5.0183 train_time:250327ms step_avg:169.83ms\nstep:1485/10000 train_loss:5.0337 train_time:250499ms step_avg:169.83ms\nstep:1486/10000 train_loss:4.9226 train_time:250668ms step_avg:169.83ms\nstep:1487/10000 train_loss:4.9392 train_time:250840ms step_avg:169.83ms\nstep:1488/10000 train_loss:4.7944 train_time:251008ms step_avg:169.83ms\nstep:1489/10000 train_loss:4.8846 train_time:251181ms step_avg:169.83ms\nstep:1490/10000 train_loss:4.9211 train_time:251348ms step_avg:169.83ms\nstep:1491/10000 train_loss:5.1246 train_time:251517ms step_avg:169.83ms\nstep:1492/10000 train_loss:4.9699 train_time:251687ms step_avg:169.83ms\nstep:1493/10000 train_loss:4.9260 train_time:251859ms step_avg:169.83ms\nstep:1494/10000 train_loss:4.9935 train_time:252027ms step_avg:169.83ms\nstep:1495/10000 train_loss:4.8710 train_time:252199ms step_avg:169.83ms\nstep:1496/10000 train_loss:4.9237 train_time:252369ms step_avg:169.83ms\nstep:1497/10000 train_loss:5.0166 train_time:252540ms step_avg:169.83ms\nstep:1498/10000 train_loss:4.9320 train_time:252714ms step_avg:169.83ms\nstep:1499/10000 train_loss:4.8254 train_time:252881ms step_avg:169.83ms\nstep:1500/10000 train_loss:4.9273 train_time:253051ms step_avg:169.83ms\nstep:1501/10000 train_loss:4.8863 train_time:253221ms step_avg:169.83ms\nstep:1502/10000 train_loss:4.9738 train_time:253388ms step_avg:169.83ms\nstep:1503/10000 train_loss:4.9462 train_time:253560ms step_avg:169.83ms\nstep:1504/10000 train_loss:4.9741 train_time:253731ms step_avg:169.83ms\nstep:1505/10000 train_loss:5.0075 train_time:253904ms step_avg:169.84ms\nstep:1506/10000 train_loss:4.8685 train_time:254073ms step_avg:169.83ms\nstep:1507/10000 train_loss:4.9267 train_time:254242ms step_avg:169.83ms\nstep:1508/10000 train_loss:4.8507 train_time:254412ms step_avg:169.83ms\nstep:1509/10000 train_loss:4.8394 train_time:254582ms step_avg:169.83ms\nstep:1510/10000 train_loss:4.2224 train_time:254752ms step_avg:169.83ms\nstep:1511/10000 train_loss:4.8767 train_time:254923ms step_avg:169.84ms\nstep:1512/10000 train_loss:4.9358 train_time:255093ms step_avg:169.84ms\nstep:1513/10000 train_loss:5.0915 train_time:255264ms step_avg:169.84ms\nstep:1514/10000 train_loss:4.7616 train_time:255435ms step_avg:169.84ms\nstep:1515/10000 train_loss:4.7676 train_time:255605ms step_avg:169.84ms\nstep:1516/10000 train_loss:5.0447 train_time:255777ms step_avg:169.84ms\nstep:1517/10000 train_loss:5.0007 train_time:255946ms step_avg:169.84ms\nstep:1518/10000 train_loss:4.8344 train_time:256117ms step_avg:169.84ms\nstep:1519/10000 train_loss:5.1314 train_time:256286ms step_avg:169.84ms\nstep:1520/10000 train_loss:5.0587 train_time:256458ms step_avg:169.84ms\nstep:1521/10000 train_loss:4.8990 train_time:256628ms step_avg:169.84ms\nstep:1522/10000 train_loss:4.9389 train_time:256799ms step_avg:169.84ms\nstep:1523/10000 train_loss:5.0246 train_time:256966ms step_avg:169.84ms\nstep:1524/10000 train_loss:4.8314 train_time:257362ms step_avg:169.99ms\nstep:1525/10000 train_loss:4.8576 train_time:257526ms step_avg:169.98ms\nstep:1526/10000 train_loss:5.0175 train_time:257694ms step_avg:169.98ms\nstep:1527/10000 train_loss:5.0618 train_time:257861ms step_avg:169.98ms\nstep:1528/10000 train_loss:4.9388 train_time:258030ms step_avg:169.98ms\nstep:1529/10000 train_loss:5.1590 train_time:258199ms step_avg:169.98ms\nstep:1530/10000 train_loss:5.0361 train_time:258379ms step_avg:169.99ms\nstep:1531/10000 train_loss:4.9363 train_time:258549ms step_avg:169.99ms\nstep:1532/10000 train_loss:4.8617 train_time:258721ms step_avg:169.99ms\nstep:1533/10000 train_loss:4.9485 train_time:258889ms step_avg:169.99ms\nstep:1534/10000 train_loss:4.8285 train_time:259057ms step_avg:169.98ms\nstep:1535/10000 train_loss:4.7905 train_time:259224ms step_avg:169.98ms\nstep:1536/10000 train_loss:4.9423 train_time:259398ms step_avg:169.99ms\nstep:1537/10000 train_loss:5.0059 train_time:259567ms step_avg:169.98ms\nstep:1538/10000 train_loss:4.9233 train_time:259735ms step_avg:169.98ms\nstep:1539/10000 train_loss:4.9008 train_time:259902ms step_avg:169.98ms\nstep:1540/10000 train_loss:4.7904 train_time:260071ms step_avg:169.98ms\nstep:1541/10000 train_loss:5.0161 train_time:260238ms step_avg:169.98ms\nstep:1542/10000 train_loss:4.8704 train_time:260410ms step_avg:169.98ms\nstep:1543/10000 train_loss:4.9399 train_time:260582ms step_avg:169.98ms\nstep:1544/10000 train_loss:4.9581 train_time:260752ms step_avg:169.98ms\nstep:1545/10000 train_loss:5.0103 train_time:260921ms step_avg:169.98ms\nstep:1546/10000 train_loss:4.9939 train_time:261089ms step_avg:169.98ms\nstep:1547/10000 train_loss:4.9064 train_time:261261ms step_avg:169.98ms\nstep:1548/10000 train_loss:4.8268 train_time:261431ms step_avg:169.98ms\nstep:1549/10000 train_loss:4.9349 train_time:261603ms step_avg:169.98ms\nstep:1550/10000 train_loss:5.2177 train_time:261772ms step_avg:169.98ms\nstep:1551/10000 train_loss:4.9045 train_time:261943ms step_avg:169.98ms\nstep:1552/10000 train_loss:4.9546 train_time:262112ms step_avg:169.98ms\nstep:1553/10000 train_loss:5.0109 train_time:262280ms step_avg:169.98ms\nstep:1554/10000 train_loss:4.8886 train_time:262447ms step_avg:169.98ms\nstep:1555/10000 train_loss:4.8199 train_time:262621ms step_avg:169.98ms\nstep:1556/10000 train_loss:4.8959 train_time:262792ms step_avg:169.98ms\nstep:1557/10000 train_loss:4.8949 train_time:262962ms step_avg:169.98ms\nstep:1558/10000 train_loss:4.7724 train_time:263131ms step_avg:169.98ms\nstep:1559/10000 train_loss:4.9241 train_time:263302ms step_avg:169.98ms\nstep:1560/10000 train_loss:4.8960 train_time:263470ms step_avg:169.98ms\nstep:1561/10000 train_loss:4.7384 train_time:263642ms step_avg:169.98ms\nstep:1562/10000 train_loss:4.9658 train_time:263813ms step_avg:169.98ms\nstep:1563/10000 train_loss:4.9719 train_time:263985ms step_avg:169.98ms\nstep:1564/10000 train_loss:4.8467 train_time:264152ms step_avg:169.98ms\nstep:1565/10000 train_loss:4.8840 train_time:264319ms step_avg:169.98ms\nstep:1566/10000 train_loss:4.8739 train_time:264490ms step_avg:169.98ms\nstep:1567/10000 train_loss:4.9200 train_time:264661ms step_avg:169.98ms\nstep:1568/10000 train_loss:4.9376 train_time:264829ms step_avg:169.98ms\nstep:1569/10000 train_loss:4.8693 train_time:264999ms step_avg:169.98ms\nstep:1570/10000 train_loss:4.9551 train_time:265168ms step_avg:169.98ms\nstep:1571/10000 train_loss:5.0561 train_time:265338ms step_avg:169.98ms\nstep:1572/10000 train_loss:4.8038 train_time:265508ms step_avg:169.98ms\nstep:1573/10000 train_loss:4.8958 train_time:265685ms step_avg:169.98ms\nstep:1574/10000 train_loss:4.7990 train_time:265854ms step_avg:169.98ms\nstep:1575/10000 train_loss:4.9709 train_time:266028ms step_avg:169.99ms\nstep:1576/10000 train_loss:4.8797 train_time:266191ms step_avg:169.98ms\nstep:1577/10000 train_loss:4.9286 train_time:266361ms step_avg:169.98ms\nstep:1578/10000 train_loss:4.9078 train_time:266530ms step_avg:169.98ms\nstep:1579/10000 train_loss:4.9528 train_time:266701ms step_avg:169.98ms\nstep:1580/10000 train_loss:4.8234 train_time:266869ms step_avg:169.98ms\nstep:1581/10000 train_loss:4.9594 train_time:267040ms step_avg:169.98ms\nstep:1582/10000 train_loss:4.7402 train_time:267209ms step_avg:169.98ms\nstep:1583/10000 train_loss:4.8640 train_time:267379ms step_avg:169.98ms\nstep:1584/10000 train_loss:4.9052 train_time:267549ms step_avg:169.98ms\nstep:1585/10000 train_loss:4.8255 train_time:267721ms step_avg:169.98ms\nstep:1586/10000 train_loss:4.8391 train_time:267891ms step_avg:169.98ms\nstep:1587/10000 train_loss:4.7152 train_time:268063ms step_avg:169.98ms\nstep:1588/10000 train_loss:4.6499 train_time:268233ms step_avg:169.98ms\nstep:1589/10000 train_loss:4.7669 train_time:268403ms step_avg:169.98ms\nstep:1590/10000 train_loss:5.0421 train_time:268573ms step_avg:169.98ms\nstep:1591/10000 train_loss:5.0993 train_time:268744ms step_avg:169.98ms\nstep:1592/10000 train_loss:4.8197 train_time:268915ms step_avg:169.98ms\nstep:1593/10000 train_loss:5.0377 train_time:269085ms step_avg:169.98ms\nstep:1594/10000 train_loss:4.9421 train_time:269254ms step_avg:169.98ms\nstep:1595/10000 train_loss:4.8275 train_time:269423ms step_avg:169.98ms\nstep:1596/10000 train_loss:4.8428 train_time:269593ms step_avg:169.98ms\nstep:1597/10000 train_loss:4.9493 train_time:269765ms step_avg:169.98ms\nstep:1598/10000 train_loss:4.8584 train_time:269933ms step_avg:169.98ms\nstep:1599/10000 train_loss:4.7963 train_time:270106ms step_avg:169.98ms\nstep:1600/10000 train_loss:4.8404 train_time:270275ms step_avg:169.98ms\nstep:1601/10000 train_loss:4.8598 train_time:270443ms step_avg:169.98ms\nstep:1602/10000 train_loss:4.9786 train_time:270612ms step_avg:169.98ms\nstep:1603/10000 train_loss:4.9358 train_time:270782ms step_avg:169.98ms\nstep:1604/10000 train_loss:4.9391 train_time:270953ms step_avg:169.98ms\nstep:1605/10000 train_loss:4.8303 train_time:271126ms step_avg:169.98ms\nstep:1606/10000 train_loss:4.9298 train_time:271295ms step_avg:169.98ms\nstep:1607/10000 train_loss:4.9366 train_time:271466ms step_avg:169.98ms\nstep:1608/10000 train_loss:4.8344 train_time:271637ms step_avg:169.99ms\nstep:1609/10000 train_loss:4.8349 train_time:271804ms step_avg:169.98ms\nstep:1610/10000 train_loss:4.9079 train_time:271973ms step_avg:169.98ms\nstep:1611/10000 train_loss:4.8866 train_time:272144ms step_avg:169.98ms\nstep:1612/10000 train_loss:5.1096 train_time:272313ms step_avg:169.98ms\nstep:1613/10000 train_loss:4.8774 train_time:272482ms step_avg:169.98ms\nstep:1614/10000 train_loss:5.0034 train_time:272653ms step_avg:169.98ms\nstep:1615/10000 train_loss:4.7746 train_time:272824ms step_avg:169.98ms\nstep:1616/10000 train_loss:5.0660 train_time:272995ms step_avg:169.98ms\nstep:1617/10000 train_loss:4.9019 train_time:273161ms step_avg:169.98ms\nstep:1618/10000 train_loss:4.8623 train_time:273330ms step_avg:169.98ms\nstep:1619/10000 train_loss:4.8300 train_time:273500ms step_avg:169.98ms\nstep:1620/10000 train_loss:4.9145 train_time:273669ms step_avg:169.98ms\nstep:1621/10000 train_loss:4.8768 train_time:273840ms step_avg:169.98ms\nstep:1622/10000 train_loss:4.8215 train_time:274008ms step_avg:169.98ms\nstep:1623/10000 train_loss:4.8532 train_time:274180ms step_avg:169.98ms\nstep:1624/10000 train_loss:4.8981 train_time:274349ms step_avg:169.98ms\nstep:1625/10000 train_loss:5.2298 train_time:274518ms step_avg:169.98ms\nstep:1626/10000 train_loss:5.0994 train_time:274686ms step_avg:169.98ms\nstep:1627/10000 train_loss:4.7916 train_time:274859ms step_avg:169.98ms\nstep:1628/10000 train_loss:4.8221 train_time:275027ms step_avg:169.98ms\nstep:1629/10000 train_loss:4.9439 train_time:275199ms step_avg:169.98ms\nstep:1630/10000 train_loss:4.8629 train_time:275367ms step_avg:169.98ms\nstep:1631/10000 train_loss:4.8932 train_time:275536ms step_avg:169.98ms\nstep:1632/10000 train_loss:4.7502 train_time:275702ms step_avg:169.98ms\nstep:1633/10000 train_loss:5.0607 train_time:275871ms step_avg:169.98ms\nstep:1634/10000 train_loss:4.8599 train_time:276042ms step_avg:169.98ms\nstep:1635/10000 train_loss:4.8576 train_time:276212ms step_avg:169.98ms\nstep:1636/10000 train_loss:4.8991 train_time:276384ms step_avg:169.98ms\nstep:1637/10000 train_loss:5.0026 train_time:276553ms step_avg:169.98ms\nstep:1638/10000 train_loss:4.8803 train_time:276725ms step_avg:169.98ms\nstep:1639/10000 train_loss:4.8010 train_time:276895ms step_avg:169.98ms\nstep:1640/10000 train_loss:4.7666 train_time:277066ms step_avg:169.98ms\nstep:1641/10000 train_loss:4.7752 train_time:277237ms step_avg:169.98ms\nstep:1642/10000 train_loss:4.7794 train_time:277407ms step_avg:169.98ms\nstep:1643/10000 train_loss:4.8601 train_time:277579ms step_avg:169.98ms\nstep:1644/10000 train_loss:4.8522 train_time:277747ms step_avg:169.98ms\nstep:1645/10000 train_loss:4.7964 train_time:277916ms step_avg:169.98ms\nstep:1646/10000 train_loss:4.8136 train_time:278085ms step_avg:169.98ms\nstep:1647/10000 train_loss:4.8066 train_time:278258ms step_avg:169.98ms\nstep:1648/10000 train_loss:4.8056 train_time:278428ms step_avg:169.98ms\nstep:1649/10000 train_loss:4.9121 train_time:278599ms step_avg:169.98ms\nstep:1650/10000 train_loss:5.2136 train_time:278768ms step_avg:169.98ms\nstep:1651/10000 train_loss:4.8535 train_time:278937ms step_avg:169.98ms\nstep:1652/10000 train_loss:4.8195 train_time:279107ms step_avg:169.98ms\nstep:1653/10000 train_loss:4.9602 train_time:279277ms step_avg:169.98ms\nstep:1654/10000 train_loss:4.6854 train_time:279447ms step_avg:169.98ms\nstep:1655/10000 train_loss:4.7625 train_time:279625ms step_avg:169.98ms\nstep:1656/10000 train_loss:4.8485 train_time:279788ms step_avg:169.98ms\nstep:1657/10000 train_loss:4.8887 train_time:279957ms step_avg:169.98ms\nstep:1658/10000 train_loss:5.0119 train_time:280125ms step_avg:169.98ms\nstep:1659/10000 train_loss:4.9481 train_time:280297ms step_avg:169.98ms\nstep:1660/10000 train_loss:4.8030 train_time:280465ms step_avg:169.98ms\nstep:1661/10000 train_loss:4.8300 train_time:280636ms step_avg:169.98ms\nstep:1662/10000 train_loss:4.7786 train_time:280802ms step_avg:169.98ms\nstep:1663/10000 train_loss:4.9080 train_time:280971ms step_avg:169.98ms\nstep:1664/10000 train_loss:4.7615 train_time:281143ms step_avg:169.98ms\nstep:1665/10000 train_loss:5.0013 train_time:281315ms step_avg:169.98ms\nstep:1666/10000 train_loss:4.7984 train_time:281484ms step_avg:169.98ms\nstep:1667/10000 train_loss:4.4962 train_time:281654ms step_avg:169.98ms\nstep:1668/10000 train_loss:4.8912 train_time:281823ms step_avg:169.98ms\nstep:1669/10000 train_loss:4.8803 train_time:281992ms step_avg:169.98ms\nstep:1670/10000 train_loss:4.9235 train_time:282163ms step_avg:169.98ms\nstep:1671/10000 train_loss:4.8107 train_time:282333ms step_avg:169.98ms\nstep:1672/10000 train_loss:4.6338 train_time:282504ms step_avg:169.98ms\nstep:1673/10000 train_loss:4.7703 train_time:282672ms step_avg:169.98ms\nstep:1674/10000 train_loss:4.7023 train_time:282842ms step_avg:169.98ms\nstep:1675/10000 train_loss:4.8867 train_time:283009ms step_avg:169.98ms\nstep:1676/10000 train_loss:4.9608 train_time:283178ms step_avg:169.97ms\nstep:1677/10000 train_loss:4.5559 train_time:283348ms step_avg:169.97ms\nstep:1678/10000 train_loss:4.7767 train_time:283519ms step_avg:169.98ms\nstep:1679/10000 train_loss:4.7674 train_time:283691ms step_avg:169.98ms\nstep:1680/10000 train_loss:4.9704 train_time:283865ms step_avg:169.98ms\nstep:1681/10000 train_loss:4.6792 train_time:284032ms step_avg:169.98ms\nstep:1682/10000 train_loss:4.8124 train_time:284197ms step_avg:169.97ms\nstep:1683/10000 train_loss:4.8512 train_time:284366ms step_avg:169.97ms\nstep:1684/10000 train_loss:4.8248 train_time:284538ms step_avg:169.97ms\nstep:1685/10000 train_loss:4.8350 train_time:284706ms step_avg:169.97ms\nstep:1686/10000 train_loss:4.6434 train_time:284877ms step_avg:169.97ms\nstep:1687/10000 train_loss:4.7193 train_time:285044ms step_avg:169.97ms\nstep:1688/10000 train_loss:4.8814 train_time:285214ms step_avg:169.97ms\nstep:1689/10000 train_loss:4.7823 train_time:285385ms step_avg:169.97ms\nstep:1690/10000 train_loss:4.7045 train_time:285557ms step_avg:169.97ms\nstep:1691/10000 train_loss:4.7336 train_time:285726ms step_avg:169.97ms\nstep:1692/10000 train_loss:4.6612 train_time:285898ms step_avg:169.97ms\nstep:1693/10000 train_loss:4.7424 train_time:286068ms step_avg:169.97ms\nstep:1694/10000 train_loss:4.8434 train_time:286239ms step_avg:169.98ms\nstep:1695/10000 train_loss:4.7792 train_time:286408ms step_avg:169.98ms\nstep:1696/10000 train_loss:4.7024 train_time:286580ms step_avg:169.98ms\nstep:1697/10000 train_loss:4.8271 train_time:286748ms step_avg:169.98ms\nstep:1698/10000 train_loss:4.7165 train_time:286918ms step_avg:169.98ms\nstep:1699/10000 train_loss:4.8546 train_time:287086ms step_avg:169.97ms\nstep:1700/10000 train_loss:4.6663 train_time:287258ms step_avg:169.97ms\nstep:1701/10000 train_loss:4.8201 train_time:287426ms step_avg:169.97ms\nstep:1702/10000 train_loss:4.8455 train_time:287598ms step_avg:169.98ms\nstep:1703/10000 train_loss:4.7219 train_time:287767ms step_avg:169.97ms\nstep:1704/10000 train_loss:4.8622 train_time:287938ms step_avg:169.98ms\nstep:1705/10000 train_loss:4.7356 train_time:288107ms step_avg:169.97ms\nstep:1706/10000 train_loss:4.7263 train_time:288277ms step_avg:169.97ms\nstep:1707/10000 train_loss:4.8708 train_time:288444ms step_avg:169.97ms\nstep:1708/10000 train_loss:4.8345 train_time:288614ms step_avg:169.97ms\nstep:1709/10000 train_loss:4.6870 train_time:288784ms step_avg:169.97ms\nstep:1710/10000 train_loss:4.8733 train_time:288959ms step_avg:169.98ms\nstep:1711/10000 train_loss:4.8166 train_time:289127ms step_avg:169.97ms\nstep:1712/10000 train_loss:4.7020 train_time:289297ms step_avg:169.97ms\nstep:1713/10000 train_loss:4.7563 train_time:289467ms step_avg:169.97ms\nstep:1714/10000 train_loss:4.8458 train_time:289638ms step_avg:169.98ms\nstep:1715/10000 train_loss:4.7889 train_time:289807ms step_avg:169.97ms\nstep:1716/10000 train_loss:4.8072 train_time:289977ms step_avg:169.97ms\nstep:1717/10000 train_loss:4.6705 train_time:290145ms step_avg:169.97ms\nstep:1718/10000 train_loss:4.7306 train_time:290315ms step_avg:169.97ms\nstep:1719/10000 train_loss:4.6587 train_time:290484ms step_avg:169.97ms\nstep:1720/10000 train_loss:4.7109 train_time:290655ms step_avg:169.97ms\nstep:1721/10000 train_loss:4.7204 train_time:290825ms step_avg:169.97ms\nstep:1722/10000 train_loss:4.7184 train_time:290995ms step_avg:169.97ms\nstep:1723/10000 train_loss:4.8800 train_time:291162ms step_avg:169.97ms\nstep:1724/10000 train_loss:4.6274 train_time:291331ms step_avg:169.97ms\nstep:1725/10000 train_loss:4.6583 train_time:291500ms step_avg:169.97ms\nstep:1726/10000 train_loss:4.7440 train_time:291668ms step_avg:169.97ms\nstep:1727/10000 train_loss:5.0331 train_time:291837ms step_avg:169.97ms\nstep:1728/10000 train_loss:4.8305 train_time:292006ms step_avg:169.97ms\nstep:1729/10000 train_loss:5.0227 train_time:292176ms step_avg:169.97ms\nstep:1730/10000 train_loss:4.7419 train_time:292344ms step_avg:169.97ms\nstep:1731/10000 train_loss:4.7550 train_time:292517ms step_avg:169.97ms\nstep:1732/10000 train_loss:4.8615 train_time:292685ms step_avg:169.97ms\nstep:1733/10000 train_loss:4.6411 train_time:292856ms step_avg:169.97ms\nstep:1734/10000 train_loss:4.8249 train_time:293024ms step_avg:169.97ms\nstep:1735/10000 train_loss:4.7386 train_time:293194ms step_avg:169.97ms\nstep:1736/10000 train_loss:4.7975 train_time:293363ms step_avg:169.97ms\nstep:1737/10000 train_loss:4.7926 train_time:293533ms step_avg:169.97ms\nstep:1738/10000 train_loss:4.7716 train_time:293703ms step_avg:169.97ms\nstep:1739/10000 train_loss:4.8082 train_time:293875ms step_avg:169.97ms\nstep:1740/10000 train_loss:4.7116 train_time:294044ms step_avg:169.97ms\nstep:1741/10000 train_loss:4.7942 train_time:294213ms step_avg:169.97ms\nstep:1742/10000 train_loss:4.7751 train_time:294383ms step_avg:169.97ms\nstep:1743/10000 train_loss:4.8538 train_time:294553ms step_avg:169.97ms\nstep:1744/10000 train_loss:4.8147 train_time:294723ms step_avg:169.97ms\nstep:1745/10000 train_loss:4.7343 train_time:294895ms step_avg:169.97ms\nstep:1746/10000 train_loss:4.7999 train_time:295065ms step_avg:169.97ms\nstep:1747/10000 train_loss:4.8573 train_time:295235ms step_avg:169.97ms\nstep:1748/10000 train_loss:4.8606 train_time:295404ms step_avg:169.97ms\nstep:1749/10000 train_loss:4.7699 train_time:295572ms step_avg:169.97ms\nstep:1750/10000 train_loss:4.7248 train_time:295743ms step_avg:169.97ms\nstep:1751/10000 train_loss:5.1269 train_time:295913ms step_avg:169.97ms\nstep:1752/10000 train_loss:4.7830 train_time:296084ms step_avg:169.97ms\nstep:1753/10000 train_loss:4.8416 train_time:296257ms step_avg:169.97ms\nstep:1754/10000 train_loss:4.7045 train_time:296425ms step_avg:169.97ms\nstep:1755/10000 train_loss:4.7453 train_time:296594ms step_avg:169.97ms\nstep:1756/10000 train_loss:4.8198 train_time:296771ms step_avg:169.97ms\nstep:1757/10000 train_loss:4.7871 train_time:296938ms step_avg:169.97ms\nstep:1758/10000 train_loss:4.6390 train_time:297107ms step_avg:169.97ms\nstep:1759/10000 train_loss:4.8085 train_time:297276ms step_avg:169.97ms\nstep:1760/10000 train_loss:4.6826 train_time:297445ms step_avg:169.97ms\nstep:1761/10000 train_loss:4.6894 train_time:297614ms step_avg:169.97ms\nstep:1762/10000 train_loss:4.8456 train_time:297785ms step_avg:169.97ms\nstep:1763/10000 train_loss:4.7578 train_time:297956ms step_avg:169.97ms\nstep:1764/10000 train_loss:4.7026 train_time:298125ms step_avg:169.97ms\nstep:1765/10000 train_loss:4.4966 train_time:298294ms step_avg:169.97ms\nstep:1766/10000 train_loss:4.7684 train_time:298463ms step_avg:169.97ms\nstep:1767/10000 train_loss:4.6633 train_time:298632ms step_avg:169.97ms\nstep:1768/10000 train_loss:4.7544 train_time:298803ms step_avg:169.97ms\nstep:1769/10000 train_loss:4.7357 train_time:298972ms step_avg:169.97ms\nstep:1770/10000 train_loss:4.7361 train_time:299142ms step_avg:169.97ms\nstep:1771/10000 train_loss:4.6881 train_time:299311ms step_avg:169.97ms\nstep:1772/10000 train_loss:4.5965 train_time:299479ms step_avg:169.97ms\nstep:1773/10000 train_loss:4.7815 train_time:299647ms step_avg:169.96ms\nstep:1774/10000 train_loss:4.6248 train_time:299819ms step_avg:169.97ms\nstep:1775/10000 train_loss:4.6544 train_time:299990ms step_avg:169.97ms\nstep:1776/10000 train_loss:4.7864 train_time:300163ms step_avg:169.97ms\nstep:1777/10000 train_loss:4.7092 train_time:300332ms step_avg:169.97ms\nstep:1778/10000 train_loss:4.8727 train_time:300501ms step_avg:169.97ms\nstep:1779/10000 train_loss:4.7933 train_time:300669ms step_avg:169.97ms\nstep:1780/10000 train_loss:4.9086 train_time:300839ms step_avg:169.97ms\nstep:1781/10000 train_loss:4.7349 train_time:301008ms step_avg:169.96ms\nstep:1782/10000 train_loss:4.7674 train_time:301178ms step_avg:169.97ms\nstep:1783/10000 train_loss:4.8838 train_time:301347ms step_avg:169.96ms\nstep:1784/10000 train_loss:4.6157 train_time:301517ms step_avg:169.96ms\nstep:1785/10000 train_loss:4.7045 train_time:301684ms step_avg:169.96ms\nstep:1786/10000 train_loss:4.7692 train_time:301854ms step_avg:169.96ms\nstep:1787/10000 train_loss:4.7359 train_time:302024ms step_avg:169.96ms\nstep:1788/10000 train_loss:4.7733 train_time:302195ms step_avg:169.96ms\nstep:1789/10000 train_loss:4.8430 train_time:302365ms step_avg:169.96ms\nstep:1790/10000 train_loss:4.7077 train_time:302536ms step_avg:169.96ms\nstep:1791/10000 train_loss:4.6596 train_time:302705ms step_avg:169.96ms\nstep:1792/10000 train_loss:4.7587 train_time:302875ms step_avg:169.96ms\nstep:1793/10000 train_loss:4.8301 train_time:303044ms step_avg:169.96ms\nstep:1794/10000 train_loss:4.7671 train_time:303215ms step_avg:169.96ms\nstep:1795/10000 train_loss:4.9592 train_time:303386ms step_avg:169.96ms\nstep:1796/10000 train_loss:4.6398 train_time:303556ms step_avg:169.96ms\nstep:1797/10000 train_loss:4.8369 train_time:303726ms step_avg:169.96ms\nstep:1798/10000 train_loss:4.6119 train_time:303896ms step_avg:169.96ms\nstep:1799/10000 train_loss:4.7959 train_time:304064ms step_avg:169.96ms\nstep:1800/10000 train_loss:4.6597 train_time:304235ms step_avg:169.96ms\nstep:1801/10000 train_loss:4.8748 train_time:304406ms step_avg:169.96ms\nstep:1802/10000 train_loss:4.6190 train_time:304577ms step_avg:169.96ms\nstep:1803/10000 train_loss:4.8048 train_time:304746ms step_avg:169.96ms\nstep:1804/10000 train_loss:4.7387 train_time:304917ms step_avg:169.96ms\nstep:1805/10000 train_loss:4.7208 train_time:305085ms step_avg:169.96ms\nstep:1806/10000 train_loss:4.5523 train_time:305254ms step_avg:169.96ms\nstep:1807/10000 train_loss:4.7465 train_time:305423ms step_avg:169.96ms\nstep:1808/10000 train_loss:5.0213 train_time:305592ms step_avg:169.96ms\nstep:1809/10000 train_loss:4.8092 train_time:305762ms step_avg:169.96ms\nstep:1810/10000 train_loss:4.6679 train_time:305934ms step_avg:169.96ms\nstep:1811/10000 train_loss:4.7300 train_time:306102ms step_avg:169.96ms\nstep:1812/10000 train_loss:4.8998 train_time:306271ms step_avg:169.96ms\nstep:1813/10000 train_loss:4.7641 train_time:306441ms step_avg:169.96ms\nstep:1814/10000 train_loss:4.6126 train_time:306611ms step_avg:169.96ms\nstep:1815/10000 train_loss:4.7079 train_time:306783ms step_avg:169.96ms\nstep:1816/10000 train_loss:4.6529 train_time:306951ms step_avg:169.96ms\nstep:1817/10000 train_loss:4.9361 train_time:307122ms step_avg:169.96ms\nstep:1818/10000 train_loss:4.6440 train_time:307291ms step_avg:169.96ms\nstep:1819/10000 train_loss:4.4998 train_time:307460ms step_avg:169.96ms\nstep:1820/10000 train_loss:4.6374 train_time:307629ms step_avg:169.96ms\nstep:1821/10000 train_loss:4.8674 train_time:307800ms step_avg:169.96ms\nstep:1822/10000 train_loss:4.8089 train_time:307970ms step_avg:169.96ms\nstep:1823/10000 train_loss:4.5864 train_time:308141ms step_avg:169.96ms\nstep:1824/10000 train_loss:4.7709 train_time:308309ms step_avg:169.96ms\nstep:1825/10000 train_loss:4.8133 train_time:308478ms step_avg:169.96ms\nstep:1826/10000 train_loss:4.6466 train_time:308647ms step_avg:169.96ms\nstep:1827/10000 train_loss:4.6855 train_time:308816ms step_avg:169.96ms\nstep:1828/10000 train_loss:4.7622 train_time:308985ms step_avg:169.96ms\nstep:1829/10000 train_loss:4.7388 train_time:309158ms step_avg:169.96ms\nstep:1830/10000 train_loss:4.7667 train_time:309325ms step_avg:169.96ms\nstep:1831/10000 train_loss:4.5977 train_time:309497ms step_avg:169.96ms\nstep:1832/10000 train_loss:4.7576 train_time:309666ms step_avg:169.96ms\nstep:1833/10000 train_loss:4.4744 train_time:309837ms step_avg:169.96ms\nstep:1834/10000 train_loss:4.7676 train_time:310007ms step_avg:169.96ms\nstep:1835/10000 train_loss:4.6532 train_time:310178ms step_avg:169.96ms\nstep:1836/10000 train_loss:4.7812 train_time:310345ms step_avg:169.96ms\nstep:1837/10000 train_loss:4.6796 train_time:310513ms step_avg:169.96ms\nstep:1838/10000 train_loss:4.5532 train_time:310682ms step_avg:169.96ms\nstep:1839/10000 train_loss:4.5704 train_time:310851ms step_avg:169.96ms\nstep:1840/10000 train_loss:4.6872 train_time:311022ms step_avg:169.96ms\nstep:1841/10000 train_loss:4.6691 train_time:311194ms step_avg:169.96ms\nstep:1842/10000 train_loss:4.7127 train_time:311366ms step_avg:169.96ms\nstep:1843/10000 train_loss:4.7408 train_time:311535ms step_avg:169.96ms\nstep:1844/10000 train_loss:4.7425 train_time:311705ms step_avg:169.96ms\nstep:1845/10000 train_loss:4.6848 train_time:311876ms step_avg:169.96ms\nstep:1846/10000 train_loss:4.6817 train_time:312046ms step_avg:169.96ms\nstep:1847/10000 train_loss:4.7752 train_time:312218ms step_avg:169.96ms\nstep:1848/10000 train_loss:4.6505 train_time:312387ms step_avg:169.96ms\nstep:1849/10000 train_loss:4.6569 train_time:312556ms step_avg:169.96ms\nstep:1850/10000 train_loss:4.5888 train_time:312725ms step_avg:169.96ms\nstep:1851/10000 train_loss:4.7007 train_time:312895ms step_avg:169.96ms\nstep:1852/10000 train_loss:4.4833 train_time:313065ms step_avg:169.96ms\nstep:1853/10000 train_loss:4.5470 train_time:313237ms step_avg:169.96ms\nstep:1854/10000 train_loss:4.7439 train_time:313407ms step_avg:169.96ms\nstep:1855/10000 train_loss:4.7019 train_time:313578ms step_avg:169.96ms\nstep:1856/10000 train_loss:4.6697 train_time:313746ms step_avg:169.96ms\nstep:1857/10000 train_loss:4.7574 train_time:313916ms step_avg:169.96ms\nstep:1858/10000 train_loss:4.6364 train_time:314085ms step_avg:169.96ms\nstep:1859/10000 train_loss:4.7579 train_time:314255ms step_avg:169.96ms\nstep:1860/10000 train_loss:4.7692 train_time:314423ms step_avg:169.96ms\nstep:1861/10000 train_loss:4.7471 train_time:314594ms step_avg:169.96ms\nstep:1862/10000 train_loss:4.6434 train_time:314765ms step_avg:169.96ms\nstep:1863/10000 train_loss:4.6784 train_time:314933ms step_avg:169.96ms\nstep:1864/10000 train_loss:4.6317 train_time:315104ms step_avg:169.96ms\nstep:1865/10000 train_loss:4.7264 train_time:315274ms step_avg:169.96ms\nstep:1866/10000 train_loss:4.7881 train_time:315443ms step_avg:169.96ms\nstep:1867/10000 train_loss:4.6697 train_time:315615ms step_avg:169.96ms\nstep:1868/10000 train_loss:4.6563 train_time:315786ms step_avg:169.96ms\nstep:1869/10000 train_loss:4.8040 train_time:315955ms step_avg:169.96ms\nstep:1870/10000 train_loss:4.8002 train_time:316126ms step_avg:169.96ms\nstep:1871/10000 train_loss:4.5926 train_time:316297ms step_avg:169.96ms\nstep:1872/10000 train_loss:4.6227 train_time:316465ms step_avg:169.96ms\nstep:1873/10000 train_loss:4.6502 train_time:316637ms step_avg:169.96ms\nstep:1874/10000 train_loss:4.7198 train_time:316806ms step_avg:169.96ms\nstep:1875/10000 train_loss:4.5993 train_time:316975ms step_avg:169.96ms\nstep:1876/10000 train_loss:4.8233 train_time:317143ms step_avg:169.96ms\nstep:1877/10000 train_loss:4.7154 train_time:317314ms step_avg:169.96ms\nstep:1878/10000 train_loss:4.6484 train_time:317484ms step_avg:169.96ms\nstep:1879/10000 train_loss:4.6585 train_time:317656ms step_avg:169.96ms\nstep:1880/10000 train_loss:4.7141 train_time:317824ms step_avg:169.96ms\nstep:1881/10000 train_loss:4.6778 train_time:317993ms step_avg:169.96ms\nstep:1882/10000 train_loss:4.7425 train_time:318165ms step_avg:169.96ms\nstep:1883/10000 train_loss:4.8218 train_time:318335ms step_avg:169.96ms\nstep:1884/10000 train_loss:4.5078 train_time:318504ms step_avg:169.96ms\nstep:1885/10000 train_loss:4.4473 train_time:318674ms step_avg:169.96ms\nstep:1886/10000 train_loss:4.7520 train_time:318842ms step_avg:169.96ms\nstep:1887/10000 train_loss:5.0249 train_time:319011ms step_avg:169.96ms\nstep:1888/10000 train_loss:4.7193 train_time:319179ms step_avg:169.96ms\nstep:1889/10000 train_loss:4.6570 train_time:319349ms step_avg:169.96ms\nstep:1890/10000 train_loss:4.6804 train_time:319520ms step_avg:169.96ms\nstep:1891/10000 train_loss:4.6538 train_time:319691ms step_avg:169.96ms\nstep:1892/10000 train_loss:4.6270 train_time:319863ms step_avg:169.96ms\nstep:1893/10000 train_loss:4.7160 train_time:320033ms step_avg:169.96ms\nstep:1894/10000 train_loss:4.6491 train_time:320203ms step_avg:169.96ms\nstep:1895/10000 train_loss:4.6975 train_time:320374ms step_avg:169.96ms\nstep:1896/10000 train_loss:4.7484 train_time:320545ms step_avg:169.96ms\nstep:1897/10000 train_loss:4.6181 train_time:320716ms step_avg:169.96ms\nstep:1898/10000 train_loss:4.5281 train_time:320885ms step_avg:169.96ms\nstep:1899/10000 train_loss:4.7837 train_time:321054ms step_avg:169.96ms\nstep:1900/10000 train_loss:4.7139 train_time:321224ms step_avg:169.96ms\nstep:1901/10000 train_loss:4.6771 train_time:321395ms step_avg:169.96ms\nstep:1902/10000 train_loss:4.9656 train_time:321564ms step_avg:169.96ms\nstep:1903/10000 train_loss:4.5767 train_time:321735ms step_avg:169.96ms\nstep:1904/10000 train_loss:4.8989 train_time:321906ms step_avg:169.96ms\nstep:1905/10000 train_loss:4.7447 train_time:322076ms step_avg:169.96ms\nstep:1906/10000 train_loss:4.7025 train_time:322243ms step_avg:169.96ms\nstep:1907/10000 train_loss:4.8606 train_time:322413ms step_avg:169.96ms\nstep:1908/10000 train_loss:4.7073 train_time:322585ms step_avg:169.96ms\nstep:1909/10000 train_loss:4.4007 train_time:322757ms step_avg:169.96ms\nstep:1910/10000 train_loss:4.5562 train_time:322928ms step_avg:169.96ms\nstep:1911/10000 train_loss:4.6285 train_time:323099ms step_avg:169.96ms\nstep:1912/10000 train_loss:4.7497 train_time:323267ms step_avg:169.96ms\nstep:1913/10000 train_loss:4.5963 train_time:323436ms step_avg:169.96ms\nstep:1914/10000 train_loss:4.6593 train_time:323605ms step_avg:169.96ms\nstep:1915/10000 train_loss:4.6037 train_time:323777ms step_avg:169.96ms\nstep:1916/10000 train_loss:4.6503 train_time:323948ms step_avg:169.96ms\nstep:1917/10000 train_loss:4.6490 train_time:324120ms step_avg:169.96ms\nstep:1918/10000 train_loss:4.6565 train_time:324287ms step_avg:169.96ms\nstep:1919/10000 train_loss:4.5966 train_time:324456ms step_avg:169.96ms\nstep:1920/10000 train_loss:4.5372 train_time:324627ms step_avg:169.96ms\nstep:1921/10000 train_loss:4.7922 train_time:324797ms step_avg:169.96ms\nstep:1922/10000 train_loss:4.7896 train_time:324965ms step_avg:169.96ms\nstep:1923/10000 train_loss:4.6245 train_time:325136ms step_avg:169.96ms\nstep:1924/10000 train_loss:4.6693 train_time:325307ms step_avg:169.96ms\nstep:1925/10000 train_loss:4.7839 train_time:325479ms step_avg:169.96ms\nstep:1926/10000 train_loss:4.6390 train_time:325649ms step_avg:169.96ms\nstep:1927/10000 train_loss:4.5503 train_time:325820ms step_avg:169.96ms\nstep:1928/10000 train_loss:4.7772 train_time:325991ms step_avg:169.96ms\nstep:1929/10000 train_loss:4.5771 train_time:326162ms step_avg:169.96ms\nstep:1930/10000 train_loss:4.4371 train_time:326333ms step_avg:169.97ms\nstep:1931/10000 train_loss:4.5650 train_time:326503ms step_avg:169.96ms\nstep:1932/10000 train_loss:4.6209 train_time:326672ms step_avg:169.96ms\nstep:1933/10000 train_loss:4.4805 train_time:326843ms step_avg:169.97ms\nstep:1934/10000 train_loss:4.6834 train_time:327014ms step_avg:169.97ms\nstep:1935/10000 train_loss:4.8372 train_time:327185ms step_avg:169.97ms\nstep:1936/10000 train_loss:4.6979 train_time:327354ms step_avg:169.97ms\nstep:1937/10000 train_loss:4.6230 train_time:327524ms step_avg:169.97ms\nstep:1938/10000 train_loss:4.5834 train_time:327694ms step_avg:169.97ms\nstep:1939/10000 train_loss:4.6883 train_time:327863ms step_avg:169.97ms\nstep:1940/10000 train_loss:4.6477 train_time:328034ms step_avg:169.97ms\nstep:1941/10000 train_loss:4.6967 train_time:328206ms step_avg:169.97ms\nstep:1942/10000 train_loss:4.6197 train_time:328376ms step_avg:169.97ms\nstep:1943/10000 train_loss:4.5650 train_time:328544ms step_avg:169.97ms\nstep:1944/10000 train_loss:4.5051 train_time:328713ms step_avg:169.97ms\nstep:1945/10000 train_loss:4.6960 train_time:328884ms step_avg:169.97ms\nstep:1946/10000 train_loss:4.5579 train_time:329055ms step_avg:169.97ms\nstep:1947/10000 train_loss:4.6410 train_time:329222ms step_avg:169.96ms\nstep:1948/10000 train_loss:4.6790 train_time:329390ms step_avg:169.96ms\nstep:1949/10000 train_loss:4.5975 train_time:329560ms step_avg:169.96ms\nstep:1950/10000 train_loss:4.5348 train_time:329735ms step_avg:169.97ms\nstep:1951/10000 train_loss:4.5883 train_time:329905ms step_avg:169.97ms\nstep:1952/10000 train_loss:4.6584 train_time:330073ms step_avg:169.97ms\nstep:1953/10000 train_loss:4.5637 train_time:330246ms step_avg:169.97ms\nstep:1954/10000 train_loss:4.7698 train_time:330417ms step_avg:169.97ms\nstep:1955/10000 train_loss:4.5987 train_time:330581ms step_avg:169.96ms\nstep:1956/10000 train_loss:4.6055 train_time:330750ms step_avg:169.96ms\nstep:1957/10000 train_loss:4.5678 train_time:330922ms step_avg:169.96ms\nstep:1958/10000 train_loss:4.6208 train_time:331091ms step_avg:169.96ms\nstep:1959/10000 train_loss:4.6480 train_time:331263ms step_avg:169.97ms\nstep:1960/10000 train_loss:4.7821 train_time:331434ms step_avg:169.97ms\nstep:1961/10000 train_loss:4.5738 train_time:331603ms step_avg:169.97ms\nstep:1962/10000 train_loss:4.5302 train_time:331772ms step_avg:169.97ms\nstep:1963/10000 train_loss:4.4729 train_time:331941ms step_avg:169.96ms\nstep:1964/10000 train_loss:4.6006 train_time:332110ms step_avg:169.96ms\nstep:1965/10000 train_loss:4.5828 train_time:332280ms step_avg:169.96ms\nstep:1966/10000 train_loss:4.5172 train_time:332450ms step_avg:169.96ms\nstep:1967/10000 train_loss:4.8485 train_time:332621ms step_avg:169.96ms\nstep:1968/10000 train_loss:4.5476 train_time:332791ms step_avg:169.96ms\nstep:1969/10000 train_loss:4.4980 train_time:332964ms step_avg:169.97ms\nstep:1970/10000 train_loss:4.6473 train_time:333134ms step_avg:169.97ms\nstep:1971/10000 train_loss:4.6698 train_time:333304ms step_avg:169.97ms\nstep:1972/10000 train_loss:4.6851 train_time:333473ms step_avg:169.97ms\nstep:1973/10000 train_loss:4.7591 train_time:333642ms step_avg:169.97ms\nstep:1974/10000 train_loss:4.4724 train_time:333813ms step_avg:169.97ms\nstep:1975/10000 train_loss:4.5396 train_time:333984ms step_avg:169.97ms\nstep:1976/10000 train_loss:4.4604 train_time:334152ms step_avg:169.97ms\nstep:1977/10000 train_loss:4.7117 train_time:334322ms step_avg:169.97ms\nstep:1978/10000 train_loss:4.6125 train_time:334490ms step_avg:169.96ms\nstep:1979/10000 train_loss:4.5516 train_time:334660ms step_avg:169.96ms\nstep:1980/10000 train_loss:4.6385 train_time:334829ms step_avg:169.96ms\nstep:1981/10000 train_loss:4.6132 train_time:335001ms step_avg:169.97ms\nstep:1982/10000 train_loss:4.5154 train_time:335168ms step_avg:169.96ms\nstep:1983/10000 train_loss:4.6440 train_time:335340ms step_avg:169.96ms\nstep:1984/10000 train_loss:4.8052 train_time:335508ms step_avg:169.96ms\nstep:1985/10000 train_loss:4.6778 train_time:335680ms step_avg:169.96ms\nstep:1986/10000 train_loss:4.5532 train_time:335848ms step_avg:169.96ms\nstep:1987/10000 train_loss:4.6618 train_time:336020ms step_avg:169.96ms\nstep:1988/10000 train_loss:4.5905 train_time:336190ms step_avg:169.96ms\nstep:1989/10000 train_loss:4.5954 train_time:336362ms step_avg:169.97ms\nstep:1990/10000 train_loss:4.6296 train_time:336532ms step_avg:169.97ms\nstep:1991/10000 train_loss:4.6058 train_time:336702ms step_avg:169.97ms\nstep:1992/10000 train_loss:4.5424 train_time:336872ms step_avg:169.97ms\nstep:1993/10000 train_loss:5.0338 train_time:337042ms step_avg:169.97ms\nstep:1994/10000 train_loss:4.7220 train_time:337211ms step_avg:169.97ms\nstep:1995/10000 train_loss:4.5925 train_time:337382ms step_avg:169.97ms\nstep:1996/10000 train_loss:4.5594 train_time:337552ms step_avg:169.97ms\nstep:1997/10000 train_loss:4.5183 train_time:337725ms step_avg:169.97ms\nstep:1998/10000 train_loss:4.7666 train_time:337895ms step_avg:169.97ms\nstep:1999/10000 train_loss:4.6929 train_time:338065ms step_avg:169.97ms\nstep:2000/10000 train_loss:4.6935 train_time:338234ms step_avg:169.97ms\nstep:2001/10000 train_loss:4.6653 train_time:338404ms step_avg:169.97ms\nstep:2002/10000 train_loss:4.6458 train_time:338574ms step_avg:169.97ms\nstep:2003/10000 train_loss:4.6487 train_time:338742ms step_avg:169.97ms\nstep:2004/10000 train_loss:4.6251 train_time:338910ms step_avg:169.97ms\nstep:2005/10000 train_loss:4.6096 train_time:339084ms step_avg:169.97ms\nstep:2006/10000 train_loss:4.6320 train_time:339253ms step_avg:169.97ms\nstep:2007/10000 train_loss:4.5826 train_time:339423ms step_avg:169.97ms\nstep:2008/10000 train_loss:4.4884 train_time:339593ms step_avg:169.97ms\nstep:2009/10000 train_loss:5.2223 train_time:339761ms step_avg:169.97ms\nstep:2010/10000 train_loss:4.6196 train_time:339929ms step_avg:169.96ms\nstep:2011/10000 train_loss:4.6306 train_time:340099ms step_avg:169.96ms\nstep:2012/10000 train_loss:4.5720 train_time:340266ms step_avg:169.96ms\nstep:2013/10000 train_loss:4.5860 train_time:340434ms step_avg:169.96ms\nstep:2014/10000 train_loss:4.6222 train_time:340605ms step_avg:169.96ms\nstep:2015/10000 train_loss:4.4859 train_time:340777ms step_avg:169.96ms\nstep:2016/10000 train_loss:4.5077 train_time:340946ms step_avg:169.96ms\nstep:2017/10000 train_loss:4.4875 train_time:341118ms step_avg:169.96ms\nstep:2018/10000 train_loss:4.6352 train_time:341287ms step_avg:169.96ms\nstep:2019/10000 train_loss:4.6762 train_time:341458ms step_avg:169.96ms\nstep:2020/10000 train_loss:4.8728 train_time:341627ms step_avg:169.96ms\nstep:2021/10000 train_loss:4.4071 train_time:341796ms step_avg:169.96ms\nstep:2022/10000 train_loss:4.7507 train_time:341965ms step_avg:169.96ms\nstep:2023/10000 train_loss:4.4191 train_time:342135ms step_avg:169.96ms\nstep:2024/10000 train_loss:4.5970 train_time:342304ms step_avg:169.96ms\nstep:2025/10000 train_loss:4.5576 train_time:342476ms step_avg:169.96ms\nstep:2026/10000 train_loss:4.4651 train_time:342645ms step_avg:169.96ms\nstep:2027/10000 train_loss:4.5209 train_time:342815ms step_avg:169.96ms\nstep:2028/10000 train_loss:4.6361 train_time:342982ms step_avg:169.96ms\nstep:2029/10000 train_loss:4.3977 train_time:343150ms step_avg:169.96ms\nstep:2030/10000 train_loss:4.6290 train_time:343321ms step_avg:169.96ms\nstep:2031/10000 train_loss:4.6282 train_time:343491ms step_avg:169.96ms\nstep:2032/10000 train_loss:4.7987 train_time:343661ms step_avg:169.96ms\nstep:2033/10000 train_loss:4.7013 train_time:343833ms step_avg:169.96ms\nstep:2034/10000 train_loss:4.7130 train_time:344004ms step_avg:169.96ms\nstep:2035/10000 train_loss:4.5087 train_time:344174ms step_avg:169.96ms\nstep:2036/10000 train_loss:4.5044 train_time:344342ms step_avg:169.96ms\nstep:2037/10000 train_loss:4.6235 train_time:344512ms step_avg:169.96ms\nstep:2038/10000 train_loss:4.6466 train_time:344680ms step_avg:169.96ms\nstep:2039/10000 train_loss:4.7016 train_time:344850ms step_avg:169.96ms\nstep:2040/10000 train_loss:4.6193 train_time:345020ms step_avg:169.96ms\nstep:2041/10000 train_loss:4.5781 train_time:345189ms step_avg:169.96ms\nstep:2042/10000 train_loss:4.4694 train_time:345360ms step_avg:169.96ms\nstep:2043/10000 train_loss:4.5379 train_time:345529ms step_avg:169.96ms\nstep:2044/10000 train_loss:4.6191 train_time:345700ms step_avg:169.96ms\nstep:2045/10000 train_loss:4.5680 train_time:345871ms step_avg:169.96ms\nstep:2046/10000 train_loss:4.6572 train_time:346042ms step_avg:169.96ms\nstep:2047/10000 train_loss:4.5796 train_time:346210ms step_avg:169.96ms\nstep:2048/10000 train_loss:4.4471 train_time:346381ms step_avg:169.96ms\nstep:2049/10000 train_loss:4.5164 train_time:346549ms step_avg:169.96ms\nstep:2050/10000 train_loss:4.5708 train_time:346719ms step_avg:169.96ms\nstep:2051/10000 train_loss:4.5692 train_time:346887ms step_avg:169.96ms\nstep:2052/10000 train_loss:4.4380 train_time:347058ms step_avg:169.96ms\nstep:2053/10000 train_loss:4.6387 train_time:347226ms step_avg:169.96ms\nstep:2054/10000 train_loss:4.5848 train_time:347395ms step_avg:169.96ms\nstep:2055/10000 train_loss:4.5306 train_time:347564ms step_avg:169.96ms\nstep:2056/10000 train_loss:4.6626 train_time:347732ms step_avg:169.96ms\nstep:2057/10000 train_loss:4.5365 train_time:347900ms step_avg:169.96ms\nstep:2058/10000 train_loss:4.5927 train_time:348071ms step_avg:169.96ms\nstep:2059/10000 train_loss:4.5097 train_time:348243ms step_avg:169.96ms\nstep:2060/10000 train_loss:4.8375 train_time:348412ms step_avg:169.96ms\nstep:2061/10000 train_loss:4.6880 train_time:348584ms step_avg:169.96ms\nstep:2062/10000 train_loss:4.6636 train_time:348754ms step_avg:169.96ms\nstep:2063/10000 train_loss:4.6010 train_time:348922ms step_avg:169.96ms\nstep:2064/10000 train_loss:4.6307 train_time:349093ms step_avg:169.96ms\nstep:2065/10000 train_loss:4.6448 train_time:349264ms step_avg:169.96ms\nstep:2066/10000 train_loss:5.0104 train_time:349435ms step_avg:169.96ms\nstep:2067/10000 train_loss:4.7237 train_time:349605ms step_avg:169.96ms\nstep:2068/10000 train_loss:4.4807 train_time:349777ms step_avg:169.96ms\nstep:2069/10000 train_loss:4.5936 train_time:349946ms step_avg:169.96ms\nstep:2070/10000 train_loss:4.5707 train_time:350117ms step_avg:169.96ms\nstep:2071/10000 train_loss:4.5923 train_time:350285ms step_avg:169.96ms\nstep:2072/10000 train_loss:4.6638 train_time:350458ms step_avg:169.96ms\nstep:2073/10000 train_loss:4.6264 train_time:350626ms step_avg:169.96ms\nstep:2074/10000 train_loss:4.6860 train_time:350795ms step_avg:169.96ms\nstep:2075/10000 train_loss:4.4186 train_time:350962ms step_avg:169.96ms\nstep:2076/10000 train_loss:4.4134 train_time:351133ms step_avg:169.96ms\nstep:2077/10000 train_loss:4.6733 train_time:351302ms step_avg:169.96ms\nstep:2078/10000 train_loss:4.6286 train_time:351471ms step_avg:169.96ms\nstep:2079/10000 train_loss:4.5067 train_time:351641ms step_avg:169.96ms\nstep:2080/10000 train_loss:4.6227 train_time:351810ms step_avg:169.96ms\nstep:2081/10000 train_loss:4.5521 train_time:351979ms step_avg:169.96ms\nstep:2082/10000 train_loss:4.6034 train_time:352148ms step_avg:169.96ms\nstep:2083/10000 train_loss:4.5173 train_time:352320ms step_avg:169.96ms\nstep:2084/10000 train_loss:4.5089 train_time:352489ms step_avg:169.96ms\nstep:2085/10000 train_loss:4.5436 train_time:352660ms step_avg:169.96ms\nstep:2086/10000 train_loss:4.4551 train_time:352831ms step_avg:169.96ms\nstep:2087/10000 train_loss:4.6232 train_time:353002ms step_avg:169.96ms\nstep:2088/10000 train_loss:4.3868 train_time:353170ms step_avg:169.96ms\nstep:2089/10000 train_loss:4.5979 train_time:353341ms step_avg:169.96ms\nstep:2090/10000 train_loss:4.5218 train_time:353510ms step_avg:169.96ms\nstep:2091/10000 train_loss:4.5767 train_time:353678ms step_avg:169.96ms\nstep:2092/10000 train_loss:4.6441 train_time:353845ms step_avg:169.95ms\nstep:2093/10000 train_loss:4.6787 train_time:354015ms step_avg:169.95ms\nstep:2094/10000 train_loss:5.0957 train_time:354182ms step_avg:169.95ms\nstep:2095/10000 train_loss:4.4484 train_time:354352ms step_avg:169.95ms\nstep:2096/10000 train_loss:4.4370 train_time:354523ms step_avg:169.95ms\nstep:2097/10000 train_loss:4.3989 train_time:354694ms step_avg:169.95ms\nstep:2098/10000 train_loss:4.6143 train_time:354864ms step_avg:169.95ms\nstep:2099/10000 train_loss:4.5247 train_time:355034ms step_avg:169.95ms\nstep:2100/10000 train_loss:4.5126 train_time:355204ms step_avg:169.95ms\nstep:2101/10000 train_loss:4.5444 train_time:355374ms step_avg:169.95ms\nstep:2102/10000 train_loss:4.5622 train_time:355541ms step_avg:169.95ms\nstep:2103/10000 train_loss:4.5125 train_time:355710ms step_avg:169.95ms\nstep:2104/10000 train_loss:4.5497 train_time:355880ms step_avg:169.95ms\nstep:2105/10000 train_loss:4.3378 train_time:356049ms step_avg:169.95ms\nstep:2106/10000 train_loss:4.5448 train_time:356218ms step_avg:169.95ms\nstep:2107/10000 train_loss:4.7069 train_time:356388ms step_avg:169.95ms\nstep:2108/10000 train_loss:4.7675 train_time:356560ms step_avg:169.95ms\nstep:2109/10000 train_loss:4.4444 train_time:356730ms step_avg:169.95ms\nstep:2110/10000 train_loss:4.4785 train_time:356899ms step_avg:169.95ms\nstep:2111/10000 train_loss:4.5561 train_time:357068ms step_avg:169.95ms\nstep:2112/10000 train_loss:4.4998 train_time:357238ms step_avg:169.95ms\nstep:2113/10000 train_loss:4.5698 train_time:357408ms step_avg:169.95ms\nstep:2114/10000 train_loss:4.4763 train_time:357579ms step_avg:169.95ms\nstep:2115/10000 train_loss:4.6574 train_time:357748ms step_avg:169.95ms\nstep:2116/10000 train_loss:4.4807 train_time:357923ms step_avg:169.95ms\nstep:2117/10000 train_loss:4.4996 train_time:358094ms step_avg:169.95ms\nstep:2118/10000 train_loss:4.6288 train_time:358264ms step_avg:169.95ms\nstep:2119/10000 train_loss:4.6510 train_time:358433ms step_avg:169.95ms\nstep:2120/10000 train_loss:4.5287 train_time:358602ms step_avg:169.95ms\nstep:2121/10000 train_loss:4.5734 train_time:358771ms step_avg:169.95ms\nstep:2122/10000 train_loss:4.4837 train_time:358942ms step_avg:169.95ms\nstep:2123/10000 train_loss:4.5478 train_time:359114ms step_avg:169.95ms\nstep:2124/10000 train_loss:4.6407 train_time:359283ms step_avg:169.95ms\nstep:2125/10000 train_loss:4.4553 train_time:359453ms step_avg:169.95ms\nstep:2126/10000 train_loss:4.7437 train_time:359622ms step_avg:169.95ms\nstep:2127/10000 train_loss:4.4200 train_time:359789ms step_avg:169.95ms\nstep:2128/10000 train_loss:4.5281 train_time:359959ms step_avg:169.95ms\nstep:2129/10000 train_loss:4.3445 train_time:360127ms step_avg:169.95ms\nstep:2130/10000 train_loss:4.5437 train_time:360299ms step_avg:169.95ms\nstep:2131/10000 train_loss:4.6443 train_time:360467ms step_avg:169.95ms\nstep:2132/10000 train_loss:4.5451 train_time:360637ms step_avg:169.95ms\nstep:2133/10000 train_loss:4.2889 train_time:360805ms step_avg:169.95ms\nstep:2134/10000 train_loss:4.5343 train_time:360975ms step_avg:169.95ms\nstep:2135/10000 train_loss:4.5879 train_time:361143ms step_avg:169.95ms\nstep:2136/10000 train_loss:4.6266 train_time:361312ms step_avg:169.95ms\nstep:2137/10000 train_loss:4.5552 train_time:361483ms step_avg:169.95ms\nstep:2138/10000 train_loss:4.5334 train_time:361652ms step_avg:169.95ms\nstep:2139/10000 train_loss:4.5895 train_time:361823ms step_avg:169.95ms\nstep:2140/10000 train_loss:4.6205 train_time:361993ms step_avg:169.95ms\nstep:2141/10000 train_loss:4.5488 train_time:362163ms step_avg:169.95ms\nstep:2142/10000 train_loss:4.4702 train_time:362334ms step_avg:169.95ms\nstep:2143/10000 train_loss:4.5119 train_time:362504ms step_avg:169.95ms\nstep:2144/10000 train_loss:4.5675 train_time:362675ms step_avg:169.95ms\nstep:2145/10000 train_loss:4.8358 train_time:362845ms step_avg:169.95ms\nstep:2146/10000 train_loss:4.5586 train_time:363014ms step_avg:169.95ms\nstep:2147/10000 train_loss:4.4062 train_time:363181ms step_avg:169.95ms\nstep:2148/10000 train_loss:4.5643 train_time:363349ms step_avg:169.95ms\nstep:2149/10000 train_loss:4.3897 train_time:363518ms step_avg:169.95ms\nstep:2150/10000 train_loss:4.5608 train_time:363686ms step_avg:169.95ms\nstep:2151/10000 train_loss:4.4411 train_time:363857ms step_avg:169.95ms\nstep:2152/10000 train_loss:4.4293 train_time:364026ms step_avg:169.95ms\nstep:2153/10000 train_loss:4.6078 train_time:364197ms step_avg:169.95ms\nstep:2154/10000 train_loss:4.5076 train_time:364367ms step_avg:169.95ms\nstep:2155/10000 train_loss:4.4181 train_time:364537ms step_avg:169.95ms\nstep:2156/10000 train_loss:4.3919 train_time:364706ms step_avg:169.95ms\nstep:2157/10000 train_loss:4.5982 train_time:364877ms step_avg:169.95ms\nstep:2158/10000 train_loss:4.9496 train_time:365046ms step_avg:169.95ms\nstep:2159/10000 train_loss:4.4466 train_time:365217ms step_avg:169.95ms\nstep:2160/10000 train_loss:4.4703 train_time:365387ms step_avg:169.95ms\nstep:2161/10000 train_loss:4.5644 train_time:365559ms step_avg:169.95ms\nstep:2162/10000 train_loss:4.5266 train_time:365729ms step_avg:169.95ms\nstep:2163/10000 train_loss:4.6444 train_time:365901ms step_avg:169.95ms\nstep:2164/10000 train_loss:4.5029 train_time:366071ms step_avg:169.95ms\nstep:2165/10000 train_loss:4.2269 train_time:366241ms step_avg:169.95ms\nstep:2166/10000 train_loss:4.6007 train_time:366411ms step_avg:169.95ms\nstep:2167/10000 train_loss:5.0447 train_time:366582ms step_avg:169.95ms\nstep:2168/10000 train_loss:4.4538 train_time:366752ms step_avg:169.95ms\nstep:2169/10000 train_loss:4.5297 train_time:366923ms step_avg:169.95ms\nstep:2170/10000 train_loss:4.4863 train_time:367091ms step_avg:169.95ms\nstep:2171/10000 train_loss:4.4313 train_time:367262ms step_avg:169.95ms\nstep:2172/10000 train_loss:4.5669 train_time:367431ms step_avg:169.95ms\nstep:2173/10000 train_loss:4.6899 train_time:367605ms step_avg:169.95ms\nstep:2174/10000 train_loss:4.4891 train_time:367774ms step_avg:169.95ms\nstep:2175/10000 train_loss:4.6209 train_time:367942ms step_avg:169.95ms\nstep:2176/10000 train_loss:4.5360 train_time:368111ms step_avg:169.95ms\nstep:2177/10000 train_loss:4.5779 train_time:368282ms step_avg:169.95ms\nstep:2178/10000 train_loss:4.6112 train_time:368449ms step_avg:169.95ms\nstep:2179/10000 train_loss:4.4502 train_time:368618ms step_avg:169.95ms\nstep:2180/10000 train_loss:4.5887 train_time:368785ms step_avg:169.95ms\nstep:2181/10000 train_loss:4.7176 train_time:368956ms step_avg:169.95ms\nstep:2182/10000 train_loss:4.4679 train_time:369126ms step_avg:169.95ms\nstep:2183/10000 train_loss:4.3774 train_time:369297ms step_avg:169.95ms\nstep:2184/10000 train_loss:4.5669 train_time:369464ms step_avg:169.95ms\nstep:2185/10000 train_loss:4.5158 train_time:369635ms step_avg:169.95ms\nstep:2186/10000 train_loss:4.5206 train_time:369803ms step_avg:169.95ms\nstep:2187/10000 train_loss:4.4404 train_time:369973ms step_avg:169.95ms\nstep:2188/10000 train_loss:4.3826 train_time:370144ms step_avg:169.95ms\nstep:2189/10000 train_loss:4.5151 train_time:370317ms step_avg:169.95ms\nstep:2190/10000 train_loss:4.4958 train_time:370485ms step_avg:169.95ms\nstep:2191/10000 train_loss:4.5559 train_time:370657ms step_avg:169.95ms\nstep:2192/10000 train_loss:4.6537 train_time:370826ms step_avg:169.95ms\nstep:2193/10000 train_loss:4.4474 train_time:370996ms step_avg:169.95ms\nstep:2194/10000 train_loss:4.5150 train_time:371166ms step_avg:169.95ms\nstep:2195/10000 train_loss:4.3580 train_time:371338ms step_avg:169.95ms\nstep:2196/10000 train_loss:4.2369 train_time:371507ms step_avg:169.95ms\nstep:2197/10000 train_loss:4.4988 train_time:371678ms step_avg:169.95ms\nstep:2198/10000 train_loss:4.5465 train_time:371845ms step_avg:169.95ms\nstep:2199/10000 train_loss:4.3936 train_time:372015ms step_avg:169.95ms\nstep:2200/10000 train_loss:4.6101 train_time:372186ms step_avg:169.95ms\nstep:2201/10000 train_loss:4.3725 train_time:372358ms step_avg:169.95ms\nstep:2202/10000 train_loss:4.4094 train_time:372525ms step_avg:169.95ms\nstep:2203/10000 train_loss:4.4579 train_time:372697ms step_avg:169.95ms\nstep:2204/10000 train_loss:4.5209 train_time:372864ms step_avg:169.95ms\nstep:2205/10000 train_loss:4.5341 train_time:373035ms step_avg:169.95ms\nstep:2206/10000 train_loss:4.4691 train_time:373205ms step_avg:169.95ms\nstep:2207/10000 train_loss:4.4253 train_time:373375ms step_avg:169.95ms\nstep:2208/10000 train_loss:4.4340 train_time:373549ms step_avg:169.95ms\nstep:2209/10000 train_loss:4.4540 train_time:373714ms step_avg:169.95ms\nstep:2210/10000 train_loss:4.7715 train_time:373884ms step_avg:169.95ms\nstep:2211/10000 train_loss:4.4821 train_time:374052ms step_avg:169.95ms\nstep:2212/10000 train_loss:4.6548 train_time:374219ms step_avg:169.95ms\nstep:2213/10000 train_loss:4.5784 train_time:374390ms step_avg:169.95ms\nstep:2214/10000 train_loss:4.5698 train_time:374561ms step_avg:169.95ms\nstep:2215/10000 train_loss:4.5323 train_time:374729ms step_avg:169.94ms\nstep:2216/10000 train_loss:4.5186 train_time:374900ms step_avg:169.95ms\nstep:2217/10000 train_loss:4.3735 train_time:375069ms step_avg:169.95ms\nstep:2218/10000 train_loss:4.5501 train_time:375238ms step_avg:169.94ms\nstep:2219/10000 train_loss:4.4798 train_time:375406ms step_avg:169.94ms\nstep:2220/10000 train_loss:4.4670 train_time:375576ms step_avg:169.94ms\nstep:2221/10000 train_loss:4.3399 train_time:375743ms step_avg:169.94ms\nstep:2222/10000 train_loss:4.5913 train_time:375913ms step_avg:169.94ms\nstep:2223/10000 train_loss:4.5645 train_time:376086ms step_avg:169.94ms\nstep:2224/10000 train_loss:4.5725 train_time:376256ms step_avg:169.94ms\nstep:2225/10000 train_loss:4.4236 train_time:376424ms step_avg:169.94ms\nstep:2226/10000 train_loss:4.5123 train_time:376595ms step_avg:169.94ms\nstep:2227/10000 train_loss:4.5070 train_time:376764ms step_avg:169.94ms\nstep:2228/10000 train_loss:4.5368 train_time:376933ms step_avg:169.94ms\nstep:2229/10000 train_loss:4.3855 train_time:377104ms step_avg:169.94ms\nstep:2230/10000 train_loss:4.2908 train_time:377274ms step_avg:169.94ms\nstep:2231/10000 train_loss:4.6314 train_time:377443ms step_avg:169.94ms\nstep:2232/10000 train_loss:4.4151 train_time:377613ms step_avg:169.94ms\nstep:2233/10000 train_loss:4.3713 train_time:377780ms step_avg:169.94ms\nstep:2234/10000 train_loss:4.4170 train_time:377949ms step_avg:169.94ms\nstep:2235/10000 train_loss:4.4419 train_time:378119ms step_avg:169.94ms\nstep:2236/10000 train_loss:4.5259 train_time:378287ms step_avg:169.94ms\nstep:2237/10000 train_loss:4.6185 train_time:378457ms step_avg:169.94ms\nstep:2238/10000 train_loss:4.4947 train_time:378627ms step_avg:169.94ms\nstep:2239/10000 train_loss:4.4677 train_time:378797ms step_avg:169.94ms\nstep:2240/10000 train_loss:4.3941 train_time:378963ms step_avg:169.94ms\nstep:2241/10000 train_loss:4.6138 train_time:379133ms step_avg:169.94ms\nstep:2242/10000 train_loss:4.5240 train_time:379300ms step_avg:169.94ms\nstep:2243/10000 train_loss:4.6695 train_time:379472ms step_avg:169.94ms\nstep:2244/10000 train_loss:4.4833 train_time:379644ms step_avg:169.94ms\nstep:2245/10000 train_loss:4.3559 train_time:379816ms step_avg:169.94ms\nstep:2246/10000 train_loss:4.4041 train_time:379984ms step_avg:169.94ms\nstep:2247/10000 train_loss:4.4790 train_time:380152ms step_avg:169.94ms\nstep:2248/10000 train_loss:4.4603 train_time:380322ms step_avg:169.94ms\nstep:2249/10000 train_loss:4.4633 train_time:380493ms step_avg:169.94ms\nstep:2250/10000 train_loss:4.3595 train_time:380664ms step_avg:169.94ms\nstep:2251/10000 train_loss:4.3775 train_time:380834ms step_avg:169.94ms\nstep:2252/10000 train_loss:4.3294 train_time:381003ms step_avg:169.94ms\nstep:2253/10000 train_loss:4.5565 train_time:381174ms step_avg:169.94ms\nstep:2254/10000 train_loss:4.5590 train_time:381343ms step_avg:169.94ms\nstep:2255/10000 train_loss:4.4193 train_time:381514ms step_avg:169.94ms\nstep:2256/10000 train_loss:4.5283 train_time:381686ms step_avg:169.94ms\nstep:2257/10000 train_loss:4.3959 train_time:381858ms step_avg:169.94ms\nstep:2258/10000 train_loss:4.4055 train_time:382026ms step_avg:169.94ms\nstep:2259/10000 train_loss:4.5396 train_time:382195ms step_avg:169.94ms\nstep:2260/10000 train_loss:4.4600 train_time:382361ms step_avg:169.94ms\nstep:2261/10000 train_loss:4.5556 train_time:382531ms step_avg:169.94ms\nstep:2262/10000 train_loss:4.5114 train_time:382701ms step_avg:169.94ms\nstep:2263/10000 train_loss:4.3717 train_time:382868ms step_avg:169.94ms\nstep:2264/10000 train_loss:4.4297 train_time:383042ms step_avg:169.94ms\nstep:2265/10000 train_loss:4.6171 train_time:383212ms step_avg:169.94ms\nstep:2266/10000 train_loss:4.4673 train_time:383383ms step_avg:169.94ms\nstep:2267/10000 train_loss:4.4171 train_time:383552ms step_avg:169.94ms\nstep:2268/10000 train_loss:4.5455 train_time:383723ms step_avg:169.94ms\nstep:2269/10000 train_loss:4.4202 train_time:383893ms step_avg:169.94ms\nstep:2270/10000 train_loss:4.4073 train_time:384068ms step_avg:169.94ms\nstep:2271/10000 train_loss:4.3776 train_time:384234ms step_avg:169.94ms\nstep:2272/10000 train_loss:4.5417 train_time:384404ms step_avg:169.94ms\nstep:2273/10000 train_loss:4.4847 train_time:384573ms step_avg:169.94ms\nstep:2274/10000 train_loss:4.4850 train_time:384741ms step_avg:169.94ms\nstep:2275/10000 train_loss:4.4786 train_time:384910ms step_avg:169.94ms\nstep:2276/10000 train_loss:4.7114 train_time:385080ms step_avg:169.94ms\nstep:2277/10000 train_loss:4.4879 train_time:385251ms step_avg:169.94ms\nstep:2278/10000 train_loss:4.4704 train_time:385422ms step_avg:169.94ms\nstep:2279/10000 train_loss:4.5811 train_time:385590ms step_avg:169.94ms\nstep:2280/10000 train_loss:4.4487 train_time:385760ms step_avg:169.94ms\nstep:2281/10000 train_loss:4.3187 train_time:385929ms step_avg:169.94ms\nstep:2282/10000 train_loss:4.4833 train_time:386100ms step_avg:169.94ms\nstep:2283/10000 train_loss:4.4993 train_time:386269ms step_avg:169.94ms\nstep:2284/10000 train_loss:4.4785 train_time:386439ms step_avg:169.94ms\nstep:2285/10000 train_loss:4.4539 train_time:386607ms step_avg:169.94ms\nstep:2286/10000 train_loss:4.3793 train_time:386996ms step_avg:170.03ms\nstep:2287/10000 train_loss:4.4624 train_time:387158ms step_avg:170.03ms\nstep:2288/10000 train_loss:4.6076 train_time:387325ms step_avg:170.03ms\nstep:2289/10000 train_loss:4.3899 train_time:387493ms step_avg:170.03ms\nstep:2290/10000 train_loss:4.3096 train_time:387660ms step_avg:170.03ms\nstep:2291/10000 train_loss:4.5061 train_time:387829ms step_avg:170.03ms\nstep:2292/10000 train_loss:4.4619 train_time:388005ms step_avg:170.03ms\nstep:2293/10000 train_loss:4.4447 train_time:388178ms step_avg:170.03ms\nstep:2294/10000 train_loss:4.3906 train_time:388346ms step_avg:170.03ms\nstep:2295/10000 train_loss:4.4442 train_time:388517ms step_avg:170.03ms\nstep:2296/10000 train_loss:4.4696 train_time:388686ms step_avg:170.03ms\nstep:2297/10000 train_loss:4.5677 train_time:388857ms step_avg:170.03ms\nstep:2298/10000 train_loss:4.7773 train_time:389029ms step_avg:170.03ms\nstep:2299/10000 train_loss:4.5349 train_time:389201ms step_avg:170.03ms\nstep:2300/10000 train_loss:4.5773 train_time:389371ms step_avg:170.03ms\nstep:2301/10000 train_loss:4.4255 train_time:389539ms step_avg:170.03ms\nstep:2302/10000 train_loss:4.3644 train_time:389706ms step_avg:170.03ms\nstep:2303/10000 train_loss:4.4077 train_time:389875ms step_avg:170.03ms\nstep:2304/10000 train_loss:4.5459 train_time:390053ms step_avg:170.03ms\nstep:2305/10000 train_loss:4.5082 train_time:390221ms step_avg:170.03ms\nstep:2306/10000 train_loss:4.4536 train_time:390389ms step_avg:170.03ms\nstep:2307/10000 train_loss:4.5250 train_time:390560ms step_avg:170.03ms\nstep:2308/10000 train_loss:4.5414 train_time:390729ms step_avg:170.03ms\nstep:2309/10000 train_loss:4.4302 train_time:390900ms step_avg:170.03ms\nstep:2310/10000 train_loss:4.6256 train_time:391070ms step_avg:170.03ms\nstep:2311/10000 train_loss:4.4548 train_time:391241ms step_avg:170.03ms\nstep:2312/10000 train_loss:4.5088 train_time:391410ms step_avg:170.03ms\nstep:2313/10000 train_loss:4.5651 train_time:391580ms step_avg:170.03ms\nstep:2314/10000 train_loss:4.3799 train_time:391748ms step_avg:170.03ms\nstep:2315/10000 train_loss:4.3346 train_time:391918ms step_avg:170.03ms\nstep:2316/10000 train_loss:4.4529 train_time:392086ms step_avg:170.03ms\nstep:2317/10000 train_loss:4.5567 train_time:392259ms step_avg:170.03ms\nstep:2318/10000 train_loss:4.3820 train_time:392435ms step_avg:170.03ms\nstep:2319/10000 train_loss:4.6925 train_time:392601ms step_avg:170.03ms\nstep:2320/10000 train_loss:4.4236 train_time:392769ms step_avg:170.03ms\nstep:2321/10000 train_loss:4.5667 train_time:392939ms step_avg:170.03ms\nstep:2322/10000 train_loss:4.5388 train_time:393108ms step_avg:170.03ms\nstep:2323/10000 train_loss:4.3678 train_time:393280ms step_avg:170.03ms\nstep:2324/10000 train_loss:4.3925 train_time:393449ms step_avg:170.03ms\nstep:2325/10000 train_loss:4.4801 train_time:393619ms step_avg:170.03ms\nstep:2326/10000 train_loss:4.5104 train_time:393789ms step_avg:170.03ms\nstep:2327/10000 train_loss:4.3967 train_time:393957ms step_avg:170.03ms\nstep:2328/10000 train_loss:4.4225 train_time:394125ms step_avg:170.03ms\nstep:2329/10000 train_loss:4.3624 train_time:394296ms step_avg:170.03ms\nstep:2330/10000 train_loss:4.4951 train_time:394468ms step_avg:170.03ms\nstep:2331/10000 train_loss:4.5459 train_time:394639ms step_avg:170.03ms\nstep:2332/10000 train_loss:4.6528 train_time:394807ms step_avg:170.03ms\nstep:2333/10000 train_loss:4.4364 train_time:394976ms step_avg:170.03ms\nstep:2334/10000 train_loss:4.6433 train_time:395142ms step_avg:170.03ms\nstep:2335/10000 train_loss:4.3322 train_time:395312ms step_avg:170.03ms\nstep:2336/10000 train_loss:4.4006 train_time:395482ms step_avg:170.03ms\nstep:2337/10000 train_loss:4.4055 train_time:395654ms step_avg:170.03ms\nstep:2338/10000 train_loss:4.3817 train_time:395823ms step_avg:170.03ms\nstep:2339/10000 train_loss:4.5079 train_time:395993ms step_avg:170.03ms\nstep:2340/10000 train_loss:4.3555 train_time:396160ms step_avg:170.03ms\nstep:2341/10000 train_loss:4.4198 train_time:396328ms step_avg:170.02ms\nstep:2342/10000 train_loss:4.5482 train_time:396499ms step_avg:170.03ms\nstep:2343/10000 train_loss:4.3482 train_time:396668ms step_avg:170.02ms\nstep:2344/10000 train_loss:4.5245 train_time:396837ms step_avg:170.02ms\nstep:2345/10000 train_loss:4.5027 train_time:397008ms step_avg:170.02ms\nstep:2346/10000 train_loss:4.4658 train_time:397178ms step_avg:170.02ms\nstep:2347/10000 train_loss:4.4159 train_time:397347ms step_avg:170.02ms\nstep:2348/10000 train_loss:4.3523 train_time:397516ms step_avg:170.02ms\nstep:2349/10000 train_loss:4.4079 train_time:397685ms step_avg:170.02ms\nstep:2350/10000 train_loss:4.7967 train_time:397855ms step_avg:170.02ms\nstep:2351/10000 train_loss:4.3422 train_time:398023ms step_avg:170.02ms\nstep:2352/10000 train_loss:4.4465 train_time:398195ms step_avg:170.02ms\nstep:2353/10000 train_loss:4.4777 train_time:398365ms step_avg:170.02ms\nstep:2354/10000 train_loss:4.2760 train_time:398536ms step_avg:170.02ms\nstep:2355/10000 train_loss:4.4040 train_time:398704ms step_avg:170.02ms\nstep:2356/10000 train_loss:4.4851 train_time:398876ms step_avg:170.02ms\nstep:2357/10000 train_loss:4.4054 train_time:399043ms step_avg:170.02ms\nstep:2358/10000 train_loss:4.6479 train_time:399215ms step_avg:170.02ms\nstep:2359/10000 train_loss:4.4428 train_time:399384ms step_avg:170.02ms\nstep:2360/10000 train_loss:4.7956 train_time:399553ms step_avg:170.02ms\nstep:2361/10000 train_loss:4.5110 train_time:399724ms step_avg:170.02ms\nstep:2362/10000 train_loss:4.5049 train_time:399894ms step_avg:170.02ms\nstep:2363/10000 train_loss:4.6175 train_time:400063ms step_avg:170.02ms\nstep:2364/10000 train_loss:4.5496 train_time:400233ms step_avg:170.02ms\nstep:2365/10000 train_loss:4.4390 train_time:400404ms step_avg:170.02ms\nstep:2366/10000 train_loss:4.2989 train_time:400573ms step_avg:170.02ms\nstep:2367/10000 train_loss:4.3374 train_time:400744ms step_avg:170.02ms\nstep:2368/10000 train_loss:4.3008 train_time:400914ms step_avg:170.02ms\nstep:2369/10000 train_loss:4.6059 train_time:401082ms step_avg:170.02ms\nstep:2370/10000 train_loss:4.3656 train_time:401254ms step_avg:170.02ms\nstep:2371/10000 train_loss:4.5549 train_time:401426ms step_avg:170.02ms\nstep:2372/10000 train_loss:4.3154 train_time:401596ms step_avg:170.02ms\nstep:2373/10000 train_loss:4.4287 train_time:401763ms step_avg:170.02ms\nstep:2374/10000 train_loss:4.5307 train_time:401933ms step_avg:170.02ms\nstep:2375/10000 train_loss:4.5418 train_time:402101ms step_avg:170.02ms\nstep:2376/10000 train_loss:4.3489 train_time:402270ms step_avg:170.02ms\nstep:2377/10000 train_loss:4.3544 train_time:402441ms step_avg:170.02ms\nstep:2378/10000 train_loss:4.4061 train_time:402609ms step_avg:170.02ms\nstep:2379/10000 train_loss:4.4713 train_time:402778ms step_avg:170.02ms\nstep:2380/10000 train_loss:4.4114 train_time:402949ms step_avg:170.02ms\nstep:2381/10000 train_loss:4.3565 train_time:403117ms step_avg:170.02ms\nstep:2382/10000 train_loss:4.4071 train_time:403286ms step_avg:170.02ms\nstep:2383/10000 train_loss:4.8019 train_time:403460ms step_avg:170.02ms\nstep:2384/10000 train_loss:4.6677 train_time:403630ms step_avg:170.02ms\nstep:2385/10000 train_loss:4.4421 train_time:403801ms step_avg:170.02ms\nstep:2386/10000 train_loss:4.4473 train_time:403970ms step_avg:170.02ms\nstep:2387/10000 train_loss:4.3209 train_time:404139ms step_avg:170.02ms\nstep:2388/10000 train_loss:4.4461 train_time:404307ms step_avg:170.02ms\nstep:2389/10000 train_loss:4.5254 train_time:404478ms step_avg:170.02ms\nstep:2390/10000 train_loss:4.4819 train_time:404647ms step_avg:170.02ms\nstep:2391/10000 train_loss:4.4214 train_time:404817ms step_avg:170.02ms\nstep:2392/10000 train_loss:4.3867 train_time:404987ms step_avg:170.02ms\nstep:2393/10000 train_loss:4.4300 train_time:405158ms step_avg:170.02ms\nstep:2394/10000 train_loss:4.5063 train_time:405324ms step_avg:170.02ms\nstep:2395/10000 train_loss:4.4610 train_time:405495ms step_avg:170.02ms\nstep:2396/10000 train_loss:4.4248 train_time:405662ms step_avg:170.02ms\nstep:2397/10000 train_loss:4.4440 train_time:405831ms step_avg:170.02ms\nstep:2398/10000 train_loss:4.5574 train_time:405998ms step_avg:170.02ms\nstep:2399/10000 train_loss:4.4087 train_time:406168ms step_avg:170.02ms\nstep:2400/10000 train_loss:4.2580 train_time:406338ms step_avg:170.02ms\nstep:2401/10000 train_loss:4.4192 train_time:406507ms step_avg:170.02ms\nstep:2402/10000 train_loss:4.4245 train_time:406680ms step_avg:170.02ms\nstep:2403/10000 train_loss:4.3684 train_time:406849ms step_avg:170.02ms\nstep:2404/10000 train_loss:4.3815 train_time:407016ms step_avg:170.02ms\nstep:2405/10000 train_loss:4.4841 train_time:407186ms step_avg:170.02ms\nstep:2406/10000 train_loss:4.3174 train_time:407358ms step_avg:170.02ms\nstep:2407/10000 train_loss:4.3179 train_time:407527ms step_avg:170.02ms\nstep:2408/10000 train_loss:4.3666 train_time:407696ms step_avg:170.02ms\nstep:2409/10000 train_loss:4.3170 train_time:407866ms step_avg:170.01ms\nstep:2410/10000 train_loss:4.4836 train_time:408035ms step_avg:170.01ms\nstep:2411/10000 train_loss:4.4395 train_time:408203ms step_avg:170.01ms\nstep:2412/10000 train_loss:4.3953 train_time:408373ms step_avg:170.01ms\nstep:2413/10000 train_loss:4.3082 train_time:408543ms step_avg:170.01ms\nstep:2414/10000 train_loss:4.3339 train_time:408713ms step_avg:170.01ms\nstep:2415/10000 train_loss:4.4670 train_time:408884ms step_avg:170.01ms\nstep:2416/10000 train_loss:4.5306 train_time:409055ms step_avg:170.01ms\nstep:2417/10000 train_loss:4.4236 train_time:409225ms step_avg:170.01ms\nstep:2418/10000 train_loss:4.4356 train_time:409395ms step_avg:170.01ms\nstep:2419/10000 train_loss:4.5409 train_time:409566ms step_avg:170.01ms\nstep:2420/10000 train_loss:4.4736 train_time:409734ms step_avg:170.01ms\nstep:2421/10000 train_loss:4.4441 train_time:409904ms step_avg:170.01ms\nstep:2422/10000 train_loss:4.6998 train_time:410078ms step_avg:170.02ms\nstep:2423/10000 train_loss:4.5294 train_time:410245ms step_avg:170.01ms\nstep:2424/10000 train_loss:4.3836 train_time:410417ms step_avg:170.02ms\nstep:2425/10000 train_loss:4.3023 train_time:410589ms step_avg:170.02ms\nstep:2426/10000 train_loss:4.4887 train_time:410761ms step_avg:170.02ms\nstep:2427/10000 train_loss:4.2443 train_time:410929ms step_avg:170.02ms\nstep:2428/10000 train_loss:4.2877 train_time:411099ms step_avg:170.02ms\nstep:2429/10000 train_loss:4.3722 train_time:411264ms step_avg:170.01ms\nstep:2430/10000 train_loss:4.5046 train_time:411434ms step_avg:170.01ms\nstep:2431/10000 train_loss:4.4067 train_time:411604ms step_avg:170.01ms\nstep:2432/10000 train_loss:4.5206 train_time:411775ms step_avg:170.01ms\nstep:2433/10000 train_loss:4.3410 train_time:411946ms step_avg:170.01ms\nstep:2434/10000 train_loss:4.4389 train_time:412118ms step_avg:170.02ms\nstep:2435/10000 train_loss:4.3922 train_time:412287ms step_avg:170.02ms\nstep:2436/10000 train_loss:4.3820 train_time:412460ms step_avg:170.02ms\nstep:2437/10000 train_loss:4.3949 train_time:412629ms step_avg:170.02ms\nstep:2438/10000 train_loss:4.3328 train_time:412799ms step_avg:170.02ms\nstep:2439/10000 train_loss:4.3575 train_time:412969ms step_avg:170.02ms\nstep:2440/10000 train_loss:4.3972 train_time:413140ms step_avg:170.02ms\nstep:2441/10000 train_loss:4.4302 train_time:413309ms step_avg:170.02ms\nstep:2442/10000 train_loss:4.4090 train_time:413480ms step_avg:170.02ms\nstep:2443/10000 train_loss:4.4025 train_time:413648ms step_avg:170.02ms\nstep:2444/10000 train_loss:4.4988 train_time:413819ms step_avg:170.02ms\nstep:2445/10000 train_loss:4.3396 train_time:413988ms step_avg:170.02ms\nstep:2446/10000 train_loss:4.3565 train_time:414158ms step_avg:170.02ms\nstep:2447/10000 train_loss:4.4083 train_time:414327ms step_avg:170.02ms\nstep:2448/10000 train_loss:4.4107 train_time:414496ms step_avg:170.01ms\nstep:2449/10000 train_loss:4.4863 train_time:414666ms step_avg:170.01ms\nstep:2450/10000 train_loss:4.5301 train_time:414838ms step_avg:170.02ms\nstep:2451/10000 train_loss:4.3177 train_time:415007ms step_avg:170.02ms\nstep:2452/10000 train_loss:4.3653 train_time:415178ms step_avg:170.02ms\nstep:2453/10000 train_loss:4.1198 train_time:415348ms step_avg:170.02ms\nstep:2454/10000 train_loss:4.3291 train_time:415518ms step_avg:170.02ms\nstep:2455/10000 train_loss:4.3843 train_time:415685ms step_avg:170.01ms\nstep:2456/10000 train_loss:4.5334 train_time:415855ms step_avg:170.01ms\nstep:2457/10000 train_loss:4.2931 train_time:416024ms step_avg:170.01ms\nstep:2458/10000 train_loss:4.4294 train_time:416197ms step_avg:170.02ms\nstep:2459/10000 train_loss:4.5259 train_time:416362ms step_avg:170.01ms\nstep:2460/10000 train_loss:4.5219 train_time:416533ms step_avg:170.01ms\nstep:2461/10000 train_loss:4.4294 train_time:416702ms step_avg:170.01ms\nstep:2462/10000 train_loss:4.3639 train_time:416872ms step_avg:170.01ms\nstep:2463/10000 train_loss:4.5267 train_time:417039ms step_avg:170.01ms\nstep:2464/10000 train_loss:4.4718 train_time:417209ms step_avg:170.01ms\nstep:2465/10000 train_loss:4.3928 train_time:417382ms step_avg:170.01ms\nstep:2466/10000 train_loss:4.4176 train_time:417550ms step_avg:170.01ms\nstep:2467/10000 train_loss:4.3557 train_time:417719ms step_avg:170.01ms\nstep:2468/10000 train_loss:4.3269 train_time:417888ms step_avg:170.01ms\nstep:2469/10000 train_loss:4.4862 train_time:418060ms step_avg:170.01ms\nstep:2470/10000 train_loss:4.4593 train_time:418228ms step_avg:170.01ms\nstep:2471/10000 train_loss:4.3379 train_time:418398ms step_avg:170.01ms\nstep:2472/10000 train_loss:4.3407 train_time:418567ms step_avg:170.01ms\nstep:2473/10000 train_loss:4.4901 train_time:418736ms step_avg:170.01ms\nstep:2474/10000 train_loss:4.2546 train_time:418905ms step_avg:170.01ms\nstep:2475/10000 train_loss:4.3989 train_time:419077ms step_avg:170.01ms\nstep:2476/10000 train_loss:4.4099 train_time:419246ms step_avg:170.01ms\nstep:2477/10000 train_loss:4.4774 train_time:419417ms step_avg:170.01ms\nstep:2478/10000 train_loss:4.5307 train_time:419584ms step_avg:170.01ms\nstep:2479/10000 train_loss:4.4017 train_time:419752ms step_avg:170.01ms\nstep:2480/10000 train_loss:4.3529 train_time:419921ms step_avg:170.01ms\nstep:2481/10000 train_loss:4.3629 train_time:420091ms step_avg:170.01ms\nstep:2482/10000 train_loss:4.4362 train_time:420262ms step_avg:170.01ms\nstep:2483/10000 train_loss:4.3195 train_time:420431ms step_avg:170.01ms\nstep:2484/10000 train_loss:4.3693 train_time:420599ms step_avg:170.01ms\nstep:2485/10000 train_loss:4.3832 train_time:420766ms step_avg:170.01ms\nstep:2486/10000 train_loss:4.4236 train_time:420936ms step_avg:170.01ms\nstep:2487/10000 train_loss:4.4220 train_time:421107ms step_avg:170.01ms\nstep:2488/10000 train_loss:4.6056 train_time:421278ms step_avg:170.01ms\nstep:2489/10000 train_loss:4.5955 train_time:421446ms step_avg:170.01ms\nstep:2490/10000 train_loss:4.2637 train_time:421615ms step_avg:170.01ms\nstep:2491/10000 train_loss:4.4221 train_time:421784ms step_avg:170.01ms\nstep:2492/10000 train_loss:4.3611 train_time:421952ms step_avg:170.00ms\nstep:2493/10000 train_loss:4.4784 train_time:422122ms step_avg:170.00ms\nstep:2494/10000 train_loss:4.4127 train_time:422296ms step_avg:170.01ms\nstep:2495/10000 train_loss:4.4472 train_time:422467ms step_avg:170.01ms\nstep:2496/10000 train_loss:4.4141 train_time:422637ms step_avg:170.01ms\nstep:2497/10000 train_loss:4.4382 train_time:422803ms step_avg:170.01ms\nstep:2498/10000 train_loss:4.4784 train_time:422972ms step_avg:170.00ms\nstep:2499/10000 train_loss:4.2347 train_time:423142ms step_avg:170.00ms\nstep:2500/10000 train_loss:4.4401 train_time:423313ms step_avg:170.01ms\nstep:2501/10000 train_loss:4.4648 train_time:423486ms step_avg:170.01ms\nstep:2502/10000 train_loss:4.3762 train_time:423657ms step_avg:170.01ms\nstep:2503/10000 train_loss:4.4039 train_time:423826ms step_avg:170.01ms\nstep:2504/10000 train_loss:4.5252 train_time:423997ms step_avg:170.01ms\nstep:2505/10000 train_loss:4.3529 train_time:424166ms step_avg:170.01ms\nstep:2506/10000 train_loss:4.3666 train_time:424340ms step_avg:170.01ms\nstep:2507/10000 train_loss:4.4388 train_time:424507ms step_avg:170.01ms\nstep:2508/10000 train_loss:4.5058 train_time:424679ms step_avg:170.01ms\nstep:2509/10000 train_loss:4.3464 train_time:424848ms step_avg:170.01ms\nstep:2510/10000 train_loss:4.2769 train_time:425018ms step_avg:170.01ms\nstep:2511/10000 train_loss:4.3839 train_time:425186ms step_avg:170.01ms\nstep:2512/10000 train_loss:4.4657 train_time:425355ms step_avg:170.01ms\nstep:2513/10000 train_loss:4.5363 train_time:425524ms step_avg:170.01ms\nstep:2514/10000 train_loss:4.4409 train_time:425693ms step_avg:170.01ms\nstep:2515/10000 train_loss:4.2998 train_time:425862ms step_avg:170.00ms\nstep:2516/10000 train_loss:4.5240 train_time:426032ms step_avg:170.00ms\nstep:2517/10000 train_loss:4.3503 train_time:426203ms step_avg:170.01ms\nstep:2518/10000 train_loss:4.2681 train_time:426374ms step_avg:170.01ms\nstep:2519/10000 train_loss:4.3568 train_time:426543ms step_avg:170.01ms\nstep:2520/10000 train_loss:4.3119 train_time:426711ms step_avg:170.00ms\nstep:2521/10000 train_loss:4.4016 train_time:426880ms step_avg:170.00ms\nstep:2522/10000 train_loss:4.4430 train_time:427050ms step_avg:170.00ms\nstep:2523/10000 train_loss:4.4816 train_time:427220ms step_avg:170.00ms\nstep:2524/10000 train_loss:4.4643 train_time:427389ms step_avg:170.00ms\nstep:2525/10000 train_loss:4.3383 train_time:427558ms step_avg:170.00ms\nstep:2526/10000 train_loss:4.2870 train_time:427726ms step_avg:170.00ms\nstep:2527/10000 train_loss:4.3575 train_time:427895ms step_avg:170.00ms\nstep:2528/10000 train_loss:4.5182 train_time:428064ms step_avg:170.00ms\nstep:2529/10000 train_loss:4.3703 train_time:428235ms step_avg:170.00ms\nstep:2530/10000 train_loss:4.4029 train_time:428403ms step_avg:170.00ms\nstep:2531/10000 train_loss:4.3374 train_time:428572ms step_avg:170.00ms\nstep:2532/10000 train_loss:4.4154 train_time:428739ms step_avg:170.00ms\nstep:2533/10000 train_loss:4.2782 train_time:428908ms step_avg:170.00ms\nstep:2534/10000 train_loss:4.5426 train_time:429077ms step_avg:170.00ms\nstep:2535/10000 train_loss:4.4980 train_time:429249ms step_avg:170.00ms\nstep:2536/10000 train_loss:4.2947 train_time:429421ms step_avg:170.00ms\nstep:2537/10000 train_loss:4.5376 train_time:429588ms step_avg:170.00ms\nstep:2538/10000 train_loss:4.3869 train_time:429760ms step_avg:170.00ms\nstep:2539/10000 train_loss:4.4509 train_time:429929ms step_avg:170.00ms\nstep:2540/10000 train_loss:4.3903 train_time:430102ms step_avg:170.00ms\nstep:2541/10000 train_loss:4.3297 train_time:430267ms step_avg:170.00ms\nstep:2542/10000 train_loss:4.2863 train_time:430439ms step_avg:170.00ms\nstep:2543/10000 train_loss:4.2374 train_time:430608ms step_avg:170.00ms\nstep:2544/10000 train_loss:4.6554 train_time:430776ms step_avg:170.00ms\nstep:2545/10000 train_loss:4.3844 train_time:430945ms step_avg:170.00ms\nstep:2546/10000 train_loss:4.3543 train_time:431115ms step_avg:170.00ms\nstep:2547/10000 train_loss:4.4426 train_time:431282ms step_avg:170.00ms\nstep:2548/10000 train_loss:4.2630 train_time:431451ms step_avg:170.00ms\nstep:2549/10000 train_loss:4.3210 train_time:431621ms step_avg:170.00ms\nstep:2550/10000 train_loss:4.2751 train_time:431790ms step_avg:170.00ms\nstep:2551/10000 train_loss:4.4966 train_time:431959ms step_avg:170.00ms\nstep:2552/10000 train_loss:4.1297 train_time:432127ms step_avg:169.99ms\nstep:2553/10000 train_loss:4.4027 train_time:432299ms step_avg:170.00ms\nstep:2554/10000 train_loss:4.3175 train_time:432468ms step_avg:170.00ms\nstep:2555/10000 train_loss:4.3150 train_time:432639ms step_avg:170.00ms\nstep:2556/10000 train_loss:4.3218 train_time:432807ms step_avg:169.99ms\nstep:2557/10000 train_loss:4.3764 train_time:432976ms step_avg:169.99ms\nstep:2558/10000 train_loss:4.3830 train_time:433146ms step_avg:169.99ms\nstep:2559/10000 train_loss:4.4513 train_time:433317ms step_avg:169.99ms\nstep:2560/10000 train_loss:4.3146 train_time:433486ms step_avg:169.99ms\nstep:2561/10000 train_loss:4.2398 train_time:433657ms step_avg:169.99ms\nstep:2562/10000 train_loss:4.2878 train_time:433824ms step_avg:169.99ms\nstep:2563/10000 train_loss:4.4554 train_time:433997ms step_avg:169.99ms\nstep:2564/10000 train_loss:4.3479 train_time:434166ms step_avg:169.99ms\nstep:2565/10000 train_loss:4.4349 train_time:434339ms step_avg:170.00ms\nstep:2566/10000 train_loss:4.3134 train_time:434508ms step_avg:170.00ms\nstep:2567/10000 train_loss:4.3867 train_time:434680ms step_avg:170.00ms\nstep:2568/10000 train_loss:4.3392 train_time:434851ms step_avg:170.00ms\nstep:2569/10000 train_loss:4.4162 train_time:435022ms step_avg:170.00ms\nstep:2570/10000 train_loss:4.4672 train_time:435194ms step_avg:170.00ms\nstep:2571/10000 train_loss:4.3435 train_time:435363ms step_avg:170.00ms\nstep:2572/10000 train_loss:4.3548 train_time:435532ms step_avg:170.00ms\nstep:2573/10000 train_loss:4.1974 train_time:435701ms step_avg:170.00ms\nstep:2574/10000 train_loss:4.1709 train_time:435870ms step_avg:170.00ms\nstep:2575/10000 train_loss:4.4123 train_time:436041ms step_avg:170.00ms\nstep:2576/10000 train_loss:4.3982 train_time:436210ms step_avg:170.00ms\nstep:2577/10000 train_loss:4.3505 train_time:436382ms step_avg:170.00ms\nstep:2578/10000 train_loss:4.3318 train_time:436551ms step_avg:170.00ms\nstep:2579/10000 train_loss:4.5194 train_time:436720ms step_avg:170.00ms\nstep:2580/10000 train_loss:4.3017 train_time:436887ms step_avg:169.99ms\nstep:2581/10000 train_loss:4.4742 train_time:437057ms step_avg:169.99ms\nstep:2582/10000 train_loss:4.2928 train_time:437226ms step_avg:169.99ms\nstep:2583/10000 train_loss:4.4431 train_time:437397ms step_avg:169.99ms\nstep:2584/10000 train_loss:4.5109 train_time:437565ms step_avg:169.99ms\nstep:2585/10000 train_loss:4.3016 train_time:437737ms step_avg:169.99ms\nstep:2586/10000 train_loss:4.4467 train_time:437905ms step_avg:169.99ms\nstep:2587/10000 train_loss:4.3109 train_time:438076ms step_avg:169.99ms\nstep:2588/10000 train_loss:4.3944 train_time:438246ms step_avg:169.99ms\nstep:2589/10000 train_loss:4.3660 train_time:438417ms step_avg:169.99ms\nstep:2590/10000 train_loss:4.3593 train_time:438585ms step_avg:169.99ms\nstep:2591/10000 train_loss:4.3139 train_time:438757ms step_avg:170.00ms\nstep:2592/10000 train_loss:4.4119 train_time:438927ms step_avg:169.99ms\nstep:2593/10000 train_loss:4.3477 train_time:439098ms step_avg:170.00ms\nstep:2594/10000 train_loss:4.6276 train_time:439265ms step_avg:169.99ms\nstep:2595/10000 train_loss:4.6656 train_time:439435ms step_avg:169.99ms\nstep:2596/10000 train_loss:4.3894 train_time:439604ms step_avg:169.99ms\nstep:2597/10000 train_loss:4.4807 train_time:439773ms step_avg:169.99ms\nstep:2598/10000 train_loss:4.3950 train_time:439943ms step_avg:169.99ms\nstep:2599/10000 train_loss:4.3694 train_time:440113ms step_avg:169.99ms\nstep:2600/10000 train_loss:4.4992 train_time:440281ms step_avg:169.99ms\nstep:2601/10000 train_loss:4.4140 train_time:440452ms step_avg:169.99ms\nstep:2602/10000 train_loss:4.2742 train_time:440623ms step_avg:169.99ms\nstep:2603/10000 train_loss:4.4484 train_time:440793ms step_avg:169.99ms\nstep:2604/10000 train_loss:4.3652 train_time:440963ms step_avg:169.99ms\nstep:2605/10000 train_loss:4.4874 train_time:441136ms step_avg:169.99ms\nstep:2606/10000 train_loss:4.2590 train_time:441305ms step_avg:169.99ms\nstep:2607/10000 train_loss:4.3651 train_time:441475ms step_avg:169.99ms\nstep:2608/10000 train_loss:4.4730 train_time:441646ms step_avg:169.99ms\nstep:2609/10000 train_loss:4.2431 train_time:441816ms step_avg:169.99ms\nstep:2610/10000 train_loss:4.3864 train_time:441984ms step_avg:169.99ms\nstep:2611/10000 train_loss:4.3212 train_time:442152ms step_avg:169.99ms\nstep:2612/10000 train_loss:4.3349 train_time:442323ms step_avg:169.99ms\nstep:2613/10000 train_loss:4.2982 train_time:442493ms step_avg:169.99ms\nstep:2614/10000 train_loss:4.3408 train_time:442664ms step_avg:169.99ms\nstep:2615/10000 train_loss:4.3363 train_time:442834ms step_avg:169.99ms\nstep:2616/10000 train_loss:4.3971 train_time:443002ms step_avg:169.99ms\nstep:2617/10000 train_loss:4.3402 train_time:443172ms step_avg:169.99ms\nstep:2618/10000 train_loss:4.4800 train_time:443340ms step_avg:169.99ms\nstep:2619/10000 train_loss:4.2970 train_time:443511ms step_avg:169.99ms\nstep:2620/10000 train_loss:4.1657 train_time:443679ms step_avg:169.99ms\nstep:2621/10000 train_loss:4.4525 train_time:443849ms step_avg:169.99ms\nstep:2622/10000 train_loss:4.5227 train_time:444020ms step_avg:169.99ms\nstep:2623/10000 train_loss:4.5068 train_time:444188ms step_avg:169.99ms\nstep:2624/10000 train_loss:4.3018 train_time:444359ms step_avg:169.99ms\nstep:2625/10000 train_loss:4.2030 train_time:444526ms step_avg:169.99ms\nstep:2626/10000 train_loss:4.4017 train_time:444695ms step_avg:169.99ms\nstep:2627/10000 train_loss:4.3953 train_time:444864ms step_avg:169.99ms\nstep:2628/10000 train_loss:4.2967 train_time:445035ms step_avg:169.99ms\nstep:2629/10000 train_loss:4.4144 train_time:445203ms step_avg:169.99ms\nstep:2630/10000 train_loss:4.3497 train_time:445376ms step_avg:169.99ms\nstep:2631/10000 train_loss:4.3702 train_time:445544ms step_avg:169.99ms\nstep:2632/10000 train_loss:4.2651 train_time:445714ms step_avg:169.99ms\nstep:2633/10000 train_loss:4.1128 train_time:445882ms step_avg:169.99ms\nstep:2634/10000 train_loss:4.2375 train_time:446052ms step_avg:169.99ms\nstep:2635/10000 train_loss:4.4008 train_time:446222ms step_avg:169.99ms\nstep:2636/10000 train_loss:4.4255 train_time:446391ms step_avg:169.99ms\nstep:2637/10000 train_loss:4.2990 train_time:446558ms step_avg:169.99ms\nstep:2638/10000 train_loss:4.3763 train_time:446726ms step_avg:169.99ms\nstep:2639/10000 train_loss:4.3395 train_time:446896ms step_avg:169.99ms\nstep:2640/10000 train_loss:4.3318 train_time:447065ms step_avg:169.99ms\nstep:2641/10000 train_loss:4.2781 train_time:447237ms step_avg:169.99ms\nstep:2642/10000 train_loss:4.4705 train_time:447407ms step_avg:169.99ms\nstep:2643/10000 train_loss:4.4170 train_time:447578ms step_avg:169.99ms\nstep:2644/10000 train_loss:4.3834 train_time:447744ms step_avg:169.99ms\nstep:2645/10000 train_loss:4.3639 train_time:447914ms step_avg:169.99ms\nstep:2646/10000 train_loss:4.3974 train_time:448082ms step_avg:169.99ms\nstep:2647/10000 train_loss:4.3878 train_time:448251ms step_avg:169.99ms\nstep:2648/10000 train_loss:4.4610 train_time:448422ms step_avg:169.99ms\nstep:2649/10000 train_loss:4.4646 train_time:448599ms step_avg:169.99ms\nstep:2650/10000 train_loss:4.2479 train_time:448763ms step_avg:169.99ms\nstep:2651/10000 train_loss:4.4354 train_time:448934ms step_avg:169.99ms\nstep:2652/10000 train_loss:4.3899 train_time:449106ms step_avg:169.99ms\nstep:2653/10000 train_loss:4.3097 train_time:449276ms step_avg:169.99ms\nstep:2654/10000 train_loss:4.3758 train_time:449446ms step_avg:169.99ms\nstep:2655/10000 train_loss:4.3363 train_time:449615ms step_avg:169.99ms\nstep:2656/10000 train_loss:4.2729 train_time:449782ms step_avg:169.99ms\nstep:2657/10000 train_loss:4.3011 train_time:449955ms step_avg:169.99ms\nstep:2658/10000 train_loss:4.3238 train_time:450124ms step_avg:169.99ms\nstep:2659/10000 train_loss:4.2920 train_time:450290ms step_avg:169.99ms\nstep:2660/10000 train_loss:4.3552 train_time:450462ms step_avg:169.99ms\nstep:2661/10000 train_loss:4.3085 train_time:450629ms step_avg:169.98ms\nstep:2662/10000 train_loss:4.2535 train_time:450797ms step_avg:169.98ms\nstep:2663/10000 train_loss:4.3686 train_time:450965ms step_avg:169.98ms\nstep:2664/10000 train_loss:4.2148 train_time:451136ms step_avg:169.98ms\nstep:2665/10000 train_loss:4.5084 train_time:451305ms step_avg:169.98ms\nstep:2666/10000 train_loss:4.3929 train_time:451476ms step_avg:169.98ms\nstep:2667/10000 train_loss:4.2439 train_time:451647ms step_avg:169.98ms\nstep:2668/10000 train_loss:4.5100 train_time:451818ms step_avg:169.98ms\nstep:2669/10000 train_loss:4.2652 train_time:451986ms step_avg:169.98ms\nstep:2670/10000 train_loss:4.4401 train_time:452157ms step_avg:169.98ms\nstep:2671/10000 train_loss:4.3929 train_time:452325ms step_avg:169.98ms\nstep:2672/10000 train_loss:4.3645 train_time:452496ms step_avg:169.98ms\nstep:2673/10000 train_loss:4.3462 train_time:452665ms step_avg:169.98ms\nstep:2674/10000 train_loss:4.4450 train_time:452833ms step_avg:169.98ms\nstep:2675/10000 train_loss:4.4379 train_time:453004ms step_avg:169.98ms\nstep:2676/10000 train_loss:4.3190 train_time:453175ms step_avg:169.98ms\nstep:2677/10000 train_loss:4.4817 train_time:453343ms step_avg:169.98ms\nstep:2678/10000 train_loss:4.2719 train_time:453513ms step_avg:169.98ms\nstep:2679/10000 train_loss:4.3495 train_time:453684ms step_avg:169.98ms\nstep:2680/10000 train_loss:4.1980 train_time:453852ms step_avg:169.98ms\nstep:2681/10000 train_loss:4.3183 train_time:454022ms step_avg:169.98ms\nstep:2682/10000 train_loss:4.3798 train_time:454197ms step_avg:169.98ms\nstep:2683/10000 train_loss:4.3169 train_time:454363ms step_avg:169.98ms\nstep:2684/10000 train_loss:4.3073 train_time:454532ms step_avg:169.98ms\nstep:2685/10000 train_loss:4.3027 train_time:454700ms step_avg:169.98ms\nstep:2686/10000 train_loss:4.3399 train_time:454869ms step_avg:169.98ms\nstep:2687/10000 train_loss:4.5523 train_time:455039ms step_avg:169.98ms\nstep:2688/10000 train_loss:4.2635 train_time:455208ms step_avg:169.98ms\nstep:2689/10000 train_loss:4.2842 train_time:455378ms step_avg:169.98ms\nstep:2690/10000 train_loss:4.1589 train_time:455549ms step_avg:169.98ms\nstep:2691/10000 train_loss:4.4451 train_time:455722ms step_avg:169.98ms\nstep:2692/10000 train_loss:4.3089 train_time:455889ms step_avg:169.98ms\nstep:2693/10000 train_loss:4.3337 train_time:456060ms step_avg:169.98ms\nstep:2694/10000 train_loss:4.6053 train_time:456229ms step_avg:169.98ms\nstep:2695/10000 train_loss:4.2925 train_time:456396ms step_avg:169.98ms\nstep:2696/10000 train_loss:4.5640 train_time:456563ms step_avg:169.98ms\nstep:2697/10000 train_loss:4.2524 train_time:456735ms step_avg:169.98ms\nstep:2698/10000 train_loss:4.4523 train_time:456903ms step_avg:169.98ms\nstep:2699/10000 train_loss:4.3001 train_time:457072ms step_avg:169.98ms\nstep:2700/10000 train_loss:4.3876 train_time:457240ms step_avg:169.98ms\nstep:2701/10000 train_loss:4.3036 train_time:457410ms step_avg:169.98ms\nstep:2702/10000 train_loss:4.3643 train_time:457579ms step_avg:169.98ms\nstep:2703/10000 train_loss:4.2204 train_time:457748ms step_avg:169.98ms\nstep:2704/10000 train_loss:4.4037 train_time:457918ms step_avg:169.98ms\nstep:2705/10000 train_loss:4.3040 train_time:458087ms step_avg:169.98ms\nstep:2706/10000 train_loss:4.3660 train_time:458257ms step_avg:169.98ms\nstep:2707/10000 train_loss:4.3189 train_time:458425ms step_avg:169.98ms\nstep:2708/10000 train_loss:4.1886 train_time:458595ms step_avg:169.98ms\nstep:2709/10000 train_loss:4.3443 train_time:458765ms step_avg:169.98ms\nstep:2710/10000 train_loss:4.3320 train_time:458935ms step_avg:169.98ms\nstep:2711/10000 train_loss:4.3339 train_time:459104ms step_avg:169.98ms\nstep:2712/10000 train_loss:4.2810 train_time:459275ms step_avg:169.98ms\nstep:2713/10000 train_loss:4.2678 train_time:459447ms step_avg:169.98ms\nstep:2714/10000 train_loss:4.2221 train_time:459618ms step_avg:169.98ms\nstep:2715/10000 train_loss:4.3600 train_time:459785ms step_avg:169.98ms\nstep:2716/10000 train_loss:4.2420 train_time:459954ms step_avg:169.98ms\nstep:2717/10000 train_loss:4.1448 train_time:460123ms step_avg:169.98ms\nstep:2718/10000 train_loss:4.3758 train_time:460291ms step_avg:169.97ms\nstep:2719/10000 train_loss:4.3954 train_time:460461ms step_avg:169.97ms\nstep:2720/10000 train_loss:4.4066 train_time:460630ms step_avg:169.97ms\nstep:2721/10000 train_loss:4.1861 train_time:460799ms step_avg:169.97ms\nstep:2722/10000 train_loss:4.3792 train_time:460969ms step_avg:169.97ms\nstep:2723/10000 train_loss:4.2512 train_time:461140ms step_avg:169.97ms\nstep:2724/10000 train_loss:4.3067 train_time:461308ms step_avg:169.97ms\nstep:2725/10000 train_loss:4.3157 train_time:461477ms step_avg:169.97ms\nstep:2726/10000 train_loss:4.4873 train_time:461645ms step_avg:169.97ms\nstep:2727/10000 train_loss:4.4015 train_time:461817ms step_avg:169.97ms\nstep:2728/10000 train_loss:4.3298 train_time:461985ms step_avg:169.97ms\nstep:2729/10000 train_loss:4.2731 train_time:462155ms step_avg:169.97ms\nstep:2730/10000 train_loss:4.2789 train_time:462325ms step_avg:169.97ms\nstep:2731/10000 train_loss:4.2770 train_time:462494ms step_avg:169.97ms\nstep:2732/10000 train_loss:4.4076 train_time:462664ms step_avg:169.97ms\nstep:2733/10000 train_loss:4.4489 train_time:462836ms step_avg:169.97ms\nstep:2734/10000 train_loss:4.3425 train_time:463007ms step_avg:169.97ms\nstep:2735/10000 train_loss:4.2857 train_time:463177ms step_avg:169.97ms\nstep:2736/10000 train_loss:4.4627 train_time:463345ms step_avg:169.97ms\nstep:2737/10000 train_loss:4.8917 train_time:463513ms step_avg:169.97ms\nstep:2738/10000 train_loss:4.3399 train_time:463683ms step_avg:169.97ms\nstep:2739/10000 train_loss:4.1535 train_time:463853ms step_avg:169.97ms\nstep:2740/10000 train_loss:4.3462 train_time:464021ms step_avg:169.97ms\nstep:2741/10000 train_loss:4.5655 train_time:464192ms step_avg:169.97ms\nstep:2742/10000 train_loss:4.2789 train_time:464361ms step_avg:169.97ms\nstep:2743/10000 train_loss:4.3937 train_time:464529ms step_avg:169.97ms\nstep:2744/10000 train_loss:4.4341 train_time:464701ms step_avg:169.97ms\nstep:2745/10000 train_loss:4.3395 train_time:464870ms step_avg:169.97ms\nstep:2746/10000 train_loss:4.4156 train_time:465039ms step_avg:169.97ms\nstep:2747/10000 train_loss:4.3069 train_time:465208ms step_avg:169.97ms\nstep:2748/10000 train_loss:4.3711 train_time:465378ms step_avg:169.97ms\nstep:2749/10000 train_loss:4.2698 train_time:465545ms step_avg:169.97ms\nstep:2750/10000 train_loss:4.3159 train_time:465717ms step_avg:169.97ms\nstep:2751/10000 train_loss:4.4476 train_time:465885ms step_avg:169.97ms\nstep:2752/10000 train_loss:4.3868 train_time:466057ms step_avg:169.97ms\nstep:2753/10000 train_loss:4.2617 train_time:466225ms step_avg:169.97ms\nstep:2754/10000 train_loss:4.2285 train_time:466397ms step_avg:169.97ms\nstep:2755/10000 train_loss:4.2728 train_time:466568ms step_avg:169.97ms\nstep:2756/10000 train_loss:4.1152 train_time:466741ms step_avg:169.97ms\nstep:2757/10000 train_loss:4.2574 train_time:466910ms step_avg:169.97ms\nstep:2758/10000 train_loss:4.3651 train_time:467082ms step_avg:169.97ms\nstep:2759/10000 train_loss:4.5072 train_time:467251ms step_avg:169.97ms\nstep:2760/10000 train_loss:4.1691 train_time:467423ms step_avg:169.97ms\nstep:2761/10000 train_loss:4.2630 train_time:467593ms step_avg:169.97ms\nstep:2762/10000 train_loss:4.1827 train_time:467766ms step_avg:169.97ms\nstep:2763/10000 train_loss:4.2912 train_time:467936ms step_avg:169.97ms\nstep:2764/10000 train_loss:4.2746 train_time:468106ms step_avg:169.97ms\nstep:2765/10000 train_loss:4.3694 train_time:468275ms step_avg:169.97ms\nstep:2766/10000 train_loss:4.3814 train_time:468445ms step_avg:169.97ms\nstep:2767/10000 train_loss:4.2736 train_time:468617ms step_avg:169.97ms\nstep:2768/10000 train_loss:4.1922 train_time:468784ms step_avg:169.97ms\nstep:2769/10000 train_loss:4.3319 train_time:468952ms step_avg:169.97ms\nstep:2770/10000 train_loss:4.3798 train_time:469123ms step_avg:169.97ms\nstep:2771/10000 train_loss:4.2889 train_time:469293ms step_avg:169.97ms\nstep:2772/10000 train_loss:4.4002 train_time:469464ms step_avg:169.97ms\nstep:2773/10000 train_loss:4.2878 train_time:469635ms step_avg:169.97ms\nstep:2774/10000 train_loss:4.1731 train_time:469807ms step_avg:169.97ms\nstep:2775/10000 train_loss:4.2995 train_time:469977ms step_avg:169.97ms\nstep:2776/10000 train_loss:4.3916 train_time:470146ms step_avg:169.97ms\nstep:2777/10000 train_loss:4.1294 train_time:470316ms step_avg:169.97ms\nstep:2778/10000 train_loss:4.3693 train_time:470484ms step_avg:169.97ms\nstep:2779/10000 train_loss:4.3536 train_time:470653ms step_avg:169.97ms\nstep:2780/10000 train_loss:4.2680 train_time:470825ms step_avg:169.97ms\nstep:2781/10000 train_loss:4.2280 train_time:470996ms step_avg:169.97ms\nstep:2782/10000 train_loss:4.3373 train_time:471165ms step_avg:169.97ms\nstep:2783/10000 train_loss:4.3697 train_time:471339ms step_avg:169.97ms\nstep:2784/10000 train_loss:4.2490 train_time:471507ms step_avg:169.97ms\nstep:2785/10000 train_loss:4.2318 train_time:471676ms step_avg:169.97ms\nstep:2786/10000 train_loss:4.0523 train_time:471843ms step_avg:169.97ms\nstep:2787/10000 train_loss:4.2910 train_time:472012ms step_avg:169.97ms\nstep:2788/10000 train_loss:4.5404 train_time:472182ms step_avg:169.97ms\nstep:2789/10000 train_loss:4.3449 train_time:472353ms step_avg:169.97ms\nstep:2790/10000 train_loss:4.4461 train_time:472525ms step_avg:169.97ms\nstep:2791/10000 train_loss:4.3534 train_time:472695ms step_avg:169.97ms\nstep:2792/10000 train_loss:4.3505 train_time:472864ms step_avg:169.97ms\nstep:2793/10000 train_loss:4.1842 train_time:473035ms step_avg:169.97ms\nstep:2794/10000 train_loss:4.3099 train_time:473202ms step_avg:169.97ms\nstep:2795/10000 train_loss:4.2452 train_time:473371ms step_avg:169.97ms\nstep:2796/10000 train_loss:4.3186 train_time:473542ms step_avg:169.97ms\nstep:2797/10000 train_loss:4.1621 train_time:473711ms step_avg:169.97ms\nstep:2798/10000 train_loss:4.2813 train_time:473879ms step_avg:169.97ms\nstep:2799/10000 train_loss:4.2554 train_time:474048ms step_avg:169.97ms\nstep:2800/10000 train_loss:4.5259 train_time:474218ms step_avg:169.97ms\nstep:2801/10000 train_loss:4.2939 train_time:474385ms step_avg:169.97ms\nstep:2802/10000 train_loss:4.2471 train_time:474555ms step_avg:169.97ms\nstep:2803/10000 train_loss:4.5701 train_time:474726ms step_avg:169.97ms\nstep:2804/10000 train_loss:4.2776 train_time:474895ms step_avg:169.97ms\nstep:2805/10000 train_loss:4.1605 train_time:475063ms step_avg:169.97ms\nstep:2806/10000 train_loss:4.2581 train_time:475232ms step_avg:169.97ms\nstep:2807/10000 train_loss:4.1647 train_time:475401ms step_avg:169.97ms\nstep:2808/10000 train_loss:4.1984 train_time:475570ms step_avg:169.97ms\nstep:2809/10000 train_loss:4.3932 train_time:475739ms step_avg:169.97ms\nstep:2810/10000 train_loss:3.8385 train_time:475907ms step_avg:169.97ms\nstep:2811/10000 train_loss:4.3853 train_time:476077ms step_avg:169.97ms\nstep:2812/10000 train_loss:4.2342 train_time:476245ms step_avg:169.97ms\nstep:2813/10000 train_loss:4.2903 train_time:476418ms step_avg:169.97ms\nstep:2814/10000 train_loss:4.3000 train_time:476585ms step_avg:169.97ms\nstep:2815/10000 train_loss:4.2767 train_time:476755ms step_avg:169.97ms\nstep:2816/10000 train_loss:4.1497 train_time:476923ms step_avg:169.97ms\nstep:2817/10000 train_loss:4.2683 train_time:477093ms step_avg:169.97ms\nstep:2818/10000 train_loss:4.1664 train_time:477264ms step_avg:169.97ms\nstep:2819/10000 train_loss:4.3002 train_time:477435ms step_avg:169.97ms\nstep:2820/10000 train_loss:4.2595 train_time:477605ms step_avg:169.97ms\nstep:2821/10000 train_loss:4.4099 train_time:477778ms step_avg:169.97ms\nstep:2822/10000 train_loss:4.3895 train_time:477947ms step_avg:169.97ms\nstep:2823/10000 train_loss:4.3888 train_time:478117ms step_avg:169.97ms\nstep:2824/10000 train_loss:4.2750 train_time:478285ms step_avg:169.97ms\nstep:2825/10000 train_loss:4.3579 train_time:478455ms step_avg:169.97ms\nstep:2826/10000 train_loss:4.2418 train_time:478624ms step_avg:169.97ms\nstep:2827/10000 train_loss:4.2874 train_time:478795ms step_avg:169.97ms\nstep:2828/10000 train_loss:4.2914 train_time:478964ms step_avg:169.97ms\nstep:2829/10000 train_loss:4.1750 train_time:479134ms step_avg:169.97ms\nstep:2830/10000 train_loss:4.3389 train_time:479304ms step_avg:169.97ms\nstep:2831/10000 train_loss:4.2837 train_time:479474ms step_avg:169.97ms\nstep:2832/10000 train_loss:4.3492 train_time:479640ms step_avg:169.96ms\nstep:2833/10000 train_loss:4.2838 train_time:479810ms step_avg:169.96ms\nstep:2834/10000 train_loss:4.2882 train_time:479979ms step_avg:169.96ms\nstep:2835/10000 train_loss:4.7248 train_time:480147ms step_avg:169.96ms\nstep:2836/10000 train_loss:4.2882 train_time:480318ms step_avg:169.96ms\nstep:2837/10000 train_loss:4.2156 train_time:480486ms step_avg:169.96ms\nstep:2838/10000 train_loss:4.3719 train_time:480656ms step_avg:169.96ms\nstep:2839/10000 train_loss:4.3315 train_time:480824ms step_avg:169.96ms\nstep:2840/10000 train_loss:4.2589 train_time:480995ms step_avg:169.96ms\nstep:2841/10000 train_loss:4.3424 train_time:481163ms step_avg:169.96ms\nstep:2842/10000 train_loss:4.3426 train_time:481335ms step_avg:169.96ms\nstep:2843/10000 train_loss:4.2496 train_time:481506ms step_avg:169.96ms\nstep:2844/10000 train_loss:4.2830 train_time:481676ms step_avg:169.96ms\nstep:2845/10000 train_loss:4.3392 train_time:481845ms step_avg:169.96ms\nstep:2846/10000 train_loss:4.3125 train_time:482015ms step_avg:169.96ms\nstep:2847/10000 train_loss:4.4234 train_time:482181ms step_avg:169.96ms\nstep:2848/10000 train_loss:4.2977 train_time:482352ms step_avg:169.96ms\nstep:2849/10000 train_loss:4.2784 train_time:482523ms step_avg:169.96ms\nstep:2850/10000 train_loss:4.2447 train_time:482695ms step_avg:169.96ms\nstep:2851/10000 train_loss:4.3511 train_time:482863ms step_avg:169.96ms\nstep:2852/10000 train_loss:4.3439 train_time:483031ms step_avg:169.96ms\nstep:2853/10000 train_loss:4.2781 train_time:483204ms step_avg:169.96ms\nstep:2854/10000 train_loss:4.2586 train_time:483375ms step_avg:169.96ms\nstep:2855/10000 train_loss:4.2072 train_time:483544ms step_avg:169.96ms\nstep:2856/10000 train_loss:4.4226 train_time:483714ms step_avg:169.96ms\nstep:2857/10000 train_loss:4.3088 train_time:483886ms step_avg:169.96ms\nstep:2858/10000 train_loss:4.3040 train_time:484061ms step_avg:169.97ms\nstep:2859/10000 train_loss:4.4580 train_time:484222ms step_avg:169.96ms\nstep:2860/10000 train_loss:4.2615 train_time:484391ms step_avg:169.96ms\nstep:2861/10000 train_loss:4.2424 train_time:484563ms step_avg:169.96ms\nstep:2862/10000 train_loss:4.1375 train_time:484733ms step_avg:169.96ms\nstep:2863/10000 train_loss:4.2097 train_time:484903ms step_avg:169.96ms\nstep:2864/10000 train_loss:4.2812 train_time:485074ms step_avg:169.96ms\nstep:2865/10000 train_loss:4.2463 train_time:485243ms step_avg:169.96ms\nstep:2866/10000 train_loss:4.5276 train_time:485412ms step_avg:169.96ms\nstep:2867/10000 train_loss:4.4824 train_time:485580ms step_avg:169.96ms\nstep:2868/10000 train_loss:4.3050 train_time:485749ms step_avg:169.96ms\nstep:2869/10000 train_loss:4.0876 train_time:485917ms step_avg:169.96ms\nstep:2870/10000 train_loss:4.4129 train_time:486085ms step_avg:169.96ms\nstep:2871/10000 train_loss:4.2469 train_time:486253ms step_avg:169.96ms\nstep:2872/10000 train_loss:4.4304 train_time:486422ms step_avg:169.96ms\nstep:2873/10000 train_loss:4.2627 train_time:486592ms step_avg:169.96ms\nstep:2874/10000 train_loss:4.4041 train_time:486763ms step_avg:169.96ms\nstep:2875/10000 train_loss:4.2980 train_time:486933ms step_avg:169.96ms\nstep:2876/10000 train_loss:4.2865 train_time:487104ms step_avg:169.96ms\nstep:2877/10000 train_loss:4.3414 train_time:487273ms step_avg:169.96ms\nstep:2878/10000 train_loss:4.3729 train_time:487442ms step_avg:169.96ms\nstep:2879/10000 train_loss:4.3158 train_time:487611ms step_avg:169.96ms\nstep:2880/10000 train_loss:4.3372 train_time:487780ms step_avg:169.96ms\nstep:2881/10000 train_loss:4.1713 train_time:487951ms step_avg:169.96ms\nstep:2882/10000 train_loss:4.3160 train_time:488122ms step_avg:169.96ms\nstep:2883/10000 train_loss:4.1610 train_time:488292ms step_avg:169.96ms\nstep:2884/10000 train_loss:4.4310 train_time:488461ms step_avg:169.96ms\nstep:2885/10000 train_loss:4.4400 train_time:488630ms step_avg:169.96ms\nstep:2886/10000 train_loss:4.1940 train_time:488800ms step_avg:169.96ms\nstep:2887/10000 train_loss:4.2500 train_time:488968ms step_avg:169.96ms\nstep:2888/10000 train_loss:4.1479 train_time:489137ms step_avg:169.96ms\nstep:2889/10000 train_loss:4.3702 train_time:489306ms step_avg:169.96ms\nstep:2890/10000 train_loss:4.2926 train_time:489476ms step_avg:169.96ms\nstep:2891/10000 train_loss:4.4837 train_time:489644ms step_avg:169.96ms\nstep:2892/10000 train_loss:4.4207 train_time:489815ms step_avg:169.96ms\nstep:2893/10000 train_loss:4.2594 train_time:489983ms step_avg:169.96ms\nstep:2894/10000 train_loss:4.2740 train_time:490151ms step_avg:169.96ms\nstep:2895/10000 train_loss:4.4330 train_time:490320ms step_avg:169.95ms\nstep:2896/10000 train_loss:4.4597 train_time:490489ms step_avg:169.95ms\nstep:2897/10000 train_loss:4.3210 train_time:490659ms step_avg:169.95ms\nstep:2898/10000 train_loss:4.3042 train_time:490828ms step_avg:169.95ms\nstep:2899/10000 train_loss:4.0790 train_time:490999ms step_avg:169.95ms\nstep:2900/10000 train_loss:4.2480 train_time:491166ms step_avg:169.95ms\nstep:2901/10000 train_loss:4.2537 train_time:491338ms step_avg:169.95ms\nstep:2902/10000 train_loss:4.2979 train_time:491507ms step_avg:169.95ms\nstep:2903/10000 train_loss:4.3280 train_time:491679ms step_avg:169.95ms\nstep:2904/10000 train_loss:4.3326 train_time:491850ms step_avg:169.95ms\nstep:2905/10000 train_loss:4.2163 train_time:492021ms step_avg:169.96ms\nstep:2906/10000 train_loss:4.1824 train_time:492189ms step_avg:169.95ms\nstep:2907/10000 train_loss:4.1204 train_time:492360ms step_avg:169.96ms\nstep:2908/10000 train_loss:4.2519 train_time:492528ms step_avg:169.95ms\nstep:2909/10000 train_loss:4.2030 train_time:492699ms step_avg:169.95ms\nstep:2910/10000 train_loss:4.3335 train_time:492868ms step_avg:169.95ms\nstep:2911/10000 train_loss:4.3419 train_time:493036ms step_avg:169.95ms\nstep:2912/10000 train_loss:4.2018 train_time:493204ms step_avg:169.95ms\nstep:2913/10000 train_loss:4.3215 train_time:493375ms step_avg:169.95ms\nstep:2914/10000 train_loss:4.2707 train_time:493545ms step_avg:169.95ms\nstep:2915/10000 train_loss:4.2636 train_time:493714ms step_avg:169.95ms\nstep:2916/10000 train_loss:4.4035 train_time:493883ms step_avg:169.95ms\nstep:2917/10000 train_loss:4.2679 train_time:494054ms step_avg:169.95ms\nstep:2918/10000 train_loss:4.0673 train_time:494222ms step_avg:169.95ms\nstep:2919/10000 train_loss:4.1146 train_time:494392ms step_avg:169.95ms\nstep:2920/10000 train_loss:4.4088 train_time:494563ms step_avg:169.95ms\nstep:2921/10000 train_loss:4.3033 train_time:494731ms step_avg:169.95ms\nstep:2922/10000 train_loss:4.2017 train_time:494900ms step_avg:169.95ms\nstep:2923/10000 train_loss:4.3321 train_time:495068ms step_avg:169.95ms\nstep:2924/10000 train_loss:4.3953 train_time:495241ms step_avg:169.95ms\nstep:2925/10000 train_loss:4.3014 train_time:495411ms step_avg:169.95ms\nstep:2926/10000 train_loss:4.3330 train_time:495583ms step_avg:169.95ms\nstep:2927/10000 train_loss:4.3423 train_time:495753ms step_avg:169.95ms\nstep:2928/10000 train_loss:4.2410 train_time:495924ms step_avg:169.95ms\nstep:2929/10000 train_loss:4.3259 train_time:496094ms step_avg:169.95ms\nstep:2930/10000 train_loss:4.3050 train_time:496263ms step_avg:169.95ms\nstep:2931/10000 train_loss:4.3320 train_time:496432ms step_avg:169.95ms\nstep:2932/10000 train_loss:4.2925 train_time:496603ms step_avg:169.95ms\nstep:2933/10000 train_loss:4.2125 train_time:496773ms step_avg:169.95ms\nstep:2934/10000 train_loss:4.3882 train_time:496945ms step_avg:169.95ms\nstep:2935/10000 train_loss:4.4380 train_time:497115ms step_avg:169.95ms\nstep:2936/10000 train_loss:4.5165 train_time:497284ms step_avg:169.95ms\nstep:2937/10000 train_loss:4.1968 train_time:497454ms step_avg:169.95ms\nstep:2938/10000 train_loss:4.2126 train_time:497624ms step_avg:169.95ms\nstep:2939/10000 train_loss:4.3332 train_time:497794ms step_avg:169.95ms\nstep:2940/10000 train_loss:4.2255 train_time:497965ms step_avg:169.95ms\nstep:2941/10000 train_loss:4.3263 train_time:498136ms step_avg:169.95ms\nstep:2942/10000 train_loss:4.3305 train_time:498305ms step_avg:169.95ms\nstep:2943/10000 train_loss:4.2553 train_time:498475ms step_avg:169.95ms\nstep:2944/10000 train_loss:4.3408 train_time:498642ms step_avg:169.95ms\nstep:2945/10000 train_loss:4.3348 train_time:498810ms step_avg:169.95ms\nstep:2946/10000 train_loss:4.3192 train_time:498979ms step_avg:169.95ms\nstep:2947/10000 train_loss:4.2519 train_time:499149ms step_avg:169.95ms\nstep:2948/10000 train_loss:4.4370 train_time:499318ms step_avg:169.95ms\nstep:2949/10000 train_loss:4.1704 train_time:499488ms step_avg:169.95ms\nstep:2950/10000 train_loss:4.3618 train_time:499658ms step_avg:169.95ms\nstep:2951/10000 train_loss:4.1749 train_time:499825ms step_avg:169.95ms\nstep:2952/10000 train_loss:4.3129 train_time:499995ms step_avg:169.95ms\nstep:2953/10000 train_loss:4.2704 train_time:500163ms step_avg:169.95ms\nstep:2954/10000 train_loss:4.2882 train_time:500332ms step_avg:169.95ms\nstep:2955/10000 train_loss:4.3521 train_time:500503ms step_avg:169.95ms\nstep:2956/10000 train_loss:4.2911 train_time:500674ms step_avg:169.95ms\nstep:2957/10000 train_loss:4.2243 train_time:500842ms step_avg:169.95ms\nstep:2958/10000 train_loss:4.3177 train_time:501011ms step_avg:169.95ms\nstep:2959/10000 train_loss:4.2449 train_time:501179ms step_avg:169.95ms\nstep:2960/10000 train_loss:4.1904 train_time:501348ms step_avg:169.95ms\nstep:2961/10000 train_loss:4.2929 train_time:501517ms step_avg:169.95ms\nstep:2962/10000 train_loss:4.3251 train_time:501686ms step_avg:169.95ms\nstep:2963/10000 train_loss:4.2006 train_time:501856ms step_avg:169.95ms\nstep:2964/10000 train_loss:4.8142 train_time:502026ms step_avg:169.95ms\nstep:2965/10000 train_loss:4.3620 train_time:502196ms step_avg:169.95ms\nstep:2966/10000 train_loss:4.3137 train_time:502364ms step_avg:169.95ms\nstep:2967/10000 train_loss:4.3216 train_time:502535ms step_avg:169.95ms\nstep:2968/10000 train_loss:4.2073 train_time:502705ms step_avg:169.95ms\nstep:2969/10000 train_loss:4.3319 train_time:502875ms step_avg:169.95ms\nstep:2970/10000 train_loss:4.3322 train_time:503042ms step_avg:169.95ms\nstep:2971/10000 train_loss:4.3611 train_time:503211ms step_avg:169.95ms\nstep:2972/10000 train_loss:4.3132 train_time:503383ms step_avg:169.95ms\nstep:2973/10000 train_loss:4.4668 train_time:503552ms step_avg:169.95ms\nstep:2974/10000 train_loss:4.2114 train_time:503723ms step_avg:169.95ms\nstep:2975/10000 train_loss:4.2801 train_time:503894ms step_avg:169.95ms\nstep:2976/10000 train_loss:4.2552 train_time:504063ms step_avg:169.95ms\nstep:2977/10000 train_loss:4.3290 train_time:504236ms step_avg:169.95ms\nstep:2978/10000 train_loss:4.4486 train_time:504405ms step_avg:169.95ms\nstep:2979/10000 train_loss:4.2383 train_time:504575ms step_avg:169.95ms\nstep:2980/10000 train_loss:4.3047 train_time:504744ms step_avg:169.95ms\nstep:2981/10000 train_loss:4.1988 train_time:504915ms step_avg:169.95ms\nstep:2982/10000 train_loss:4.2234 train_time:505083ms step_avg:169.95ms\nstep:2983/10000 train_loss:4.0365 train_time:505254ms step_avg:169.95ms\nstep:2984/10000 train_loss:4.3648 train_time:505425ms step_avg:169.95ms\nstep:2985/10000 train_loss:4.2417 train_time:505598ms step_avg:169.95ms\nstep:2986/10000 train_loss:4.2681 train_time:505767ms step_avg:169.95ms\nstep:2987/10000 train_loss:4.1603 train_time:505938ms step_avg:169.95ms\nstep:2988/10000 train_loss:4.2849 train_time:506104ms step_avg:169.95ms\nstep:2989/10000 train_loss:4.3192 train_time:506274ms step_avg:169.95ms\nstep:2990/10000 train_loss:4.3861 train_time:506445ms step_avg:169.95ms\nstep:2991/10000 train_loss:4.2185 train_time:506618ms step_avg:169.95ms\nstep:2992/10000 train_loss:4.3067 train_time:506786ms step_avg:169.95ms\nstep:2993/10000 train_loss:4.2933 train_time:506958ms step_avg:169.95ms\nstep:2994/10000 train_loss:4.2476 train_time:507127ms step_avg:169.95ms\nstep:2995/10000 train_loss:4.2801 train_time:507296ms step_avg:169.95ms\nstep:2996/10000 train_loss:4.2201 train_time:507465ms step_avg:169.95ms\nstep:2997/10000 train_loss:4.3717 train_time:507637ms step_avg:169.95ms\nstep:2998/10000 train_loss:4.3301 train_time:507805ms step_avg:169.95ms\nstep:2999/10000 train_loss:4.4090 train_time:507974ms step_avg:169.95ms\nstep:3000/10000 train_loss:4.2477 train_time:508144ms step_avg:169.95ms\nstep:3001/10000 train_loss:4.1624 train_time:508314ms step_avg:169.95ms\nstep:3002/10000 train_loss:4.2017 train_time:508485ms step_avg:169.95ms\nstep:3003/10000 train_loss:4.3176 train_time:508658ms step_avg:169.95ms\nstep:3004/10000 train_loss:4.2844 train_time:508826ms step_avg:169.95ms\nstep:3005/10000 train_loss:4.2382 train_time:508995ms step_avg:169.95ms\nstep:3006/10000 train_loss:4.3407 train_time:509164ms step_avg:169.95ms\nstep:3007/10000 train_loss:4.1619 train_time:509333ms step_avg:169.95ms\nstep:3008/10000 train_loss:4.2436 train_time:509506ms step_avg:169.95ms\nstep:3009/10000 train_loss:4.3645 train_time:509677ms step_avg:169.95ms\nstep:3010/10000 train_loss:4.1727 train_time:509846ms step_avg:169.95ms\nstep:3011/10000 train_loss:4.2065 train_time:510016ms step_avg:169.95ms\nstep:3012/10000 train_loss:4.2540 train_time:510185ms step_avg:169.95ms\nstep:3013/10000 train_loss:4.2469 train_time:510354ms step_avg:169.95ms\nstep:3014/10000 train_loss:4.2386 train_time:510523ms step_avg:169.95ms\nstep:3015/10000 train_loss:4.2896 train_time:510698ms step_avg:169.95ms\nstep:3016/10000 train_loss:4.3086 train_time:510868ms step_avg:169.95ms\nstep:3017/10000 train_loss:4.2162 train_time:511037ms step_avg:169.95ms\nstep:3018/10000 train_loss:4.2091 train_time:511205ms step_avg:169.95ms\nstep:3019/10000 train_loss:4.3764 train_time:511368ms step_avg:169.95ms\nstep:3020/10000 train_loss:4.2422 train_time:511539ms step_avg:169.95ms\nstep:3021/10000 train_loss:4.4180 train_time:511707ms step_avg:169.95ms\nstep:3022/10000 train_loss:4.2583 train_time:511878ms step_avg:169.95ms\nstep:3023/10000 train_loss:4.1157 train_time:512046ms step_avg:169.95ms\nstep:3024/10000 train_loss:4.3432 train_time:512217ms step_avg:169.95ms\nstep:3025/10000 train_loss:4.3123 train_time:512387ms step_avg:169.95ms\nstep:3026/10000 train_loss:4.1658 train_time:512558ms step_avg:169.95ms\nstep:3027/10000 train_loss:4.3499 train_time:512728ms step_avg:169.95ms\nstep:3028/10000 train_loss:4.1846 train_time:512897ms step_avg:169.95ms\nstep:3029/10000 train_loss:4.1776 train_time:513065ms step_avg:169.95ms\nstep:3030/10000 train_loss:4.4337 train_time:513234ms step_avg:169.95ms\nstep:3031/10000 train_loss:4.3808 train_time:513402ms step_avg:169.94ms\nstep:3032/10000 train_loss:4.3852 train_time:513572ms step_avg:169.94ms\nstep:3033/10000 train_loss:4.1990 train_time:513741ms step_avg:169.94ms\nstep:3034/10000 train_loss:4.5147 train_time:513910ms step_avg:169.94ms\nstep:3035/10000 train_loss:3.9883 train_time:514081ms step_avg:169.94ms\nstep:3036/10000 train_loss:4.3041 train_time:514249ms step_avg:169.94ms\nstep:3037/10000 train_loss:4.2621 train_time:514420ms step_avg:169.94ms\nstep:3038/10000 train_loss:4.2543 train_time:514588ms step_avg:169.94ms\nstep:3039/10000 train_loss:4.2140 train_time:514762ms step_avg:169.94ms\nstep:3040/10000 train_loss:4.2167 train_time:514933ms step_avg:169.94ms\nstep:3041/10000 train_loss:4.2585 train_time:515102ms step_avg:169.94ms\nstep:3042/10000 train_loss:4.2003 train_time:515271ms step_avg:169.94ms\nstep:3043/10000 train_loss:4.3046 train_time:515439ms step_avg:169.94ms\nstep:3044/10000 train_loss:4.3273 train_time:515606ms step_avg:169.94ms\nstep:3045/10000 train_loss:4.2325 train_time:515775ms step_avg:169.94ms\nstep:3046/10000 train_loss:4.3834 train_time:515942ms step_avg:169.94ms\nstep:3047/10000 train_loss:4.3090 train_time:516114ms step_avg:169.94ms\nstep:3048/10000 train_loss:4.2894 train_time:516502ms step_avg:170.01ms\nstep:3049/10000 train_loss:4.2675 train_time:516666ms step_avg:170.01ms\nstep:3050/10000 train_loss:4.0875 train_time:516833ms step_avg:170.01ms\nstep:3051/10000 train_loss:4.2253 train_time:517000ms step_avg:170.01ms\nstep:3052/10000 train_loss:4.1814 train_time:517167ms step_avg:170.01ms\nstep:3053/10000 train_loss:4.1875 train_time:517336ms step_avg:170.01ms\nstep:3054/10000 train_loss:4.2536 train_time:517510ms step_avg:170.01ms\nstep:3055/10000 train_loss:4.3669 train_time:517681ms step_avg:170.01ms\nstep:3056/10000 train_loss:4.0571 train_time:517850ms step_avg:170.01ms\nstep:3057/10000 train_loss:4.2458 train_time:518019ms step_avg:170.01ms\nstep:3058/10000 train_loss:4.3504 train_time:518188ms step_avg:170.01ms\nstep:3059/10000 train_loss:4.2372 train_time:518358ms step_avg:170.01ms\nstep:3060/10000 train_loss:4.2423 train_time:518527ms step_avg:170.01ms\nstep:3061/10000 train_loss:4.2974 train_time:518699ms step_avg:170.01ms\nstep:3062/10000 train_loss:4.0190 train_time:518866ms step_avg:170.01ms\nstep:3063/10000 train_loss:4.2708 train_time:519037ms step_avg:170.01ms\nstep:3064/10000 train_loss:4.2416 train_time:519205ms step_avg:170.01ms\nstep:3065/10000 train_loss:4.2171 train_time:519374ms step_avg:170.01ms\nstep:3066/10000 train_loss:4.4163 train_time:519542ms step_avg:170.01ms\nstep:3067/10000 train_loss:4.2425 train_time:519714ms step_avg:170.01ms\nstep:3068/10000 train_loss:5.2186 train_time:519883ms step_avg:170.01ms\nstep:3069/10000 train_loss:4.1879 train_time:520053ms step_avg:170.01ms\nstep:3070/10000 train_loss:4.3291 train_time:520223ms step_avg:170.01ms\nstep:3071/10000 train_loss:4.2711 train_time:520392ms step_avg:170.01ms\nstep:3072/10000 train_loss:4.2024 train_time:520562ms step_avg:170.01ms\nstep:3073/10000 train_loss:4.2614 train_time:520731ms step_avg:170.01ms\nstep:3074/10000 train_loss:4.2263 train_time:520900ms step_avg:170.01ms\nstep:3075/10000 train_loss:4.4586 train_time:521068ms step_avg:170.01ms\nstep:3076/10000 train_loss:4.2993 train_time:521237ms step_avg:170.01ms\nstep:3077/10000 train_loss:4.3076 train_time:521407ms step_avg:170.01ms\nstep:3078/10000 train_loss:4.2542 train_time:521578ms step_avg:170.01ms\nstep:3079/10000 train_loss:4.3184 train_time:521746ms step_avg:170.01ms\nstep:3080/10000 train_loss:4.3015 train_time:521916ms step_avg:170.01ms\nstep:3081/10000 train_loss:4.1743 train_time:522082ms step_avg:170.00ms\nstep:3082/10000 train_loss:4.2927 train_time:522250ms step_avg:170.00ms\nstep:3083/10000 train_loss:4.2484 train_time:522419ms step_avg:170.00ms\nstep:3084/10000 train_loss:4.8263 train_time:522587ms step_avg:170.00ms\nstep:3085/10000 train_loss:4.1142 train_time:522755ms step_avg:170.00ms\nstep:3086/10000 train_loss:4.3470 train_time:522924ms step_avg:170.00ms\nstep:3087/10000 train_loss:4.0548 train_time:523094ms step_avg:170.00ms\nstep:3088/10000 train_loss:4.2965 train_time:523264ms step_avg:170.00ms\nstep:3089/10000 train_loss:4.3658 train_time:523436ms step_avg:170.00ms\nstep:3090/10000 train_loss:4.5486 train_time:523606ms step_avg:170.00ms\nstep:3091/10000 train_loss:4.2842 train_time:523778ms step_avg:170.00ms\nstep:3092/10000 train_loss:4.2493 train_time:523947ms step_avg:170.00ms\nstep:3093/10000 train_loss:4.2883 train_time:524117ms step_avg:170.00ms\nstep:3094/10000 train_loss:4.1236 train_time:524284ms step_avg:170.00ms\nstep:3095/10000 train_loss:4.1436 train_time:524455ms step_avg:170.00ms\nstep:3096/10000 train_loss:4.2922 train_time:524623ms step_avg:170.00ms\nstep:3097/10000 train_loss:4.2458 train_time:524791ms step_avg:170.00ms\nstep:3098/10000 train_loss:4.3686 train_time:524959ms step_avg:170.00ms\nstep:3099/10000 train_loss:4.2006 train_time:525127ms step_avg:170.00ms\nstep:3100/10000 train_loss:4.4127 train_time:525296ms step_avg:170.00ms\nstep:3101/10000 train_loss:4.1165 train_time:525467ms step_avg:170.00ms\nstep:3102/10000 train_loss:4.2980 train_time:525639ms step_avg:170.00ms\nstep:3103/10000 train_loss:4.5922 train_time:525807ms step_avg:170.00ms\nstep:3104/10000 train_loss:4.2161 train_time:525976ms step_avg:170.00ms\nstep:3105/10000 train_loss:4.0512 train_time:526143ms step_avg:170.00ms\nstep:3106/10000 train_loss:4.2598 train_time:526312ms step_avg:170.00ms\nstep:3107/10000 train_loss:4.1058 train_time:526482ms step_avg:170.00ms\nstep:3108/10000 train_loss:4.2833 train_time:526658ms step_avg:170.00ms\nstep:3109/10000 train_loss:4.2697 train_time:526825ms step_avg:170.00ms\nstep:3110/10000 train_loss:4.1358 train_time:526994ms step_avg:170.00ms\nstep:3111/10000 train_loss:4.2390 train_time:527162ms step_avg:170.00ms\nstep:3112/10000 train_loss:4.2876 train_time:527330ms step_avg:170.00ms\nstep:3113/10000 train_loss:4.2113 train_time:527499ms step_avg:170.00ms\nstep:3114/10000 train_loss:4.0836 train_time:527669ms step_avg:170.00ms\nstep:3115/10000 train_loss:4.3199 train_time:527840ms step_avg:170.00ms\nstep:3116/10000 train_loss:4.2580 train_time:528009ms step_avg:170.00ms\nstep:3117/10000 train_loss:4.1241 train_time:528180ms step_avg:170.00ms\nstep:3118/10000 train_loss:4.1409 train_time:528349ms step_avg:170.00ms\nstep:3119/10000 train_loss:4.3105 train_time:528521ms step_avg:170.00ms\nstep:3120/10000 train_loss:4.2278 train_time:528689ms step_avg:170.00ms\nstep:3121/10000 train_loss:4.3292 train_time:528857ms step_avg:170.00ms\nstep:3122/10000 train_loss:4.2080 train_time:529026ms step_avg:170.00ms\nstep:3123/10000 train_loss:4.2411 train_time:529196ms step_avg:170.00ms\nstep:3124/10000 train_loss:4.0804 train_time:529365ms step_avg:170.00ms\nstep:3125/10000 train_loss:4.2632 train_time:529536ms step_avg:170.00ms\nstep:3126/10000 train_loss:4.2704 train_time:529704ms step_avg:169.99ms\nstep:3127/10000 train_loss:4.2260 train_time:529873ms step_avg:169.99ms\nstep:3128/10000 train_loss:4.2279 train_time:530041ms step_avg:169.99ms\nstep:3129/10000 train_loss:4.2513 train_time:530210ms step_avg:169.99ms\nstep:3130/10000 train_loss:4.3226 train_time:530379ms step_avg:169.99ms\nstep:3131/10000 train_loss:4.0483 train_time:530548ms step_avg:169.99ms\nstep:3132/10000 train_loss:4.2458 train_time:530718ms step_avg:169.99ms\nstep:3133/10000 train_loss:4.1884 train_time:530888ms step_avg:169.99ms\nstep:3134/10000 train_loss:4.1815 train_time:531058ms step_avg:169.99ms\nstep:3135/10000 train_loss:4.1789 train_time:531226ms step_avg:169.99ms\nstep:3136/10000 train_loss:4.2166 train_time:531396ms step_avg:169.99ms\nstep:3137/10000 train_loss:4.2924 train_time:531565ms step_avg:169.99ms\nstep:3138/10000 train_loss:4.2625 train_time:531735ms step_avg:169.99ms\nstep:3139/10000 train_loss:4.1851 train_time:531905ms step_avg:169.99ms\nstep:3140/10000 train_loss:4.1789 train_time:532076ms step_avg:169.99ms\nstep:3141/10000 train_loss:4.1566 train_time:532243ms step_avg:169.99ms\nstep:3142/10000 train_loss:4.1919 train_time:532412ms step_avg:169.99ms\nstep:3143/10000 train_loss:4.1225 train_time:532583ms step_avg:169.99ms\nstep:3144/10000 train_loss:4.3034 train_time:532751ms step_avg:169.99ms\nstep:3145/10000 train_loss:4.2538 train_time:532921ms step_avg:169.99ms\nstep:3146/10000 train_loss:4.2208 train_time:533090ms step_avg:169.99ms\nstep:3147/10000 train_loss:4.2291 train_time:533262ms step_avg:169.99ms\nstep:3148/10000 train_loss:4.1750 train_time:533432ms step_avg:169.99ms\nstep:3149/10000 train_loss:4.0386 train_time:533602ms step_avg:169.99ms\nstep:3150/10000 train_loss:4.1214 train_time:533769ms step_avg:169.99ms\nstep:3151/10000 train_loss:4.2818 train_time:533940ms step_avg:169.99ms\nstep:3152/10000 train_loss:4.2424 train_time:534110ms step_avg:169.99ms\nstep:3153/10000 train_loss:4.3258 train_time:534279ms step_avg:169.99ms\nstep:3154/10000 train_loss:4.3860 train_time:534447ms step_avg:169.99ms\nstep:3155/10000 train_loss:4.2034 train_time:534615ms step_avg:169.99ms\nstep:3156/10000 train_loss:4.2500 train_time:534781ms step_avg:169.99ms\nstep:3157/10000 train_loss:4.2623 train_time:534951ms step_avg:169.99ms\nstep:3158/10000 train_loss:4.1673 train_time:535120ms step_avg:169.99ms\nstep:3159/10000 train_loss:4.1514 train_time:535290ms step_avg:169.99ms\nstep:3160/10000 train_loss:4.1973 train_time:535460ms step_avg:169.99ms\nstep:3161/10000 train_loss:4.2037 train_time:535631ms step_avg:169.99ms\nstep:3162/10000 train_loss:4.2399 train_time:535802ms step_avg:169.99ms\nstep:3163/10000 train_loss:4.2267 train_time:535970ms step_avg:169.99ms\nstep:3164/10000 train_loss:4.2105 train_time:536138ms step_avg:169.99ms\nstep:3165/10000 train_loss:4.6095 train_time:536307ms step_avg:169.99ms\nstep:3166/10000 train_loss:4.2803 train_time:536478ms step_avg:169.99ms\nstep:3167/10000 train_loss:4.1378 train_time:536647ms step_avg:169.99ms\nstep:3168/10000 train_loss:4.2722 train_time:536819ms step_avg:169.99ms\nstep:3169/10000 train_loss:4.2780 train_time:536989ms step_avg:169.99ms\nstep:3170/10000 train_loss:4.3962 train_time:537158ms step_avg:169.99ms\nstep:3171/10000 train_loss:4.4290 train_time:537326ms step_avg:169.99ms\nstep:3172/10000 train_loss:4.2865 train_time:537497ms step_avg:169.99ms\nstep:3173/10000 train_loss:4.1642 train_time:537666ms step_avg:169.99ms\nstep:3174/10000 train_loss:4.4303 train_time:537836ms step_avg:169.99ms\nstep:3175/10000 train_loss:4.4032 train_time:538005ms step_avg:169.99ms\nstep:3176/10000 train_loss:4.2645 train_time:538174ms step_avg:169.99ms\nstep:3177/10000 train_loss:4.3501 train_time:538344ms step_avg:169.99ms\nstep:3178/10000 train_loss:4.1963 train_time:538515ms step_avg:169.99ms\nstep:3179/10000 train_loss:4.1190 train_time:538683ms step_avg:169.99ms\nstep:3180/10000 train_loss:4.2115 train_time:538857ms step_avg:169.99ms\nstep:3181/10000 train_loss:4.2465 train_time:539023ms step_avg:169.99ms\nstep:3182/10000 train_loss:4.2463 train_time:539194ms step_avg:169.99ms\nstep:3183/10000 train_loss:4.1999 train_time:539365ms step_avg:169.99ms\nstep:3184/10000 train_loss:4.2964 train_time:539536ms step_avg:169.99ms\nstep:3185/10000 train_loss:5.2757 train_time:539704ms step_avg:169.99ms\nstep:3186/10000 train_loss:4.3811 train_time:539876ms step_avg:169.99ms\nstep:3187/10000 train_loss:4.2299 train_time:540045ms step_avg:169.99ms\nstep:3188/10000 train_loss:4.2873 train_time:540215ms step_avg:169.99ms\nstep:3189/10000 train_loss:4.2389 train_time:540383ms step_avg:169.99ms\nstep:3190/10000 train_loss:4.4604 train_time:540554ms step_avg:169.99ms\nstep:3191/10000 train_loss:4.6472 train_time:540722ms step_avg:169.98ms\nstep:3192/10000 train_loss:4.0808 train_time:540892ms step_avg:169.98ms\nstep:3193/10000 train_loss:4.2224 train_time:541062ms step_avg:169.98ms\nstep:3194/10000 train_loss:4.1916 train_time:541232ms step_avg:169.98ms\nstep:3195/10000 train_loss:4.3724 train_time:541404ms step_avg:169.99ms\nstep:3196/10000 train_loss:4.1260 train_time:541575ms step_avg:169.99ms\nstep:3197/10000 train_loss:4.2302 train_time:541745ms step_avg:169.99ms\nstep:3198/10000 train_loss:4.3252 train_time:541917ms step_avg:169.99ms\nstep:3199/10000 train_loss:4.3667 train_time:542086ms step_avg:169.99ms\nstep:3200/10000 train_loss:4.9740 train_time:542257ms step_avg:169.99ms\nstep:3201/10000 train_loss:4.1777 train_time:542428ms step_avg:169.99ms\nstep:3202/10000 train_loss:4.2292 train_time:542597ms step_avg:169.99ms\nstep:3203/10000 train_loss:4.3508 train_time:542766ms step_avg:169.99ms\nstep:3204/10000 train_loss:4.3552 train_time:542937ms step_avg:169.99ms\nstep:3205/10000 train_loss:4.2338 train_time:543107ms step_avg:169.99ms\nstep:3206/10000 train_loss:4.2451 train_time:543277ms step_avg:169.99ms\nstep:3207/10000 train_loss:4.1547 train_time:543445ms step_avg:169.99ms\nstep:3208/10000 train_loss:4.2360 train_time:543616ms step_avg:169.99ms\nstep:3209/10000 train_loss:4.3050 train_time:543787ms step_avg:169.99ms\nstep:3210/10000 train_loss:4.3368 train_time:543960ms step_avg:169.99ms\nstep:3211/10000 train_loss:4.4537 train_time:544130ms step_avg:169.99ms\nstep:3212/10000 train_loss:4.2753 train_time:544302ms step_avg:169.99ms\nstep:3213/10000 train_loss:4.4128 train_time:544469ms step_avg:169.99ms\nstep:3214/10000 train_loss:4.1653 train_time:544639ms step_avg:169.99ms\nstep:3215/10000 train_loss:4.2440 train_time:544807ms step_avg:169.99ms\nstep:3216/10000 train_loss:4.2630 train_time:544975ms step_avg:169.99ms\nstep:3217/10000 train_loss:4.1863 train_time:545146ms step_avg:169.99ms\nstep:3218/10000 train_loss:4.2236 train_time:545315ms step_avg:169.99ms\nstep:3219/10000 train_loss:4.2146 train_time:545483ms step_avg:169.99ms\nstep:3220/10000 train_loss:4.1769 train_time:545651ms step_avg:169.98ms\nstep:3221/10000 train_loss:4.1834 train_time:545820ms step_avg:169.98ms\nstep:3222/10000 train_loss:4.3026 train_time:545988ms step_avg:169.98ms\nstep:3223/10000 train_loss:4.1904 train_time:546160ms step_avg:169.98ms\nstep:3224/10000 train_loss:4.0945 train_time:546331ms step_avg:169.98ms\nstep:3225/10000 train_loss:4.1952 train_time:546500ms step_avg:169.98ms\nstep:3226/10000 train_loss:4.1955 train_time:546669ms step_avg:169.98ms\nstep:3227/10000 train_loss:4.3400 train_time:546840ms step_avg:169.98ms\nstep:3228/10000 train_loss:4.2218 train_time:547009ms step_avg:169.98ms\nstep:3229/10000 train_loss:4.2967 train_time:547179ms step_avg:169.98ms\nstep:3230/10000 train_loss:4.1856 train_time:547349ms step_avg:169.98ms\nstep:3231/10000 train_loss:4.1365 train_time:547517ms step_avg:169.98ms\nstep:3232/10000 train_loss:4.2573 train_time:547686ms step_avg:169.98ms\nstep:3233/10000 train_loss:4.3245 train_time:547856ms step_avg:169.98ms\nstep:3234/10000 train_loss:4.3044 train_time:548024ms step_avg:169.98ms\nstep:3235/10000 train_loss:4.1290 train_time:548195ms step_avg:169.98ms\nstep:3236/10000 train_loss:4.4820 train_time:548367ms step_avg:169.98ms\nstep:3237/10000 train_loss:4.2245 train_time:548538ms step_avg:169.98ms\nstep:3238/10000 train_loss:4.2154 train_time:548706ms step_avg:169.98ms\nstep:3239/10000 train_loss:4.2435 train_time:548874ms step_avg:169.98ms\nstep:3240/10000 train_loss:4.3245 train_time:549042ms step_avg:169.98ms\nstep:3241/10000 train_loss:4.3130 train_time:549212ms step_avg:169.98ms\nstep:3242/10000 train_loss:4.1253 train_time:549383ms step_avg:169.98ms\nstep:3243/10000 train_loss:4.6797 train_time:549556ms step_avg:169.98ms\nstep:3244/10000 train_loss:4.2118 train_time:549724ms step_avg:169.98ms\nstep:3245/10000 train_loss:4.2895 train_time:549896ms step_avg:169.98ms\nstep:3246/10000 train_loss:4.2330 train_time:550065ms step_avg:169.98ms\nstep:3247/10000 train_loss:4.1681 train_time:550236ms step_avg:169.98ms\nstep:3248/10000 train_loss:4.2596 train_time:550404ms step_avg:169.98ms\nstep:3249/10000 train_loss:4.2701 train_time:550575ms step_avg:169.98ms\nstep:3250/10000 train_loss:4.2702 train_time:550743ms step_avg:169.98ms\nstep:3251/10000 train_loss:4.2683 train_time:550914ms step_avg:169.98ms\nstep:3252/10000 train_loss:4.2333 train_time:551084ms step_avg:169.98ms\nstep:3253/10000 train_loss:4.4294 train_time:551255ms step_avg:169.98ms\nstep:3254/10000 train_loss:4.2312 train_time:551424ms step_avg:169.98ms\nstep:3255/10000 train_loss:4.2736 train_time:551595ms step_avg:169.98ms\nstep:3256/10000 train_loss:4.2595 train_time:551763ms step_avg:169.98ms\nstep:3257/10000 train_loss:4.2096 train_time:551932ms step_avg:169.98ms\nstep:3258/10000 train_loss:4.2923 train_time:552102ms step_avg:169.98ms\nstep:3259/10000 train_loss:4.2253 train_time:552273ms step_avg:169.98ms\nstep:3260/10000 train_loss:4.0837 train_time:552444ms step_avg:169.98ms\nstep:3261/10000 train_loss:4.4492 train_time:552616ms step_avg:169.98ms\nstep:3262/10000 train_loss:4.2389 train_time:552785ms step_avg:169.98ms\nstep:3263/10000 train_loss:4.4338 train_time:552954ms step_avg:169.98ms\nstep:3264/10000 train_loss:4.4291 train_time:553122ms step_avg:169.98ms\nstep:3265/10000 train_loss:4.1939 train_time:553292ms step_avg:169.98ms\nstep:3266/10000 train_loss:4.2161 train_time:553463ms step_avg:169.98ms\nstep:3267/10000 train_loss:4.1730 train_time:553636ms step_avg:169.98ms\nstep:3268/10000 train_loss:4.1764 train_time:553804ms step_avg:169.98ms\nstep:3269/10000 train_loss:4.2852 train_time:553975ms step_avg:169.98ms\nstep:3270/10000 train_loss:4.2673 train_time:554143ms step_avg:169.98ms\nstep:3271/10000 train_loss:4.1609 train_time:554313ms step_avg:169.98ms\nstep:3272/10000 train_loss:4.1738 train_time:554483ms step_avg:169.98ms\nstep:3273/10000 train_loss:4.1615 train_time:554653ms step_avg:169.98ms\nstep:3274/10000 train_loss:4.3692 train_time:554823ms step_avg:169.98ms\nstep:3275/10000 train_loss:4.1742 train_time:554993ms step_avg:169.98ms\nstep:3276/10000 train_loss:4.1670 train_time:555164ms step_avg:169.98ms\nstep:3277/10000 train_loss:4.2252 train_time:555334ms step_avg:169.98ms\nstep:3278/10000 train_loss:4.2009 train_time:555505ms step_avg:169.98ms\nstep:3279/10000 train_loss:4.1971 train_time:555676ms step_avg:169.98ms\nstep:3280/10000 train_loss:4.2338 train_time:555844ms step_avg:169.98ms\nstep:3281/10000 train_loss:4.1655 train_time:556014ms step_avg:169.98ms\nstep:3282/10000 train_loss:4.3922 train_time:556183ms step_avg:169.98ms\nstep:3283/10000 train_loss:4.2453 train_time:556353ms step_avg:169.98ms\nstep:3284/10000 train_loss:4.0442 train_time:556523ms step_avg:169.98ms\nstep:3285/10000 train_loss:4.1185 train_time:556695ms step_avg:169.98ms\nstep:3286/10000 train_loss:4.1666 train_time:556862ms step_avg:169.98ms\nstep:3287/10000 train_loss:4.2520 train_time:557032ms step_avg:169.98ms\nstep:3288/10000 train_loss:4.2086 train_time:557203ms step_avg:169.98ms\nstep:3289/10000 train_loss:4.1465 train_time:557372ms step_avg:169.98ms\nstep:3290/10000 train_loss:4.1789 train_time:557543ms step_avg:169.98ms\nstep:3291/10000 train_loss:4.2086 train_time:557712ms step_avg:169.98ms\nstep:3292/10000 train_loss:4.3013 train_time:557884ms step_avg:169.98ms\nstep:3293/10000 train_loss:4.1160 train_time:558052ms step_avg:169.98ms\nstep:3294/10000 train_loss:4.2745 train_time:558222ms step_avg:169.98ms\nstep:3295/10000 train_loss:4.1898 train_time:558392ms step_avg:169.98ms\nstep:3296/10000 train_loss:4.0082 train_time:558562ms step_avg:169.98ms\nstep:3297/10000 train_loss:4.1594 train_time:558731ms step_avg:169.98ms\nstep:3298/10000 train_loss:4.2269 train_time:558901ms step_avg:169.98ms\nstep:3299/10000 train_loss:4.1725 train_time:559072ms step_avg:169.98ms\nstep:3300/10000 train_loss:4.2161 train_time:559242ms step_avg:169.98ms\nstep:3301/10000 train_loss:4.1542 train_time:559412ms step_avg:169.98ms\nstep:3302/10000 train_loss:4.1547 train_time:559583ms step_avg:169.98ms\nstep:3303/10000 train_loss:4.2267 train_time:559752ms step_avg:169.98ms\nstep:3304/10000 train_loss:4.3495 train_time:559922ms step_avg:169.98ms\nstep:3305/10000 train_loss:4.1873 train_time:560092ms step_avg:169.98ms\nstep:3306/10000 train_loss:4.2352 train_time:560261ms step_avg:169.98ms\nstep:3307/10000 train_loss:4.1784 train_time:560430ms step_avg:169.98ms\nstep:3308/10000 train_loss:4.1232 train_time:560602ms step_avg:169.98ms\nstep:3309/10000 train_loss:4.0429 train_time:560770ms step_avg:169.98ms\nstep:3310/10000 train_loss:4.1697 train_time:560940ms step_avg:169.98ms\nstep:3311/10000 train_loss:4.2769 train_time:561111ms step_avg:169.98ms\nstep:3312/10000 train_loss:4.1764 train_time:561279ms step_avg:169.98ms\nstep:3313/10000 train_loss:4.1688 train_time:561449ms step_avg:169.98ms\nstep:3314/10000 train_loss:4.3756 train_time:561620ms step_avg:169.98ms\nstep:3315/10000 train_loss:4.1722 train_time:561788ms step_avg:169.98ms\nstep:3316/10000 train_loss:4.2054 train_time:561959ms step_avg:169.98ms\nstep:3317/10000 train_loss:4.1738 train_time:562129ms step_avg:169.98ms\nstep:3318/10000 train_loss:4.2574 train_time:562301ms step_avg:169.98ms\nstep:3319/10000 train_loss:4.2254 train_time:562471ms step_avg:169.98ms\nstep:3320/10000 train_loss:4.2671 train_time:562640ms step_avg:169.98ms\nstep:3321/10000 train_loss:4.1110 train_time:562809ms step_avg:169.98ms\nstep:3322/10000 train_loss:4.3716 train_time:562980ms step_avg:169.98ms\nstep:3323/10000 train_loss:4.1744 train_time:563150ms step_avg:169.98ms\nstep:3324/10000 train_loss:4.0761 train_time:563320ms step_avg:169.98ms\nstep:3325/10000 train_loss:4.2302 train_time:563489ms step_avg:169.98ms\nstep:3326/10000 train_loss:4.1657 train_time:563661ms step_avg:169.98ms\nstep:3327/10000 train_loss:4.2575 train_time:563829ms step_avg:169.98ms\nstep:3328/10000 train_loss:4.2856 train_time:564002ms step_avg:169.98ms\nstep:3329/10000 train_loss:4.2039 train_time:564173ms step_avg:169.98ms\nstep:3330/10000 train_loss:4.2127 train_time:564343ms step_avg:169.98ms\nstep:3331/10000 train_loss:4.4643 train_time:564513ms step_avg:169.98ms\nstep:3332/10000 train_loss:4.2586 train_time:564680ms step_avg:169.98ms\nstep:3333/10000 train_loss:4.1743 train_time:564852ms step_avg:169.98ms\nstep:3334/10000 train_loss:4.1689 train_time:565017ms step_avg:169.98ms\nstep:3335/10000 train_loss:4.2587 train_time:565187ms step_avg:169.98ms\nstep:3336/10000 train_loss:4.2503 train_time:565359ms step_avg:169.98ms\nstep:3337/10000 train_loss:4.2119 train_time:565528ms step_avg:169.98ms\nstep:3338/10000 train_loss:4.2696 train_time:565697ms step_avg:169.98ms\nstep:3339/10000 train_loss:4.3407 train_time:565864ms step_avg:169.98ms\nstep:3340/10000 train_loss:4.1323 train_time:566034ms step_avg:169.98ms\nstep:3341/10000 train_loss:4.3240 train_time:566204ms step_avg:169.98ms\nstep:3342/10000 train_loss:4.1838 train_time:566376ms step_avg:169.98ms\nstep:3343/10000 train_loss:4.1898 train_time:566547ms step_avg:169.98ms\nstep:3344/10000 train_loss:4.3547 train_time:566717ms step_avg:169.98ms\nstep:3345/10000 train_loss:4.1115 train_time:566885ms step_avg:169.98ms\nstep:3346/10000 train_loss:4.1118 train_time:567053ms step_avg:169.98ms\nstep:3347/10000 train_loss:4.1183 train_time:567224ms step_avg:169.98ms\nstep:3348/10000 train_loss:4.0863 train_time:567394ms step_avg:169.98ms\nstep:3349/10000 train_loss:4.2370 train_time:567563ms step_avg:169.98ms\nstep:3350/10000 train_loss:4.3821 train_time:567734ms step_avg:169.98ms\nstep:3351/10000 train_loss:4.1403 train_time:567904ms step_avg:169.98ms\nstep:3352/10000 train_loss:4.0421 train_time:568073ms step_avg:169.98ms\nstep:3353/10000 train_loss:3.9956 train_time:568241ms step_avg:169.98ms\nstep:3354/10000 train_loss:4.0897 train_time:568411ms step_avg:169.98ms\nstep:3355/10000 train_loss:4.2324 train_time:568580ms step_avg:169.98ms\nstep:3356/10000 train_loss:4.2490 train_time:568749ms step_avg:169.98ms\nstep:3357/10000 train_loss:4.1803 train_time:568921ms step_avg:169.98ms\nstep:3358/10000 train_loss:4.2231 train_time:569088ms step_avg:169.98ms\nstep:3359/10000 train_loss:4.1518 train_time:569257ms step_avg:169.98ms\nstep:3360/10000 train_loss:4.1846 train_time:569426ms step_avg:169.98ms\nstep:3361/10000 train_loss:4.1214 train_time:569597ms step_avg:169.98ms\nstep:3362/10000 train_loss:4.0913 train_time:569765ms step_avg:169.98ms\nstep:3363/10000 train_loss:4.2165 train_time:569936ms step_avg:169.98ms\nstep:3364/10000 train_loss:4.2336 train_time:570104ms step_avg:169.98ms\nstep:3365/10000 train_loss:4.2333 train_time:570273ms step_avg:169.98ms\nstep:3366/10000 train_loss:4.1247 train_time:570442ms step_avg:169.98ms\nstep:3367/10000 train_loss:4.3001 train_time:570612ms step_avg:169.98ms\nstep:3368/10000 train_loss:4.1612 train_time:570780ms step_avg:169.98ms\nstep:3369/10000 train_loss:4.1555 train_time:570950ms step_avg:169.98ms\nstep:3370/10000 train_loss:4.2092 train_time:571122ms step_avg:169.98ms\nstep:3371/10000 train_loss:4.2135 train_time:571290ms step_avg:169.98ms\nstep:3372/10000 train_loss:4.1386 train_time:571460ms step_avg:169.98ms\nstep:3373/10000 train_loss:4.2561 train_time:571628ms step_avg:169.98ms\nstep:3374/10000 train_loss:4.1959 train_time:571796ms step_avg:169.97ms\nstep:3375/10000 train_loss:4.0491 train_time:571965ms step_avg:169.97ms\nstep:3376/10000 train_loss:4.1401 train_time:572137ms step_avg:169.98ms\nstep:3377/10000 train_loss:4.1808 train_time:572306ms step_avg:169.98ms\nstep:3378/10000 train_loss:4.2216 train_time:572477ms step_avg:169.98ms\nstep:3379/10000 train_loss:4.3117 train_time:572646ms step_avg:169.98ms\nstep:3380/10000 train_loss:4.2103 train_time:572816ms step_avg:169.98ms\nstep:3381/10000 train_loss:3.9590 train_time:572984ms step_avg:169.97ms\nstep:3382/10000 train_loss:4.0601 train_time:573154ms step_avg:169.97ms\nstep:3383/10000 train_loss:4.2144 train_time:573323ms step_avg:169.97ms\nstep:3384/10000 train_loss:4.1791 train_time:573492ms step_avg:169.97ms\nstep:3385/10000 train_loss:4.2750 train_time:573661ms step_avg:169.97ms\nstep:3386/10000 train_loss:4.3043 train_time:573830ms step_avg:169.97ms\nstep:3387/10000 train_loss:4.1503 train_time:574000ms step_avg:169.97ms\nstep:3388/10000 train_loss:4.2293 train_time:574169ms step_avg:169.97ms\nstep:3389/10000 train_loss:4.2190 train_time:574340ms step_avg:169.97ms\nstep:3390/10000 train_loss:4.1722 train_time:574509ms step_avg:169.97ms\nstep:3391/10000 train_loss:4.2286 train_time:574681ms step_avg:169.97ms\nstep:3392/10000 train_loss:4.1782 train_time:574849ms step_avg:169.97ms\nstep:3393/10000 train_loss:4.1522 train_time:575018ms step_avg:169.97ms\nstep:3394/10000 train_loss:4.1363 train_time:575186ms step_avg:169.97ms\nstep:3395/10000 train_loss:4.2587 train_time:575356ms step_avg:169.97ms\nstep:3396/10000 train_loss:4.1024 train_time:575525ms step_avg:169.97ms\nstep:3397/10000 train_loss:4.2619 train_time:575695ms step_avg:169.97ms\nstep:3398/10000 train_loss:4.2456 train_time:575864ms step_avg:169.97ms\nstep:3399/10000 train_loss:4.2938 train_time:576032ms step_avg:169.97ms\nstep:3400/10000 train_loss:4.2500 train_time:576200ms step_avg:169.97ms\nstep:3401/10000 train_loss:4.2120 train_time:576368ms step_avg:169.97ms\nstep:3402/10000 train_loss:4.1227 train_time:576539ms step_avg:169.97ms\nstep:3403/10000 train_loss:4.2331 train_time:576708ms step_avg:169.97ms\nstep:3404/10000 train_loss:4.1195 train_time:576877ms step_avg:169.97ms\nstep:3405/10000 train_loss:4.2292 train_time:577046ms step_avg:169.97ms\nstep:3406/10000 train_loss:4.0987 train_time:577216ms step_avg:169.97ms\nstep:3407/10000 train_loss:4.4898 train_time:577385ms step_avg:169.97ms\nstep:3408/10000 train_loss:4.2433 train_time:577556ms step_avg:169.97ms\nstep:3409/10000 train_loss:4.3123 train_time:577724ms step_avg:169.97ms\nstep:3410/10000 train_loss:4.4153 train_time:577896ms step_avg:169.97ms\nstep:3411/10000 train_loss:4.1952 train_time:578066ms step_avg:169.97ms\nstep:3412/10000 train_loss:3.9758 train_time:578234ms step_avg:169.97ms\nstep:3413/10000 train_loss:4.3540 train_time:578403ms step_avg:169.97ms\nstep:3414/10000 train_loss:4.1183 train_time:578572ms step_avg:169.97ms\nstep:3415/10000 train_loss:4.1971 train_time:578741ms step_avg:169.97ms\nstep:3416/10000 train_loss:4.3008 train_time:578909ms step_avg:169.97ms\nstep:3417/10000 train_loss:4.1177 train_time:579080ms step_avg:169.97ms\nstep:3418/10000 train_loss:4.2085 train_time:579249ms step_avg:169.97ms\nstep:3419/10000 train_loss:4.1912 train_time:579418ms step_avg:169.97ms\nstep:3420/10000 train_loss:4.1640 train_time:579587ms step_avg:169.97ms\nstep:3421/10000 train_loss:4.2202 train_time:579755ms step_avg:169.97ms\nstep:3422/10000 train_loss:4.2924 train_time:579923ms step_avg:169.97ms\nstep:3423/10000 train_loss:4.1877 train_time:580095ms step_avg:169.97ms\nstep:3424/10000 train_loss:3.9459 train_time:580265ms step_avg:169.97ms\nstep:3425/10000 train_loss:4.2184 train_time:580434ms step_avg:169.97ms\nstep:3426/10000 train_loss:4.2061 train_time:580603ms step_avg:169.97ms\nstep:3427/10000 train_loss:4.1165 train_time:580771ms step_avg:169.97ms\nstep:3428/10000 train_loss:4.2519 train_time:580940ms step_avg:169.96ms\nstep:3429/10000 train_loss:4.2530 train_time:581109ms step_avg:169.96ms\nstep:3430/10000 train_loss:4.1754 train_time:581278ms step_avg:169.96ms\nstep:3431/10000 train_loss:4.1896 train_time:581448ms step_avg:169.96ms\nstep:3432/10000 train_loss:4.3245 train_time:581619ms step_avg:169.96ms\nstep:3433/10000 train_loss:4.1931 train_time:581787ms step_avg:169.96ms\nstep:3434/10000 train_loss:4.2132 train_time:581961ms step_avg:169.97ms\nstep:3435/10000 train_loss:4.2093 train_time:582128ms step_avg:169.96ms\nstep:3436/10000 train_loss:4.0702 train_time:582298ms step_avg:169.96ms\nstep:3437/10000 train_loss:4.1953 train_time:582465ms step_avg:169.96ms\nstep:3438/10000 train_loss:4.1837 train_time:582636ms step_avg:169.96ms\nstep:3439/10000 train_loss:4.0333 train_time:582804ms step_avg:169.96ms\nstep:3440/10000 train_loss:4.1730 train_time:582976ms step_avg:169.96ms\nstep:3441/10000 train_loss:4.4972 train_time:583145ms step_avg:169.96ms\nstep:3442/10000 train_loss:4.2612 train_time:583315ms step_avg:169.96ms\nstep:3443/10000 train_loss:4.0700 train_time:583482ms step_avg:169.96ms\nstep:3444/10000 train_loss:4.3043 train_time:583651ms step_avg:169.96ms\nstep:3445/10000 train_loss:4.3252 train_time:583820ms step_avg:169.96ms\nstep:3446/10000 train_loss:4.2434 train_time:583990ms step_avg:169.96ms\nstep:3447/10000 train_loss:4.2087 train_time:584160ms step_avg:169.96ms\nstep:3448/10000 train_loss:4.0304 train_time:584328ms step_avg:169.96ms\nstep:3449/10000 train_loss:4.2259 train_time:584498ms step_avg:169.96ms\nstep:3450/10000 train_loss:4.3084 train_time:584666ms step_avg:169.96ms\nstep:3451/10000 train_loss:4.0472 train_time:584836ms step_avg:169.96ms\nstep:3452/10000 train_loss:4.1446 train_time:585010ms step_avg:169.96ms\nstep:3453/10000 train_loss:4.1479 train_time:585179ms step_avg:169.96ms\nstep:3454/10000 train_loss:4.1939 train_time:585350ms step_avg:169.96ms\nstep:3455/10000 train_loss:4.1106 train_time:585521ms step_avg:169.96ms\nstep:3456/10000 train_loss:4.4393 train_time:585689ms step_avg:169.96ms\nstep:3457/10000 train_loss:4.1757 train_time:585859ms step_avg:169.96ms\nstep:3458/10000 train_loss:4.2031 train_time:586028ms step_avg:169.96ms\nstep:3459/10000 train_loss:4.1642 train_time:586197ms step_avg:169.96ms\nstep:3460/10000 train_loss:4.4228 train_time:586366ms step_avg:169.96ms\nstep:3461/10000 train_loss:4.1478 train_time:586537ms step_avg:169.96ms\nstep:3462/10000 train_loss:4.0867 train_time:586705ms step_avg:169.96ms\nstep:3463/10000 train_loss:4.3156 train_time:586877ms step_avg:169.96ms\nstep:3464/10000 train_loss:4.2074 train_time:587045ms step_avg:169.96ms\nstep:3465/10000 train_loss:4.3320 train_time:587216ms step_avg:169.96ms\nstep:3466/10000 train_loss:4.2262 train_time:587383ms step_avg:169.96ms\nstep:3467/10000 train_loss:4.0710 train_time:587554ms step_avg:169.96ms\nstep:3468/10000 train_loss:4.1532 train_time:587721ms step_avg:169.96ms\nstep:3469/10000 train_loss:4.4756 train_time:587891ms step_avg:169.96ms\nstep:3470/10000 train_loss:4.5335 train_time:588058ms step_avg:169.96ms\nstep:3471/10000 train_loss:4.1398 train_time:588228ms step_avg:169.96ms\nstep:3472/10000 train_loss:4.4089 train_time:588396ms step_avg:169.96ms\nstep:3473/10000 train_loss:4.1444 train_time:588565ms step_avg:169.96ms\nstep:3474/10000 train_loss:4.0776 train_time:588736ms step_avg:169.96ms\nstep:3475/10000 train_loss:4.2976 train_time:588905ms step_avg:169.96ms\nstep:3476/10000 train_loss:4.1101 train_time:589076ms step_avg:169.96ms\nstep:3477/10000 train_loss:4.1739 train_time:589244ms step_avg:169.96ms\nstep:3478/10000 train_loss:4.2518 train_time:589413ms step_avg:169.96ms\nstep:3479/10000 train_loss:4.1655 train_time:589582ms step_avg:169.96ms\nstep:3480/10000 train_loss:4.0227 train_time:589754ms step_avg:169.96ms\nstep:3481/10000 train_loss:4.1659 train_time:589924ms step_avg:169.96ms\nstep:3482/10000 train_loss:4.2545 train_time:590093ms step_avg:169.96ms\nstep:3483/10000 train_loss:4.2652 train_time:590264ms step_avg:169.96ms\nstep:3484/10000 train_loss:4.2573 train_time:590436ms step_avg:169.96ms\nstep:3485/10000 train_loss:4.2748 train_time:590604ms step_avg:169.96ms\nstep:3486/10000 train_loss:4.0701 train_time:590775ms step_avg:169.96ms\nstep:3487/10000 train_loss:4.2749 train_time:590943ms step_avg:169.96ms\nstep:3488/10000 train_loss:4.1951 train_time:591112ms step_avg:169.96ms\nstep:3489/10000 train_loss:4.1013 train_time:591284ms step_avg:169.96ms\nstep:3490/10000 train_loss:4.0873 train_time:591454ms step_avg:169.96ms\nstep:3491/10000 train_loss:4.1285 train_time:591624ms step_avg:169.96ms\nstep:3492/10000 train_loss:4.1296 train_time:591797ms step_avg:169.96ms\nstep:3493/10000 train_loss:4.2833 train_time:591966ms step_avg:169.96ms\nstep:3494/10000 train_loss:4.2920 train_time:592134ms step_avg:169.96ms\nstep:3495/10000 train_loss:4.1951 train_time:592304ms step_avg:169.96ms\nstep:3496/10000 train_loss:4.2052 train_time:592474ms step_avg:169.96ms\nstep:3497/10000 train_loss:4.2491 train_time:592644ms step_avg:169.96ms\nstep:3498/10000 train_loss:4.0800 train_time:592813ms step_avg:169.96ms\nstep:3499/10000 train_loss:4.1938 train_time:592984ms step_avg:169.96ms\nstep:3500/10000 train_loss:4.1279 train_time:593154ms step_avg:169.96ms\nstep:3501/10000 train_loss:4.2379 train_time:593321ms step_avg:169.96ms\nstep:3502/10000 train_loss:3.6788 train_time:593494ms step_avg:169.96ms\nstep:3503/10000 train_loss:4.1757 train_time:593664ms step_avg:169.96ms\nstep:3504/10000 train_loss:4.2563 train_time:593834ms step_avg:169.96ms\nstep:3505/10000 train_loss:4.2071 train_time:594004ms step_avg:169.96ms\nstep:3506/10000 train_loss:4.2606 train_time:594175ms step_avg:169.96ms\nstep:3507/10000 train_loss:4.2012 train_time:594343ms step_avg:169.96ms\nstep:3508/10000 train_loss:3.9971 train_time:594515ms step_avg:169.96ms\nstep:3509/10000 train_loss:4.1973 train_time:594682ms step_avg:169.96ms\nstep:3510/10000 train_loss:4.1086 train_time:594853ms step_avg:169.96ms\nstep:3511/10000 train_loss:3.9703 train_time:595021ms step_avg:169.96ms\nstep:3512/10000 train_loss:4.2895 train_time:595192ms step_avg:169.96ms\nstep:3513/10000 train_loss:4.4565 train_time:595360ms step_avg:169.96ms\nstep:3514/10000 train_loss:4.2700 train_time:595530ms step_avg:169.96ms\nstep:3515/10000 train_loss:4.1498 train_time:595701ms step_avg:169.96ms\nstep:3516/10000 train_loss:4.1291 train_time:595870ms step_avg:169.96ms\nstep:3517/10000 train_loss:4.1150 train_time:596041ms step_avg:169.96ms\nstep:3518/10000 train_loss:4.1429 train_time:596210ms step_avg:169.96ms\nstep:3519/10000 train_loss:4.2112 train_time:596382ms step_avg:169.96ms\nstep:3520/10000 train_loss:4.4960 train_time:596551ms step_avg:169.96ms\nstep:3521/10000 train_loss:4.2140 train_time:596722ms step_avg:169.96ms\nstep:3522/10000 train_loss:4.2901 train_time:596892ms step_avg:169.96ms\nstep:3523/10000 train_loss:4.0205 train_time:597062ms step_avg:169.96ms\nstep:3524/10000 train_loss:4.2249 train_time:597231ms step_avg:169.96ms\nstep:3525/10000 train_loss:4.1292 train_time:597402ms step_avg:169.96ms\nstep:3526/10000 train_loss:3.9721 train_time:597570ms step_avg:169.96ms\nstep:3527/10000 train_loss:4.0987 train_time:597739ms step_avg:169.96ms\nstep:3528/10000 train_loss:4.0361 train_time:597908ms step_avg:169.96ms\nstep:3529/10000 train_loss:4.0809 train_time:598078ms step_avg:169.96ms\nstep:3530/10000 train_loss:4.1404 train_time:598247ms step_avg:169.96ms\nstep:3531/10000 train_loss:4.2541 train_time:598418ms step_avg:169.96ms\nstep:3532/10000 train_loss:4.3268 train_time:598588ms step_avg:169.96ms\nstep:3533/10000 train_loss:4.1282 train_time:598755ms step_avg:169.96ms\nstep:3534/10000 train_loss:4.3345 train_time:598923ms step_avg:169.96ms\nstep:3535/10000 train_loss:4.1695 train_time:599093ms step_avg:169.96ms\nstep:3536/10000 train_loss:4.0490 train_time:599264ms step_avg:169.96ms\nstep:3537/10000 train_loss:4.1882 train_time:599435ms step_avg:169.96ms\nstep:3538/10000 train_loss:4.4832 train_time:599604ms step_avg:169.96ms\nstep:3539/10000 train_loss:4.0629 train_time:599775ms step_avg:169.96ms\nstep:3540/10000 train_loss:4.2769 train_time:599946ms step_avg:169.96ms\nstep:3541/10000 train_loss:4.0699 train_time:600115ms step_avg:169.96ms\nstep:3542/10000 train_loss:4.2212 train_time:600285ms step_avg:169.96ms\nstep:3543/10000 train_loss:4.1477 train_time:600455ms step_avg:169.96ms\nstep:3544/10000 train_loss:4.1523 train_time:600624ms step_avg:169.96ms\nstep:3545/10000 train_loss:4.0538 train_time:600792ms step_avg:169.96ms\nstep:3546/10000 train_loss:4.1099 train_time:600961ms step_avg:169.96ms\nstep:3547/10000 train_loss:4.0777 train_time:601131ms step_avg:169.96ms\nstep:3548/10000 train_loss:4.3240 train_time:601302ms step_avg:169.96ms\nstep:3549/10000 train_loss:4.1550 train_time:601473ms step_avg:169.96ms\nstep:3550/10000 train_loss:4.2410 train_time:601643ms step_avg:169.96ms\nstep:3551/10000 train_loss:4.1802 train_time:601815ms step_avg:169.96ms\nstep:3552/10000 train_loss:4.1946 train_time:601984ms step_avg:169.96ms\nstep:3553/10000 train_loss:4.0715 train_time:602155ms step_avg:169.96ms\nstep:3554/10000 train_loss:4.6810 train_time:602323ms step_avg:169.96ms\nstep:3555/10000 train_loss:4.3474 train_time:602492ms step_avg:169.96ms\nstep:3556/10000 train_loss:4.2551 train_time:602658ms step_avg:169.95ms\nstep:3557/10000 train_loss:4.0532 train_time:602826ms step_avg:169.95ms\nstep:3558/10000 train_loss:4.4329 train_time:602996ms step_avg:169.95ms\nstep:3559/10000 train_loss:4.1451 train_time:603164ms step_avg:169.95ms\nstep:3560/10000 train_loss:4.2132 train_time:603338ms step_avg:169.95ms\nstep:3561/10000 train_loss:4.2329 train_time:603508ms step_avg:169.95ms\nstep:3562/10000 train_loss:4.1670 train_time:603677ms step_avg:169.95ms\nstep:3563/10000 train_loss:4.0238 train_time:603842ms step_avg:169.95ms\nstep:3564/10000 train_loss:4.0329 train_time:604011ms step_avg:169.95ms\nstep:3565/10000 train_loss:4.0465 train_time:604181ms step_avg:169.95ms\nstep:3566/10000 train_loss:4.1723 train_time:604352ms step_avg:169.95ms\nstep:3567/10000 train_loss:3.9368 train_time:604524ms step_avg:169.95ms\nstep:3568/10000 train_loss:4.1922 train_time:604695ms step_avg:169.95ms\nstep:3569/10000 train_loss:4.0800 train_time:604864ms step_avg:169.95ms\nstep:3570/10000 train_loss:3.9867 train_time:605032ms step_avg:169.95ms\nstep:3571/10000 train_loss:4.0135 train_time:605203ms step_avg:169.95ms\nstep:3572/10000 train_loss:4.0960 train_time:605375ms step_avg:169.95ms\nstep:3573/10000 train_loss:4.2616 train_time:605547ms step_avg:169.95ms\nstep:3574/10000 train_loss:4.2941 train_time:605717ms step_avg:169.95ms\nstep:3575/10000 train_loss:4.3278 train_time:605886ms step_avg:169.95ms\nstep:3576/10000 train_loss:4.1559 train_time:606055ms step_avg:169.95ms\nstep:3577/10000 train_loss:3.9961 train_time:606223ms step_avg:169.95ms\nstep:3578/10000 train_loss:4.0699 train_time:606397ms step_avg:169.95ms\nstep:3579/10000 train_loss:4.1332 train_time:606567ms step_avg:169.95ms\nstep:3580/10000 train_loss:3.9882 train_time:606739ms step_avg:169.95ms\nstep:3581/10000 train_loss:4.0650 train_time:606906ms step_avg:169.95ms\nstep:3582/10000 train_loss:4.4120 train_time:607074ms step_avg:169.95ms\nstep:3583/10000 train_loss:4.1758 train_time:607244ms step_avg:169.95ms\nstep:3584/10000 train_loss:3.9894 train_time:607416ms step_avg:169.95ms\nstep:3585/10000 train_loss:4.2925 train_time:607585ms step_avg:169.95ms\nstep:3586/10000 train_loss:4.3360 train_time:607756ms step_avg:169.95ms\nstep:3587/10000 train_loss:4.0891 train_time:607925ms step_avg:169.95ms\nstep:3588/10000 train_loss:4.0803 train_time:608096ms step_avg:169.95ms\nstep:3589/10000 train_loss:4.0550 train_time:608266ms step_avg:169.95ms\nstep:3590/10000 train_loss:4.1365 train_time:608437ms step_avg:169.95ms\nstep:3591/10000 train_loss:4.1266 train_time:608610ms step_avg:169.96ms\nstep:3592/10000 train_loss:4.2682 train_time:608778ms step_avg:169.95ms\nstep:3593/10000 train_loss:4.1736 train_time:608945ms step_avg:169.95ms\nstep:3594/10000 train_loss:4.1492 train_time:609114ms step_avg:169.95ms\nstep:3595/10000 train_loss:4.1813 train_time:609282ms step_avg:169.95ms\nstep:3596/10000 train_loss:4.1540 train_time:609454ms step_avg:169.95ms\nstep:3597/10000 train_loss:4.1844 train_time:609625ms step_avg:169.95ms\nstep:3598/10000 train_loss:4.2031 train_time:609795ms step_avg:169.95ms\nstep:3599/10000 train_loss:4.1265 train_time:609962ms step_avg:169.95ms\nstep:3600/10000 train_loss:4.1614 train_time:610133ms step_avg:169.95ms\nstep:3601/10000 train_loss:4.7281 train_time:610301ms step_avg:169.95ms\nstep:3602/10000 train_loss:4.1059 train_time:610471ms step_avg:169.95ms\nstep:3603/10000 train_loss:4.1060 train_time:610641ms step_avg:169.95ms\nstep:3604/10000 train_loss:4.3575 train_time:610810ms step_avg:169.95ms\nstep:3605/10000 train_loss:4.0767 train_time:610980ms step_avg:169.95ms\nstep:3606/10000 train_loss:4.0503 train_time:611149ms step_avg:169.95ms\nstep:3607/10000 train_loss:4.1405 train_time:611318ms step_avg:169.95ms\nstep:3608/10000 train_loss:4.1682 train_time:611486ms step_avg:169.95ms\nstep:3609/10000 train_loss:4.1172 train_time:611656ms step_avg:169.95ms\nstep:3610/10000 train_loss:4.2633 train_time:611825ms step_avg:169.95ms\nstep:3611/10000 train_loss:4.1077 train_time:611996ms step_avg:169.95ms\nstep:3612/10000 train_loss:4.0635 train_time:612164ms step_avg:169.95ms\nstep:3613/10000 train_loss:4.1795 train_time:612336ms step_avg:169.95ms\nstep:3614/10000 train_loss:4.2706 train_time:612504ms step_avg:169.95ms\nstep:3615/10000 train_loss:4.5459 train_time:612672ms step_avg:169.95ms\nstep:3616/10000 train_loss:4.3215 train_time:612839ms step_avg:169.95ms\nstep:3617/10000 train_loss:4.1409 train_time:613009ms step_avg:169.95ms\nstep:3618/10000 train_loss:4.1585 train_time:613178ms step_avg:169.95ms\nstep:3619/10000 train_loss:4.1191 train_time:613347ms step_avg:169.95ms\nstep:3620/10000 train_loss:4.2490 train_time:613517ms step_avg:169.95ms\nstep:3621/10000 train_loss:4.1469 train_time:613685ms step_avg:169.95ms\nstep:3622/10000 train_loss:4.1356 train_time:613855ms step_avg:169.95ms\nstep:3623/10000 train_loss:4.1972 train_time:614026ms step_avg:169.95ms\nstep:3624/10000 train_loss:4.1552 train_time:614198ms step_avg:169.95ms\nstep:3625/10000 train_loss:4.1049 train_time:614366ms step_avg:169.95ms\nstep:3626/10000 train_loss:4.1267 train_time:614537ms step_avg:169.95ms\nstep:3627/10000 train_loss:4.0186 train_time:614706ms step_avg:169.95ms\nstep:3628/10000 train_loss:4.0914 train_time:614876ms step_avg:169.95ms\nstep:3629/10000 train_loss:4.1097 train_time:615046ms step_avg:169.95ms\nstep:3630/10000 train_loss:4.3620 train_time:615216ms step_avg:169.95ms\nstep:3631/10000 train_loss:4.1424 train_time:615383ms step_avg:169.95ms\nstep:3632/10000 train_loss:4.3863 train_time:615554ms step_avg:169.95ms\nstep:3633/10000 train_loss:4.1675 train_time:615723ms step_avg:169.95ms\nstep:3634/10000 train_loss:4.1381 train_time:615894ms step_avg:169.95ms\nstep:3635/10000 train_loss:4.2481 train_time:616063ms step_avg:169.95ms\nstep:3636/10000 train_loss:4.1089 train_time:616236ms step_avg:169.95ms\nstep:3637/10000 train_loss:4.0872 train_time:616405ms step_avg:169.95ms\nstep:3638/10000 train_loss:4.1552 train_time:616574ms step_avg:169.95ms\nstep:3639/10000 train_loss:4.1368 train_time:616742ms step_avg:169.95ms\nstep:3640/10000 train_loss:4.2428 train_time:616910ms step_avg:169.95ms\nstep:3641/10000 train_loss:4.1450 train_time:617081ms step_avg:169.95ms\nstep:3642/10000 train_loss:4.0765 train_time:617249ms step_avg:169.95ms\nstep:3643/10000 train_loss:4.2418 train_time:617420ms step_avg:169.95ms\nstep:3644/10000 train_loss:4.4776 train_time:617589ms step_avg:169.95ms\nstep:3645/10000 train_loss:4.2402 train_time:617759ms step_avg:169.95ms\nstep:3646/10000 train_loss:4.1829 train_time:617927ms step_avg:169.95ms\nstep:3647/10000 train_loss:4.1198 train_time:618098ms step_avg:169.95ms\nstep:3648/10000 train_loss:4.0130 train_time:618267ms step_avg:169.95ms\nstep:3649/10000 train_loss:4.1201 train_time:618438ms step_avg:169.95ms\nstep:3650/10000 train_loss:4.2077 train_time:618607ms step_avg:169.95ms\nstep:3651/10000 train_loss:4.0726 train_time:618777ms step_avg:169.95ms\nstep:3652/10000 train_loss:4.2731 train_time:618944ms step_avg:169.95ms\nstep:3653/10000 train_loss:4.1436 train_time:619114ms step_avg:169.95ms\nstep:3654/10000 train_loss:4.2404 train_time:619283ms step_avg:169.95ms\nstep:3655/10000 train_loss:4.0635 train_time:619455ms step_avg:169.95ms\nstep:3656/10000 train_loss:4.2027 train_time:619623ms step_avg:169.95ms\nstep:3657/10000 train_loss:4.1826 train_time:619794ms step_avg:169.95ms\nstep:3658/10000 train_loss:4.1433 train_time:619961ms step_avg:169.95ms\nstep:3659/10000 train_loss:4.1799 train_time:620128ms step_avg:169.94ms\nstep:3660/10000 train_loss:4.1654 train_time:620298ms step_avg:169.94ms\nstep:3661/10000 train_loss:3.9858 train_time:620466ms step_avg:169.94ms\nstep:3662/10000 train_loss:4.3472 train_time:620635ms step_avg:169.94ms\nstep:3663/10000 train_loss:4.1737 train_time:620805ms step_avg:169.94ms\nstep:3664/10000 train_loss:4.3489 train_time:620974ms step_avg:169.94ms\nstep:3665/10000 train_loss:4.1651 train_time:621143ms step_avg:169.94ms\nstep:3666/10000 train_loss:4.1838 train_time:621313ms step_avg:169.94ms\nstep:3667/10000 train_loss:4.0713 train_time:621483ms step_avg:169.94ms\nstep:3668/10000 train_loss:4.0926 train_time:621653ms step_avg:169.94ms\nstep:3669/10000 train_loss:4.3503 train_time:621823ms step_avg:169.94ms\nstep:3670/10000 train_loss:4.2521 train_time:621993ms step_avg:169.94ms\nstep:3671/10000 train_loss:4.2137 train_time:622161ms step_avg:169.94ms\nstep:3672/10000 train_loss:4.0725 train_time:622330ms step_avg:169.94ms\nstep:3673/10000 train_loss:4.6161 train_time:622499ms step_avg:169.94ms\nstep:3674/10000 train_loss:4.2506 train_time:622669ms step_avg:169.94ms\nstep:3675/10000 train_loss:4.0873 train_time:622841ms step_avg:169.94ms\nstep:3676/10000 train_loss:4.1395 train_time:623010ms step_avg:169.94ms\nstep:3677/10000 train_loss:4.1427 train_time:623181ms step_avg:169.94ms\nstep:3678/10000 train_loss:5.3595 train_time:623350ms step_avg:169.94ms\nstep:3679/10000 train_loss:4.1067 train_time:623522ms step_avg:169.94ms\nstep:3680/10000 train_loss:4.0772 train_time:623693ms step_avg:169.94ms\nstep:3681/10000 train_loss:3.8213 train_time:623864ms step_avg:169.94ms\nstep:3682/10000 train_loss:4.1988 train_time:624034ms step_avg:169.94ms\nstep:3683/10000 train_loss:3.9584 train_time:624204ms step_avg:169.94ms\nstep:3684/10000 train_loss:4.1310 train_time:624373ms step_avg:169.94ms\nstep:3685/10000 train_loss:4.2370 train_time:624543ms step_avg:169.94ms\nstep:3686/10000 train_loss:4.1728 train_time:624714ms step_avg:169.94ms\nstep:3687/10000 train_loss:4.1989 train_time:624884ms step_avg:169.94ms\nstep:3688/10000 train_loss:4.0812 train_time:625054ms step_avg:169.94ms\nstep:3689/10000 train_loss:4.1378 train_time:625223ms step_avg:169.94ms\nstep:3690/10000 train_loss:4.1343 train_time:625392ms step_avg:169.94ms\nstep:3691/10000 train_loss:4.0508 train_time:625562ms step_avg:169.94ms\nstep:3692/10000 train_loss:4.1786 train_time:625732ms step_avg:169.94ms\nstep:3693/10000 train_loss:4.1485 train_time:625903ms step_avg:169.94ms\nstep:3694/10000 train_loss:4.2204 train_time:626073ms step_avg:169.94ms\nstep:3695/10000 train_loss:4.1092 train_time:626242ms step_avg:169.94ms\nstep:3696/10000 train_loss:4.1786 train_time:626411ms step_avg:169.94ms\nstep:3697/10000 train_loss:4.1161 train_time:626581ms step_avg:169.94ms\nstep:3698/10000 train_loss:4.2015 train_time:626753ms step_avg:169.94ms\nstep:3699/10000 train_loss:4.0599 train_time:626924ms step_avg:169.94ms\nstep:3700/10000 train_loss:4.0518 train_time:627095ms step_avg:169.94ms\nstep:3701/10000 train_loss:4.1338 train_time:627263ms step_avg:169.94ms\nstep:3702/10000 train_loss:4.2773 train_time:627431ms step_avg:169.94ms\nstep:3703/10000 train_loss:4.0314 train_time:627601ms step_avg:169.94ms\nstep:3704/10000 train_loss:4.1534 train_time:627771ms step_avg:169.94ms\nstep:3705/10000 train_loss:4.0852 train_time:627942ms step_avg:169.94ms\nstep:3706/10000 train_loss:4.2726 train_time:628114ms step_avg:169.94ms\nstep:3707/10000 train_loss:4.1265 train_time:628283ms step_avg:169.94ms\nstep:3708/10000 train_loss:4.0731 train_time:628452ms step_avg:169.94ms\nstep:3709/10000 train_loss:3.9526 train_time:628622ms step_avg:169.94ms\nstep:3710/10000 train_loss:4.1559 train_time:628790ms step_avg:169.94ms\nstep:3711/10000 train_loss:4.0488 train_time:628961ms step_avg:169.94ms\nstep:3712/10000 train_loss:4.2462 train_time:629131ms step_avg:169.94ms\nstep:3713/10000 train_loss:4.2794 train_time:629302ms step_avg:169.94ms\nstep:3714/10000 train_loss:4.1583 train_time:629474ms step_avg:169.94ms\nstep:3715/10000 train_loss:3.9823 train_time:629642ms step_avg:169.94ms\nstep:3716/10000 train_loss:4.1311 train_time:629813ms step_avg:169.94ms\nstep:3717/10000 train_loss:4.0564 train_time:629985ms step_avg:169.94ms\nstep:3718/10000 train_loss:4.3301 train_time:630156ms step_avg:169.94ms\nstep:3719/10000 train_loss:4.1932 train_time:630325ms step_avg:169.94ms\nstep:3720/10000 train_loss:3.9621 train_time:630496ms step_avg:169.94ms\nstep:3721/10000 train_loss:4.1739 train_time:630667ms step_avg:169.95ms\nstep:3722/10000 train_loss:4.2200 train_time:630836ms step_avg:169.94ms\nstep:3723/10000 train_loss:4.2376 train_time:631005ms step_avg:169.94ms\nstep:3724/10000 train_loss:4.3222 train_time:631176ms step_avg:169.95ms\nstep:3725/10000 train_loss:4.3361 train_time:631344ms step_avg:169.94ms\nstep:3726/10000 train_loss:3.9783 train_time:631514ms step_avg:169.94ms\nstep:3727/10000 train_loss:4.2380 train_time:631682ms step_avg:169.94ms\nstep:3728/10000 train_loss:4.0695 train_time:631851ms step_avg:169.94ms\nstep:3729/10000 train_loss:4.1277 train_time:632020ms step_avg:169.94ms\nstep:3730/10000 train_loss:4.1168 train_time:632192ms step_avg:169.94ms\nstep:3731/10000 train_loss:4.2053 train_time:632362ms step_avg:169.94ms\nstep:3732/10000 train_loss:4.0349 train_time:632531ms step_avg:169.94ms\nstep:3733/10000 train_loss:4.1236 train_time:632703ms step_avg:169.94ms\nstep:3734/10000 train_loss:4.2991 train_time:632871ms step_avg:169.94ms\nstep:3735/10000 train_loss:4.3979 train_time:633041ms step_avg:169.94ms\nstep:3736/10000 train_loss:4.1703 train_time:633210ms step_avg:169.94ms\nstep:3737/10000 train_loss:4.1371 train_time:633381ms step_avg:169.94ms\nstep:3738/10000 train_loss:4.0586 train_time:633551ms step_avg:169.94ms\nstep:3739/10000 train_loss:4.0589 train_time:633721ms step_avg:169.94ms\nstep:3740/10000 train_loss:4.1947 train_time:633889ms step_avg:169.94ms\nstep:3741/10000 train_loss:4.0922 train_time:634063ms step_avg:169.94ms\nstep:3742/10000 train_loss:4.1562 train_time:634233ms step_avg:169.94ms\nstep:3743/10000 train_loss:4.3096 train_time:634404ms step_avg:169.94ms\nstep:3744/10000 train_loss:4.0407 train_time:634572ms step_avg:169.94ms\nstep:3745/10000 train_loss:4.2975 train_time:634742ms step_avg:169.94ms\nstep:3746/10000 train_loss:4.0993 train_time:634913ms step_avg:169.94ms\nstep:3747/10000 train_loss:4.0452 train_time:635084ms step_avg:169.94ms\nstep:3748/10000 train_loss:4.0115 train_time:635254ms step_avg:169.94ms\nstep:3749/10000 train_loss:4.3840 train_time:635423ms step_avg:169.94ms\nstep:3750/10000 train_loss:4.1271 train_time:635594ms step_avg:169.94ms\nstep:3751/10000 train_loss:4.4031 train_time:635763ms step_avg:169.94ms\nstep:3752/10000 train_loss:4.1276 train_time:635934ms step_avg:169.94ms\nstep:3753/10000 train_loss:4.0899 train_time:636104ms step_avg:169.94ms\nstep:3754/10000 train_loss:4.2181 train_time:636274ms step_avg:169.95ms\nstep:3755/10000 train_loss:4.2228 train_time:636442ms step_avg:169.94ms\nstep:3756/10000 train_loss:3.9442 train_time:636611ms step_avg:169.94ms\nstep:3757/10000 train_loss:4.1042 train_time:636780ms step_avg:169.94ms\nstep:3758/10000 train_loss:4.1424 train_time:636951ms step_avg:169.94ms\nstep:3759/10000 train_loss:4.1080 train_time:637121ms step_avg:169.94ms\nstep:3760/10000 train_loss:4.1892 train_time:637289ms step_avg:169.94ms\nstep:3761/10000 train_loss:4.1815 train_time:637461ms step_avg:169.94ms\nstep:3762/10000 train_loss:4.3286 train_time:637630ms step_avg:169.94ms\nstep:3763/10000 train_loss:4.0647 train_time:637800ms step_avg:169.94ms\nstep:3764/10000 train_loss:4.0436 train_time:637969ms step_avg:169.94ms\nstep:3765/10000 train_loss:4.1906 train_time:638140ms step_avg:169.94ms\nstep:3766/10000 train_loss:4.1154 train_time:638309ms step_avg:169.94ms\nstep:3767/10000 train_loss:4.0406 train_time:638481ms step_avg:169.94ms\nstep:3768/10000 train_loss:4.0048 train_time:638648ms step_avg:169.94ms\nstep:3769/10000 train_loss:4.1109 train_time:638819ms step_avg:169.94ms\nstep:3770/10000 train_loss:4.1662 train_time:638987ms step_avg:169.94ms\nstep:3771/10000 train_loss:4.7245 train_time:639158ms step_avg:169.94ms\nstep:3772/10000 train_loss:4.1558 train_time:639327ms step_avg:169.94ms\nstep:3773/10000 train_loss:4.1560 train_time:639498ms step_avg:169.94ms\nstep:3774/10000 train_loss:3.9914 train_time:639667ms step_avg:169.94ms\nstep:3775/10000 train_loss:3.9803 train_time:639838ms step_avg:169.94ms\nstep:3776/10000 train_loss:4.0544 train_time:640006ms step_avg:169.94ms\nstep:3777/10000 train_loss:4.3253 train_time:640177ms step_avg:169.94ms\nstep:3778/10000 train_loss:4.2846 train_time:640345ms step_avg:169.94ms\nstep:3779/10000 train_loss:4.3847 train_time:640518ms step_avg:169.94ms\nstep:3780/10000 train_loss:4.0921 train_time:640688ms step_avg:169.94ms\nstep:3781/10000 train_loss:4.1423 train_time:640858ms step_avg:169.94ms\nstep:3782/10000 train_loss:4.0197 train_time:641026ms step_avg:169.94ms\nstep:3783/10000 train_loss:4.0957 train_time:641195ms step_avg:169.94ms\nstep:3784/10000 train_loss:4.3640 train_time:641365ms step_avg:169.94ms\nstep:3785/10000 train_loss:4.2012 train_time:641536ms step_avg:169.94ms\nstep:3786/10000 train_loss:4.0536 train_time:641703ms step_avg:169.94ms\nstep:3787/10000 train_loss:4.3016 train_time:641875ms step_avg:169.94ms\nstep:3788/10000 train_loss:4.1359 train_time:642045ms step_avg:169.94ms\nstep:3789/10000 train_loss:4.0588 train_time:642213ms step_avg:169.94ms\nstep:3790/10000 train_loss:5.5672 train_time:642379ms step_avg:169.94ms\nstep:3791/10000 train_loss:4.0647 train_time:642550ms step_avg:169.94ms\nstep:3792/10000 train_loss:4.2071 train_time:642720ms step_avg:169.94ms\nstep:3793/10000 train_loss:4.1954 train_time:642890ms step_avg:169.94ms\nstep:3794/10000 train_loss:4.1942 train_time:643061ms step_avg:169.94ms\nstep:3795/10000 train_loss:4.4887 train_time:643231ms step_avg:169.94ms\nstep:3796/10000 train_loss:4.3822 train_time:643402ms step_avg:169.94ms\nstep:3797/10000 train_loss:4.1790 train_time:643571ms step_avg:169.94ms\nstep:3798/10000 train_loss:4.1145 train_time:643742ms step_avg:169.94ms\nstep:3799/10000 train_loss:4.1433 train_time:643912ms step_avg:169.94ms\nstep:3800/10000 train_loss:3.9781 train_time:644081ms step_avg:169.94ms\nstep:3801/10000 train_loss:4.1642 train_time:644249ms step_avg:169.94ms\nstep:3802/10000 train_loss:4.0185 train_time:644420ms step_avg:169.94ms\nstep:3803/10000 train_loss:4.0837 train_time:644590ms step_avg:169.94ms\nstep:3804/10000 train_loss:4.0715 train_time:644760ms step_avg:169.94ms\nstep:3805/10000 train_loss:4.0466 train_time:644933ms step_avg:169.94ms\nstep:3806/10000 train_loss:4.0714 train_time:645104ms step_avg:169.94ms\nstep:3807/10000 train_loss:3.9936 train_time:645273ms step_avg:169.94ms\nstep:3808/10000 train_loss:4.3513 train_time:645443ms step_avg:169.94ms\nstep:3809/10000 train_loss:4.1302 train_time:645612ms step_avg:169.94ms\nstep:3810/10000 train_loss:4.2959 train_time:646002ms step_avg:170.00ms\nstep:3811/10000 train_loss:4.1656 train_time:646164ms step_avg:170.00ms\nstep:3812/10000 train_loss:4.0278 train_time:646332ms step_avg:170.00ms\nstep:3813/10000 train_loss:4.0883 train_time:646496ms step_avg:170.00ms\nstep:3814/10000 train_loss:4.1960 train_time:646665ms step_avg:170.00ms\nstep:3815/10000 train_loss:4.1335 train_time:646834ms step_avg:170.00ms\nstep:3816/10000 train_loss:4.0916 train_time:647011ms step_avg:170.00ms\nstep:3817/10000 train_loss:4.1180 train_time:647184ms step_avg:170.00ms\nstep:3818/10000 train_loss:3.8870 train_time:647352ms step_avg:170.00ms\nstep:3819/10000 train_loss:4.0704 train_time:647520ms step_avg:170.00ms\nstep:3820/10000 train_loss:4.1427 train_time:647688ms step_avg:170.00ms\nstep:3821/10000 train_loss:4.0912 train_time:647858ms step_avg:170.00ms\nstep:3822/10000 train_loss:4.0866 train_time:648028ms step_avg:170.00ms\nstep:3823/10000 train_loss:4.0496 train_time:648199ms step_avg:170.00ms\nstep:3824/10000 train_loss:4.0092 train_time:648369ms step_avg:170.00ms\nstep:3825/10000 train_loss:4.1764 train_time:648538ms step_avg:170.00ms\nstep:3826/10000 train_loss:4.0611 train_time:648705ms step_avg:170.00ms\nstep:3827/10000 train_loss:4.0762 train_time:648875ms step_avg:170.00ms\nstep:3828/10000 train_loss:4.0949 train_time:649044ms step_avg:170.00ms\nstep:3829/10000 train_loss:4.0716 train_time:649215ms step_avg:170.00ms\nstep:3830/10000 train_loss:4.0832 train_time:649384ms step_avg:170.00ms\nstep:3831/10000 train_loss:4.1734 train_time:649552ms step_avg:170.00ms\nstep:3832/10000 train_loss:4.0392 train_time:649722ms step_avg:170.00ms\nstep:3833/10000 train_loss:4.6363 train_time:649891ms step_avg:170.00ms\nstep:3834/10000 train_loss:4.0836 train_time:650060ms step_avg:169.99ms\nstep:3835/10000 train_loss:4.0406 train_time:650232ms step_avg:170.00ms\nstep:3836/10000 train_loss:4.1091 train_time:650400ms step_avg:169.99ms\nstep:3837/10000 train_loss:4.1559 train_time:650569ms step_avg:169.99ms\nstep:3838/10000 train_loss:4.2503 train_time:650738ms step_avg:169.99ms\nstep:3839/10000 train_loss:4.1150 train_time:650908ms step_avg:169.99ms\nstep:3840/10000 train_loss:4.0956 train_time:651077ms step_avg:169.99ms\nstep:3841/10000 train_loss:4.3528 train_time:651248ms step_avg:169.99ms\nstep:3842/10000 train_loss:4.2928 train_time:651419ms step_avg:169.99ms\nstep:3843/10000 train_loss:4.0101 train_time:651588ms step_avg:169.99ms\nstep:3844/10000 train_loss:4.1167 train_time:651757ms step_avg:169.99ms\nstep:3845/10000 train_loss:4.3685 train_time:651926ms step_avg:169.99ms\nstep:3846/10000 train_loss:4.2237 train_time:652095ms step_avg:169.99ms\nstep:3847/10000 train_loss:4.1521 train_time:652264ms step_avg:169.99ms\nstep:3848/10000 train_loss:3.9767 train_time:652435ms step_avg:169.99ms\nstep:3849/10000 train_loss:4.2751 train_time:652602ms step_avg:169.99ms\nstep:3850/10000 train_loss:4.1190 train_time:652771ms step_avg:169.99ms\nstep:3851/10000 train_loss:4.1380 train_time:652939ms step_avg:169.99ms\nstep:3852/10000 train_loss:4.2804 train_time:653108ms step_avg:169.99ms\nstep:3853/10000 train_loss:4.1191 train_time:653277ms step_avg:169.99ms\nstep:3854/10000 train_loss:3.9830 train_time:653450ms step_avg:169.99ms\nstep:3855/10000 train_loss:4.0892 train_time:653622ms step_avg:169.99ms\nstep:3856/10000 train_loss:4.2148 train_time:653790ms step_avg:169.99ms\nstep:3857/10000 train_loss:4.1405 train_time:653957ms step_avg:169.99ms\nstep:3858/10000 train_loss:4.0859 train_time:654123ms step_avg:169.99ms\nstep:3859/10000 train_loss:4.0405 train_time:654293ms step_avg:169.99ms\nstep:3860/10000 train_loss:4.0596 train_time:654462ms step_avg:169.99ms\nstep:3861/10000 train_loss:4.1453 train_time:654633ms step_avg:169.99ms\nstep:3862/10000 train_loss:4.1896 train_time:654804ms step_avg:169.99ms\nstep:3863/10000 train_loss:4.2439 train_time:654973ms step_avg:169.99ms\nstep:3864/10000 train_loss:4.0438 train_time:655141ms step_avg:169.99ms\nstep:3865/10000 train_loss:3.9846 train_time:655312ms step_avg:169.99ms\nstep:3866/10000 train_loss:4.2395 train_time:655482ms step_avg:169.99ms\nstep:3867/10000 train_loss:4.0876 train_time:655651ms step_avg:169.99ms\nstep:3868/10000 train_loss:4.0850 train_time:655818ms step_avg:169.99ms\nstep:3869/10000 train_loss:4.1246 train_time:655986ms step_avg:169.99ms\nstep:3870/10000 train_loss:4.1058 train_time:656154ms step_avg:169.99ms\nstep:3871/10000 train_loss:4.0896 train_time:656325ms step_avg:169.99ms\nstep:3872/10000 train_loss:4.2878 train_time:656497ms step_avg:169.99ms\nstep:3873/10000 train_loss:4.1686 train_time:656667ms step_avg:169.99ms\nstep:3874/10000 train_loss:4.1713 train_time:656836ms step_avg:169.99ms\nstep:3875/10000 train_loss:4.0136 train_time:657005ms step_avg:169.99ms\nstep:3876/10000 train_loss:4.1416 train_time:657175ms step_avg:169.99ms\nstep:3877/10000 train_loss:4.1379 train_time:657344ms step_avg:169.99ms\nstep:3878/10000 train_loss:4.1462 train_time:657516ms step_avg:169.99ms\nstep:3879/10000 train_loss:4.1782 train_time:657685ms step_avg:169.99ms\nstep:3880/10000 train_loss:4.1429 train_time:657857ms step_avg:169.99ms\nstep:3881/10000 train_loss:4.0370 train_time:658026ms step_avg:169.99ms\nstep:3882/10000 train_loss:4.1711 train_time:658198ms step_avg:169.99ms\nstep:3883/10000 train_loss:4.0405 train_time:658367ms step_avg:169.99ms\nstep:3884/10000 train_loss:4.1506 train_time:658539ms step_avg:169.99ms\nstep:3885/10000 train_loss:4.1108 train_time:658708ms step_avg:169.99ms\nstep:3886/10000 train_loss:4.0457 train_time:658879ms step_avg:169.99ms\nstep:3887/10000 train_loss:4.0051 train_time:659047ms step_avg:169.99ms\nstep:3888/10000 train_loss:4.0022 train_time:659218ms step_avg:169.99ms\nstep:3889/10000 train_loss:4.2565 train_time:659386ms step_avg:169.99ms\nstep:3890/10000 train_loss:3.9592 train_time:659556ms step_avg:169.99ms\nstep:3891/10000 train_loss:4.1503 train_time:659723ms step_avg:169.99ms\nstep:3892/10000 train_loss:4.1388 train_time:659895ms step_avg:169.99ms\nstep:3893/10000 train_loss:4.1166 train_time:660064ms step_avg:169.99ms\nstep:3894/10000 train_loss:4.1408 train_time:660236ms step_avg:169.99ms\nstep:3895/10000 train_loss:4.0186 train_time:660405ms step_avg:169.99ms\nstep:3896/10000 train_loss:4.1023 train_time:660576ms step_avg:169.99ms\nstep:3897/10000 train_loss:4.0595 train_time:660745ms step_avg:169.99ms\nstep:3898/10000 train_loss:3.9794 train_time:660914ms step_avg:169.99ms\nstep:3899/10000 train_loss:3.9667 train_time:661083ms step_avg:169.99ms\nstep:3900/10000 train_loss:4.1589 train_time:661257ms step_avg:169.99ms\nstep:3901/10000 train_loss:4.0863 train_time:661425ms step_avg:169.99ms\nstep:3902/10000 train_loss:4.1168 train_time:661596ms step_avg:169.99ms\nstep:3903/10000 train_loss:4.1875 train_time:661764ms step_avg:169.99ms\nstep:3904/10000 train_loss:4.2427 train_time:661934ms step_avg:169.99ms\nstep:3905/10000 train_loss:3.9945 train_time:662103ms step_avg:169.99ms\nstep:3906/10000 train_loss:4.1792 train_time:662274ms step_avg:169.99ms\nstep:3907/10000 train_loss:4.0743 train_time:662445ms step_avg:169.99ms\nstep:3908/10000 train_loss:4.0820 train_time:662617ms step_avg:169.99ms\nstep:3909/10000 train_loss:4.0840 train_time:662786ms step_avg:169.99ms\nstep:3910/10000 train_loss:4.0608 train_time:662956ms step_avg:169.99ms\nstep:3911/10000 train_loss:4.4217 train_time:663124ms step_avg:169.99ms\nstep:3912/10000 train_loss:4.2813 train_time:663296ms step_avg:169.99ms\nstep:3913/10000 train_loss:4.0617 train_time:663466ms step_avg:169.99ms\nstep:3914/10000 train_loss:4.0978 train_time:663636ms step_avg:169.99ms\nstep:3915/10000 train_loss:4.2565 train_time:663804ms step_avg:169.99ms\nstep:3916/10000 train_loss:4.1674 train_time:663974ms step_avg:169.99ms\nstep:3917/10000 train_loss:4.0327 train_time:664143ms step_avg:169.99ms\nstep:3918/10000 train_loss:4.1627 train_time:664314ms step_avg:169.99ms\nstep:3919/10000 train_loss:4.1455 train_time:664484ms step_avg:169.99ms\nstep:3920/10000 train_loss:4.0249 train_time:664655ms step_avg:169.99ms\nstep:3921/10000 train_loss:4.1861 train_time:664823ms step_avg:169.99ms\nstep:3922/10000 train_loss:3.9830 train_time:664991ms step_avg:169.99ms\nstep:3923/10000 train_loss:4.0246 train_time:665160ms step_avg:169.99ms\nstep:3924/10000 train_loss:4.3179 train_time:665329ms step_avg:169.99ms\nstep:3925/10000 train_loss:4.1547 train_time:665500ms step_avg:169.99ms\nstep:3926/10000 train_loss:4.0872 train_time:665668ms step_avg:169.99ms\nstep:3927/10000 train_loss:4.0652 train_time:665835ms step_avg:169.99ms\nstep:3928/10000 train_loss:4.0218 train_time:666004ms step_avg:169.99ms\nstep:3929/10000 train_loss:4.2186 train_time:666176ms step_avg:169.99ms\nstep:3930/10000 train_loss:4.0479 train_time:666345ms step_avg:169.99ms\nstep:3931/10000 train_loss:4.1445 train_time:666515ms step_avg:169.99ms\nstep:3932/10000 train_loss:4.1786 train_time:666685ms step_avg:169.99ms\nstep:3933/10000 train_loss:4.1041 train_time:666856ms step_avg:169.99ms\nstep:3934/10000 train_loss:4.0846 train_time:667024ms step_avg:169.99ms\nstep:3935/10000 train_loss:4.0965 train_time:667193ms step_avg:169.99ms\nstep:3936/10000 train_loss:4.1672 train_time:667364ms step_avg:169.99ms\nstep:3937/10000 train_loss:4.1236 train_time:667533ms step_avg:169.99ms\nstep:3938/10000 train_loss:4.2043 train_time:667702ms step_avg:169.99ms\nstep:3939/10000 train_loss:3.9554 train_time:667873ms step_avg:169.99ms\nstep:3940/10000 train_loss:4.2181 train_time:668041ms step_avg:169.99ms\nstep:3941/10000 train_loss:4.0983 train_time:668209ms step_avg:169.98ms\nstep:3942/10000 train_loss:4.0801 train_time:668379ms step_avg:169.98ms\nstep:3943/10000 train_loss:4.0536 train_time:668547ms step_avg:169.98ms\nstep:3944/10000 train_loss:4.1082 train_time:668719ms step_avg:169.98ms\nstep:3945/10000 train_loss:4.1912 train_time:668888ms step_avg:169.98ms\nstep:3946/10000 train_loss:4.0753 train_time:669058ms step_avg:169.98ms\nstep:3947/10000 train_loss:3.9913 train_time:669226ms step_avg:169.98ms\nstep:3948/10000 train_loss:4.2031 train_time:669394ms step_avg:169.98ms\nstep:3949/10000 train_loss:4.1583 train_time:669563ms step_avg:169.98ms\nstep:3950/10000 train_loss:4.0974 train_time:669735ms step_avg:169.98ms\nstep:3951/10000 train_loss:4.5541 train_time:669905ms step_avg:169.98ms\nstep:3952/10000 train_loss:4.0234 train_time:670075ms step_avg:169.98ms\nstep:3953/10000 train_loss:4.1914 train_time:670243ms step_avg:169.98ms\nstep:3954/10000 train_loss:4.2064 train_time:670413ms step_avg:169.98ms\nstep:3955/10000 train_loss:4.0127 train_time:670581ms step_avg:169.98ms\nstep:3956/10000 train_loss:4.0991 train_time:670750ms step_avg:169.98ms\nstep:3957/10000 train_loss:4.2590 train_time:670921ms step_avg:169.98ms\nstep:3958/10000 train_loss:3.9651 train_time:671091ms step_avg:169.98ms\nstep:3959/10000 train_loss:4.1921 train_time:671262ms step_avg:169.98ms\nstep:3960/10000 train_loss:4.0855 train_time:671432ms step_avg:169.98ms\nstep:3961/10000 train_loss:4.1499 train_time:671601ms step_avg:169.98ms\nstep:3962/10000 train_loss:4.0011 train_time:671770ms step_avg:169.98ms\nstep:3963/10000 train_loss:3.9285 train_time:671940ms step_avg:169.98ms\nstep:3964/10000 train_loss:4.0510 train_time:672109ms step_avg:169.98ms\nstep:3965/10000 train_loss:4.0746 train_time:672277ms step_avg:169.98ms\nstep:3966/10000 train_loss:4.1647 train_time:672446ms step_avg:169.98ms\nstep:3967/10000 train_loss:4.0291 train_time:672618ms step_avg:169.98ms\nstep:3968/10000 train_loss:4.1996 train_time:672784ms step_avg:169.98ms\nstep:3969/10000 train_loss:4.0294 train_time:672955ms step_avg:169.98ms\nstep:3970/10000 train_loss:4.0984 train_time:673124ms step_avg:169.98ms\nstep:3971/10000 train_loss:4.0978 train_time:673291ms step_avg:169.98ms\nstep:3972/10000 train_loss:4.0865 train_time:673459ms step_avg:169.98ms\nstep:3973/10000 train_loss:4.1226 train_time:673629ms step_avg:169.98ms\nstep:3974/10000 train_loss:4.1214 train_time:673798ms step_avg:169.98ms\nstep:3975/10000 train_loss:4.0116 train_time:673967ms step_avg:169.98ms\nstep:3976/10000 train_loss:4.0380 train_time:674139ms step_avg:169.98ms\nstep:3977/10000 train_loss:4.1587 train_time:674309ms step_avg:169.98ms\nstep:3978/10000 train_loss:4.0758 train_time:674478ms step_avg:169.98ms\nstep:3979/10000 train_loss:4.1387 train_time:674646ms step_avg:169.98ms\nstep:3980/10000 train_loss:4.1318 train_time:674817ms step_avg:169.98ms\nstep:3981/10000 train_loss:4.0153 train_time:674986ms step_avg:169.98ms\nstep:3982/10000 train_loss:4.0231 train_time:675157ms step_avg:169.98ms\nstep:3983/10000 train_loss:4.3743 train_time:675324ms step_avg:169.98ms\nstep:3984/10000 train_loss:4.1935 train_time:675496ms step_avg:169.98ms\nstep:3985/10000 train_loss:4.2206 train_time:675665ms step_avg:169.98ms\nstep:3986/10000 train_loss:4.0660 train_time:675836ms step_avg:169.98ms\nstep:3987/10000 train_loss:4.1682 train_time:676005ms step_avg:169.98ms\nstep:3988/10000 train_loss:4.1439 train_time:676176ms step_avg:169.98ms\nstep:3989/10000 train_loss:4.0472 train_time:676345ms step_avg:169.98ms\nstep:3990/10000 train_loss:4.0654 train_time:676515ms step_avg:169.98ms\nstep:3991/10000 train_loss:4.0295 train_time:676684ms step_avg:169.98ms\nstep:3992/10000 train_loss:4.0409 train_time:676854ms step_avg:169.98ms\nstep:3993/10000 train_loss:3.8764 train_time:677024ms step_avg:169.98ms\nstep:3994/10000 train_loss:4.1953 train_time:677196ms step_avg:169.98ms\nstep:3995/10000 train_loss:4.1096 train_time:677365ms step_avg:169.98ms\nstep:3996/10000 train_loss:4.4368 train_time:677537ms step_avg:169.98ms\nstep:3997/10000 train_loss:4.0131 train_time:677707ms step_avg:169.98ms\nstep:3998/10000 train_loss:4.0128 train_time:677877ms step_avg:169.98ms\nstep:3999/10000 train_loss:4.1265 train_time:678045ms step_avg:169.98ms\nstep:4000/10000 train_loss:4.0525 train_time:678215ms step_avg:169.98ms\nstep:4001/10000 train_loss:4.1548 train_time:678384ms step_avg:169.98ms\nstep:4002/10000 train_loss:4.2389 train_time:678556ms step_avg:169.98ms\nstep:4003/10000 train_loss:4.1392 train_time:678724ms step_avg:169.98ms\nstep:4004/10000 train_loss:4.1665 train_time:678893ms step_avg:169.98ms\nstep:4005/10000 train_loss:4.1328 train_time:679061ms step_avg:169.98ms\nstep:4006/10000 train_loss:4.1691 train_time:679232ms step_avg:169.98ms\nstep:4007/10000 train_loss:4.2230 train_time:679403ms step_avg:169.98ms\nstep:4008/10000 train_loss:4.0418 train_time:679575ms step_avg:169.98ms\nstep:4009/10000 train_loss:4.3105 train_time:679743ms step_avg:169.98ms\nstep:4010/10000 train_loss:4.1457 train_time:679911ms step_avg:169.98ms\nstep:4011/10000 train_loss:4.1762 train_time:680082ms step_avg:169.98ms\nstep:4012/10000 train_loss:4.1278 train_time:680252ms step_avg:169.98ms\nstep:4013/10000 train_loss:4.0853 train_time:680423ms step_avg:169.98ms\nstep:4014/10000 train_loss:4.1224 train_time:680594ms step_avg:169.98ms\nstep:4015/10000 train_loss:4.0895 train_time:680764ms step_avg:169.98ms\nstep:4016/10000 train_loss:3.9918 train_time:680935ms step_avg:169.98ms\nstep:4017/10000 train_loss:4.1614 train_time:681103ms step_avg:169.98ms\nstep:4018/10000 train_loss:4.1016 train_time:681271ms step_avg:169.98ms\nstep:4019/10000 train_loss:4.1517 train_time:681443ms step_avg:169.98ms\nstep:4020/10000 train_loss:4.1807 train_time:681615ms step_avg:169.98ms\nstep:4021/10000 train_loss:4.0410 train_time:681784ms step_avg:169.98ms\nstep:4022/10000 train_loss:4.1117 train_time:681953ms step_avg:169.98ms\nstep:4023/10000 train_loss:4.3288 train_time:682122ms step_avg:169.98ms\nstep:4024/10000 train_loss:4.2256 train_time:682290ms step_avg:169.98ms\nstep:4025/10000 train_loss:4.0950 train_time:682461ms step_avg:169.98ms\nstep:4026/10000 train_loss:4.0696 train_time:682630ms step_avg:169.98ms\nstep:4027/10000 train_loss:4.2880 train_time:682803ms step_avg:169.98ms\nstep:4028/10000 train_loss:4.0008 train_time:682974ms step_avg:169.98ms\nstep:4029/10000 train_loss:4.0107 train_time:683143ms step_avg:169.98ms\nstep:4030/10000 train_loss:4.0649 train_time:683310ms step_avg:169.98ms\nstep:4031/10000 train_loss:4.1695 train_time:683480ms step_avg:169.98ms\nstep:4032/10000 train_loss:4.0793 train_time:683650ms step_avg:169.98ms\nstep:4033/10000 train_loss:4.0380 train_time:683819ms step_avg:169.98ms\nstep:4034/10000 train_loss:4.0714 train_time:683988ms step_avg:169.98ms\nstep:4035/10000 train_loss:4.0461 train_time:684157ms step_avg:169.98ms\nstep:4036/10000 train_loss:4.0548 train_time:684326ms step_avg:169.98ms\nstep:4037/10000 train_loss:4.3210 train_time:684496ms step_avg:169.98ms\nstep:4038/10000 train_loss:4.0681 train_time:684664ms step_avg:169.98ms\nstep:4039/10000 train_loss:4.0969 train_time:684835ms step_avg:169.98ms\nstep:4040/10000 train_loss:4.1763 train_time:685003ms step_avg:169.98ms\nstep:4041/10000 train_loss:4.0539 train_time:685174ms step_avg:169.98ms\nstep:4042/10000 train_loss:4.3494 train_time:685343ms step_avg:169.98ms\nstep:4043/10000 train_loss:4.1594 train_time:685514ms step_avg:169.98ms\nstep:4044/10000 train_loss:4.2536 train_time:685682ms step_avg:169.98ms\nstep:4045/10000 train_loss:4.1322 train_time:685851ms step_avg:169.98ms\nstep:4046/10000 train_loss:4.2523 train_time:686018ms step_avg:169.97ms\nstep:4047/10000 train_loss:4.1999 train_time:686186ms step_avg:169.97ms\nstep:4048/10000 train_loss:4.1526 train_time:686357ms step_avg:169.97ms\nstep:4049/10000 train_loss:4.2234 train_time:686528ms step_avg:169.97ms\nstep:4050/10000 train_loss:4.0134 train_time:686698ms step_avg:169.97ms\nstep:4051/10000 train_loss:4.1473 train_time:686867ms step_avg:169.97ms\nstep:4052/10000 train_loss:4.1017 train_time:687038ms step_avg:169.97ms\nstep:4053/10000 train_loss:4.1090 train_time:687207ms step_avg:169.97ms\nstep:4054/10000 train_loss:4.0622 train_time:687377ms step_avg:169.97ms\nstep:4055/10000 train_loss:4.4294 train_time:687545ms step_avg:169.97ms\nstep:4056/10000 train_loss:3.9752 train_time:687716ms step_avg:169.97ms\nstep:4057/10000 train_loss:4.1825 train_time:687883ms step_avg:169.97ms\nstep:4058/10000 train_loss:4.0575 train_time:688052ms step_avg:169.97ms\nstep:4059/10000 train_loss:4.0867 train_time:688221ms step_avg:169.97ms\nstep:4060/10000 train_loss:4.1271 train_time:688391ms step_avg:169.97ms\nstep:4061/10000 train_loss:4.1506 train_time:688562ms step_avg:169.97ms\nstep:4062/10000 train_loss:4.1185 train_time:688732ms step_avg:169.97ms\nstep:4063/10000 train_loss:4.1729 train_time:688904ms step_avg:169.97ms\nstep:4064/10000 train_loss:4.2511 train_time:689073ms step_avg:169.97ms\nstep:4065/10000 train_loss:3.9923 train_time:689242ms step_avg:169.97ms\nstep:4066/10000 train_loss:4.1988 train_time:689411ms step_avg:169.97ms\nstep:4067/10000 train_loss:4.0306 train_time:689580ms step_avg:169.97ms\nstep:4068/10000 train_loss:3.9418 train_time:689752ms step_avg:169.97ms\nstep:4069/10000 train_loss:4.3361 train_time:689923ms step_avg:169.97ms\nstep:4070/10000 train_loss:4.0223 train_time:690092ms step_avg:169.97ms\nstep:4071/10000 train_loss:4.0587 train_time:690260ms step_avg:169.97ms\nstep:4072/10000 train_loss:4.0603 train_time:690429ms step_avg:169.97ms\nstep:4073/10000 train_loss:4.0832 train_time:690603ms step_avg:169.97ms\nstep:4074/10000 train_loss:4.0835 train_time:690770ms step_avg:169.97ms\nstep:4075/10000 train_loss:4.1025 train_time:690938ms step_avg:169.97ms\nstep:4076/10000 train_loss:4.0952 train_time:691105ms step_avg:169.97ms\nstep:4077/10000 train_loss:4.0913 train_time:691280ms step_avg:169.97ms\nstep:4078/10000 train_loss:4.0512 train_time:691445ms step_avg:169.97ms\nstep:4079/10000 train_loss:4.0961 train_time:691617ms step_avg:169.97ms\nstep:4080/10000 train_loss:3.9915 train_time:691784ms step_avg:169.97ms\nstep:4081/10000 train_loss:4.0001 train_time:691954ms step_avg:169.97ms\nstep:4082/10000 train_loss:4.2228 train_time:692122ms step_avg:169.97ms\nstep:4083/10000 train_loss:4.0959 train_time:692293ms step_avg:169.97ms\nstep:4084/10000 train_loss:4.1443 train_time:692465ms step_avg:169.97ms\nstep:4085/10000 train_loss:4.4444 train_time:692638ms step_avg:169.97ms\nstep:4086/10000 train_loss:3.9905 train_time:692808ms step_avg:169.97ms\nstep:4087/10000 train_loss:4.1223 train_time:692977ms step_avg:169.97ms\nstep:4088/10000 train_loss:4.0794 train_time:693146ms step_avg:169.97ms\nstep:4089/10000 train_loss:4.0718 train_time:693315ms step_avg:169.97ms\nstep:4090/10000 train_loss:4.3381 train_time:693484ms step_avg:169.97ms\nstep:4091/10000 train_loss:4.1119 train_time:693655ms step_avg:169.97ms\nstep:4092/10000 train_loss:3.9968 train_time:693829ms step_avg:169.97ms\nstep:4093/10000 train_loss:4.0458 train_time:693996ms step_avg:169.97ms\nstep:4094/10000 train_loss:4.1967 train_time:694163ms step_avg:169.97ms\nstep:4095/10000 train_loss:4.1532 train_time:694336ms step_avg:169.97ms\nstep:4096/10000 train_loss:4.0879 train_time:694511ms step_avg:169.97ms\nstep:4097/10000 train_loss:4.0839 train_time:694678ms step_avg:169.97ms\nstep:4098/10000 train_loss:3.9359 train_time:694851ms step_avg:169.97ms\nstep:4099/10000 train_loss:4.1749 train_time:695019ms step_avg:169.97ms\nstep:4100/10000 train_loss:4.1205 train_time:695184ms step_avg:169.97ms\nstep:4101/10000 train_loss:4.1842 train_time:695355ms step_avg:169.97ms\nstep:4102/10000 train_loss:4.0987 train_time:695525ms step_avg:169.97ms\nstep:4103/10000 train_loss:4.1032 train_time:695694ms step_avg:169.97ms\nstep:4104/10000 train_loss:4.0745 train_time:695864ms step_avg:169.97ms\nstep:4105/10000 train_loss:4.1046 train_time:696034ms step_avg:169.97ms\nstep:4106/10000 train_loss:4.0574 train_time:696204ms step_avg:169.97ms\nstep:4107/10000 train_loss:4.1516 train_time:696373ms step_avg:169.97ms\nstep:4108/10000 train_loss:4.1332 train_time:696542ms step_avg:169.97ms\nstep:4109/10000 train_loss:4.0081 train_time:696717ms step_avg:169.97ms\nstep:4110/10000 train_loss:4.0942 train_time:696883ms step_avg:169.97ms\nstep:4111/10000 train_loss:4.2179 train_time:697054ms step_avg:169.97ms\nstep:4112/10000 train_loss:4.2711 train_time:697224ms step_avg:169.97ms\nstep:4113/10000 train_loss:3.9915 train_time:697395ms step_avg:169.97ms\nstep:4114/10000 train_loss:4.0549 train_time:697564ms step_avg:169.97ms\nstep:4115/10000 train_loss:4.1297 train_time:697733ms step_avg:169.97ms\nstep:4116/10000 train_loss:3.9338 train_time:697903ms step_avg:169.97ms\nstep:4117/10000 train_loss:4.2441 train_time:698073ms step_avg:169.97ms\nstep:4118/10000 train_loss:3.9741 train_time:698241ms step_avg:169.97ms\nstep:4119/10000 train_loss:4.1243 train_time:698412ms step_avg:169.97ms\nstep:4120/10000 train_loss:3.9618 train_time:698582ms step_avg:169.97ms\nstep:4121/10000 train_loss:4.0920 train_time:698750ms step_avg:169.97ms\nstep:4122/10000 train_loss:4.0676 train_time:698920ms step_avg:169.97ms\nstep:4123/10000 train_loss:3.9722 train_time:699088ms step_avg:169.97ms\nstep:4124/10000 train_loss:3.9844 train_time:699256ms step_avg:169.97ms\nstep:4125/10000 train_loss:4.1458 train_time:699425ms step_avg:169.97ms\nstep:4126/10000 train_loss:4.4929 train_time:699597ms step_avg:169.97ms\nstep:4127/10000 train_loss:3.9917 train_time:699767ms step_avg:169.97ms\nstep:4128/10000 train_loss:4.0798 train_time:699938ms step_avg:169.97ms\nstep:4129/10000 train_loss:4.0132 train_time:700106ms step_avg:169.97ms\nstep:4130/10000 train_loss:4.0270 train_time:700278ms step_avg:169.97ms\nstep:4131/10000 train_loss:4.1066 train_time:700445ms step_avg:169.97ms\nstep:4132/10000 train_loss:4.1104 train_time:700616ms step_avg:169.97ms\nstep:4133/10000 train_loss:3.9034 train_time:700784ms step_avg:169.97ms\nstep:4134/10000 train_loss:4.0155 train_time:700954ms step_avg:169.97ms\nstep:4135/10000 train_loss:4.0829 train_time:701122ms step_avg:169.97ms\nstep:4136/10000 train_loss:4.2672 train_time:701293ms step_avg:169.97ms\nstep:4137/10000 train_loss:4.1179 train_time:701464ms step_avg:169.97ms\nstep:4138/10000 train_loss:4.1017 train_time:701634ms step_avg:169.97ms\nstep:4139/10000 train_loss:4.1691 train_time:701800ms step_avg:169.97ms\nstep:4140/10000 train_loss:4.0944 train_time:701969ms step_avg:169.97ms\nstep:4141/10000 train_loss:4.0563 train_time:702136ms step_avg:169.97ms\nstep:4142/10000 train_loss:4.1109 train_time:702306ms step_avg:169.97ms\nstep:4143/10000 train_loss:4.1719 train_time:702478ms step_avg:169.97ms\nstep:4144/10000 train_loss:4.0786 train_time:702647ms step_avg:169.97ms\nstep:4145/10000 train_loss:4.1154 train_time:702815ms step_avg:169.97ms\nstep:4146/10000 train_loss:3.9578 train_time:702984ms step_avg:169.97ms\nstep:4147/10000 train_loss:4.2502 train_time:703153ms step_avg:169.97ms\nstep:4148/10000 train_loss:4.0150 train_time:703324ms step_avg:169.97ms\nstep:4149/10000 train_loss:4.0503 train_time:703495ms step_avg:169.97ms\nstep:4150/10000 train_loss:3.9331 train_time:703663ms step_avg:169.97ms\nstep:4151/10000 train_loss:4.0934 train_time:703834ms step_avg:169.97ms\nstep:4152/10000 train_loss:4.1164 train_time:704003ms step_avg:169.97ms\nstep:4153/10000 train_loss:4.0259 train_time:704175ms step_avg:169.97ms\nstep:4154/10000 train_loss:4.1512 train_time:704346ms step_avg:169.97ms\nstep:4155/10000 train_loss:4.2269 train_time:704519ms step_avg:169.97ms\nstep:4156/10000 train_loss:4.4354 train_time:704686ms step_avg:169.97ms\nstep:4157/10000 train_loss:4.0565 train_time:704857ms step_avg:169.97ms\nstep:4158/10000 train_loss:3.9527 train_time:705025ms step_avg:169.97ms\nstep:4159/10000 train_loss:4.1715 train_time:705194ms step_avg:169.97ms\nstep:4160/10000 train_loss:4.2189 train_time:705365ms step_avg:169.97ms\nstep:4161/10000 train_loss:4.1061 train_time:705536ms step_avg:169.97ms\nstep:4162/10000 train_loss:3.9924 train_time:705705ms step_avg:169.97ms\nstep:4163/10000 train_loss:4.0466 train_time:705875ms step_avg:169.97ms\nstep:4164/10000 train_loss:4.1127 train_time:706044ms step_avg:169.97ms\nstep:4165/10000 train_loss:3.9911 train_time:706214ms step_avg:169.97ms\nstep:4166/10000 train_loss:4.0964 train_time:706382ms step_avg:169.97ms\nstep:4167/10000 train_loss:4.1055 train_time:706553ms step_avg:169.97ms\nstep:4168/10000 train_loss:4.0228 train_time:706722ms step_avg:169.97ms\nstep:4169/10000 train_loss:4.1009 train_time:706894ms step_avg:169.97ms\nstep:4170/10000 train_loss:3.9825 train_time:707063ms step_avg:169.97ms\nstep:4171/10000 train_loss:4.1574 train_time:707233ms step_avg:169.97ms\nstep:4172/10000 train_loss:4.0611 train_time:707401ms step_avg:169.97ms\nstep:4173/10000 train_loss:4.2772 train_time:707570ms step_avg:169.97ms\nstep:4174/10000 train_loss:4.0141 train_time:707739ms step_avg:169.97ms\nstep:4175/10000 train_loss:4.0165 train_time:707908ms step_avg:169.97ms\nstep:4176/10000 train_loss:4.1256 train_time:708077ms step_avg:169.97ms\nstep:4177/10000 train_loss:4.0949 train_time:708244ms step_avg:169.96ms\nstep:4178/10000 train_loss:4.1178 train_time:708412ms step_avg:169.96ms\nstep:4179/10000 train_loss:4.2698 train_time:708580ms step_avg:169.96ms\nstep:4180/10000 train_loss:4.0430 train_time:708749ms step_avg:169.96ms\nstep:4181/10000 train_loss:4.0568 train_time:708919ms step_avg:169.96ms\nstep:4182/10000 train_loss:4.2525 train_time:709090ms step_avg:169.96ms\nstep:4183/10000 train_loss:4.1814 train_time:709260ms step_avg:169.96ms\nstep:4184/10000 train_loss:4.0503 train_time:709430ms step_avg:169.96ms\nstep:4185/10000 train_loss:4.0209 train_time:709599ms step_avg:169.96ms\nstep:4186/10000 train_loss:3.9279 train_time:709768ms step_avg:169.96ms\nstep:4187/10000 train_loss:4.0338 train_time:709936ms step_avg:169.96ms\nstep:4188/10000 train_loss:4.1526 train_time:710104ms step_avg:169.96ms\nstep:4189/10000 train_loss:4.1260 train_time:710275ms step_avg:169.96ms\nstep:4190/10000 train_loss:4.2849 train_time:710443ms step_avg:169.96ms\nstep:4191/10000 train_loss:4.0595 train_time:710616ms step_avg:169.96ms\nstep:4192/10000 train_loss:4.0570 train_time:710785ms step_avg:169.96ms\nstep:4193/10000 train_loss:4.0693 train_time:710961ms step_avg:169.96ms\nstep:4194/10000 train_loss:3.9804 train_time:711125ms step_avg:169.96ms\nstep:4195/10000 train_loss:4.0692 train_time:711293ms step_avg:169.96ms\nstep:4196/10000 train_loss:4.2892 train_time:711464ms step_avg:169.96ms\nstep:4197/10000 train_loss:4.0523 train_time:711636ms step_avg:169.96ms\nstep:4198/10000 train_loss:4.5364 train_time:711803ms step_avg:169.96ms\nstep:4199/10000 train_loss:4.0779 train_time:711976ms step_avg:169.96ms\nstep:4200/10000 train_loss:4.1036 train_time:712144ms step_avg:169.96ms\nstep:4201/10000 train_loss:3.9123 train_time:712316ms step_avg:169.96ms\nstep:4202/10000 train_loss:4.1193 train_time:712483ms step_avg:169.96ms\nstep:4203/10000 train_loss:4.1061 train_time:712653ms step_avg:169.96ms\nstep:4204/10000 train_loss:4.0521 train_time:712821ms step_avg:169.96ms\nstep:4205/10000 train_loss:4.1290 train_time:712990ms step_avg:169.96ms\nstep:4206/10000 train_loss:4.0240 train_time:713160ms step_avg:169.96ms\nstep:4207/10000 train_loss:4.0416 train_time:713331ms step_avg:169.96ms\nstep:4208/10000 train_loss:3.9957 train_time:713503ms step_avg:169.96ms\nstep:4209/10000 train_loss:4.0807 train_time:713672ms step_avg:169.96ms\nstep:4210/10000 train_loss:3.9869 train_time:713843ms step_avg:169.96ms\nstep:4211/10000 train_loss:4.0674 train_time:714012ms step_avg:169.96ms\nstep:4212/10000 train_loss:4.1188 train_time:714182ms step_avg:169.96ms\nstep:4213/10000 train_loss:4.0912 train_time:714350ms step_avg:169.96ms\nstep:4214/10000 train_loss:4.1001 train_time:714522ms step_avg:169.96ms\nstep:4215/10000 train_loss:4.1402 train_time:714691ms step_avg:169.96ms\nstep:4216/10000 train_loss:4.0157 train_time:714862ms step_avg:169.96ms\nstep:4217/10000 train_loss:4.0564 train_time:715030ms step_avg:169.96ms\nstep:4218/10000 train_loss:4.0596 train_time:715199ms step_avg:169.96ms\nstep:4219/10000 train_loss:4.1410 train_time:715370ms step_avg:169.96ms\nstep:4220/10000 train_loss:4.3231 train_time:715540ms step_avg:169.96ms\nstep:4221/10000 train_loss:4.1213 train_time:715708ms step_avg:169.96ms\nstep:4222/10000 train_loss:4.0653 train_time:715878ms step_avg:169.96ms\nstep:4223/10000 train_loss:3.9985 train_time:716047ms step_avg:169.96ms\nstep:4224/10000 train_loss:4.1341 train_time:716216ms step_avg:169.96ms\nstep:4225/10000 train_loss:4.1622 train_time:716384ms step_avg:169.96ms\nstep:4226/10000 train_loss:4.0114 train_time:716554ms step_avg:169.96ms\nstep:4227/10000 train_loss:3.9727 train_time:716724ms step_avg:169.96ms\nstep:4228/10000 train_loss:4.0128 train_time:716895ms step_avg:169.96ms\nstep:4229/10000 train_loss:3.9924 train_time:717065ms step_avg:169.96ms\nstep:4230/10000 train_loss:4.0422 train_time:717234ms step_avg:169.96ms\nstep:4231/10000 train_loss:4.0728 train_time:717404ms step_avg:169.96ms\nstep:4232/10000 train_loss:4.1106 train_time:717574ms step_avg:169.96ms\nstep:4233/10000 train_loss:3.9744 train_time:717743ms step_avg:169.96ms\nstep:4234/10000 train_loss:4.1232 train_time:717914ms step_avg:169.96ms\nstep:4235/10000 train_loss:3.9686 train_time:718082ms step_avg:169.96ms\nstep:4236/10000 train_loss:4.1729 train_time:718250ms step_avg:169.96ms\nstep:4237/10000 train_loss:4.0542 train_time:718422ms step_avg:169.96ms\nstep:4238/10000 train_loss:4.2528 train_time:718593ms step_avg:169.96ms\nstep:4239/10000 train_loss:4.0144 train_time:718763ms step_avg:169.96ms\nstep:4240/10000 train_loss:3.9101 train_time:718934ms step_avg:169.96ms\nstep:4241/10000 train_loss:4.0179 train_time:719104ms step_avg:169.96ms\nstep:4242/10000 train_loss:4.0539 train_time:719273ms step_avg:169.96ms\nstep:4243/10000 train_loss:4.0658 train_time:719442ms step_avg:169.96ms\nstep:4244/10000 train_loss:4.0729 train_time:719613ms step_avg:169.96ms\nstep:4245/10000 train_loss:4.1119 train_time:719784ms step_avg:169.96ms\nstep:4246/10000 train_loss:4.2727 train_time:719955ms step_avg:169.96ms\nstep:4247/10000 train_loss:4.1071 train_time:720123ms step_avg:169.96ms\nstep:4248/10000 train_loss:4.0662 train_time:720293ms step_avg:169.96ms\nstep:4249/10000 train_loss:4.0544 train_time:720463ms step_avg:169.96ms\nstep:4250/10000 train_loss:3.9776 train_time:720633ms step_avg:169.96ms\nstep:4251/10000 train_loss:3.8861 train_time:720806ms step_avg:169.96ms\nstep:4252/10000 train_loss:4.0316 train_time:720977ms step_avg:169.96ms\nstep:4253/10000 train_loss:4.2103 train_time:721145ms step_avg:169.96ms\nstep:4254/10000 train_loss:4.0145 train_time:721314ms step_avg:169.96ms\nstep:4255/10000 train_loss:4.1440 train_time:721485ms step_avg:169.96ms\nstep:4256/10000 train_loss:3.9999 train_time:721652ms step_avg:169.96ms\nstep:4257/10000 train_loss:4.0188 train_time:721824ms step_avg:169.96ms\nstep:4258/10000 train_loss:4.1856 train_time:721996ms step_avg:169.96ms\nstep:4259/10000 train_loss:4.0730 train_time:722162ms step_avg:169.96ms\nstep:4260/10000 train_loss:4.1193 train_time:722333ms step_avg:169.96ms\nstep:4261/10000 train_loss:4.0083 train_time:722502ms step_avg:169.96ms\nstep:4262/10000 train_loss:4.0931 train_time:722672ms step_avg:169.96ms\nstep:4263/10000 train_loss:4.0506 train_time:722843ms step_avg:169.96ms\nstep:4264/10000 train_loss:4.0230 train_time:723016ms step_avg:169.96ms\nstep:4265/10000 train_loss:4.1350 train_time:723186ms step_avg:169.96ms\nstep:4266/10000 train_loss:4.2130 train_time:723356ms step_avg:169.96ms\nstep:4267/10000 train_loss:4.1813 train_time:723524ms step_avg:169.96ms\nstep:4268/10000 train_loss:3.9737 train_time:723694ms step_avg:169.96ms\nstep:4269/10000 train_loss:4.1328 train_time:723865ms step_avg:169.96ms\nstep:4270/10000 train_loss:3.7619 train_time:724036ms step_avg:169.96ms\nstep:4271/10000 train_loss:4.1958 train_time:724204ms step_avg:169.96ms\nstep:4272/10000 train_loss:3.9803 train_time:724375ms step_avg:169.96ms\nstep:4273/10000 train_loss:4.1469 train_time:724544ms step_avg:169.96ms\nstep:4274/10000 train_loss:3.9757 train_time:724714ms step_avg:169.96ms\nstep:4275/10000 train_loss:4.0577 train_time:724885ms step_avg:169.96ms\nstep:4276/10000 train_loss:3.9396 train_time:725057ms step_avg:169.96ms\nstep:4277/10000 train_loss:4.1053 train_time:725224ms step_avg:169.96ms\nstep:4278/10000 train_loss:4.0787 train_time:725393ms step_avg:169.96ms\nstep:4279/10000 train_loss:4.0127 train_time:725562ms step_avg:169.96ms\nstep:4280/10000 train_loss:4.0118 train_time:725730ms step_avg:169.96ms\nstep:4281/10000 train_loss:4.0014 train_time:725903ms step_avg:169.96ms\nstep:4282/10000 train_loss:4.0746 train_time:726076ms step_avg:169.96ms\nstep:4283/10000 train_loss:4.2690 train_time:726243ms step_avg:169.96ms\nstep:4284/10000 train_loss:3.9311 train_time:726413ms step_avg:169.96ms\nstep:4285/10000 train_loss:4.1400 train_time:726582ms step_avg:169.96ms\nstep:4286/10000 train_loss:4.0260 train_time:726749ms step_avg:169.96ms\nstep:4287/10000 train_loss:4.1485 train_time:726919ms step_avg:169.96ms\nstep:4288/10000 train_loss:4.1261 train_time:727089ms step_avg:169.96ms\nstep:4289/10000 train_loss:4.1991 train_time:727257ms step_avg:169.96ms\nstep:4290/10000 train_loss:3.9603 train_time:727426ms step_avg:169.96ms\nstep:4291/10000 train_loss:4.0169 train_time:727595ms step_avg:169.96ms\nstep:4292/10000 train_loss:4.0995 train_time:727765ms step_avg:169.96ms\nstep:4293/10000 train_loss:3.9997 train_time:727937ms step_avg:169.96ms\nstep:4294/10000 train_loss:4.0189 train_time:728105ms step_avg:169.96ms\nstep:4295/10000 train_loss:4.0184 train_time:728273ms step_avg:169.96ms\nstep:4296/10000 train_loss:4.0611 train_time:728441ms step_avg:169.96ms\nstep:4297/10000 train_loss:4.0117 train_time:728611ms step_avg:169.96ms\nstep:4298/10000 train_loss:4.1031 train_time:728778ms step_avg:169.96ms\nstep:4299/10000 train_loss:4.1555 train_time:728948ms step_avg:169.96ms\nstep:4300/10000 train_loss:4.1138 train_time:729122ms step_avg:169.96ms\nstep:4301/10000 train_loss:3.8501 train_time:729291ms step_avg:169.96ms\nstep:4302/10000 train_loss:3.9743 train_time:729459ms step_avg:169.96ms\nstep:4303/10000 train_loss:3.9678 train_time:729628ms step_avg:169.96ms\nstep:4304/10000 train_loss:4.1006 train_time:729796ms step_avg:169.96ms\nstep:4305/10000 train_loss:4.1964 train_time:729963ms step_avg:169.96ms\nstep:4306/10000 train_loss:4.1846 train_time:730132ms step_avg:169.96ms\nstep:4307/10000 train_loss:3.9801 train_time:730302ms step_avg:169.96ms\nstep:4308/10000 train_loss:4.0011 train_time:730472ms step_avg:169.96ms\nstep:4309/10000 train_loss:4.1309 train_time:730638ms step_avg:169.96ms\nstep:4310/10000 train_loss:4.0757 train_time:730807ms step_avg:169.96ms\nstep:4311/10000 train_loss:4.0979 train_time:730976ms step_avg:169.95ms\nstep:4312/10000 train_loss:4.3728 train_time:731144ms step_avg:169.95ms\nstep:4313/10000 train_loss:4.1747 train_time:731314ms step_avg:169.95ms\nstep:4314/10000 train_loss:4.0519 train_time:731485ms step_avg:169.95ms\nstep:4315/10000 train_loss:4.0794 train_time:731655ms step_avg:169.95ms\nstep:4316/10000 train_loss:4.1368 train_time:731821ms step_avg:169.95ms\nstep:4317/10000 train_loss:3.9430 train_time:731990ms step_avg:169.95ms\nstep:4318/10000 train_loss:4.0335 train_time:732163ms step_avg:169.95ms\nstep:4319/10000 train_loss:4.0739 train_time:732335ms step_avg:169.95ms\nstep:4320/10000 train_loss:3.8249 train_time:732505ms step_avg:169.95ms\nstep:4321/10000 train_loss:4.0137 train_time:732676ms step_avg:169.95ms\nstep:4322/10000 train_loss:4.1179 train_time:732845ms step_avg:169.95ms\nstep:4323/10000 train_loss:4.0946 train_time:733014ms step_avg:169.95ms\nstep:4324/10000 train_loss:4.1282 train_time:733184ms step_avg:169.95ms\nstep:4325/10000 train_loss:4.0605 train_time:733355ms step_avg:169.95ms\nstep:4326/10000 train_loss:4.1317 train_time:733524ms step_avg:169.95ms\nstep:4327/10000 train_loss:4.0295 train_time:733695ms step_avg:169.95ms\nstep:4328/10000 train_loss:4.0554 train_time:733862ms step_avg:169.95ms\nstep:4329/10000 train_loss:4.0773 train_time:734035ms step_avg:169.95ms\nstep:4330/10000 train_loss:4.0732 train_time:734203ms step_avg:169.95ms\nstep:4331/10000 train_loss:3.9772 train_time:734374ms step_avg:169.95ms\nstep:4332/10000 train_loss:4.1377 train_time:734540ms step_avg:169.95ms\nstep:4333/10000 train_loss:3.9617 train_time:734709ms step_avg:169.95ms\nstep:4334/10000 train_loss:4.0308 train_time:734880ms step_avg:169.95ms\nstep:4335/10000 train_loss:3.9946 train_time:735049ms step_avg:169.95ms\nstep:4336/10000 train_loss:4.2266 train_time:735223ms step_avg:169.95ms\nstep:4337/10000 train_loss:3.9152 train_time:735393ms step_avg:169.95ms\nstep:4338/10000 train_loss:3.8334 train_time:735563ms step_avg:169.95ms\nstep:4339/10000 train_loss:3.9866 train_time:735732ms step_avg:169.95ms\nstep:4340/10000 train_loss:4.1100 train_time:735902ms step_avg:169.95ms\nstep:4341/10000 train_loss:4.0132 train_time:736070ms step_avg:169.95ms\nstep:4342/10000 train_loss:3.9872 train_time:736240ms step_avg:169.95ms\nstep:4343/10000 train_loss:3.9656 train_time:736410ms step_avg:169.95ms\nstep:4344/10000 train_loss:4.1153 train_time:736580ms step_avg:169.95ms\nstep:4345/10000 train_loss:4.1171 train_time:736752ms step_avg:169.95ms\nstep:4346/10000 train_loss:4.1090 train_time:736922ms step_avg:169.95ms\nstep:4347/10000 train_loss:4.0018 train_time:737090ms step_avg:169.95ms\nstep:4348/10000 train_loss:4.0437 train_time:737261ms step_avg:169.95ms\nstep:4349/10000 train_loss:4.0853 train_time:737430ms step_avg:169.95ms\nstep:4350/10000 train_loss:3.9593 train_time:737603ms step_avg:169.95ms\nstep:4351/10000 train_loss:4.0504 train_time:737773ms step_avg:169.95ms\nstep:4352/10000 train_loss:4.2495 train_time:737941ms step_avg:169.95ms\nstep:4353/10000 train_loss:4.1189 train_time:738110ms step_avg:169.95ms\nstep:4354/10000 train_loss:3.9147 train_time:738280ms step_avg:169.95ms\nstep:4355/10000 train_loss:4.0943 train_time:738447ms step_avg:169.95ms\nstep:4356/10000 train_loss:4.1747 train_time:738614ms step_avg:169.95ms\nstep:4357/10000 train_loss:4.0972 train_time:738781ms step_avg:169.95ms\nstep:4358/10000 train_loss:4.0593 train_time:738951ms step_avg:169.95ms\nstep:4359/10000 train_loss:3.9022 train_time:739122ms step_avg:169.95ms\nstep:4360/10000 train_loss:3.9478 train_time:739295ms step_avg:169.95ms\nstep:4361/10000 train_loss:3.9821 train_time:739465ms step_avg:169.95ms\nstep:4362/10000 train_loss:4.0968 train_time:739635ms step_avg:169.95ms\nstep:4363/10000 train_loss:3.9975 train_time:739804ms step_avg:169.95ms\nstep:4364/10000 train_loss:4.0002 train_time:739973ms step_avg:169.95ms\nstep:4365/10000 train_loss:4.0187 train_time:740142ms step_avg:169.95ms\nstep:4366/10000 train_loss:4.1070 train_time:740314ms step_avg:169.95ms\nstep:4367/10000 train_loss:4.0047 train_time:740485ms step_avg:169.95ms\nstep:4368/10000 train_loss:4.1163 train_time:740655ms step_avg:169.95ms\nstep:4369/10000 train_loss:4.0723 train_time:740823ms step_avg:169.95ms\nstep:4370/10000 train_loss:4.0847 train_time:740992ms step_avg:169.95ms\nstep:4371/10000 train_loss:3.9934 train_time:741161ms step_avg:169.95ms\nstep:4372/10000 train_loss:3.9075 train_time:741331ms step_avg:169.95ms\nstep:4373/10000 train_loss:4.0200 train_time:741501ms step_avg:169.95ms\nstep:4374/10000 train_loss:3.9711 train_time:741671ms step_avg:169.95ms\nstep:4375/10000 train_loss:4.0409 train_time:741839ms step_avg:169.95ms\nstep:4376/10000 train_loss:4.1180 train_time:742007ms step_avg:169.95ms\nstep:4377/10000 train_loss:3.9882 train_time:742176ms step_avg:169.95ms\nstep:4378/10000 train_loss:3.9689 train_time:742346ms step_avg:169.95ms\nstep:4379/10000 train_loss:4.0302 train_time:742516ms step_avg:169.95ms\nstep:4380/10000 train_loss:4.2268 train_time:742686ms step_avg:169.95ms\nstep:4381/10000 train_loss:4.0702 train_time:742855ms step_avg:169.95ms\nstep:4382/10000 train_loss:4.2117 train_time:743024ms step_avg:169.95ms\nstep:4383/10000 train_loss:4.0296 train_time:743194ms step_avg:169.95ms\nstep:4384/10000 train_loss:4.0758 train_time:743363ms step_avg:169.95ms\nstep:4385/10000 train_loss:4.2107 train_time:743532ms step_avg:169.95ms\nstep:4386/10000 train_loss:4.0062 train_time:743700ms step_avg:169.95ms\nstep:4387/10000 train_loss:3.9801 train_time:743869ms step_avg:169.95ms\nstep:4388/10000 train_loss:4.0298 train_time:744040ms step_avg:169.95ms\nstep:4389/10000 train_loss:4.1136 train_time:744208ms step_avg:169.95ms\nstep:4390/10000 train_loss:3.9914 train_time:744379ms step_avg:169.95ms\nstep:4391/10000 train_loss:4.0767 train_time:744548ms step_avg:169.95ms\nstep:4392/10000 train_loss:4.0588 train_time:744717ms step_avg:169.95ms\nstep:4393/10000 train_loss:4.1287 train_time:744885ms step_avg:169.95ms\nstep:4394/10000 train_loss:4.0180 train_time:745054ms step_avg:169.95ms\nstep:4395/10000 train_loss:3.9614 train_time:745222ms step_avg:169.95ms\nstep:4396/10000 train_loss:4.1093 train_time:745394ms step_avg:169.95ms\nstep:4397/10000 train_loss:4.0651 train_time:745566ms step_avg:169.95ms\nstep:4398/10000 train_loss:4.0806 train_time:745735ms step_avg:169.95ms\nstep:4399/10000 train_loss:4.0848 train_time:745906ms step_avg:169.95ms\nstep:4400/10000 train_loss:4.1539 train_time:746077ms step_avg:169.95ms\nstep:4401/10000 train_loss:3.9569 train_time:746249ms step_avg:169.95ms\nstep:4402/10000 train_loss:4.1474 train_time:746421ms step_avg:169.95ms\nstep:4403/10000 train_loss:4.0161 train_time:746590ms step_avg:169.95ms\nstep:4404/10000 train_loss:4.1058 train_time:746764ms step_avg:169.95ms\nstep:4405/10000 train_loss:4.0230 train_time:746929ms step_avg:169.95ms\nstep:4406/10000 train_loss:4.0059 train_time:747099ms step_avg:169.95ms\nstep:4407/10000 train_loss:3.9499 train_time:747268ms step_avg:169.95ms\nstep:4408/10000 train_loss:4.0715 train_time:747439ms step_avg:169.95ms\nstep:4409/10000 train_loss:4.0818 train_time:747607ms step_avg:169.95ms\nstep:4410/10000 train_loss:4.0602 train_time:747777ms step_avg:169.95ms\nstep:4411/10000 train_loss:3.9761 train_time:747945ms step_avg:169.95ms\nstep:4412/10000 train_loss:3.9152 train_time:748114ms step_avg:169.95ms\nstep:4413/10000 train_loss:4.0890 train_time:748283ms step_avg:169.95ms\nstep:4414/10000 train_loss:3.9901 train_time:748457ms step_avg:169.95ms\nstep:4415/10000 train_loss:4.1906 train_time:748623ms step_avg:169.95ms\nstep:4416/10000 train_loss:4.0210 train_time:748795ms step_avg:169.95ms\nstep:4417/10000 train_loss:4.1278 train_time:748964ms step_avg:169.95ms\nstep:4418/10000 train_loss:3.9487 train_time:749136ms step_avg:169.95ms\nstep:4419/10000 train_loss:4.3292 train_time:749302ms step_avg:169.95ms\nstep:4420/10000 train_loss:4.0517 train_time:749476ms step_avg:169.95ms\nstep:4421/10000 train_loss:4.1106 train_time:749641ms step_avg:169.95ms\nstep:4422/10000 train_loss:4.0777 train_time:749810ms step_avg:169.95ms\nstep:4423/10000 train_loss:4.0625 train_time:749981ms step_avg:169.95ms\nstep:4424/10000 train_loss:3.9440 train_time:750153ms step_avg:169.95ms\nstep:4425/10000 train_loss:4.2859 train_time:750323ms step_avg:169.95ms\nstep:4426/10000 train_loss:4.1665 train_time:750492ms step_avg:169.95ms\nstep:4427/10000 train_loss:4.3141 train_time:750661ms step_avg:169.95ms\nstep:4428/10000 train_loss:4.6691 train_time:750832ms step_avg:169.95ms\nstep:4429/10000 train_loss:4.0692 train_time:751002ms step_avg:169.95ms\nstep:4430/10000 train_loss:4.0007 train_time:751172ms step_avg:169.95ms\nstep:4431/10000 train_loss:4.1298 train_time:751342ms step_avg:169.95ms\nstep:4432/10000 train_loss:3.8533 train_time:751511ms step_avg:169.95ms\nstep:4433/10000 train_loss:4.1089 train_time:751677ms step_avg:169.95ms\nstep:4434/10000 train_loss:4.2362 train_time:751845ms step_avg:169.95ms\nstep:4435/10000 train_loss:4.1298 train_time:752016ms step_avg:169.95ms\nstep:4436/10000 train_loss:4.1704 train_time:752185ms step_avg:169.95ms\nstep:4437/10000 train_loss:4.1209 train_time:752358ms step_avg:169.95ms\nstep:4438/10000 train_loss:3.9540 train_time:752527ms step_avg:169.95ms\nstep:4439/10000 train_loss:4.2604 train_time:752695ms step_avg:169.95ms\nstep:4440/10000 train_loss:3.9865 train_time:752863ms step_avg:169.95ms\nstep:4441/10000 train_loss:3.9980 train_time:753033ms step_avg:169.95ms\nstep:4442/10000 train_loss:4.0833 train_time:753200ms step_avg:169.95ms\nstep:4443/10000 train_loss:4.1770 train_time:753370ms step_avg:169.95ms\nstep:4444/10000 train_loss:4.0920 train_time:753544ms step_avg:169.95ms\nstep:4445/10000 train_loss:3.9895 train_time:753714ms step_avg:169.95ms\nstep:4446/10000 train_loss:4.0982 train_time:753883ms step_avg:169.95ms\nstep:4447/10000 train_loss:3.9486 train_time:754051ms step_avg:169.95ms\nstep:4448/10000 train_loss:4.0567 train_time:754221ms step_avg:169.95ms\nstep:4449/10000 train_loss:3.9997 train_time:754393ms step_avg:169.95ms\nstep:4450/10000 train_loss:4.0634 train_time:754562ms step_avg:169.95ms\nstep:4451/10000 train_loss:3.9954 train_time:754730ms step_avg:169.95ms\nstep:4452/10000 train_loss:4.0758 train_time:754896ms step_avg:169.95ms\nstep:4453/10000 train_loss:3.8703 train_time:755065ms step_avg:169.94ms\nstep:4454/10000 train_loss:4.0444 train_time:755235ms step_avg:169.94ms\nstep:4455/10000 train_loss:3.9118 train_time:755404ms step_avg:169.94ms\nstep:4456/10000 train_loss:4.1730 train_time:755574ms step_avg:169.94ms\nstep:4457/10000 train_loss:4.1341 train_time:755741ms step_avg:169.94ms\nstep:4458/10000 train_loss:3.9467 train_time:755911ms step_avg:169.94ms\nstep:4459/10000 train_loss:4.0617 train_time:756081ms step_avg:169.94ms\nstep:4460/10000 train_loss:4.0426 train_time:756252ms step_avg:169.94ms\nstep:4461/10000 train_loss:3.9998 train_time:756421ms step_avg:169.94ms\nstep:4462/10000 train_loss:4.1414 train_time:756592ms step_avg:169.94ms\nstep:4463/10000 train_loss:4.1546 train_time:756762ms step_avg:169.94ms\nstep:4464/10000 train_loss:3.9622 train_time:756931ms step_avg:169.94ms\nstep:4465/10000 train_loss:4.1206 train_time:757101ms step_avg:169.94ms\nstep:4466/10000 train_loss:4.1312 train_time:757272ms step_avg:169.94ms\nstep:4467/10000 train_loss:4.2684 train_time:757443ms step_avg:169.94ms\nstep:4468/10000 train_loss:3.9050 train_time:757613ms step_avg:169.94ms\nstep:4469/10000 train_loss:4.0545 train_time:757782ms step_avg:169.94ms\nstep:4470/10000 train_loss:3.9610 train_time:757953ms step_avg:169.94ms\nstep:4471/10000 train_loss:3.9728 train_time:758124ms step_avg:169.94ms\nstep:4472/10000 train_loss:4.0489 train_time:758293ms step_avg:169.94ms\nstep:4473/10000 train_loss:4.1481 train_time:758461ms step_avg:169.94ms\nstep:4474/10000 train_loss:4.2135 train_time:758629ms step_avg:169.94ms\nstep:4475/10000 train_loss:3.9252 train_time:758799ms step_avg:169.94ms\nstep:4476/10000 train_loss:4.1995 train_time:758967ms step_avg:169.94ms\nstep:4477/10000 train_loss:4.0154 train_time:759140ms step_avg:169.94ms\nstep:4478/10000 train_loss:4.0472 train_time:759309ms step_avg:169.94ms\nstep:4479/10000 train_loss:4.1691 train_time:759480ms step_avg:169.94ms\nstep:4480/10000 train_loss:4.2564 train_time:759649ms step_avg:169.94ms\nstep:4481/10000 train_loss:4.0689 train_time:759819ms step_avg:169.94ms\nstep:4482/10000 train_loss:4.0476 train_time:759988ms step_avg:169.94ms\nstep:4483/10000 train_loss:4.0528 train_time:760161ms step_avg:169.94ms\nstep:4484/10000 train_loss:4.0779 train_time:760331ms step_avg:169.94ms\nstep:4485/10000 train_loss:4.1459 train_time:760502ms step_avg:169.94ms\nstep:4486/10000 train_loss:4.1194 train_time:760673ms step_avg:169.94ms\nstep:4487/10000 train_loss:4.0101 train_time:760841ms step_avg:169.94ms\nstep:4488/10000 train_loss:4.1077 train_time:761008ms step_avg:169.94ms\nstep:4489/10000 train_loss:3.9773 train_time:761178ms step_avg:169.94ms\nstep:4490/10000 train_loss:4.3034 train_time:761347ms step_avg:169.94ms\nstep:4491/10000 train_loss:4.1386 train_time:761516ms step_avg:169.94ms\nstep:4492/10000 train_loss:4.0104 train_time:761684ms step_avg:169.94ms\nstep:4493/10000 train_loss:4.1198 train_time:761855ms step_avg:169.94ms\nstep:4494/10000 train_loss:4.0085 train_time:762024ms step_avg:169.94ms\nstep:4495/10000 train_loss:3.9982 train_time:762194ms step_avg:169.94ms\nstep:4496/10000 train_loss:4.0542 train_time:762363ms step_avg:169.94ms\nstep:4497/10000 train_loss:4.0720 train_time:762535ms step_avg:169.94ms\nstep:4498/10000 train_loss:4.0057 train_time:762705ms step_avg:169.94ms\nstep:4499/10000 train_loss:4.0398 train_time:762876ms step_avg:169.94ms\nstep:4500/10000 train_loss:4.2035 train_time:763044ms step_avg:169.94ms\nstep:4501/10000 train_loss:3.9739 train_time:763214ms step_avg:169.94ms\nstep:4502/10000 train_loss:4.0176 train_time:763384ms step_avg:169.94ms\nstep:4503/10000 train_loss:4.0958 train_time:763554ms step_avg:169.94ms\nstep:4504/10000 train_loss:3.9865 train_time:763723ms step_avg:169.94ms\nstep:4505/10000 train_loss:4.1242 train_time:763893ms step_avg:169.94ms\nstep:4506/10000 train_loss:4.0688 train_time:764060ms step_avg:169.94ms\nstep:4507/10000 train_loss:4.0063 train_time:764230ms step_avg:169.94ms\nstep:4508/10000 train_loss:3.8836 train_time:764403ms step_avg:169.94ms\nstep:4509/10000 train_loss:4.1005 train_time:764569ms step_avg:169.94ms\nstep:4510/10000 train_loss:4.1604 train_time:764747ms step_avg:169.94ms\nstep:4511/10000 train_loss:4.2414 train_time:764913ms step_avg:169.94ms\nstep:4512/10000 train_loss:4.0951 train_time:765082ms step_avg:169.94ms\nstep:4513/10000 train_loss:3.9630 train_time:765250ms step_avg:169.94ms\nstep:4514/10000 train_loss:3.9339 train_time:765419ms step_avg:169.94ms\nstep:4515/10000 train_loss:4.0210 train_time:765586ms step_avg:169.94ms\nstep:4516/10000 train_loss:4.3044 train_time:765757ms step_avg:169.94ms\nstep:4517/10000 train_loss:4.0300 train_time:765927ms step_avg:169.94ms\nstep:4518/10000 train_loss:3.9910 train_time:766097ms step_avg:169.94ms\nstep:4519/10000 train_loss:4.2007 train_time:766266ms step_avg:169.94ms\nstep:4520/10000 train_loss:3.8441 train_time:766436ms step_avg:169.94ms\nstep:4521/10000 train_loss:3.9231 train_time:766604ms step_avg:169.94ms\nstep:4522/10000 train_loss:4.0923 train_time:766774ms step_avg:169.94ms\nstep:4523/10000 train_loss:4.0162 train_time:766942ms step_avg:169.94ms\nstep:4524/10000 train_loss:4.1581 train_time:767114ms step_avg:169.94ms\nstep:4525/10000 train_loss:4.0868 train_time:767279ms step_avg:169.94ms\nstep:4526/10000 train_loss:3.9530 train_time:767450ms step_avg:169.94ms\nstep:4527/10000 train_loss:4.0620 train_time:767619ms step_avg:169.94ms\nstep:4528/10000 train_loss:4.0098 train_time:767790ms step_avg:169.94ms\nstep:4529/10000 train_loss:4.0198 train_time:767966ms step_avg:169.94ms\nstep:4530/10000 train_loss:4.1085 train_time:768130ms step_avg:169.94ms\nstep:4531/10000 train_loss:3.8949 train_time:768300ms step_avg:169.94ms\nstep:4532/10000 train_loss:4.0150 train_time:768468ms step_avg:169.94ms\nstep:4533/10000 train_loss:4.1842 train_time:768638ms step_avg:169.94ms\nstep:4534/10000 train_loss:4.0294 train_time:768806ms step_avg:169.94ms\nstep:4535/10000 train_loss:3.9134 train_time:768977ms step_avg:169.94ms\nstep:4536/10000 train_loss:3.9944 train_time:769146ms step_avg:169.94ms\nstep:4537/10000 train_loss:4.1145 train_time:769317ms step_avg:169.94ms\nstep:4538/10000 train_loss:4.0434 train_time:769486ms step_avg:169.94ms\nstep:4539/10000 train_loss:4.0304 train_time:769655ms step_avg:169.94ms\nstep:4540/10000 train_loss:4.1674 train_time:769825ms step_avg:169.94ms\nstep:4541/10000 train_loss:4.0134 train_time:769994ms step_avg:169.94ms\nstep:4542/10000 train_loss:3.9906 train_time:770164ms step_avg:169.94ms\nstep:4543/10000 train_loss:3.9290 train_time:770335ms step_avg:169.94ms\nstep:4544/10000 train_loss:4.0870 train_time:770504ms step_avg:169.94ms\nstep:4545/10000 train_loss:4.1374 train_time:770674ms step_avg:169.94ms\nstep:4546/10000 train_loss:4.2557 train_time:770843ms step_avg:169.94ms\nstep:4547/10000 train_loss:3.9850 train_time:771012ms step_avg:169.94ms\nstep:4548/10000 train_loss:4.1072 train_time:771180ms step_avg:169.94ms\nstep:4549/10000 train_loss:4.1404 train_time:771351ms step_avg:169.94ms\nstep:4550/10000 train_loss:4.1371 train_time:771522ms step_avg:169.94ms\nstep:4551/10000 train_loss:4.0702 train_time:771697ms step_avg:169.94ms\nstep:4552/10000 train_loss:4.1320 train_time:771863ms step_avg:169.94ms\nstep:4553/10000 train_loss:3.9591 train_time:772031ms step_avg:169.94ms\nstep:4554/10000 train_loss:3.9641 train_time:772200ms step_avg:169.94ms\nstep:4555/10000 train_loss:3.9370 train_time:772368ms step_avg:169.94ms\nstep:4556/10000 train_loss:4.0725 train_time:772540ms step_avg:169.94ms\nstep:4557/10000 train_loss:4.0922 train_time:772709ms step_avg:169.94ms\nstep:4558/10000 train_loss:4.0939 train_time:772878ms step_avg:169.94ms\nstep:4559/10000 train_loss:4.0395 train_time:773046ms step_avg:169.94ms\nstep:4560/10000 train_loss:3.9767 train_time:773217ms step_avg:169.94ms\nstep:4561/10000 train_loss:3.9229 train_time:773385ms step_avg:169.94ms\nstep:4562/10000 train_loss:4.0070 train_time:773554ms step_avg:169.94ms\nstep:4563/10000 train_loss:4.0111 train_time:773722ms step_avg:169.94ms\nstep:4564/10000 train_loss:4.2551 train_time:773892ms step_avg:169.94ms\nstep:4565/10000 train_loss:3.9593 train_time:774062ms step_avg:169.94ms\nstep:4566/10000 train_loss:4.0400 train_time:774230ms step_avg:169.94ms\nstep:4567/10000 train_loss:4.0976 train_time:774399ms step_avg:169.94ms\nstep:4568/10000 train_loss:4.0712 train_time:774567ms step_avg:169.94ms\nstep:4569/10000 train_loss:3.9205 train_time:774739ms step_avg:169.94ms\nstep:4570/10000 train_loss:4.0804 train_time:774910ms step_avg:169.94ms\nstep:4571/10000 train_loss:3.9533 train_time:775080ms step_avg:169.94ms\nstep:4572/10000 train_loss:4.1754 train_time:775474ms step_avg:169.99ms\nstep:4573/10000 train_loss:4.0032 train_time:775637ms step_avg:169.98ms\nstep:4574/10000 train_loss:4.1260 train_time:775804ms step_avg:169.98ms\nstep:4575/10000 train_loss:3.9940 train_time:775974ms step_avg:169.98ms\nstep:4576/10000 train_loss:4.1870 train_time:776142ms step_avg:169.98ms\nstep:4577/10000 train_loss:4.1543 train_time:776309ms step_avg:169.98ms\nstep:4578/10000 train_loss:4.0777 train_time:776482ms step_avg:169.98ms\nstep:4579/10000 train_loss:4.0291 train_time:776657ms step_avg:169.98ms\nstep:4580/10000 train_loss:3.9043 train_time:776826ms step_avg:169.98ms\nstep:4581/10000 train_loss:4.1443 train_time:776995ms step_avg:169.98ms\nstep:4582/10000 train_loss:3.9944 train_time:777165ms step_avg:169.98ms\nstep:4583/10000 train_loss:4.0354 train_time:777335ms step_avg:169.98ms\nstep:4584/10000 train_loss:4.1456 train_time:777505ms step_avg:169.98ms\nstep:4585/10000 train_loss:3.9869 train_time:777675ms step_avg:169.98ms\nstep:4586/10000 train_loss:4.0857 train_time:777844ms step_avg:169.98ms\nstep:4587/10000 train_loss:3.9409 train_time:778014ms step_avg:169.98ms\nstep:4588/10000 train_loss:3.9563 train_time:778183ms step_avg:169.98ms\nstep:4589/10000 train_loss:4.0883 train_time:778353ms step_avg:169.98ms\nstep:4590/10000 train_loss:4.2122 train_time:778522ms step_avg:169.98ms\nstep:4591/10000 train_loss:3.9951 train_time:778693ms step_avg:169.98ms\nstep:4592/10000 train_loss:3.9986 train_time:778862ms step_avg:169.98ms\nstep:4593/10000 train_loss:4.0657 train_time:779032ms step_avg:169.98ms\nstep:4594/10000 train_loss:4.1342 train_time:779200ms step_avg:169.98ms\nstep:4595/10000 train_loss:4.3572 train_time:779369ms step_avg:169.98ms\nstep:4596/10000 train_loss:4.0858 train_time:779540ms step_avg:169.98ms\nstep:4597/10000 train_loss:4.0248 train_time:779709ms step_avg:169.98ms\nstep:4598/10000 train_loss:4.0634 train_time:779878ms step_avg:169.98ms\nstep:4599/10000 train_loss:4.1753 train_time:780046ms step_avg:169.98ms\nstep:4600/10000 train_loss:4.1279 train_time:780216ms step_avg:169.98ms\nstep:4601/10000 train_loss:3.9995 train_time:780384ms step_avg:169.98ms\nstep:4602/10000 train_loss:4.1054 train_time:780552ms step_avg:169.98ms\nstep:4603/10000 train_loss:4.1630 train_time:780723ms step_avg:169.98ms\nstep:4604/10000 train_loss:4.0919 train_time:780894ms step_avg:169.98ms\nstep:4605/10000 train_loss:4.0433 train_time:781061ms step_avg:169.98ms\nstep:4606/10000 train_loss:3.9682 train_time:781231ms step_avg:169.98ms\nstep:4607/10000 train_loss:3.9920 train_time:781399ms step_avg:169.98ms\nstep:4608/10000 train_loss:4.0098 train_time:781567ms step_avg:169.98ms\nstep:4609/10000 train_loss:3.9484 train_time:781738ms step_avg:169.98ms\nstep:4610/10000 train_loss:3.9625 train_time:781906ms step_avg:169.98ms\nstep:4611/10000 train_loss:4.1145 train_time:782137ms step_avg:169.99ms\nstep:4612/10000 train_loss:3.9460 train_time:782304ms step_avg:169.99ms\nstep:4613/10000 train_loss:4.0651 train_time:782472ms step_avg:169.99ms\nstep:4614/10000 train_loss:3.9562 train_time:782641ms step_avg:169.99ms\nstep:4615/10000 train_loss:4.0097 train_time:782809ms step_avg:169.99ms\nstep:4616/10000 train_loss:4.2238 train_time:782980ms step_avg:169.99ms\nstep:4617/10000 train_loss:3.8480 train_time:783156ms step_avg:169.99ms\nstep:4618/10000 train_loss:4.1559 train_time:783326ms step_avg:169.99ms\nstep:4619/10000 train_loss:4.1703 train_time:783497ms step_avg:169.99ms\nstep:4620/10000 train_loss:4.2109 train_time:783662ms step_avg:169.99ms\nstep:4621/10000 train_loss:4.2794 train_time:783829ms step_avg:169.99ms\nstep:4622/10000 train_loss:4.0852 train_time:783996ms step_avg:169.99ms\nstep:4623/10000 train_loss:3.9180 train_time:784166ms step_avg:169.99ms\nstep:4624/10000 train_loss:3.9064 train_time:784336ms step_avg:169.99ms\nstep:4625/10000 train_loss:4.1175 train_time:784507ms step_avg:169.99ms\nstep:4626/10000 train_loss:4.1151 train_time:784679ms step_avg:169.99ms\nstep:4627/10000 train_loss:3.9365 train_time:784848ms step_avg:169.99ms\nstep:4628/10000 train_loss:3.9167 train_time:785017ms step_avg:169.99ms\nstep:4629/10000 train_loss:4.1246 train_time:785186ms step_avg:169.99ms\nstep:4630/10000 train_loss:4.0168 train_time:785357ms step_avg:169.99ms\nstep:4631/10000 train_loss:4.0352 train_time:785525ms step_avg:169.99ms\nstep:4632/10000 train_loss:4.0549 train_time:785696ms step_avg:169.99ms\nstep:4633/10000 train_loss:4.0246 train_time:785865ms step_avg:169.99ms\nstep:4634/10000 train_loss:4.1345 train_time:786037ms step_avg:169.99ms\nstep:4635/10000 train_loss:4.1099 train_time:786204ms step_avg:169.99ms\nstep:4636/10000 train_loss:4.1135 train_time:786376ms step_avg:169.99ms\nstep:4637/10000 train_loss:4.1341 train_time:786546ms step_avg:169.99ms\nstep:4638/10000 train_loss:4.1122 train_time:786716ms step_avg:169.99ms\nstep:4639/10000 train_loss:3.9482 train_time:786884ms step_avg:169.99ms\nstep:4640/10000 train_loss:3.8429 train_time:787053ms step_avg:169.99ms\nstep:4641/10000 train_loss:4.1271 train_time:787221ms step_avg:169.99ms\nstep:4642/10000 train_loss:3.9752 train_time:787390ms step_avg:169.99ms\nstep:4643/10000 train_loss:4.3848 train_time:787562ms step_avg:169.99ms\nstep:4644/10000 train_loss:3.9136 train_time:787730ms step_avg:169.99ms\nstep:4645/10000 train_loss:4.0924 train_time:787900ms step_avg:169.99ms\nstep:4646/10000 train_loss:3.9268 train_time:788069ms step_avg:169.99ms\nstep:4647/10000 train_loss:4.1153 train_time:788238ms step_avg:169.99ms\nstep:4648/10000 train_loss:3.9219 train_time:788408ms step_avg:169.99ms\nstep:4649/10000 train_loss:3.9508 train_time:788577ms step_avg:169.99ms\nstep:4650/10000 train_loss:3.9256 train_time:788745ms step_avg:169.99ms\nstep:4651/10000 train_loss:3.9699 train_time:788913ms step_avg:169.99ms\nstep:4652/10000 train_loss:4.0942 train_time:789082ms step_avg:169.99ms\nstep:4653/10000 train_loss:4.0645 train_time:789250ms step_avg:169.99ms\nstep:4654/10000 train_loss:3.7578 train_time:789419ms step_avg:169.99ms\nstep:4655/10000 train_loss:3.9491 train_time:789588ms step_avg:169.99ms\nstep:4656/10000 train_loss:3.8889 train_time:789760ms step_avg:169.99ms\nstep:4657/10000 train_loss:3.9158 train_time:789929ms step_avg:169.99ms\nstep:4658/10000 train_loss:4.0729 train_time:790095ms step_avg:169.99ms\nstep:4659/10000 train_loss:4.0475 train_time:790262ms step_avg:169.99ms\nstep:4660/10000 train_loss:4.0326 train_time:790432ms step_avg:169.99ms\nstep:4661/10000 train_loss:4.1027 train_time:790602ms step_avg:169.99ms\nstep:4662/10000 train_loss:4.0317 train_time:790774ms step_avg:169.99ms\nstep:4663/10000 train_loss:3.8836 train_time:790944ms step_avg:169.99ms\nstep:4664/10000 train_loss:4.0947 train_time:791114ms step_avg:169.99ms\nstep:4665/10000 train_loss:4.0621 train_time:791282ms step_avg:169.99ms\nstep:4666/10000 train_loss:3.8962 train_time:791452ms step_avg:169.99ms\nstep:4667/10000 train_loss:4.1909 train_time:791619ms step_avg:169.98ms\nstep:4668/10000 train_loss:4.0430 train_time:791795ms step_avg:169.99ms\nstep:4669/10000 train_loss:3.9695 train_time:791960ms step_avg:169.99ms\nstep:4670/10000 train_loss:3.9203 train_time:792129ms step_avg:169.98ms\nstep:4671/10000 train_loss:4.0865 train_time:792299ms step_avg:169.98ms\nstep:4672/10000 train_loss:3.9929 train_time:792468ms step_avg:169.98ms\nstep:4673/10000 train_loss:4.0526 train_time:792639ms step_avg:169.98ms\nstep:4674/10000 train_loss:3.9836 train_time:792809ms step_avg:169.98ms\nstep:4675/10000 train_loss:4.0399 train_time:792979ms step_avg:169.98ms\nstep:4676/10000 train_loss:4.0023 train_time:793148ms step_avg:169.98ms\nstep:4677/10000 train_loss:4.1087 train_time:793316ms step_avg:169.98ms\nstep:4678/10000 train_loss:3.9621 train_time:793484ms step_avg:169.98ms\nstep:4679/10000 train_loss:3.9949 train_time:793654ms step_avg:169.98ms\nstep:4680/10000 train_loss:4.1870 train_time:793823ms step_avg:169.98ms\nstep:4681/10000 train_loss:4.0246 train_time:793992ms step_avg:169.98ms\nstep:4682/10000 train_loss:4.2938 train_time:794159ms step_avg:169.98ms\nstep:4683/10000 train_loss:4.0306 train_time:794327ms step_avg:169.98ms\nstep:4684/10000 train_loss:3.9970 train_time:794495ms step_avg:169.98ms\nstep:4685/10000 train_loss:4.0673 train_time:794664ms step_avg:169.98ms\nstep:4686/10000 train_loss:4.0040 train_time:794833ms step_avg:169.98ms\nstep:4687/10000 train_loss:3.9766 train_time:795003ms step_avg:169.98ms\nstep:4688/10000 train_loss:4.0796 train_time:795173ms step_avg:169.98ms\nstep:4689/10000 train_loss:3.9556 train_time:795341ms step_avg:169.98ms\nstep:4690/10000 train_loss:4.0276 train_time:795511ms step_avg:169.98ms\nstep:4691/10000 train_loss:4.0202 train_time:795679ms step_avg:169.98ms\nstep:4692/10000 train_loss:4.0598 train_time:795849ms step_avg:169.98ms\nstep:4693/10000 train_loss:4.0501 train_time:796020ms step_avg:169.98ms\nstep:4694/10000 train_loss:3.9650 train_time:796188ms step_avg:169.98ms\nstep:4695/10000 train_loss:3.8930 train_time:796357ms step_avg:169.98ms\nstep:4696/10000 train_loss:3.9356 train_time:796525ms step_avg:169.98ms\nstep:4697/10000 train_loss:3.8632 train_time:796695ms step_avg:169.98ms\nstep:4698/10000 train_loss:3.9976 train_time:796863ms step_avg:169.98ms\nstep:4699/10000 train_loss:4.2065 train_time:797033ms step_avg:169.98ms\nstep:4700/10000 train_loss:4.0370 train_time:797203ms step_avg:169.98ms\nstep:4701/10000 train_loss:4.0534 train_time:797375ms step_avg:169.98ms\nstep:4702/10000 train_loss:4.1615 train_time:797542ms step_avg:169.98ms\nstep:4703/10000 train_loss:4.0370 train_time:797711ms step_avg:169.98ms\nstep:4704/10000 train_loss:4.3699 train_time:797881ms step_avg:169.98ms\nstep:4705/10000 train_loss:4.1404 train_time:798051ms step_avg:169.98ms\nstep:4706/10000 train_loss:4.0012 train_time:798220ms step_avg:169.98ms\nstep:4707/10000 train_loss:3.9619 train_time:798392ms step_avg:169.98ms\nstep:4708/10000 train_loss:3.9844 train_time:798560ms step_avg:169.98ms\nstep:4709/10000 train_loss:4.0389 train_time:798729ms step_avg:169.98ms\nstep:4710/10000 train_loss:4.0335 train_time:798898ms step_avg:169.98ms\nstep:4711/10000 train_loss:3.9952 train_time:799068ms step_avg:169.98ms\nstep:4712/10000 train_loss:3.9319 train_time:799242ms step_avg:169.98ms\nstep:4713/10000 train_loss:3.9855 train_time:799412ms step_avg:169.98ms\nstep:4714/10000 train_loss:4.0757 train_time:799579ms step_avg:169.98ms\nstep:4715/10000 train_loss:4.1278 train_time:799747ms step_avg:169.98ms\nstep:4716/10000 train_loss:3.9453 train_time:799916ms step_avg:169.98ms\nstep:4717/10000 train_loss:4.0744 train_time:800083ms step_avg:169.98ms\nstep:4718/10000 train_loss:4.0507 train_time:800254ms step_avg:169.98ms\nstep:4719/10000 train_loss:4.0875 train_time:800424ms step_avg:169.98ms\nstep:4720/10000 train_loss:4.0462 train_time:800594ms step_avg:169.98ms\nstep:4721/10000 train_loss:4.0016 train_time:800765ms step_avg:169.98ms\nstep:4722/10000 train_loss:4.0976 train_time:800934ms step_avg:169.98ms\nstep:4723/10000 train_loss:3.9741 train_time:801103ms step_avg:169.98ms\nstep:4724/10000 train_loss:4.0909 train_time:801273ms step_avg:169.98ms\nstep:4725/10000 train_loss:3.9466 train_time:801443ms step_avg:169.98ms\nstep:4726/10000 train_loss:4.0275 train_time:801612ms step_avg:169.98ms\nstep:4727/10000 train_loss:4.1418 train_time:801781ms step_avg:169.98ms\nstep:4728/10000 train_loss:4.0160 train_time:801952ms step_avg:169.98ms\nstep:4729/10000 train_loss:3.9780 train_time:802122ms step_avg:169.98ms\nstep:4730/10000 train_loss:4.0477 train_time:802293ms step_avg:169.98ms\nstep:4731/10000 train_loss:4.0382 train_time:802461ms step_avg:169.98ms\nstep:4732/10000 train_loss:3.8825 train_time:802631ms step_avg:169.98ms\nstep:4733/10000 train_loss:3.9841 train_time:802801ms step_avg:169.98ms\nstep:4734/10000 train_loss:3.9615 train_time:802971ms step_avg:169.98ms\nstep:4735/10000 train_loss:4.0491 train_time:803139ms step_avg:169.98ms\nstep:4736/10000 train_loss:4.0076 train_time:803310ms step_avg:169.98ms\nstep:4737/10000 train_loss:3.9343 train_time:803482ms step_avg:169.98ms\nstep:4738/10000 train_loss:4.0283 train_time:803651ms step_avg:169.98ms\nstep:4739/10000 train_loss:4.1168 train_time:803821ms step_avg:169.98ms\nstep:4740/10000 train_loss:4.1474 train_time:803991ms step_avg:169.98ms\nstep:4741/10000 train_loss:4.0615 train_time:804161ms step_avg:169.98ms\nstep:4742/10000 train_loss:3.9579 train_time:804330ms step_avg:169.98ms\nstep:4743/10000 train_loss:3.9994 train_time:804499ms step_avg:169.98ms\nstep:4744/10000 train_loss:4.2783 train_time:804667ms step_avg:169.98ms\nstep:4745/10000 train_loss:3.9400 train_time:804834ms step_avg:169.98ms\nstep:4746/10000 train_loss:4.0970 train_time:805003ms step_avg:169.98ms\nstep:4747/10000 train_loss:3.9556 train_time:805175ms step_avg:169.98ms\nstep:4748/10000 train_loss:4.1194 train_time:805344ms step_avg:169.98ms\nstep:4749/10000 train_loss:3.8738 train_time:805516ms step_avg:169.98ms\nstep:4750/10000 train_loss:4.1612 train_time:805684ms step_avg:169.98ms\nstep:4751/10000 train_loss:4.1371 train_time:805853ms step_avg:169.98ms\nstep:4752/10000 train_loss:3.9925 train_time:806020ms step_avg:169.97ms\nstep:4753/10000 train_loss:3.9473 train_time:806188ms step_avg:169.97ms\nstep:4754/10000 train_loss:3.9387 train_time:806359ms step_avg:169.97ms\nstep:4755/10000 train_loss:4.0373 train_time:806528ms step_avg:169.97ms\nstep:4756/10000 train_loss:3.8709 train_time:806699ms step_avg:169.97ms\nstep:4757/10000 train_loss:4.0079 train_time:806867ms step_avg:169.97ms\nstep:4758/10000 train_loss:4.0108 train_time:807038ms step_avg:169.97ms\nstep:4759/10000 train_loss:4.0661 train_time:807209ms step_avg:169.97ms\nstep:4760/10000 train_loss:3.9653 train_time:807379ms step_avg:169.97ms\nstep:4761/10000 train_loss:3.9804 train_time:807550ms step_avg:169.97ms\nstep:4762/10000 train_loss:4.0484 train_time:807722ms step_avg:169.98ms\nstep:4763/10000 train_loss:4.2470 train_time:807889ms step_avg:169.97ms\nstep:4764/10000 train_loss:4.0126 train_time:808058ms step_avg:169.97ms\nstep:4765/10000 train_loss:3.8887 train_time:808226ms step_avg:169.97ms\nstep:4766/10000 train_loss:3.8865 train_time:808396ms step_avg:169.97ms\nstep:4767/10000 train_loss:4.2041 train_time:808569ms step_avg:169.97ms\nstep:4768/10000 train_loss:4.0588 train_time:808737ms step_avg:169.97ms\nstep:4769/10000 train_loss:4.0543 train_time:808904ms step_avg:169.97ms\nstep:4770/10000 train_loss:4.0669 train_time:809074ms step_avg:169.97ms\nstep:4771/10000 train_loss:3.8943 train_time:809243ms step_avg:169.97ms\nstep:4772/10000 train_loss:4.0321 train_time:809415ms step_avg:169.97ms\nstep:4773/10000 train_loss:4.1737 train_time:809585ms step_avg:169.97ms\nstep:4774/10000 train_loss:4.1889 train_time:809756ms step_avg:169.97ms\nstep:4775/10000 train_loss:4.1914 train_time:809924ms step_avg:169.97ms\nstep:4776/10000 train_loss:4.1213 train_time:810094ms step_avg:169.97ms\nstep:4777/10000 train_loss:4.0110 train_time:810262ms step_avg:169.97ms\nstep:4778/10000 train_loss:4.0129 train_time:810431ms step_avg:169.97ms\nstep:4779/10000 train_loss:3.9803 train_time:810603ms step_avg:169.97ms\nstep:4780/10000 train_loss:3.9353 train_time:810775ms step_avg:169.97ms\nstep:4781/10000 train_loss:4.5196 train_time:810942ms step_avg:169.97ms\nstep:4782/10000 train_loss:3.9052 train_time:811112ms step_avg:169.97ms\nstep:4783/10000 train_loss:3.9650 train_time:811282ms step_avg:169.97ms\nstep:4784/10000 train_loss:3.9828 train_time:811451ms step_avg:169.97ms\nstep:4785/10000 train_loss:3.9876 train_time:811622ms step_avg:169.97ms\nstep:4786/10000 train_loss:4.2043 train_time:811790ms step_avg:169.97ms\nstep:4787/10000 train_loss:3.9473 train_time:811961ms step_avg:169.97ms\nstep:4788/10000 train_loss:3.9516 train_time:812130ms step_avg:169.97ms\nstep:4789/10000 train_loss:3.8558 train_time:812303ms step_avg:169.97ms\nstep:4790/10000 train_loss:3.9828 train_time:812473ms step_avg:169.97ms\nstep:4791/10000 train_loss:4.0059 train_time:812643ms step_avg:169.97ms\nstep:4792/10000 train_loss:4.0211 train_time:812814ms step_avg:169.97ms\nstep:4793/10000 train_loss:3.8244 train_time:812982ms step_avg:169.97ms\nstep:4794/10000 train_loss:3.9774 train_time:813152ms step_avg:169.97ms\nstep:4795/10000 train_loss:4.1145 train_time:813322ms step_avg:169.97ms\nstep:4796/10000 train_loss:4.0200 train_time:813491ms step_avg:169.97ms\nstep:4797/10000 train_loss:4.0264 train_time:813663ms step_avg:169.97ms\nstep:4798/10000 train_loss:3.8879 train_time:813831ms step_avg:169.97ms\nstep:4799/10000 train_loss:3.9076 train_time:814000ms step_avg:169.97ms\nstep:4800/10000 train_loss:3.9562 train_time:814169ms step_avg:169.97ms\nstep:4801/10000 train_loss:3.8642 train_time:814339ms step_avg:169.97ms\nstep:4802/10000 train_loss:3.8672 train_time:814508ms step_avg:169.97ms\nstep:4803/10000 train_loss:3.9314 train_time:814681ms step_avg:169.97ms\nstep:4804/10000 train_loss:4.3651 train_time:814851ms step_avg:169.97ms\nstep:4805/10000 train_loss:4.0089 train_time:815020ms step_avg:169.97ms\nstep:4806/10000 train_loss:3.9465 train_time:815189ms step_avg:169.97ms\nstep:4807/10000 train_loss:4.0632 train_time:815359ms step_avg:169.97ms\nstep:4808/10000 train_loss:3.9241 train_time:815529ms step_avg:169.97ms\nstep:4809/10000 train_loss:3.9553 train_time:815701ms step_avg:169.97ms\nstep:4810/10000 train_loss:4.0084 train_time:815871ms step_avg:169.97ms\nstep:4811/10000 train_loss:3.9272 train_time:816047ms step_avg:169.97ms\nstep:4812/10000 train_loss:4.0032 train_time:816211ms step_avg:169.97ms\nstep:4813/10000 train_loss:3.9088 train_time:816381ms step_avg:169.97ms\nstep:4814/10000 train_loss:4.1241 train_time:816550ms step_avg:169.97ms\nstep:4815/10000 train_loss:4.0335 train_time:816721ms step_avg:169.97ms\nstep:4816/10000 train_loss:3.9875 train_time:816892ms step_avg:169.97ms\nstep:4817/10000 train_loss:3.8268 train_time:817059ms step_avg:169.97ms\nstep:4818/10000 train_loss:4.0672 train_time:817228ms step_avg:169.97ms\nstep:4819/10000 train_loss:3.9199 train_time:817398ms step_avg:169.97ms\nstep:4820/10000 train_loss:4.1069 train_time:817568ms step_avg:169.97ms\nstep:4821/10000 train_loss:3.9634 train_time:817739ms step_avg:169.97ms\nstep:4822/10000 train_loss:4.0425 train_time:817909ms step_avg:169.97ms\nstep:4823/10000 train_loss:3.9285 train_time:818079ms step_avg:169.97ms\nstep:4824/10000 train_loss:4.0873 train_time:818248ms step_avg:169.97ms\nstep:4825/10000 train_loss:4.0995 train_time:818419ms step_avg:169.97ms\nstep:4826/10000 train_loss:3.9876 train_time:818587ms step_avg:169.97ms\nstep:4827/10000 train_loss:3.9035 train_time:818758ms step_avg:169.97ms\nstep:4828/10000 train_loss:4.0419 train_time:818925ms step_avg:169.97ms\nstep:4829/10000 train_loss:4.1921 train_time:819095ms step_avg:169.97ms\nstep:4830/10000 train_loss:4.0148 train_time:819265ms step_avg:169.97ms\nstep:4831/10000 train_loss:4.0962 train_time:819436ms step_avg:169.97ms\nstep:4832/10000 train_loss:3.9639 train_time:819604ms step_avg:169.97ms\nstep:4833/10000 train_loss:4.1482 train_time:819775ms step_avg:169.97ms\nstep:4834/10000 train_loss:4.0637 train_time:819943ms step_avg:169.97ms\nstep:4835/10000 train_loss:3.9284 train_time:820112ms step_avg:169.97ms\nstep:4836/10000 train_loss:4.0023 train_time:820282ms step_avg:169.97ms\nstep:4837/10000 train_loss:3.8517 train_time:820452ms step_avg:169.97ms\nstep:4838/10000 train_loss:4.1242 train_time:820621ms step_avg:169.97ms\nstep:4839/10000 train_loss:4.1845 train_time:820790ms step_avg:169.97ms\nstep:4840/10000 train_loss:3.9439 train_time:820961ms step_avg:169.97ms\nstep:4841/10000 train_loss:3.9548 train_time:821129ms step_avg:169.97ms\nstep:4842/10000 train_loss:3.9985 train_time:821301ms step_avg:169.97ms\nstep:4843/10000 train_loss:3.9535 train_time:821471ms step_avg:169.97ms\nstep:4844/10000 train_loss:4.0265 train_time:821642ms step_avg:169.97ms\nstep:4845/10000 train_loss:3.9453 train_time:821812ms step_avg:169.97ms\nstep:4846/10000 train_loss:3.8547 train_time:821979ms step_avg:169.97ms\nstep:4847/10000 train_loss:3.9311 train_time:822148ms step_avg:169.97ms\nstep:4848/10000 train_loss:4.2703 train_time:822318ms step_avg:169.97ms\nstep:4849/10000 train_loss:3.9221 train_time:822488ms step_avg:169.97ms\nstep:4850/10000 train_loss:3.9064 train_time:822657ms step_avg:169.97ms\nstep:4851/10000 train_loss:4.0467 train_time:822826ms step_avg:169.97ms\nstep:4852/10000 train_loss:3.9284 train_time:822995ms step_avg:169.97ms\nstep:4853/10000 train_loss:4.0240 train_time:823162ms step_avg:169.97ms\nstep:4854/10000 train_loss:4.0111 train_time:823331ms step_avg:169.97ms\nstep:4855/10000 train_loss:3.9959 train_time:823498ms step_avg:169.97ms\nstep:4856/10000 train_loss:4.1932 train_time:823667ms step_avg:169.97ms\nstep:4857/10000 train_loss:3.9108 train_time:823838ms step_avg:169.97ms\nstep:4858/10000 train_loss:4.0308 train_time:824009ms step_avg:169.97ms\nstep:4859/10000 train_loss:3.8993 train_time:824179ms step_avg:169.97ms\nstep:4860/10000 train_loss:4.0463 train_time:824347ms step_avg:169.97ms\nstep:4861/10000 train_loss:3.9695 train_time:824517ms step_avg:169.97ms\nstep:4862/10000 train_loss:4.0595 train_time:824685ms step_avg:169.97ms\nstep:4863/10000 train_loss:4.0851 train_time:824855ms step_avg:169.97ms\nstep:4864/10000 train_loss:4.0576 train_time:825026ms step_avg:169.97ms\nstep:4865/10000 train_loss:4.0039 train_time:825196ms step_avg:169.97ms\nstep:4866/10000 train_loss:4.0174 train_time:825364ms step_avg:169.97ms\nstep:4867/10000 train_loss:4.0478 train_time:825535ms step_avg:169.97ms\nstep:4868/10000 train_loss:4.0371 train_time:825706ms step_avg:169.97ms\nstep:4869/10000 train_loss:3.9677 train_time:825877ms step_avg:169.97ms\nstep:4870/10000 train_loss:4.0228 train_time:826047ms step_avg:169.97ms\nstep:4871/10000 train_loss:3.9484 train_time:826218ms step_avg:169.97ms\nstep:4872/10000 train_loss:4.0400 train_time:826387ms step_avg:169.97ms\nstep:4873/10000 train_loss:4.1797 train_time:826557ms step_avg:169.97ms\nstep:4874/10000 train_loss:3.9591 train_time:826726ms step_avg:169.97ms\nstep:4875/10000 train_loss:3.8360 train_time:826897ms step_avg:169.97ms\nstep:4876/10000 train_loss:4.0815 train_time:827068ms step_avg:169.97ms\nstep:4877/10000 train_loss:3.9840 train_time:827239ms step_avg:169.97ms\nstep:4878/10000 train_loss:3.9919 train_time:827409ms step_avg:169.97ms\nstep:4879/10000 train_loss:4.0537 train_time:827580ms step_avg:169.97ms\nstep:4880/10000 train_loss:3.9964 train_time:827749ms step_avg:169.97ms\nstep:4881/10000 train_loss:3.9025 train_time:827918ms step_avg:169.97ms\nstep:4882/10000 train_loss:4.0494 train_time:828087ms step_avg:169.97ms\nstep:4883/10000 train_loss:4.0268 train_time:828258ms step_avg:169.97ms\nstep:4884/10000 train_loss:4.0431 train_time:828427ms step_avg:169.97ms\nstep:4885/10000 train_loss:3.9768 train_time:828597ms step_avg:169.97ms\nstep:4886/10000 train_loss:3.8837 train_time:828764ms step_avg:169.97ms\nstep:4887/10000 train_loss:4.0486 train_time:828932ms step_avg:169.97ms\nstep:4888/10000 train_loss:4.0342 train_time:829101ms step_avg:169.97ms\nstep:4889/10000 train_loss:4.0972 train_time:829272ms step_avg:169.97ms\nstep:4890/10000 train_loss:4.0942 train_time:829442ms step_avg:169.97ms\nstep:4891/10000 train_loss:3.9349 train_time:829615ms step_avg:169.97ms\nstep:4892/10000 train_loss:4.1089 train_time:829786ms step_avg:169.97ms\nstep:4893/10000 train_loss:4.0304 train_time:829956ms step_avg:169.97ms\nstep:4894/10000 train_loss:4.1308 train_time:830125ms step_avg:169.97ms\nstep:4895/10000 train_loss:3.9282 train_time:830294ms step_avg:169.97ms\nstep:4896/10000 train_loss:4.0574 train_time:830463ms step_avg:169.97ms\nstep:4897/10000 train_loss:4.0077 train_time:830633ms step_avg:169.97ms\nstep:4898/10000 train_loss:4.0765 train_time:830802ms step_avg:169.97ms\nstep:4899/10000 train_loss:4.1068 train_time:830971ms step_avg:169.97ms\nstep:4900/10000 train_loss:4.0057 train_time:831140ms step_avg:169.97ms\nstep:4901/10000 train_loss:4.1911 train_time:831311ms step_avg:169.97ms\nstep:4902/10000 train_loss:3.8947 train_time:831483ms step_avg:169.97ms\nstep:4903/10000 train_loss:4.0223 train_time:831652ms step_avg:169.97ms\nstep:4904/10000 train_loss:3.9475 train_time:831821ms step_avg:169.97ms\nstep:4905/10000 train_loss:4.2243 train_time:831991ms step_avg:169.97ms\nstep:4906/10000 train_loss:3.9218 train_time:832163ms step_avg:169.97ms\nstep:4907/10000 train_loss:4.1825 train_time:832333ms step_avg:169.97ms\nstep:4908/10000 train_loss:4.0611 train_time:832501ms step_avg:169.97ms\nstep:4909/10000 train_loss:4.0660 train_time:832669ms step_avg:169.97ms\nstep:4910/10000 train_loss:3.9999 train_time:832840ms step_avg:169.97ms\nstep:4911/10000 train_loss:4.0252 train_time:833010ms step_avg:169.97ms\nstep:4912/10000 train_loss:3.8583 train_time:833181ms step_avg:169.97ms\nstep:4913/10000 train_loss:3.9867 train_time:833351ms step_avg:169.97ms\nstep:4914/10000 train_loss:3.9652 train_time:833522ms step_avg:169.97ms\nstep:4915/10000 train_loss:4.0529 train_time:833691ms step_avg:169.97ms\nstep:4916/10000 train_loss:4.2121 train_time:833859ms step_avg:169.97ms\nstep:4917/10000 train_loss:4.2007 train_time:834027ms step_avg:169.97ms\nstep:4918/10000 train_loss:3.8901 train_time:834197ms step_avg:169.97ms\nstep:4919/10000 train_loss:3.9799 train_time:834366ms step_avg:169.97ms\nstep:4920/10000 train_loss:3.9518 train_time:834536ms step_avg:169.97ms\nstep:4921/10000 train_loss:3.9743 train_time:834705ms step_avg:169.97ms\nstep:4922/10000 train_loss:4.0732 train_time:834873ms step_avg:169.97ms\nstep:4923/10000 train_loss:4.0780 train_time:835042ms step_avg:169.97ms\nstep:4924/10000 train_loss:3.9556 train_time:835213ms step_avg:169.97ms\nstep:4925/10000 train_loss:3.8452 train_time:835384ms step_avg:169.97ms\nstep:4926/10000 train_loss:3.9858 train_time:835553ms step_avg:169.97ms\nstep:4927/10000 train_loss:4.0028 train_time:835722ms step_avg:169.97ms\nstep:4928/10000 train_loss:4.1282 train_time:835891ms step_avg:169.97ms\nstep:4929/10000 train_loss:3.8063 train_time:836061ms step_avg:169.97ms\nstep:4930/10000 train_loss:4.0560 train_time:836233ms step_avg:169.97ms\nstep:4931/10000 train_loss:3.8430 train_time:836403ms step_avg:169.97ms\nstep:4932/10000 train_loss:3.8607 train_time:836573ms step_avg:169.97ms\nstep:4933/10000 train_loss:4.0191 train_time:836739ms step_avg:169.97ms\nstep:4934/10000 train_loss:4.2088 train_time:836909ms step_avg:169.97ms\nstep:4935/10000 train_loss:4.0489 train_time:837078ms step_avg:169.97ms\nstep:4936/10000 train_loss:3.9710 train_time:837246ms step_avg:169.96ms\nstep:4937/10000 train_loss:3.9244 train_time:837415ms step_avg:169.96ms\nstep:4938/10000 train_loss:4.1134 train_time:837585ms step_avg:169.96ms\nstep:4939/10000 train_loss:4.1706 train_time:837755ms step_avg:169.96ms\nstep:4940/10000 train_loss:3.9800 train_time:837923ms step_avg:169.96ms\nstep:4941/10000 train_loss:3.9648 train_time:838093ms step_avg:169.96ms\nstep:4942/10000 train_loss:4.0335 train_time:838261ms step_avg:169.96ms\nstep:4943/10000 train_loss:3.7614 train_time:838429ms step_avg:169.96ms\nstep:4944/10000 train_loss:4.0428 train_time:838596ms step_avg:169.96ms\nstep:4945/10000 train_loss:3.9042 train_time:838765ms step_avg:169.96ms\nstep:4946/10000 train_loss:4.1897 train_time:838936ms step_avg:169.96ms\nstep:4947/10000 train_loss:3.9020 train_time:839104ms step_avg:169.96ms\nstep:4948/10000 train_loss:3.9498 train_time:839276ms step_avg:169.96ms\nstep:4949/10000 train_loss:3.8730 train_time:839444ms step_avg:169.96ms\nstep:4950/10000 train_loss:3.9816 train_time:839614ms step_avg:169.96ms\nstep:4951/10000 train_loss:4.0077 train_time:839782ms step_avg:169.96ms\nstep:4952/10000 train_loss:3.9645 train_time:839951ms step_avg:169.96ms\nstep:4953/10000 train_loss:4.0013 train_time:840123ms step_avg:169.96ms\nstep:4954/10000 train_loss:3.9352 train_time:840295ms step_avg:169.96ms\nstep:4955/10000 train_loss:4.0732 train_time:840466ms step_avg:169.96ms\nstep:4956/10000 train_loss:4.1959 train_time:840635ms step_avg:169.96ms\nstep:4957/10000 train_loss:4.0060 train_time:840805ms step_avg:169.96ms\nstep:4958/10000 train_loss:3.8705 train_time:840977ms step_avg:169.96ms\nstep:4959/10000 train_loss:4.1508 train_time:841144ms step_avg:169.96ms\nstep:4960/10000 train_loss:3.9401 train_time:841316ms step_avg:169.96ms\nstep:4961/10000 train_loss:3.8593 train_time:841485ms step_avg:169.96ms\nstep:4962/10000 train_loss:4.0021 train_time:841654ms step_avg:169.96ms\nstep:4963/10000 train_loss:4.1134 train_time:841824ms step_avg:169.96ms\nstep:4964/10000 train_loss:3.8376 train_time:841996ms step_avg:169.96ms\nstep:4965/10000 train_loss:3.9343 train_time:842163ms step_avg:169.96ms\nstep:4966/10000 train_loss:3.9482 train_time:842334ms step_avg:169.96ms\nstep:4967/10000 train_loss:4.0806 train_time:842503ms step_avg:169.96ms\nstep:4968/10000 train_loss:4.1747 train_time:842674ms step_avg:169.96ms\nstep:4969/10000 train_loss:4.0213 train_time:842843ms step_avg:169.96ms\nstep:4970/10000 train_loss:3.9991 train_time:843014ms step_avg:169.96ms\nstep:4971/10000 train_loss:4.0055 train_time:843184ms step_avg:169.96ms\nstep:4972/10000 train_loss:3.8022 train_time:843356ms step_avg:169.96ms\nstep:4973/10000 train_loss:3.9446 train_time:843524ms step_avg:169.96ms\nstep:4974/10000 train_loss:4.0421 train_time:843693ms step_avg:169.96ms\nstep:4975/10000 train_loss:4.0102 train_time:843862ms step_avg:169.96ms\nstep:4976/10000 train_loss:4.0471 train_time:844033ms step_avg:169.96ms\nstep:4977/10000 train_loss:4.0289 train_time:844201ms step_avg:169.96ms\nstep:4978/10000 train_loss:3.9074 train_time:844372ms step_avg:169.96ms\nstep:4979/10000 train_loss:3.9259 train_time:844541ms step_avg:169.96ms\nstep:4980/10000 train_loss:4.2675 train_time:844710ms step_avg:169.96ms\nstep:4981/10000 train_loss:4.2076 train_time:844881ms step_avg:169.96ms\nstep:4982/10000 train_loss:4.0005 train_time:845050ms step_avg:169.96ms\nstep:4983/10000 train_loss:4.1101 train_time:845219ms step_avg:169.96ms\nstep:4984/10000 train_loss:3.9193 train_time:845389ms step_avg:169.96ms\nstep:4985/10000 train_loss:3.9688 train_time:845559ms step_avg:169.96ms\nstep:4986/10000 train_loss:4.0208 train_time:845729ms step_avg:169.96ms\nstep:4987/10000 train_loss:4.1215 train_time:845902ms step_avg:169.96ms\nstep:4988/10000 train_loss:3.9823 train_time:846071ms step_avg:169.96ms\nstep:4989/10000 train_loss:3.9498 train_time:846240ms step_avg:169.96ms\nstep:4990/10000 train_loss:4.0193 train_time:846410ms step_avg:169.96ms\nstep:4991/10000 train_loss:4.0128 train_time:846579ms step_avg:169.96ms\nstep:4992/10000 train_loss:4.1063 train_time:846749ms step_avg:169.96ms\nstep:4993/10000 train_loss:4.0197 train_time:846920ms step_avg:169.96ms\nstep:4994/10000 train_loss:3.8590 train_time:847088ms step_avg:169.96ms\nstep:4995/10000 train_loss:4.0366 train_time:847258ms step_avg:169.96ms\nstep:4996/10000 train_loss:3.9366 train_time:847428ms step_avg:169.96ms\nstep:4997/10000 train_loss:4.0065 train_time:847598ms step_avg:169.96ms\nstep:4998/10000 train_loss:4.0146 train_time:847766ms step_avg:169.96ms\nstep:4999/10000 train_loss:3.9939 train_time:847937ms step_avg:169.96ms\nstep:5000/10000 train_loss:3.8946 train_time:848104ms step_avg:169.96ms\nstep:5001/10000 train_loss:4.0122 train_time:848274ms step_avg:169.96ms\nstep:5002/10000 train_loss:4.0708 train_time:848443ms step_avg:169.96ms\nstep:5003/10000 train_loss:3.9965 train_time:848614ms step_avg:169.96ms\nstep:5004/10000 train_loss:3.9278 train_time:848782ms step_avg:169.96ms\nstep:5005/10000 train_loss:3.9900 train_time:848952ms step_avg:169.96ms\nstep:5006/10000 train_loss:4.0222 train_time:849121ms step_avg:169.96ms\nstep:5007/10000 train_loss:4.0367 train_time:849293ms step_avg:169.96ms\nstep:5008/10000 train_loss:3.7900 train_time:849463ms step_avg:169.96ms\nstep:5009/10000 train_loss:3.9561 train_time:849634ms step_avg:169.96ms\nstep:5010/10000 train_loss:3.9084 train_time:849806ms step_avg:169.96ms\nstep:5011/10000 train_loss:3.8733 train_time:849970ms step_avg:169.96ms\nstep:5012/10000 train_loss:4.0597 train_time:850137ms step_avg:169.96ms\nstep:5013/10000 train_loss:3.9092 train_time:850305ms step_avg:169.96ms\nstep:5014/10000 train_loss:4.1381 train_time:850476ms step_avg:169.96ms\nstep:5015/10000 train_loss:3.8996 train_time:850645ms step_avg:169.96ms\nstep:5016/10000 train_loss:4.1488 train_time:850822ms step_avg:169.96ms\nstep:5017/10000 train_loss:3.8908 train_time:850990ms step_avg:169.96ms\nstep:5018/10000 train_loss:4.0262 train_time:851161ms step_avg:169.96ms\nstep:5019/10000 train_loss:3.9995 train_time:851323ms step_avg:169.96ms\nstep:5020/10000 train_loss:3.9143 train_time:851493ms step_avg:169.96ms\nstep:5021/10000 train_loss:3.9693 train_time:851667ms step_avg:169.96ms\nstep:5022/10000 train_loss:3.9103 train_time:851833ms step_avg:169.96ms\nstep:5023/10000 train_loss:4.0623 train_time:852002ms step_avg:169.96ms\nstep:5024/10000 train_loss:4.0877 train_time:852171ms step_avg:169.96ms\nstep:5025/10000 train_loss:4.0132 train_time:852342ms step_avg:169.96ms\nstep:5026/10000 train_loss:4.0653 train_time:852513ms step_avg:169.96ms\nstep:5027/10000 train_loss:3.9811 train_time:852682ms step_avg:169.96ms\nstep:5028/10000 train_loss:3.8357 train_time:852851ms step_avg:169.96ms\nstep:5029/10000 train_loss:4.1016 train_time:853022ms step_avg:169.96ms\nstep:5030/10000 train_loss:4.1246 train_time:853191ms step_avg:169.96ms\nstep:5031/10000 train_loss:4.0428 train_time:853362ms step_avg:169.96ms\nstep:5032/10000 train_loss:4.0310 train_time:853532ms step_avg:169.96ms\nstep:5033/10000 train_loss:3.8709 train_time:853702ms step_avg:169.96ms\nstep:5034/10000 train_loss:3.9462 train_time:853873ms step_avg:169.96ms\nstep:5035/10000 train_loss:4.0428 train_time:854045ms step_avg:169.96ms\nstep:5036/10000 train_loss:4.1513 train_time:854215ms step_avg:169.96ms\nstep:5037/10000 train_loss:3.9882 train_time:854385ms step_avg:169.96ms\nstep:5038/10000 train_loss:3.9393 train_time:854556ms step_avg:169.96ms\nstep:5039/10000 train_loss:3.9098 train_time:854724ms step_avg:169.96ms\nstep:5040/10000 train_loss:3.9412 train_time:854894ms step_avg:169.96ms\nstep:5041/10000 train_loss:4.1280 train_time:855061ms step_avg:169.96ms\nstep:5042/10000 train_loss:3.9745 train_time:855231ms step_avg:169.96ms\nstep:5043/10000 train_loss:4.8098 train_time:855400ms step_avg:169.96ms\nstep:5044/10000 train_loss:4.0219 train_time:855572ms step_avg:169.96ms\nstep:5045/10000 train_loss:4.0254 train_time:855742ms step_avg:169.96ms\nstep:5046/10000 train_loss:3.9847 train_time:855913ms step_avg:169.96ms\nstep:5047/10000 train_loss:4.0043 train_time:856083ms step_avg:169.96ms\nstep:5048/10000 train_loss:4.1435 train_time:856254ms step_avg:169.96ms\nstep:5049/10000 train_loss:3.9721 train_time:856422ms step_avg:169.96ms\nstep:5050/10000 train_loss:3.8229 train_time:856591ms step_avg:169.96ms\nstep:5051/10000 train_loss:4.1026 train_time:856761ms step_avg:169.96ms\nstep:5052/10000 train_loss:3.8504 train_time:856931ms step_avg:169.96ms\nstep:5053/10000 train_loss:4.0112 train_time:857102ms step_avg:169.96ms\nstep:5054/10000 train_loss:3.9020 train_time:857271ms step_avg:169.96ms\nstep:5055/10000 train_loss:3.9966 train_time:857441ms step_avg:169.96ms\nstep:5056/10000 train_loss:4.0613 train_time:857609ms step_avg:169.96ms\nstep:5057/10000 train_loss:4.1042 train_time:857778ms step_avg:169.96ms\nstep:5058/10000 train_loss:4.0307 train_time:857946ms step_avg:169.96ms\nstep:5059/10000 train_loss:4.1778 train_time:858116ms step_avg:169.96ms\nstep:5060/10000 train_loss:3.9745 train_time:858283ms step_avg:169.96ms\nstep:5061/10000 train_loss:4.0343 train_time:858453ms step_avg:169.96ms\nstep:5062/10000 train_loss:3.8220 train_time:858620ms step_avg:169.96ms\nstep:5063/10000 train_loss:4.0318 train_time:858787ms step_avg:169.96ms\nstep:5064/10000 train_loss:3.9924 train_time:858955ms step_avg:169.96ms\nstep:5065/10000 train_loss:4.1602 train_time:859123ms step_avg:169.96ms\nstep:5066/10000 train_loss:4.0664 train_time:859294ms step_avg:169.96ms\nstep:5067/10000 train_loss:3.9642 train_time:859464ms step_avg:169.96ms\nstep:5068/10000 train_loss:3.9080 train_time:859635ms step_avg:169.96ms\nstep:5069/10000 train_loss:3.9293 train_time:859802ms step_avg:169.95ms\nstep:5070/10000 train_loss:4.0491 train_time:859973ms step_avg:169.96ms\nstep:5071/10000 train_loss:4.0968 train_time:860141ms step_avg:169.95ms\nstep:5072/10000 train_loss:3.9201 train_time:860314ms step_avg:169.96ms\nstep:5073/10000 train_loss:3.8640 train_time:860483ms step_avg:169.96ms\nstep:5074/10000 train_loss:4.0458 train_time:860650ms step_avg:169.95ms\nstep:5075/10000 train_loss:3.8900 train_time:860819ms step_avg:169.95ms\nstep:5076/10000 train_loss:4.0227 train_time:860987ms step_avg:169.95ms\nstep:5077/10000 train_loss:4.0606 train_time:861157ms step_avg:169.95ms\nstep:5078/10000 train_loss:3.9829 train_time:861327ms step_avg:169.95ms\nstep:5079/10000 train_loss:3.7967 train_time:861499ms step_avg:169.95ms\nstep:5080/10000 train_loss:3.9079 train_time:861669ms step_avg:169.95ms\nstep:5081/10000 train_loss:4.0001 train_time:861837ms step_avg:169.95ms\nstep:5082/10000 train_loss:4.0308 train_time:862006ms step_avg:169.95ms\nstep:5083/10000 train_loss:3.9207 train_time:862176ms step_avg:169.95ms\nstep:5084/10000 train_loss:3.9969 train_time:862342ms step_avg:169.95ms\nstep:5085/10000 train_loss:3.9085 train_time:862512ms step_avg:169.95ms\nstep:5086/10000 train_loss:3.8932 train_time:862680ms step_avg:169.95ms\nstep:5087/10000 train_loss:4.0431 train_time:862850ms step_avg:169.95ms\nstep:5088/10000 train_loss:3.9931 train_time:863019ms step_avg:169.95ms\nstep:5089/10000 train_loss:3.9269 train_time:863188ms step_avg:169.95ms\nstep:5090/10000 train_loss:3.7366 train_time:863359ms step_avg:169.95ms\nstep:5091/10000 train_loss:3.9731 train_time:863529ms step_avg:169.95ms\nstep:5092/10000 train_loss:4.0093 train_time:863697ms step_avg:169.95ms\nstep:5093/10000 train_loss:4.1708 train_time:863865ms step_avg:169.95ms\nstep:5094/10000 train_loss:3.9961 train_time:864033ms step_avg:169.95ms\nstep:5095/10000 train_loss:3.9541 train_time:864201ms step_avg:169.95ms\nstep:5096/10000 train_loss:3.9223 train_time:864370ms step_avg:169.95ms\nstep:5097/10000 train_loss:3.9502 train_time:864539ms step_avg:169.95ms\nstep:5098/10000 train_loss:3.9674 train_time:864708ms step_avg:169.95ms\nstep:5099/10000 train_loss:4.0199 train_time:864879ms step_avg:169.95ms\nstep:5100/10000 train_loss:4.0493 train_time:865047ms step_avg:169.95ms\nstep:5101/10000 train_loss:3.9889 train_time:865217ms step_avg:169.95ms\nstep:5102/10000 train_loss:4.0395 train_time:865387ms step_avg:169.95ms\nstep:5103/10000 train_loss:4.1186 train_time:865559ms step_avg:169.95ms\nstep:5104/10000 train_loss:3.9529 train_time:865728ms step_avg:169.95ms\nstep:5105/10000 train_loss:4.0223 train_time:865896ms step_avg:169.95ms\nstep:5106/10000 train_loss:3.9491 train_time:866064ms step_avg:169.95ms\nstep:5107/10000 train_loss:3.9612 train_time:866234ms step_avg:169.95ms\nstep:5108/10000 train_loss:4.0332 train_time:866402ms step_avg:169.95ms\nstep:5109/10000 train_loss:4.0198 train_time:866580ms step_avg:169.95ms\nstep:5110/10000 train_loss:4.0583 train_time:866740ms step_avg:169.95ms\nstep:5111/10000 train_loss:3.8518 train_time:866910ms step_avg:169.95ms\nstep:5112/10000 train_loss:3.9986 train_time:867080ms step_avg:169.95ms\nstep:5113/10000 train_loss:3.8835 train_time:867249ms step_avg:169.95ms\nstep:5114/10000 train_loss:4.0864 train_time:867418ms step_avg:169.95ms\nstep:5115/10000 train_loss:4.0386 train_time:867588ms step_avg:169.95ms\nstep:5116/10000 train_loss:3.9333 train_time:867758ms step_avg:169.95ms\nstep:5117/10000 train_loss:4.0796 train_time:867927ms step_avg:169.95ms\nstep:5118/10000 train_loss:4.0420 train_time:868100ms step_avg:169.95ms\nstep:5119/10000 train_loss:3.9562 train_time:868266ms step_avg:169.95ms\nstep:5120/10000 train_loss:3.8622 train_time:868438ms step_avg:169.95ms\nstep:5121/10000 train_loss:3.9173 train_time:868610ms step_avg:169.95ms\nstep:5122/10000 train_loss:4.0319 train_time:868778ms step_avg:169.95ms\nstep:5123/10000 train_loss:4.0360 train_time:868947ms step_avg:169.95ms\nstep:5124/10000 train_loss:4.0206 train_time:869119ms step_avg:169.95ms\nstep:5125/10000 train_loss:3.9657 train_time:869287ms step_avg:169.95ms\nstep:5126/10000 train_loss:3.8478 train_time:869458ms step_avg:169.95ms\nstep:5127/10000 train_loss:3.7846 train_time:869626ms step_avg:169.95ms\nstep:5128/10000 train_loss:3.9222 train_time:869797ms step_avg:169.95ms\nstep:5129/10000 train_loss:4.1095 train_time:869965ms step_avg:169.95ms\nstep:5130/10000 train_loss:4.0020 train_time:870136ms step_avg:169.95ms\nstep:5131/10000 train_loss:4.0194 train_time:870305ms step_avg:169.95ms\nstep:5132/10000 train_loss:4.1213 train_time:870475ms step_avg:169.95ms\nstep:5133/10000 train_loss:3.9928 train_time:870643ms step_avg:169.95ms\nstep:5134/10000 train_loss:4.0559 train_time:870813ms step_avg:169.95ms\nstep:5135/10000 train_loss:3.9570 train_time:870985ms step_avg:169.95ms\nstep:5136/10000 train_loss:3.9518 train_time:871154ms step_avg:169.95ms\nstep:5137/10000 train_loss:4.0769 train_time:871322ms step_avg:169.95ms\nstep:5138/10000 train_loss:4.0770 train_time:871496ms step_avg:169.95ms\nstep:5139/10000 train_loss:3.9246 train_time:871666ms step_avg:169.95ms\nstep:5140/10000 train_loss:4.0615 train_time:871835ms step_avg:169.95ms\nstep:5141/10000 train_loss:3.9700 train_time:872003ms step_avg:169.95ms\nstep:5142/10000 train_loss:3.9700 train_time:872175ms step_avg:169.95ms\nstep:5143/10000 train_loss:3.8690 train_time:872344ms step_avg:169.95ms\nstep:5144/10000 train_loss:3.9244 train_time:872513ms step_avg:169.95ms\nstep:5145/10000 train_loss:3.9705 train_time:872684ms step_avg:169.95ms\nstep:5146/10000 train_loss:4.0189 train_time:872854ms step_avg:169.95ms\nstep:5147/10000 train_loss:3.8618 train_time:873023ms step_avg:169.95ms\nstep:5148/10000 train_loss:4.0264 train_time:873193ms step_avg:169.95ms\nstep:5149/10000 train_loss:3.8761 train_time:873362ms step_avg:169.95ms\nstep:5150/10000 train_loss:3.9743 train_time:873531ms step_avg:169.95ms\nstep:5151/10000 train_loss:4.0664 train_time:873702ms step_avg:169.95ms\nstep:5152/10000 train_loss:4.0421 train_time:873875ms step_avg:169.95ms\nstep:5153/10000 train_loss:4.0534 train_time:874044ms step_avg:169.95ms\nstep:5154/10000 train_loss:3.9582 train_time:874216ms step_avg:169.95ms\nstep:5155/10000 train_loss:3.9459 train_time:874384ms step_avg:169.95ms\nstep:5156/10000 train_loss:3.9702 train_time:874553ms step_avg:169.95ms\nstep:5157/10000 train_loss:4.1518 train_time:874722ms step_avg:169.95ms\nstep:5158/10000 train_loss:4.0614 train_time:874890ms step_avg:169.95ms\nstep:5159/10000 train_loss:4.2285 train_time:875059ms step_avg:169.95ms\nstep:5160/10000 train_loss:3.9898 train_time:875231ms step_avg:169.95ms\nstep:5161/10000 train_loss:3.8919 train_time:875403ms step_avg:169.95ms\nstep:5162/10000 train_loss:3.9378 train_time:875571ms step_avg:169.95ms\nstep:5163/10000 train_loss:3.9793 train_time:875741ms step_avg:169.95ms\nstep:5164/10000 train_loss:4.1306 train_time:875910ms step_avg:169.95ms\nstep:5165/10000 train_loss:3.8698 train_time:876078ms step_avg:169.95ms\nstep:5166/10000 train_loss:3.9961 train_time:876247ms step_avg:169.95ms\nstep:5167/10000 train_loss:3.8009 train_time:876418ms step_avg:169.95ms\nstep:5168/10000 train_loss:3.9783 train_time:876586ms step_avg:169.95ms\nstep:5169/10000 train_loss:4.0609 train_time:876757ms step_avg:169.95ms\nstep:5170/10000 train_loss:4.0711 train_time:876926ms step_avg:169.95ms\nstep:5171/10000 train_loss:4.0183 train_time:877095ms step_avg:169.95ms\nstep:5172/10000 train_loss:3.7458 train_time:877264ms step_avg:169.95ms\nstep:5173/10000 train_loss:3.8468 train_time:877436ms step_avg:169.95ms\nstep:5174/10000 train_loss:4.0042 train_time:877604ms step_avg:169.95ms\nstep:5175/10000 train_loss:3.9313 train_time:877773ms step_avg:169.95ms\nstep:5176/10000 train_loss:3.9849 train_time:877949ms step_avg:169.95ms\nstep:5177/10000 train_loss:4.0760 train_time:878116ms step_avg:169.95ms\nstep:5178/10000 train_loss:4.0940 train_time:878287ms step_avg:169.95ms\nstep:5179/10000 train_loss:3.9583 train_time:878451ms step_avg:169.95ms\nstep:5180/10000 train_loss:4.0006 train_time:878618ms step_avg:169.95ms\nstep:5181/10000 train_loss:4.1698 train_time:878787ms step_avg:169.95ms\nstep:5182/10000 train_loss:4.1183 train_time:878958ms step_avg:169.95ms\nstep:5183/10000 train_loss:3.9623 train_time:879127ms step_avg:169.95ms\nstep:5184/10000 train_loss:4.0331 train_time:879301ms step_avg:169.95ms\nstep:5185/10000 train_loss:4.1400 train_time:879471ms step_avg:169.95ms\nstep:5186/10000 train_loss:4.1016 train_time:879642ms step_avg:169.95ms\nstep:5187/10000 train_loss:3.9658 train_time:879812ms step_avg:169.95ms\nstep:5188/10000 train_loss:4.0468 train_time:879978ms step_avg:169.95ms\nstep:5189/10000 train_loss:3.9235 train_time:880148ms step_avg:169.95ms\nstep:5190/10000 train_loss:3.9021 train_time:880318ms step_avg:169.95ms\nstep:5191/10000 train_loss:4.0918 train_time:880488ms step_avg:169.95ms\nstep:5192/10000 train_loss:4.0153 train_time:880659ms step_avg:169.95ms\nstep:5193/10000 train_loss:4.0631 train_time:880836ms step_avg:169.95ms\nstep:5194/10000 train_loss:3.9713 train_time:881000ms step_avg:169.95ms\nstep:5195/10000 train_loss:4.1042 train_time:881168ms step_avg:169.95ms\nstep:5196/10000 train_loss:4.0848 train_time:881338ms step_avg:169.95ms\nstep:5197/10000 train_loss:3.9726 train_time:881508ms step_avg:169.95ms\nstep:5198/10000 train_loss:3.8634 train_time:881676ms step_avg:169.95ms\nstep:5199/10000 train_loss:4.1650 train_time:881845ms step_avg:169.95ms\nstep:5200/10000 train_loss:3.9842 train_time:882013ms step_avg:169.94ms\nstep:5201/10000 train_loss:3.8810 train_time:882182ms step_avg:169.94ms\nstep:5202/10000 train_loss:3.9728 train_time:882351ms step_avg:169.94ms\nstep:5203/10000 train_loss:3.9278 train_time:882522ms step_avg:169.94ms\nstep:5204/10000 train_loss:4.1059 train_time:882693ms step_avg:169.94ms\nstep:5205/10000 train_loss:4.0766 train_time:882862ms step_avg:169.94ms\nstep:5206/10000 train_loss:3.9916 train_time:883030ms step_avg:169.94ms\nstep:5207/10000 train_loss:3.9329 train_time:883198ms step_avg:169.94ms\nstep:5208/10000 train_loss:4.0338 train_time:883365ms step_avg:169.94ms\nstep:5209/10000 train_loss:3.8717 train_time:883536ms step_avg:169.94ms\nstep:5210/10000 train_loss:4.0285 train_time:883703ms step_avg:169.94ms\nstep:5211/10000 train_loss:3.9041 train_time:883875ms step_avg:169.94ms\nstep:5212/10000 train_loss:4.0170 train_time:884043ms step_avg:169.94ms\nstep:5213/10000 train_loss:4.0966 train_time:884213ms step_avg:169.94ms\nstep:5214/10000 train_loss:4.0364 train_time:884383ms step_avg:169.94ms\nstep:5215/10000 train_loss:4.1434 train_time:884553ms step_avg:169.94ms\nstep:5216/10000 train_loss:3.9449 train_time:884723ms step_avg:169.94ms\nstep:5217/10000 train_loss:3.9190 train_time:884895ms step_avg:169.94ms\nstep:5218/10000 train_loss:4.1536 train_time:885064ms step_avg:169.94ms\nstep:5219/10000 train_loss:3.9749 train_time:885235ms step_avg:169.94ms\nstep:5220/10000 train_loss:4.0822 train_time:885405ms step_avg:169.94ms\nstep:5221/10000 train_loss:3.9658 train_time:885576ms step_avg:169.94ms\nstep:5222/10000 train_loss:3.9663 train_time:885744ms step_avg:169.94ms\nstep:5223/10000 train_loss:3.9513 train_time:885913ms step_avg:169.94ms\nstep:5224/10000 train_loss:4.1266 train_time:886084ms step_avg:169.94ms\nstep:5225/10000 train_loss:4.0130 train_time:886257ms step_avg:169.94ms\nstep:5226/10000 train_loss:4.1719 train_time:886426ms step_avg:169.94ms\nstep:5227/10000 train_loss:3.9653 train_time:886597ms step_avg:169.94ms\nstep:5228/10000 train_loss:3.9144 train_time:886765ms step_avg:169.94ms\nstep:5229/10000 train_loss:3.9504 train_time:886933ms step_avg:169.94ms\nstep:5230/10000 train_loss:3.9096 train_time:887104ms step_avg:169.94ms\nstep:5231/10000 train_loss:3.9398 train_time:887277ms step_avg:169.94ms\nstep:5232/10000 train_loss:3.9229 train_time:887444ms step_avg:169.94ms\nstep:5233/10000 train_loss:3.9957 train_time:887616ms step_avg:169.94ms\nstep:5234/10000 train_loss:4.0471 train_time:887783ms step_avg:169.94ms\nstep:5235/10000 train_loss:3.9766 train_time:887953ms step_avg:169.94ms\nstep:5236/10000 train_loss:3.9227 train_time:888122ms step_avg:169.94ms\nstep:5237/10000 train_loss:3.9983 train_time:888292ms step_avg:169.94ms\nstep:5238/10000 train_loss:4.0665 train_time:888462ms step_avg:169.94ms\nstep:5239/10000 train_loss:3.8877 train_time:888632ms step_avg:169.94ms\nstep:5240/10000 train_loss:3.9387 train_time:888801ms step_avg:169.94ms\nstep:5241/10000 train_loss:3.9330 train_time:888971ms step_avg:169.94ms\nstep:5242/10000 train_loss:3.9666 train_time:889138ms step_avg:169.94ms\nstep:5243/10000 train_loss:4.0045 train_time:889308ms step_avg:169.94ms\nstep:5244/10000 train_loss:3.9003 train_time:889480ms step_avg:169.94ms\nstep:5245/10000 train_loss:3.9764 train_time:889650ms step_avg:169.94ms\nstep:5246/10000 train_loss:3.8700 train_time:889821ms step_avg:169.94ms\nstep:5247/10000 train_loss:4.0555 train_time:889989ms step_avg:169.94ms\nstep:5248/10000 train_loss:4.0768 train_time:890159ms step_avg:169.94ms\nstep:5249/10000 train_loss:4.2615 train_time:890328ms step_avg:169.94ms\nstep:5250/10000 train_loss:3.9370 train_time:890499ms step_avg:169.94ms\nstep:5251/10000 train_loss:3.8235 train_time:890666ms step_avg:169.94ms\nstep:5252/10000 train_loss:3.9890 train_time:890837ms step_avg:169.94ms\nstep:5253/10000 train_loss:3.9783 train_time:891005ms step_avg:169.94ms\nstep:5254/10000 train_loss:3.8888 train_time:891174ms step_avg:169.94ms\nstep:5255/10000 train_loss:4.0632 train_time:891344ms step_avg:169.94ms\nstep:5256/10000 train_loss:4.0478 train_time:891519ms step_avg:169.94ms\nstep:5257/10000 train_loss:3.9042 train_time:891681ms step_avg:169.94ms\nstep:5258/10000 train_loss:4.0383 train_time:891849ms step_avg:169.94ms\nstep:5259/10000 train_loss:4.0067 train_time:892024ms step_avg:169.94ms\nstep:5260/10000 train_loss:4.0294 train_time:892192ms step_avg:169.94ms\nstep:5261/10000 train_loss:3.9420 train_time:892356ms step_avg:169.94ms\nstep:5262/10000 train_loss:3.9653 train_time:892526ms step_avg:169.94ms\nstep:5263/10000 train_loss:3.9508 train_time:892698ms step_avg:169.94ms\nstep:5264/10000 train_loss:4.0215 train_time:892866ms step_avg:169.94ms\nstep:5265/10000 train_loss:3.8464 train_time:893036ms step_avg:169.94ms\nstep:5266/10000 train_loss:3.7029 train_time:893204ms step_avg:169.94ms\nstep:5267/10000 train_loss:3.8832 train_time:893374ms step_avg:169.94ms\nstep:5268/10000 train_loss:3.8763 train_time:893543ms step_avg:169.94ms\nstep:5269/10000 train_loss:3.8785 train_time:893713ms step_avg:169.94ms\nstep:5270/10000 train_loss:3.9273 train_time:893883ms step_avg:169.94ms\nstep:5271/10000 train_loss:4.0216 train_time:894052ms step_avg:169.94ms\nstep:5272/10000 train_loss:4.1122 train_time:894222ms step_avg:169.94ms\nstep:5273/10000 train_loss:3.8813 train_time:894394ms step_avg:169.94ms\nstep:5274/10000 train_loss:3.9862 train_time:894563ms step_avg:169.94ms\nstep:5275/10000 train_loss:4.0758 train_time:894736ms step_avg:169.94ms\nstep:5276/10000 train_loss:4.0477 train_time:894905ms step_avg:169.94ms\nstep:5277/10000 train_loss:4.0719 train_time:895075ms step_avg:169.94ms\nstep:5278/10000 train_loss:4.0111 train_time:895245ms step_avg:169.94ms\nstep:5279/10000 train_loss:3.8434 train_time:895417ms step_avg:169.94ms\nstep:5280/10000 train_loss:3.9693 train_time:895584ms step_avg:169.94ms\nstep:5281/10000 train_loss:4.0484 train_time:895754ms step_avg:169.94ms\nstep:5282/10000 train_loss:3.8581 train_time:895923ms step_avg:169.94ms\nstep:5283/10000 train_loss:3.9546 train_time:896093ms step_avg:169.94ms\nstep:5284/10000 train_loss:4.0574 train_time:896262ms step_avg:169.94ms\nstep:5285/10000 train_loss:3.9068 train_time:896433ms step_avg:169.94ms\nstep:5286/10000 train_loss:4.0292 train_time:896604ms step_avg:169.94ms\nstep:5287/10000 train_loss:3.8762 train_time:896774ms step_avg:169.94ms\nstep:5288/10000 train_loss:4.0128 train_time:896943ms step_avg:169.94ms\nstep:5289/10000 train_loss:4.0417 train_time:897114ms step_avg:169.94ms\nstep:5290/10000 train_loss:4.0717 train_time:897285ms step_avg:169.94ms\nstep:5291/10000 train_loss:3.9690 train_time:897457ms step_avg:169.94ms\nstep:5292/10000 train_loss:3.9362 train_time:897625ms step_avg:169.94ms\nstep:5293/10000 train_loss:4.1054 train_time:897796ms step_avg:169.94ms\nstep:5294/10000 train_loss:3.9616 train_time:897964ms step_avg:169.94ms\nstep:5295/10000 train_loss:3.7201 train_time:898133ms step_avg:169.94ms\nstep:5296/10000 train_loss:4.1588 train_time:898303ms step_avg:169.94ms\nstep:5297/10000 train_loss:4.0887 train_time:898474ms step_avg:169.94ms\nstep:5298/10000 train_loss:3.9401 train_time:898643ms step_avg:169.94ms\nstep:5299/10000 train_loss:4.0223 train_time:898814ms step_avg:169.94ms\nstep:5300/10000 train_loss:4.0557 train_time:898983ms step_avg:169.94ms\nstep:5301/10000 train_loss:3.7726 train_time:899152ms step_avg:169.94ms\nstep:5302/10000 train_loss:3.9818 train_time:899322ms step_avg:169.94ms\nstep:5303/10000 train_loss:3.8697 train_time:899494ms step_avg:169.94ms\nstep:5304/10000 train_loss:4.0730 train_time:899664ms step_avg:169.94ms\nstep:5305/10000 train_loss:3.9481 train_time:899834ms step_avg:169.94ms\nstep:5306/10000 train_loss:4.0581 train_time:900004ms step_avg:169.94ms\nstep:5307/10000 train_loss:3.9615 train_time:900174ms step_avg:169.94ms\nstep:5308/10000 train_loss:4.1131 train_time:900343ms step_avg:169.94ms\nstep:5309/10000 train_loss:4.0457 train_time:900513ms step_avg:169.94ms\nstep:5310/10000 train_loss:4.1179 train_time:900683ms step_avg:169.94ms\nstep:5311/10000 train_loss:3.9024 train_time:900853ms step_avg:169.94ms\nstep:5312/10000 train_loss:3.8855 train_time:901020ms step_avg:169.94ms\nstep:5313/10000 train_loss:3.9103 train_time:901189ms step_avg:169.94ms\nstep:5314/10000 train_loss:3.9023 train_time:901360ms step_avg:169.94ms\nstep:5315/10000 train_loss:4.0200 train_time:901531ms step_avg:169.94ms\nstep:5316/10000 train_loss:3.9350 train_time:901698ms step_avg:169.94ms\nstep:5317/10000 train_loss:3.9316 train_time:901867ms step_avg:169.94ms\nstep:5318/10000 train_loss:3.7998 train_time:902036ms step_avg:169.94ms\nstep:5319/10000 train_loss:4.0145 train_time:902203ms step_avg:169.94ms\nstep:5320/10000 train_loss:4.0160 train_time:902373ms step_avg:169.94ms\nstep:5321/10000 train_loss:3.9561 train_time:902545ms step_avg:169.94ms\nstep:5322/10000 train_loss:4.0680 train_time:902716ms step_avg:169.94ms\nstep:5323/10000 train_loss:4.0298 train_time:902884ms step_avg:169.94ms\nstep:5324/10000 train_loss:2.8755 train_time:903053ms step_avg:169.94ms\nstep:5325/10000 train_loss:3.9620 train_time:903225ms step_avg:169.94ms\nstep:5326/10000 train_loss:3.9858 train_time:903394ms step_avg:169.94ms\nstep:5327/10000 train_loss:3.8998 train_time:903564ms step_avg:169.94ms\nstep:5328/10000 train_loss:3.9873 train_time:903735ms step_avg:169.94ms\nstep:5329/10000 train_loss:4.2670 train_time:903904ms step_avg:169.94ms\nstep:5330/10000 train_loss:4.0698 train_time:904072ms step_avg:169.94ms\nstep:5331/10000 train_loss:3.9576 train_time:904243ms step_avg:169.94ms\nstep:5332/10000 train_loss:3.9669 train_time:904414ms step_avg:169.94ms\nstep:5333/10000 train_loss:3.7663 train_time:904584ms step_avg:169.94ms\nstep:5334/10000 train_loss:3.9143 train_time:904966ms step_avg:169.98ms\nstep:5335/10000 train_loss:3.9525 train_time:905130ms step_avg:169.98ms\nstep:5336/10000 train_loss:4.0049 train_time:905299ms step_avg:169.98ms\nstep:5337/10000 train_loss:4.0103 train_time:905468ms step_avg:169.98ms\nstep:5338/10000 train_loss:3.9335 train_time:905638ms step_avg:169.98ms\nstep:5339/10000 train_loss:3.9423 train_time:905806ms step_avg:169.98ms\nstep:5340/10000 train_loss:4.3545 train_time:905983ms step_avg:169.98ms\nstep:5341/10000 train_loss:4.0893 train_time:906152ms step_avg:169.98ms\nstep:5342/10000 train_loss:3.9107 train_time:906321ms step_avg:169.98ms\nstep:5343/10000 train_loss:3.9525 train_time:906488ms step_avg:169.98ms\nstep:5344/10000 train_loss:4.0521 train_time:906656ms step_avg:169.98ms\nstep:5345/10000 train_loss:4.0554 train_time:906822ms step_avg:169.98ms\nstep:5346/10000 train_loss:4.0188 train_time:906992ms step_avg:169.98ms\nstep:5347/10000 train_loss:3.9315 train_time:907162ms step_avg:169.98ms\nstep:5348/10000 train_loss:3.9651 train_time:907332ms step_avg:169.98ms\nstep:5349/10000 train_loss:3.9362 train_time:907500ms step_avg:169.98ms\nstep:5350/10000 train_loss:3.9442 train_time:907669ms step_avg:169.98ms\nstep:5351/10000 train_loss:3.9857 train_time:907836ms step_avg:169.97ms\nstep:5352/10000 train_loss:3.9424 train_time:908005ms step_avg:169.97ms\nstep:5353/10000 train_loss:4.4251 train_time:908176ms step_avg:169.97ms\nstep:5354/10000 train_loss:4.0220 train_time:908347ms step_avg:169.98ms\nstep:5355/10000 train_loss:4.0043 train_time:908517ms step_avg:169.98ms\nstep:5356/10000 train_loss:3.9253 train_time:908686ms step_avg:169.97ms\nstep:5357/10000 train_loss:4.0120 train_time:908858ms step_avg:169.98ms\nstep:5358/10000 train_loss:3.9305 train_time:909026ms step_avg:169.97ms\nstep:5359/10000 train_loss:3.9556 train_time:909196ms step_avg:169.98ms\nstep:5360/10000 train_loss:4.1812 train_time:909365ms step_avg:169.97ms\nstep:5361/10000 train_loss:3.9248 train_time:909536ms step_avg:169.97ms\nstep:5362/10000 train_loss:3.8495 train_time:909706ms step_avg:169.97ms\nstep:5363/10000 train_loss:4.1506 train_time:909875ms step_avg:169.97ms\nstep:5364/10000 train_loss:4.0830 train_time:910045ms step_avg:169.97ms\nstep:5365/10000 train_loss:3.9580 train_time:910217ms step_avg:169.98ms\nstep:5366/10000 train_loss:3.8968 train_time:910384ms step_avg:169.97ms\nstep:5367/10000 train_loss:3.8282 train_time:910554ms step_avg:169.97ms\nstep:5368/10000 train_loss:3.9517 train_time:910722ms step_avg:169.97ms\nstep:5369/10000 train_loss:3.9173 train_time:910891ms step_avg:169.97ms\nstep:5370/10000 train_loss:4.0163 train_time:911061ms step_avg:169.97ms\nstep:5371/10000 train_loss:3.5655 train_time:911229ms step_avg:169.97ms\nstep:5372/10000 train_loss:3.9216 train_time:911399ms step_avg:169.97ms\nstep:5373/10000 train_loss:3.8319 train_time:911569ms step_avg:169.97ms\nstep:5374/10000 train_loss:3.9702 train_time:911737ms step_avg:169.97ms\nstep:5375/10000 train_loss:3.9844 train_time:911905ms step_avg:169.97ms\nstep:5376/10000 train_loss:4.2309 train_time:912075ms step_avg:169.97ms\nstep:5377/10000 train_loss:3.9026 train_time:912244ms step_avg:169.97ms\nstep:5378/10000 train_loss:4.1014 train_time:912414ms step_avg:169.97ms\nstep:5379/10000 train_loss:3.9551 train_time:912583ms step_avg:169.97ms\nstep:5380/10000 train_loss:4.0010 train_time:912752ms step_avg:169.97ms\nstep:5381/10000 train_loss:4.4050 train_time:912919ms step_avg:169.97ms\nstep:5382/10000 train_loss:3.9777 train_time:913089ms step_avg:169.97ms\nstep:5383/10000 train_loss:3.9323 train_time:913260ms step_avg:169.97ms\nstep:5384/10000 train_loss:3.9863 train_time:913429ms step_avg:169.97ms\nstep:5385/10000 train_loss:4.0242 train_time:913598ms step_avg:169.97ms\nstep:5386/10000 train_loss:3.9453 train_time:913768ms step_avg:169.97ms\nstep:5387/10000 train_loss:3.9788 train_time:913937ms step_avg:169.97ms\nstep:5388/10000 train_loss:4.0472 train_time:914106ms step_avg:169.97ms\nstep:5389/10000 train_loss:3.9560 train_time:914277ms step_avg:169.97ms\nstep:5390/10000 train_loss:3.9319 train_time:914446ms step_avg:169.97ms\nstep:5391/10000 train_loss:3.9148 train_time:914622ms step_avg:169.97ms\nstep:5392/10000 train_loss:4.0021 train_time:914784ms step_avg:169.97ms\nstep:5393/10000 train_loss:3.8355 train_time:914956ms step_avg:169.97ms\nstep:5394/10000 train_loss:3.8955 train_time:915123ms step_avg:169.97ms\nstep:5395/10000 train_loss:4.0701 train_time:915294ms step_avg:169.97ms\nstep:5396/10000 train_loss:3.9101 train_time:915463ms step_avg:169.97ms\nstep:5397/10000 train_loss:3.9556 train_time:915634ms step_avg:169.97ms\nstep:5398/10000 train_loss:3.8669 train_time:915801ms step_avg:169.97ms\nstep:5399/10000 train_loss:4.0413 train_time:915972ms step_avg:169.97ms\nstep:5400/10000 train_loss:3.8053 train_time:916142ms step_avg:169.97ms\nstep:5401/10000 train_loss:3.8911 train_time:916314ms step_avg:169.97ms\nstep:5402/10000 train_loss:4.1511 train_time:916483ms step_avg:169.97ms\nstep:5403/10000 train_loss:3.9919 train_time:916655ms step_avg:169.97ms\nstep:5404/10000 train_loss:3.9330 train_time:916820ms step_avg:169.97ms\nstep:5405/10000 train_loss:3.9963 train_time:916988ms step_avg:169.97ms\nstep:5406/10000 train_loss:3.8604 train_time:917160ms step_avg:169.97ms\nstep:5407/10000 train_loss:3.9767 train_time:917329ms step_avg:169.97ms\nstep:5408/10000 train_loss:3.8890 train_time:917498ms step_avg:169.97ms\nstep:5409/10000 train_loss:3.8640 train_time:917667ms step_avg:169.97ms\nstep:5410/10000 train_loss:3.9655 train_time:917836ms step_avg:169.97ms\nstep:5411/10000 train_loss:3.9041 train_time:918005ms step_avg:169.97ms\nstep:5412/10000 train_loss:4.0188 train_time:918174ms step_avg:169.97ms\nstep:5413/10000 train_loss:3.8787 train_time:918344ms step_avg:169.97ms\nstep:5414/10000 train_loss:4.0614 train_time:918513ms step_avg:169.97ms\nstep:5415/10000 train_loss:4.0218 train_time:918682ms step_avg:169.97ms\nstep:5416/10000 train_loss:3.8599 train_time:918852ms step_avg:169.97ms\nstep:5417/10000 train_loss:4.0846 train_time:919021ms step_avg:169.97ms\nstep:5418/10000 train_loss:4.0073 train_time:919191ms step_avg:169.97ms\nstep:5419/10000 train_loss:4.0419 train_time:919362ms step_avg:169.97ms\nstep:5420/10000 train_loss:3.9138 train_time:919530ms step_avg:169.97ms\nstep:5421/10000 train_loss:3.8281 train_time:919701ms step_avg:169.97ms\nstep:5422/10000 train_loss:3.9221 train_time:919869ms step_avg:169.97ms\nstep:5423/10000 train_loss:3.9568 train_time:920039ms step_avg:169.97ms\nstep:5424/10000 train_loss:4.0551 train_time:920209ms step_avg:169.97ms\nstep:5425/10000 train_loss:4.0779 train_time:920382ms step_avg:169.97ms\nstep:5426/10000 train_loss:3.9235 train_time:920551ms step_avg:169.97ms\nstep:5427/10000 train_loss:3.8911 train_time:920717ms step_avg:169.97ms\nstep:5428/10000 train_loss:3.8723 train_time:920886ms step_avg:169.97ms\nstep:5429/10000 train_loss:3.8779 train_time:921058ms step_avg:169.97ms\nstep:5430/10000 train_loss:4.0406 train_time:921227ms step_avg:169.97ms\nstep:5431/10000 train_loss:3.9224 train_time:921395ms step_avg:169.97ms\nstep:5432/10000 train_loss:3.7985 train_time:921565ms step_avg:169.97ms\nstep:5433/10000 train_loss:3.9091 train_time:921737ms step_avg:169.97ms\nstep:5434/10000 train_loss:3.9143 train_time:921907ms step_avg:169.97ms\nstep:5435/10000 train_loss:3.9313 train_time:922078ms step_avg:169.97ms\nstep:5436/10000 train_loss:4.1468 train_time:922246ms step_avg:169.97ms\nstep:5437/10000 train_loss:4.0293 train_time:922415ms step_avg:169.97ms\nstep:5438/10000 train_loss:3.9883 train_time:922584ms step_avg:169.97ms\nstep:5439/10000 train_loss:3.9091 train_time:922753ms step_avg:169.97ms\nstep:5440/10000 train_loss:4.0886 train_time:922923ms step_avg:169.97ms\nstep:5441/10000 train_loss:4.1415 train_time:923095ms step_avg:169.97ms\nstep:5442/10000 train_loss:4.1296 train_time:923262ms step_avg:169.97ms\nstep:5443/10000 train_loss:3.9431 train_time:923435ms step_avg:169.97ms\nstep:5444/10000 train_loss:3.8020 train_time:923603ms step_avg:169.97ms\nstep:5445/10000 train_loss:3.9978 train_time:923773ms step_avg:169.97ms\nstep:5446/10000 train_loss:4.1268 train_time:923944ms step_avg:169.97ms\nstep:5447/10000 train_loss:4.0533 train_time:924121ms step_avg:169.97ms\nstep:5448/10000 train_loss:4.0969 train_time:924286ms step_avg:169.97ms\nstep:5449/10000 train_loss:3.9333 train_time:924457ms step_avg:169.97ms\nstep:5450/10000 train_loss:3.8882 train_time:924625ms step_avg:169.97ms\nstep:5451/10000 train_loss:3.9579 train_time:924795ms step_avg:169.97ms\nstep:5452/10000 train_loss:3.9527 train_time:924965ms step_avg:169.97ms\nstep:5453/10000 train_loss:4.0446 train_time:925135ms step_avg:169.97ms\nstep:5454/10000 train_loss:3.9081 train_time:925304ms step_avg:169.97ms\nstep:5455/10000 train_loss:3.7671 train_time:925475ms step_avg:169.97ms\nstep:5456/10000 train_loss:3.8851 train_time:925643ms step_avg:169.97ms\nstep:5457/10000 train_loss:3.9523 train_time:925814ms step_avg:169.97ms\nstep:5458/10000 train_loss:3.9243 train_time:925983ms step_avg:169.97ms\nstep:5459/10000 train_loss:3.9703 train_time:926157ms step_avg:169.97ms\nstep:5460/10000 train_loss:3.9954 train_time:926326ms step_avg:169.97ms\nstep:5461/10000 train_loss:3.9980 train_time:926498ms step_avg:169.97ms\nstep:5462/10000 train_loss:3.8649 train_time:926665ms step_avg:169.97ms\nstep:5463/10000 train_loss:4.0181 train_time:926835ms step_avg:169.97ms\nstep:5464/10000 train_loss:3.8859 train_time:927003ms step_avg:169.97ms\nstep:5465/10000 train_loss:4.0621 train_time:927175ms step_avg:169.97ms\nstep:5466/10000 train_loss:3.8033 train_time:927343ms step_avg:169.97ms\nstep:5467/10000 train_loss:3.8286 train_time:927514ms step_avg:169.97ms\nstep:5468/10000 train_loss:3.9436 train_time:927682ms step_avg:169.97ms\nstep:5469/10000 train_loss:4.0274 train_time:927850ms step_avg:169.97ms\nstep:5470/10000 train_loss:3.8286 train_time:928019ms step_avg:169.97ms\nstep:5471/10000 train_loss:3.9657 train_time:928190ms step_avg:169.97ms\nstep:5472/10000 train_loss:3.9450 train_time:928360ms step_avg:169.97ms\nstep:5473/10000 train_loss:3.9613 train_time:928532ms step_avg:169.97ms\nstep:5474/10000 train_loss:3.9145 train_time:928700ms step_avg:169.97ms\nstep:5475/10000 train_loss:3.8767 train_time:928868ms step_avg:169.97ms\nstep:5476/10000 train_loss:4.0397 train_time:929037ms step_avg:169.97ms\nstep:5477/10000 train_loss:3.9287 train_time:929205ms step_avg:169.97ms\nstep:5478/10000 train_loss:3.9732 train_time:929377ms step_avg:169.97ms\nstep:5479/10000 train_loss:3.9451 train_time:929546ms step_avg:169.97ms\nstep:5480/10000 train_loss:4.0226 train_time:929715ms step_avg:169.97ms\nstep:5481/10000 train_loss:4.1155 train_time:929883ms step_avg:169.97ms\nstep:5482/10000 train_loss:3.9568 train_time:930055ms step_avg:169.97ms\nstep:5483/10000 train_loss:3.9792 train_time:930223ms step_avg:169.97ms\nstep:5484/10000 train_loss:3.9942 train_time:930396ms step_avg:169.97ms\nstep:5485/10000 train_loss:3.9297 train_time:930566ms step_avg:169.97ms\nstep:5486/10000 train_loss:4.1371 train_time:930735ms step_avg:169.97ms\nstep:5487/10000 train_loss:3.9588 train_time:930905ms step_avg:169.97ms\nstep:5488/10000 train_loss:3.8380 train_time:931074ms step_avg:169.97ms\nstep:5489/10000 train_loss:3.8550 train_time:931245ms step_avg:169.97ms\nstep:5490/10000 train_loss:3.9510 train_time:931414ms step_avg:169.97ms\nstep:5491/10000 train_loss:3.8869 train_time:931581ms step_avg:169.97ms\nstep:5492/10000 train_loss:4.2028 train_time:931750ms step_avg:169.97ms\nstep:5493/10000 train_loss:3.9013 train_time:931920ms step_avg:169.97ms\nstep:5494/10000 train_loss:3.8138 train_time:932088ms step_avg:169.97ms\nstep:5495/10000 train_loss:4.4417 train_time:932259ms step_avg:169.97ms\nstep:5496/10000 train_loss:3.9772 train_time:932429ms step_avg:169.97ms\nstep:5497/10000 train_loss:3.9312 train_time:932601ms step_avg:169.97ms\nstep:5498/10000 train_loss:3.8345 train_time:932771ms step_avg:169.97ms\nstep:5499/10000 train_loss:3.9137 train_time:932943ms step_avg:169.97ms\nstep:5500/10000 train_loss:3.9921 train_time:933112ms step_avg:169.97ms\nstep:5501/10000 train_loss:3.8922 train_time:933285ms step_avg:169.97ms\nstep:5502/10000 train_loss:4.0607 train_time:933455ms step_avg:169.97ms\nstep:5503/10000 train_loss:4.0443 train_time:933626ms step_avg:169.97ms\nstep:5504/10000 train_loss:3.9713 train_time:933797ms step_avg:169.97ms\nstep:5505/10000 train_loss:3.8532 train_time:933966ms step_avg:169.97ms\nstep:5506/10000 train_loss:3.9635 train_time:934135ms step_avg:169.97ms\nstep:5507/10000 train_loss:3.9641 train_time:934303ms step_avg:169.97ms\nstep:5508/10000 train_loss:3.8122 train_time:934474ms step_avg:169.97ms\nstep:5509/10000 train_loss:3.9229 train_time:934642ms step_avg:169.97ms\nstep:5510/10000 train_loss:4.0395 train_time:934811ms step_avg:169.97ms\nstep:5511/10000 train_loss:3.9118 train_time:934982ms step_avg:169.97ms\nstep:5512/10000 train_loss:4.2055 train_time:935152ms step_avg:169.97ms\nstep:5513/10000 train_loss:4.0329 train_time:935320ms step_avg:169.97ms\nstep:5514/10000 train_loss:3.8643 train_time:935491ms step_avg:169.97ms\nstep:5515/10000 train_loss:3.9605 train_time:935663ms step_avg:169.97ms\nstep:5516/10000 train_loss:3.9633 train_time:935833ms step_avg:169.97ms\nstep:5517/10000 train_loss:3.9366 train_time:936004ms step_avg:169.97ms\nstep:5518/10000 train_loss:3.9483 train_time:936173ms step_avg:169.97ms\nstep:5519/10000 train_loss:4.1613 train_time:936340ms step_avg:169.97ms\nstep:5520/10000 train_loss:4.0979 train_time:936511ms step_avg:169.97ms\nstep:5521/10000 train_loss:3.9641 train_time:936681ms step_avg:169.97ms\nstep:5522/10000 train_loss:3.8943 train_time:936850ms step_avg:169.97ms\nstep:5523/10000 train_loss:3.8962 train_time:937019ms step_avg:169.97ms\nstep:5524/10000 train_loss:4.1526 train_time:937187ms step_avg:169.97ms\nstep:5525/10000 train_loss:3.8278 train_time:937357ms step_avg:169.97ms\nstep:5526/10000 train_loss:3.9206 train_time:937527ms step_avg:169.96ms\nstep:5527/10000 train_loss:3.9214 train_time:937696ms step_avg:169.96ms\nstep:5528/10000 train_loss:3.8163 train_time:937864ms step_avg:169.96ms\nstep:5529/10000 train_loss:3.8753 train_time:938035ms step_avg:169.96ms\nstep:5530/10000 train_loss:4.1266 train_time:938203ms step_avg:169.96ms\nstep:5531/10000 train_loss:3.7757 train_time:938375ms step_avg:169.96ms\nstep:5532/10000 train_loss:3.9567 train_time:938542ms step_avg:169.96ms\nstep:5533/10000 train_loss:3.8578 train_time:938714ms step_avg:169.96ms\nstep:5534/10000 train_loss:3.8623 train_time:938883ms step_avg:169.96ms\nstep:5535/10000 train_loss:3.7449 train_time:939053ms step_avg:169.96ms\nstep:5536/10000 train_loss:4.4261 train_time:939222ms step_avg:169.96ms\nstep:5537/10000 train_loss:3.9559 train_time:939390ms step_avg:169.96ms\nstep:5538/10000 train_loss:3.8817 train_time:939559ms step_avg:169.96ms\nstep:5539/10000 train_loss:3.9748 train_time:939728ms step_avg:169.96ms\nstep:5540/10000 train_loss:3.8892 train_time:939898ms step_avg:169.96ms\nstep:5541/10000 train_loss:4.0214 train_time:940066ms step_avg:169.96ms\nstep:5542/10000 train_loss:3.9668 train_time:940236ms step_avg:169.96ms\nstep:5543/10000 train_loss:3.8618 train_time:940404ms step_avg:169.96ms\nstep:5544/10000 train_loss:4.0233 train_time:940574ms step_avg:169.96ms\nstep:5545/10000 train_loss:3.7227 train_time:940744ms step_avg:169.96ms\nstep:5546/10000 train_loss:3.9445 train_time:940914ms step_avg:169.96ms\nstep:5547/10000 train_loss:4.2160 train_time:941082ms step_avg:169.96ms\nstep:5548/10000 train_loss:4.0943 train_time:941251ms step_avg:169.96ms\nstep:5549/10000 train_loss:3.8682 train_time:941424ms step_avg:169.96ms\nstep:5550/10000 train_loss:3.8985 train_time:941594ms step_avg:169.96ms\nstep:5551/10000 train_loss:3.9198 train_time:941764ms step_avg:169.96ms\nstep:5552/10000 train_loss:3.9883 train_time:941935ms step_avg:169.96ms\nstep:5553/10000 train_loss:3.9555 train_time:942104ms step_avg:169.96ms\nstep:5554/10000 train_loss:3.8480 train_time:942274ms step_avg:169.96ms\nstep:5555/10000 train_loss:3.8023 train_time:942448ms step_avg:169.96ms\nstep:5556/10000 train_loss:4.0246 train_time:942613ms step_avg:169.96ms\nstep:5557/10000 train_loss:4.0288 train_time:942782ms step_avg:169.96ms\nstep:5558/10000 train_loss:3.9893 train_time:942951ms step_avg:169.96ms\nstep:5559/10000 train_loss:4.0192 train_time:943120ms step_avg:169.96ms\nstep:5560/10000 train_loss:3.8227 train_time:943290ms step_avg:169.96ms\nstep:5561/10000 train_loss:3.9958 train_time:943467ms step_avg:169.96ms\nstep:5562/10000 train_loss:4.0150 train_time:943632ms step_avg:169.96ms\nstep:5563/10000 train_loss:3.8994 train_time:943800ms step_avg:169.96ms\nstep:5564/10000 train_loss:3.9660 train_time:943969ms step_avg:169.96ms\nstep:5565/10000 train_loss:3.8541 train_time:944140ms step_avg:169.96ms\nstep:5566/10000 train_loss:4.0425 train_time:944310ms step_avg:169.96ms\nstep:5567/10000 train_loss:3.9730 train_time:944482ms step_avg:169.96ms\nstep:5568/10000 train_loss:3.9202 train_time:944653ms step_avg:169.96ms\nstep:5569/10000 train_loss:3.8639 train_time:944823ms step_avg:169.96ms\nstep:5570/10000 train_loss:4.1120 train_time:944990ms step_avg:169.96ms\nstep:5571/10000 train_loss:4.0311 train_time:945159ms step_avg:169.96ms\nstep:5572/10000 train_loss:4.2151 train_time:945327ms step_avg:169.96ms\nstep:5573/10000 train_loss:3.9194 train_time:945495ms step_avg:169.96ms\nstep:5574/10000 train_loss:3.9968 train_time:945664ms step_avg:169.96ms\nstep:5575/10000 train_loss:3.8618 train_time:945833ms step_avg:169.96ms\nstep:5576/10000 train_loss:3.9161 train_time:946001ms step_avg:169.96ms\nstep:5577/10000 train_loss:4.0752 train_time:946171ms step_avg:169.96ms\nstep:5578/10000 train_loss:4.0174 train_time:946342ms step_avg:169.96ms\nstep:5579/10000 train_loss:3.9285 train_time:946513ms step_avg:169.96ms\nstep:5580/10000 train_loss:3.9116 train_time:946681ms step_avg:169.96ms\nstep:5581/10000 train_loss:3.8914 train_time:946852ms step_avg:169.96ms\nstep:5582/10000 train_loss:3.9846 train_time:947022ms step_avg:169.96ms\nstep:5583/10000 train_loss:3.9981 train_time:947193ms step_avg:169.96ms\nstep:5584/10000 train_loss:3.9060 train_time:947363ms step_avg:169.96ms\nstep:5585/10000 train_loss:3.9950 train_time:947535ms step_avg:169.96ms\nstep:5586/10000 train_loss:3.7843 train_time:947704ms step_avg:169.96ms\nstep:5587/10000 train_loss:3.9317 train_time:947874ms step_avg:169.96ms\nstep:5588/10000 train_loss:4.1651 train_time:948042ms step_avg:169.96ms\nstep:5589/10000 train_loss:4.0993 train_time:948211ms step_avg:169.96ms\nstep:5590/10000 train_loss:4.0569 train_time:948379ms step_avg:169.96ms\nstep:5591/10000 train_loss:3.9022 train_time:948548ms step_avg:169.96ms\nstep:5592/10000 train_loss:4.0618 train_time:948717ms step_avg:169.96ms\nstep:5593/10000 train_loss:3.9863 train_time:948884ms step_avg:169.96ms\nstep:5594/10000 train_loss:3.9336 train_time:949053ms step_avg:169.96ms\nstep:5595/10000 train_loss:4.0724 train_time:949224ms step_avg:169.96ms\nstep:5596/10000 train_loss:4.0500 train_time:949392ms step_avg:169.96ms\nstep:5597/10000 train_loss:3.9346 train_time:949559ms step_avg:169.96ms\nstep:5598/10000 train_loss:3.8469 train_time:949734ms step_avg:169.96ms\nstep:5599/10000 train_loss:3.9402 train_time:949900ms step_avg:169.96ms\nstep:5600/10000 train_loss:3.9003 train_time:950069ms step_avg:169.96ms\nstep:5601/10000 train_loss:4.0193 train_time:950238ms step_avg:169.96ms\nstep:5602/10000 train_loss:3.7825 train_time:950408ms step_avg:169.96ms\nstep:5603/10000 train_loss:4.0254 train_time:950580ms step_avg:169.96ms\nstep:5604/10000 train_loss:3.9132 train_time:950749ms step_avg:169.96ms\nstep:5605/10000 train_loss:4.0391 train_time:950917ms step_avg:169.96ms\nstep:5606/10000 train_loss:3.7735 train_time:951086ms step_avg:169.96ms\nstep:5607/10000 train_loss:3.9687 train_time:951256ms step_avg:169.96ms\nstep:5608/10000 train_loss:3.9771 train_time:951424ms step_avg:169.96ms\nstep:5609/10000 train_loss:4.0850 train_time:951593ms step_avg:169.96ms\nstep:5610/10000 train_loss:4.1264 train_time:951763ms step_avg:169.96ms\nstep:5611/10000 train_loss:3.7273 train_time:951934ms step_avg:169.96ms\nstep:5612/10000 train_loss:3.9169 train_time:952102ms step_avg:169.96ms\nstep:5613/10000 train_loss:3.9714 train_time:952272ms step_avg:169.96ms\nstep:5614/10000 train_loss:4.1663 train_time:952443ms step_avg:169.96ms\nstep:5615/10000 train_loss:3.9363 train_time:952612ms step_avg:169.96ms\nstep:5616/10000 train_loss:3.9338 train_time:952780ms step_avg:169.96ms\nstep:5617/10000 train_loss:3.7601 train_time:952948ms step_avg:169.96ms\nstep:5618/10000 train_loss:4.0152 train_time:953117ms step_avg:169.96ms\nstep:5619/10000 train_loss:4.0086 train_time:953286ms step_avg:169.96ms\nstep:5620/10000 train_loss:3.9261 train_time:953457ms step_avg:169.96ms\nstep:5621/10000 train_loss:3.9156 train_time:953626ms step_avg:169.96ms\nstep:5622/10000 train_loss:3.9153 train_time:953798ms step_avg:169.96ms\nstep:5623/10000 train_loss:3.9456 train_time:953966ms step_avg:169.96ms\nstep:5624/10000 train_loss:4.1408 train_time:954135ms step_avg:169.96ms\nstep:5625/10000 train_loss:3.9827 train_time:954304ms step_avg:169.96ms\nstep:5626/10000 train_loss:4.0169 train_time:954475ms step_avg:169.96ms\nstep:5627/10000 train_loss:3.9712 train_time:954644ms step_avg:169.96ms\nstep:5628/10000 train_loss:3.8847 train_time:954815ms step_avg:169.96ms\nstep:5629/10000 train_loss:3.9044 train_time:954984ms step_avg:169.96ms\nstep:5630/10000 train_loss:3.8855 train_time:955154ms step_avg:169.96ms\nstep:5631/10000 train_loss:3.9018 train_time:955322ms step_avg:169.96ms\nstep:5632/10000 train_loss:3.9541 train_time:955491ms step_avg:169.96ms\nstep:5633/10000 train_loss:3.9550 train_time:955662ms step_avg:169.96ms\nstep:5634/10000 train_loss:3.7506 train_time:955834ms step_avg:169.96ms\nstep:5635/10000 train_loss:3.9772 train_time:956002ms step_avg:169.96ms\nstep:5636/10000 train_loss:3.9585 train_time:956172ms step_avg:169.96ms\nstep:5637/10000 train_loss:4.1378 train_time:956343ms step_avg:169.96ms\nstep:5638/10000 train_loss:3.8921 train_time:956511ms step_avg:169.96ms\nstep:5639/10000 train_loss:3.8802 train_time:956681ms step_avg:169.96ms\nstep:5640/10000 train_loss:3.8308 train_time:956849ms step_avg:169.96ms\nstep:5641/10000 train_loss:4.0417 train_time:957019ms step_avg:169.96ms\nstep:5642/10000 train_loss:3.9746 train_time:957188ms step_avg:169.96ms\nstep:5643/10000 train_loss:3.9302 train_time:957359ms step_avg:169.96ms\nstep:5644/10000 train_loss:3.9074 train_time:957527ms step_avg:169.96ms\nstep:5645/10000 train_loss:4.0263 train_time:957695ms step_avg:169.95ms\nstep:5646/10000 train_loss:4.2890 train_time:957862ms step_avg:169.95ms\nstep:5647/10000 train_loss:3.9324 train_time:958031ms step_avg:169.95ms\nstep:5648/10000 train_loss:3.8970 train_time:958199ms step_avg:169.95ms\nstep:5649/10000 train_loss:3.9723 train_time:958367ms step_avg:169.95ms\nstep:5650/10000 train_loss:4.0869 train_time:958535ms step_avg:169.95ms\nstep:5651/10000 train_loss:3.9580 train_time:958705ms step_avg:169.95ms\nstep:5652/10000 train_loss:3.8859 train_time:958876ms step_avg:169.95ms\nstep:5653/10000 train_loss:3.9696 train_time:959046ms step_avg:169.95ms\nstep:5654/10000 train_loss:4.0942 train_time:959216ms step_avg:169.95ms\nstep:5655/10000 train_loss:3.9291 train_time:959385ms step_avg:169.95ms\nstep:5656/10000 train_loss:4.0067 train_time:959553ms step_avg:169.95ms\nstep:5657/10000 train_loss:4.0246 train_time:959721ms step_avg:169.95ms\nstep:5658/10000 train_loss:4.0969 train_time:959890ms step_avg:169.95ms\nstep:5659/10000 train_loss:3.9159 train_time:960063ms step_avg:169.95ms\nstep:5660/10000 train_loss:3.8892 train_time:960234ms step_avg:169.95ms\nstep:5661/10000 train_loss:3.9653 train_time:960402ms step_avg:169.95ms\nstep:5662/10000 train_loss:3.8946 train_time:960570ms step_avg:169.95ms\nstep:5663/10000 train_loss:4.0193 train_time:960739ms step_avg:169.95ms\nstep:5664/10000 train_loss:3.9809 train_time:960908ms step_avg:169.95ms\nstep:5665/10000 train_loss:3.9419 train_time:961078ms step_avg:169.95ms\nstep:5666/10000 train_loss:3.9731 train_time:961246ms step_avg:169.95ms\nstep:5667/10000 train_loss:4.0072 train_time:961417ms step_avg:169.95ms\nstep:5668/10000 train_loss:3.9882 train_time:961588ms step_avg:169.95ms\nstep:5669/10000 train_loss:3.7948 train_time:961759ms step_avg:169.95ms\nstep:5670/10000 train_loss:3.7604 train_time:961927ms step_avg:169.95ms\nstep:5671/10000 train_loss:3.9275 train_time:962098ms step_avg:169.95ms\nstep:5672/10000 train_loss:4.0481 train_time:962268ms step_avg:169.95ms\nstep:5673/10000 train_loss:3.9827 train_time:962442ms step_avg:169.95ms\nstep:5674/10000 train_loss:3.4009 train_time:962608ms step_avg:169.95ms\nstep:5675/10000 train_loss:3.9686 train_time:962781ms step_avg:169.95ms\nstep:5676/10000 train_loss:3.8489 train_time:962957ms step_avg:169.95ms\nstep:5677/10000 train_loss:4.0186 train_time:963124ms step_avg:169.95ms\nstep:5678/10000 train_loss:3.9800 train_time:963295ms step_avg:169.95ms\nstep:5679/10000 train_loss:3.9570 train_time:963464ms step_avg:169.95ms\nstep:5680/10000 train_loss:3.9088 train_time:963635ms step_avg:169.95ms\nstep:5681/10000 train_loss:4.1423 train_time:963803ms step_avg:169.95ms\nstep:5682/10000 train_loss:4.0208 train_time:963973ms step_avg:169.95ms\nstep:5683/10000 train_loss:4.0324 train_time:964143ms step_avg:169.95ms\nstep:5684/10000 train_loss:3.9981 train_time:964315ms step_avg:169.95ms\nstep:5685/10000 train_loss:4.0269 train_time:964485ms step_avg:169.95ms\nstep:5686/10000 train_loss:3.9331 train_time:964656ms step_avg:169.95ms\nstep:5687/10000 train_loss:3.8592 train_time:964825ms step_avg:169.95ms\nstep:5688/10000 train_loss:4.0821 train_time:964994ms step_avg:169.95ms\nstep:5689/10000 train_loss:4.1710 train_time:965165ms step_avg:169.95ms\nstep:5690/10000 train_loss:3.9963 train_time:965336ms step_avg:169.95ms\nstep:5691/10000 train_loss:3.9987 train_time:965505ms step_avg:169.95ms\nstep:5692/10000 train_loss:4.0053 train_time:965678ms step_avg:169.95ms\nstep:5693/10000 train_loss:3.8754 train_time:965847ms step_avg:169.95ms\nstep:5694/10000 train_loss:3.9892 train_time:966016ms step_avg:169.95ms\nstep:5695/10000 train_loss:3.9541 train_time:966184ms step_avg:169.95ms\nstep:5696/10000 train_loss:3.9748 train_time:966355ms step_avg:169.95ms\nstep:5697/10000 train_loss:3.9012 train_time:966524ms step_avg:169.95ms\nstep:5698/10000 train_loss:3.8342 train_time:966696ms step_avg:169.95ms\nstep:5699/10000 train_loss:3.9397 train_time:966865ms step_avg:169.95ms\nstep:5700/10000 train_loss:3.9991 train_time:967036ms step_avg:169.95ms\nstep:5701/10000 train_loss:3.9868 train_time:967204ms step_avg:169.95ms\nstep:5702/10000 train_loss:3.8499 train_time:967373ms step_avg:169.95ms\nstep:5703/10000 train_loss:4.1922 train_time:967541ms step_avg:169.95ms\nstep:5704/10000 train_loss:3.9928 train_time:967710ms step_avg:169.95ms\nstep:5705/10000 train_loss:3.9701 train_time:967879ms step_avg:169.95ms\nstep:5706/10000 train_loss:4.0751 train_time:968047ms step_avg:169.95ms\nstep:5707/10000 train_loss:3.8547 train_time:968214ms step_avg:169.95ms\nstep:5708/10000 train_loss:3.9565 train_time:968383ms step_avg:169.95ms\nstep:5709/10000 train_loss:3.8657 train_time:968554ms step_avg:169.95ms\nstep:5710/10000 train_loss:3.7939 train_time:968721ms step_avg:169.95ms\nstep:5711/10000 train_loss:4.1243 train_time:968891ms step_avg:169.95ms\nstep:5712/10000 train_loss:3.8218 train_time:969061ms step_avg:169.95ms\nstep:5713/10000 train_loss:4.0182 train_time:969231ms step_avg:169.95ms\nstep:5714/10000 train_loss:3.9857 train_time:969398ms step_avg:169.95ms\nstep:5715/10000 train_loss:3.9189 train_time:969566ms step_avg:169.95ms\nstep:5716/10000 train_loss:3.9618 train_time:969735ms step_avg:169.95ms\nstep:5717/10000 train_loss:4.0830 train_time:969904ms step_avg:169.95ms\nstep:5718/10000 train_loss:4.0374 train_time:970075ms step_avg:169.95ms\nstep:5719/10000 train_loss:3.8516 train_time:970244ms step_avg:169.95ms\nstep:5720/10000 train_loss:3.8591 train_time:970415ms step_avg:169.95ms\nstep:5721/10000 train_loss:3.9202 train_time:970581ms step_avg:169.95ms\nstep:5722/10000 train_loss:3.9406 train_time:970750ms step_avg:169.95ms\nstep:5723/10000 train_loss:3.9725 train_time:970921ms step_avg:169.95ms\nstep:5724/10000 train_loss:3.9835 train_time:971090ms step_avg:169.95ms\nstep:5725/10000 train_loss:4.0617 train_time:971260ms step_avg:169.95ms\nstep:5726/10000 train_loss:3.9391 train_time:971430ms step_avg:169.95ms\nstep:5727/10000 train_loss:3.8600 train_time:971600ms step_avg:169.95ms\nstep:5728/10000 train_loss:3.9596 train_time:971769ms step_avg:169.95ms\nstep:5729/10000 train_loss:3.9629 train_time:971938ms step_avg:169.95ms\nstep:5730/10000 train_loss:3.8607 train_time:972106ms step_avg:169.95ms\nstep:5731/10000 train_loss:3.9346 train_time:972277ms step_avg:169.95ms\nstep:5732/10000 train_loss:3.8904 train_time:972446ms step_avg:169.95ms\nstep:5733/10000 train_loss:3.8793 train_time:972617ms step_avg:169.95ms\nstep:5734/10000 train_loss:3.9871 train_time:972788ms step_avg:169.95ms\nstep:5735/10000 train_loss:3.8678 train_time:972958ms step_avg:169.95ms\nstep:5736/10000 train_loss:3.9230 train_time:973127ms step_avg:169.95ms\nstep:5737/10000 train_loss:3.9040 train_time:973297ms step_avg:169.95ms\nstep:5738/10000 train_loss:3.8983 train_time:973464ms step_avg:169.95ms\nstep:5739/10000 train_loss:3.9562 train_time:973634ms step_avg:169.95ms\nstep:5740/10000 train_loss:3.9952 train_time:973803ms step_avg:169.95ms\nstep:5741/10000 train_loss:3.9108 train_time:973973ms step_avg:169.95ms\nstep:5742/10000 train_loss:4.3831 train_time:974144ms step_avg:169.95ms\nstep:5743/10000 train_loss:3.8915 train_time:974314ms step_avg:169.95ms\nstep:5744/10000 train_loss:3.8290 train_time:974482ms step_avg:169.95ms\nstep:5745/10000 train_loss:3.9996 train_time:974651ms step_avg:169.95ms\nstep:5746/10000 train_loss:3.7754 train_time:974819ms step_avg:169.95ms\nstep:5747/10000 train_loss:3.9067 train_time:974990ms step_avg:169.95ms\nstep:5748/10000 train_loss:3.9516 train_time:975162ms step_avg:169.95ms\nstep:5749/10000 train_loss:3.9162 train_time:975333ms step_avg:169.95ms\nstep:5750/10000 train_loss:3.8853 train_time:975504ms step_avg:169.95ms\nstep:5751/10000 train_loss:3.8088 train_time:975675ms step_avg:169.95ms\nstep:5752/10000 train_loss:3.8205 train_time:975843ms step_avg:169.95ms\nstep:5753/10000 train_loss:3.9103 train_time:976012ms step_avg:169.95ms\nstep:5754/10000 train_loss:3.8920 train_time:976182ms step_avg:169.95ms\nstep:5755/10000 train_loss:3.7419 train_time:976352ms step_avg:169.95ms\nstep:5756/10000 train_loss:3.8229 train_time:976522ms step_avg:169.95ms\nstep:5757/10000 train_loss:3.7625 train_time:976694ms step_avg:169.95ms\nstep:5758/10000 train_loss:3.9131 train_time:976862ms step_avg:169.95ms\nstep:5759/10000 train_loss:3.8492 train_time:977030ms step_avg:169.95ms\nstep:5760/10000 train_loss:4.0356 train_time:977200ms step_avg:169.95ms\nstep:5761/10000 train_loss:3.9766 train_time:977368ms step_avg:169.95ms\nstep:5762/10000 train_loss:3.8574 train_time:977537ms step_avg:169.95ms\nstep:5763/10000 train_loss:3.9622 train_time:977706ms step_avg:169.95ms\nstep:5764/10000 train_loss:3.9762 train_time:977876ms step_avg:169.95ms\nstep:5765/10000 train_loss:3.9542 train_time:978043ms step_avg:169.95ms\nstep:5766/10000 train_loss:4.0106 train_time:978212ms step_avg:169.95ms\nstep:5767/10000 train_loss:3.9344 train_time:978383ms step_avg:169.95ms\nstep:5768/10000 train_loss:3.9376 train_time:978552ms step_avg:169.95ms\nstep:5769/10000 train_loss:4.0163 train_time:978718ms step_avg:169.95ms\nstep:5770/10000 train_loss:3.8625 train_time:978886ms step_avg:169.95ms\nstep:5771/10000 train_loss:3.8877 train_time:979055ms step_avg:169.95ms\nstep:5772/10000 train_loss:3.7918 train_time:979223ms step_avg:169.95ms\nstep:5773/10000 train_loss:3.8007 train_time:979394ms step_avg:169.95ms\nstep:5774/10000 train_loss:3.9976 train_time:979564ms step_avg:169.95ms\nstep:5775/10000 train_loss:4.0214 train_time:979734ms step_avg:169.95ms\nstep:5776/10000 train_loss:3.9858 train_time:979902ms step_avg:169.94ms\nstep:5777/10000 train_loss:3.8476 train_time:980074ms step_avg:169.95ms\nstep:5778/10000 train_loss:3.9410 train_time:980241ms step_avg:169.94ms\nstep:5779/10000 train_loss:3.9090 train_time:980413ms step_avg:169.95ms\nstep:5780/10000 train_loss:4.0461 train_time:980582ms step_avg:169.94ms\nstep:5781/10000 train_loss:4.0630 train_time:980752ms step_avg:169.94ms\nstep:5782/10000 train_loss:3.9093 train_time:980921ms step_avg:169.94ms\nstep:5783/10000 train_loss:3.8008 train_time:981091ms step_avg:169.94ms\nstep:5784/10000 train_loss:4.0670 train_time:981260ms step_avg:169.94ms\nstep:5785/10000 train_loss:4.0030 train_time:981429ms step_avg:169.94ms\nstep:5786/10000 train_loss:4.1177 train_time:981601ms step_avg:169.94ms\nstep:5787/10000 train_loss:3.9886 train_time:981771ms step_avg:169.94ms\nstep:5788/10000 train_loss:4.0824 train_time:981941ms step_avg:169.94ms\nstep:5789/10000 train_loss:3.8750 train_time:982110ms step_avg:169.94ms\nstep:5790/10000 train_loss:3.8361 train_time:982280ms step_avg:169.94ms\nstep:5791/10000 train_loss:3.9924 train_time:982451ms step_avg:169.94ms\nstep:5792/10000 train_loss:3.9557 train_time:982622ms step_avg:169.95ms\nstep:5793/10000 train_loss:3.9769 train_time:982789ms step_avg:169.94ms\nstep:5794/10000 train_loss:3.7357 train_time:982960ms step_avg:169.94ms\nstep:5795/10000 train_loss:3.9809 train_time:983130ms step_avg:169.94ms\nstep:5796/10000 train_loss:3.8500 train_time:983300ms step_avg:169.94ms\nstep:5797/10000 train_loss:3.9560 train_time:983470ms step_avg:169.94ms\nstep:5798/10000 train_loss:4.0910 train_time:983642ms step_avg:169.95ms\nstep:5799/10000 train_loss:3.9738 train_time:983811ms step_avg:169.94ms\nstep:5800/10000 train_loss:3.8988 train_time:983982ms step_avg:169.95ms\nstep:5801/10000 train_loss:3.9718 train_time:984149ms step_avg:169.94ms\nstep:5802/10000 train_loss:3.9369 train_time:984319ms step_avg:169.94ms\nstep:5803/10000 train_loss:3.7411 train_time:984488ms step_avg:169.94ms\nstep:5804/10000 train_loss:3.9366 train_time:984657ms step_avg:169.94ms\nstep:5805/10000 train_loss:3.9891 train_time:984825ms step_avg:169.94ms\nstep:5806/10000 train_loss:3.9340 train_time:984995ms step_avg:169.94ms\nstep:5807/10000 train_loss:4.0132 train_time:985163ms step_avg:169.94ms\nstep:5808/10000 train_loss:4.0466 train_time:985331ms step_avg:169.94ms\nstep:5809/10000 train_loss:3.9182 train_time:985500ms step_avg:169.94ms\nstep:5810/10000 train_loss:3.8322 train_time:985669ms step_avg:169.94ms\nstep:5811/10000 train_loss:3.7420 train_time:985841ms step_avg:169.94ms\nstep:5812/10000 train_loss:4.0558 train_time:986013ms step_avg:169.94ms\nstep:5813/10000 train_loss:3.9546 train_time:986185ms step_avg:169.94ms\nstep:5814/10000 train_loss:3.7806 train_time:986357ms step_avg:169.94ms\nstep:5815/10000 train_loss:4.0777 train_time:986526ms step_avg:169.94ms\nstep:5816/10000 train_loss:3.8670 train_time:986694ms step_avg:169.94ms\nstep:5817/10000 train_loss:4.0155 train_time:986862ms step_avg:169.94ms\nstep:5818/10000 train_loss:3.9015 train_time:987032ms step_avg:169.94ms\nstep:5819/10000 train_loss:3.7484 train_time:987202ms step_avg:169.94ms\nstep:5820/10000 train_loss:3.9593 train_time:987372ms step_avg:169.94ms\nstep:5821/10000 train_loss:3.9144 train_time:987543ms step_avg:169.94ms\nstep:5822/10000 train_loss:3.9089 train_time:987713ms step_avg:169.94ms\nstep:5823/10000 train_loss:4.0071 train_time:987884ms step_avg:169.94ms\nstep:5824/10000 train_loss:3.8618 train_time:988053ms step_avg:169.94ms\nstep:5825/10000 train_loss:3.9343 train_time:988225ms step_avg:169.94ms\nstep:5826/10000 train_loss:3.7882 train_time:988395ms step_avg:169.94ms\nstep:5827/10000 train_loss:3.9170 train_time:988562ms step_avg:169.94ms\nstep:5828/10000 train_loss:3.9002 train_time:988732ms step_avg:169.94ms\nstep:5829/10000 train_loss:4.0488 train_time:988904ms step_avg:169.94ms\nstep:5830/10000 train_loss:4.0136 train_time:989073ms step_avg:169.94ms\nstep:5831/10000 train_loss:3.9737 train_time:989239ms step_avg:169.94ms\nstep:5832/10000 train_loss:3.9575 train_time:989409ms step_avg:169.94ms\nstep:5833/10000 train_loss:4.0294 train_time:989578ms step_avg:169.94ms\nstep:5834/10000 train_loss:4.0246 train_time:989747ms step_avg:169.94ms\nstep:5835/10000 train_loss:3.8307 train_time:989917ms step_avg:169.94ms\nstep:5836/10000 train_loss:3.8593 train_time:990086ms step_avg:169.94ms\nstep:5837/10000 train_loss:3.8714 train_time:990258ms step_avg:169.94ms\nstep:5838/10000 train_loss:3.8720 train_time:990428ms step_avg:169.94ms\nstep:5839/10000 train_loss:4.0015 train_time:990598ms step_avg:169.94ms\nstep:5840/10000 train_loss:3.7929 train_time:990764ms step_avg:169.94ms\nstep:5841/10000 train_loss:3.8685 train_time:990931ms step_avg:169.94ms\nstep:5842/10000 train_loss:3.9538 train_time:991097ms step_avg:169.94ms\nstep:5843/10000 train_loss:4.0367 train_time:991265ms step_avg:169.94ms\nstep:5844/10000 train_loss:4.0008 train_time:991434ms step_avg:169.94ms\nstep:5845/10000 train_loss:4.3483 train_time:991606ms step_avg:169.94ms\nstep:5846/10000 train_loss:3.8781 train_time:991775ms step_avg:169.94ms\nstep:5847/10000 train_loss:3.9049 train_time:991947ms step_avg:169.94ms\nstep:5848/10000 train_loss:3.9066 train_time:992114ms step_avg:169.94ms\nstep:5849/10000 train_loss:3.8669 train_time:992283ms step_avg:169.94ms\nstep:5850/10000 train_loss:3.9494 train_time:992457ms step_avg:169.94ms\nstep:5851/10000 train_loss:3.9446 train_time:992626ms step_avg:169.94ms\nstep:5852/10000 train_loss:3.9393 train_time:992796ms step_avg:169.94ms\nstep:5853/10000 train_loss:3.8220 train_time:992967ms step_avg:169.94ms\nstep:5854/10000 train_loss:3.8934 train_time:993135ms step_avg:169.94ms\nstep:5855/10000 train_loss:3.8610 train_time:993302ms step_avg:169.94ms\nstep:5856/10000 train_loss:4.0865 train_time:993471ms step_avg:169.94ms\nstep:5857/10000 train_loss:3.8684 train_time:993638ms step_avg:169.94ms\nstep:5858/10000 train_loss:3.8904 train_time:993810ms step_avg:169.94ms\nstep:5859/10000 train_loss:3.9583 train_time:993980ms step_avg:169.94ms\nstep:5860/10000 train_loss:3.9029 train_time:994148ms step_avg:169.94ms\nstep:5861/10000 train_loss:3.8240 train_time:994314ms step_avg:169.94ms\nstep:5862/10000 train_loss:3.8206 train_time:994482ms step_avg:169.94ms\nstep:5863/10000 train_loss:3.8293 train_time:994652ms step_avg:169.94ms\nstep:5864/10000 train_loss:4.0524 train_time:994820ms step_avg:169.94ms\nstep:5865/10000 train_loss:3.9144 train_time:994989ms step_avg:169.94ms\nstep:5866/10000 train_loss:4.0254 train_time:995158ms step_avg:169.94ms\nstep:5867/10000 train_loss:4.0522 train_time:995327ms step_avg:169.94ms\nstep:5868/10000 train_loss:3.9534 train_time:995497ms step_avg:169.94ms\nstep:5869/10000 train_loss:3.9675 train_time:995666ms step_avg:169.94ms\nstep:5870/10000 train_loss:3.9828 train_time:995836ms step_avg:169.94ms\nstep:5871/10000 train_loss:3.9337 train_time:996004ms step_avg:169.94ms\nstep:5872/10000 train_loss:3.9748 train_time:996178ms step_avg:169.94ms\nstep:5873/10000 train_loss:4.0396 train_time:996345ms step_avg:169.94ms\nstep:5874/10000 train_loss:3.8748 train_time:996511ms step_avg:169.94ms\nstep:5875/10000 train_loss:3.9125 train_time:996681ms step_avg:169.94ms\nstep:5876/10000 train_loss:3.8944 train_time:996851ms step_avg:169.94ms\nstep:5877/10000 train_loss:3.9556 train_time:997019ms step_avg:169.94ms\nstep:5878/10000 train_loss:3.9004 train_time:997187ms step_avg:169.94ms\nstep:5879/10000 train_loss:3.9839 train_time:997358ms step_avg:169.94ms\nstep:5880/10000 train_loss:4.0741 train_time:997529ms step_avg:169.94ms\nstep:5881/10000 train_loss:3.8825 train_time:997701ms step_avg:169.94ms\nstep:5882/10000 train_loss:3.9527 train_time:997873ms step_avg:169.94ms\nstep:5883/10000 train_loss:3.8744 train_time:998042ms step_avg:169.94ms\nstep:5884/10000 train_loss:3.9118 train_time:998208ms step_avg:169.94ms\nstep:5885/10000 train_loss:3.9715 train_time:998378ms step_avg:169.94ms\nstep:5886/10000 train_loss:3.8051 train_time:998547ms step_avg:169.94ms\nstep:5887/10000 train_loss:3.9087 train_time:998717ms step_avg:169.94ms\nstep:5888/10000 train_loss:3.8630 train_time:998885ms step_avg:169.94ms\nstep:5889/10000 train_loss:3.8685 train_time:999056ms step_avg:169.94ms\nstep:5890/10000 train_loss:3.8106 train_time:999224ms step_avg:169.94ms\nstep:5891/10000 train_loss:3.9514 train_time:999393ms step_avg:169.94ms\nstep:5892/10000 train_loss:3.9321 train_time:999562ms step_avg:169.94ms\nstep:5893/10000 train_loss:3.8903 train_time:999732ms step_avg:169.94ms\nstep:5894/10000 train_loss:3.9860 train_time:999899ms step_avg:169.94ms\nstep:5895/10000 train_loss:3.7021 train_time:1000068ms step_avg:169.94ms\nstep:5896/10000 train_loss:3.8571 train_time:1000235ms step_avg:169.93ms\nstep:5897/10000 train_loss:3.9520 train_time:1000403ms step_avg:169.93ms\nstep:5898/10000 train_loss:3.8953 train_time:1000575ms step_avg:169.93ms\nstep:5899/10000 train_loss:3.8975 train_time:1000744ms step_avg:169.93ms\nstep:5900/10000 train_loss:3.9937 train_time:1000916ms step_avg:169.93ms\nstep:5901/10000 train_loss:3.8984 train_time:1001084ms step_avg:169.93ms\nstep:5902/10000 train_loss:4.0209 train_time:1001254ms step_avg:169.93ms\nstep:5903/10000 train_loss:3.9816 train_time:1001423ms step_avg:169.93ms\nstep:5904/10000 train_loss:4.0327 train_time:1001593ms step_avg:169.93ms\nstep:5905/10000 train_loss:3.9501 train_time:1001763ms step_avg:169.93ms\nstep:5906/10000 train_loss:3.8502 train_time:1001934ms step_avg:169.93ms\nstep:5907/10000 train_loss:3.8785 train_time:1002103ms step_avg:169.93ms\nstep:5908/10000 train_loss:3.9344 train_time:1002274ms step_avg:169.93ms\nstep:5909/10000 train_loss:3.8840 train_time:1002443ms step_avg:169.93ms\nstep:5910/10000 train_loss:3.9471 train_time:1002614ms step_avg:169.93ms\nstep:5911/10000 train_loss:3.8973 train_time:1002784ms step_avg:169.93ms\nstep:5912/10000 train_loss:3.7416 train_time:1002956ms step_avg:169.93ms\nstep:5913/10000 train_loss:3.7451 train_time:1003123ms step_avg:169.93ms\nstep:5914/10000 train_loss:4.0641 train_time:1003292ms step_avg:169.93ms\nstep:5915/10000 train_loss:3.8896 train_time:1003459ms step_avg:169.93ms\nstep:5916/10000 train_loss:3.8595 train_time:1003630ms step_avg:169.93ms\nstep:5917/10000 train_loss:3.8336 train_time:1003800ms step_avg:169.93ms\nstep:5918/10000 train_loss:3.8661 train_time:1003970ms step_avg:169.93ms\nstep:5919/10000 train_loss:4.0473 train_time:1004141ms step_avg:169.93ms\nstep:5920/10000 train_loss:3.8373 train_time:1004310ms step_avg:169.93ms\nstep:5921/10000 train_loss:3.6927 train_time:1004479ms step_avg:169.93ms\nstep:5922/10000 train_loss:3.8417 train_time:1004648ms step_avg:169.93ms\nstep:5923/10000 train_loss:3.8174 train_time:1004817ms step_avg:169.93ms\nstep:5924/10000 train_loss:3.9555 train_time:1004986ms step_avg:169.93ms\nstep:5925/10000 train_loss:3.9328 train_time:1005158ms step_avg:169.93ms\nstep:5926/10000 train_loss:3.8909 train_time:1005327ms step_avg:169.93ms\nstep:5927/10000 train_loss:4.0035 train_time:1005498ms step_avg:169.93ms\nstep:5928/10000 train_loss:4.0130 train_time:1005667ms step_avg:169.93ms\nstep:5929/10000 train_loss:3.8632 train_time:1005837ms step_avg:169.93ms\nstep:5930/10000 train_loss:3.8929 train_time:1006005ms step_avg:169.93ms\nstep:5931/10000 train_loss:3.8301 train_time:1006177ms step_avg:169.93ms\nstep:5932/10000 train_loss:3.8932 train_time:1006345ms step_avg:169.93ms\nstep:5933/10000 train_loss:3.9028 train_time:1006515ms step_avg:169.93ms\nstep:5934/10000 train_loss:4.0648 train_time:1006681ms step_avg:169.93ms\nstep:5935/10000 train_loss:4.0028 train_time:1006853ms step_avg:169.93ms\nstep:5936/10000 train_loss:4.0602 train_time:1007023ms step_avg:169.93ms\nstep:5937/10000 train_loss:3.8929 train_time:1007195ms step_avg:169.93ms\nstep:5938/10000 train_loss:3.7768 train_time:1007364ms step_avg:169.93ms\nstep:5939/10000 train_loss:3.9395 train_time:1007536ms step_avg:169.93ms\nstep:5940/10000 train_loss:4.0051 train_time:1007702ms step_avg:169.93ms\nstep:5941/10000 train_loss:3.8544 train_time:1007872ms step_avg:169.93ms\nstep:5942/10000 train_loss:3.9175 train_time:1008043ms step_avg:169.93ms\nstep:5943/10000 train_loss:3.9299 train_time:1008215ms step_avg:169.93ms\nstep:5944/10000 train_loss:3.8951 train_time:1008386ms step_avg:169.93ms\nstep:5945/10000 train_loss:3.9910 train_time:1008555ms step_avg:169.93ms\nstep:5946/10000 train_loss:4.0122 train_time:1008723ms step_avg:169.93ms\nstep:5947/10000 train_loss:3.8420 train_time:1008893ms step_avg:169.93ms\nstep:5948/10000 train_loss:3.8836 train_time:1009060ms step_avg:169.93ms\nstep:5949/10000 train_loss:4.0032 train_time:1009230ms step_avg:169.93ms\nstep:5950/10000 train_loss:3.9025 train_time:1009400ms step_avg:169.93ms\nstep:5951/10000 train_loss:3.9608 train_time:1009569ms step_avg:169.93ms\nstep:5952/10000 train_loss:3.8507 train_time:1009737ms step_avg:169.93ms\nstep:5953/10000 train_loss:3.9457 train_time:1009905ms step_avg:169.93ms\nstep:5954/10000 train_loss:3.7975 train_time:1010076ms step_avg:169.93ms\nstep:5955/10000 train_loss:3.9995 train_time:1010246ms step_avg:169.93ms\nstep:5956/10000 train_loss:3.8535 train_time:1010417ms step_avg:169.93ms\nstep:5957/10000 train_loss:3.9174 train_time:1010585ms step_avg:169.93ms\nstep:5958/10000 train_loss:3.9179 train_time:1010762ms step_avg:169.93ms\nstep:5959/10000 train_loss:3.9626 train_time:1010930ms step_avg:169.93ms\nstep:5960/10000 train_loss:3.9152 train_time:1011098ms step_avg:169.93ms\nstep:5961/10000 train_loss:4.0870 train_time:1011264ms step_avg:169.93ms\nstep:5962/10000 train_loss:3.7254 train_time:1011436ms step_avg:169.93ms\nstep:5963/10000 train_loss:3.9509 train_time:1011603ms step_avg:169.93ms\nstep:5964/10000 train_loss:3.8228 train_time:1011773ms step_avg:169.93ms\nstep:5965/10000 train_loss:3.9850 train_time:1011941ms step_avg:169.93ms\nstep:5966/10000 train_loss:3.6556 train_time:1012111ms step_avg:169.93ms\nstep:5967/10000 train_loss:4.3467 train_time:1012279ms step_avg:169.93ms\nstep:5968/10000 train_loss:3.9542 train_time:1012450ms step_avg:169.93ms\nstep:5969/10000 train_loss:3.9397 train_time:1012618ms step_avg:169.93ms\nstep:5970/10000 train_loss:3.7674 train_time:1012785ms step_avg:169.93ms\nstep:5971/10000 train_loss:4.0844 train_time:1012955ms step_avg:169.93ms\nstep:5972/10000 train_loss:3.8784 train_time:1013121ms step_avg:169.93ms\nstep:5973/10000 train_loss:3.8471 train_time:1013291ms step_avg:169.93ms\nstep:5974/10000 train_loss:4.1214 train_time:1013461ms step_avg:169.93ms\nstep:5975/10000 train_loss:4.1158 train_time:1013633ms step_avg:169.93ms\nstep:5976/10000 train_loss:3.8285 train_time:1013799ms step_avg:169.93ms\nstep:5977/10000 train_loss:4.0262 train_time:1013967ms step_avg:169.93ms\nstep:5978/10000 train_loss:3.9811 train_time:1014134ms step_avg:169.93ms\nstep:5979/10000 train_loss:3.9686 train_time:1014303ms step_avg:169.93ms\nstep:5980/10000 train_loss:3.8598 train_time:1014474ms step_avg:169.93ms\nstep:5981/10000 train_loss:3.7575 train_time:1014643ms step_avg:169.93ms\nstep:5982/10000 train_loss:3.8789 train_time:1014814ms step_avg:169.93ms\nstep:5983/10000 train_loss:3.9403 train_time:1014981ms step_avg:169.93ms\nstep:5984/10000 train_loss:4.0618 train_time:1015150ms step_avg:169.93ms\nstep:5985/10000 train_loss:3.9050 train_time:1015317ms step_avg:169.93ms\nstep:5986/10000 train_loss:4.0990 train_time:1015486ms step_avg:169.93ms\nstep:5987/10000 train_loss:3.9139 train_time:1015662ms step_avg:169.93ms\nstep:5988/10000 train_loss:3.9162 train_time:1015824ms step_avg:169.93ms\nstep:5989/10000 train_loss:3.8201 train_time:1015993ms step_avg:169.93ms\nstep:5990/10000 train_loss:3.9471 train_time:1016160ms step_avg:169.93ms\nstep:5991/10000 train_loss:3.9172 train_time:1016327ms step_avg:169.93ms\nstep:5992/10000 train_loss:3.6666 train_time:1016494ms step_avg:169.93ms\nstep:5993/10000 train_loss:4.0207 train_time:1016663ms step_avg:169.93ms\nstep:5994/10000 train_loss:3.9110 train_time:1016833ms step_avg:169.93ms\nstep:5995/10000 train_loss:3.8498 train_time:1017001ms step_avg:169.93ms\nstep:5996/10000 train_loss:3.9532 train_time:1017172ms step_avg:169.93ms\nstep:5997/10000 train_loss:3.8496 train_time:1017342ms step_avg:169.93ms\nstep:5998/10000 train_loss:3.8676 train_time:1017512ms step_avg:169.93ms\nstep:5999/10000 train_loss:3.9747 train_time:1017678ms step_avg:169.92ms\nstep:6000/10000 train_loss:3.9204 train_time:1017846ms step_avg:169.92ms\nstep:6001/10000 train_loss:4.0538 train_time:1018016ms step_avg:169.92ms\nstep:6002/10000 train_loss:3.9550 train_time:1018186ms step_avg:169.92ms\nstep:6003/10000 train_loss:3.9349 train_time:1018357ms step_avg:169.92ms\nstep:6004/10000 train_loss:3.9501 train_time:1018525ms step_avg:169.92ms\nstep:6005/10000 train_loss:3.7612 train_time:1018694ms step_avg:169.92ms\nstep:6006/10000 train_loss:3.9149 train_time:1018864ms step_avg:169.92ms\nstep:6007/10000 train_loss:3.8640 train_time:1019035ms step_avg:169.92ms\nstep:6008/10000 train_loss:3.9425 train_time:1019202ms step_avg:169.92ms\nstep:6009/10000 train_loss:3.8334 train_time:1019371ms step_avg:169.92ms\nstep:6010/10000 train_loss:3.8828 train_time:1019540ms step_avg:169.92ms\nstep:6011/10000 train_loss:3.9068 train_time:1019708ms step_avg:169.92ms\nstep:6012/10000 train_loss:3.8791 train_time:1019876ms step_avg:169.92ms\nstep:6013/10000 train_loss:4.0477 train_time:1020045ms step_avg:169.92ms\nstep:6014/10000 train_loss:3.8421 train_time:1020215ms step_avg:169.92ms\nstep:6015/10000 train_loss:3.9714 train_time:1020383ms step_avg:169.92ms\nstep:6016/10000 train_loss:3.7112 train_time:1020553ms step_avg:169.92ms\nstep:6017/10000 train_loss:3.5770 train_time:1020722ms step_avg:169.92ms\nstep:6018/10000 train_loss:3.8247 train_time:1020892ms step_avg:169.92ms\nstep:6019/10000 train_loss:4.1731 train_time:1021063ms step_avg:169.92ms\nstep:6020/10000 train_loss:3.9699 train_time:1021233ms step_avg:169.92ms\nstep:6021/10000 train_loss:3.8175 train_time:1021403ms step_avg:169.92ms\nstep:6022/10000 train_loss:3.9135 train_time:1021570ms step_avg:169.92ms\nstep:6023/10000 train_loss:3.9450 train_time:1021739ms step_avg:169.92ms\nstep:6024/10000 train_loss:3.8588 train_time:1021906ms step_avg:169.92ms\nstep:6025/10000 train_loss:3.9199 train_time:1022075ms step_avg:169.92ms\nstep:6026/10000 train_loss:3.9562 train_time:1022244ms step_avg:169.92ms\nstep:6027/10000 train_loss:3.6103 train_time:1022413ms step_avg:169.92ms\nstep:6028/10000 train_loss:3.8744 train_time:1022581ms step_avg:169.92ms\nstep:6029/10000 train_loss:3.8734 train_time:1022750ms step_avg:169.92ms\nstep:6030/10000 train_loss:3.9079 train_time:1022919ms step_avg:169.92ms\nstep:6031/10000 train_loss:3.7948 train_time:1023089ms step_avg:169.92ms\nstep:6032/10000 train_loss:3.8368 train_time:1023260ms step_avg:169.92ms\nstep:6033/10000 train_loss:3.8826 train_time:1023430ms step_avg:169.92ms\nstep:6034/10000 train_loss:3.8350 train_time:1023599ms step_avg:169.92ms\nstep:6035/10000 train_loss:3.8127 train_time:1023768ms step_avg:169.92ms\nstep:6036/10000 train_loss:3.9147 train_time:1023939ms step_avg:169.92ms\nstep:6037/10000 train_loss:4.0963 train_time:1024107ms step_avg:169.92ms\nstep:6038/10000 train_loss:3.8467 train_time:1024278ms step_avg:169.92ms\nstep:6039/10000 train_loss:3.8585 train_time:1024446ms step_avg:169.92ms\nstep:6040/10000 train_loss:3.8630 train_time:1024614ms step_avg:169.92ms\nstep:6041/10000 train_loss:3.8413 train_time:1024783ms step_avg:169.92ms\nstep:6042/10000 train_loss:3.8841 train_time:1024952ms step_avg:169.92ms\nstep:6043/10000 train_loss:3.8225 train_time:1025122ms step_avg:169.92ms\nstep:6044/10000 train_loss:4.1436 train_time:1025290ms step_avg:169.92ms\nstep:6045/10000 train_loss:3.9840 train_time:1025459ms step_avg:169.92ms\nstep:6046/10000 train_loss:3.8845 train_time:1025628ms step_avg:169.92ms\nstep:6047/10000 train_loss:3.8303 train_time:1025799ms step_avg:169.92ms\nstep:6048/10000 train_loss:3.8701 train_time:1025967ms step_avg:169.92ms\nstep:6049/10000 train_loss:4.0385 train_time:1026138ms step_avg:169.92ms\nstep:6050/10000 train_loss:3.9368 train_time:1026305ms step_avg:169.92ms\nstep:6051/10000 train_loss:3.8337 train_time:1026474ms step_avg:169.92ms\nstep:6052/10000 train_loss:3.8979 train_time:1026643ms step_avg:169.92ms\nstep:6053/10000 train_loss:3.8689 train_time:1026812ms step_avg:169.92ms\nstep:6054/10000 train_loss:3.8240 train_time:1026981ms step_avg:169.92ms\nstep:6055/10000 train_loss:3.9899 train_time:1027153ms step_avg:169.92ms\nstep:6056/10000 train_loss:3.9865 train_time:1027323ms step_avg:169.92ms\nstep:6057/10000 train_loss:3.8789 train_time:1027492ms step_avg:169.92ms\nstep:6058/10000 train_loss:3.9840 train_time:1027660ms step_avg:169.92ms\nstep:6059/10000 train_loss:3.9534 train_time:1027831ms step_avg:169.92ms\nstep:6060/10000 train_loss:4.0304 train_time:1028001ms step_avg:169.92ms\nstep:6061/10000 train_loss:3.9839 train_time:1028170ms step_avg:169.92ms\nstep:6062/10000 train_loss:3.8589 train_time:1028341ms step_avg:169.92ms\nstep:6063/10000 train_loss:3.8555 train_time:1028508ms step_avg:169.92ms\nstep:6064/10000 train_loss:3.8971 train_time:1028677ms step_avg:169.92ms\nstep:6065/10000 train_loss:3.9132 train_time:1028846ms step_avg:169.92ms\nstep:6066/10000 train_loss:3.9037 train_time:1029016ms step_avg:169.92ms\nstep:6067/10000 train_loss:3.9392 train_time:1029184ms step_avg:169.92ms\nstep:6068/10000 train_loss:3.7172 train_time:1029356ms step_avg:169.92ms\nstep:6069/10000 train_loss:3.9961 train_time:1029526ms step_avg:169.92ms\nstep:6070/10000 train_loss:3.8954 train_time:1029695ms step_avg:169.92ms\nstep:6071/10000 train_loss:3.9928 train_time:1029864ms step_avg:169.92ms\nstep:6072/10000 train_loss:3.8481 train_time:1030035ms step_avg:169.92ms\nstep:6073/10000 train_loss:4.0234 train_time:1030204ms step_avg:169.92ms\nstep:6074/10000 train_loss:3.8668 train_time:1030374ms step_avg:169.92ms\nstep:6075/10000 train_loss:3.8856 train_time:1030542ms step_avg:169.92ms\nstep:6076/10000 train_loss:3.8696 train_time:1030713ms step_avg:169.92ms\nstep:6077/10000 train_loss:4.0855 train_time:1030882ms step_avg:169.92ms\nstep:6078/10000 train_loss:3.9558 train_time:1031051ms step_avg:169.92ms\nstep:6079/10000 train_loss:3.9078 train_time:1031220ms step_avg:169.92ms\nstep:6080/10000 train_loss:3.9669 train_time:1031390ms step_avg:169.92ms\nstep:6081/10000 train_loss:3.9479 train_time:1031561ms step_avg:169.92ms\nstep:6082/10000 train_loss:3.8219 train_time:1031732ms step_avg:169.92ms\nstep:6083/10000 train_loss:3.9431 train_time:1031903ms step_avg:169.92ms\nstep:6084/10000 train_loss:3.8588 train_time:1032073ms step_avg:169.92ms\nstep:6085/10000 train_loss:3.9063 train_time:1032241ms step_avg:169.92ms\nstep:6086/10000 train_loss:3.9942 train_time:1032416ms step_avg:169.92ms\nstep:6087/10000 train_loss:3.8625 train_time:1032578ms step_avg:169.92ms\nstep:6088/10000 train_loss:4.1156 train_time:1032748ms step_avg:169.92ms\nstep:6089/10000 train_loss:4.0506 train_time:1032917ms step_avg:169.92ms\nstep:6090/10000 train_loss:3.9931 train_time:1033085ms step_avg:169.92ms\nstep:6091/10000 train_loss:4.0051 train_time:1033261ms step_avg:169.92ms\nstep:6092/10000 train_loss:3.8338 train_time:1033427ms step_avg:169.92ms\nstep:6093/10000 train_loss:3.8607 train_time:1033604ms step_avg:169.92ms\nstep:6094/10000 train_loss:3.9317 train_time:1033765ms step_avg:169.92ms\nstep:6095/10000 train_loss:3.8901 train_time:1033938ms step_avg:169.92ms\nstep:6096/10000 train_loss:3.8903 train_time:1034319ms step_avg:169.95ms\nstep:6097/10000 train_loss:3.8294 train_time:1034483ms step_avg:169.95ms\nstep:6098/10000 train_loss:3.8840 train_time:1034653ms step_avg:169.95ms\nstep:6099/10000 train_loss:3.9562 train_time:1034822ms step_avg:169.95ms\nstep:6100/10000 train_loss:3.9076 train_time:1034990ms step_avg:169.95ms\nstep:6101/10000 train_loss:3.8922 train_time:1035158ms step_avg:169.95ms\nstep:6102/10000 train_loss:3.9884 train_time:1035338ms step_avg:169.95ms\nstep:6103/10000 train_loss:3.8091 train_time:1035505ms step_avg:169.95ms\nstep:6104/10000 train_loss:3.9108 train_time:1035675ms step_avg:169.95ms\nstep:6105/10000 train_loss:3.8764 train_time:1035844ms step_avg:169.95ms\nstep:6106/10000 train_loss:3.8624 train_time:1036012ms step_avg:169.95ms\nstep:6107/10000 train_loss:3.7818 train_time:1036181ms step_avg:169.95ms\nstep:6108/10000 train_loss:3.9832 train_time:1036353ms step_avg:169.95ms\nstep:6109/10000 train_loss:4.0263 train_time:1036524ms step_avg:169.95ms\nstep:6110/10000 train_loss:3.9367 train_time:1036694ms step_avg:169.95ms\nstep:6111/10000 train_loss:3.9759 train_time:1036862ms step_avg:169.95ms\nstep:6112/10000 train_loss:3.8611 train_time:1037030ms step_avg:169.95ms\nstep:6113/10000 train_loss:3.9187 train_time:1037201ms step_avg:169.95ms\nstep:6114/10000 train_loss:3.9314 train_time:1037373ms step_avg:169.95ms\nstep:6115/10000 train_loss:3.9387 train_time:1037542ms step_avg:169.95ms\nstep:6116/10000 train_loss:3.8679 train_time:1037713ms step_avg:169.95ms\nstep:6117/10000 train_loss:3.9272 train_time:1037881ms step_avg:169.95ms\nstep:6118/10000 train_loss:3.9106 train_time:1038051ms step_avg:169.95ms\nstep:6119/10000 train_loss:3.9808 train_time:1038223ms step_avg:169.95ms\nstep:6120/10000 train_loss:3.8653 train_time:1038394ms step_avg:169.95ms\nstep:6121/10000 train_loss:3.9219 train_time:1038563ms step_avg:169.95ms\nstep:6122/10000 train_loss:3.9002 train_time:1038734ms step_avg:169.95ms\nstep:6123/10000 train_loss:3.8777 train_time:1038902ms step_avg:169.95ms\nstep:6124/10000 train_loss:3.9034 train_time:1039071ms step_avg:169.95ms\nstep:6125/10000 train_loss:3.9122 train_time:1039238ms step_avg:169.95ms\nstep:6126/10000 train_loss:4.0441 train_time:1039406ms step_avg:169.95ms\nstep:6127/10000 train_loss:4.0538 train_time:1039578ms step_avg:169.95ms\nstep:6128/10000 train_loss:3.8717 train_time:1039747ms step_avg:169.95ms\nstep:6129/10000 train_loss:3.8675 train_time:1039916ms step_avg:169.95ms\nstep:6130/10000 train_loss:4.0246 train_time:1040083ms step_avg:169.95ms\nstep:6131/10000 train_loss:3.9679 train_time:1040255ms step_avg:169.95ms\nstep:6132/10000 train_loss:3.8641 train_time:1040423ms step_avg:169.95ms\nstep:6133/10000 train_loss:3.9046 train_time:1040594ms step_avg:169.95ms\nstep:6134/10000 train_loss:3.8534 train_time:1040763ms step_avg:169.95ms\nstep:6135/10000 train_loss:3.8721 train_time:1040933ms step_avg:169.95ms\nstep:6136/10000 train_loss:4.0498 train_time:1041101ms step_avg:169.95ms\nstep:6137/10000 train_loss:3.9031 train_time:1041271ms step_avg:169.95ms\nstep:6138/10000 train_loss:3.8385 train_time:1041439ms step_avg:169.95ms\nstep:6139/10000 train_loss:4.0304 train_time:1041610ms step_avg:169.95ms\nstep:6140/10000 train_loss:3.8827 train_time:1041778ms step_avg:169.95ms\nstep:6141/10000 train_loss:3.9734 train_time:1041946ms step_avg:169.95ms\nstep:6142/10000 train_loss:3.9870 train_time:1042114ms step_avg:169.95ms\nstep:6143/10000 train_loss:3.8501 train_time:1042284ms step_avg:169.95ms\nstep:6144/10000 train_loss:3.9444 train_time:1042456ms step_avg:169.95ms\nstep:6145/10000 train_loss:3.8587 train_time:1042624ms step_avg:169.95ms\nstep:6146/10000 train_loss:3.9251 train_time:1042795ms step_avg:169.95ms\nstep:6147/10000 train_loss:3.8499 train_time:1042964ms step_avg:169.95ms\nstep:6148/10000 train_loss:3.8413 train_time:1043134ms step_avg:169.95ms\nstep:6149/10000 train_loss:3.9649 train_time:1043304ms step_avg:169.95ms\nstep:6150/10000 train_loss:3.9379 train_time:1043474ms step_avg:169.95ms\nstep:6151/10000 train_loss:3.9108 train_time:1043644ms step_avg:169.95ms\nstep:6152/10000 train_loss:3.7852 train_time:1043814ms step_avg:169.95ms\nstep:6153/10000 train_loss:3.8298 train_time:1043984ms step_avg:169.95ms\nstep:6154/10000 train_loss:3.8981 train_time:1044155ms step_avg:169.95ms\nstep:6155/10000 train_loss:3.8982 train_time:1044323ms step_avg:169.95ms\nstep:6156/10000 train_loss:3.6891 train_time:1044500ms step_avg:169.95ms\nstep:6157/10000 train_loss:4.1282 train_time:1044664ms step_avg:169.95ms\nstep:6158/10000 train_loss:3.8323 train_time:1044834ms step_avg:169.95ms\nstep:6159/10000 train_loss:3.9380 train_time:1045004ms step_avg:169.95ms\nstep:6160/10000 train_loss:3.8278 train_time:1045173ms step_avg:169.95ms\nstep:6161/10000 train_loss:3.7821 train_time:1045340ms step_avg:169.95ms\nstep:6162/10000 train_loss:3.9265 train_time:1045511ms step_avg:169.95ms\nstep:6163/10000 train_loss:3.9537 train_time:1045681ms step_avg:169.95ms\nstep:6164/10000 train_loss:3.8290 train_time:1045850ms step_avg:169.95ms\nstep:6165/10000 train_loss:3.9300 train_time:1046021ms step_avg:169.95ms\nstep:6166/10000 train_loss:4.0759 train_time:1046189ms step_avg:169.95ms\nstep:6167/10000 train_loss:3.9228 train_time:1046360ms step_avg:169.95ms\nstep:6168/10000 train_loss:3.7402 train_time:1046528ms step_avg:169.95ms\nstep:6169/10000 train_loss:3.8885 train_time:1046699ms step_avg:169.95ms\nstep:6170/10000 train_loss:3.9889 train_time:1046868ms step_avg:169.95ms\nstep:6171/10000 train_loss:4.3045 train_time:1047040ms step_avg:169.95ms\nstep:6172/10000 train_loss:4.0437 train_time:1047209ms step_avg:169.95ms\nstep:6173/10000 train_loss:3.7808 train_time:1047380ms step_avg:169.95ms\nstep:6174/10000 train_loss:3.9103 train_time:1047549ms step_avg:169.95ms\nstep:6175/10000 train_loss:3.8536 train_time:1047720ms step_avg:169.95ms\nstep:6176/10000 train_loss:3.8181 train_time:1047889ms step_avg:169.95ms\nstep:6177/10000 train_loss:3.8682 train_time:1048057ms step_avg:169.95ms\nstep:6178/10000 train_loss:3.8883 train_time:1048226ms step_avg:169.95ms\nstep:6179/10000 train_loss:4.0026 train_time:1048395ms step_avg:169.95ms\nstep:6180/10000 train_loss:3.6684 train_time:1048565ms step_avg:169.95ms\nstep:6181/10000 train_loss:4.0369 train_time:1048734ms step_avg:169.95ms\nstep:6182/10000 train_loss:3.8475 train_time:1048901ms step_avg:169.95ms\nstep:6183/10000 train_loss:3.9713 train_time:1049070ms step_avg:169.94ms\nstep:6184/10000 train_loss:3.8952 train_time:1049240ms step_avg:169.94ms\nstep:6185/10000 train_loss:3.7882 train_time:1049409ms step_avg:169.94ms\nstep:6186/10000 train_loss:3.8534 train_time:1049579ms step_avg:169.94ms\nstep:6187/10000 train_loss:3.9889 train_time:1049748ms step_avg:169.94ms\nstep:6188/10000 train_loss:3.9159 train_time:1049923ms step_avg:169.95ms\nstep:6189/10000 train_loss:3.8975 train_time:1050087ms step_avg:169.94ms\nstep:6190/10000 train_loss:3.9379 train_time:1050260ms step_avg:169.94ms\nstep:6191/10000 train_loss:3.9816 train_time:1050429ms step_avg:169.94ms\nstep:6192/10000 train_loss:3.6869 train_time:1050597ms step_avg:169.94ms\nstep:6193/10000 train_loss:3.8836 train_time:1050766ms step_avg:169.94ms\nstep:6194/10000 train_loss:3.8985 train_time:1050936ms step_avg:169.94ms\nstep:6195/10000 train_loss:3.9603 train_time:1051105ms step_avg:169.94ms\nstep:6196/10000 train_loss:4.1010 train_time:1051275ms step_avg:169.94ms\nstep:6197/10000 train_loss:4.1254 train_time:1051444ms step_avg:169.94ms\nstep:6198/10000 train_loss:3.8525 train_time:1051614ms step_avg:169.94ms\nstep:6199/10000 train_loss:3.8480 train_time:1051782ms step_avg:169.94ms\nstep:6200/10000 train_loss:4.0145 train_time:1051954ms step_avg:169.94ms\nstep:6201/10000 train_loss:3.8459 train_time:1052123ms step_avg:169.94ms\nstep:6202/10000 train_loss:3.8364 train_time:1052294ms step_avg:169.94ms\nstep:6203/10000 train_loss:3.9366 train_time:1052460ms step_avg:169.94ms\nstep:6204/10000 train_loss:3.9347 train_time:1052628ms step_avg:169.94ms\nstep:6205/10000 train_loss:3.9988 train_time:1052798ms step_avg:169.94ms\nstep:6206/10000 train_loss:3.8449 train_time:1052966ms step_avg:169.94ms\nstep:6207/10000 train_loss:4.0141 train_time:1053135ms step_avg:169.94ms\nstep:6208/10000 train_loss:4.0393 train_time:1053311ms step_avg:169.94ms\nstep:6209/10000 train_loss:3.9431 train_time:1053477ms step_avg:169.94ms\nstep:6210/10000 train_loss:3.9711 train_time:1053645ms step_avg:169.94ms\nstep:6211/10000 train_loss:3.9353 train_time:1053815ms step_avg:169.94ms\nstep:6212/10000 train_loss:3.9578 train_time:1053983ms step_avg:169.94ms\nstep:6213/10000 train_loss:3.9723 train_time:1054152ms step_avg:169.94ms\nstep:6214/10000 train_loss:3.8834 train_time:1054323ms step_avg:169.94ms\nstep:6215/10000 train_loss:4.0023 train_time:1054496ms step_avg:169.94ms\nstep:6216/10000 train_loss:3.8514 train_time:1054664ms step_avg:169.94ms\nstep:6217/10000 train_loss:4.0081 train_time:1054835ms step_avg:169.94ms\nstep:6218/10000 train_loss:3.8359 train_time:1055003ms step_avg:169.94ms\nstep:6219/10000 train_loss:3.9568 train_time:1055172ms step_avg:169.94ms\nstep:6220/10000 train_loss:4.0292 train_time:1055342ms step_avg:169.94ms\nstep:6221/10000 train_loss:3.8289 train_time:1055516ms step_avg:169.94ms\nstep:6222/10000 train_loss:3.8236 train_time:1055684ms step_avg:169.94ms\nstep:6223/10000 train_loss:3.9721 train_time:1055895ms step_avg:169.95ms\nstep:6224/10000 train_loss:3.9717 train_time:1056046ms step_avg:169.95ms\nstep:6225/10000 train_loss:3.9420 train_time:1056196ms step_avg:169.94ms\nstep:6226/10000 train_loss:3.8065 train_time:1056366ms step_avg:169.94ms\nstep:6227/10000 train_loss:3.8669 train_time:1056535ms step_avg:169.94ms\nstep:6228/10000 train_loss:3.8857 train_time:1056704ms step_avg:169.94ms\nstep:6229/10000 train_loss:3.8994 train_time:1056876ms step_avg:169.94ms\nstep:6230/10000 train_loss:3.9325 train_time:1057045ms step_avg:169.94ms\nstep:6231/10000 train_loss:3.8600 train_time:1057216ms step_avg:169.94ms\nstep:6232/10000 train_loss:3.8241 train_time:1057384ms step_avg:169.94ms\nstep:6233/10000 train_loss:3.9445 train_time:1057554ms step_avg:169.94ms\nstep:6234/10000 train_loss:3.9428 train_time:1057723ms step_avg:169.94ms\nstep:6235/10000 train_loss:3.8676 train_time:1057893ms step_avg:169.94ms\nstep:6236/10000 train_loss:3.8943 train_time:1058060ms step_avg:169.94ms\nstep:6237/10000 train_loss:3.9971 train_time:1058230ms step_avg:169.94ms\nstep:6238/10000 train_loss:3.8824 train_time:1058400ms step_avg:169.94ms\nstep:6239/10000 train_loss:4.0153 train_time:1058568ms step_avg:169.94ms\nstep:6240/10000 train_loss:3.8494 train_time:1058735ms step_avg:169.94ms\nstep:6241/10000 train_loss:3.9192 train_time:1058905ms step_avg:169.94ms\nstep:6242/10000 train_loss:3.9574 train_time:1059075ms step_avg:169.94ms\nstep:6243/10000 train_loss:3.9762 train_time:1059243ms step_avg:169.94ms\nstep:6244/10000 train_loss:3.9077 train_time:1059414ms step_avg:169.94ms\nstep:6245/10000 train_loss:3.7131 train_time:1059584ms step_avg:169.94ms\nstep:6246/10000 train_loss:3.8845 train_time:1059752ms step_avg:169.94ms\nstep:6247/10000 train_loss:3.8991 train_time:1059921ms step_avg:169.94ms\nstep:6248/10000 train_loss:3.8672 train_time:1060088ms step_avg:169.94ms\nstep:6249/10000 train_loss:3.7359 train_time:1060259ms step_avg:169.94ms\nstep:6250/10000 train_loss:3.8277 train_time:1060428ms step_avg:169.94ms\nstep:6251/10000 train_loss:3.9002 train_time:1060599ms step_avg:169.94ms\nstep:6252/10000 train_loss:3.9868 train_time:1060769ms step_avg:169.94ms\nstep:6253/10000 train_loss:4.0907 train_time:1060940ms step_avg:169.94ms\nstep:6254/10000 train_loss:4.0137 train_time:1061108ms step_avg:169.94ms\nstep:6255/10000 train_loss:3.9181 train_time:1061277ms step_avg:169.94ms\nstep:6256/10000 train_loss:3.8525 train_time:1061448ms step_avg:169.94ms\nstep:6257/10000 train_loss:3.8417 train_time:1061618ms step_avg:169.94ms\nstep:6258/10000 train_loss:3.8687 train_time:1061787ms step_avg:169.94ms\nstep:6259/10000 train_loss:3.8953 train_time:1061958ms step_avg:169.94ms\nstep:6260/10000 train_loss:3.9696 train_time:1062127ms step_avg:169.94ms\nstep:6261/10000 train_loss:3.8995 train_time:1062296ms step_avg:169.94ms\nstep:6262/10000 train_loss:3.8196 train_time:1062464ms step_avg:169.94ms\nstep:6263/10000 train_loss:4.0524 train_time:1062633ms step_avg:169.94ms\nstep:6264/10000 train_loss:3.8179 train_time:1062801ms step_avg:169.94ms\nstep:6265/10000 train_loss:3.9112 train_time:1062971ms step_avg:169.94ms\nstep:6266/10000 train_loss:3.9780 train_time:1063139ms step_avg:169.94ms\nstep:6267/10000 train_loss:3.7876 train_time:1063306ms step_avg:169.94ms\nstep:6268/10000 train_loss:3.9404 train_time:1063474ms step_avg:169.94ms\nstep:6269/10000 train_loss:3.9325 train_time:1063644ms step_avg:169.94ms\nstep:6270/10000 train_loss:3.7749 train_time:1063815ms step_avg:169.94ms\nstep:6271/10000 train_loss:3.9206 train_time:1063984ms step_avg:169.94ms\nstep:6272/10000 train_loss:3.7418 train_time:1064155ms step_avg:169.94ms\nstep:6273/10000 train_loss:3.9278 train_time:1064325ms step_avg:169.94ms\nstep:6274/10000 train_loss:3.8808 train_time:1064495ms step_avg:169.94ms\nstep:6275/10000 train_loss:3.9013 train_time:1064665ms step_avg:169.94ms\nstep:6276/10000 train_loss:3.8250 train_time:1064836ms step_avg:169.94ms\nstep:6277/10000 train_loss:3.8375 train_time:1065004ms step_avg:169.94ms\nstep:6278/10000 train_loss:3.9617 train_time:1065177ms step_avg:169.94ms\nstep:6279/10000 train_loss:3.9029 train_time:1065346ms step_avg:169.94ms\nstep:6280/10000 train_loss:3.7948 train_time:1065516ms step_avg:169.94ms\nstep:6281/10000 train_loss:3.7996 train_time:1065682ms step_avg:169.94ms\nstep:6282/10000 train_loss:3.8744 train_time:1065853ms step_avg:169.94ms\nstep:6283/10000 train_loss:3.8685 train_time:1066022ms step_avg:169.94ms\nstep:6284/10000 train_loss:3.8057 train_time:1066193ms step_avg:169.94ms\nstep:6285/10000 train_loss:3.8725 train_time:1066364ms step_avg:169.94ms\nstep:6286/10000 train_loss:3.8463 train_time:1066535ms step_avg:169.94ms\nstep:6287/10000 train_loss:3.8913 train_time:1066704ms step_avg:169.94ms\nstep:6288/10000 train_loss:3.8198 train_time:1066875ms step_avg:169.94ms\nstep:6289/10000 train_loss:3.9535 train_time:1067044ms step_avg:169.94ms\nstep:6290/10000 train_loss:3.9405 train_time:1067214ms step_avg:169.94ms\nstep:6291/10000 train_loss:3.8674 train_time:1067381ms step_avg:169.94ms\nstep:6292/10000 train_loss:3.8748 train_time:1067550ms step_avg:169.94ms\nstep:6293/10000 train_loss:3.8647 train_time:1067718ms step_avg:169.94ms\nstep:6294/10000 train_loss:3.9737 train_time:1067889ms step_avg:169.94ms\nstep:6295/10000 train_loss:3.7272 train_time:1068059ms step_avg:169.94ms\nstep:6296/10000 train_loss:3.6924 train_time:1068227ms step_avg:169.94ms\nstep:6297/10000 train_loss:3.9304 train_time:1068399ms step_avg:169.94ms\nstep:6298/10000 train_loss:3.7837 train_time:1068567ms step_avg:169.94ms\nstep:6299/10000 train_loss:3.8428 train_time:1068737ms step_avg:169.94ms\nstep:6300/10000 train_loss:3.8880 train_time:1068906ms step_avg:169.94ms\nstep:6301/10000 train_loss:3.8729 train_time:1069076ms step_avg:169.94ms\nstep:6302/10000 train_loss:3.8124 train_time:1069243ms step_avg:169.94ms\nstep:6303/10000 train_loss:3.7822 train_time:1069415ms step_avg:169.94ms\nstep:6304/10000 train_loss:3.8945 train_time:1069583ms step_avg:169.94ms\nstep:6305/10000 train_loss:3.8686 train_time:1069750ms step_avg:169.94ms\nstep:6306/10000 train_loss:3.9329 train_time:1069919ms step_avg:169.94ms\nstep:6307/10000 train_loss:3.8589 train_time:1070091ms step_avg:169.94ms\nstep:6308/10000 train_loss:3.8731 train_time:1070263ms step_avg:169.94ms\nstep:6309/10000 train_loss:3.9457 train_time:1070435ms step_avg:169.94ms\nstep:6310/10000 train_loss:3.7956 train_time:1070604ms step_avg:169.94ms\nstep:6311/10000 train_loss:3.8900 train_time:1070776ms step_avg:169.94ms\nstep:6312/10000 train_loss:3.8834 train_time:1070944ms step_avg:169.94ms\nstep:6313/10000 train_loss:4.0232 train_time:1071116ms step_avg:169.94ms\nstep:6314/10000 train_loss:3.9287 train_time:1071284ms step_avg:169.94ms\nstep:6315/10000 train_loss:3.8484 train_time:1071454ms step_avg:169.94ms\nstep:6316/10000 train_loss:3.8645 train_time:1071623ms step_avg:169.94ms\nstep:6317/10000 train_loss:3.8849 train_time:1071791ms step_avg:169.94ms\nstep:6318/10000 train_loss:3.8448 train_time:1071962ms step_avg:169.94ms\nstep:6319/10000 train_loss:3.9322 train_time:1072132ms step_avg:169.94ms\nstep:6320/10000 train_loss:3.8497 train_time:1072301ms step_avg:169.94ms\nstep:6321/10000 train_loss:4.2219 train_time:1072473ms step_avg:169.94ms\nstep:6322/10000 train_loss:3.9143 train_time:1072644ms step_avg:169.94ms\nstep:6323/10000 train_loss:3.8238 train_time:1072814ms step_avg:169.94ms\nstep:6324/10000 train_loss:4.0380 train_time:1072983ms step_avg:169.94ms\nstep:6325/10000 train_loss:3.9339 train_time:1073153ms step_avg:169.94ms\nstep:6326/10000 train_loss:3.8564 train_time:1073323ms step_avg:169.94ms\nstep:6327/10000 train_loss:3.7399 train_time:1073492ms step_avg:169.94ms\nstep:6328/10000 train_loss:3.9173 train_time:1073660ms step_avg:169.94ms\nstep:6329/10000 train_loss:4.0952 train_time:1073827ms step_avg:169.94ms\nstep:6330/10000 train_loss:3.8311 train_time:1073996ms step_avg:169.94ms\nstep:6331/10000 train_loss:3.8611 train_time:1074167ms step_avg:169.94ms\nstep:6332/10000 train_loss:3.9628 train_time:1074337ms step_avg:169.94ms\nstep:6333/10000 train_loss:3.8985 train_time:1074507ms step_avg:169.94ms\nstep:6334/10000 train_loss:3.8177 train_time:1074679ms step_avg:169.94ms\nstep:6335/10000 train_loss:3.7772 train_time:1074848ms step_avg:169.94ms\nstep:6336/10000 train_loss:3.7433 train_time:1075015ms step_avg:169.94ms\nstep:6337/10000 train_loss:3.9059 train_time:1075185ms step_avg:169.94ms\nstep:6338/10000 train_loss:4.0323 train_time:1075353ms step_avg:169.94ms\nstep:6339/10000 train_loss:3.9378 train_time:1075520ms step_avg:169.94ms\nstep:6340/10000 train_loss:3.8509 train_time:1075690ms step_avg:169.94ms\nstep:6341/10000 train_loss:4.0554 train_time:1075859ms step_avg:169.94ms\nstep:6342/10000 train_loss:3.8247 train_time:1076028ms step_avg:169.93ms\nstep:6343/10000 train_loss:4.0498 train_time:1076199ms step_avg:169.94ms\nstep:6344/10000 train_loss:3.8308 train_time:1076367ms step_avg:169.93ms\nstep:6345/10000 train_loss:3.8521 train_time:1076536ms step_avg:169.93ms\nstep:6346/10000 train_loss:3.9695 train_time:1076704ms step_avg:169.93ms\nstep:6347/10000 train_loss:3.9207 train_time:1076875ms step_avg:169.93ms\nstep:6348/10000 train_loss:3.9956 train_time:1077044ms step_avg:169.93ms\nstep:6349/10000 train_loss:3.8626 train_time:1077217ms step_avg:169.93ms\nstep:6350/10000 train_loss:3.8281 train_time:1077385ms step_avg:169.93ms\nstep:6351/10000 train_loss:3.9005 train_time:1077554ms step_avg:169.93ms\nstep:6352/10000 train_loss:3.8520 train_time:1077721ms step_avg:169.93ms\nstep:6353/10000 train_loss:3.9569 train_time:1077891ms step_avg:169.93ms\nstep:6354/10000 train_loss:3.9701 train_time:1078061ms step_avg:169.93ms\nstep:6355/10000 train_loss:4.1529 train_time:1078230ms step_avg:169.93ms\nstep:6356/10000 train_loss:3.7998 train_time:1078401ms step_avg:169.93ms\nstep:6357/10000 train_loss:3.8417 train_time:1078570ms step_avg:169.93ms\nstep:6358/10000 train_loss:3.7433 train_time:1078740ms step_avg:169.93ms\nstep:6359/10000 train_loss:3.7993 train_time:1078909ms step_avg:169.93ms\nstep:6360/10000 train_loss:3.9405 train_time:1079080ms step_avg:169.93ms\nstep:6361/10000 train_loss:3.8268 train_time:1079249ms step_avg:169.93ms\nstep:6362/10000 train_loss:3.9463 train_time:1079420ms step_avg:169.93ms\nstep:6363/10000 train_loss:3.9294 train_time:1079588ms step_avg:169.93ms\nstep:6364/10000 train_loss:3.8654 train_time:1079756ms step_avg:169.93ms\nstep:6365/10000 train_loss:3.8075 train_time:1079924ms step_avg:169.93ms\nstep:6366/10000 train_loss:3.8487 train_time:1080093ms step_avg:169.93ms\nstep:6367/10000 train_loss:4.0703 train_time:1080261ms step_avg:169.93ms\nstep:6368/10000 train_loss:3.8239 train_time:1080433ms step_avg:169.93ms\nstep:6369/10000 train_loss:3.9276 train_time:1080601ms step_avg:169.93ms\nstep:6370/10000 train_loss:4.0200 train_time:1080770ms step_avg:169.93ms\nstep:6371/10000 train_loss:3.9256 train_time:1080942ms step_avg:169.93ms\nstep:6372/10000 train_loss:3.8111 train_time:1081111ms step_avg:169.93ms\nstep:6373/10000 train_loss:3.8066 train_time:1081281ms step_avg:169.93ms\nstep:6374/10000 train_loss:3.8373 train_time:1081454ms step_avg:169.93ms\nstep:6375/10000 train_loss:3.8971 train_time:1081623ms step_avg:169.93ms\nstep:6376/10000 train_loss:4.2003 train_time:1081792ms step_avg:169.93ms\nstep:6377/10000 train_loss:3.5656 train_time:1081961ms step_avg:169.93ms\nstep:6378/10000 train_loss:3.8505 train_time:1082129ms step_avg:169.93ms\nstep:6379/10000 train_loss:3.9585 train_time:1082297ms step_avg:169.93ms\nstep:6380/10000 train_loss:3.7313 train_time:1082464ms step_avg:169.93ms\nstep:6381/10000 train_loss:3.8138 train_time:1082634ms step_avg:169.93ms\nstep:6382/10000 train_loss:3.9273 train_time:1082805ms step_avg:169.93ms\nstep:6383/10000 train_loss:3.8870 train_time:1082975ms step_avg:169.93ms\nstep:6384/10000 train_loss:3.8169 train_time:1083144ms step_avg:169.93ms\nstep:6385/10000 train_loss:3.8337 train_time:1083315ms step_avg:169.93ms\nstep:6386/10000 train_loss:3.9124 train_time:1083484ms step_avg:169.93ms\nstep:6387/10000 train_loss:3.6776 train_time:1083657ms step_avg:169.93ms\nstep:6388/10000 train_loss:3.9071 train_time:1083824ms step_avg:169.93ms\nstep:6389/10000 train_loss:3.9913 train_time:1083993ms step_avg:169.93ms\nstep:6390/10000 train_loss:3.8909 train_time:1084164ms step_avg:169.93ms\nstep:6391/10000 train_loss:3.8877 train_time:1084333ms step_avg:169.93ms\nstep:6392/10000 train_loss:3.8467 train_time:1084503ms step_avg:169.93ms\nstep:6393/10000 train_loss:3.8321 train_time:1084674ms step_avg:169.93ms\nstep:6394/10000 train_loss:4.0299 train_time:1084843ms step_avg:169.93ms\nstep:6395/10000 train_loss:4.0160 train_time:1085015ms step_avg:169.93ms\nstep:6396/10000 train_loss:3.7115 train_time:1085186ms step_avg:169.93ms\nstep:6397/10000 train_loss:3.9072 train_time:1085356ms step_avg:169.93ms\nstep:6398/10000 train_loss:3.7924 train_time:1085525ms step_avg:169.93ms\nstep:6399/10000 train_loss:3.8935 train_time:1085693ms step_avg:169.93ms\nstep:6400/10000 train_loss:3.8696 train_time:1085863ms step_avg:169.93ms\nstep:6401/10000 train_loss:3.9105 train_time:1086033ms step_avg:169.93ms\nstep:6402/10000 train_loss:3.9163 train_time:1086203ms step_avg:169.93ms\nstep:6403/10000 train_loss:3.9092 train_time:1086373ms step_avg:169.93ms\nstep:6404/10000 train_loss:3.8477 train_time:1086542ms step_avg:169.93ms\nstep:6405/10000 train_loss:3.7722 train_time:1086710ms step_avg:169.93ms\nstep:6406/10000 train_loss:3.9464 train_time:1086878ms step_avg:169.93ms\nstep:6407/10000 train_loss:4.3146 train_time:1087047ms step_avg:169.93ms\nstep:6408/10000 train_loss:3.9024 train_time:1087218ms step_avg:169.93ms\nstep:6409/10000 train_loss:3.8588 train_time:1087387ms step_avg:169.93ms\nstep:6410/10000 train_loss:4.0055 train_time:1087561ms step_avg:169.93ms\nstep:6411/10000 train_loss:3.8043 train_time:1087725ms step_avg:169.93ms\nstep:6412/10000 train_loss:3.8587 train_time:1087894ms step_avg:169.93ms\nstep:6413/10000 train_loss:3.8386 train_time:1088063ms step_avg:169.93ms\nstep:6414/10000 train_loss:3.8382 train_time:1088233ms step_avg:169.93ms\nstep:6415/10000 train_loss:3.8315 train_time:1088401ms step_avg:169.93ms\nstep:6416/10000 train_loss:3.7930 train_time:1088572ms step_avg:169.93ms\nstep:6417/10000 train_loss:3.8796 train_time:1088742ms step_avg:169.93ms\nstep:6418/10000 train_loss:4.1348 train_time:1088914ms step_avg:169.93ms\nstep:6419/10000 train_loss:4.0002 train_time:1089082ms step_avg:169.93ms\nstep:6420/10000 train_loss:3.8575 train_time:1089250ms step_avg:169.93ms\nstep:6421/10000 train_loss:3.9379 train_time:1089421ms step_avg:169.93ms\nstep:6422/10000 train_loss:4.0489 train_time:1089591ms step_avg:169.93ms\nstep:6423/10000 train_loss:3.8933 train_time:1089760ms step_avg:169.93ms\nstep:6424/10000 train_loss:3.7706 train_time:1089927ms step_avg:169.93ms\nstep:6425/10000 train_loss:3.7325 train_time:1090099ms step_avg:169.93ms\nstep:6426/10000 train_loss:3.8671 train_time:1090267ms step_avg:169.93ms\nstep:6427/10000 train_loss:3.8360 train_time:1090437ms step_avg:169.93ms\nstep:6428/10000 train_loss:3.8773 train_time:1090607ms step_avg:169.93ms\nstep:6429/10000 train_loss:3.9010 train_time:1090776ms step_avg:169.93ms\nstep:6430/10000 train_loss:3.8721 train_time:1090944ms step_avg:169.93ms\nstep:6431/10000 train_loss:3.8043 train_time:1091113ms step_avg:169.93ms\nstep:6432/10000 train_loss:3.7635 train_time:1091281ms step_avg:169.93ms\nstep:6433/10000 train_loss:3.9927 train_time:1091455ms step_avg:169.93ms\nstep:6434/10000 train_loss:3.7351 train_time:1091625ms step_avg:169.93ms\nstep:6435/10000 train_loss:3.9263 train_time:1091794ms step_avg:169.93ms\nstep:6436/10000 train_loss:3.8279 train_time:1091961ms step_avg:169.93ms\nstep:6437/10000 train_loss:3.8401 train_time:1092131ms step_avg:169.93ms\nstep:6438/10000 train_loss:4.0814 train_time:1092300ms step_avg:169.93ms\nstep:6439/10000 train_loss:3.9978 train_time:1092473ms step_avg:169.93ms\nstep:6440/10000 train_loss:3.8619 train_time:1092641ms step_avg:169.93ms\nstep:6441/10000 train_loss:4.0048 train_time:1092812ms step_avg:169.93ms\nstep:6442/10000 train_loss:3.8456 train_time:1092979ms step_avg:169.93ms\nstep:6443/10000 train_loss:3.8928 train_time:1093147ms step_avg:169.93ms\nstep:6444/10000 train_loss:3.7086 train_time:1093314ms step_avg:169.93ms\nstep:6445/10000 train_loss:3.7553 train_time:1093485ms step_avg:169.93ms\nstep:6446/10000 train_loss:3.8722 train_time:1093653ms step_avg:169.93ms\nstep:6447/10000 train_loss:3.6638 train_time:1093821ms step_avg:169.93ms\nstep:6448/10000 train_loss:3.8776 train_time:1093990ms step_avg:169.93ms\nstep:6449/10000 train_loss:3.8319 train_time:1094160ms step_avg:169.93ms\nstep:6450/10000 train_loss:3.8211 train_time:1094329ms step_avg:169.93ms\nstep:6451/10000 train_loss:3.8629 train_time:1094498ms step_avg:169.93ms\nstep:6452/10000 train_loss:3.8507 train_time:1094668ms step_avg:169.93ms\nstep:6453/10000 train_loss:3.9120 train_time:1094839ms step_avg:169.93ms\nstep:6454/10000 train_loss:4.4138 train_time:1095007ms step_avg:169.93ms\nstep:6455/10000 train_loss:3.9337 train_time:1095177ms step_avg:169.93ms\nstep:6456/10000 train_loss:3.8810 train_time:1095346ms step_avg:169.93ms\nstep:6457/10000 train_loss:3.8580 train_time:1095515ms step_avg:169.93ms\nstep:6458/10000 train_loss:4.0802 train_time:1095684ms step_avg:169.93ms\nstep:6459/10000 train_loss:3.9265 train_time:1095856ms step_avg:169.93ms\nstep:6460/10000 train_loss:3.9566 train_time:1096024ms step_avg:169.93ms\nstep:6461/10000 train_loss:3.8873 train_time:1096194ms step_avg:169.93ms\nstep:6462/10000 train_loss:4.0218 train_time:1096361ms step_avg:169.93ms\nstep:6463/10000 train_loss:3.9823 train_time:1096530ms step_avg:169.93ms\nstep:6464/10000 train_loss:3.8708 train_time:1096699ms step_avg:169.93ms\nstep:6465/10000 train_loss:3.8202 train_time:1096869ms step_avg:169.93ms\nstep:6466/10000 train_loss:3.8853 train_time:1097040ms step_avg:169.93ms\nstep:6467/10000 train_loss:4.0430 train_time:1097211ms step_avg:169.93ms\nstep:6468/10000 train_loss:3.9212 train_time:1097379ms step_avg:169.93ms\nstep:6469/10000 train_loss:3.8075 train_time:1097548ms step_avg:169.93ms\nstep:6470/10000 train_loss:3.9240 train_time:1097717ms step_avg:169.93ms\nstep:6471/10000 train_loss:3.9038 train_time:1097886ms step_avg:169.93ms\nstep:6472/10000 train_loss:3.8870 train_time:1098059ms step_avg:169.93ms\nstep:6473/10000 train_loss:3.8959 train_time:1098228ms step_avg:169.93ms\nstep:6474/10000 train_loss:3.8881 train_time:1098396ms step_avg:169.93ms\nstep:6475/10000 train_loss:3.7777 train_time:1098562ms step_avg:169.92ms\nstep:6476/10000 train_loss:3.9764 train_time:1098732ms step_avg:169.92ms\nstep:6477/10000 train_loss:3.8111 train_time:1098903ms step_avg:169.92ms\nstep:6478/10000 train_loss:3.8297 train_time:1099073ms step_avg:169.92ms\nstep:6479/10000 train_loss:3.8609 train_time:1099241ms step_avg:169.92ms\nstep:6480/10000 train_loss:3.9969 train_time:1099409ms step_avg:169.92ms\nstep:6481/10000 train_loss:3.8913 train_time:1099579ms step_avg:169.92ms\nstep:6482/10000 train_loss:3.9177 train_time:1099747ms step_avg:169.92ms\nstep:6483/10000 train_loss:3.8813 train_time:1099920ms step_avg:169.92ms\nstep:6484/10000 train_loss:3.8527 train_time:1100089ms step_avg:169.92ms\nstep:6485/10000 train_loss:3.7687 train_time:1100259ms step_avg:169.92ms\nstep:6486/10000 train_loss:3.9084 train_time:1100428ms step_avg:169.92ms\nstep:6487/10000 train_loss:3.7859 train_time:1100598ms step_avg:169.92ms\nstep:6488/10000 train_loss:4.1632 train_time:1100767ms step_avg:169.92ms\nstep:6489/10000 train_loss:4.0043 train_time:1100933ms step_avg:169.92ms\nstep:6490/10000 train_loss:3.7763 train_time:1101102ms step_avg:169.92ms\nstep:6491/10000 train_loss:3.8704 train_time:1101275ms step_avg:169.92ms\nstep:6492/10000 train_loss:3.7737 train_time:1101442ms step_avg:169.92ms\nstep:6493/10000 train_loss:3.7397 train_time:1101613ms step_avg:169.92ms\nstep:6494/10000 train_loss:4.0470 train_time:1101784ms step_avg:169.92ms\nstep:6495/10000 train_loss:3.9530 train_time:1101954ms step_avg:169.92ms\nstep:6496/10000 train_loss:4.1110 train_time:1102121ms step_avg:169.92ms\nstep:6497/10000 train_loss:3.8623 train_time:1102292ms step_avg:169.92ms\nstep:6498/10000 train_loss:3.9047 train_time:1102460ms step_avg:169.92ms\nstep:6499/10000 train_loss:3.9037 train_time:1102631ms step_avg:169.92ms\nstep:6500/10000 train_loss:3.8285 train_time:1102801ms step_avg:169.92ms\nstep:6501/10000 train_loss:3.7786 train_time:1102972ms step_avg:169.92ms\nstep:6502/10000 train_loss:3.7476 train_time:1103141ms step_avg:169.92ms\nstep:6503/10000 train_loss:3.9709 train_time:1103310ms step_avg:169.92ms\nstep:6504/10000 train_loss:3.9218 train_time:1103481ms step_avg:169.92ms\nstep:6505/10000 train_loss:3.7562 train_time:1103652ms step_avg:169.92ms\nstep:6506/10000 train_loss:3.8329 train_time:1103821ms step_avg:169.92ms\nstep:6507/10000 train_loss:3.8495 train_time:1103991ms step_avg:169.92ms\nstep:6508/10000 train_loss:3.9165 train_time:1104160ms step_avg:169.92ms\nstep:6509/10000 train_loss:3.8393 train_time:1104328ms step_avg:169.92ms\nstep:6510/10000 train_loss:3.8437 train_time:1104497ms step_avg:169.92ms\nstep:6511/10000 train_loss:3.8288 train_time:1104666ms step_avg:169.92ms\nstep:6512/10000 train_loss:3.9863 train_time:1104837ms step_avg:169.92ms\nstep:6513/10000 train_loss:3.5847 train_time:1105007ms step_avg:169.92ms\nstep:6514/10000 train_loss:3.8820 train_time:1105181ms step_avg:169.92ms\nstep:6515/10000 train_loss:3.8417 train_time:1105351ms step_avg:169.92ms\nstep:6516/10000 train_loss:3.8306 train_time:1105522ms step_avg:169.92ms\nstep:6517/10000 train_loss:3.9459 train_time:1105690ms step_avg:169.92ms\nstep:6518/10000 train_loss:3.8719 train_time:1105861ms step_avg:169.92ms\nstep:6519/10000 train_loss:3.9132 train_time:1106029ms step_avg:169.92ms\nstep:6520/10000 train_loss:3.8573 train_time:1106197ms step_avg:169.92ms\nstep:6521/10000 train_loss:3.9343 train_time:1106366ms step_avg:169.92ms\nstep:6522/10000 train_loss:3.8099 train_time:1106535ms step_avg:169.92ms\nstep:6523/10000 train_loss:3.9598 train_time:1106704ms step_avg:169.92ms\nstep:6524/10000 train_loss:3.8780 train_time:1106872ms step_avg:169.92ms\nstep:6525/10000 train_loss:3.7493 train_time:1107040ms step_avg:169.92ms\nstep:6526/10000 train_loss:3.9148 train_time:1107209ms step_avg:169.92ms\nstep:6527/10000 train_loss:3.7017 train_time:1107378ms step_avg:169.92ms\nstep:6528/10000 train_loss:3.7514 train_time:1107546ms step_avg:169.92ms\nstep:6529/10000 train_loss:3.8174 train_time:1107719ms step_avg:169.92ms\nstep:6530/10000 train_loss:3.8226 train_time:1107888ms step_avg:169.92ms\nstep:6531/10000 train_loss:3.8835 train_time:1108061ms step_avg:169.92ms\nstep:6532/10000 train_loss:3.8200 train_time:1108229ms step_avg:169.92ms\nstep:6533/10000 train_loss:3.9791 train_time:1108397ms step_avg:169.92ms\nstep:6534/10000 train_loss:3.8593 train_time:1108565ms step_avg:169.92ms\nstep:6535/10000 train_loss:3.8652 train_time:1108736ms step_avg:169.92ms\nstep:6536/10000 train_loss:3.8404 train_time:1108903ms step_avg:169.92ms\nstep:6537/10000 train_loss:4.0270 train_time:1109074ms step_avg:169.92ms\nstep:6538/10000 train_loss:3.7237 train_time:1109242ms step_avg:169.92ms\nstep:6539/10000 train_loss:3.8615 train_time:1109412ms step_avg:169.92ms\nstep:6540/10000 train_loss:3.8626 train_time:1109579ms step_avg:169.92ms\nstep:6541/10000 train_loss:3.8662 train_time:1109748ms step_avg:169.92ms\nstep:6542/10000 train_loss:3.7219 train_time:1109921ms step_avg:169.92ms\nstep:6543/10000 train_loss:3.9621 train_time:1110091ms step_avg:169.92ms\nstep:6544/10000 train_loss:3.9036 train_time:1110262ms step_avg:169.92ms\nstep:6545/10000 train_loss:3.7952 train_time:1110434ms step_avg:169.92ms\nstep:6546/10000 train_loss:3.8874 train_time:1110600ms step_avg:169.92ms\nstep:6547/10000 train_loss:4.0359 train_time:1110772ms step_avg:169.92ms\nstep:6548/10000 train_loss:4.1808 train_time:1110942ms step_avg:169.92ms\nstep:6549/10000 train_loss:3.9126 train_time:1111111ms step_avg:169.92ms\nstep:6550/10000 train_loss:3.8658 train_time:1111283ms step_avg:169.92ms\nstep:6551/10000 train_loss:4.0419 train_time:1111455ms step_avg:169.92ms\nstep:6552/10000 train_loss:3.8104 train_time:1111622ms step_avg:169.92ms\nstep:6553/10000 train_loss:3.7083 train_time:1111791ms step_avg:169.92ms\nstep:6554/10000 train_loss:3.8074 train_time:1111963ms step_avg:169.92ms\nstep:6555/10000 train_loss:3.9269 train_time:1112127ms step_avg:169.92ms\nstep:6556/10000 train_loss:3.7953 train_time:1112298ms step_avg:169.92ms\nstep:6557/10000 train_loss:4.0209 train_time:1112465ms step_avg:169.92ms\nstep:6558/10000 train_loss:3.9062 train_time:1112633ms step_avg:169.92ms\nstep:6559/10000 train_loss:3.9020 train_time:1112801ms step_avg:169.92ms\nstep:6560/10000 train_loss:3.8253 train_time:1112970ms step_avg:169.92ms\nstep:6561/10000 train_loss:3.8418 train_time:1113138ms step_avg:169.92ms\nstep:6562/10000 train_loss:3.9680 train_time:1113308ms step_avg:169.92ms\nstep:6563/10000 train_loss:3.7030 train_time:1113480ms step_avg:169.92ms\nstep:6564/10000 train_loss:3.9594 train_time:1113648ms step_avg:169.92ms\nstep:6565/10000 train_loss:4.0683 train_time:1113816ms step_avg:169.92ms\nstep:6566/10000 train_loss:3.9582 train_time:1113985ms step_avg:169.92ms\nstep:6567/10000 train_loss:3.9090 train_time:1114160ms step_avg:169.92ms\nstep:6568/10000 train_loss:3.8557 train_time:1114331ms step_avg:169.92ms\nstep:6569/10000 train_loss:3.7079 train_time:1114502ms step_avg:169.92ms\nstep:6570/10000 train_loss:4.1401 train_time:1114668ms step_avg:169.92ms\nstep:6571/10000 train_loss:4.0476 train_time:1114834ms step_avg:169.92ms\nstep:6572/10000 train_loss:3.9181 train_time:1115003ms step_avg:169.92ms\nstep:6573/10000 train_loss:3.7448 train_time:1115174ms step_avg:169.92ms\nstep:6574/10000 train_loss:3.9962 train_time:1115344ms step_avg:169.92ms\nstep:6575/10000 train_loss:3.8498 train_time:1115516ms step_avg:169.92ms\nstep:6576/10000 train_loss:3.7700 train_time:1115684ms step_avg:169.92ms\nstep:6577/10000 train_loss:3.7709 train_time:1115853ms step_avg:169.92ms\nstep:6578/10000 train_loss:3.8725 train_time:1116020ms step_avg:169.92ms\nstep:6579/10000 train_loss:3.8949 train_time:1116189ms step_avg:169.92ms\nstep:6580/10000 train_loss:3.8239 train_time:1116360ms step_avg:169.92ms\nstep:6581/10000 train_loss:3.8550 train_time:1116530ms step_avg:169.92ms\nstep:6582/10000 train_loss:3.9458 train_time:1116699ms step_avg:169.92ms\nstep:6583/10000 train_loss:3.9599 train_time:1116869ms step_avg:169.92ms\nstep:6584/10000 train_loss:3.8998 train_time:1117039ms step_avg:169.92ms\nstep:6585/10000 train_loss:3.8082 train_time:1117208ms step_avg:169.92ms\nstep:6586/10000 train_loss:3.8429 train_time:1117377ms step_avg:169.92ms\nstep:6587/10000 train_loss:3.8336 train_time:1117548ms step_avg:169.92ms\nstep:6588/10000 train_loss:3.7842 train_time:1117718ms step_avg:169.92ms\nstep:6589/10000 train_loss:3.9749 train_time:1117888ms step_avg:169.92ms\nstep:6590/10000 train_loss:3.8716 train_time:1118057ms step_avg:169.92ms\nstep:6591/10000 train_loss:4.0233 train_time:1118225ms step_avg:169.92ms\nstep:6592/10000 train_loss:3.8804 train_time:1118394ms step_avg:169.92ms\nstep:6593/10000 train_loss:3.8177 train_time:1118562ms step_avg:169.92ms\nstep:6594/10000 train_loss:3.8756 train_time:1118733ms step_avg:169.92ms\nstep:6595/10000 train_loss:3.8333 train_time:1118902ms step_avg:169.92ms\nstep:6596/10000 train_loss:3.9434 train_time:1119071ms step_avg:169.92ms\nstep:6597/10000 train_loss:3.8336 train_time:1119237ms step_avg:169.92ms\nstep:6598/10000 train_loss:3.6816 train_time:1119406ms step_avg:169.92ms\nstep:6599/10000 train_loss:3.8765 train_time:1119574ms step_avg:169.92ms\nstep:6600/10000 train_loss:3.8332 train_time:1119743ms step_avg:169.92ms\nstep:6601/10000 train_loss:3.7617 train_time:1119918ms step_avg:169.92ms\nstep:6602/10000 train_loss:3.9255 train_time:1120089ms step_avg:169.92ms\nstep:6603/10000 train_loss:3.8534 train_time:1120259ms step_avg:169.92ms\nstep:6604/10000 train_loss:3.9020 train_time:1120422ms step_avg:169.92ms\nstep:6605/10000 train_loss:3.8566 train_time:1120590ms step_avg:169.92ms\nstep:6606/10000 train_loss:3.7709 train_time:1120760ms step_avg:169.92ms\nstep:6607/10000 train_loss:3.8552 train_time:1120931ms step_avg:169.92ms\nstep:6608/10000 train_loss:3.8909 train_time:1121100ms step_avg:169.92ms\nstep:6609/10000 train_loss:4.0587 train_time:1121272ms step_avg:169.92ms\nstep:6610/10000 train_loss:3.8501 train_time:1121439ms step_avg:169.91ms\nstep:6611/10000 train_loss:3.9195 train_time:1121607ms step_avg:169.91ms\nstep:6612/10000 train_loss:3.8504 train_time:1121774ms step_avg:169.91ms\nstep:6613/10000 train_loss:3.9422 train_time:1121943ms step_avg:169.91ms\nstep:6614/10000 train_loss:3.8286 train_time:1122113ms step_avg:169.91ms\nstep:6615/10000 train_loss:3.8464 train_time:1122282ms step_avg:169.91ms\nstep:6616/10000 train_loss:3.9569 train_time:1122453ms step_avg:169.91ms\nstep:6617/10000 train_loss:3.7159 train_time:1122621ms step_avg:169.91ms\nstep:6618/10000 train_loss:3.8857 train_time:1122791ms step_avg:169.91ms\nstep:6619/10000 train_loss:3.9655 train_time:1122959ms step_avg:169.91ms\nstep:6620/10000 train_loss:3.7622 train_time:1123129ms step_avg:169.91ms\nstep:6621/10000 train_loss:3.8246 train_time:1123300ms step_avg:169.91ms\nstep:6622/10000 train_loss:3.8565 train_time:1123472ms step_avg:169.91ms\nstep:6623/10000 train_loss:3.8734 train_time:1123641ms step_avg:169.91ms\nstep:6624/10000 train_loss:3.6205 train_time:1123810ms step_avg:169.91ms\nstep:6625/10000 train_loss:3.8691 train_time:1123982ms step_avg:169.91ms\nstep:6626/10000 train_loss:3.9830 train_time:1124150ms step_avg:169.91ms\nstep:6627/10000 train_loss:3.8040 train_time:1124320ms step_avg:169.91ms\nstep:6628/10000 train_loss:3.8971 train_time:1124490ms step_avg:169.91ms\nstep:6629/10000 train_loss:3.8139 train_time:1124660ms step_avg:169.91ms\nstep:6630/10000 train_loss:3.9364 train_time:1124829ms step_avg:169.91ms\nstep:6631/10000 train_loss:3.8355 train_time:1124998ms step_avg:169.91ms\nstep:6632/10000 train_loss:3.9180 train_time:1125168ms step_avg:169.91ms\nstep:6633/10000 train_loss:3.8917 train_time:1125338ms step_avg:169.91ms\nstep:6634/10000 train_loss:3.7653 train_time:1125513ms step_avg:169.91ms\nstep:6635/10000 train_loss:4.0298 train_time:1125683ms step_avg:169.91ms\nstep:6636/10000 train_loss:3.8403 train_time:1125847ms step_avg:169.91ms\nstep:6637/10000 train_loss:3.8405 train_time:1126016ms step_avg:169.91ms\nstep:6638/10000 train_loss:3.9149 train_time:1126184ms step_avg:169.91ms\nstep:6639/10000 train_loss:3.9009 train_time:1126353ms step_avg:169.91ms\nstep:6640/10000 train_loss:4.0805 train_time:1126523ms step_avg:169.91ms\nstep:6641/10000 train_loss:3.9619 train_time:1126693ms step_avg:169.91ms\nstep:6642/10000 train_loss:3.8281 train_time:1126861ms step_avg:169.91ms\nstep:6643/10000 train_loss:3.9492 train_time:1127032ms step_avg:169.91ms\nstep:6644/10000 train_loss:3.9016 train_time:1127203ms step_avg:169.91ms\nstep:6645/10000 train_loss:3.9224 train_time:1127372ms step_avg:169.91ms\nstep:6646/10000 train_loss:3.8073 train_time:1127541ms step_avg:169.91ms\nstep:6647/10000 train_loss:3.8304 train_time:1127709ms step_avg:169.91ms\nstep:6648/10000 train_loss:3.9152 train_time:1127882ms step_avg:169.91ms\nstep:6649/10000 train_loss:3.9903 train_time:1128050ms step_avg:169.91ms\nstep:6650/10000 train_loss:3.8677 train_time:1128222ms step_avg:169.91ms\nstep:6651/10000 train_loss:3.9564 train_time:1128393ms step_avg:169.91ms\nstep:6652/10000 train_loss:3.9106 train_time:1128560ms step_avg:169.91ms\nstep:6653/10000 train_loss:3.9007 train_time:1128727ms step_avg:169.91ms\nstep:6654/10000 train_loss:3.8917 train_time:1128899ms step_avg:169.91ms\nstep:6655/10000 train_loss:4.0514 train_time:1129069ms step_avg:169.91ms\nstep:6656/10000 train_loss:3.9342 train_time:1129238ms step_avg:169.91ms\nstep:6657/10000 train_loss:3.8315 train_time:1129408ms step_avg:169.91ms\nstep:6658/10000 train_loss:3.8262 train_time:1129583ms step_avg:169.91ms\nstep:6659/10000 train_loss:3.9100 train_time:1129750ms step_avg:169.91ms\nstep:6660/10000 train_loss:3.7984 train_time:1129921ms step_avg:169.91ms\nstep:6661/10000 train_loss:3.9892 train_time:1130090ms step_avg:169.91ms\nstep:6662/10000 train_loss:3.9009 train_time:1130262ms step_avg:169.91ms\nstep:6663/10000 train_loss:3.9986 train_time:1130432ms step_avg:169.91ms\nstep:6664/10000 train_loss:3.8651 train_time:1130602ms step_avg:169.91ms\nstep:6665/10000 train_loss:3.7385 train_time:1130770ms step_avg:169.91ms\nstep:6666/10000 train_loss:3.9233 train_time:1130941ms step_avg:169.91ms\nstep:6667/10000 train_loss:3.9716 train_time:1131109ms step_avg:169.91ms\nstep:6668/10000 train_loss:3.7645 train_time:1131280ms step_avg:169.91ms\nstep:6669/10000 train_loss:3.8654 train_time:1131452ms step_avg:169.91ms\nstep:6670/10000 train_loss:3.9334 train_time:1131622ms step_avg:169.91ms\nstep:6671/10000 train_loss:3.9058 train_time:1131790ms step_avg:169.91ms\nstep:6672/10000 train_loss:3.8425 train_time:1131960ms step_avg:169.91ms\nstep:6673/10000 train_loss:3.8333 train_time:1132128ms step_avg:169.91ms\nstep:6674/10000 train_loss:3.9384 train_time:1132296ms step_avg:169.91ms\nstep:6675/10000 train_loss:3.8670 train_time:1132466ms step_avg:169.91ms\nstep:6676/10000 train_loss:3.8917 train_time:1132637ms step_avg:169.91ms\nstep:6677/10000 train_loss:3.8544 train_time:1132805ms step_avg:169.91ms\nstep:6678/10000 train_loss:4.0371 train_time:1132974ms step_avg:169.91ms\nstep:6679/10000 train_loss:3.9299 train_time:1133142ms step_avg:169.91ms\nstep:6680/10000 train_loss:3.6974 train_time:1133312ms step_avg:169.91ms\nstep:6681/10000 train_loss:3.7635 train_time:1133481ms step_avg:169.91ms\nstep:6682/10000 train_loss:3.7864 train_time:1133650ms step_avg:169.91ms\nstep:6683/10000 train_loss:3.7784 train_time:1133820ms step_avg:169.91ms\nstep:6684/10000 train_loss:3.9170 train_time:1133990ms step_avg:169.91ms\nstep:6685/10000 train_loss:3.9317 train_time:1134162ms step_avg:169.91ms\nstep:6686/10000 train_loss:3.8394 train_time:1134333ms step_avg:169.91ms\nstep:6687/10000 train_loss:3.9424 train_time:1134505ms step_avg:169.91ms\nstep:6688/10000 train_loss:3.7255 train_time:1134672ms step_avg:169.91ms\nstep:6689/10000 train_loss:3.8244 train_time:1134840ms step_avg:169.91ms\nstep:6690/10000 train_loss:3.8048 train_time:1135009ms step_avg:169.91ms\nstep:6691/10000 train_loss:3.9192 train_time:1135177ms step_avg:169.91ms\nstep:6692/10000 train_loss:4.0092 train_time:1135346ms step_avg:169.91ms\nstep:6693/10000 train_loss:3.8539 train_time:1135516ms step_avg:169.91ms\nstep:6694/10000 train_loss:3.7803 train_time:1135686ms step_avg:169.91ms\nstep:6695/10000 train_loss:3.7603 train_time:1135856ms step_avg:169.91ms\nstep:6696/10000 train_loss:3.8867 train_time:1136023ms step_avg:169.91ms\nstep:6697/10000 train_loss:3.8415 train_time:1136193ms step_avg:169.91ms\nstep:6698/10000 train_loss:3.9630 train_time:1136362ms step_avg:169.91ms\nstep:6699/10000 train_loss:3.9925 train_time:1136533ms step_avg:169.91ms\nstep:6700/10000 train_loss:3.8630 train_time:1136702ms step_avg:169.91ms\nstep:6701/10000 train_loss:4.0365 train_time:1136875ms step_avg:169.91ms\nstep:6702/10000 train_loss:4.1404 train_time:1137043ms step_avg:169.91ms\nstep:6703/10000 train_loss:3.7177 train_time:1137214ms step_avg:169.91ms\nstep:6704/10000 train_loss:3.7484 train_time:1137382ms step_avg:169.91ms\nstep:6705/10000 train_loss:3.8791 train_time:1137555ms step_avg:169.91ms\nstep:6706/10000 train_loss:3.8602 train_time:1137724ms step_avg:169.91ms\nstep:6707/10000 train_loss:3.9401 train_time:1137894ms step_avg:169.91ms\nstep:6708/10000 train_loss:3.8242 train_time:1138061ms step_avg:169.91ms\nstep:6709/10000 train_loss:3.7686 train_time:1138232ms step_avg:169.91ms\nstep:6710/10000 train_loss:3.8259 train_time:1138400ms step_avg:169.91ms\nstep:6711/10000 train_loss:3.8897 train_time:1138568ms step_avg:169.91ms\nstep:6712/10000 train_loss:4.1262 train_time:1138739ms step_avg:169.91ms\nstep:6713/10000 train_loss:3.9332 train_time:1138908ms step_avg:169.91ms\nstep:6714/10000 train_loss:3.7415 train_time:1139076ms step_avg:169.91ms\nstep:6715/10000 train_loss:3.8231 train_time:1139247ms step_avg:169.91ms\nstep:6716/10000 train_loss:3.8929 train_time:1139419ms step_avg:169.91ms\nstep:6717/10000 train_loss:3.8277 train_time:1139588ms step_avg:169.91ms\nstep:6718/10000 train_loss:3.9303 train_time:1139759ms step_avg:169.91ms\nstep:6719/10000 train_loss:3.8002 train_time:1139926ms step_avg:169.91ms\nstep:6720/10000 train_loss:3.8443 train_time:1140094ms step_avg:169.91ms\nstep:6721/10000 train_loss:3.9800 train_time:1140263ms step_avg:169.91ms\nstep:6722/10000 train_loss:3.8317 train_time:1140433ms step_avg:169.91ms\nstep:6723/10000 train_loss:4.0726 train_time:1140602ms step_avg:169.91ms\nstep:6724/10000 train_loss:3.9750 train_time:1140773ms step_avg:169.91ms\nstep:6725/10000 train_loss:4.0340 train_time:1140944ms step_avg:169.91ms\nstep:6726/10000 train_loss:3.8534 train_time:1141113ms step_avg:169.91ms\nstep:6727/10000 train_loss:3.8924 train_time:1141282ms step_avg:169.91ms\nstep:6728/10000 train_loss:3.9244 train_time:1141453ms step_avg:169.91ms\nstep:6729/10000 train_loss:3.9799 train_time:1141623ms step_avg:169.91ms\nstep:6730/10000 train_loss:3.8452 train_time:1141792ms step_avg:169.91ms\nstep:6731/10000 train_loss:3.7794 train_time:1141961ms step_avg:169.91ms\nstep:6732/10000 train_loss:3.8129 train_time:1142129ms step_avg:169.91ms\nstep:6733/10000 train_loss:3.8372 train_time:1142297ms step_avg:169.91ms\nstep:6734/10000 train_loss:3.6817 train_time:1142465ms step_avg:169.91ms\nstep:6735/10000 train_loss:3.7987 train_time:1142635ms step_avg:169.91ms\nstep:6736/10000 train_loss:3.9073 train_time:1142806ms step_avg:169.91ms\nstep:6737/10000 train_loss:3.8928 train_time:1142975ms step_avg:169.91ms\nstep:6738/10000 train_loss:3.8343 train_time:1143145ms step_avg:169.91ms\nstep:6739/10000 train_loss:3.8101 train_time:1143316ms step_avg:169.91ms\nstep:6740/10000 train_loss:3.7566 train_time:1143484ms step_avg:169.91ms\nstep:6741/10000 train_loss:3.8436 train_time:1143654ms step_avg:169.91ms\nstep:6742/10000 train_loss:3.8752 train_time:1143822ms step_avg:169.91ms\nstep:6743/10000 train_loss:3.8172 train_time:1143993ms step_avg:169.91ms\nstep:6744/10000 train_loss:3.9001 train_time:1144162ms step_avg:169.91ms\nstep:6745/10000 train_loss:3.8303 train_time:1144332ms step_avg:169.91ms\nstep:6746/10000 train_loss:4.0050 train_time:1144500ms step_avg:169.91ms\nstep:6747/10000 train_loss:3.7190 train_time:1144670ms step_avg:169.91ms\nstep:6748/10000 train_loss:3.7987 train_time:1144841ms step_avg:169.91ms\nstep:6749/10000 train_loss:3.8376 train_time:1145012ms step_avg:169.91ms\nstep:6750/10000 train_loss:3.9223 train_time:1145178ms step_avg:169.91ms\nstep:6751/10000 train_loss:3.9244 train_time:1145348ms step_avg:169.91ms\nstep:6752/10000 train_loss:3.6872 train_time:1145520ms step_avg:169.91ms\nstep:6753/10000 train_loss:3.9063 train_time:1145689ms step_avg:169.91ms\nstep:6754/10000 train_loss:3.8430 train_time:1145858ms step_avg:169.91ms\nstep:6755/10000 train_loss:3.8212 train_time:1146026ms step_avg:169.91ms\nstep:6756/10000 train_loss:4.1082 train_time:1146194ms step_avg:169.91ms\nstep:6757/10000 train_loss:3.8858 train_time:1146364ms step_avg:169.91ms\nstep:6758/10000 train_loss:3.9129 train_time:1146536ms step_avg:169.91ms\nstep:6759/10000 train_loss:3.7756 train_time:1146704ms step_avg:169.91ms\nstep:6760/10000 train_loss:3.8989 train_time:1146874ms step_avg:169.91ms\nstep:6761/10000 train_loss:3.9215 train_time:1147042ms step_avg:169.91ms\nstep:6762/10000 train_loss:3.8040 train_time:1147213ms step_avg:169.91ms\nstep:6763/10000 train_loss:3.8021 train_time:1147383ms step_avg:169.91ms\nstep:6764/10000 train_loss:3.8970 train_time:1147555ms step_avg:169.91ms\nstep:6765/10000 train_loss:3.8381 train_time:1147723ms step_avg:169.91ms\nstep:6766/10000 train_loss:3.9157 train_time:1147895ms step_avg:169.91ms\nstep:6767/10000 train_loss:3.8740 train_time:1148064ms step_avg:169.91ms\nstep:6768/10000 train_loss:3.8305 train_time:1148234ms step_avg:169.91ms\nstep:6769/10000 train_loss:4.0002 train_time:1148403ms step_avg:169.91ms\nstep:6770/10000 train_loss:3.7979 train_time:1148574ms step_avg:169.91ms\nstep:6771/10000 train_loss:3.8578 train_time:1148743ms step_avg:169.91ms\nstep:6772/10000 train_loss:3.7615 train_time:1148914ms step_avg:169.91ms\nstep:6773/10000 train_loss:3.8457 train_time:1149083ms step_avg:169.91ms\nstep:6774/10000 train_loss:3.7574 train_time:1149253ms step_avg:169.91ms\nstep:6775/10000 train_loss:3.8367 train_time:1149425ms step_avg:169.91ms\nstep:6776/10000 train_loss:3.8806 train_time:1149591ms step_avg:169.91ms\nstep:6777/10000 train_loss:3.6768 train_time:1149761ms step_avg:169.91ms\nstep:6778/10000 train_loss:3.8356 train_time:1149930ms step_avg:169.91ms\nstep:6779/10000 train_loss:3.8188 train_time:1150099ms step_avg:169.91ms\nstep:6780/10000 train_loss:3.8329 train_time:1150268ms step_avg:169.91ms\nstep:6781/10000 train_loss:3.8149 train_time:1150437ms step_avg:169.91ms\nstep:6782/10000 train_loss:3.8340 train_time:1150606ms step_avg:169.91ms\nstep:6783/10000 train_loss:3.8789 train_time:1150775ms step_avg:169.91ms\nstep:6784/10000 train_loss:3.8914 train_time:1150944ms step_avg:169.91ms\nstep:6785/10000 train_loss:3.8359 train_time:1151114ms step_avg:169.91ms\nstep:6786/10000 train_loss:3.7718 train_time:1151282ms step_avg:169.91ms\nstep:6787/10000 train_loss:3.9691 train_time:1151454ms step_avg:169.91ms\nstep:6788/10000 train_loss:3.8891 train_time:1151623ms step_avg:169.91ms\nstep:6789/10000 train_loss:3.8115 train_time:1151792ms step_avg:169.91ms\nstep:6790/10000 train_loss:4.2129 train_time:1151961ms step_avg:169.91ms\nstep:6791/10000 train_loss:3.8814 train_time:1152130ms step_avg:169.91ms\nstep:6792/10000 train_loss:3.9377 train_time:1152300ms step_avg:169.91ms\nstep:6793/10000 train_loss:3.8786 train_time:1152472ms step_avg:169.91ms\nstep:6794/10000 train_loss:3.9208 train_time:1152640ms step_avg:169.91ms\nstep:6795/10000 train_loss:3.8481 train_time:1152809ms step_avg:169.91ms\nstep:6796/10000 train_loss:3.8818 train_time:1152979ms step_avg:169.91ms\nstep:6797/10000 train_loss:3.8770 train_time:1153149ms step_avg:169.91ms\nstep:6798/10000 train_loss:3.9022 train_time:1153321ms step_avg:169.91ms\nstep:6799/10000 train_loss:3.7650 train_time:1153490ms step_avg:169.91ms\nstep:6800/10000 train_loss:3.8248 train_time:1153657ms step_avg:169.91ms\nstep:6801/10000 train_loss:3.8639 train_time:1153826ms step_avg:169.91ms\nstep:6802/10000 train_loss:3.8110 train_time:1153994ms step_avg:169.90ms\nstep:6803/10000 train_loss:3.9889 train_time:1154162ms step_avg:169.90ms\nstep:6804/10000 train_loss:3.7502 train_time:1154331ms step_avg:169.90ms\nstep:6805/10000 train_loss:3.9185 train_time:1154501ms step_avg:169.90ms\nstep:6806/10000 train_loss:3.7826 train_time:1154669ms step_avg:169.90ms\nstep:6807/10000 train_loss:3.8200 train_time:1154838ms step_avg:169.90ms\nstep:6808/10000 train_loss:3.7828 train_time:1155007ms step_avg:169.90ms\nstep:6809/10000 train_loss:3.8294 train_time:1155180ms step_avg:169.90ms\nstep:6810/10000 train_loss:3.8490 train_time:1155349ms step_avg:169.90ms\nstep:6811/10000 train_loss:3.8766 train_time:1155519ms step_avg:169.90ms\nstep:6812/10000 train_loss:3.8333 train_time:1155691ms step_avg:169.90ms\nstep:6813/10000 train_loss:3.8635 train_time:1155857ms step_avg:169.90ms\nstep:6814/10000 train_loss:3.7949 train_time:1156025ms step_avg:169.90ms\nstep:6815/10000 train_loss:3.8295 train_time:1156195ms step_avg:169.90ms\nstep:6816/10000 train_loss:3.8447 train_time:1156364ms step_avg:169.90ms\nstep:6817/10000 train_loss:4.0710 train_time:1156535ms step_avg:169.90ms\nstep:6818/10000 train_loss:3.8658 train_time:1156702ms step_avg:169.90ms\nstep:6819/10000 train_loss:3.9011 train_time:1156872ms step_avg:169.90ms\nstep:6820/10000 train_loss:3.8620 train_time:1157041ms step_avg:169.90ms\nstep:6821/10000 train_loss:3.9272 train_time:1157211ms step_avg:169.90ms\nstep:6822/10000 train_loss:3.8021 train_time:1157381ms step_avg:169.90ms\nstep:6823/10000 train_loss:3.9174 train_time:1157551ms step_avg:169.90ms\nstep:6824/10000 train_loss:3.9016 train_time:1157720ms step_avg:169.90ms\nstep:6825/10000 train_loss:3.8410 train_time:1157890ms step_avg:169.90ms\nstep:6826/10000 train_loss:4.0264 train_time:1158059ms step_avg:169.90ms\nstep:6827/10000 train_loss:3.7993 train_time:1158229ms step_avg:169.90ms\nstep:6828/10000 train_loss:3.8075 train_time:1158400ms step_avg:169.90ms\nstep:6829/10000 train_loss:3.8511 train_time:1158570ms step_avg:169.90ms\nstep:6830/10000 train_loss:3.7744 train_time:1158738ms step_avg:169.90ms\nstep:6831/10000 train_loss:3.7808 train_time:1158907ms step_avg:169.90ms\nstep:6832/10000 train_loss:3.9513 train_time:1159079ms step_avg:169.90ms\nstep:6833/10000 train_loss:3.9166 train_time:1159247ms step_avg:169.90ms\nstep:6834/10000 train_loss:3.9896 train_time:1159420ms step_avg:169.90ms\nstep:6835/10000 train_loss:3.8927 train_time:1159589ms step_avg:169.90ms\nstep:6836/10000 train_loss:3.7177 train_time:1159756ms step_avg:169.90ms\nstep:6837/10000 train_loss:3.9064 train_time:1159928ms step_avg:169.90ms\nstep:6838/10000 train_loss:3.8568 train_time:1160098ms step_avg:169.90ms\nstep:6839/10000 train_loss:3.6274 train_time:1160266ms step_avg:169.90ms\nstep:6840/10000 train_loss:3.8476 train_time:1160434ms step_avg:169.90ms\nstep:6841/10000 train_loss:3.9122 train_time:1160601ms step_avg:169.90ms\nstep:6842/10000 train_loss:3.5834 train_time:1160769ms step_avg:169.90ms\nstep:6843/10000 train_loss:3.9544 train_time:1160938ms step_avg:169.90ms\nstep:6844/10000 train_loss:3.7875 train_time:1161107ms step_avg:169.90ms\nstep:6845/10000 train_loss:3.6866 train_time:1161277ms step_avg:169.90ms\nstep:6846/10000 train_loss:3.9576 train_time:1161447ms step_avg:169.90ms\nstep:6847/10000 train_loss:3.8960 train_time:1161620ms step_avg:169.90ms\nstep:6848/10000 train_loss:3.7857 train_time:1161790ms step_avg:169.90ms\nstep:6849/10000 train_loss:3.7486 train_time:1161960ms step_avg:169.90ms\nstep:6850/10000 train_loss:3.8848 train_time:1162126ms step_avg:169.90ms\nstep:6851/10000 train_loss:3.9350 train_time:1162296ms step_avg:169.90ms\nstep:6852/10000 train_loss:4.2785 train_time:1162464ms step_avg:169.90ms\nstep:6853/10000 train_loss:3.9098 train_time:1162638ms step_avg:169.90ms\nstep:6854/10000 train_loss:3.8856 train_time:1162805ms step_avg:169.90ms\nstep:6855/10000 train_loss:4.2439 train_time:1162975ms step_avg:169.90ms\nstep:6856/10000 train_loss:3.8533 train_time:1163143ms step_avg:169.90ms\nstep:6857/10000 train_loss:3.7117 train_time:1163314ms step_avg:169.90ms\nstep:6858/10000 train_loss:3.8543 train_time:1163702ms step_avg:169.93ms\nstep:6859/10000 train_loss:3.8499 train_time:1163867ms step_avg:169.93ms\nstep:6860/10000 train_loss:3.8499 train_time:1164037ms step_avg:169.93ms\nstep:6861/10000 train_loss:3.6932 train_time:1164204ms step_avg:169.93ms\nstep:6862/10000 train_loss:4.0907 train_time:1164372ms step_avg:169.93ms\nstep:6863/10000 train_loss:3.7698 train_time:1164542ms step_avg:169.93ms\nstep:6864/10000 train_loss:3.9014 train_time:1164721ms step_avg:169.93ms\nstep:6865/10000 train_loss:3.8086 train_time:1164892ms step_avg:169.93ms\nstep:6866/10000 train_loss:3.8582 train_time:1165060ms step_avg:169.93ms\nstep:6867/10000 train_loss:3.9691 train_time:1165229ms step_avg:169.93ms\nstep:6868/10000 train_loss:3.9152 train_time:1165398ms step_avg:169.93ms\nstep:6869/10000 train_loss:3.8342 train_time:1165567ms step_avg:169.93ms\nstep:6870/10000 train_loss:3.8703 train_time:1165741ms step_avg:169.93ms\nstep:6871/10000 train_loss:3.8721 train_time:1165911ms step_avg:169.93ms\nstep:6872/10000 train_loss:3.8782 train_time:1166080ms step_avg:169.93ms\nstep:6873/10000 train_loss:3.8090 train_time:1166249ms step_avg:169.93ms\nstep:6874/10000 train_loss:3.9336 train_time:1166420ms step_avg:169.93ms\nstep:6875/10000 train_loss:4.0715 train_time:1166589ms step_avg:169.93ms\nstep:6876/10000 train_loss:3.8133 train_time:1166758ms step_avg:169.93ms\nstep:6877/10000 train_loss:4.0270 train_time:1166926ms step_avg:169.93ms\nstep:6878/10000 train_loss:4.0059 train_time:1167094ms step_avg:169.93ms\nstep:6879/10000 train_loss:3.8117 train_time:1167263ms step_avg:169.93ms\nstep:6880/10000 train_loss:3.6973 train_time:1167433ms step_avg:169.93ms\nstep:6881/10000 train_loss:3.8313 train_time:1167604ms step_avg:169.93ms\nstep:6882/10000 train_loss:4.0179 train_time:1167773ms step_avg:169.93ms\nstep:6883/10000 train_loss:3.6982 train_time:1167941ms step_avg:169.93ms\nstep:6884/10000 train_loss:3.8044 train_time:1168110ms step_avg:169.93ms\nstep:6885/10000 train_loss:3.8861 train_time:1168278ms step_avg:169.93ms\nstep:6886/10000 train_loss:3.8704 train_time:1168449ms step_avg:169.93ms\nstep:6887/10000 train_loss:3.9222 train_time:1168619ms step_avg:169.93ms\nstep:6888/10000 train_loss:3.8418 train_time:1168789ms step_avg:169.93ms\nstep:6889/10000 train_loss:3.8482 train_time:1168960ms step_avg:169.93ms\nstep:6890/10000 train_loss:3.8473 train_time:1169128ms step_avg:169.93ms\nstep:6891/10000 train_loss:3.8423 train_time:1169295ms step_avg:169.93ms\nstep:6892/10000 train_loss:3.7841 train_time:1169462ms step_avg:169.93ms\nstep:6893/10000 train_loss:3.9963 train_time:1169632ms step_avg:169.93ms\nstep:6894/10000 train_loss:3.8343 train_time:1169802ms step_avg:169.93ms\nstep:6895/10000 train_loss:3.7936 train_time:1169973ms step_avg:169.93ms\nstep:6896/10000 train_loss:3.9260 train_time:1170142ms step_avg:169.93ms\nstep:6897/10000 train_loss:3.9098 train_time:1170313ms step_avg:169.93ms\nstep:6898/10000 train_loss:3.4364 train_time:1170480ms step_avg:169.93ms\nstep:6899/10000 train_loss:3.8022 train_time:1170650ms step_avg:169.93ms\nstep:6900/10000 train_loss:3.7663 train_time:1170826ms step_avg:169.93ms\nstep:6901/10000 train_loss:3.8323 train_time:1170993ms step_avg:169.93ms\nstep:6902/10000 train_loss:3.7597 train_time:1171161ms step_avg:169.93ms\nstep:6903/10000 train_loss:3.9524 train_time:1171333ms step_avg:169.93ms\nstep:6904/10000 train_loss:3.8789 train_time:1171502ms step_avg:169.93ms\nstep:6905/10000 train_loss:3.9310 train_time:1171679ms step_avg:169.93ms\nstep:6906/10000 train_loss:3.8893 train_time:1171849ms step_avg:169.93ms\nstep:6907/10000 train_loss:3.9251 train_time:1172016ms step_avg:169.93ms\nstep:6908/10000 train_loss:3.9633 train_time:1172180ms step_avg:169.93ms\nstep:6909/10000 train_loss:4.0022 train_time:1172349ms step_avg:169.93ms\nstep:6910/10000 train_loss:3.8303 train_time:1172519ms step_avg:169.93ms\nstep:6911/10000 train_loss:3.7708 train_time:1172686ms step_avg:169.93ms\nstep:6912/10000 train_loss:3.8201 train_time:1172857ms step_avg:169.93ms\nstep:6913/10000 train_loss:3.7557 train_time:1173027ms step_avg:169.93ms\nstep:6914/10000 train_loss:3.9030 train_time:1173196ms step_avg:169.93ms\nstep:6915/10000 train_loss:4.0550 train_time:1173363ms step_avg:169.93ms\nstep:6916/10000 train_loss:4.0615 train_time:1173532ms step_avg:169.93ms\nstep:6917/10000 train_loss:3.8717 train_time:1173701ms step_avg:169.93ms\nstep:6918/10000 train_loss:3.8440 train_time:1173871ms step_avg:169.93ms\nstep:6919/10000 train_loss:3.7771 train_time:1174040ms step_avg:169.93ms\nstep:6920/10000 train_loss:3.8431 train_time:1174211ms step_avg:169.93ms\nstep:6921/10000 train_loss:3.8982 train_time:1174382ms step_avg:169.93ms\nstep:6922/10000 train_loss:3.8223 train_time:1174551ms step_avg:169.93ms\nstep:6923/10000 train_loss:3.8293 train_time:1174719ms step_avg:169.93ms\nstep:6924/10000 train_loss:3.8901 train_time:1174888ms step_avg:169.93ms\nstep:6925/10000 train_loss:3.9411 train_time:1175059ms step_avg:169.93ms\nstep:6926/10000 train_loss:3.8308 train_time:1175229ms step_avg:169.93ms\nstep:6927/10000 train_loss:3.7933 train_time:1175400ms step_avg:169.93ms\nstep:6928/10000 train_loss:3.8748 train_time:1175570ms step_avg:169.93ms\nstep:6929/10000 train_loss:3.8264 train_time:1175740ms step_avg:169.93ms\nstep:6930/10000 train_loss:4.0118 train_time:1175907ms step_avg:169.93ms\nstep:6931/10000 train_loss:3.9130 train_time:1176077ms step_avg:169.93ms\nstep:6932/10000 train_loss:3.9072 train_time:1176246ms step_avg:169.93ms\nstep:6933/10000 train_loss:4.0828 train_time:1176417ms step_avg:169.93ms\nstep:6934/10000 train_loss:3.7859 train_time:1176584ms step_avg:169.93ms\nstep:6935/10000 train_loss:3.7716 train_time:1176754ms step_avg:169.93ms\nstep:6936/10000 train_loss:4.2806 train_time:1176921ms step_avg:169.93ms\nstep:6937/10000 train_loss:3.8061 train_time:1177091ms step_avg:169.93ms\nstep:6938/10000 train_loss:3.8486 train_time:1177260ms step_avg:169.93ms\nstep:6939/10000 train_loss:4.0893 train_time:1177429ms step_avg:169.93ms\nstep:6940/10000 train_loss:3.8543 train_time:1177601ms step_avg:169.93ms\nstep:6941/10000 train_loss:3.8891 train_time:1177769ms step_avg:169.93ms\nstep:6942/10000 train_loss:3.8416 train_time:1177938ms step_avg:169.93ms\nstep:6943/10000 train_loss:3.8166 train_time:1178105ms step_avg:169.93ms\nstep:6944/10000 train_loss:3.9782 train_time:1178273ms step_avg:169.93ms\nstep:6945/10000 train_loss:3.8413 train_time:1178442ms step_avg:169.93ms\nstep:6946/10000 train_loss:3.8112 train_time:1178611ms step_avg:169.93ms\nstep:6947/10000 train_loss:3.9752 train_time:1178781ms step_avg:169.93ms\nstep:6948/10000 train_loss:3.7630 train_time:1178951ms step_avg:169.93ms\nstep:6949/10000 train_loss:3.9410 train_time:1179123ms step_avg:169.93ms\nstep:6950/10000 train_loss:3.7797 train_time:1179293ms step_avg:169.93ms\nstep:6951/10000 train_loss:3.8381 train_time:1179462ms step_avg:169.93ms\nstep:6952/10000 train_loss:3.7954 train_time:1179633ms step_avg:169.93ms\nstep:6953/10000 train_loss:3.7733 train_time:1179802ms step_avg:169.93ms\nstep:6954/10000 train_loss:3.9562 train_time:1179973ms step_avg:169.93ms\nstep:6955/10000 train_loss:4.7234 train_time:1180141ms step_avg:169.93ms\nstep:6956/10000 train_loss:3.8266 train_time:1180311ms step_avg:169.93ms\nstep:6957/10000 train_loss:3.8781 train_time:1180481ms step_avg:169.93ms\nstep:6958/10000 train_loss:3.8459 train_time:1180654ms step_avg:169.93ms\nstep:6959/10000 train_loss:3.8217 train_time:1180822ms step_avg:169.93ms\nstep:6960/10000 train_loss:3.8092 train_time:1180991ms step_avg:169.93ms\nstep:6961/10000 train_loss:4.1241 train_time:1181158ms step_avg:169.93ms\nstep:6962/10000 train_loss:3.9664 train_time:1181327ms step_avg:169.93ms\nstep:6963/10000 train_loss:3.7962 train_time:1181497ms step_avg:169.93ms\nstep:6964/10000 train_loss:3.9813 train_time:1181665ms step_avg:169.93ms\nstep:6965/10000 train_loss:4.0969 train_time:1181833ms step_avg:169.93ms\nstep:6966/10000 train_loss:3.8597 train_time:1182000ms step_avg:169.93ms\nstep:6967/10000 train_loss:3.8534 train_time:1182170ms step_avg:169.93ms\nstep:6968/10000 train_loss:3.8224 train_time:1182338ms step_avg:169.93ms\nstep:6969/10000 train_loss:3.6582 train_time:1182509ms step_avg:169.93ms\nstep:6970/10000 train_loss:3.9489 train_time:1182675ms step_avg:169.92ms\nstep:6971/10000 train_loss:3.7467 train_time:1182844ms step_avg:169.92ms\nstep:6972/10000 train_loss:3.9033 train_time:1183015ms step_avg:169.92ms\nstep:6973/10000 train_loss:3.8131 train_time:1183184ms step_avg:169.92ms\nstep:6974/10000 train_loss:3.9536 train_time:1183356ms step_avg:169.92ms\nstep:6975/10000 train_loss:3.8109 train_time:1183524ms step_avg:169.92ms\nstep:6976/10000 train_loss:3.9161 train_time:1183692ms step_avg:169.92ms\nstep:6977/10000 train_loss:3.8366 train_time:1183860ms step_avg:169.92ms\nstep:6978/10000 train_loss:3.9240 train_time:1184029ms step_avg:169.92ms\nstep:6979/10000 train_loss:3.7346 train_time:1184200ms step_avg:169.92ms\nstep:6980/10000 train_loss:3.7431 train_time:1184369ms step_avg:169.92ms\nstep:6981/10000 train_loss:3.6910 train_time:1184540ms step_avg:169.92ms\nstep:6982/10000 train_loss:3.9413 train_time:1184708ms step_avg:169.92ms\nstep:6983/10000 train_loss:3.7849 train_time:1184879ms step_avg:169.92ms\nstep:6984/10000 train_loss:3.6125 train_time:1185049ms step_avg:169.92ms\nstep:6985/10000 train_loss:3.9631 train_time:1185220ms step_avg:169.92ms\nstep:6986/10000 train_loss:3.9692 train_time:1185389ms step_avg:169.92ms\nstep:6987/10000 train_loss:3.8154 train_time:1185560ms step_avg:169.92ms\nstep:6988/10000 train_loss:3.5698 train_time:1185731ms step_avg:169.92ms\nstep:6989/10000 train_loss:3.6742 train_time:1185901ms step_avg:169.92ms\nstep:6990/10000 train_loss:3.7979 train_time:1186070ms step_avg:169.92ms\nstep:6991/10000 train_loss:3.9099 train_time:1186241ms step_avg:169.92ms\nstep:6992/10000 train_loss:3.8569 train_time:1186410ms step_avg:169.92ms\nstep:6993/10000 train_loss:3.8601 train_time:1186581ms step_avg:169.92ms\nstep:6994/10000 train_loss:3.7766 train_time:1186749ms step_avg:169.92ms\nstep:6995/10000 train_loss:3.7867 train_time:1186919ms step_avg:169.92ms\nstep:6996/10000 train_loss:3.8678 train_time:1187091ms step_avg:169.92ms\nstep:6997/10000 train_loss:3.9144 train_time:1187257ms step_avg:169.92ms\nstep:6998/10000 train_loss:3.8984 train_time:1187424ms step_avg:169.92ms\nstep:6999/10000 train_loss:3.8258 train_time:1187594ms step_avg:169.92ms\nstep:7000/10000 train_loss:3.8533 train_time:1187762ms step_avg:169.92ms\nstep:7001/10000 train_loss:3.9101 train_time:1187933ms step_avg:169.92ms\nstep:7002/10000 train_loss:3.9277 train_time:1188103ms step_avg:169.92ms\nstep:7003/10000 train_loss:3.8928 train_time:1188275ms step_avg:169.92ms\nstep:7004/10000 train_loss:3.8199 train_time:1188443ms step_avg:169.92ms\nstep:7005/10000 train_loss:3.7969 train_time:1188615ms step_avg:169.92ms\nstep:7006/10000 train_loss:3.8680 train_time:1188784ms step_avg:169.92ms\nstep:7007/10000 train_loss:3.8846 train_time:1188955ms step_avg:169.92ms\nstep:7008/10000 train_loss:4.1085 train_time:1189122ms step_avg:169.92ms\nstep:7009/10000 train_loss:3.8192 train_time:1189292ms step_avg:169.92ms\nstep:7010/10000 train_loss:3.9174 train_time:1189463ms step_avg:169.92ms\nstep:7011/10000 train_loss:4.1449 train_time:1189634ms step_avg:169.92ms\nstep:7012/10000 train_loss:3.8071 train_time:1189803ms step_avg:169.92ms\nstep:7013/10000 train_loss:3.8719 train_time:1189973ms step_avg:169.92ms\nstep:7014/10000 train_loss:3.8290 train_time:1190142ms step_avg:169.92ms\nstep:7015/10000 train_loss:4.0433 train_time:1190312ms step_avg:169.92ms\nstep:7016/10000 train_loss:3.8962 train_time:1190479ms step_avg:169.92ms\nstep:7017/10000 train_loss:3.9350 train_time:1190648ms step_avg:169.92ms\nstep:7018/10000 train_loss:3.8280 train_time:1190818ms step_avg:169.92ms\nstep:7019/10000 train_loss:3.8082 train_time:1190987ms step_avg:169.92ms\nstep:7020/10000 train_loss:3.8525 train_time:1191155ms step_avg:169.92ms\nstep:7021/10000 train_loss:3.8338 train_time:1191323ms step_avg:169.92ms\nstep:7022/10000 train_loss:3.8550 train_time:1191493ms step_avg:169.92ms\nstep:7023/10000 train_loss:4.0080 train_time:1191661ms step_avg:169.92ms\nstep:7024/10000 train_loss:3.5296 train_time:1191831ms step_avg:169.92ms\nstep:7025/10000 train_loss:3.9626 train_time:1192000ms step_avg:169.92ms\nstep:7026/10000 train_loss:3.9952 train_time:1192169ms step_avg:169.92ms\nstep:7027/10000 train_loss:3.9195 train_time:1192341ms step_avg:169.92ms\nstep:7028/10000 train_loss:3.6483 train_time:1192512ms step_avg:169.92ms\nstep:7029/10000 train_loss:3.8575 train_time:1192682ms step_avg:169.92ms\nstep:7030/10000 train_loss:3.8985 train_time:1192853ms step_avg:169.92ms\nstep:7031/10000 train_loss:3.9804 train_time:1193022ms step_avg:169.92ms\nstep:7032/10000 train_loss:3.7407 train_time:1193191ms step_avg:169.92ms\nstep:7033/10000 train_loss:3.9628 train_time:1193362ms step_avg:169.92ms\nstep:7034/10000 train_loss:3.9479 train_time:1193533ms step_avg:169.92ms\nstep:7035/10000 train_loss:3.7703 train_time:1193700ms step_avg:169.92ms\nstep:7036/10000 train_loss:3.6693 train_time:1193871ms step_avg:169.92ms\nstep:7037/10000 train_loss:3.8339 train_time:1194041ms step_avg:169.92ms\nstep:7038/10000 train_loss:3.8632 train_time:1194211ms step_avg:169.92ms\nstep:7039/10000 train_loss:3.8066 train_time:1194380ms step_avg:169.92ms\nstep:7040/10000 train_loss:3.8260 train_time:1194547ms step_avg:169.92ms\nstep:7041/10000 train_loss:3.8078 train_time:1194714ms step_avg:169.92ms\nstep:7042/10000 train_loss:3.8254 train_time:1194881ms step_avg:169.92ms\nstep:7043/10000 train_loss:3.7497 train_time:1195053ms step_avg:169.92ms\nstep:7044/10000 train_loss:3.7244 train_time:1195224ms step_avg:169.92ms\nstep:7045/10000 train_loss:3.8072 train_time:1195394ms step_avg:169.92ms\nstep:7046/10000 train_loss:3.9451 train_time:1195563ms step_avg:169.92ms\nstep:7047/10000 train_loss:3.8449 train_time:1195735ms step_avg:169.92ms\nstep:7048/10000 train_loss:3.6937 train_time:1195905ms step_avg:169.92ms\nstep:7049/10000 train_loss:3.6976 train_time:1196075ms step_avg:169.92ms\nstep:7050/10000 train_loss:3.8258 train_time:1196244ms step_avg:169.92ms\nstep:7051/10000 train_loss:3.9175 train_time:1196416ms step_avg:169.92ms\nstep:7052/10000 train_loss:4.7627 train_time:1196583ms step_avg:169.92ms\nstep:7053/10000 train_loss:3.8397 train_time:1196752ms step_avg:169.92ms\nstep:7054/10000 train_loss:3.8728 train_time:1196922ms step_avg:169.92ms\nstep:7055/10000 train_loss:3.8406 train_time:1197091ms step_avg:169.92ms\nstep:7056/10000 train_loss:3.8178 train_time:1197260ms step_avg:169.92ms\nstep:7057/10000 train_loss:3.8778 train_time:1197430ms step_avg:169.92ms\nstep:7058/10000 train_loss:3.8430 train_time:1197599ms step_avg:169.92ms\nstep:7059/10000 train_loss:3.9609 train_time:1197768ms step_avg:169.92ms\nstep:7060/10000 train_loss:3.8337 train_time:1197937ms step_avg:169.92ms\nstep:7061/10000 train_loss:4.0045 train_time:1198105ms step_avg:169.92ms\nstep:7062/10000 train_loss:3.9618 train_time:1198277ms step_avg:169.92ms\nstep:7063/10000 train_loss:3.7677 train_time:1198445ms step_avg:169.92ms\nstep:7064/10000 train_loss:3.8705 train_time:1198615ms step_avg:169.92ms\nstep:7065/10000 train_loss:3.8479 train_time:1198784ms step_avg:169.92ms\nstep:7066/10000 train_loss:3.8405 train_time:1198954ms step_avg:169.92ms\nstep:7067/10000 train_loss:3.7749 train_time:1199122ms step_avg:169.92ms\nstep:7068/10000 train_loss:3.8826 train_time:1199294ms step_avg:169.92ms\nstep:7069/10000 train_loss:3.8664 train_time:1199463ms step_avg:169.92ms\nstep:7070/10000 train_loss:3.8018 train_time:1199634ms step_avg:169.92ms\nstep:7071/10000 train_loss:3.8745 train_time:1199802ms step_avg:169.92ms\nstep:7072/10000 train_loss:3.7898 train_time:1199971ms step_avg:169.92ms\nstep:7073/10000 train_loss:3.7998 train_time:1200144ms step_avg:169.92ms\nstep:7074/10000 train_loss:3.8720 train_time:1200317ms step_avg:169.92ms\nstep:7075/10000 train_loss:3.9074 train_time:1200486ms step_avg:169.92ms\nstep:7076/10000 train_loss:3.8173 train_time:1200658ms step_avg:169.92ms\nstep:7077/10000 train_loss:3.7679 train_time:1200827ms step_avg:169.92ms\nstep:7078/10000 train_loss:3.8631 train_time:1200997ms step_avg:169.92ms\nstep:7079/10000 train_loss:3.8136 train_time:1201164ms step_avg:169.92ms\nstep:7080/10000 train_loss:3.8658 train_time:1201334ms step_avg:169.92ms\nstep:7081/10000 train_loss:3.7919 train_time:1201503ms step_avg:169.92ms\nstep:7082/10000 train_loss:3.7926 train_time:1201673ms step_avg:169.92ms\nstep:7083/10000 train_loss:3.7332 train_time:1201841ms step_avg:169.92ms\nstep:7084/10000 train_loss:4.1083 train_time:1202011ms step_avg:169.92ms\nstep:7085/10000 train_loss:3.8884 train_time:1202183ms step_avg:169.92ms\nstep:7086/10000 train_loss:3.8794 train_time:1202352ms step_avg:169.92ms\nstep:7087/10000 train_loss:3.8587 train_time:1202518ms step_avg:169.92ms\nstep:7088/10000 train_loss:3.8317 train_time:1202687ms step_avg:169.92ms\nstep:7089/10000 train_loss:3.8332 train_time:1202857ms step_avg:169.92ms\nstep:7090/10000 train_loss:3.8592 train_time:1203029ms step_avg:169.92ms\nstep:7091/10000 train_loss:3.8446 train_time:1203197ms step_avg:169.92ms\nstep:7092/10000 train_loss:3.9072 train_time:1203366ms step_avg:169.92ms\nstep:7093/10000 train_loss:3.9608 train_time:1203537ms step_avg:169.92ms\nstep:7094/10000 train_loss:3.8565 train_time:1203704ms step_avg:169.92ms\nstep:7095/10000 train_loss:4.2150 train_time:1203874ms step_avg:169.92ms\nstep:7096/10000 train_loss:4.2420 train_time:1204044ms step_avg:169.92ms\nstep:7097/10000 train_loss:3.8080 train_time:1204215ms step_avg:169.92ms\nstep:7098/10000 train_loss:3.8314 train_time:1204383ms step_avg:169.92ms\nstep:7099/10000 train_loss:3.8958 train_time:1204553ms step_avg:169.92ms\nstep:7100/10000 train_loss:3.8707 train_time:1204723ms step_avg:169.92ms\nstep:7101/10000 train_loss:3.9046 train_time:1204894ms step_avg:169.92ms\nstep:7102/10000 train_loss:3.8531 train_time:1205062ms step_avg:169.92ms\nstep:7103/10000 train_loss:3.9986 train_time:1205233ms step_avg:169.92ms\nstep:7104/10000 train_loss:3.7412 train_time:1205404ms step_avg:169.92ms\nstep:7105/10000 train_loss:3.8234 train_time:1205575ms step_avg:169.92ms\nstep:7106/10000 train_loss:3.9041 train_time:1205743ms step_avg:169.92ms\nstep:7107/10000 train_loss:3.7467 train_time:1205916ms step_avg:169.92ms\nstep:7108/10000 train_loss:4.0202 train_time:1206084ms step_avg:169.92ms\nstep:7109/10000 train_loss:4.1891 train_time:1206254ms step_avg:169.92ms\nstep:7110/10000 train_loss:3.7994 train_time:1206424ms step_avg:169.92ms\nstep:7111/10000 train_loss:3.7559 train_time:1206594ms step_avg:169.92ms\nstep:7112/10000 train_loss:3.6596 train_time:1206763ms step_avg:169.92ms\nstep:7113/10000 train_loss:3.9198 train_time:1206933ms step_avg:169.92ms\nstep:7114/10000 train_loss:3.9612 train_time:1207103ms step_avg:169.92ms\nstep:7115/10000 train_loss:3.9218 train_time:1207274ms step_avg:169.92ms\nstep:7116/10000 train_loss:3.8759 train_time:1207443ms step_avg:169.92ms\nstep:7117/10000 train_loss:3.9390 train_time:1207614ms step_avg:169.92ms\nstep:7118/10000 train_loss:4.0846 train_time:1207783ms step_avg:169.92ms\nstep:7119/10000 train_loss:3.8069 train_time:1207955ms step_avg:169.92ms\nstep:7120/10000 train_loss:3.9653 train_time:1208123ms step_avg:169.92ms\nstep:7121/10000 train_loss:3.8298 train_time:1208293ms step_avg:169.92ms\nstep:7122/10000 train_loss:3.8081 train_time:1208461ms step_avg:169.92ms\nstep:7123/10000 train_loss:4.0813 train_time:1208630ms step_avg:169.92ms\nstep:7124/10000 train_loss:4.0302 train_time:1208801ms step_avg:169.92ms\nstep:7125/10000 train_loss:4.0411 train_time:1208972ms step_avg:169.92ms\nstep:7126/10000 train_loss:3.8479 train_time:1209138ms step_avg:169.92ms\nstep:7127/10000 train_loss:3.7323 train_time:1209308ms step_avg:169.92ms\nstep:7128/10000 train_loss:3.8650 train_time:1209478ms step_avg:169.92ms\nstep:7129/10000 train_loss:3.9061 train_time:1209647ms step_avg:169.92ms\nstep:7130/10000 train_loss:3.7907 train_time:1209818ms step_avg:169.92ms\nstep:7131/10000 train_loss:4.0311 train_time:1209986ms step_avg:169.92ms\nstep:7132/10000 train_loss:4.1261 train_time:1210155ms step_avg:169.92ms\nstep:7133/10000 train_loss:4.0158 train_time:1210323ms step_avg:169.92ms\nstep:7134/10000 train_loss:3.8252 train_time:1210494ms step_avg:169.92ms\nstep:7135/10000 train_loss:3.8628 train_time:1210660ms step_avg:169.92ms\nstep:7136/10000 train_loss:3.9359 train_time:1210829ms step_avg:169.92ms\nstep:7137/10000 train_loss:3.9333 train_time:1210998ms step_avg:169.92ms\nstep:7138/10000 train_loss:3.8482 train_time:1211167ms step_avg:169.92ms\nstep:7139/10000 train_loss:3.7565 train_time:1211337ms step_avg:169.92ms\nstep:7140/10000 train_loss:3.9411 train_time:1211507ms step_avg:169.92ms\nstep:7141/10000 train_loss:3.8180 train_time:1211676ms step_avg:169.92ms\nstep:7142/10000 train_loss:3.7531 train_time:1211843ms step_avg:169.92ms\nstep:7143/10000 train_loss:3.7631 train_time:1212014ms step_avg:169.92ms\nstep:7144/10000 train_loss:3.7417 train_time:1212184ms step_avg:169.92ms\nstep:7145/10000 train_loss:3.8904 train_time:1212353ms step_avg:169.92ms\nstep:7146/10000 train_loss:3.9896 train_time:1212522ms step_avg:169.92ms\nstep:7147/10000 train_loss:3.9008 train_time:1212694ms step_avg:169.92ms\nstep:7148/10000 train_loss:3.8053 train_time:1212862ms step_avg:169.92ms\nstep:7149/10000 train_loss:3.8900 train_time:1213034ms step_avg:169.92ms\nstep:7150/10000 train_loss:3.9489 train_time:1213204ms step_avg:169.92ms\nstep:7151/10000 train_loss:3.8918 train_time:1213374ms step_avg:169.92ms\nstep:7152/10000 train_loss:3.9216 train_time:1213544ms step_avg:169.92ms\nstep:7153/10000 train_loss:3.9381 train_time:1213714ms step_avg:169.92ms\nstep:7154/10000 train_loss:3.6884 train_time:1213882ms step_avg:169.92ms\nstep:7155/10000 train_loss:3.7666 train_time:1214053ms step_avg:169.92ms\nstep:7156/10000 train_loss:3.8221 train_time:1214222ms step_avg:169.92ms\nstep:7157/10000 train_loss:3.8685 train_time:1214391ms step_avg:169.92ms\nstep:7158/10000 train_loss:3.8642 train_time:1214562ms step_avg:169.92ms\nstep:7159/10000 train_loss:3.9959 train_time:1214732ms step_avg:169.92ms\nstep:7160/10000 train_loss:3.9869 train_time:1214903ms step_avg:169.92ms\nstep:7161/10000 train_loss:3.7291 train_time:1215073ms step_avg:169.92ms\nstep:7162/10000 train_loss:3.8539 train_time:1215242ms step_avg:169.92ms\nstep:7163/10000 train_loss:3.9207 train_time:1215413ms step_avg:169.92ms\nstep:7164/10000 train_loss:3.8384 train_time:1215583ms step_avg:169.92ms\nstep:7165/10000 train_loss:4.1507 train_time:1215753ms step_avg:169.92ms\nstep:7166/10000 train_loss:3.8501 train_time:1215923ms step_avg:169.92ms\nstep:7167/10000 train_loss:3.8597 train_time:1216094ms step_avg:169.92ms\nstep:7168/10000 train_loss:3.7944 train_time:1216264ms step_avg:169.92ms\nstep:7169/10000 train_loss:3.6698 train_time:1216435ms step_avg:169.92ms\nstep:7170/10000 train_loss:3.9602 train_time:1216602ms step_avg:169.92ms\nstep:7171/10000 train_loss:3.8014 train_time:1216771ms step_avg:169.92ms\nstep:7172/10000 train_loss:3.9421 train_time:1216943ms step_avg:169.92ms\nstep:7173/10000 train_loss:3.8165 train_time:1217113ms step_avg:169.92ms\nstep:7174/10000 train_loss:3.9686 train_time:1217282ms step_avg:169.92ms\nstep:7175/10000 train_loss:3.7572 train_time:1217451ms step_avg:169.92ms\nstep:7176/10000 train_loss:3.8188 train_time:1217621ms step_avg:169.92ms\nstep:7177/10000 train_loss:3.7799 train_time:1217789ms step_avg:169.92ms\nstep:7178/10000 train_loss:3.8847 train_time:1217958ms step_avg:169.92ms\nstep:7179/10000 train_loss:3.8284 train_time:1218127ms step_avg:169.92ms\nstep:7180/10000 train_loss:3.7800 train_time:1218296ms step_avg:169.92ms\nstep:7181/10000 train_loss:3.8302 train_time:1218464ms step_avg:169.92ms\nstep:7182/10000 train_loss:3.7692 train_time:1218632ms step_avg:169.92ms\nstep:7183/10000 train_loss:3.9788 train_time:1218799ms step_avg:169.91ms\nstep:7184/10000 train_loss:3.9056 train_time:1218968ms step_avg:169.91ms\nstep:7185/10000 train_loss:3.8086 train_time:1219139ms step_avg:169.91ms\nstep:7186/10000 train_loss:3.9320 train_time:1219308ms step_avg:169.91ms\nstep:7187/10000 train_loss:3.8181 train_time:1219479ms step_avg:169.91ms\nstep:7188/10000 train_loss:3.7870 train_time:1219649ms step_avg:169.91ms\nstep:7189/10000 train_loss:3.7520 train_time:1219818ms step_avg:169.91ms\nstep:7190/10000 train_loss:3.9185 train_time:1219987ms step_avg:169.91ms\nstep:7191/10000 train_loss:3.9340 train_time:1220160ms step_avg:169.91ms\nstep:7192/10000 train_loss:3.7894 train_time:1220331ms step_avg:169.92ms\nstep:7193/10000 train_loss:3.8634 train_time:1220501ms step_avg:169.92ms\nstep:7194/10000 train_loss:3.8512 train_time:1220671ms step_avg:169.92ms\nstep:7195/10000 train_loss:3.8306 train_time:1220839ms step_avg:169.91ms\nstep:7196/10000 train_loss:3.8223 train_time:1221006ms step_avg:169.91ms\nstep:7197/10000 train_loss:3.7976 train_time:1221175ms step_avg:169.91ms\nstep:7198/10000 train_loss:3.9284 train_time:1221343ms step_avg:169.91ms\nstep:7199/10000 train_loss:3.8692 train_time:1221515ms step_avg:169.91ms\nstep:7200/10000 train_loss:3.8313 train_time:1221685ms step_avg:169.91ms\nstep:7201/10000 train_loss:3.9261 train_time:1221854ms step_avg:169.91ms\nstep:7202/10000 train_loss:3.9030 train_time:1222022ms step_avg:169.91ms\nstep:7203/10000 train_loss:4.2705 train_time:1222193ms step_avg:169.91ms\nstep:7204/10000 train_loss:3.9251 train_time:1222361ms step_avg:169.91ms\nstep:7205/10000 train_loss:3.8188 train_time:1222530ms step_avg:169.91ms\nstep:7206/10000 train_loss:3.8860 train_time:1222698ms step_avg:169.91ms\nstep:7207/10000 train_loss:3.8334 train_time:1222867ms step_avg:169.91ms\nstep:7208/10000 train_loss:3.8219 train_time:1223037ms step_avg:169.91ms\nstep:7209/10000 train_loss:3.8358 train_time:1223206ms step_avg:169.91ms\nstep:7210/10000 train_loss:3.8825 train_time:1223378ms step_avg:169.91ms\nstep:7211/10000 train_loss:3.9324 train_time:1223547ms step_avg:169.91ms\nstep:7212/10000 train_loss:3.8883 train_time:1223717ms step_avg:169.91ms\nstep:7213/10000 train_loss:3.8611 train_time:1223886ms step_avg:169.91ms\nstep:7214/10000 train_loss:3.7831 train_time:1224056ms step_avg:169.91ms\nstep:7215/10000 train_loss:4.0685 train_time:1224224ms step_avg:169.91ms\nstep:7216/10000 train_loss:3.8332 train_time:1224396ms step_avg:169.91ms\nstep:7217/10000 train_loss:3.8213 train_time:1224563ms step_avg:169.91ms\nstep:7218/10000 train_loss:3.8108 train_time:1224733ms step_avg:169.91ms\nstep:7219/10000 train_loss:3.7877 train_time:1224900ms step_avg:169.91ms\nstep:7220/10000 train_loss:3.8561 train_time:1225069ms step_avg:169.91ms\nstep:7221/10000 train_loss:3.8094 train_time:1225243ms step_avg:169.91ms\nstep:7222/10000 train_loss:4.0101 train_time:1225416ms step_avg:169.91ms\nstep:7223/10000 train_loss:3.9735 train_time:1225582ms step_avg:169.91ms\nstep:7224/10000 train_loss:3.8193 train_time:1225753ms step_avg:169.91ms\nstep:7225/10000 train_loss:3.8425 train_time:1225921ms step_avg:169.91ms\nstep:7226/10000 train_loss:3.7496 train_time:1226090ms step_avg:169.91ms\nstep:7227/10000 train_loss:3.8625 train_time:1226258ms step_avg:169.91ms\nstep:7228/10000 train_loss:3.7467 train_time:1226427ms step_avg:169.91ms\nstep:7229/10000 train_loss:3.8338 train_time:1226598ms step_avg:169.91ms\nstep:7230/10000 train_loss:3.8303 train_time:1226767ms step_avg:169.91ms\nstep:7231/10000 train_loss:4.0612 train_time:1226934ms step_avg:169.91ms\nstep:7232/10000 train_loss:3.9597 train_time:1227102ms step_avg:169.91ms\nstep:7233/10000 train_loss:3.7933 train_time:1227273ms step_avg:169.91ms\nstep:7234/10000 train_loss:3.8454 train_time:1227443ms step_avg:169.91ms\nstep:7235/10000 train_loss:3.7908 train_time:1227613ms step_avg:169.91ms\nstep:7236/10000 train_loss:3.8799 train_time:1227781ms step_avg:169.91ms\nstep:7237/10000 train_loss:3.8188 train_time:1227950ms step_avg:169.91ms\nstep:7238/10000 train_loss:3.9372 train_time:1228119ms step_avg:169.91ms\nstep:7239/10000 train_loss:3.8746 train_time:1228288ms step_avg:169.91ms\nstep:7240/10000 train_loss:3.8676 train_time:1228459ms step_avg:169.91ms\nstep:7241/10000 train_loss:3.8773 train_time:1228631ms step_avg:169.91ms\nstep:7242/10000 train_loss:3.9561 train_time:1228798ms step_avg:169.91ms\nstep:7243/10000 train_loss:3.7818 train_time:1228967ms step_avg:169.91ms\nstep:7244/10000 train_loss:3.9960 train_time:1229135ms step_avg:169.91ms\nstep:7245/10000 train_loss:3.8619 train_time:1229303ms step_avg:169.91ms\nstep:7246/10000 train_loss:3.7740 train_time:1229472ms step_avg:169.91ms\nstep:7247/10000 train_loss:3.7848 train_time:1229642ms step_avg:169.91ms\nstep:7248/10000 train_loss:3.7914 train_time:1229813ms step_avg:169.91ms\nstep:7249/10000 train_loss:3.9090 train_time:1229982ms step_avg:169.91ms\nstep:7250/10000 train_loss:3.9126 train_time:1230152ms step_avg:169.91ms\nstep:7251/10000 train_loss:3.9672 train_time:1230320ms step_avg:169.91ms\nstep:7252/10000 train_loss:3.9099 train_time:1230489ms step_avg:169.91ms\nstep:7253/10000 train_loss:3.9090 train_time:1230660ms step_avg:169.91ms\nstep:7254/10000 train_loss:3.7666 train_time:1230829ms step_avg:169.91ms\nstep:7255/10000 train_loss:3.8913 train_time:1230997ms step_avg:169.91ms\nstep:7256/10000 train_loss:4.0159 train_time:1231167ms step_avg:169.91ms\nstep:7257/10000 train_loss:3.7816 train_time:1231338ms step_avg:169.91ms\nstep:7258/10000 train_loss:3.8460 train_time:1231506ms step_avg:169.91ms\nstep:7259/10000 train_loss:3.8801 train_time:1231677ms step_avg:169.91ms\nstep:7260/10000 train_loss:3.8891 train_time:1231845ms step_avg:169.91ms\nstep:7261/10000 train_loss:3.7915 train_time:1232013ms step_avg:169.91ms\nstep:7262/10000 train_loss:3.8183 train_time:1232180ms step_avg:169.91ms\nstep:7263/10000 train_loss:3.7562 train_time:1232349ms step_avg:169.91ms\nstep:7264/10000 train_loss:3.9193 train_time:1232519ms step_avg:169.91ms\nstep:7265/10000 train_loss:3.8352 train_time:1232688ms step_avg:169.91ms\nstep:7266/10000 train_loss:3.6943 train_time:1232859ms step_avg:169.91ms\nstep:7267/10000 train_loss:3.8897 train_time:1233029ms step_avg:169.91ms\nstep:7268/10000 train_loss:3.9351 train_time:1233199ms step_avg:169.91ms\nstep:7269/10000 train_loss:3.8442 train_time:1233367ms step_avg:169.91ms\nstep:7270/10000 train_loss:3.8831 train_time:1233535ms step_avg:169.91ms\nstep:7271/10000 train_loss:3.8304 train_time:1233705ms step_avg:169.91ms\nstep:7272/10000 train_loss:3.8920 train_time:1233873ms step_avg:169.91ms\nstep:7273/10000 train_loss:3.8601 train_time:1234041ms step_avg:169.91ms\nstep:7274/10000 train_loss:3.6737 train_time:1234212ms step_avg:169.91ms\nstep:7275/10000 train_loss:3.7894 train_time:1234382ms step_avg:169.91ms\nstep:7276/10000 train_loss:3.9486 train_time:1234551ms step_avg:169.91ms\nstep:7277/10000 train_loss:3.8249 train_time:1234719ms step_avg:169.91ms\nstep:7278/10000 train_loss:3.8478 train_time:1234896ms step_avg:169.91ms\nstep:7279/10000 train_loss:3.9178 train_time:1235063ms step_avg:169.91ms\nstep:7280/10000 train_loss:3.8891 train_time:1235231ms step_avg:169.91ms\nstep:7281/10000 train_loss:3.8509 train_time:1235392ms step_avg:169.91ms\nstep:7282/10000 train_loss:3.8177 train_time:1235563ms step_avg:169.91ms\nstep:7283/10000 train_loss:3.8088 train_time:1235733ms step_avg:169.91ms\nstep:7284/10000 train_loss:3.9273 train_time:1235904ms step_avg:169.91ms\nstep:7285/10000 train_loss:3.7490 train_time:1236073ms step_avg:169.91ms\nstep:7286/10000 train_loss:3.9404 train_time:1236242ms step_avg:169.91ms\nstep:7287/10000 train_loss:3.8501 train_time:1236414ms step_avg:169.91ms\nstep:7288/10000 train_loss:3.8607 train_time:1236581ms step_avg:169.91ms\nstep:7289/10000 train_loss:3.8977 train_time:1236749ms step_avg:169.91ms\nstep:7290/10000 train_loss:3.8224 train_time:1236919ms step_avg:169.91ms\nstep:7291/10000 train_loss:3.8954 train_time:1237088ms step_avg:169.91ms\nstep:7292/10000 train_loss:3.8662 train_time:1237259ms step_avg:169.91ms\nstep:7293/10000 train_loss:3.8779 train_time:1237430ms step_avg:169.91ms\nstep:7294/10000 train_loss:3.8052 train_time:1237600ms step_avg:169.91ms\nstep:7295/10000 train_loss:3.8899 train_time:1237769ms step_avg:169.91ms\nstep:7296/10000 train_loss:3.7926 train_time:1237939ms step_avg:169.91ms\nstep:7297/10000 train_loss:3.7137 train_time:1238107ms step_avg:169.91ms\nstep:7298/10000 train_loss:3.8474 train_time:1238277ms step_avg:169.91ms\nstep:7299/10000 train_loss:3.8099 train_time:1238446ms step_avg:169.91ms\nstep:7300/10000 train_loss:3.8033 train_time:1238618ms step_avg:169.91ms\nstep:7301/10000 train_loss:3.9664 train_time:1238788ms step_avg:169.91ms\nstep:7302/10000 train_loss:3.8036 train_time:1238958ms step_avg:169.91ms\nstep:7303/10000 train_loss:3.7836 train_time:1239125ms step_avg:169.91ms\nstep:7304/10000 train_loss:3.6819 train_time:1239294ms step_avg:169.91ms\nstep:7305/10000 train_loss:3.8140 train_time:1239462ms step_avg:169.91ms\nstep:7306/10000 train_loss:3.7981 train_time:1239631ms step_avg:169.91ms\nstep:7307/10000 train_loss:3.9145 train_time:1239799ms step_avg:169.91ms\nstep:7308/10000 train_loss:3.8497 train_time:1239969ms step_avg:169.91ms\nstep:7309/10000 train_loss:4.0412 train_time:1240139ms step_avg:169.91ms\nstep:7310/10000 train_loss:3.9123 train_time:1240308ms step_avg:169.91ms\nstep:7311/10000 train_loss:3.7482 train_time:1240478ms step_avg:169.91ms\nstep:7312/10000 train_loss:3.9749 train_time:1240647ms step_avg:169.91ms\nstep:7313/10000 train_loss:3.9200 train_time:1240822ms step_avg:169.91ms\nstep:7314/10000 train_loss:3.6858 train_time:1240986ms step_avg:169.90ms\nstep:7315/10000 train_loss:3.9443 train_time:1241155ms step_avg:169.90ms\nstep:7316/10000 train_loss:3.9291 train_time:1241323ms step_avg:169.90ms\nstep:7317/10000 train_loss:4.0299 train_time:1241496ms step_avg:169.91ms\nstep:7318/10000 train_loss:4.2357 train_time:1241663ms step_avg:169.90ms\nstep:7319/10000 train_loss:3.7167 train_time:1241832ms step_avg:169.90ms\nstep:7320/10000 train_loss:3.9193 train_time:1242003ms step_avg:169.90ms\nstep:7321/10000 train_loss:3.8137 train_time:1242174ms step_avg:169.90ms\nstep:7322/10000 train_loss:3.8328 train_time:1242344ms step_avg:169.90ms\nstep:7323/10000 train_loss:3.8736 train_time:1242515ms step_avg:169.90ms\nstep:7324/10000 train_loss:3.7733 train_time:1242686ms step_avg:169.91ms\nstep:7325/10000 train_loss:3.8835 train_time:1242855ms step_avg:169.91ms\nstep:7326/10000 train_loss:3.5275 train_time:1243025ms step_avg:169.90ms\nstep:7327/10000 train_loss:3.8258 train_time:1243196ms step_avg:169.91ms\nstep:7328/10000 train_loss:3.8439 train_time:1243364ms step_avg:169.90ms\nstep:7329/10000 train_loss:3.9257 train_time:1243535ms step_avg:169.91ms\nstep:7330/10000 train_loss:3.8582 train_time:1243703ms step_avg:169.90ms\nstep:7331/10000 train_loss:3.9853 train_time:1243872ms step_avg:169.90ms\nstep:7332/10000 train_loss:3.9269 train_time:1244041ms step_avg:169.90ms\nstep:7333/10000 train_loss:3.7664 train_time:1244212ms step_avg:169.90ms\nstep:7334/10000 train_loss:3.7020 train_time:1244383ms step_avg:169.90ms\nstep:7335/10000 train_loss:3.8643 train_time:1244555ms step_avg:169.91ms\nstep:7336/10000 train_loss:3.7399 train_time:1244723ms step_avg:169.90ms\nstep:7337/10000 train_loss:3.8894 train_time:1244892ms step_avg:169.90ms\nstep:7338/10000 train_loss:3.8770 train_time:1245062ms step_avg:169.90ms\nstep:7339/10000 train_loss:3.8320 train_time:1245233ms step_avg:169.90ms\nstep:7340/10000 train_loss:3.8815 train_time:1245401ms step_avg:169.90ms\nstep:7341/10000 train_loss:3.7639 train_time:1245571ms step_avg:169.90ms\nstep:7342/10000 train_loss:3.9445 train_time:1245741ms step_avg:169.90ms\nstep:7343/10000 train_loss:3.8466 train_time:1245910ms step_avg:169.90ms\nstep:7344/10000 train_loss:3.7460 train_time:1246083ms step_avg:169.90ms\nstep:7345/10000 train_loss:3.9175 train_time:1246254ms step_avg:169.91ms\nstep:7346/10000 train_loss:3.8669 train_time:1246423ms step_avg:169.90ms\nstep:7347/10000 train_loss:3.9614 train_time:1246592ms step_avg:169.90ms\nstep:7348/10000 train_loss:4.0942 train_time:1246760ms step_avg:169.90ms\nstep:7349/10000 train_loss:3.9249 train_time:1246931ms step_avg:169.90ms\nstep:7350/10000 train_loss:3.7431 train_time:1247099ms step_avg:169.90ms\nstep:7351/10000 train_loss:3.7790 train_time:1247268ms step_avg:169.90ms\nstep:7352/10000 train_loss:3.8995 train_time:1247438ms step_avg:169.90ms\nstep:7353/10000 train_loss:3.9134 train_time:1247607ms step_avg:169.90ms\nstep:7354/10000 train_loss:3.9245 train_time:1247778ms step_avg:169.90ms\nstep:7355/10000 train_loss:3.7036 train_time:1247948ms step_avg:169.90ms\nstep:7356/10000 train_loss:3.8798 train_time:1248118ms step_avg:169.90ms\nstep:7357/10000 train_loss:3.8746 train_time:1248286ms step_avg:169.90ms\nstep:7358/10000 train_loss:3.8802 train_time:1248455ms step_avg:169.90ms\nstep:7359/10000 train_loss:3.8630 train_time:1248624ms step_avg:169.90ms\nstep:7360/10000 train_loss:3.9857 train_time:1248793ms step_avg:169.90ms\nstep:7361/10000 train_loss:3.8057 train_time:1248962ms step_avg:169.90ms\nstep:7362/10000 train_loss:3.8152 train_time:1249133ms step_avg:169.90ms\nstep:7363/10000 train_loss:3.8397 train_time:1249302ms step_avg:169.90ms\nstep:7364/10000 train_loss:3.8632 train_time:1249474ms step_avg:169.90ms\nstep:7365/10000 train_loss:3.8814 train_time:1249643ms step_avg:169.90ms\nstep:7366/10000 train_loss:3.7930 train_time:1249820ms step_avg:169.90ms\nstep:7367/10000 train_loss:3.7767 train_time:1249988ms step_avg:169.90ms\nstep:7368/10000 train_loss:3.7215 train_time:1250154ms step_avg:169.90ms\nstep:7369/10000 train_loss:3.8293 train_time:1250327ms step_avg:169.90ms\nstep:7370/10000 train_loss:3.6582 train_time:1250492ms step_avg:169.90ms\nstep:7371/10000 train_loss:3.7615 train_time:1250662ms step_avg:169.90ms\nstep:7372/10000 train_loss:3.8845 train_time:1250833ms step_avg:169.90ms\nstep:7373/10000 train_loss:3.8123 train_time:1251001ms step_avg:169.90ms\nstep:7374/10000 train_loss:3.8778 train_time:1251177ms step_avg:169.90ms\nstep:7375/10000 train_loss:3.9287 train_time:1251339ms step_avg:169.90ms\nstep:7376/10000 train_loss:3.7484 train_time:1251506ms step_avg:169.90ms\nstep:7377/10000 train_loss:3.8216 train_time:1251674ms step_avg:169.90ms\nstep:7378/10000 train_loss:3.9170 train_time:1251845ms step_avg:169.90ms\nstep:7379/10000 train_loss:3.7620 train_time:1252014ms step_avg:169.90ms\nstep:7380/10000 train_loss:3.8501 train_time:1252183ms step_avg:169.90ms\nstep:7381/10000 train_loss:3.8015 train_time:1252354ms step_avg:169.90ms\nstep:7382/10000 train_loss:3.7834 train_time:1252522ms step_avg:169.90ms\nstep:7383/10000 train_loss:3.8159 train_time:1252690ms step_avg:169.90ms\nstep:7384/10000 train_loss:3.9144 train_time:1252861ms step_avg:169.90ms\nstep:7385/10000 train_loss:3.8105 train_time:1253032ms step_avg:169.90ms\nstep:7386/10000 train_loss:3.9061 train_time:1253202ms step_avg:169.90ms\nstep:7387/10000 train_loss:3.6096 train_time:1253374ms step_avg:169.90ms\nstep:7388/10000 train_loss:3.7463 train_time:1253543ms step_avg:169.90ms\nstep:7389/10000 train_loss:3.8967 train_time:1253712ms step_avg:169.90ms\nstep:7390/10000 train_loss:3.6536 train_time:1253883ms step_avg:169.90ms\nstep:7391/10000 train_loss:3.8442 train_time:1254052ms step_avg:169.90ms\nstep:7392/10000 train_loss:3.8161 train_time:1254222ms step_avg:169.90ms\nstep:7393/10000 train_loss:3.7763 train_time:1254393ms step_avg:169.90ms\nstep:7394/10000 train_loss:3.8219 train_time:1254562ms step_avg:169.90ms\nstep:7395/10000 train_loss:3.8285 train_time:1254731ms step_avg:169.90ms\nstep:7396/10000 train_loss:3.8281 train_time:1254899ms step_avg:169.90ms\nstep:7397/10000 train_loss:3.8689 train_time:1255067ms step_avg:169.90ms\nstep:7398/10000 train_loss:3.7815 train_time:1255237ms step_avg:169.90ms\nstep:7399/10000 train_loss:3.8094 train_time:1255406ms step_avg:169.90ms\nstep:7400/10000 train_loss:3.8170 train_time:1255576ms step_avg:169.90ms\nstep:7401/10000 train_loss:3.9224 train_time:1255742ms step_avg:169.90ms\nstep:7402/10000 train_loss:3.8410 train_time:1255911ms step_avg:169.90ms\nstep:7403/10000 train_loss:3.7992 train_time:1256080ms step_avg:169.90ms\nstep:7404/10000 train_loss:3.6957 train_time:1256251ms step_avg:169.90ms\nstep:7405/10000 train_loss:3.8553 train_time:1256419ms step_avg:169.90ms\nstep:7406/10000 train_loss:3.8523 train_time:1256588ms step_avg:169.90ms\nstep:7407/10000 train_loss:3.9000 train_time:1256756ms step_avg:169.90ms\nstep:7408/10000 train_loss:3.9653 train_time:1256924ms step_avg:169.90ms\nstep:7409/10000 train_loss:3.8720 train_time:1257093ms step_avg:169.90ms\nstep:7410/10000 train_loss:3.8528 train_time:1257262ms step_avg:169.90ms\nstep:7411/10000 train_loss:3.8016 train_time:1257432ms step_avg:169.90ms\nstep:7412/10000 train_loss:3.7407 train_time:1257602ms step_avg:169.90ms\nstep:7413/10000 train_loss:3.9196 train_time:1257773ms step_avg:169.90ms\nstep:7414/10000 train_loss:3.8934 train_time:1257941ms step_avg:169.90ms\nstep:7415/10000 train_loss:3.7505 train_time:1258110ms step_avg:169.90ms\nstep:7416/10000 train_loss:3.8089 train_time:1258278ms step_avg:169.90ms\nstep:7417/10000 train_loss:3.7658 train_time:1258447ms step_avg:169.90ms\nstep:7418/10000 train_loss:3.8318 train_time:1258618ms step_avg:169.90ms\nstep:7419/10000 train_loss:3.7282 train_time:1258787ms step_avg:169.90ms\nstep:7420/10000 train_loss:3.7774 train_time:1258956ms step_avg:169.90ms\nstep:7421/10000 train_loss:3.8446 train_time:1259125ms step_avg:169.90ms\nstep:7422/10000 train_loss:3.8720 train_time:1259294ms step_avg:169.90ms\nstep:7423/10000 train_loss:3.7624 train_time:1259462ms step_avg:169.90ms\nstep:7424/10000 train_loss:3.9631 train_time:1259633ms step_avg:169.90ms\nstep:7425/10000 train_loss:3.5479 train_time:1259806ms step_avg:169.90ms\nstep:7426/10000 train_loss:3.8523 train_time:1259972ms step_avg:169.90ms\nstep:7427/10000 train_loss:3.8355 train_time:1260141ms step_avg:169.90ms\nstep:7428/10000 train_loss:3.9179 train_time:1260312ms step_avg:169.90ms\nstep:7429/10000 train_loss:3.9122 train_time:1260480ms step_avg:169.90ms\nstep:7430/10000 train_loss:3.8607 train_time:1260649ms step_avg:169.90ms\nstep:7431/10000 train_loss:3.8405 train_time:1260820ms step_avg:169.90ms\nstep:7432/10000 train_loss:3.8258 train_time:1260988ms step_avg:169.90ms\nstep:7433/10000 train_loss:3.8136 train_time:1261160ms step_avg:169.90ms\nstep:7434/10000 train_loss:3.8078 train_time:1261328ms step_avg:169.90ms\nstep:7435/10000 train_loss:3.9046 train_time:1261499ms step_avg:169.90ms\nstep:7436/10000 train_loss:3.7736 train_time:1261668ms step_avg:169.90ms\nstep:7437/10000 train_loss:3.8953 train_time:1261837ms step_avg:169.90ms\nstep:7438/10000 train_loss:3.7869 train_time:1262008ms step_avg:169.90ms\nstep:7439/10000 train_loss:3.7388 train_time:1262179ms step_avg:169.90ms\nstep:7440/10000 train_loss:3.8248 train_time:1262347ms step_avg:169.90ms\nstep:7441/10000 train_loss:3.9746 train_time:1262517ms step_avg:169.90ms\nstep:7442/10000 train_loss:3.7657 train_time:1262686ms step_avg:169.90ms\nstep:7443/10000 train_loss:3.8131 train_time:1262862ms step_avg:169.90ms\nstep:7444/10000 train_loss:4.0303 train_time:1263029ms step_avg:169.90ms\nstep:7445/10000 train_loss:3.7789 train_time:1263203ms step_avg:169.90ms\nstep:7446/10000 train_loss:3.8318 train_time:1263370ms step_avg:169.90ms\nstep:7447/10000 train_loss:3.8318 train_time:1263541ms step_avg:169.90ms\nstep:7448/10000 train_loss:3.9029 train_time:1263707ms step_avg:169.90ms\nstep:7449/10000 train_loss:3.8381 train_time:1263870ms step_avg:169.90ms\nstep:7450/10000 train_loss:4.0663 train_time:1264041ms step_avg:169.90ms\nstep:7451/10000 train_loss:3.8799 train_time:1264211ms step_avg:169.90ms\nstep:7452/10000 train_loss:3.8308 train_time:1264379ms step_avg:169.90ms\nstep:7453/10000 train_loss:3.7733 train_time:1264549ms step_avg:169.90ms\nstep:7454/10000 train_loss:3.7764 train_time:1264718ms step_avg:169.90ms\nstep:7455/10000 train_loss:3.8721 train_time:1264887ms step_avg:169.90ms\nstep:7456/10000 train_loss:3.8679 train_time:1265059ms step_avg:169.90ms\nstep:7457/10000 train_loss:3.8151 train_time:1265226ms step_avg:169.90ms\nstep:7458/10000 train_loss:3.8740 train_time:1265393ms step_avg:169.90ms\nstep:7459/10000 train_loss:3.6979 train_time:1265561ms step_avg:169.90ms\nstep:7460/10000 train_loss:3.7969 train_time:1265730ms step_avg:169.90ms\nstep:7461/10000 train_loss:3.7888 train_time:1265899ms step_avg:169.90ms\nstep:7462/10000 train_loss:3.7696 train_time:1266069ms step_avg:169.90ms\nstep:7463/10000 train_loss:3.7641 train_time:1266241ms step_avg:169.90ms\nstep:7464/10000 train_loss:3.7653 train_time:1266410ms step_avg:169.90ms\nstep:7465/10000 train_loss:4.0111 train_time:1266580ms step_avg:169.90ms\nstep:7466/10000 train_loss:3.8245 train_time:1266749ms step_avg:169.90ms\nstep:7467/10000 train_loss:3.8179 train_time:1266918ms step_avg:169.90ms\nstep:7468/10000 train_loss:3.8374 train_time:1267089ms step_avg:169.90ms\nstep:7469/10000 train_loss:4.0133 train_time:1267259ms step_avg:169.90ms\nstep:7470/10000 train_loss:3.8896 train_time:1267428ms step_avg:169.90ms\nstep:7471/10000 train_loss:3.8563 train_time:1267600ms step_avg:169.90ms\nstep:7472/10000 train_loss:3.8635 train_time:1267769ms step_avg:169.90ms\nstep:7473/10000 train_loss:3.7803 train_time:1267938ms step_avg:169.90ms\nstep:7474/10000 train_loss:3.7723 train_time:1268106ms step_avg:169.90ms\nstep:7475/10000 train_loss:4.1106 train_time:1268277ms step_avg:169.90ms\nstep:7476/10000 train_loss:3.6417 train_time:1268446ms step_avg:169.90ms\nstep:7477/10000 train_loss:3.7777 train_time:1268616ms step_avg:169.90ms\nstep:7478/10000 train_loss:3.9873 train_time:1268785ms step_avg:169.90ms\nstep:7479/10000 train_loss:3.8337 train_time:1268955ms step_avg:169.90ms\nstep:7480/10000 train_loss:3.7559 train_time:1269123ms step_avg:169.90ms\nstep:7481/10000 train_loss:3.7547 train_time:1269292ms step_avg:169.90ms\nstep:7482/10000 train_loss:3.8110 train_time:1269461ms step_avg:169.90ms\nstep:7483/10000 train_loss:3.8039 train_time:1269631ms step_avg:169.90ms\nstep:7484/10000 train_loss:3.7054 train_time:1269798ms step_avg:169.90ms\nstep:7485/10000 train_loss:4.0424 train_time:1269970ms step_avg:169.90ms\nstep:7486/10000 train_loss:3.8801 train_time:1270142ms step_avg:169.90ms\nstep:7487/10000 train_loss:3.8099 train_time:1270312ms step_avg:169.90ms\nstep:7488/10000 train_loss:3.7217 train_time:1270479ms step_avg:169.90ms\nstep:7489/10000 train_loss:3.9810 train_time:1270648ms step_avg:169.90ms\nstep:7490/10000 train_loss:3.9048 train_time:1270819ms step_avg:169.90ms\nstep:7491/10000 train_loss:3.8506 train_time:1270987ms step_avg:169.90ms\nstep:7492/10000 train_loss:3.8765 train_time:1271157ms step_avg:169.90ms\nstep:7493/10000 train_loss:3.6483 train_time:1271326ms step_avg:169.90ms\nstep:7494/10000 train_loss:3.7566 train_time:1271496ms step_avg:169.90ms\nstep:7495/10000 train_loss:3.8501 train_time:1271665ms step_avg:169.90ms\nstep:7496/10000 train_loss:3.8562 train_time:1271834ms step_avg:169.89ms\nstep:7497/10000 train_loss:3.9066 train_time:1272003ms step_avg:169.89ms\nstep:7498/10000 train_loss:3.8686 train_time:1272174ms step_avg:169.90ms\nstep:7499/10000 train_loss:3.7930 train_time:1272343ms step_avg:169.89ms\nstep:7500/10000 train_loss:3.8117 train_time:1272513ms step_avg:169.89ms\nstep:7501/10000 train_loss:3.8456 train_time:1272682ms step_avg:169.89ms\nstep:7502/10000 train_loss:3.8060 train_time:1272851ms step_avg:169.89ms\nstep:7503/10000 train_loss:3.9084 train_time:1273020ms step_avg:169.89ms\nstep:7504/10000 train_loss:3.8097 train_time:1273189ms step_avg:169.89ms\nstep:7505/10000 train_loss:3.9203 train_time:1273358ms step_avg:169.89ms\nstep:7506/10000 train_loss:3.7640 train_time:1273524ms step_avg:169.89ms\nstep:7507/10000 train_loss:3.6116 train_time:1273695ms step_avg:169.89ms\nstep:7508/10000 train_loss:3.7492 train_time:1273864ms step_avg:169.89ms\nstep:7509/10000 train_loss:3.9012 train_time:1274035ms step_avg:169.89ms\nstep:7510/10000 train_loss:3.7769 train_time:1274204ms step_avg:169.89ms\nstep:7511/10000 train_loss:3.8297 train_time:1274374ms step_avg:169.89ms\nstep:7512/10000 train_loss:3.8587 train_time:1274542ms step_avg:169.89ms\nstep:7513/10000 train_loss:3.7217 train_time:1274715ms step_avg:169.89ms\nstep:7514/10000 train_loss:3.8144 train_time:1274886ms step_avg:169.89ms\nstep:7515/10000 train_loss:3.8639 train_time:1275056ms step_avg:169.89ms\nstep:7516/10000 train_loss:3.8578 train_time:1275224ms step_avg:169.89ms\nstep:7517/10000 train_loss:3.9655 train_time:1275395ms step_avg:169.89ms\nstep:7518/10000 train_loss:4.1768 train_time:1275564ms step_avg:169.89ms\nstep:7519/10000 train_loss:3.7819 train_time:1275736ms step_avg:169.89ms\nstep:7520/10000 train_loss:3.7870 train_time:1275905ms step_avg:169.89ms\nstep:7521/10000 train_loss:3.7204 train_time:1276076ms step_avg:169.89ms\nstep:7522/10000 train_loss:3.9065 train_time:1276244ms step_avg:169.89ms\nstep:7523/10000 train_loss:3.7199 train_time:1276413ms step_avg:169.89ms\nstep:7524/10000 train_loss:3.8235 train_time:1276579ms step_avg:169.89ms\nstep:7525/10000 train_loss:3.9616 train_time:1276746ms step_avg:169.89ms\nstep:7526/10000 train_loss:3.8477 train_time:1276918ms step_avg:169.89ms\nstep:7527/10000 train_loss:3.8784 train_time:1277090ms step_avg:169.89ms\nstep:7528/10000 train_loss:3.9310 train_time:1277262ms step_avg:169.89ms\nstep:7529/10000 train_loss:3.8410 train_time:1277433ms step_avg:169.89ms\nstep:7530/10000 train_loss:3.9464 train_time:1277601ms step_avg:169.89ms\nstep:7531/10000 train_loss:3.9598 train_time:1277771ms step_avg:169.89ms\nstep:7532/10000 train_loss:3.8031 train_time:1277940ms step_avg:169.89ms\nstep:7533/10000 train_loss:4.2051 train_time:1278109ms step_avg:169.89ms\nstep:7534/10000 train_loss:3.8891 train_time:1278280ms step_avg:169.89ms\nstep:7535/10000 train_loss:3.8649 train_time:1278452ms step_avg:169.89ms\nstep:7536/10000 train_loss:3.8718 train_time:1278623ms step_avg:169.89ms\nstep:7537/10000 train_loss:3.8566 train_time:1278791ms step_avg:169.89ms\nstep:7538/10000 train_loss:3.7559 train_time:1278958ms step_avg:169.89ms\nstep:7539/10000 train_loss:3.8252 train_time:1279127ms step_avg:169.89ms\nstep:7540/10000 train_loss:3.7720 train_time:1279297ms step_avg:169.89ms\nstep:7541/10000 train_loss:3.8214 train_time:1279467ms step_avg:169.89ms\nstep:7542/10000 train_loss:3.7419 train_time:1279637ms step_avg:169.89ms\nstep:7543/10000 train_loss:3.7605 train_time:1279805ms step_avg:169.89ms\nstep:7544/10000 train_loss:3.8531 train_time:1279975ms step_avg:169.89ms\nstep:7545/10000 train_loss:3.8853 train_time:1280143ms step_avg:169.89ms\nstep:7546/10000 train_loss:3.7842 train_time:1280314ms step_avg:169.89ms\nstep:7547/10000 train_loss:3.8235 train_time:1280482ms step_avg:169.89ms\nstep:7548/10000 train_loss:3.8516 train_time:1280653ms step_avg:169.89ms\nstep:7549/10000 train_loss:3.8576 train_time:1280822ms step_avg:169.89ms\nstep:7550/10000 train_loss:3.8125 train_time:1280992ms step_avg:169.89ms\nstep:7551/10000 train_loss:3.8073 train_time:1281160ms step_avg:169.89ms\nstep:7552/10000 train_loss:3.5900 train_time:1281331ms step_avg:169.89ms\nstep:7553/10000 train_loss:3.9414 train_time:1281499ms step_avg:169.89ms\nstep:7554/10000 train_loss:4.0666 train_time:1281666ms step_avg:169.89ms\nstep:7555/10000 train_loss:3.6866 train_time:1281836ms step_avg:169.89ms\nstep:7556/10000 train_loss:4.0256 train_time:1282004ms step_avg:169.89ms\nstep:7557/10000 train_loss:3.8347 train_time:1282174ms step_avg:169.89ms\nstep:7558/10000 train_loss:3.9349 train_time:1282343ms step_avg:169.89ms\nstep:7559/10000 train_loss:3.8677 train_time:1282515ms step_avg:169.89ms\nstep:7560/10000 train_loss:3.7290 train_time:1282683ms step_avg:169.89ms\nstep:7561/10000 train_loss:3.8545 train_time:1282853ms step_avg:169.89ms\nstep:7562/10000 train_loss:3.7540 train_time:1283023ms step_avg:169.89ms\nstep:7563/10000 train_loss:3.9070 train_time:1283196ms step_avg:169.89ms\nstep:7564/10000 train_loss:3.7724 train_time:1283364ms step_avg:169.89ms\nstep:7565/10000 train_loss:3.8427 train_time:1283535ms step_avg:169.89ms\nstep:7566/10000 train_loss:3.7810 train_time:1283704ms step_avg:169.89ms\nstep:7567/10000 train_loss:3.7940 train_time:1283875ms step_avg:169.89ms\nstep:7568/10000 train_loss:3.8019 train_time:1284048ms step_avg:169.89ms\nstep:7569/10000 train_loss:3.8039 train_time:1284211ms step_avg:169.89ms\nstep:7570/10000 train_loss:3.7377 train_time:1284382ms step_avg:169.89ms\nstep:7571/10000 train_loss:3.7775 train_time:1284554ms step_avg:169.89ms\nstep:7572/10000 train_loss:3.8500 train_time:1284721ms step_avg:169.89ms\nstep:7573/10000 train_loss:3.8589 train_time:1284890ms step_avg:169.89ms\nstep:7574/10000 train_loss:3.7422 train_time:1285059ms step_avg:169.89ms\nstep:7575/10000 train_loss:3.8160 train_time:1285229ms step_avg:169.89ms\nstep:7576/10000 train_loss:3.7134 train_time:1285399ms step_avg:169.89ms\nstep:7577/10000 train_loss:3.9022 train_time:1285569ms step_avg:169.89ms\nstep:7578/10000 train_loss:3.7999 train_time:1285739ms step_avg:169.89ms\nstep:7579/10000 train_loss:3.8294 train_time:1285907ms step_avg:169.89ms\nstep:7580/10000 train_loss:3.9718 train_time:1286078ms step_avg:169.89ms\nstep:7581/10000 train_loss:3.7621 train_time:1286247ms step_avg:169.89ms\nstep:7582/10000 train_loss:3.9309 train_time:1286417ms step_avg:169.89ms\nstep:7583/10000 train_loss:3.8512 train_time:1286585ms step_avg:169.89ms\nstep:7584/10000 train_loss:3.7477 train_time:1286754ms step_avg:169.89ms\nstep:7585/10000 train_loss:3.6530 train_time:1286923ms step_avg:169.89ms\nstep:7586/10000 train_loss:3.8418 train_time:1287093ms step_avg:169.89ms\nstep:7587/10000 train_loss:3.7822 train_time:1287262ms step_avg:169.89ms\nstep:7588/10000 train_loss:3.8354 train_time:1287433ms step_avg:169.89ms\nstep:7589/10000 train_loss:3.8318 train_time:1287602ms step_avg:169.89ms\nstep:7590/10000 train_loss:3.9519 train_time:1287772ms step_avg:169.89ms\nstep:7591/10000 train_loss:3.8289 train_time:1287939ms step_avg:169.89ms\nstep:7592/10000 train_loss:3.9044 train_time:1288109ms step_avg:169.89ms\nstep:7593/10000 train_loss:3.8221 train_time:1288277ms step_avg:169.89ms\nstep:7594/10000 train_loss:3.8464 train_time:1288446ms step_avg:169.89ms\nstep:7595/10000 train_loss:3.8392 train_time:1288616ms step_avg:169.89ms\nstep:7596/10000 train_loss:3.6975 train_time:1288784ms step_avg:169.89ms\nstep:7597/10000 train_loss:3.7740 train_time:1288954ms step_avg:169.89ms\nstep:7598/10000 train_loss:3.8426 train_time:1289123ms step_avg:169.89ms\nstep:7599/10000 train_loss:3.7167 train_time:1289293ms step_avg:169.89ms\nstep:7600/10000 train_loss:3.8413 train_time:1289461ms step_avg:169.89ms\nstep:7601/10000 train_loss:3.7980 train_time:1289630ms step_avg:169.89ms\nstep:7602/10000 train_loss:4.0194 train_time:1289796ms step_avg:169.89ms\nstep:7603/10000 train_loss:3.7314 train_time:1289966ms step_avg:169.89ms\nstep:7604/10000 train_loss:3.8248 train_time:1290136ms step_avg:169.89ms\nstep:7605/10000 train_loss:3.9720 train_time:1290308ms step_avg:169.89ms\nstep:7606/10000 train_loss:3.8631 train_time:1290478ms step_avg:169.89ms\nstep:7607/10000 train_loss:3.9438 train_time:1290647ms step_avg:169.89ms\nstep:7608/10000 train_loss:3.8978 train_time:1290817ms step_avg:169.89ms\nstep:7609/10000 train_loss:3.7983 train_time:1290986ms step_avg:169.89ms\nstep:7610/10000 train_loss:3.8412 train_time:1291155ms step_avg:169.89ms\nstep:7611/10000 train_loss:3.7212 train_time:1291324ms step_avg:169.89ms\nstep:7612/10000 train_loss:3.6854 train_time:1291495ms step_avg:169.89ms\nstep:7613/10000 train_loss:3.6647 train_time:1291673ms step_avg:169.89ms\nstep:7614/10000 train_loss:3.9551 train_time:1291841ms step_avg:169.89ms\nstep:7615/10000 train_loss:3.7763 train_time:1292006ms step_avg:169.89ms\nstep:7616/10000 train_loss:3.9133 train_time:1292171ms step_avg:169.89ms\nstep:7617/10000 train_loss:3.8479 train_time:1292340ms step_avg:169.89ms\nstep:7618/10000 train_loss:3.7850 train_time:1292511ms step_avg:169.89ms\nstep:7619/10000 train_loss:3.7902 train_time:1292680ms step_avg:169.89ms\nstep:7620/10000 train_loss:4.0970 train_time:1293078ms step_avg:169.92ms\nstep:7621/10000 train_loss:3.8796 train_time:1293243ms step_avg:169.92ms\nstep:7622/10000 train_loss:3.7979 train_time:1293413ms step_avg:169.92ms\nstep:7623/10000 train_loss:3.8402 train_time:1293581ms step_avg:169.92ms\nstep:7624/10000 train_loss:3.7960 train_time:1293748ms step_avg:169.92ms\nstep:7625/10000 train_loss:3.6713 train_time:1293917ms step_avg:169.92ms\nstep:7626/10000 train_loss:3.9465 train_time:1294091ms step_avg:169.92ms\nstep:7627/10000 train_loss:3.7981 train_time:1294264ms step_avg:169.92ms\nstep:7628/10000 train_loss:3.7819 train_time:1294433ms step_avg:169.92ms\nstep:7629/10000 train_loss:3.8876 train_time:1294602ms step_avg:169.92ms\nstep:7630/10000 train_loss:3.7786 train_time:1294770ms step_avg:169.92ms\nstep:7631/10000 train_loss:3.9301 train_time:1294941ms step_avg:169.92ms\nstep:7632/10000 train_loss:3.7903 train_time:1295114ms step_avg:169.92ms\nstep:7633/10000 train_loss:3.9497 train_time:1295283ms step_avg:169.92ms\nstep:7634/10000 train_loss:4.0463 train_time:1295454ms step_avg:169.92ms\nstep:7635/10000 train_loss:3.7662 train_time:1295621ms step_avg:169.92ms\nstep:7636/10000 train_loss:3.8063 train_time:1295789ms step_avg:169.92ms\nstep:7637/10000 train_loss:3.8850 train_time:1295955ms step_avg:169.92ms\nstep:7638/10000 train_loss:3.7900 train_time:1296124ms step_avg:169.92ms\nstep:7639/10000 train_loss:3.7874 train_time:1296292ms step_avg:169.92ms\nstep:7640/10000 train_loss:3.8152 train_time:1296461ms step_avg:169.92ms\nstep:7641/10000 train_loss:3.8245 train_time:1296632ms step_avg:169.92ms\nstep:7642/10000 train_loss:3.7719 train_time:1296801ms step_avg:169.92ms\nstep:7643/10000 train_loss:3.7693 train_time:1296972ms step_avg:169.92ms\nstep:7644/10000 train_loss:3.9172 train_time:1297143ms step_avg:169.92ms\nstep:7645/10000 train_loss:3.8354 train_time:1297314ms step_avg:169.92ms\nstep:7646/10000 train_loss:3.8665 train_time:1297483ms step_avg:169.92ms\nstep:7647/10000 train_loss:3.6965 train_time:1297652ms step_avg:169.92ms\nstep:7648/10000 train_loss:3.8316 train_time:1297821ms step_avg:169.92ms\nstep:7649/10000 train_loss:3.8305 train_time:1297992ms step_avg:169.92ms\nstep:7650/10000 train_loss:3.7574 train_time:1298161ms step_avg:169.92ms\nstep:7651/10000 train_loss:3.8505 train_time:1298332ms step_avg:169.92ms\nstep:7652/10000 train_loss:3.8528 train_time:1298502ms step_avg:169.92ms\nstep:7653/10000 train_loss:3.8189 train_time:1298673ms step_avg:169.92ms\nstep:7654/10000 train_loss:3.8084 train_time:1298842ms step_avg:169.92ms\nstep:7655/10000 train_loss:3.8068 train_time:1299013ms step_avg:169.92ms\nstep:7656/10000 train_loss:3.8832 train_time:1299183ms step_avg:169.92ms\nstep:7657/10000 train_loss:3.7856 train_time:1299352ms step_avg:169.92ms\nstep:7658/10000 train_loss:3.8934 train_time:1299523ms step_avg:169.92ms\nstep:7659/10000 train_loss:3.7699 train_time:1299694ms step_avg:169.92ms\nstep:7660/10000 train_loss:3.8017 train_time:1299861ms step_avg:169.92ms\nstep:7661/10000 train_loss:3.7334 train_time:1300031ms step_avg:169.92ms\nstep:7662/10000 train_loss:3.8287 train_time:1300201ms step_avg:169.92ms\nstep:7663/10000 train_loss:3.6876 train_time:1300373ms step_avg:169.92ms\nstep:7664/10000 train_loss:3.9332 train_time:1300544ms step_avg:169.92ms\nstep:7665/10000 train_loss:3.8570 train_time:1300715ms step_avg:169.92ms\nstep:7666/10000 train_loss:3.6923 train_time:1300881ms step_avg:169.92ms\nstep:7667/10000 train_loss:3.7178 train_time:1301047ms step_avg:169.92ms\nstep:7668/10000 train_loss:3.7103 train_time:1301219ms step_avg:169.92ms\nstep:7669/10000 train_loss:3.7886 train_time:1301388ms step_avg:169.92ms\nstep:7670/10000 train_loss:3.6197 train_time:1301557ms step_avg:169.92ms\nstep:7671/10000 train_loss:3.9315 train_time:1301727ms step_avg:169.92ms\nstep:7672/10000 train_loss:4.2796 train_time:1301896ms step_avg:169.92ms\nstep:7673/10000 train_loss:3.7945 train_time:1302064ms step_avg:169.92ms\nstep:7674/10000 train_loss:3.7869 train_time:1302234ms step_avg:169.92ms\nstep:7675/10000 train_loss:3.8111 train_time:1302402ms step_avg:169.92ms\nstep:7676/10000 train_loss:5.3140 train_time:1302573ms step_avg:169.92ms\nstep:7677/10000 train_loss:3.8332 train_time:1302742ms step_avg:169.92ms\nstep:7678/10000 train_loss:4.0254 train_time:1302913ms step_avg:169.92ms\nstep:7679/10000 train_loss:3.7568 train_time:1303080ms step_avg:169.92ms\nstep:7680/10000 train_loss:3.7469 train_time:1303248ms step_avg:169.92ms\nstep:7681/10000 train_loss:3.6826 train_time:1303416ms step_avg:169.91ms\nstep:7682/10000 train_loss:3.9705 train_time:1303584ms step_avg:169.91ms\nstep:7683/10000 train_loss:3.8308 train_time:1303754ms step_avg:169.91ms\nstep:7684/10000 train_loss:3.6123 train_time:1303924ms step_avg:169.91ms\nstep:7685/10000 train_loss:3.8899 train_time:1304093ms step_avg:169.91ms\nstep:7686/10000 train_loss:3.9216 train_time:1304261ms step_avg:169.91ms\nstep:7687/10000 train_loss:3.7827 train_time:1304429ms step_avg:169.91ms\nstep:7688/10000 train_loss:3.8508 train_time:1304597ms step_avg:169.91ms\nstep:7689/10000 train_loss:3.7835 train_time:1304768ms step_avg:169.91ms\nstep:7690/10000 train_loss:3.8046 train_time:1304940ms step_avg:169.91ms\nstep:7691/10000 train_loss:3.7993 train_time:1305112ms step_avg:169.91ms\nstep:7692/10000 train_loss:3.9026 train_time:1305282ms step_avg:169.91ms\nstep:7693/10000 train_loss:3.8392 train_time:1305451ms step_avg:169.91ms\nstep:7694/10000 train_loss:3.9953 train_time:1305619ms step_avg:169.91ms\nstep:7695/10000 train_loss:3.8662 train_time:1305788ms step_avg:169.91ms\nstep:7696/10000 train_loss:3.8594 train_time:1305956ms step_avg:169.91ms\nstep:7697/10000 train_loss:3.8055 train_time:1306124ms step_avg:169.91ms\nstep:7698/10000 train_loss:3.8335 train_time:1306292ms step_avg:169.91ms\nstep:7699/10000 train_loss:3.8224 train_time:1306461ms step_avg:169.91ms\nstep:7700/10000 train_loss:3.7489 train_time:1306631ms step_avg:169.91ms\nstep:7701/10000 train_loss:3.7761 train_time:1306799ms step_avg:169.91ms\nstep:7702/10000 train_loss:3.7510 train_time:1306968ms step_avg:169.91ms\nstep:7703/10000 train_loss:3.9203 train_time:1307136ms step_avg:169.91ms\nstep:7704/10000 train_loss:3.7393 train_time:1307304ms step_avg:169.91ms\nstep:7705/10000 train_loss:3.8669 train_time:1307473ms step_avg:169.91ms\nstep:7706/10000 train_loss:3.9334 train_time:1307645ms step_avg:169.91ms\nstep:7707/10000 train_loss:3.7376 train_time:1307810ms step_avg:169.91ms\nstep:7708/10000 train_loss:3.8522 train_time:1307980ms step_avg:169.91ms\nstep:7709/10000 train_loss:3.7275 train_time:1308149ms step_avg:169.91ms\nstep:7710/10000 train_loss:3.8994 train_time:1308317ms step_avg:169.91ms\nstep:7711/10000 train_loss:3.8548 train_time:1308486ms step_avg:169.91ms\nstep:7712/10000 train_loss:3.7659 train_time:1308657ms step_avg:169.91ms\nstep:7713/10000 train_loss:3.8912 train_time:1308826ms step_avg:169.91ms\nstep:7714/10000 train_loss:3.7287 train_time:1308997ms step_avg:169.91ms\nstep:7715/10000 train_loss:3.8057 train_time:1309164ms step_avg:169.91ms\nstep:7716/10000 train_loss:3.8126 train_time:1309333ms step_avg:169.91ms\nstep:7717/10000 train_loss:3.8445 train_time:1309503ms step_avg:169.91ms\nstep:7718/10000 train_loss:3.7772 train_time:1309673ms step_avg:169.91ms\nstep:7719/10000 train_loss:3.8457 train_time:1309844ms step_avg:169.91ms\nstep:7720/10000 train_loss:3.8711 train_time:1310012ms step_avg:169.91ms\nstep:7721/10000 train_loss:3.7907 train_time:1310182ms step_avg:169.91ms\nstep:7722/10000 train_loss:3.6859 train_time:1310352ms step_avg:169.91ms\nstep:7723/10000 train_loss:3.7730 train_time:1310520ms step_avg:169.91ms\nstep:7724/10000 train_loss:3.9315 train_time:1310690ms step_avg:169.91ms\nstep:7725/10000 train_loss:3.8403 train_time:1310858ms step_avg:169.91ms\nstep:7726/10000 train_loss:3.7634 train_time:1311027ms step_avg:169.91ms\nstep:7727/10000 train_loss:3.8703 train_time:1311199ms step_avg:169.91ms\nstep:7728/10000 train_loss:3.7152 train_time:1311369ms step_avg:169.91ms\nstep:7729/10000 train_loss:4.1361 train_time:1311539ms step_avg:169.91ms\nstep:7730/10000 train_loss:3.6953 train_time:1311707ms step_avg:169.91ms\nstep:7731/10000 train_loss:4.0310 train_time:1311877ms step_avg:169.91ms\nstep:7732/10000 train_loss:3.8187 train_time:1312046ms step_avg:169.91ms\nstep:7733/10000 train_loss:3.8969 train_time:1312216ms step_avg:169.91ms\nstep:7734/10000 train_loss:3.7566 train_time:1312385ms step_avg:169.91ms\nstep:7735/10000 train_loss:3.8414 train_time:1312555ms step_avg:169.91ms\nstep:7736/10000 train_loss:3.9767 train_time:1312723ms step_avg:169.91ms\nstep:7737/10000 train_loss:3.8904 train_time:1312895ms step_avg:169.91ms\nstep:7738/10000 train_loss:3.8851 train_time:1313062ms step_avg:169.91ms\nstep:7739/10000 train_loss:4.0199 train_time:1313230ms step_avg:169.91ms\nstep:7740/10000 train_loss:3.8126 train_time:1313400ms step_avg:169.91ms\nstep:7741/10000 train_loss:3.8564 train_time:1313569ms step_avg:169.91ms\nstep:7742/10000 train_loss:3.7678 train_time:1313742ms step_avg:169.91ms\nstep:7743/10000 train_loss:3.7416 train_time:1313912ms step_avg:169.91ms\nstep:7744/10000 train_loss:3.6614 train_time:1314081ms step_avg:169.91ms\nstep:7745/10000 train_loss:3.7161 train_time:1314252ms step_avg:169.91ms\nstep:7746/10000 train_loss:3.8912 train_time:1314421ms step_avg:169.91ms\nstep:7747/10000 train_loss:3.6749 train_time:1314590ms step_avg:169.91ms\nstep:7748/10000 train_loss:5.6038 train_time:1314760ms step_avg:169.91ms\nstep:7749/10000 train_loss:3.9305 train_time:1314931ms step_avg:169.91ms\nstep:7750/10000 train_loss:3.8176 train_time:1315099ms step_avg:169.91ms\nstep:7751/10000 train_loss:3.7476 train_time:1315269ms step_avg:169.91ms\nstep:7752/10000 train_loss:3.8935 train_time:1315439ms step_avg:169.91ms\nstep:7753/10000 train_loss:3.7017 train_time:1315609ms step_avg:169.91ms\nstep:7754/10000 train_loss:3.8324 train_time:1315780ms step_avg:169.91ms\nstep:7755/10000 train_loss:3.8156 train_time:1315949ms step_avg:169.91ms\nstep:7756/10000 train_loss:3.8235 train_time:1316118ms step_avg:169.91ms\nstep:7757/10000 train_loss:3.8514 train_time:1316288ms step_avg:169.91ms\nstep:7758/10000 train_loss:3.8847 train_time:1316461ms step_avg:169.91ms\nstep:7759/10000 train_loss:3.8284 train_time:1316632ms step_avg:169.91ms\nstep:7760/10000 train_loss:4.0987 train_time:1316802ms step_avg:169.91ms\nstep:7761/10000 train_loss:3.7936 train_time:1316975ms step_avg:169.91ms\nstep:7762/10000 train_loss:3.8204 train_time:1317141ms step_avg:169.91ms\nstep:7763/10000 train_loss:3.8790 train_time:1317309ms step_avg:169.91ms\nstep:7764/10000 train_loss:3.8492 train_time:1317479ms step_avg:169.91ms\nstep:7765/10000 train_loss:3.8039 train_time:1317647ms step_avg:169.91ms\nstep:7766/10000 train_loss:3.8496 train_time:1317818ms step_avg:169.91ms\nstep:7767/10000 train_loss:3.7818 train_time:1317988ms step_avg:169.91ms\nstep:7768/10000 train_loss:3.8289 train_time:1318158ms step_avg:169.91ms\nstep:7769/10000 train_loss:3.7969 train_time:1318326ms step_avg:169.91ms\nstep:7770/10000 train_loss:3.7893 train_time:1318497ms step_avg:169.91ms\nstep:7771/10000 train_loss:3.8511 train_time:1318664ms step_avg:169.91ms\nstep:7772/10000 train_loss:3.8296 train_time:1318837ms step_avg:169.91ms\nstep:7773/10000 train_loss:3.7573 train_time:1319008ms step_avg:169.91ms\nstep:7774/10000 train_loss:3.8724 train_time:1319179ms step_avg:169.91ms\nstep:7775/10000 train_loss:3.8177 train_time:1319348ms step_avg:169.91ms\nstep:7776/10000 train_loss:3.8595 train_time:1319517ms step_avg:169.91ms\nstep:7777/10000 train_loss:3.7527 train_time:1319685ms step_avg:169.91ms\nstep:7778/10000 train_loss:3.9037 train_time:1319854ms step_avg:169.91ms\nstep:7779/10000 train_loss:3.8783 train_time:1320022ms step_avg:169.91ms\nstep:7780/10000 train_loss:3.7240 train_time:1320194ms step_avg:169.91ms\nstep:7781/10000 train_loss:3.9421 train_time:1320362ms step_avg:169.91ms\nstep:7782/10000 train_loss:3.7286 train_time:1320533ms step_avg:169.91ms\nstep:7783/10000 train_loss:3.8051 train_time:1320699ms step_avg:169.91ms\nstep:7784/10000 train_loss:3.8681 train_time:1320868ms step_avg:169.91ms\nstep:7785/10000 train_loss:3.8704 train_time:1321036ms step_avg:169.91ms\nstep:7786/10000 train_loss:3.7507 train_time:1321206ms step_avg:169.91ms\nstep:7787/10000 train_loss:3.7689 train_time:1321379ms step_avg:169.91ms\nstep:7788/10000 train_loss:3.6484 train_time:1321548ms step_avg:169.91ms\nstep:7789/10000 train_loss:3.7936 train_time:1321719ms step_avg:169.91ms\nstep:7790/10000 train_loss:3.8412 train_time:1321886ms step_avg:169.91ms\nstep:7791/10000 train_loss:3.8431 train_time:1322056ms step_avg:169.91ms\nstep:7792/10000 train_loss:4.0008 train_time:1322224ms step_avg:169.91ms\nstep:7793/10000 train_loss:3.9434 train_time:1322394ms step_avg:169.91ms\nstep:7794/10000 train_loss:3.6894 train_time:1322563ms step_avg:169.91ms\nstep:7795/10000 train_loss:3.7504 train_time:1322735ms step_avg:169.91ms\nstep:7796/10000 train_loss:3.6716 train_time:1322904ms step_avg:169.91ms\nstep:7797/10000 train_loss:3.8635 train_time:1323074ms step_avg:169.91ms\nstep:7798/10000 train_loss:3.9372 train_time:1323241ms step_avg:169.91ms\nstep:7799/10000 train_loss:3.6901 train_time:1323410ms step_avg:169.91ms\nstep:7800/10000 train_loss:3.8823 train_time:1323580ms step_avg:169.91ms\nstep:7801/10000 train_loss:3.8177 train_time:1323748ms step_avg:169.91ms\nstep:7802/10000 train_loss:3.8182 train_time:1323920ms step_avg:169.91ms\nstep:7803/10000 train_loss:3.8268 train_time:1324090ms step_avg:169.91ms\nstep:7804/10000 train_loss:3.8406 train_time:1324262ms step_avg:169.91ms\nstep:7805/10000 train_loss:3.9410 train_time:1324433ms step_avg:169.91ms\nstep:7806/10000 train_loss:3.8255 train_time:1324603ms step_avg:169.91ms\nstep:7807/10000 train_loss:3.7377 train_time:1324773ms step_avg:169.91ms\nstep:7808/10000 train_loss:3.7257 train_time:1324940ms step_avg:169.91ms\nstep:7809/10000 train_loss:3.7260 train_time:1325109ms step_avg:169.91ms\nstep:7810/10000 train_loss:3.8137 train_time:1325281ms step_avg:169.91ms\nstep:7811/10000 train_loss:3.8256 train_time:1325452ms step_avg:169.91ms\nstep:7812/10000 train_loss:3.9275 train_time:1325620ms step_avg:169.91ms\nstep:7813/10000 train_loss:3.9414 train_time:1325789ms step_avg:169.91ms\nstep:7814/10000 train_loss:3.7027 train_time:1325959ms step_avg:169.91ms\nstep:7815/10000 train_loss:3.6916 train_time:1326127ms step_avg:169.91ms\nstep:7816/10000 train_loss:3.8614 train_time:1326300ms step_avg:169.91ms\nstep:7817/10000 train_loss:3.8526 train_time:1326469ms step_avg:169.91ms\nstep:7818/10000 train_loss:3.8219 train_time:1326638ms step_avg:169.91ms\nstep:7819/10000 train_loss:3.8292 train_time:1326807ms step_avg:169.91ms\nstep:7820/10000 train_loss:3.7829 train_time:1326980ms step_avg:169.91ms\nstep:7821/10000 train_loss:3.8732 train_time:1327147ms step_avg:169.91ms\nstep:7822/10000 train_loss:3.6618 train_time:1327318ms step_avg:169.91ms\nstep:7823/10000 train_loss:3.8441 train_time:1327486ms step_avg:169.91ms\nstep:7824/10000 train_loss:3.8403 train_time:1327658ms step_avg:169.91ms\nstep:7825/10000 train_loss:3.9394 train_time:1327825ms step_avg:169.91ms\nstep:7826/10000 train_loss:3.8616 train_time:1327995ms step_avg:169.91ms\nstep:7827/10000 train_loss:3.7709 train_time:1328164ms step_avg:169.91ms\nstep:7828/10000 train_loss:3.7523 train_time:1328333ms step_avg:169.91ms\nstep:7829/10000 train_loss:3.8269 train_time:1328503ms step_avg:169.91ms\nstep:7830/10000 train_loss:3.8731 train_time:1328673ms step_avg:169.91ms\nstep:7831/10000 train_loss:3.9081 train_time:1328842ms step_avg:169.91ms\nstep:7832/10000 train_loss:3.7372 train_time:1329014ms step_avg:169.91ms\nstep:7833/10000 train_loss:3.8613 train_time:1329182ms step_avg:169.91ms\nstep:7834/10000 train_loss:3.8110 train_time:1329351ms step_avg:169.91ms\nstep:7835/10000 train_loss:3.7281 train_time:1329523ms step_avg:169.91ms\nstep:7836/10000 train_loss:3.7254 train_time:1329694ms step_avg:169.91ms\nstep:7837/10000 train_loss:3.7395 train_time:1329862ms step_avg:169.91ms\nstep:7838/10000 train_loss:3.7070 train_time:1330033ms step_avg:169.91ms\nstep:7839/10000 train_loss:3.9178 train_time:1330201ms step_avg:169.91ms\nstep:7840/10000 train_loss:3.8617 train_time:1330369ms step_avg:169.91ms\nstep:7841/10000 train_loss:3.9652 train_time:1330541ms step_avg:169.91ms\nstep:7842/10000 train_loss:3.7804 train_time:1330712ms step_avg:169.91ms\nstep:7843/10000 train_loss:3.7252 train_time:1330882ms step_avg:169.91ms\nstep:7844/10000 train_loss:3.8826 train_time:1331052ms step_avg:169.91ms\nstep:7845/10000 train_loss:3.9546 train_time:1331221ms step_avg:169.91ms\nstep:7846/10000 train_loss:3.9693 train_time:1331391ms step_avg:169.91ms\nstep:7847/10000 train_loss:3.8646 train_time:1331563ms step_avg:169.91ms\nstep:7848/10000 train_loss:3.6697 train_time:1331733ms step_avg:169.91ms\nstep:7849/10000 train_loss:3.8063 train_time:1331901ms step_avg:169.91ms\nstep:7850/10000 train_loss:3.8953 train_time:1332072ms step_avg:169.91ms\nstep:7851/10000 train_loss:3.8215 train_time:1332239ms step_avg:169.91ms\nstep:7852/10000 train_loss:3.8761 train_time:1332408ms step_avg:169.91ms\nstep:7853/10000 train_loss:3.7840 train_time:1332579ms step_avg:169.91ms\nstep:7854/10000 train_loss:3.9021 train_time:1332749ms step_avg:169.91ms\nstep:7855/10000 train_loss:3.8410 train_time:1332919ms step_avg:169.91ms\nstep:7856/10000 train_loss:3.8429 train_time:1333088ms step_avg:169.91ms\nstep:7857/10000 train_loss:4.0236 train_time:1333258ms step_avg:169.91ms\nstep:7858/10000 train_loss:3.7286 train_time:1333427ms step_avg:169.91ms\nstep:7859/10000 train_loss:3.9842 train_time:1333600ms step_avg:169.91ms\nstep:7860/10000 train_loss:3.6809 train_time:1333765ms step_avg:169.91ms\nstep:7861/10000 train_loss:3.8391 train_time:1333933ms step_avg:169.91ms\nstep:7862/10000 train_loss:3.8385 train_time:1334102ms step_avg:169.91ms\nstep:7863/10000 train_loss:3.8715 train_time:1334271ms step_avg:169.91ms\nstep:7864/10000 train_loss:3.9928 train_time:1334442ms step_avg:169.91ms\nstep:7865/10000 train_loss:3.8554 train_time:1334611ms step_avg:169.91ms\nstep:7866/10000 train_loss:3.8696 train_time:1334780ms step_avg:169.91ms\nstep:7867/10000 train_loss:3.7965 train_time:1334949ms step_avg:169.91ms\nstep:7868/10000 train_loss:3.8404 train_time:1335119ms step_avg:169.91ms\nstep:7869/10000 train_loss:3.7555 train_time:1335287ms step_avg:169.91ms\nstep:7870/10000 train_loss:3.8350 train_time:1335459ms step_avg:169.91ms\nstep:7871/10000 train_loss:3.8041 train_time:1335630ms step_avg:169.91ms\nstep:7872/10000 train_loss:3.7366 train_time:1335800ms step_avg:169.91ms\nstep:7873/10000 train_loss:3.8996 train_time:1335968ms step_avg:169.91ms\nstep:7874/10000 train_loss:3.7194 train_time:1336139ms step_avg:169.91ms\nstep:7875/10000 train_loss:3.8159 train_time:1336309ms step_avg:169.91ms\nstep:7876/10000 train_loss:3.7703 train_time:1336480ms step_avg:169.91ms\nstep:7877/10000 train_loss:3.8635 train_time:1336650ms step_avg:169.91ms\nstep:7878/10000 train_loss:4.1195 train_time:1336819ms step_avg:169.91ms\nstep:7879/10000 train_loss:3.7896 train_time:1336987ms step_avg:169.91ms\nstep:7880/10000 train_loss:3.8835 train_time:1337157ms step_avg:169.91ms\nstep:7881/10000 train_loss:3.7978 train_time:1337324ms step_avg:169.91ms\nstep:7882/10000 train_loss:3.7837 train_time:1337495ms step_avg:169.91ms\nstep:7883/10000 train_loss:3.8197 train_time:1337665ms step_avg:169.91ms\nstep:7884/10000 train_loss:3.9953 train_time:1337832ms step_avg:169.91ms\nstep:7885/10000 train_loss:3.9496 train_time:1338001ms step_avg:169.90ms\nstep:7886/10000 train_loss:3.7775 train_time:1338173ms step_avg:169.91ms\nstep:7887/10000 train_loss:3.8425 train_time:1338342ms step_avg:169.91ms\nstep:7888/10000 train_loss:3.6344 train_time:1338512ms step_avg:169.91ms\nstep:7889/10000 train_loss:3.8446 train_time:1338682ms step_avg:169.91ms\nstep:7890/10000 train_loss:3.8469 train_time:1338851ms step_avg:169.90ms\nstep:7891/10000 train_loss:3.8126 train_time:1339019ms step_avg:169.90ms\nstep:7892/10000 train_loss:3.8182 train_time:1339188ms step_avg:169.90ms\nstep:7893/10000 train_loss:3.8592 train_time:1339358ms step_avg:169.90ms\nstep:7894/10000 train_loss:3.8012 train_time:1339527ms step_avg:169.90ms\nstep:7895/10000 train_loss:3.8884 train_time:1339695ms step_avg:169.90ms\nstep:7896/10000 train_loss:3.8934 train_time:1339865ms step_avg:169.90ms\nstep:7897/10000 train_loss:3.6368 train_time:1340031ms step_avg:169.90ms\nstep:7898/10000 train_loss:3.7165 train_time:1340202ms step_avg:169.90ms\nstep:7899/10000 train_loss:3.9084 train_time:1340373ms step_avg:169.90ms\nstep:7900/10000 train_loss:3.7969 train_time:1340541ms step_avg:169.90ms\nstep:7901/10000 train_loss:3.7429 train_time:1340713ms step_avg:169.90ms\nstep:7902/10000 train_loss:3.6870 train_time:1340883ms step_avg:169.90ms\nstep:7903/10000 train_loss:3.8525 train_time:1341053ms step_avg:169.90ms\nstep:7904/10000 train_loss:3.9908 train_time:1341222ms step_avg:169.90ms\nstep:7905/10000 train_loss:3.9225 train_time:1341392ms step_avg:169.90ms\nstep:7906/10000 train_loss:3.7926 train_time:1341561ms step_avg:169.90ms\nstep:7907/10000 train_loss:3.7048 train_time:1341731ms step_avg:169.90ms\nstep:7908/10000 train_loss:3.8285 train_time:1341899ms step_avg:169.90ms\nstep:7909/10000 train_loss:3.8549 train_time:1342068ms step_avg:169.90ms\nstep:7910/10000 train_loss:3.7911 train_time:1342239ms step_avg:169.90ms\nstep:7911/10000 train_loss:3.6053 train_time:1342408ms step_avg:169.90ms\nstep:7912/10000 train_loss:3.8802 train_time:1342580ms step_avg:169.90ms\nstep:7913/10000 train_loss:3.6582 train_time:1342750ms step_avg:169.90ms\nstep:7914/10000 train_loss:3.7086 train_time:1342921ms step_avg:169.90ms\nstep:7915/10000 train_loss:3.8006 train_time:1343090ms step_avg:169.90ms\nstep:7916/10000 train_loss:3.8187 train_time:1343261ms step_avg:169.90ms\nstep:7917/10000 train_loss:3.8046 train_time:1343434ms step_avg:169.90ms\nstep:7918/10000 train_loss:3.8796 train_time:1343602ms step_avg:169.90ms\nstep:7919/10000 train_loss:3.8644 train_time:1343771ms step_avg:169.90ms\nstep:7920/10000 train_loss:3.7863 train_time:1343942ms step_avg:169.90ms\nstep:7921/10000 train_loss:3.8501 train_time:1344111ms step_avg:169.90ms\nstep:7922/10000 train_loss:3.7519 train_time:1344281ms step_avg:169.90ms\nstep:7923/10000 train_loss:3.6786 train_time:1344451ms step_avg:169.90ms\nstep:7924/10000 train_loss:3.8772 train_time:1344618ms step_avg:169.90ms\nstep:7925/10000 train_loss:3.7943 train_time:1344787ms step_avg:169.90ms\nstep:7926/10000 train_loss:4.1253 train_time:1344956ms step_avg:169.90ms\nstep:7927/10000 train_loss:3.9038 train_time:1345126ms step_avg:169.90ms\nstep:7928/10000 train_loss:3.8718 train_time:1345298ms step_avg:169.90ms\nstep:7929/10000 train_loss:3.7922 train_time:1345468ms step_avg:169.90ms\nstep:7930/10000 train_loss:3.8346 train_time:1345637ms step_avg:169.90ms\nstep:7931/10000 train_loss:3.8105 train_time:1345805ms step_avg:169.90ms\nstep:7932/10000 train_loss:3.8959 train_time:1345975ms step_avg:169.90ms\nstep:7933/10000 train_loss:3.7686 train_time:1346142ms step_avg:169.90ms\nstep:7934/10000 train_loss:3.8087 train_time:1346313ms step_avg:169.90ms\nstep:7935/10000 train_loss:3.8178 train_time:1346482ms step_avg:169.90ms\nstep:7936/10000 train_loss:3.7803 train_time:1346652ms step_avg:169.90ms\nstep:7937/10000 train_loss:3.8288 train_time:1346820ms step_avg:169.90ms\nstep:7938/10000 train_loss:3.7984 train_time:1346988ms step_avg:169.90ms\nstep:7939/10000 train_loss:3.8589 train_time:1347158ms step_avg:169.90ms\nstep:7940/10000 train_loss:3.7472 train_time:1347327ms step_avg:169.90ms\nstep:7941/10000 train_loss:3.8572 train_time:1347498ms step_avg:169.90ms\nstep:7942/10000 train_loss:3.8401 train_time:1347667ms step_avg:169.90ms\nstep:7943/10000 train_loss:3.7851 train_time:1347836ms step_avg:169.90ms\nstep:7944/10000 train_loss:3.9290 train_time:1348004ms step_avg:169.90ms\nstep:7945/10000 train_loss:3.8038 train_time:1348174ms step_avg:169.90ms\nstep:7946/10000 train_loss:3.7818 train_time:1348341ms step_avg:169.90ms\nstep:7947/10000 train_loss:3.8460 train_time:1348513ms step_avg:169.90ms\nstep:7948/10000 train_loss:3.9088 train_time:1348683ms step_avg:169.90ms\nstep:7949/10000 train_loss:3.9361 train_time:1348852ms step_avg:169.90ms\nstep:7950/10000 train_loss:3.7613 train_time:1349021ms step_avg:169.90ms\nstep:7951/10000 train_loss:3.8591 train_time:1349190ms step_avg:169.90ms\nstep:7952/10000 train_loss:4.0814 train_time:1349358ms step_avg:169.90ms\nstep:7953/10000 train_loss:3.7841 train_time:1349526ms step_avg:169.90ms\nstep:7954/10000 train_loss:3.7901 train_time:1349701ms step_avg:169.90ms\nstep:7955/10000 train_loss:3.5684 train_time:1349870ms step_avg:169.90ms\nstep:7956/10000 train_loss:3.8882 train_time:1350040ms step_avg:169.90ms\nstep:7957/10000 train_loss:3.7849 train_time:1350210ms step_avg:169.90ms\nstep:7958/10000 train_loss:3.8448 train_time:1350378ms step_avg:169.90ms\nstep:7959/10000 train_loss:3.9173 train_time:1350546ms step_avg:169.90ms\nstep:7960/10000 train_loss:3.7759 train_time:1350716ms step_avg:169.90ms\nstep:7961/10000 train_loss:3.8154 train_time:1350883ms step_avg:169.90ms\nstep:7962/10000 train_loss:3.8496 train_time:1351052ms step_avg:169.90ms\nstep:7963/10000 train_loss:3.7620 train_time:1351223ms step_avg:169.90ms\nstep:7964/10000 train_loss:3.8859 train_time:1351393ms step_avg:169.90ms\nstep:7965/10000 train_loss:3.8651 train_time:1351561ms step_avg:169.90ms\nstep:7966/10000 train_loss:3.7922 train_time:1351730ms step_avg:169.90ms\nstep:7967/10000 train_loss:3.7320 train_time:1351902ms step_avg:169.90ms\nstep:7968/10000 train_loss:3.7840 train_time:1352073ms step_avg:169.90ms\nstep:7969/10000 train_loss:3.8695 train_time:1352241ms step_avg:169.90ms\nstep:7970/10000 train_loss:3.8856 train_time:1352412ms step_avg:169.90ms\nstep:7971/10000 train_loss:3.7931 train_time:1352581ms step_avg:169.90ms\nstep:7972/10000 train_loss:3.9152 train_time:1352749ms step_avg:169.90ms\nstep:7973/10000 train_loss:3.8936 train_time:1352920ms step_avg:169.90ms\nstep:7974/10000 train_loss:3.9102 train_time:1353089ms step_avg:169.90ms\nstep:7975/10000 train_loss:3.7105 train_time:1353261ms step_avg:169.90ms\nstep:7976/10000 train_loss:3.8451 train_time:1353431ms step_avg:169.90ms\nstep:7977/10000 train_loss:3.6461 train_time:1353603ms step_avg:169.90ms\nstep:7978/10000 train_loss:3.6680 train_time:1353770ms step_avg:169.90ms\nstep:7979/10000 train_loss:3.8409 train_time:1353939ms step_avg:169.90ms\nstep:7980/10000 train_loss:3.6807 train_time:1354108ms step_avg:169.90ms\nstep:7981/10000 train_loss:4.0377 train_time:1354276ms step_avg:169.90ms\nstep:7982/10000 train_loss:3.7069 train_time:1354445ms step_avg:169.90ms\nstep:7983/10000 train_loss:3.8910 train_time:1354614ms step_avg:169.90ms\nstep:7984/10000 train_loss:3.8677 train_time:1354783ms step_avg:169.90ms\nstep:7985/10000 train_loss:3.8498 train_time:1354953ms step_avg:169.90ms\nstep:7986/10000 train_loss:3.8859 train_time:1355120ms step_avg:169.90ms\nstep:7987/10000 train_loss:3.8490 train_time:1355291ms step_avg:169.90ms\nstep:7988/10000 train_loss:3.8434 train_time:1355463ms step_avg:169.90ms\nstep:7989/10000 train_loss:3.8653 train_time:1355631ms step_avg:169.90ms\nstep:7990/10000 train_loss:3.6849 train_time:1355799ms step_avg:169.90ms\nstep:7991/10000 train_loss:3.9011 train_time:1355968ms step_avg:169.90ms\nstep:7992/10000 train_loss:3.8513 train_time:1356140ms step_avg:169.90ms\nstep:7993/10000 train_loss:3.9140 train_time:1356308ms step_avg:169.90ms\nstep:7994/10000 train_loss:3.8087 train_time:1356478ms step_avg:169.90ms\nstep:7995/10000 train_loss:3.7481 train_time:1356647ms step_avg:169.90ms\nstep:7996/10000 train_loss:4.4600 train_time:1356816ms step_avg:169.90ms\nstep:7997/10000 train_loss:3.7828 train_time:1356983ms step_avg:169.90ms\nstep:7998/10000 train_loss:3.8709 train_time:1357151ms step_avg:169.90ms\nstep:7999/10000 train_loss:3.9427 train_time:1357321ms step_avg:169.90ms\nstep:8000/10000 train_loss:3.8032 train_time:1357491ms step_avg:169.90ms\nstep:8001/10000 train_loss:3.7100 train_time:1357659ms step_avg:169.90ms\nstep:8002/10000 train_loss:3.7181 train_time:1357829ms step_avg:169.90ms\nstep:8003/10000 train_loss:3.9722 train_time:1357999ms step_avg:169.90ms\nstep:8004/10000 train_loss:3.8559 train_time:1358167ms step_avg:169.90ms\nstep:8005/10000 train_loss:4.0877 train_time:1358334ms step_avg:169.90ms\nstep:8006/10000 train_loss:3.8290 train_time:1358503ms step_avg:169.90ms\nstep:8007/10000 train_loss:3.7946 train_time:1358672ms step_avg:169.90ms\nstep:8008/10000 train_loss:3.7862 train_time:1358839ms step_avg:169.90ms\nstep:8009/10000 train_loss:4.1201 train_time:1359008ms step_avg:169.90ms\nstep:8010/10000 train_loss:3.8624 train_time:1359177ms step_avg:169.90ms\nstep:8011/10000 train_loss:3.7940 train_time:1359345ms step_avg:169.90ms\nstep:8012/10000 train_loss:3.8964 train_time:1359515ms step_avg:169.90ms\nstep:8013/10000 train_loss:3.8074 train_time:1359685ms step_avg:169.90ms\nstep:8014/10000 train_loss:3.7655 train_time:1359856ms step_avg:169.90ms\nstep:8015/10000 train_loss:4.3371 train_time:1360024ms step_avg:169.90ms\nstep:8016/10000 train_loss:3.8374 train_time:1360193ms step_avg:169.90ms\nstep:8017/10000 train_loss:3.8221 train_time:1360362ms step_avg:169.90ms\nstep:8018/10000 train_loss:3.8902 train_time:1360533ms step_avg:169.90ms\nstep:8019/10000 train_loss:3.6035 train_time:1360701ms step_avg:169.90ms\nstep:8020/10000 train_loss:3.7765 train_time:1360871ms step_avg:169.90ms\nstep:8021/10000 train_loss:3.6453 train_time:1361039ms step_avg:169.90ms\nstep:8022/10000 train_loss:3.8594 train_time:1361207ms step_avg:169.90ms\nstep:8023/10000 train_loss:3.8502 train_time:1361376ms step_avg:169.90ms\nstep:8024/10000 train_loss:3.8184 train_time:1361547ms step_avg:169.90ms\nstep:8025/10000 train_loss:3.8922 train_time:1361719ms step_avg:169.90ms\nstep:8026/10000 train_loss:3.9341 train_time:1361888ms step_avg:169.90ms\nstep:8027/10000 train_loss:3.8328 train_time:1362059ms step_avg:169.90ms\nstep:8028/10000 train_loss:3.6643 train_time:1362228ms step_avg:169.90ms\nstep:8029/10000 train_loss:3.9409 train_time:1362399ms step_avg:169.90ms\nstep:8030/10000 train_loss:3.8457 train_time:1362569ms step_avg:169.90ms\nstep:8031/10000 train_loss:3.7863 train_time:1362739ms step_avg:169.90ms\nstep:8032/10000 train_loss:3.8239 train_time:1362908ms step_avg:169.90ms\nstep:8033/10000 train_loss:3.8415 train_time:1363076ms step_avg:169.90ms\nstep:8034/10000 train_loss:3.8912 train_time:1363244ms step_avg:169.90ms\nstep:8035/10000 train_loss:4.8211 train_time:1363417ms step_avg:169.90ms\nstep:8036/10000 train_loss:3.8503 train_time:1363585ms step_avg:169.90ms\nstep:8037/10000 train_loss:3.8774 train_time:1363754ms step_avg:169.90ms\nstep:8038/10000 train_loss:3.7493 train_time:1363922ms step_avg:169.90ms\nstep:8039/10000 train_loss:3.7276 train_time:1364092ms step_avg:169.90ms\nstep:8040/10000 train_loss:3.7999 train_time:1364260ms step_avg:169.90ms\nstep:8041/10000 train_loss:3.8386 train_time:1364432ms step_avg:169.90ms\nstep:8042/10000 train_loss:3.7897 train_time:1364604ms step_avg:169.90ms\nstep:8043/10000 train_loss:3.8456 train_time:1364773ms step_avg:169.90ms\nstep:8044/10000 train_loss:3.7446 train_time:1364943ms step_avg:169.90ms\nstep:8045/10000 train_loss:3.8015 train_time:1365112ms step_avg:169.90ms\nstep:8046/10000 train_loss:3.7870 train_time:1365280ms step_avg:169.90ms\nstep:8047/10000 train_loss:3.8656 train_time:1365452ms step_avg:169.90ms\nstep:8048/10000 train_loss:3.7782 train_time:1365623ms step_avg:169.90ms\nstep:8049/10000 train_loss:3.8565 train_time:1365795ms step_avg:169.90ms\nstep:8050/10000 train_loss:3.8501 train_time:1365963ms step_avg:169.90ms\nstep:8051/10000 train_loss:3.7030 train_time:1366133ms step_avg:169.90ms\nstep:8052/10000 train_loss:3.6282 train_time:1366301ms step_avg:169.90ms\nstep:8053/10000 train_loss:3.7699 train_time:1366472ms step_avg:169.90ms\nstep:8054/10000 train_loss:3.7559 train_time:1366641ms step_avg:169.90ms\nstep:8055/10000 train_loss:3.8442 train_time:1366812ms step_avg:169.90ms\nstep:8056/10000 train_loss:3.7342 train_time:1366979ms step_avg:169.90ms\nstep:8057/10000 train_loss:4.0094 train_time:1367146ms step_avg:169.90ms\nstep:8058/10000 train_loss:3.8455 train_time:1367313ms step_avg:169.89ms\nstep:8059/10000 train_loss:3.9438 train_time:1367482ms step_avg:169.89ms\nstep:8060/10000 train_loss:3.7432 train_time:1367653ms step_avg:169.89ms\nstep:8061/10000 train_loss:3.7763 train_time:1367822ms step_avg:169.89ms\nstep:8062/10000 train_loss:3.8006 train_time:1367991ms step_avg:169.89ms\nstep:8063/10000 train_loss:3.9747 train_time:1368161ms step_avg:169.89ms\nstep:8064/10000 train_loss:3.7869 train_time:1368331ms step_avg:169.89ms\nstep:8065/10000 train_loss:3.8796 train_time:1368500ms step_avg:169.89ms\nstep:8066/10000 train_loss:3.8284 train_time:1368670ms step_avg:169.89ms\nstep:8067/10000 train_loss:3.7695 train_time:1368842ms step_avg:169.89ms\nstep:8068/10000 train_loss:3.7901 train_time:1369011ms step_avg:169.89ms\nstep:8069/10000 train_loss:3.9918 train_time:1369182ms step_avg:169.89ms\nstep:8070/10000 train_loss:3.7612 train_time:1369352ms step_avg:169.89ms\nstep:8071/10000 train_loss:3.9168 train_time:1369522ms step_avg:169.89ms\nstep:8072/10000 train_loss:3.7574 train_time:1369695ms step_avg:169.90ms\nstep:8073/10000 train_loss:3.8812 train_time:1369865ms step_avg:169.90ms\nstep:8074/10000 train_loss:3.7989 train_time:1370034ms step_avg:169.90ms\nstep:8075/10000 train_loss:3.7179 train_time:1370202ms step_avg:169.89ms\nstep:8076/10000 train_loss:3.7320 train_time:1370372ms step_avg:169.89ms\nstep:8077/10000 train_loss:3.9680 train_time:1370543ms step_avg:169.90ms\nstep:8078/10000 train_loss:3.7883 train_time:1370714ms step_avg:169.90ms\nstep:8079/10000 train_loss:3.7884 train_time:1370881ms step_avg:169.89ms\nstep:8080/10000 train_loss:3.6651 train_time:1371051ms step_avg:169.89ms\nstep:8081/10000 train_loss:3.7992 train_time:1371221ms step_avg:169.89ms\nstep:8082/10000 train_loss:3.9446 train_time:1371391ms step_avg:169.89ms\nstep:8083/10000 train_loss:3.8535 train_time:1371561ms step_avg:169.89ms\nstep:8084/10000 train_loss:3.9607 train_time:1371733ms step_avg:169.90ms\nstep:8085/10000 train_loss:3.8487 train_time:1371902ms step_avg:169.90ms\nstep:8086/10000 train_loss:3.7551 train_time:1372072ms step_avg:169.90ms\nstep:8087/10000 train_loss:3.8402 train_time:1372244ms step_avg:169.90ms\nstep:8088/10000 train_loss:3.8547 train_time:1372414ms step_avg:169.90ms\nstep:8089/10000 train_loss:3.7171 train_time:1372584ms step_avg:169.90ms\nstep:8090/10000 train_loss:3.9347 train_time:1372753ms step_avg:169.90ms\nstep:8091/10000 train_loss:3.8583 train_time:1372922ms step_avg:169.90ms\nstep:8092/10000 train_loss:3.8051 train_time:1373092ms step_avg:169.90ms\nstep:8093/10000 train_loss:3.8255 train_time:1373261ms step_avg:169.89ms\nstep:8094/10000 train_loss:3.9227 train_time:1373434ms step_avg:169.90ms\nstep:8095/10000 train_loss:3.7859 train_time:1373603ms step_avg:169.90ms\nstep:8096/10000 train_loss:3.8148 train_time:1373773ms step_avg:169.90ms\nstep:8097/10000 train_loss:3.9367 train_time:1373941ms step_avg:169.90ms\nstep:8098/10000 train_loss:3.7088 train_time:1374112ms step_avg:169.90ms\nstep:8099/10000 train_loss:3.8042 train_time:1374281ms step_avg:169.89ms\nstep:8100/10000 train_loss:3.7796 train_time:1374451ms step_avg:169.90ms\nstep:8101/10000 train_loss:3.8045 train_time:1374618ms step_avg:169.89ms\nstep:8102/10000 train_loss:3.8966 train_time:1374787ms step_avg:169.89ms\nstep:8103/10000 train_loss:3.8289 train_time:1374956ms step_avg:169.89ms\nstep:8104/10000 train_loss:3.7446 train_time:1375124ms step_avg:169.89ms\nstep:8105/10000 train_loss:3.7706 train_time:1375293ms step_avg:169.89ms\nstep:8106/10000 train_loss:3.8100 train_time:1375464ms step_avg:169.89ms\nstep:8107/10000 train_loss:3.8768 train_time:1375635ms step_avg:169.89ms\nstep:8108/10000 train_loss:3.8110 train_time:1375803ms step_avg:169.89ms\nstep:8109/10000 train_loss:3.7060 train_time:1375974ms step_avg:169.89ms\nstep:8110/10000 train_loss:3.8983 train_time:1376143ms step_avg:169.89ms\nstep:8111/10000 train_loss:3.8205 train_time:1376312ms step_avg:169.89ms\nstep:8112/10000 train_loss:3.8896 train_time:1376481ms step_avg:169.89ms\nstep:8113/10000 train_loss:3.6612 train_time:1376653ms step_avg:169.89ms\nstep:8114/10000 train_loss:3.9653 train_time:1376822ms step_avg:169.89ms\nstep:8115/10000 train_loss:3.8309 train_time:1376994ms step_avg:169.89ms\nstep:8116/10000 train_loss:3.6990 train_time:1377161ms step_avg:169.89ms\nstep:8117/10000 train_loss:3.9322 train_time:1377332ms step_avg:169.89ms\nstep:8118/10000 train_loss:3.7372 train_time:1377501ms step_avg:169.89ms\nstep:8119/10000 train_loss:3.7463 train_time:1377672ms step_avg:169.89ms\nstep:8120/10000 train_loss:3.7674 train_time:1377839ms step_avg:169.89ms\nstep:8121/10000 train_loss:3.9132 train_time:1378007ms step_avg:169.89ms\nstep:8122/10000 train_loss:3.8089 train_time:1378179ms step_avg:169.89ms\nstep:8123/10000 train_loss:3.8784 train_time:1378349ms step_avg:169.89ms\nstep:8124/10000 train_loss:3.8490 train_time:1378519ms step_avg:169.89ms\nstep:8125/10000 train_loss:3.8301 train_time:1378688ms step_avg:169.89ms\nstep:8126/10000 train_loss:3.6700 train_time:1378858ms step_avg:169.89ms\nstep:8127/10000 train_loss:3.8401 train_time:1379027ms step_avg:169.89ms\nstep:8128/10000 train_loss:3.8889 train_time:1379197ms step_avg:169.89ms\nstep:8129/10000 train_loss:3.7840 train_time:1379364ms step_avg:169.89ms\nstep:8130/10000 train_loss:3.7968 train_time:1379534ms step_avg:169.89ms\nstep:8131/10000 train_loss:3.7691 train_time:1379702ms step_avg:169.89ms\nstep:8132/10000 train_loss:3.5758 train_time:1379874ms step_avg:169.89ms\nstep:8133/10000 train_loss:3.7943 train_time:1380042ms step_avg:169.89ms\nstep:8134/10000 train_loss:3.7158 train_time:1380212ms step_avg:169.89ms\nstep:8135/10000 train_loss:3.8652 train_time:1380380ms step_avg:169.89ms\nstep:8136/10000 train_loss:3.8114 train_time:1380548ms step_avg:169.89ms\nstep:8137/10000 train_loss:3.8100 train_time:1380720ms step_avg:169.89ms\nstep:8138/10000 train_loss:3.7871 train_time:1380892ms step_avg:169.89ms\nstep:8139/10000 train_loss:3.6634 train_time:1381062ms step_avg:169.89ms\nstep:8140/10000 train_loss:3.7422 train_time:1381234ms step_avg:169.89ms\nstep:8141/10000 train_loss:3.8687 train_time:1381403ms step_avg:169.89ms\nstep:8142/10000 train_loss:3.8050 train_time:1381573ms step_avg:169.89ms\nstep:8143/10000 train_loss:3.8118 train_time:1381744ms step_avg:169.89ms\nstep:8144/10000 train_loss:3.8477 train_time:1381914ms step_avg:169.89ms\nstep:8145/10000 train_loss:3.7730 train_time:1382081ms step_avg:169.89ms\nstep:8146/10000 train_loss:3.8184 train_time:1382252ms step_avg:169.89ms\nstep:8147/10000 train_loss:3.8535 train_time:1382422ms step_avg:169.89ms\nstep:8148/10000 train_loss:3.7791 train_time:1382591ms step_avg:169.89ms\nstep:8149/10000 train_loss:3.8287 train_time:1382761ms step_avg:169.89ms\nstep:8150/10000 train_loss:3.7139 train_time:1382932ms step_avg:169.89ms\nstep:8151/10000 train_loss:3.7928 train_time:1383101ms step_avg:169.89ms\nstep:8152/10000 train_loss:3.8242 train_time:1383274ms step_avg:169.89ms\nstep:8153/10000 train_loss:3.8704 train_time:1383442ms step_avg:169.89ms\nstep:8154/10000 train_loss:3.8519 train_time:1383614ms step_avg:169.89ms\nstep:8155/10000 train_loss:3.8720 train_time:1383783ms step_avg:169.89ms\nstep:8156/10000 train_loss:3.8355 train_time:1383955ms step_avg:169.89ms\nstep:8157/10000 train_loss:3.8362 train_time:1384121ms step_avg:169.89ms\nstep:8158/10000 train_loss:3.8507 train_time:1384294ms step_avg:169.89ms\nstep:8159/10000 train_loss:3.5369 train_time:1384462ms step_avg:169.89ms\nstep:8160/10000 train_loss:3.8585 train_time:1384632ms step_avg:169.89ms\nstep:8161/10000 train_loss:3.9808 train_time:1384801ms step_avg:169.89ms\nstep:8162/10000 train_loss:3.8537 train_time:1384973ms step_avg:169.89ms\nstep:8163/10000 train_loss:3.7164 train_time:1385141ms step_avg:169.89ms\nstep:8164/10000 train_loss:4.0822 train_time:1385312ms step_avg:169.89ms\nstep:8165/10000 train_loss:3.7048 train_time:1385481ms step_avg:169.89ms\nstep:8166/10000 train_loss:3.7062 train_time:1385650ms step_avg:169.89ms\nstep:8167/10000 train_loss:3.7261 train_time:1385820ms step_avg:169.89ms\nstep:8168/10000 train_loss:3.8229 train_time:1385991ms step_avg:169.89ms\nstep:8169/10000 train_loss:4.0572 train_time:1386159ms step_avg:169.89ms\nstep:8170/10000 train_loss:3.9793 train_time:1386329ms step_avg:169.89ms\nstep:8171/10000 train_loss:3.7411 train_time:1386499ms step_avg:169.89ms\nstep:8172/10000 train_loss:3.7782 train_time:1386669ms step_avg:169.89ms\nstep:8173/10000 train_loss:3.7420 train_time:1386841ms step_avg:169.89ms\nstep:8174/10000 train_loss:3.8056 train_time:1387013ms step_avg:169.89ms\nstep:8175/10000 train_loss:3.7717 train_time:1387180ms step_avg:169.89ms\nstep:8176/10000 train_loss:3.7302 train_time:1387349ms step_avg:169.89ms\nstep:8177/10000 train_loss:3.8032 train_time:1387517ms step_avg:169.89ms\nstep:8178/10000 train_loss:3.8366 train_time:1387686ms step_avg:169.89ms\nstep:8179/10000 train_loss:3.8219 train_time:1387857ms step_avg:169.89ms\nstep:8180/10000 train_loss:3.8043 train_time:1388027ms step_avg:169.89ms\nstep:8181/10000 train_loss:3.8548 train_time:1388197ms step_avg:169.89ms\nstep:8182/10000 train_loss:3.7775 train_time:1388365ms step_avg:169.89ms\nstep:8183/10000 train_loss:3.9303 train_time:1388534ms step_avg:169.89ms\nstep:8184/10000 train_loss:3.7271 train_time:1388700ms step_avg:169.89ms\nstep:8185/10000 train_loss:3.5830 train_time:1388872ms step_avg:169.89ms\nstep:8186/10000 train_loss:3.6608 train_time:1389042ms step_avg:169.89ms\nstep:8187/10000 train_loss:3.8500 train_time:1389211ms step_avg:169.89ms\nstep:8188/10000 train_loss:3.7041 train_time:1389379ms step_avg:169.89ms\nstep:8189/10000 train_loss:3.9674 train_time:1389546ms step_avg:169.89ms\nstep:8190/10000 train_loss:3.8176 train_time:1389715ms step_avg:169.89ms\nstep:8191/10000 train_loss:3.8193 train_time:1389883ms step_avg:169.89ms\nstep:8192/10000 train_loss:3.7365 train_time:1390054ms step_avg:169.89ms\nstep:8193/10000 train_loss:3.8058 train_time:1390224ms step_avg:169.89ms\nstep:8194/10000 train_loss:3.8374 train_time:1390396ms step_avg:169.89ms\nstep:8195/10000 train_loss:3.8454 train_time:1390563ms step_avg:169.89ms\nstep:8196/10000 train_loss:3.7646 train_time:1390734ms step_avg:169.89ms\nstep:8197/10000 train_loss:3.8717 train_time:1390903ms step_avg:169.89ms\nstep:8198/10000 train_loss:3.6723 train_time:1391076ms step_avg:169.89ms\nstep:8199/10000 train_loss:3.7752 train_time:1391245ms step_avg:169.89ms\nstep:8200/10000 train_loss:3.8324 train_time:1391415ms step_avg:169.89ms\nstep:8201/10000 train_loss:3.7318 train_time:1391584ms step_avg:169.89ms\nstep:8202/10000 train_loss:3.7567 train_time:1391756ms step_avg:169.89ms\nstep:8203/10000 train_loss:3.9716 train_time:1391925ms step_avg:169.89ms\nstep:8204/10000 train_loss:3.7974 train_time:1392094ms step_avg:169.89ms\nstep:8205/10000 train_loss:3.8521 train_time:1392265ms step_avg:169.89ms\nstep:8206/10000 train_loss:3.8082 train_time:1392435ms step_avg:169.89ms\nstep:8207/10000 train_loss:3.8443 train_time:1392603ms step_avg:169.89ms\nstep:8208/10000 train_loss:3.8356 train_time:1392775ms step_avg:169.89ms\nstep:8209/10000 train_loss:3.7780 train_time:1392942ms step_avg:169.89ms\nstep:8210/10000 train_loss:3.7668 train_time:1393111ms step_avg:169.89ms\nstep:8211/10000 train_loss:3.7969 train_time:1393280ms step_avg:169.89ms\nstep:8212/10000 train_loss:3.7170 train_time:1393451ms step_avg:169.89ms\nstep:8213/10000 train_loss:3.7651 train_time:1393618ms step_avg:169.89ms\nstep:8214/10000 train_loss:3.7901 train_time:1393788ms step_avg:169.89ms\nstep:8215/10000 train_loss:3.7429 train_time:1393956ms step_avg:169.89ms\nstep:8216/10000 train_loss:3.9234 train_time:1394123ms step_avg:169.89ms\nstep:8217/10000 train_loss:3.8476 train_time:1394295ms step_avg:169.89ms\nstep:8218/10000 train_loss:3.9581 train_time:1394464ms step_avg:169.89ms\nstep:8219/10000 train_loss:3.7917 train_time:1394633ms step_avg:169.89ms\nstep:8220/10000 train_loss:3.7985 train_time:1394801ms step_avg:169.89ms\nstep:8221/10000 train_loss:3.8322 train_time:1394971ms step_avg:169.89ms\nstep:8222/10000 train_loss:3.8320 train_time:1395143ms step_avg:169.89ms\nstep:8223/10000 train_loss:3.7711 train_time:1395312ms step_avg:169.89ms\nstep:8224/10000 train_loss:3.9019 train_time:1395482ms step_avg:169.89ms\nstep:8225/10000 train_loss:3.8041 train_time:1395657ms step_avg:169.89ms\nstep:8226/10000 train_loss:3.7679 train_time:1395823ms step_avg:169.89ms\nstep:8227/10000 train_loss:3.8775 train_time:1395992ms step_avg:169.89ms\nstep:8228/10000 train_loss:3.8277 train_time:1396160ms step_avg:169.89ms\nstep:8229/10000 train_loss:3.8423 train_time:1396330ms step_avg:169.89ms\nstep:8230/10000 train_loss:3.7628 train_time:1396498ms step_avg:169.89ms\nstep:8231/10000 train_loss:4.0065 train_time:1396666ms step_avg:169.89ms\nstep:8232/10000 train_loss:3.8148 train_time:1396835ms step_avg:169.89ms\nstep:8233/10000 train_loss:3.8371 train_time:1397004ms step_avg:169.89ms\nstep:8234/10000 train_loss:3.6285 train_time:1397173ms step_avg:169.89ms\nstep:8235/10000 train_loss:3.9328 train_time:1397342ms step_avg:169.89ms\nstep:8236/10000 train_loss:3.8752 train_time:1397513ms step_avg:169.89ms\nstep:8237/10000 train_loss:3.7708 train_time:1397683ms step_avg:169.89ms\nstep:8238/10000 train_loss:3.7329 train_time:1397854ms step_avg:169.89ms\nstep:8239/10000 train_loss:3.8380 train_time:1398024ms step_avg:169.89ms\nstep:8240/10000 train_loss:3.8008 train_time:1398192ms step_avg:169.89ms\nstep:8241/10000 train_loss:3.7765 train_time:1398363ms step_avg:169.89ms\nstep:8242/10000 train_loss:3.9205 train_time:1398534ms step_avg:169.89ms\nstep:8243/10000 train_loss:3.8032 train_time:1398703ms step_avg:169.89ms\nstep:8244/10000 train_loss:3.7338 train_time:1398873ms step_avg:169.89ms\nstep:8245/10000 train_loss:3.6806 train_time:1399042ms step_avg:169.89ms\nstep:8246/10000 train_loss:3.9114 train_time:1399211ms step_avg:169.89ms\nstep:8247/10000 train_loss:3.5168 train_time:1399381ms step_avg:169.89ms\nstep:8248/10000 train_loss:3.6966 train_time:1399552ms step_avg:169.89ms\nstep:8249/10000 train_loss:3.8670 train_time:1399719ms step_avg:169.89ms\nstep:8250/10000 train_loss:3.7898 train_time:1399888ms step_avg:169.89ms\nstep:8251/10000 train_loss:4.0358 train_time:1400055ms step_avg:169.89ms\nstep:8252/10000 train_loss:3.8839 train_time:1400224ms step_avg:169.89ms\nstep:8253/10000 train_loss:3.6930 train_time:1400394ms step_avg:169.89ms\nstep:8254/10000 train_loss:3.8422 train_time:1400564ms step_avg:169.89ms\nstep:8255/10000 train_loss:3.7891 train_time:1400733ms step_avg:169.89ms\nstep:8256/10000 train_loss:3.9792 train_time:1400904ms step_avg:169.89ms\nstep:8257/10000 train_loss:3.8199 train_time:1401075ms step_avg:169.89ms\nstep:8258/10000 train_loss:3.8309 train_time:1401243ms step_avg:169.89ms\nstep:8259/10000 train_loss:3.7325 train_time:1401413ms step_avg:169.89ms\nstep:8260/10000 train_loss:3.9229 train_time:1401581ms step_avg:169.89ms\nstep:8261/10000 train_loss:3.5778 train_time:1401751ms step_avg:169.89ms\nstep:8262/10000 train_loss:3.7928 train_time:1401920ms step_avg:169.89ms\nstep:8263/10000 train_loss:3.6534 train_time:1402090ms step_avg:169.89ms\nstep:8264/10000 train_loss:3.8502 train_time:1402259ms step_avg:169.89ms\nstep:8265/10000 train_loss:3.6624 train_time:1402427ms step_avg:169.89ms\nstep:8266/10000 train_loss:3.7128 train_time:1402595ms step_avg:169.89ms\nstep:8267/10000 train_loss:3.7810 train_time:1402762ms step_avg:169.89ms\nstep:8268/10000 train_loss:3.9640 train_time:1402934ms step_avg:169.89ms\nstep:8269/10000 train_loss:3.8130 train_time:1403104ms step_avg:169.89ms\nstep:8270/10000 train_loss:4.0072 train_time:1403273ms step_avg:169.89ms\nstep:8271/10000 train_loss:3.8766 train_time:1403440ms step_avg:169.89ms\nstep:8272/10000 train_loss:3.7216 train_time:1403609ms step_avg:169.89ms\nstep:8273/10000 train_loss:3.8506 train_time:1403778ms step_avg:169.89ms\nstep:8274/10000 train_loss:3.8629 train_time:1403947ms step_avg:169.89ms\nstep:8275/10000 train_loss:3.7457 train_time:1404118ms step_avg:169.89ms\nstep:8276/10000 train_loss:3.8312 train_time:1404285ms step_avg:169.89ms\nstep:8277/10000 train_loss:3.7618 train_time:1404456ms step_avg:169.89ms\nstep:8278/10000 train_loss:3.7981 train_time:1404626ms step_avg:169.89ms\nstep:8279/10000 train_loss:3.7831 train_time:1404793ms step_avg:169.89ms\nstep:8280/10000 train_loss:3.6398 train_time:1404963ms step_avg:169.89ms\nstep:8281/10000 train_loss:3.8682 train_time:1405136ms step_avg:169.89ms\nstep:8282/10000 train_loss:3.8539 train_time:1405304ms step_avg:169.89ms\nstep:8283/10000 train_loss:3.8868 train_time:1405475ms step_avg:169.89ms\nstep:8284/10000 train_loss:3.8366 train_time:1405643ms step_avg:169.89ms\nstep:8285/10000 train_loss:3.7643 train_time:1405815ms step_avg:169.89ms\nstep:8286/10000 train_loss:3.7626 train_time:1405983ms step_avg:169.89ms\nstep:8287/10000 train_loss:3.8041 train_time:1406154ms step_avg:169.89ms\nstep:8288/10000 train_loss:3.8968 train_time:1406324ms step_avg:169.89ms\nstep:8289/10000 train_loss:3.7752 train_time:1406497ms step_avg:169.89ms\nstep:8290/10000 train_loss:3.8160 train_time:1406666ms step_avg:169.89ms\nstep:8291/10000 train_loss:3.5520 train_time:1406838ms step_avg:169.89ms\nstep:8292/10000 train_loss:4.0184 train_time:1407006ms step_avg:169.89ms\nstep:8293/10000 train_loss:3.8422 train_time:1407175ms step_avg:169.89ms\nstep:8294/10000 train_loss:3.9435 train_time:1407344ms step_avg:169.89ms\nstep:8295/10000 train_loss:3.8378 train_time:1407514ms step_avg:169.89ms\nstep:8296/10000 train_loss:3.9865 train_time:1407682ms step_avg:169.89ms\nstep:8297/10000 train_loss:3.8449 train_time:1407853ms step_avg:169.89ms\nstep:8298/10000 train_loss:3.9499 train_time:1408022ms step_avg:169.89ms\nstep:8299/10000 train_loss:3.7385 train_time:1408192ms step_avg:169.89ms\nstep:8300/10000 train_loss:3.8266 train_time:1408361ms step_avg:169.89ms\nstep:8301/10000 train_loss:3.7904 train_time:1408536ms step_avg:169.89ms\nstep:8302/10000 train_loss:3.8004 train_time:1408698ms step_avg:169.89ms\nstep:8303/10000 train_loss:4.1145 train_time:1408866ms step_avg:169.89ms\nstep:8304/10000 train_loss:3.9547 train_time:1409037ms step_avg:169.89ms\nstep:8305/10000 train_loss:3.7756 train_time:1409205ms step_avg:169.89ms\nstep:8306/10000 train_loss:3.8292 train_time:1409377ms step_avg:169.89ms\nstep:8307/10000 train_loss:3.7988 train_time:1409546ms step_avg:169.89ms\nstep:8308/10000 train_loss:3.9665 train_time:1409715ms step_avg:169.89ms\nstep:8309/10000 train_loss:3.7684 train_time:1409883ms step_avg:169.89ms\nstep:8310/10000 train_loss:3.7236 train_time:1410052ms step_avg:169.89ms\nstep:8311/10000 train_loss:3.7463 train_time:1410218ms step_avg:169.89ms\nstep:8312/10000 train_loss:4.6400 train_time:1410387ms step_avg:169.89ms\nstep:8313/10000 train_loss:3.7784 train_time:1410558ms step_avg:169.89ms\nstep:8314/10000 train_loss:4.2412 train_time:1410727ms step_avg:169.89ms\nstep:8315/10000 train_loss:3.7658 train_time:1410897ms step_avg:169.89ms\nstep:8316/10000 train_loss:3.7961 train_time:1411063ms step_avg:169.88ms\nstep:8317/10000 train_loss:3.6983 train_time:1411232ms step_avg:169.88ms\nstep:8318/10000 train_loss:3.6644 train_time:1411400ms step_avg:169.88ms\nstep:8319/10000 train_loss:3.8106 train_time:1411570ms step_avg:169.88ms\nstep:8320/10000 train_loss:3.8299 train_time:1411739ms step_avg:169.88ms\nstep:8321/10000 train_loss:3.8540 train_time:1411909ms step_avg:169.88ms\nstep:8322/10000 train_loss:3.8434 train_time:1412078ms step_avg:169.88ms\nstep:8323/10000 train_loss:3.8820 train_time:1412247ms step_avg:169.88ms\nstep:8324/10000 train_loss:3.8041 train_time:1412417ms step_avg:169.88ms\nstep:8325/10000 train_loss:3.8643 train_time:1412587ms step_avg:169.88ms\nstep:8326/10000 train_loss:3.7332 train_time:1412756ms step_avg:169.88ms\nstep:8327/10000 train_loss:3.8079 train_time:1412925ms step_avg:169.88ms\nstep:8328/10000 train_loss:3.8126 train_time:1413097ms step_avg:169.88ms\nstep:8329/10000 train_loss:3.6979 train_time:1413263ms step_avg:169.88ms\nstep:8330/10000 train_loss:3.7010 train_time:1413433ms step_avg:169.88ms\nstep:8331/10000 train_loss:3.7357 train_time:1413601ms step_avg:169.88ms\nstep:8332/10000 train_loss:3.7320 train_time:1413768ms step_avg:169.88ms\nstep:8333/10000 train_loss:3.7260 train_time:1413935ms step_avg:169.88ms\nstep:8334/10000 train_loss:3.7895 train_time:1414105ms step_avg:169.88ms\nstep:8335/10000 train_loss:3.8119 train_time:1414275ms step_avg:169.88ms\nstep:8336/10000 train_loss:3.8312 train_time:1414443ms step_avg:169.88ms\nstep:8337/10000 train_loss:3.7070 train_time:1414614ms step_avg:169.88ms\nstep:8338/10000 train_loss:4.1121 train_time:1414783ms step_avg:169.88ms\nstep:8339/10000 train_loss:3.8445 train_time:1414953ms step_avg:169.88ms\nstep:8340/10000 train_loss:3.7450 train_time:1415122ms step_avg:169.88ms\nstep:8341/10000 train_loss:3.9013 train_time:1415294ms step_avg:169.88ms\nstep:8342/10000 train_loss:3.6931 train_time:1415461ms step_avg:169.88ms\nstep:8343/10000 train_loss:3.8815 train_time:1415631ms step_avg:169.88ms\nstep:8344/10000 train_loss:3.8475 train_time:1415802ms step_avg:169.88ms\nstep:8345/10000 train_loss:3.9371 train_time:1415973ms step_avg:169.88ms\nstep:8346/10000 train_loss:4.0180 train_time:1416140ms step_avg:169.88ms\nstep:8347/10000 train_loss:3.8383 train_time:1416311ms step_avg:169.88ms\nstep:8348/10000 train_loss:3.7990 train_time:1416478ms step_avg:169.88ms\nstep:8349/10000 train_loss:3.7564 train_time:1416647ms step_avg:169.88ms\nstep:8350/10000 train_loss:3.7437 train_time:1416817ms step_avg:169.88ms\nstep:8351/10000 train_loss:3.7857 train_time:1416986ms step_avg:169.88ms\nstep:8352/10000 train_loss:3.7500 train_time:1417155ms step_avg:169.88ms\nstep:8353/10000 train_loss:3.8006 train_time:1417325ms step_avg:169.88ms\nstep:8354/10000 train_loss:3.6924 train_time:1417495ms step_avg:169.88ms\nstep:8355/10000 train_loss:3.7901 train_time:1417663ms step_avg:169.88ms\nstep:8356/10000 train_loss:3.7912 train_time:1417831ms step_avg:169.88ms\nstep:8357/10000 train_loss:3.7893 train_time:1417999ms step_avg:169.88ms\nstep:8358/10000 train_loss:3.9324 train_time:1418167ms step_avg:169.88ms\nstep:8359/10000 train_loss:4.0963 train_time:1418335ms step_avg:169.88ms\nstep:8360/10000 train_loss:3.7585 train_time:1418505ms step_avg:169.88ms\nstep:8361/10000 train_loss:3.8237 train_time:1418676ms step_avg:169.88ms\nstep:8362/10000 train_loss:3.8961 train_time:1418845ms step_avg:169.88ms\nstep:8363/10000 train_loss:3.7411 train_time:1419014ms step_avg:169.88ms\nstep:8364/10000 train_loss:3.7302 train_time:1419185ms step_avg:169.88ms\nstep:8365/10000 train_loss:3.8199 train_time:1419353ms step_avg:169.88ms\nstep:8366/10000 train_loss:3.8576 train_time:1419522ms step_avg:169.88ms\nstep:8367/10000 train_loss:3.6302 train_time:1419692ms step_avg:169.88ms\nstep:8368/10000 train_loss:3.7021 train_time:1419861ms step_avg:169.88ms\nstep:8369/10000 train_loss:3.8088 train_time:1420034ms step_avg:169.88ms\nstep:8370/10000 train_loss:3.9127 train_time:1420202ms step_avg:169.88ms\nstep:8371/10000 train_loss:3.8984 train_time:1420372ms step_avg:169.88ms\nstep:8372/10000 train_loss:3.7365 train_time:1420540ms step_avg:169.88ms\nstep:8373/10000 train_loss:3.7151 train_time:1420709ms step_avg:169.88ms\nstep:8374/10000 train_loss:3.8358 train_time:1420876ms step_avg:169.88ms\nstep:8375/10000 train_loss:3.6007 train_time:1421044ms step_avg:169.88ms\nstep:8376/10000 train_loss:3.7264 train_time:1421212ms step_avg:169.88ms\nstep:8377/10000 train_loss:3.8367 train_time:1421382ms step_avg:169.88ms\nstep:8378/10000 train_loss:3.8204 train_time:1421558ms step_avg:169.88ms\nstep:8379/10000 train_loss:3.7967 train_time:1421721ms step_avg:169.88ms\nstep:8380/10000 train_loss:3.8040 train_time:1421892ms step_avg:169.88ms\nstep:8381/10000 train_loss:3.6386 train_time:1422059ms step_avg:169.88ms\nstep:8382/10000 train_loss:3.8081 train_time:1422480ms step_avg:169.91ms\nstep:8383/10000 train_loss:3.7766 train_time:1422646ms step_avg:169.91ms\nstep:8384/10000 train_loss:3.7893 train_time:1422815ms step_avg:169.91ms\nstep:8385/10000 train_loss:3.7478 train_time:1422982ms step_avg:169.91ms\nstep:8386/10000 train_loss:3.7200 train_time:1423151ms step_avg:169.91ms\nstep:8387/10000 train_loss:3.7554 train_time:1423318ms step_avg:169.91ms\nstep:8388/10000 train_loss:3.8553 train_time:1423494ms step_avg:169.91ms\nstep:8389/10000 train_loss:3.7479 train_time:1423666ms step_avg:169.91ms\nstep:8390/10000 train_loss:3.8285 train_time:1423835ms step_avg:169.91ms\nstep:8391/10000 train_loss:3.7632 train_time:1424003ms step_avg:169.91ms\nstep:8392/10000 train_loss:3.7002 train_time:1424171ms step_avg:169.91ms\nstep:8393/10000 train_loss:3.6925 train_time:1424342ms step_avg:169.91ms\nstep:8394/10000 train_loss:3.8677 train_time:1424514ms step_avg:169.91ms\nstep:8395/10000 train_loss:3.7680 train_time:1424682ms step_avg:169.91ms\nstep:8396/10000 train_loss:3.5419 train_time:1424851ms step_avg:169.91ms\nstep:8397/10000 train_loss:4.1180 train_time:1425019ms step_avg:169.91ms\nstep:8398/10000 train_loss:3.7137 train_time:1425187ms step_avg:169.91ms\nstep:8399/10000 train_loss:4.0145 train_time:1425360ms step_avg:169.91ms\nstep:8400/10000 train_loss:3.7778 train_time:1425530ms step_avg:169.91ms\nstep:8401/10000 train_loss:3.9064 train_time:1425701ms step_avg:169.91ms\nstep:8402/10000 train_loss:3.6253 train_time:1425870ms step_avg:169.91ms\nstep:8403/10000 train_loss:3.8891 train_time:1426038ms step_avg:169.91ms\nstep:8404/10000 train_loss:3.9674 train_time:1426208ms step_avg:169.91ms\nstep:8405/10000 train_loss:3.6883 train_time:1426379ms step_avg:169.91ms\nstep:8406/10000 train_loss:3.8526 train_time:1426552ms step_avg:169.91ms\nstep:8407/10000 train_loss:3.8527 train_time:1426723ms step_avg:169.91ms\nstep:8408/10000 train_loss:3.8225 train_time:1426893ms step_avg:169.91ms\nstep:8409/10000 train_loss:3.8606 train_time:1427062ms step_avg:169.91ms\nstep:8410/10000 train_loss:3.8029 train_time:1427232ms step_avg:169.91ms\nstep:8411/10000 train_loss:3.7635 train_time:1427404ms step_avg:169.91ms\nstep:8412/10000 train_loss:3.6627 train_time:1427576ms step_avg:169.91ms\nstep:8413/10000 train_loss:3.7824 train_time:1427745ms step_avg:169.91ms\nstep:8414/10000 train_loss:3.8241 train_time:1427913ms step_avg:169.91ms\nstep:8415/10000 train_loss:3.7276 train_time:1428081ms step_avg:169.91ms\nstep:8416/10000 train_loss:3.7714 train_time:1428251ms step_avg:169.91ms\nstep:8417/10000 train_loss:3.9313 train_time:1428421ms step_avg:169.91ms\nstep:8418/10000 train_loss:3.7190 train_time:1428593ms step_avg:169.91ms\nstep:8419/10000 train_loss:3.8124 train_time:1428763ms step_avg:169.91ms\nstep:8420/10000 train_loss:3.7516 train_time:1428934ms step_avg:169.91ms\nstep:8421/10000 train_loss:3.7964 train_time:1429103ms step_avg:169.91ms\nstep:8422/10000 train_loss:3.8081 train_time:1429273ms step_avg:169.91ms\nstep:8423/10000 train_loss:3.9819 train_time:1429443ms step_avg:169.91ms\nstep:8424/10000 train_loss:3.8664 train_time:1429615ms step_avg:169.91ms\nstep:8425/10000 train_loss:3.7593 train_time:1429784ms step_avg:169.91ms\nstep:8426/10000 train_loss:3.8357 train_time:1429954ms step_avg:169.91ms\nstep:8427/10000 train_loss:3.8677 train_time:1430122ms step_avg:169.91ms\nstep:8428/10000 train_loss:3.7138 train_time:1430293ms step_avg:169.91ms\nstep:8429/10000 train_loss:3.8085 train_time:1430461ms step_avg:169.91ms\nstep:8430/10000 train_loss:3.9463 train_time:1430632ms step_avg:169.91ms\nstep:8431/10000 train_loss:3.9269 train_time:1430802ms step_avg:169.91ms\nstep:8432/10000 train_loss:3.6627 train_time:1430970ms step_avg:169.91ms\nstep:8433/10000 train_loss:3.6455 train_time:1431141ms step_avg:169.91ms\nstep:8434/10000 train_loss:3.8946 train_time:1431312ms step_avg:169.91ms\nstep:8435/10000 train_loss:3.7632 train_time:1431481ms step_avg:169.91ms\nstep:8436/10000 train_loss:3.9358 train_time:1431652ms step_avg:169.91ms\nstep:8437/10000 train_loss:3.7555 train_time:1431821ms step_avg:169.91ms\nstep:8438/10000 train_loss:3.8115 train_time:1431991ms step_avg:169.91ms\nstep:8439/10000 train_loss:3.6740 train_time:1432158ms step_avg:169.91ms\nstep:8440/10000 train_loss:3.6944 train_time:1432328ms step_avg:169.91ms\nstep:8441/10000 train_loss:3.8221 train_time:1432500ms step_avg:169.91ms\nstep:8442/10000 train_loss:3.9682 train_time:1432670ms step_avg:169.91ms\nstep:8443/10000 train_loss:3.8382 train_time:1432839ms step_avg:169.91ms\nstep:8444/10000 train_loss:4.0295 train_time:1433008ms step_avg:169.91ms\nstep:8445/10000 train_loss:3.8785 train_time:1433178ms step_avg:169.91ms\nstep:8446/10000 train_loss:3.7717 train_time:1433345ms step_avg:169.91ms\nstep:8447/10000 train_loss:3.6801 train_time:1433515ms step_avg:169.91ms\nstep:8448/10000 train_loss:3.8928 train_time:1433686ms step_avg:169.91ms\nstep:8449/10000 train_loss:3.8540 train_time:1433855ms step_avg:169.91ms\nstep:8450/10000 train_loss:3.9664 train_time:1434024ms step_avg:169.91ms\nstep:8451/10000 train_loss:3.7152 train_time:1434195ms step_avg:169.91ms\nstep:8452/10000 train_loss:3.8181 train_time:1434365ms step_avg:169.91ms\nstep:8453/10000 train_loss:3.7767 train_time:1434536ms step_avg:169.91ms\nstep:8454/10000 train_loss:4.0335 train_time:1434705ms step_avg:169.91ms\nstep:8455/10000 train_loss:3.7223 train_time:1434873ms step_avg:169.91ms\nstep:8456/10000 train_loss:3.8287 train_time:1435041ms step_avg:169.91ms\nstep:8457/10000 train_loss:3.8021 train_time:1435210ms step_avg:169.91ms\nstep:8458/10000 train_loss:3.6676 train_time:1435381ms step_avg:169.91ms\nstep:8459/10000 train_loss:4.1060 train_time:1435550ms step_avg:169.91ms\nstep:8460/10000 train_loss:3.6603 train_time:1435719ms step_avg:169.91ms\nstep:8461/10000 train_loss:3.8334 train_time:1435887ms step_avg:169.91ms\nstep:8462/10000 train_loss:3.7580 train_time:1436055ms step_avg:169.91ms\nstep:8463/10000 train_loss:3.6649 train_time:1436224ms step_avg:169.91ms\nstep:8464/10000 train_loss:4.1115 train_time:1436395ms step_avg:169.91ms\nstep:8465/10000 train_loss:3.7282 train_time:1436564ms step_avg:169.91ms\nstep:8466/10000 train_loss:3.6380 train_time:1436734ms step_avg:169.91ms\nstep:8467/10000 train_loss:3.8375 train_time:1436904ms step_avg:169.91ms\nstep:8468/10000 train_loss:3.8475 train_time:1437074ms step_avg:169.91ms\nstep:8469/10000 train_loss:3.8340 train_time:1437243ms step_avg:169.91ms\nstep:8470/10000 train_loss:3.8024 train_time:1437413ms step_avg:169.91ms\nstep:8471/10000 train_loss:3.7525 train_time:1437584ms step_avg:169.91ms\nstep:8472/10000 train_loss:3.7343 train_time:1437753ms step_avg:169.91ms\nstep:8473/10000 train_loss:3.6947 train_time:1437921ms step_avg:169.91ms\nstep:8474/10000 train_loss:3.8608 train_time:1438091ms step_avg:169.91ms\nstep:8475/10000 train_loss:3.7036 train_time:1438258ms step_avg:169.91ms\nstep:8476/10000 train_loss:3.8730 train_time:1438429ms step_avg:169.91ms\nstep:8477/10000 train_loss:3.8748 train_time:1438600ms step_avg:169.91ms\nstep:8478/10000 train_loss:3.7653 train_time:1438768ms step_avg:169.91ms\nstep:8479/10000 train_loss:3.7890 train_time:1438939ms step_avg:169.91ms\nstep:8480/10000 train_loss:3.8270 train_time:1439110ms step_avg:169.91ms\nstep:8481/10000 train_loss:3.7307 train_time:1439281ms step_avg:169.91ms\nstep:8482/10000 train_loss:3.9497 train_time:1439450ms step_avg:169.91ms\nstep:8483/10000 train_loss:3.7061 train_time:1439621ms step_avg:169.91ms\nstep:8484/10000 train_loss:3.7825 train_time:1439789ms step_avg:169.91ms\nstep:8485/10000 train_loss:3.7680 train_time:1439957ms step_avg:169.91ms\nstep:8486/10000 train_loss:3.9033 train_time:1440126ms step_avg:169.91ms\nstep:8487/10000 train_loss:4.0947 train_time:1440296ms step_avg:169.91ms\nstep:8488/10000 train_loss:3.8534 train_time:1440463ms step_avg:169.91ms\nstep:8489/10000 train_loss:3.6950 train_time:1440634ms step_avg:169.91ms\nstep:8490/10000 train_loss:3.9033 train_time:1440804ms step_avg:169.91ms\nstep:8491/10000 train_loss:3.7178 train_time:1440973ms step_avg:169.91ms\nstep:8492/10000 train_loss:3.7090 train_time:1441144ms step_avg:169.91ms\nstep:8493/10000 train_loss:4.4228 train_time:1441314ms step_avg:169.91ms\nstep:8494/10000 train_loss:3.8356 train_time:1441482ms step_avg:169.91ms\nstep:8495/10000 train_loss:3.7456 train_time:1441651ms step_avg:169.91ms\nstep:8496/10000 train_loss:3.7602 train_time:1441819ms step_avg:169.91ms\nstep:8497/10000 train_loss:3.6780 train_time:1441987ms step_avg:169.91ms\nstep:8498/10000 train_loss:3.7058 train_time:1442157ms step_avg:169.91ms\nstep:8499/10000 train_loss:3.8439 train_time:1442326ms step_avg:169.91ms\nstep:8500/10000 train_loss:3.4932 train_time:1442497ms step_avg:169.91ms\nstep:8501/10000 train_loss:3.8475 train_time:1442665ms step_avg:169.91ms\nstep:8502/10000 train_loss:3.7208 train_time:1442836ms step_avg:169.91ms\nstep:8503/10000 train_loss:3.7228 train_time:1443002ms step_avg:169.90ms\nstep:8504/10000 train_loss:3.6853 train_time:1443173ms step_avg:169.90ms\nstep:8505/10000 train_loss:3.8172 train_time:1443340ms step_avg:169.90ms\nstep:8506/10000 train_loss:3.8408 train_time:1443510ms step_avg:169.90ms\nstep:8507/10000 train_loss:3.8433 train_time:1443680ms step_avg:169.90ms\nstep:8508/10000 train_loss:3.8230 train_time:1443848ms step_avg:169.90ms\nstep:8509/10000 train_loss:3.7436 train_time:1444017ms step_avg:169.90ms\nstep:8510/10000 train_loss:3.7861 train_time:1444188ms step_avg:169.90ms\nstep:8511/10000 train_loss:3.7928 train_time:1444358ms step_avg:169.90ms\nstep:8512/10000 train_loss:3.7907 train_time:1444529ms step_avg:169.90ms\nstep:8513/10000 train_loss:3.7758 train_time:1444698ms step_avg:169.90ms\nstep:8514/10000 train_loss:3.8104 train_time:1444862ms step_avg:169.90ms\nstep:8515/10000 train_loss:3.6916 train_time:1445033ms step_avg:169.90ms\nstep:8516/10000 train_loss:3.8645 train_time:1445201ms step_avg:169.90ms\nstep:8517/10000 train_loss:3.6501 train_time:1445374ms step_avg:169.90ms\nstep:8518/10000 train_loss:3.7846 train_time:1445543ms step_avg:169.90ms\nstep:8519/10000 train_loss:3.8907 train_time:1445713ms step_avg:169.90ms\nstep:8520/10000 train_loss:3.7621 train_time:1445883ms step_avg:169.90ms\nstep:8521/10000 train_loss:3.6698 train_time:1446053ms step_avg:169.90ms\nstep:8522/10000 train_loss:3.7939 train_time:1446221ms step_avg:169.90ms\nstep:8523/10000 train_loss:3.6907 train_time:1446389ms step_avg:169.90ms\nstep:8524/10000 train_loss:3.9066 train_time:1446558ms step_avg:169.90ms\nstep:8525/10000 train_loss:3.7850 train_time:1446733ms step_avg:169.90ms\nstep:8526/10000 train_loss:3.8354 train_time:1446904ms step_avg:169.90ms\nstep:8527/10000 train_loss:4.0503 train_time:1447066ms step_avg:169.90ms\nstep:8528/10000 train_loss:3.8709 train_time:1447237ms step_avg:169.90ms\nstep:8529/10000 train_loss:3.8821 train_time:1447405ms step_avg:169.90ms\nstep:8530/10000 train_loss:3.6557 train_time:1447574ms step_avg:169.90ms\nstep:8531/10000 train_loss:3.9415 train_time:1447741ms step_avg:169.90ms\nstep:8532/10000 train_loss:3.7475 train_time:1447911ms step_avg:169.90ms\nstep:8533/10000 train_loss:3.7160 train_time:1448079ms step_avg:169.90ms\nstep:8534/10000 train_loss:3.7199 train_time:1448249ms step_avg:169.90ms\nstep:8535/10000 train_loss:3.7782 train_time:1448419ms step_avg:169.90ms\nstep:8536/10000 train_loss:3.6953 train_time:1448587ms step_avg:169.90ms\nstep:8537/10000 train_loss:3.8124 train_time:1448757ms step_avg:169.90ms\nstep:8538/10000 train_loss:3.8583 train_time:1448926ms step_avg:169.90ms\nstep:8539/10000 train_loss:3.8725 train_time:1449097ms step_avg:169.90ms\nstep:8540/10000 train_loss:3.7384 train_time:1449266ms step_avg:169.90ms\nstep:8541/10000 train_loss:3.8803 train_time:1449437ms step_avg:169.90ms\nstep:8542/10000 train_loss:3.7108 train_time:1449607ms step_avg:169.90ms\nstep:8543/10000 train_loss:3.7582 train_time:1449776ms step_avg:169.90ms\nstep:8544/10000 train_loss:3.8880 train_time:1449946ms step_avg:169.90ms\nstep:8545/10000 train_loss:4.0179 train_time:1450116ms step_avg:169.90ms\nstep:8546/10000 train_loss:3.6570 train_time:1450285ms step_avg:169.90ms\nstep:8547/10000 train_loss:3.8109 train_time:1450456ms step_avg:169.90ms\nstep:8548/10000 train_loss:3.7676 train_time:1450625ms step_avg:169.90ms\nstep:8549/10000 train_loss:3.7424 train_time:1450793ms step_avg:169.90ms\nstep:8550/10000 train_loss:3.7236 train_time:1450961ms step_avg:169.90ms\nstep:8551/10000 train_loss:3.8791 train_time:1451131ms step_avg:169.90ms\nstep:8552/10000 train_loss:3.6922 train_time:1451302ms step_avg:169.90ms\nstep:8553/10000 train_loss:3.9196 train_time:1451474ms step_avg:169.90ms\nstep:8554/10000 train_loss:3.7498 train_time:1451643ms step_avg:169.90ms\nstep:8555/10000 train_loss:3.6858 train_time:1451814ms step_avg:169.90ms\nstep:8556/10000 train_loss:3.7901 train_time:1451982ms step_avg:169.90ms\nstep:8557/10000 train_loss:3.7675 train_time:1452152ms step_avg:169.90ms\nstep:8558/10000 train_loss:3.8716 train_time:1452324ms step_avg:169.90ms\nstep:8559/10000 train_loss:3.7314 train_time:1452494ms step_avg:169.90ms\nstep:8560/10000 train_loss:3.8728 train_time:1452663ms step_avg:169.90ms\nstep:8561/10000 train_loss:3.6928 train_time:1452834ms step_avg:169.90ms\nstep:8562/10000 train_loss:3.9349 train_time:1453002ms step_avg:169.90ms\nstep:8563/10000 train_loss:3.7519 train_time:1453172ms step_avg:169.90ms\nstep:8564/10000 train_loss:3.8735 train_time:1453342ms step_avg:169.90ms\nstep:8565/10000 train_loss:3.7734 train_time:1453513ms step_avg:169.90ms\nstep:8566/10000 train_loss:3.8405 train_time:1453681ms step_avg:169.90ms\nstep:8567/10000 train_loss:3.7379 train_time:1453854ms step_avg:169.90ms\nstep:8568/10000 train_loss:3.8358 train_time:1454021ms step_avg:169.90ms\nstep:8569/10000 train_loss:3.5378 train_time:1454190ms step_avg:169.90ms\nstep:8570/10000 train_loss:3.9324 train_time:1454360ms step_avg:169.90ms\nstep:8571/10000 train_loss:3.8386 train_time:1454529ms step_avg:169.90ms\nstep:8572/10000 train_loss:3.7960 train_time:1454700ms step_avg:169.90ms\nstep:8573/10000 train_loss:3.7581 train_time:1454870ms step_avg:169.90ms\nstep:8574/10000 train_loss:3.8058 train_time:1455041ms step_avg:169.90ms\nstep:8575/10000 train_loss:3.7758 train_time:1455209ms step_avg:169.90ms\nstep:8576/10000 train_loss:3.7968 train_time:1455378ms step_avg:169.90ms\nstep:8577/10000 train_loss:3.8388 train_time:1455547ms step_avg:169.90ms\nstep:8578/10000 train_loss:3.6614 train_time:1455720ms step_avg:169.90ms\nstep:8579/10000 train_loss:3.8560 train_time:1455889ms step_avg:169.90ms\nstep:8580/10000 train_loss:3.8818 train_time:1456061ms step_avg:169.90ms\nstep:8581/10000 train_loss:4.0320 train_time:1456231ms step_avg:169.90ms\nstep:8582/10000 train_loss:3.9509 train_time:1456401ms step_avg:169.90ms\nstep:8583/10000 train_loss:3.9017 train_time:1456570ms step_avg:169.90ms\nstep:8584/10000 train_loss:4.1645 train_time:1456738ms step_avg:169.90ms\nstep:8585/10000 train_loss:3.8610 train_time:1456906ms step_avg:169.90ms\nstep:8586/10000 train_loss:3.7250 train_time:1457078ms step_avg:169.90ms\nstep:8587/10000 train_loss:3.7538 train_time:1457247ms step_avg:169.90ms\nstep:8588/10000 train_loss:3.7019 train_time:1457418ms step_avg:169.90ms\nstep:8589/10000 train_loss:3.8910 train_time:1457584ms step_avg:169.90ms\nstep:8590/10000 train_loss:3.7628 train_time:1457754ms step_avg:169.90ms\nstep:8591/10000 train_loss:3.7903 train_time:1457923ms step_avg:169.90ms\nstep:8592/10000 train_loss:3.7491 train_time:1458093ms step_avg:169.90ms\nstep:8593/10000 train_loss:3.7700 train_time:1458261ms step_avg:169.90ms\nstep:8594/10000 train_loss:3.8162 train_time:1458434ms step_avg:169.90ms\nstep:8595/10000 train_loss:3.6693 train_time:1458607ms step_avg:169.90ms\nstep:8596/10000 train_loss:3.7742 train_time:1458776ms step_avg:169.90ms\nstep:8597/10000 train_loss:3.7660 train_time:1458944ms step_avg:169.90ms\nstep:8598/10000 train_loss:3.7872 train_time:1459114ms step_avg:169.90ms\nstep:8599/10000 train_loss:3.6684 train_time:1459283ms step_avg:169.90ms\nstep:8600/10000 train_loss:3.8914 train_time:1459452ms step_avg:169.90ms\nstep:8601/10000 train_loss:4.0158 train_time:1459619ms step_avg:169.90ms\nstep:8602/10000 train_loss:3.7829 train_time:1459788ms step_avg:169.90ms\nstep:8603/10000 train_loss:3.8051 train_time:1459959ms step_avg:169.90ms\nstep:8604/10000 train_loss:3.8359 train_time:1460127ms step_avg:169.90ms\nstep:8605/10000 train_loss:3.8576 train_time:1460296ms step_avg:169.90ms\nstep:8606/10000 train_loss:3.7281 train_time:1460466ms step_avg:169.90ms\nstep:8607/10000 train_loss:3.7952 train_time:1460637ms step_avg:169.90ms\nstep:8608/10000 train_loss:4.2569 train_time:1460808ms step_avg:169.90ms\nstep:8609/10000 train_loss:3.8062 train_time:1460979ms step_avg:169.90ms\nstep:8610/10000 train_loss:4.0512 train_time:1461147ms step_avg:169.90ms\nstep:8611/10000 train_loss:3.7000 train_time:1461318ms step_avg:169.90ms\nstep:8612/10000 train_loss:3.7456 train_time:1461486ms step_avg:169.90ms\nstep:8613/10000 train_loss:3.7811 train_time:1461660ms step_avg:169.90ms\nstep:8614/10000 train_loss:3.9654 train_time:1461829ms step_avg:169.90ms\nstep:8615/10000 train_loss:3.7594 train_time:1462000ms step_avg:169.90ms\nstep:8616/10000 train_loss:3.7596 train_time:1462169ms step_avg:169.90ms\nstep:8617/10000 train_loss:3.7509 train_time:1462338ms step_avg:169.90ms\nstep:8618/10000 train_loss:3.7285 train_time:1462507ms step_avg:169.90ms\nstep:8619/10000 train_loss:3.8052 train_time:1462676ms step_avg:169.90ms\nstep:8620/10000 train_loss:3.7806 train_time:1462846ms step_avg:169.90ms\nstep:8621/10000 train_loss:3.8058 train_time:1463015ms step_avg:169.90ms\nstep:8622/10000 train_loss:3.8739 train_time:1463183ms step_avg:169.90ms\nstep:8623/10000 train_loss:4.0662 train_time:1463354ms step_avg:169.90ms\nstep:8624/10000 train_loss:3.8381 train_time:1463525ms step_avg:169.90ms\nstep:8625/10000 train_loss:3.7637 train_time:1463694ms step_avg:169.90ms\nstep:8626/10000 train_loss:3.8530 train_time:1463863ms step_avg:169.90ms\nstep:8627/10000 train_loss:3.7825 train_time:1464031ms step_avg:169.90ms\nstep:8628/10000 train_loss:3.8160 train_time:1464200ms step_avg:169.90ms\nstep:8629/10000 train_loss:4.7839 train_time:1464369ms step_avg:169.90ms\nstep:8630/10000 train_loss:3.8320 train_time:1464538ms step_avg:169.90ms\nstep:8631/10000 train_loss:3.6845 train_time:1464707ms step_avg:169.90ms\nstep:8632/10000 train_loss:3.7263 train_time:1464875ms step_avg:169.90ms\nstep:8633/10000 train_loss:3.7500 train_time:1465043ms step_avg:169.90ms\nstep:8634/10000 train_loss:3.8182 train_time:1465213ms step_avg:169.90ms\nstep:8635/10000 train_loss:3.9071 train_time:1465381ms step_avg:169.90ms\nstep:8636/10000 train_loss:3.9434 train_time:1465552ms step_avg:169.90ms\nstep:8637/10000 train_loss:4.0005 train_time:1465724ms step_avg:169.90ms\nstep:8638/10000 train_loss:3.6116 train_time:1465894ms step_avg:169.90ms\nstep:8639/10000 train_loss:4.0168 train_time:1466062ms step_avg:169.90ms\nstep:8640/10000 train_loss:3.5807 train_time:1466232ms step_avg:169.90ms\nstep:8641/10000 train_loss:3.8452 train_time:1466403ms step_avg:169.90ms\nstep:8642/10000 train_loss:3.7724 train_time:1466573ms step_avg:169.90ms\nstep:8643/10000 train_loss:3.7706 train_time:1466746ms step_avg:169.90ms\nstep:8644/10000 train_loss:3.7305 train_time:1466914ms step_avg:169.90ms\nstep:8645/10000 train_loss:3.8038 train_time:1467082ms step_avg:169.90ms\nstep:8646/10000 train_loss:3.6942 train_time:1467251ms step_avg:169.90ms\nstep:8647/10000 train_loss:3.6571 train_time:1467422ms step_avg:169.90ms\nstep:8648/10000 train_loss:3.8646 train_time:1467592ms step_avg:169.90ms\nstep:8649/10000 train_loss:3.7912 train_time:1467761ms step_avg:169.90ms\nstep:8650/10000 train_loss:3.8126 train_time:1467931ms step_avg:169.90ms\nstep:8651/10000 train_loss:3.8534 train_time:1468101ms step_avg:169.90ms\nstep:8652/10000 train_loss:3.7910 train_time:1468271ms step_avg:169.90ms\nstep:8653/10000 train_loss:3.7070 train_time:1468438ms step_avg:169.90ms\nstep:8654/10000 train_loss:3.6533 train_time:1468607ms step_avg:169.90ms\nstep:8655/10000 train_loss:3.6553 train_time:1468777ms step_avg:169.90ms\nstep:8656/10000 train_loss:3.9331 train_time:1468953ms step_avg:169.90ms\nstep:8657/10000 train_loss:3.6807 train_time:1469123ms step_avg:169.90ms\nstep:8658/10000 train_loss:3.7808 train_time:1469290ms step_avg:169.90ms\nstep:8659/10000 train_loss:4.0545 train_time:1469454ms step_avg:169.90ms\nstep:8660/10000 train_loss:3.8839 train_time:1469623ms step_avg:169.90ms\nstep:8661/10000 train_loss:3.7869 train_time:1469794ms step_avg:169.90ms\nstep:8662/10000 train_loss:3.8827 train_time:1469962ms step_avg:169.90ms\nstep:8663/10000 train_loss:3.8246 train_time:1470132ms step_avg:169.90ms\nstep:8664/10000 train_loss:3.7918 train_time:1470302ms step_avg:169.90ms\nstep:8665/10000 train_loss:3.6554 train_time:1470474ms step_avg:169.90ms\nstep:8666/10000 train_loss:3.7626 train_time:1470642ms step_avg:169.90ms\nstep:8667/10000 train_loss:3.8824 train_time:1470813ms step_avg:169.90ms\nstep:8668/10000 train_loss:3.7519 train_time:1470982ms step_avg:169.90ms\nstep:8669/10000 train_loss:3.6891 train_time:1471154ms step_avg:169.90ms\nstep:8670/10000 train_loss:3.7696 train_time:1471323ms step_avg:169.90ms\nstep:8671/10000 train_loss:3.9233 train_time:1471494ms step_avg:169.90ms\nstep:8672/10000 train_loss:3.7777 train_time:1471663ms step_avg:169.90ms\nstep:8673/10000 train_loss:3.7868 train_time:1471833ms step_avg:169.90ms\nstep:8674/10000 train_loss:3.9463 train_time:1472002ms step_avg:169.90ms\nstep:8675/10000 train_loss:3.9893 train_time:1472173ms step_avg:169.90ms\nstep:8676/10000 train_loss:3.8490 train_time:1472342ms step_avg:169.90ms\nstep:8677/10000 train_loss:3.8056 train_time:1472512ms step_avg:169.90ms\nstep:8678/10000 train_loss:3.7662 train_time:1472682ms step_avg:169.90ms\nstep:8679/10000 train_loss:3.7812 train_time:1472853ms step_avg:169.90ms\nstep:8680/10000 train_loss:3.8630 train_time:1473020ms step_avg:169.90ms\nstep:8681/10000 train_loss:4.1389 train_time:1473190ms step_avg:169.90ms\nstep:8682/10000 train_loss:3.6961 train_time:1473359ms step_avg:169.90ms\nstep:8683/10000 train_loss:3.8061 train_time:1473528ms step_avg:169.90ms\nstep:8684/10000 train_loss:3.8364 train_time:1473699ms step_avg:169.90ms\nstep:8685/10000 train_loss:3.9105 train_time:1473868ms step_avg:169.90ms\nstep:8686/10000 train_loss:3.9033 train_time:1474039ms step_avg:169.90ms\nstep:8687/10000 train_loss:3.8172 train_time:1474207ms step_avg:169.90ms\nstep:8688/10000 train_loss:3.6852 train_time:1474376ms step_avg:169.90ms\nstep:8689/10000 train_loss:3.7206 train_time:1474544ms step_avg:169.90ms\nstep:8690/10000 train_loss:3.8414 train_time:1474714ms step_avg:169.90ms\nstep:8691/10000 train_loss:3.9311 train_time:1474883ms step_avg:169.90ms\nstep:8692/10000 train_loss:3.7542 train_time:1475052ms step_avg:169.90ms\nstep:8693/10000 train_loss:3.9273 train_time:1475221ms step_avg:169.90ms\nstep:8694/10000 train_loss:3.6636 train_time:1475392ms step_avg:169.90ms\nstep:8695/10000 train_loss:3.9348 train_time:1475562ms step_avg:169.90ms\nstep:8696/10000 train_loss:3.8367 train_time:1475731ms step_avg:169.90ms\nstep:8697/10000 train_loss:3.7356 train_time:1475900ms step_avg:169.90ms\nstep:8698/10000 train_loss:4.1072 train_time:1476070ms step_avg:169.90ms\nstep:8699/10000 train_loss:3.8600 train_time:1476238ms step_avg:169.90ms\nstep:8700/10000 train_loss:3.9234 train_time:1476409ms step_avg:169.90ms\nstep:8701/10000 train_loss:3.7086 train_time:1476575ms step_avg:169.90ms\nstep:8702/10000 train_loss:3.8165 train_time:1476744ms step_avg:169.90ms\nstep:8703/10000 train_loss:3.7891 train_time:1476914ms step_avg:169.90ms\nstep:8704/10000 train_loss:3.7547 train_time:1477084ms step_avg:169.90ms\nstep:8705/10000 train_loss:3.8784 train_time:1477255ms step_avg:169.90ms\nstep:8706/10000 train_loss:3.7852 train_time:1477424ms step_avg:169.90ms\nstep:8707/10000 train_loss:3.6575 train_time:1477593ms step_avg:169.90ms\nstep:8708/10000 train_loss:3.7199 train_time:1477761ms step_avg:169.90ms\nstep:8709/10000 train_loss:3.7151 train_time:1477931ms step_avg:169.90ms\nstep:8710/10000 train_loss:3.8920 train_time:1478099ms step_avg:169.90ms\nstep:8711/10000 train_loss:3.8592 train_time:1478269ms step_avg:169.90ms\nstep:8712/10000 train_loss:3.7093 train_time:1478440ms step_avg:169.90ms\nstep:8713/10000 train_loss:3.8861 train_time:1478608ms step_avg:169.90ms\nstep:8714/10000 train_loss:4.1920 train_time:1478776ms step_avg:169.90ms\nstep:8715/10000 train_loss:3.7375 train_time:1478945ms step_avg:169.90ms\nstep:8716/10000 train_loss:3.7923 train_time:1479115ms step_avg:169.90ms\nstep:8717/10000 train_loss:3.7605 train_time:1479283ms step_avg:169.90ms\nstep:8718/10000 train_loss:3.8379 train_time:1479452ms step_avg:169.90ms\nstep:8719/10000 train_loss:3.7140 train_time:1479619ms step_avg:169.90ms\nstep:8720/10000 train_loss:3.7695 train_time:1479788ms step_avg:169.90ms\nstep:8721/10000 train_loss:3.8394 train_time:1479958ms step_avg:169.90ms\nstep:8722/10000 train_loss:3.9536 train_time:1480128ms step_avg:169.90ms\nstep:8723/10000 train_loss:3.8084 train_time:1480297ms step_avg:169.90ms\nstep:8724/10000 train_loss:3.7287 train_time:1480464ms step_avg:169.89ms\nstep:8725/10000 train_loss:3.8685 train_time:1480634ms step_avg:169.89ms\nstep:8726/10000 train_loss:3.8146 train_time:1480802ms step_avg:169.89ms\nstep:8727/10000 train_loss:3.7725 train_time:1480970ms step_avg:169.89ms\nstep:8728/10000 train_loss:3.9412 train_time:1481142ms step_avg:169.89ms\nstep:8729/10000 train_loss:3.7447 train_time:1481313ms step_avg:169.89ms\nstep:8730/10000 train_loss:4.0719 train_time:1481482ms step_avg:169.89ms\nstep:8731/10000 train_loss:3.7293 train_time:1481653ms step_avg:169.89ms\nstep:8732/10000 train_loss:3.7524 train_time:1481821ms step_avg:169.89ms\nstep:8733/10000 train_loss:3.7204 train_time:1481990ms step_avg:169.89ms\nstep:8734/10000 train_loss:3.7735 train_time:1482161ms step_avg:169.89ms\nstep:8735/10000 train_loss:3.8647 train_time:1482334ms step_avg:169.89ms\nstep:8736/10000 train_loss:3.7411 train_time:1482503ms step_avg:169.89ms\nstep:8737/10000 train_loss:3.7297 train_time:1482675ms step_avg:169.90ms\nstep:8738/10000 train_loss:3.6420 train_time:1482843ms step_avg:169.89ms\nstep:8739/10000 train_loss:3.8900 train_time:1483013ms step_avg:169.89ms\nstep:8740/10000 train_loss:3.8636 train_time:1483183ms step_avg:169.89ms\nstep:8741/10000 train_loss:3.9155 train_time:1483354ms step_avg:169.90ms\nstep:8742/10000 train_loss:4.2396 train_time:1483523ms step_avg:169.89ms\nstep:8743/10000 train_loss:3.8354 train_time:1483693ms step_avg:169.90ms\nstep:8744/10000 train_loss:3.7893 train_time:1483861ms step_avg:169.89ms\nstep:8745/10000 train_loss:3.7060 train_time:1484028ms step_avg:169.89ms\nstep:8746/10000 train_loss:3.9106 train_time:1484197ms step_avg:169.89ms\nstep:8747/10000 train_loss:3.8326 train_time:1484368ms step_avg:169.89ms\nstep:8748/10000 train_loss:3.8158 train_time:1484541ms step_avg:169.89ms\nstep:8749/10000 train_loss:3.9263 train_time:1484710ms step_avg:169.89ms\nstep:8750/10000 train_loss:3.7715 train_time:1484879ms step_avg:169.89ms\nstep:8751/10000 train_loss:3.6601 train_time:1485047ms step_avg:169.89ms\nstep:8752/10000 train_loss:3.6948 train_time:1485216ms step_avg:169.89ms\nstep:8753/10000 train_loss:3.7587 train_time:1485392ms step_avg:169.90ms\nstep:8754/10000 train_loss:3.7393 train_time:1485556ms step_avg:169.89ms\nstep:8755/10000 train_loss:3.7163 train_time:1485725ms step_avg:169.89ms\nstep:8756/10000 train_loss:3.8317 train_time:1485893ms step_avg:169.89ms\nstep:8757/10000 train_loss:3.6234 train_time:1486064ms step_avg:169.89ms\nstep:8758/10000 train_loss:3.8056 train_time:1486234ms step_avg:169.89ms\nstep:8759/10000 train_loss:3.7969 train_time:1486404ms step_avg:169.89ms\nstep:8760/10000 train_loss:4.0234 train_time:1486573ms step_avg:169.89ms\nstep:8761/10000 train_loss:3.8893 train_time:1486742ms step_avg:169.89ms\nstep:8762/10000 train_loss:3.8661 train_time:1486912ms step_avg:169.89ms\nstep:8763/10000 train_loss:3.8527 train_time:1487082ms step_avg:169.89ms\nstep:8764/10000 train_loss:3.6506 train_time:1487252ms step_avg:169.89ms\nstep:8765/10000 train_loss:3.7620 train_time:1487422ms step_avg:169.89ms\nstep:8766/10000 train_loss:3.5474 train_time:1487593ms step_avg:169.89ms\nstep:8767/10000 train_loss:3.8397 train_time:1487761ms step_avg:169.89ms\nstep:8768/10000 train_loss:3.7142 train_time:1487933ms step_avg:169.89ms\nstep:8769/10000 train_loss:3.7249 train_time:1488102ms step_avg:169.89ms\nstep:8770/10000 train_loss:3.7190 train_time:1488273ms step_avg:169.89ms\nstep:8771/10000 train_loss:3.7020 train_time:1488441ms step_avg:169.89ms\nstep:8772/10000 train_loss:3.6109 train_time:1488610ms step_avg:169.89ms\nstep:8773/10000 train_loss:3.7857 train_time:1488779ms step_avg:169.89ms\nstep:8774/10000 train_loss:3.7920 train_time:1488949ms step_avg:169.89ms\nstep:8775/10000 train_loss:3.7316 train_time:1489119ms step_avg:169.89ms\nstep:8776/10000 train_loss:3.6995 train_time:1489288ms step_avg:169.89ms\nstep:8777/10000 train_loss:3.7208 train_time:1489458ms step_avg:169.89ms\nstep:8778/10000 train_loss:3.6783 train_time:1489628ms step_avg:169.89ms\nstep:8779/10000 train_loss:3.6784 train_time:1489794ms step_avg:169.89ms\nstep:8780/10000 train_loss:3.8380 train_time:1489962ms step_avg:169.89ms\nstep:8781/10000 train_loss:3.7751 train_time:1490132ms step_avg:169.89ms\nstep:8782/10000 train_loss:3.8658 train_time:1490304ms step_avg:169.89ms\nstep:8783/10000 train_loss:3.7675 train_time:1490476ms step_avg:169.89ms\nstep:8784/10000 train_loss:3.7985 train_time:1490646ms step_avg:169.89ms\nstep:8785/10000 train_loss:3.6106 train_time:1490814ms step_avg:169.89ms\nstep:8786/10000 train_loss:3.8806 train_time:1490982ms step_avg:169.89ms\nstep:8787/10000 train_loss:3.7322 train_time:1491154ms step_avg:169.89ms\nstep:8788/10000 train_loss:4.0022 train_time:1491322ms step_avg:169.89ms\nstep:8789/10000 train_loss:3.8074 train_time:1491494ms step_avg:169.89ms\nstep:8790/10000 train_loss:3.8899 train_time:1491664ms step_avg:169.89ms\nstep:8791/10000 train_loss:3.7673 train_time:1491836ms step_avg:169.89ms\nstep:8792/10000 train_loss:3.7840 train_time:1492002ms step_avg:169.89ms\nstep:8793/10000 train_loss:3.9079 train_time:1492172ms step_avg:169.89ms\nstep:8794/10000 train_loss:3.8490 train_time:1492341ms step_avg:169.89ms\nstep:8795/10000 train_loss:3.8417 train_time:1492513ms step_avg:169.89ms\nstep:8796/10000 train_loss:3.8628 train_time:1492680ms step_avg:169.89ms\nstep:8797/10000 train_loss:3.8333 train_time:1492850ms step_avg:169.89ms\nstep:8798/10000 train_loss:3.8578 train_time:1493020ms step_avg:169.89ms\nstep:8799/10000 train_loss:3.9488 train_time:1493189ms step_avg:169.89ms\nstep:8800/10000 train_loss:3.7798 train_time:1493358ms step_avg:169.89ms\nstep:8801/10000 train_loss:3.7401 train_time:1493527ms step_avg:169.89ms\nstep:8802/10000 train_loss:3.8521 train_time:1493699ms step_avg:169.89ms\nstep:8803/10000 train_loss:3.7501 train_time:1493869ms step_avg:169.89ms\nstep:8804/10000 train_loss:3.6888 train_time:1494037ms step_avg:169.89ms\nstep:8805/10000 train_loss:3.8923 train_time:1494206ms step_avg:169.89ms\nstep:8806/10000 train_loss:3.8169 train_time:1494377ms step_avg:169.89ms\nstep:8807/10000 train_loss:3.8013 train_time:1494544ms step_avg:169.89ms\nstep:8808/10000 train_loss:3.7905 train_time:1494714ms step_avg:169.89ms\nstep:8809/10000 train_loss:3.7966 train_time:1494882ms step_avg:169.89ms\nstep:8810/10000 train_loss:3.9566 train_time:1495053ms step_avg:169.89ms\nstep:8811/10000 train_loss:3.7421 train_time:1495220ms step_avg:169.89ms\nstep:8812/10000 train_loss:3.7485 train_time:1495388ms step_avg:169.89ms\nstep:8813/10000 train_loss:3.8416 train_time:1495556ms step_avg:169.89ms\nstep:8814/10000 train_loss:3.7746 train_time:1495726ms step_avg:169.89ms\nstep:8815/10000 train_loss:3.7513 train_time:1495897ms step_avg:169.89ms\nstep:8816/10000 train_loss:3.7419 train_time:1496068ms step_avg:169.89ms\nstep:8817/10000 train_loss:3.7292 train_time:1496239ms step_avg:169.89ms\nstep:8818/10000 train_loss:3.8468 train_time:1496409ms step_avg:169.89ms\nstep:8819/10000 train_loss:3.6868 train_time:1496578ms step_avg:169.89ms\nstep:8820/10000 train_loss:3.8041 train_time:1496748ms step_avg:169.89ms\nstep:8821/10000 train_loss:3.8778 train_time:1496919ms step_avg:169.89ms\nstep:8822/10000 train_loss:3.8068 train_time:1497089ms step_avg:169.89ms\nstep:8823/10000 train_loss:3.7597 train_time:1497261ms step_avg:169.89ms\nstep:8824/10000 train_loss:3.7561 train_time:1497431ms step_avg:169.89ms\nstep:8825/10000 train_loss:3.7849 train_time:1497601ms step_avg:169.89ms\nstep:8826/10000 train_loss:3.6863 train_time:1497772ms step_avg:169.89ms\nstep:8827/10000 train_loss:3.7320 train_time:1497943ms step_avg:169.89ms\nstep:8828/10000 train_loss:3.7386 train_time:1498114ms step_avg:169.89ms\nstep:8829/10000 train_loss:3.9080 train_time:1498282ms step_avg:169.89ms\nstep:8830/10000 train_loss:3.9440 train_time:1498450ms step_avg:169.89ms\nstep:8831/10000 train_loss:5.0594 train_time:1498620ms step_avg:169.89ms\nstep:8832/10000 train_loss:4.2489 train_time:1498788ms step_avg:169.89ms\nstep:8833/10000 train_loss:3.8105 train_time:1498959ms step_avg:169.89ms\nstep:8834/10000 train_loss:3.9138 train_time:1499130ms step_avg:169.89ms\nstep:8835/10000 train_loss:3.8579 train_time:1499302ms step_avg:169.89ms\nstep:8836/10000 train_loss:3.6684 train_time:1499471ms step_avg:169.89ms\nstep:8837/10000 train_loss:3.7963 train_time:1499641ms step_avg:169.89ms\nstep:8838/10000 train_loss:3.6773 train_time:1499811ms step_avg:169.89ms\nstep:8839/10000 train_loss:3.8022 train_time:1499980ms step_avg:169.89ms\nstep:8840/10000 train_loss:3.7945 train_time:1500150ms step_avg:169.89ms\nstep:8841/10000 train_loss:3.8589 train_time:1500322ms step_avg:169.89ms\nstep:8842/10000 train_loss:3.8423 train_time:1500494ms step_avg:169.89ms\nstep:8843/10000 train_loss:3.7917 train_time:1500661ms step_avg:169.89ms\nstep:8844/10000 train_loss:3.7817 train_time:1500829ms step_avg:169.89ms\nstep:8845/10000 train_loss:3.9039 train_time:1500998ms step_avg:169.89ms\nstep:8846/10000 train_loss:3.7709 train_time:1501166ms step_avg:169.89ms\nstep:8847/10000 train_loss:3.8067 train_time:1501337ms step_avg:169.89ms\nstep:8848/10000 train_loss:3.8434 train_time:1501507ms step_avg:169.89ms\nstep:8849/10000 train_loss:3.8383 train_time:1501676ms step_avg:169.89ms\nstep:8850/10000 train_loss:3.7610 train_time:1501845ms step_avg:169.89ms\nstep:8851/10000 train_loss:3.8687 train_time:1502017ms step_avg:169.89ms\nstep:8852/10000 train_loss:3.6986 train_time:1502182ms step_avg:169.89ms\nstep:8853/10000 train_loss:3.9332 train_time:1502354ms step_avg:169.89ms\nstep:8854/10000 train_loss:3.7361 train_time:1502525ms step_avg:169.89ms\nstep:8855/10000 train_loss:3.8878 train_time:1502692ms step_avg:169.89ms\nstep:8856/10000 train_loss:3.7875 train_time:1502863ms step_avg:169.89ms\nstep:8857/10000 train_loss:3.8296 train_time:1503032ms step_avg:169.89ms\nstep:8858/10000 train_loss:3.8102 train_time:1503202ms step_avg:169.89ms\nstep:8859/10000 train_loss:3.7427 train_time:1503373ms step_avg:169.89ms\nstep:8860/10000 train_loss:3.7978 train_time:1503544ms step_avg:169.89ms\nstep:8861/10000 train_loss:3.7454 train_time:1503715ms step_avg:169.89ms\nstep:8862/10000 train_loss:3.6128 train_time:1503882ms step_avg:169.89ms\nstep:8863/10000 train_loss:3.8314 train_time:1504052ms step_avg:169.89ms\nstep:8864/10000 train_loss:3.7919 train_time:1504220ms step_avg:169.89ms\nstep:8865/10000 train_loss:3.7423 train_time:1504391ms step_avg:169.89ms\nstep:8866/10000 train_loss:3.6160 train_time:1504561ms step_avg:169.89ms\nstep:8867/10000 train_loss:3.8276 train_time:1504731ms step_avg:169.89ms\nstep:8868/10000 train_loss:3.8375 train_time:1504901ms step_avg:169.89ms\nstep:8869/10000 train_loss:3.7880 train_time:1505070ms step_avg:169.89ms\nstep:8870/10000 train_loss:3.9406 train_time:1505241ms step_avg:169.89ms\nstep:8871/10000 train_loss:3.9325 train_time:1505413ms step_avg:169.89ms\nstep:8872/10000 train_loss:3.6858 train_time:1505581ms step_avg:169.89ms\nstep:8873/10000 train_loss:3.8851 train_time:1505751ms step_avg:169.89ms\nstep:8874/10000 train_loss:3.8086 train_time:1505921ms step_avg:169.89ms\nstep:8875/10000 train_loss:3.6669 train_time:1506092ms step_avg:169.89ms\nstep:8876/10000 train_loss:3.7385 train_time:1506261ms step_avg:169.89ms\nstep:8877/10000 train_loss:3.8074 train_time:1506432ms step_avg:169.89ms\nstep:8878/10000 train_loss:3.9004 train_time:1506601ms step_avg:169.89ms\nstep:8879/10000 train_loss:3.7535 train_time:1506779ms step_avg:169.89ms\nstep:8880/10000 train_loss:3.8860 train_time:1506949ms step_avg:169.89ms\nstep:8881/10000 train_loss:3.6680 train_time:1507118ms step_avg:169.89ms\nstep:8882/10000 train_loss:3.9145 train_time:1507288ms step_avg:169.89ms\nstep:8883/10000 train_loss:3.8289 train_time:1507460ms step_avg:169.89ms\nstep:8884/10000 train_loss:3.8495 train_time:1507630ms step_avg:169.89ms\nstep:8885/10000 train_loss:3.4859 train_time:1507798ms step_avg:169.89ms\nstep:8886/10000 train_loss:4.1614 train_time:1507969ms step_avg:169.89ms\nstep:8887/10000 train_loss:3.8749 train_time:1508140ms step_avg:169.89ms\nstep:8888/10000 train_loss:3.7892 train_time:1508307ms step_avg:169.89ms\nstep:8889/10000 train_loss:3.7057 train_time:1508473ms step_avg:169.89ms\nstep:8890/10000 train_loss:3.8703 train_time:1508644ms step_avg:169.89ms\nstep:8891/10000 train_loss:3.7986 train_time:1508817ms step_avg:169.89ms\nstep:8892/10000 train_loss:3.7953 train_time:1508984ms step_avg:169.89ms\nstep:8893/10000 train_loss:3.6930 train_time:1509155ms step_avg:169.89ms\nstep:8894/10000 train_loss:3.8219 train_time:1509323ms step_avg:169.89ms\nstep:8895/10000 train_loss:3.5975 train_time:1509494ms step_avg:169.89ms\nstep:8896/10000 train_loss:3.7084 train_time:1509662ms step_avg:169.89ms\nstep:8897/10000 train_loss:3.8096 train_time:1509832ms step_avg:169.89ms\nstep:8898/10000 train_loss:3.8199 train_time:1510001ms step_avg:169.89ms\nstep:8899/10000 train_loss:3.9835 train_time:1510173ms step_avg:169.89ms\nstep:8900/10000 train_loss:3.9778 train_time:1510342ms step_avg:169.89ms\nstep:8901/10000 train_loss:3.7094 train_time:1510514ms step_avg:169.89ms\nstep:8902/10000 train_loss:3.6977 train_time:1510690ms step_avg:169.89ms\nstep:8903/10000 train_loss:3.8218 train_time:1510859ms step_avg:169.89ms\nstep:8904/10000 train_loss:3.8733 train_time:1511026ms step_avg:169.89ms\nstep:8905/10000 train_loss:3.8029 train_time:1511190ms step_avg:169.89ms\nstep:8906/10000 train_loss:3.7602 train_time:1511356ms step_avg:169.89ms\nstep:8907/10000 train_loss:3.7544 train_time:1511525ms step_avg:169.89ms\nstep:8908/10000 train_loss:3.9118 train_time:1511697ms step_avg:169.89ms\nstep:8909/10000 train_loss:3.7138 train_time:1511865ms step_avg:169.89ms\nstep:8910/10000 train_loss:3.7515 train_time:1512038ms step_avg:169.89ms\nstep:8911/10000 train_loss:3.8777 train_time:1512206ms step_avg:169.89ms\nstep:8912/10000 train_loss:3.7773 train_time:1512373ms step_avg:169.89ms\nstep:8913/10000 train_loss:3.7361 train_time:1512541ms step_avg:169.89ms\nstep:8914/10000 train_loss:4.0871 train_time:1512713ms step_avg:169.89ms\nstep:8915/10000 train_loss:3.7177 train_time:1512883ms step_avg:169.89ms\nstep:8916/10000 train_loss:3.8917 train_time:1513053ms step_avg:169.89ms\nstep:8917/10000 train_loss:3.7073 train_time:1513221ms step_avg:169.89ms\nstep:8918/10000 train_loss:3.8464 train_time:1513392ms step_avg:169.89ms\nstep:8919/10000 train_loss:3.7772 train_time:1513561ms step_avg:169.89ms\nstep:8920/10000 train_loss:3.6495 train_time:1513733ms step_avg:169.89ms\nstep:8921/10000 train_loss:3.7587 train_time:1513901ms step_avg:169.89ms\nstep:8922/10000 train_loss:3.7629 train_time:1514072ms step_avg:169.89ms\nstep:8923/10000 train_loss:3.7758 train_time:1514242ms step_avg:169.89ms\nstep:8924/10000 train_loss:3.8900 train_time:1514412ms step_avg:169.89ms\nstep:8925/10000 train_loss:3.6805 train_time:1514578ms step_avg:169.89ms\nstep:8926/10000 train_loss:3.8078 train_time:1514747ms step_avg:169.89ms\nstep:8927/10000 train_loss:3.7654 train_time:1514918ms step_avg:169.89ms\nstep:8928/10000 train_loss:3.8479 train_time:1515088ms step_avg:169.89ms\nstep:8929/10000 train_loss:4.1711 train_time:1515259ms step_avg:169.89ms\nstep:8930/10000 train_loss:4.0213 train_time:1515429ms step_avg:169.89ms\nstep:8931/10000 train_loss:3.6374 train_time:1515597ms step_avg:169.89ms\nstep:8932/10000 train_loss:3.6547 train_time:1515765ms step_avg:169.89ms\nstep:8933/10000 train_loss:3.6895 train_time:1515934ms step_avg:169.89ms\nstep:8934/10000 train_loss:3.7145 train_time:1516102ms step_avg:169.89ms\nstep:8935/10000 train_loss:3.7715 train_time:1516272ms step_avg:169.89ms\nstep:8936/10000 train_loss:3.9975 train_time:1516440ms step_avg:169.89ms\nstep:8937/10000 train_loss:3.8472 train_time:1516611ms step_avg:169.89ms\nstep:8938/10000 train_loss:3.8028 train_time:1516779ms step_avg:169.89ms\nstep:8939/10000 train_loss:3.7485 train_time:1516948ms step_avg:169.89ms\nstep:8940/10000 train_loss:3.7005 train_time:1517117ms step_avg:169.89ms\nstep:8941/10000 train_loss:3.8980 train_time:1517286ms step_avg:169.89ms\nstep:8942/10000 train_loss:3.6750 train_time:1517458ms step_avg:169.89ms\nstep:8943/10000 train_loss:3.8113 train_time:1517626ms step_avg:169.89ms\nstep:8944/10000 train_loss:4.1556 train_time:1517793ms step_avg:169.89ms\nstep:8945/10000 train_loss:3.9496 train_time:1517961ms step_avg:169.89ms\nstep:8946/10000 train_loss:3.7134 train_time:1518132ms step_avg:169.89ms\nstep:8947/10000 train_loss:3.6564 train_time:1518301ms step_avg:169.89ms\nstep:8948/10000 train_loss:3.8524 train_time:1518472ms step_avg:169.89ms\nstep:8949/10000 train_loss:3.6746 train_time:1518644ms step_avg:169.89ms\nstep:8950/10000 train_loss:3.7474 train_time:1518813ms step_avg:169.89ms\nstep:8951/10000 train_loss:3.7734 train_time:1518982ms step_avg:169.89ms\nstep:8952/10000 train_loss:3.7684 train_time:1519153ms step_avg:169.89ms\nstep:8953/10000 train_loss:3.7935 train_time:1519321ms step_avg:169.89ms\nstep:8954/10000 train_loss:3.8408 train_time:1519491ms step_avg:169.89ms\nstep:8955/10000 train_loss:3.8175 train_time:1519661ms step_avg:169.89ms\nstep:8956/10000 train_loss:3.8297 train_time:1519833ms step_avg:169.89ms\nstep:8957/10000 train_loss:3.9372 train_time:1520002ms step_avg:169.89ms\nstep:8958/10000 train_loss:3.7815 train_time:1520171ms step_avg:169.89ms\nstep:8959/10000 train_loss:3.7825 train_time:1520342ms step_avg:169.89ms\nstep:8960/10000 train_loss:3.6389 train_time:1520514ms step_avg:169.89ms\nstep:8961/10000 train_loss:3.5838 train_time:1520682ms step_avg:169.89ms\nstep:8962/10000 train_loss:3.8202 train_time:1520851ms step_avg:169.89ms\nstep:8963/10000 train_loss:3.7834 train_time:1521019ms step_avg:169.89ms\nstep:8964/10000 train_loss:3.5983 train_time:1521187ms step_avg:169.89ms\nstep:8965/10000 train_loss:3.9653 train_time:1521357ms step_avg:169.89ms\nstep:8966/10000 train_loss:3.9505 train_time:1521526ms step_avg:169.89ms\nstep:8967/10000 train_loss:3.7374 train_time:1521698ms step_avg:169.89ms\nstep:8968/10000 train_loss:3.8381 train_time:1521866ms step_avg:169.89ms\nstep:8969/10000 train_loss:3.8362 train_time:1522033ms step_avg:169.89ms\nstep:8970/10000 train_loss:3.7613 train_time:1522201ms step_avg:169.89ms\nstep:8971/10000 train_loss:3.6519 train_time:1522372ms step_avg:169.89ms\nstep:8972/10000 train_loss:3.9200 train_time:1522542ms step_avg:169.89ms\nstep:8973/10000 train_loss:3.6477 train_time:1522711ms step_avg:169.89ms\nstep:8974/10000 train_loss:3.8190 train_time:1522881ms step_avg:169.89ms\nstep:8975/10000 train_loss:3.8037 train_time:1523052ms step_avg:169.89ms\nstep:8976/10000 train_loss:3.6427 train_time:1523220ms step_avg:169.89ms\nstep:8977/10000 train_loss:3.7032 train_time:1523390ms step_avg:169.89ms\nstep:8978/10000 train_loss:3.8142 train_time:1523564ms step_avg:169.89ms\nstep:8979/10000 train_loss:3.6451 train_time:1523733ms step_avg:169.89ms\nstep:8980/10000 train_loss:3.7436 train_time:1523904ms step_avg:169.89ms\nstep:8981/10000 train_loss:3.7751 train_time:1524069ms step_avg:169.89ms\nstep:8982/10000 train_loss:3.6409 train_time:1524234ms step_avg:169.89ms\nstep:8983/10000 train_loss:3.7564 train_time:1524402ms step_avg:169.89ms\nstep:8984/10000 train_loss:3.8804 train_time:1524572ms step_avg:169.89ms\nstep:8985/10000 train_loss:3.6938 train_time:1524739ms step_avg:169.89ms\nstep:8986/10000 train_loss:3.7711 train_time:1524910ms step_avg:169.89ms\nstep:8987/10000 train_loss:3.8138 train_time:1525078ms step_avg:169.89ms\nstep:8988/10000 train_loss:3.8658 train_time:1525249ms step_avg:169.89ms\nstep:8989/10000 train_loss:3.6331 train_time:1525419ms step_avg:169.89ms\nstep:8990/10000 train_loss:3.8019 train_time:1525588ms step_avg:169.89ms\nstep:8991/10000 train_loss:3.8768 train_time:1525756ms step_avg:169.89ms\nstep:8992/10000 train_loss:3.7395 train_time:1525926ms step_avg:169.89ms\nstep:8993/10000 train_loss:3.7793 train_time:1526096ms step_avg:169.89ms\nstep:8994/10000 train_loss:3.7128 train_time:1526266ms step_avg:169.89ms\nstep:8995/10000 train_loss:4.0300 train_time:1526435ms step_avg:169.89ms\nstep:8996/10000 train_loss:3.7757 train_time:1526604ms step_avg:169.89ms\nstep:8997/10000 train_loss:3.7251 train_time:1526776ms step_avg:169.89ms\nstep:8998/10000 train_loss:3.7269 train_time:1526945ms step_avg:169.89ms\nstep:8999/10000 train_loss:3.7400 train_time:1527114ms step_avg:169.89ms\nstep:9000/10000 train_loss:3.8133 train_time:1527283ms step_avg:169.89ms\nstep:9001/10000 train_loss:3.7317 train_time:1527455ms step_avg:169.89ms\nstep:9002/10000 train_loss:3.6676 train_time:1527623ms step_avg:169.89ms\nstep:9003/10000 train_loss:3.8334 train_time:1527794ms step_avg:169.89ms\nstep:9004/10000 train_loss:3.6883 train_time:1527963ms step_avg:169.89ms\nstep:9005/10000 train_loss:3.8554 train_time:1528132ms step_avg:169.89ms\nstep:9006/10000 train_loss:3.7542 train_time:1528302ms step_avg:169.89ms\nstep:9007/10000 train_loss:3.8296 train_time:1528474ms step_avg:169.89ms\nstep:9008/10000 train_loss:3.6619 train_time:1528646ms step_avg:169.89ms\nstep:9009/10000 train_loss:3.7910 train_time:1528811ms step_avg:169.89ms\nstep:9010/10000 train_loss:3.7837 train_time:1528977ms step_avg:169.89ms\nstep:9011/10000 train_loss:3.7383 train_time:1529146ms step_avg:169.89ms\nstep:9012/10000 train_loss:3.8931 train_time:1529317ms step_avg:169.89ms\nstep:9013/10000 train_loss:3.7678 train_time:1529486ms step_avg:169.89ms\nstep:9014/10000 train_loss:3.7452 train_time:1529655ms step_avg:169.89ms\nstep:9015/10000 train_loss:3.8176 train_time:1529823ms step_avg:169.89ms\nstep:9016/10000 train_loss:3.6570 train_time:1529992ms step_avg:169.89ms\nstep:9017/10000 train_loss:3.8043 train_time:1530160ms step_avg:169.89ms\nstep:9018/10000 train_loss:3.7548 train_time:1530330ms step_avg:169.89ms\nstep:9019/10000 train_loss:3.8726 train_time:1530501ms step_avg:169.89ms\nstep:9020/10000 train_loss:3.6427 train_time:1530673ms step_avg:169.89ms\nstep:9021/10000 train_loss:3.6506 train_time:1530841ms step_avg:169.89ms\nstep:9022/10000 train_loss:3.8693 train_time:1531008ms step_avg:169.89ms\nstep:9023/10000 train_loss:3.6525 train_time:1531179ms step_avg:169.89ms\nstep:9024/10000 train_loss:3.7022 train_time:1531350ms step_avg:169.89ms\nstep:9025/10000 train_loss:3.8381 train_time:1531520ms step_avg:169.89ms\nstep:9026/10000 train_loss:3.7786 train_time:1531690ms step_avg:169.89ms\nstep:9027/10000 train_loss:3.6802 train_time:1531859ms step_avg:169.89ms\nstep:9028/10000 train_loss:3.7094 train_time:1532027ms step_avg:169.89ms\nstep:9029/10000 train_loss:3.9680 train_time:1532196ms step_avg:169.89ms\nstep:9030/10000 train_loss:3.8806 train_time:1532364ms step_avg:169.89ms\nstep:9031/10000 train_loss:3.9139 train_time:1532533ms step_avg:169.89ms\nstep:9032/10000 train_loss:3.7265 train_time:1532703ms step_avg:169.89ms\nstep:9033/10000 train_loss:3.7131 train_time:1532872ms step_avg:169.88ms\nstep:9034/10000 train_loss:3.7326 train_time:1533042ms step_avg:169.88ms\nstep:9035/10000 train_loss:3.6574 train_time:1533212ms step_avg:169.88ms\nstep:9036/10000 train_loss:3.7849 train_time:1533382ms step_avg:169.89ms\nstep:9037/10000 train_loss:3.7485 train_time:1533554ms step_avg:169.89ms\nstep:9038/10000 train_loss:3.6324 train_time:1533723ms step_avg:169.89ms\nstep:9039/10000 train_loss:3.8240 train_time:1533893ms step_avg:169.89ms\nstep:9040/10000 train_loss:3.6987 train_time:1534063ms step_avg:169.89ms\nstep:9041/10000 train_loss:3.8215 train_time:1534233ms step_avg:169.89ms\nstep:9042/10000 train_loss:3.9243 train_time:1534401ms step_avg:169.88ms\nstep:9043/10000 train_loss:3.8457 train_time:1534572ms step_avg:169.89ms\nstep:9044/10000 train_loss:3.9461 train_time:1534742ms step_avg:169.89ms\nstep:9045/10000 train_loss:3.6486 train_time:1534914ms step_avg:169.89ms\nstep:9046/10000 train_loss:3.7135 train_time:1535082ms step_avg:169.89ms\nstep:9047/10000 train_loss:3.8229 train_time:1535252ms step_avg:169.89ms\nstep:9048/10000 train_loss:3.8566 train_time:1535421ms step_avg:169.89ms\nstep:9049/10000 train_loss:3.8358 train_time:1535591ms step_avg:169.89ms\nstep:9050/10000 train_loss:4.9390 train_time:1535762ms step_avg:169.89ms\nstep:9051/10000 train_loss:3.6454 train_time:1535933ms step_avg:169.89ms\nstep:9052/10000 train_loss:4.1529 train_time:1536101ms step_avg:169.89ms\nstep:9053/10000 train_loss:3.6999 train_time:1536271ms step_avg:169.89ms\nstep:9054/10000 train_loss:3.9034 train_time:1536442ms step_avg:169.89ms\nstep:9055/10000 train_loss:3.7844 train_time:1536613ms step_avg:169.89ms\nstep:9056/10000 train_loss:3.8303 train_time:1536780ms step_avg:169.89ms\nstep:9057/10000 train_loss:3.7351 train_time:1536951ms step_avg:169.89ms\nstep:9058/10000 train_loss:3.7995 train_time:1537119ms step_avg:169.88ms\nstep:9059/10000 train_loss:3.8074 train_time:1537289ms step_avg:169.88ms\nstep:9060/10000 train_loss:3.8944 train_time:1537458ms step_avg:169.88ms\nstep:9061/10000 train_loss:3.8098 train_time:1537629ms step_avg:169.88ms\nstep:9062/10000 train_loss:3.7434 train_time:1537800ms step_avg:169.89ms\nstep:9063/10000 train_loss:3.6171 train_time:1537970ms step_avg:169.89ms\nstep:9064/10000 train_loss:3.8101 train_time:1538139ms step_avg:169.89ms\nstep:9065/10000 train_loss:3.8666 train_time:1538308ms step_avg:169.88ms\nstep:9066/10000 train_loss:3.7422 train_time:1538480ms step_avg:169.89ms\nstep:9067/10000 train_loss:3.7687 train_time:1538651ms step_avg:169.89ms\nstep:9068/10000 train_loss:3.8078 train_time:1538823ms step_avg:169.89ms\nstep:9069/10000 train_loss:3.7057 train_time:1538993ms step_avg:169.89ms\nstep:9070/10000 train_loss:3.7622 train_time:1539161ms step_avg:169.89ms\nstep:9071/10000 train_loss:3.7578 train_time:1539330ms step_avg:169.89ms\nstep:9072/10000 train_loss:3.5289 train_time:1539499ms step_avg:169.89ms\nstep:9073/10000 train_loss:3.8422 train_time:1539668ms step_avg:169.89ms\nstep:9074/10000 train_loss:3.7822 train_time:1539838ms step_avg:169.89ms\nstep:9075/10000 train_loss:3.8037 train_time:1540010ms step_avg:169.89ms\nstep:9076/10000 train_loss:3.6956 train_time:1540177ms step_avg:169.88ms\nstep:9077/10000 train_loss:3.7292 train_time:1540346ms step_avg:169.88ms\nstep:9078/10000 train_loss:3.8306 train_time:1540515ms step_avg:169.88ms\nstep:9079/10000 train_loss:3.8277 train_time:1540684ms step_avg:169.88ms\nstep:9080/10000 train_loss:3.8929 train_time:1540852ms step_avg:169.88ms\nstep:9081/10000 train_loss:3.7324 train_time:1541020ms step_avg:169.88ms\nstep:9082/10000 train_loss:3.7163 train_time:1541190ms step_avg:169.88ms\nstep:9083/10000 train_loss:3.7909 train_time:1541359ms step_avg:169.88ms\nstep:9084/10000 train_loss:3.6195 train_time:1541530ms step_avg:169.88ms\nstep:9085/10000 train_loss:3.8278 train_time:1541700ms step_avg:169.88ms\nstep:9086/10000 train_loss:3.6907 train_time:1541871ms step_avg:169.88ms\nstep:9087/10000 train_loss:3.8026 train_time:1542040ms step_avg:169.88ms\nstep:9088/10000 train_loss:3.6903 train_time:1542211ms step_avg:169.88ms\nstep:9089/10000 train_loss:3.8336 train_time:1542378ms step_avg:169.88ms\nstep:9090/10000 train_loss:3.9090 train_time:1542546ms step_avg:169.88ms\nstep:9091/10000 train_loss:3.7391 train_time:1542716ms step_avg:169.88ms\nstep:9092/10000 train_loss:3.6615 train_time:1542885ms step_avg:169.88ms\nstep:9093/10000 train_loss:3.6905 train_time:1543054ms step_avg:169.88ms\nstep:9094/10000 train_loss:3.7562 train_time:1543223ms step_avg:169.88ms\nstep:9095/10000 train_loss:3.7384 train_time:1543394ms step_avg:169.88ms\nstep:9096/10000 train_loss:3.6401 train_time:1543562ms step_avg:169.88ms\nstep:9097/10000 train_loss:3.6721 train_time:1543730ms step_avg:169.88ms\nstep:9098/10000 train_loss:3.8255 train_time:1543897ms step_avg:169.88ms\nstep:9099/10000 train_loss:3.7651 train_time:1544066ms step_avg:169.88ms\nstep:9100/10000 train_loss:4.0390 train_time:1544236ms step_avg:169.88ms\nstep:9101/10000 train_loss:3.6439 train_time:1544403ms step_avg:169.88ms\nstep:9102/10000 train_loss:3.7423 train_time:1544573ms step_avg:169.88ms\nstep:9103/10000 train_loss:3.7672 train_time:1544744ms step_avg:169.88ms\nstep:9104/10000 train_loss:3.6871 train_time:1544914ms step_avg:169.88ms\nstep:9105/10000 train_loss:3.9022 train_time:1545083ms step_avg:169.88ms\nstep:9106/10000 train_loss:3.8545 train_time:1545255ms step_avg:169.88ms\nstep:9107/10000 train_loss:3.7918 train_time:1545425ms step_avg:169.88ms\nstep:9108/10000 train_loss:3.7070 train_time:1545593ms step_avg:169.88ms\nstep:9109/10000 train_loss:3.7681 train_time:1545762ms step_avg:169.88ms\nstep:9110/10000 train_loss:3.8359 train_time:1545934ms step_avg:169.88ms\nstep:9111/10000 train_loss:3.6884 train_time:1546101ms step_avg:169.88ms\nstep:9112/10000 train_loss:3.7835 train_time:1546271ms step_avg:169.88ms\nstep:9113/10000 train_loss:3.7614 train_time:1546441ms step_avg:169.88ms\nstep:9114/10000 train_loss:3.7586 train_time:1546611ms step_avg:169.88ms\nstep:9115/10000 train_loss:3.8067 train_time:1546780ms step_avg:169.88ms\nstep:9116/10000 train_loss:3.7363 train_time:1546952ms step_avg:169.88ms\nstep:9117/10000 train_loss:3.7457 train_time:1547121ms step_avg:169.88ms\nstep:9118/10000 train_loss:3.8636 train_time:1547291ms step_avg:169.88ms\nstep:9119/10000 train_loss:3.9640 train_time:1547462ms step_avg:169.88ms\nstep:9120/10000 train_loss:3.7484 train_time:1547631ms step_avg:169.88ms\nstep:9121/10000 train_loss:3.7464 train_time:1547801ms step_avg:169.88ms\nstep:9122/10000 train_loss:3.9239 train_time:1547969ms step_avg:169.88ms\nstep:9123/10000 train_loss:3.7394 train_time:1548139ms step_avg:169.88ms\nstep:9124/10000 train_loss:3.6207 train_time:1548309ms step_avg:169.88ms\nstep:9125/10000 train_loss:3.7063 train_time:1548480ms step_avg:169.88ms\nstep:9126/10000 train_loss:3.7135 train_time:1548648ms step_avg:169.88ms\nstep:9127/10000 train_loss:3.7711 train_time:1548816ms step_avg:169.88ms\nstep:9128/10000 train_loss:3.7324 train_time:1548983ms step_avg:169.88ms\nstep:9129/10000 train_loss:3.6943 train_time:1549153ms step_avg:169.88ms\nstep:9130/10000 train_loss:3.6538 train_time:1549324ms step_avg:169.88ms\nstep:9131/10000 train_loss:3.7867 train_time:1549496ms step_avg:169.88ms\nstep:9132/10000 train_loss:3.8707 train_time:1549666ms step_avg:169.88ms\nstep:9133/10000 train_loss:3.8075 train_time:1549836ms step_avg:169.88ms\nstep:9134/10000 train_loss:3.8562 train_time:1550005ms step_avg:169.88ms\nstep:9135/10000 train_loss:3.7376 train_time:1550175ms step_avg:169.88ms\nstep:9136/10000 train_loss:3.6545 train_time:1550345ms step_avg:169.88ms\nstep:9137/10000 train_loss:3.8784 train_time:1550516ms step_avg:169.88ms\nstep:9138/10000 train_loss:3.6951 train_time:1550684ms step_avg:169.88ms\nstep:9139/10000 train_loss:3.8210 train_time:1550853ms step_avg:169.88ms\nstep:9140/10000 train_loss:3.7092 train_time:1551021ms step_avg:169.88ms\nstep:9141/10000 train_loss:3.6732 train_time:1551191ms step_avg:169.88ms\nstep:9142/10000 train_loss:3.6215 train_time:1551360ms step_avg:169.88ms\nstep:9143/10000 train_loss:3.7586 train_time:1551528ms step_avg:169.88ms\nstep:9144/10000 train_loss:3.8618 train_time:1551932ms step_avg:169.91ms\nstep:9145/10000 train_loss:3.8730 train_time:1552096ms step_avg:169.91ms\nstep:9146/10000 train_loss:3.7315 train_time:1552265ms step_avg:169.91ms\nstep:9147/10000 train_loss:3.7721 train_time:1552431ms step_avg:169.91ms\nstep:9148/10000 train_loss:3.8471 train_time:1552598ms step_avg:169.91ms\nstep:9149/10000 train_loss:3.8236 train_time:1552767ms step_avg:169.91ms\nstep:9150/10000 train_loss:3.8740 train_time:1552946ms step_avg:169.91ms\nstep:9151/10000 train_loss:3.6934 train_time:1553118ms step_avg:169.91ms\nstep:9152/10000 train_loss:3.6709 train_time:1553286ms step_avg:169.91ms\nstep:9153/10000 train_loss:3.7674 train_time:1553456ms step_avg:169.91ms\nstep:9154/10000 train_loss:3.7478 train_time:1553624ms step_avg:169.91ms\nstep:9155/10000 train_loss:3.7776 train_time:1553792ms step_avg:169.91ms\nstep:9156/10000 train_loss:3.8050 train_time:1553961ms step_avg:169.91ms\nstep:9157/10000 train_loss:3.7753 train_time:1554133ms step_avg:169.91ms\nstep:9158/10000 train_loss:3.7554 train_time:1554301ms step_avg:169.91ms\nstep:9159/10000 train_loss:3.7759 train_time:1554471ms step_avg:169.91ms\nstep:9160/10000 train_loss:3.7382 train_time:1554641ms step_avg:169.91ms\nstep:9161/10000 train_loss:3.8727 train_time:1554809ms step_avg:169.91ms\nstep:9162/10000 train_loss:3.7716 train_time:1554977ms step_avg:169.91ms\nstep:9163/10000 train_loss:3.7507 train_time:1555145ms step_avg:169.91ms\nstep:9164/10000 train_loss:3.7429 train_time:1555315ms step_avg:169.91ms\nstep:9165/10000 train_loss:3.7897 train_time:1555484ms step_avg:169.91ms\nstep:9166/10000 train_loss:3.7581 train_time:1555654ms step_avg:169.91ms\nstep:9167/10000 train_loss:3.8363 train_time:1555824ms step_avg:169.91ms\nstep:9168/10000 train_loss:3.9362 train_time:1555995ms step_avg:169.91ms\nstep:9169/10000 train_loss:3.7689 train_time:1556162ms step_avg:169.91ms\nstep:9170/10000 train_loss:3.7084 train_time:1556332ms step_avg:169.91ms\nstep:9171/10000 train_loss:3.8159 train_time:1556502ms step_avg:169.91ms\nstep:9172/10000 train_loss:3.7090 train_time:1556671ms step_avg:169.91ms\nstep:9173/10000 train_loss:3.7958 train_time:1556843ms step_avg:169.91ms\nstep:9174/10000 train_loss:3.8536 train_time:1557013ms step_avg:169.91ms\nstep:9175/10000 train_loss:3.8187 train_time:1557182ms step_avg:169.91ms\nstep:9176/10000 train_loss:3.7597 train_time:1557354ms step_avg:169.91ms\nstep:9177/10000 train_loss:3.8004 train_time:1557523ms step_avg:169.91ms\nstep:9178/10000 train_loss:3.8376 train_time:1557693ms step_avg:169.91ms\nstep:9179/10000 train_loss:3.7848 train_time:1557859ms step_avg:169.91ms\nstep:9180/10000 train_loss:3.7057 train_time:1558031ms step_avg:169.91ms\nstep:9181/10000 train_loss:3.7267 train_time:1558199ms step_avg:169.91ms\nstep:9182/10000 train_loss:3.7075 train_time:1558368ms step_avg:169.90ms\nstep:9183/10000 train_loss:3.7047 train_time:1558535ms step_avg:169.90ms\nstep:9184/10000 train_loss:3.7435 train_time:1558703ms step_avg:169.90ms\nstep:9185/10000 train_loss:3.7346 train_time:1558873ms step_avg:169.90ms\nstep:9186/10000 train_loss:4.0323 train_time:1559041ms step_avg:169.90ms\nstep:9187/10000 train_loss:3.8108 train_time:1559211ms step_avg:169.90ms\nstep:9188/10000 train_loss:3.8014 train_time:1559378ms step_avg:169.90ms\nstep:9189/10000 train_loss:3.7347 train_time:1559546ms step_avg:169.90ms\nstep:9190/10000 train_loss:3.7080 train_time:1559715ms step_avg:169.90ms\nstep:9191/10000 train_loss:3.7321 train_time:1559883ms step_avg:169.90ms\nstep:9192/10000 train_loss:3.8650 train_time:1560055ms step_avg:169.90ms\nstep:9193/10000 train_loss:3.6797 train_time:1560224ms step_avg:169.90ms\nstep:9194/10000 train_loss:3.6762 train_time:1560392ms step_avg:169.90ms\nstep:9195/10000 train_loss:3.8327 train_time:1560563ms step_avg:169.90ms\nstep:9196/10000 train_loss:3.7858 train_time:1560733ms step_avg:169.90ms\nstep:9197/10000 train_loss:3.8709 train_time:1560901ms step_avg:169.90ms\nstep:9198/10000 train_loss:3.8375 train_time:1561072ms step_avg:169.90ms\nstep:9199/10000 train_loss:3.8163 train_time:1561240ms step_avg:169.90ms\nstep:9200/10000 train_loss:3.7381 train_time:1561410ms step_avg:169.90ms\nstep:9201/10000 train_loss:3.8246 train_time:1561581ms step_avg:169.90ms\nstep:9202/10000 train_loss:3.7992 train_time:1561754ms step_avg:169.90ms\nstep:9203/10000 train_loss:3.7255 train_time:1561923ms step_avg:169.90ms\nstep:9204/10000 train_loss:3.8755 train_time:1562096ms step_avg:169.90ms\nstep:9205/10000 train_loss:3.7821 train_time:1562263ms step_avg:169.90ms\nstep:9206/10000 train_loss:3.7278 train_time:1562433ms step_avg:169.90ms\nstep:9207/10000 train_loss:3.7625 train_time:1562602ms step_avg:169.90ms\nstep:9208/10000 train_loss:3.8566 train_time:1562773ms step_avg:169.90ms\nstep:9209/10000 train_loss:3.7822 train_time:1562942ms step_avg:169.90ms\nstep:9210/10000 train_loss:4.1327 train_time:1563114ms step_avg:169.90ms\nstep:9211/10000 train_loss:3.6080 train_time:1563281ms step_avg:169.90ms\nstep:9212/10000 train_loss:3.6210 train_time:1563452ms step_avg:169.90ms\nstep:9213/10000 train_loss:3.7352 train_time:1563624ms step_avg:169.90ms\nstep:9214/10000 train_loss:3.6732 train_time:1563794ms step_avg:169.90ms\nstep:9215/10000 train_loss:3.9143 train_time:1563964ms step_avg:169.90ms\nstep:9216/10000 train_loss:3.7181 train_time:1564133ms step_avg:169.90ms\nstep:9217/10000 train_loss:3.6671 train_time:1564301ms step_avg:169.90ms\nstep:9218/10000 train_loss:3.6967 train_time:1564471ms step_avg:169.90ms\nstep:9219/10000 train_loss:3.9080 train_time:1564641ms step_avg:169.90ms\nstep:9220/10000 train_loss:3.7848 train_time:1564813ms step_avg:169.90ms\nstep:9221/10000 train_loss:3.8051 train_time:1564984ms step_avg:169.90ms\nstep:9222/10000 train_loss:4.0173 train_time:1565155ms step_avg:169.90ms\nstep:9223/10000 train_loss:3.6786 train_time:1565323ms step_avg:169.90ms\nstep:9224/10000 train_loss:3.5895 train_time:1565492ms step_avg:169.90ms\nstep:9225/10000 train_loss:3.7792 train_time:1565660ms step_avg:169.90ms\nstep:9226/10000 train_loss:3.7351 train_time:1565831ms step_avg:169.90ms\nstep:9227/10000 train_loss:3.7674 train_time:1566000ms step_avg:169.90ms\nstep:9228/10000 train_loss:3.7279 train_time:1566172ms step_avg:169.90ms\nstep:9229/10000 train_loss:3.7574 train_time:1566341ms step_avg:169.90ms\nstep:9230/10000 train_loss:3.7311 train_time:1566511ms step_avg:169.90ms\nstep:9231/10000 train_loss:3.5506 train_time:1566680ms step_avg:169.90ms\nstep:9232/10000 train_loss:3.8898 train_time:1566849ms step_avg:169.90ms\nstep:9233/10000 train_loss:3.6733 train_time:1567019ms step_avg:169.90ms\nstep:9234/10000 train_loss:3.7406 train_time:1567188ms step_avg:169.90ms\nstep:9235/10000 train_loss:3.6644 train_time:1567355ms step_avg:169.90ms\nstep:9236/10000 train_loss:3.7339 train_time:1567524ms step_avg:169.90ms\nstep:9237/10000 train_loss:3.7609 train_time:1567693ms step_avg:169.90ms\nstep:9238/10000 train_loss:3.6737 train_time:1567862ms step_avg:169.90ms\nstep:9239/10000 train_loss:3.6852 train_time:1568033ms step_avg:169.90ms\nstep:9240/10000 train_loss:3.6809 train_time:1568201ms step_avg:169.90ms\nstep:9241/10000 train_loss:3.8563 train_time:1568370ms step_avg:169.90ms\nstep:9242/10000 train_loss:3.8216 train_time:1568540ms step_avg:169.90ms\nstep:9243/10000 train_loss:3.6835 train_time:1568711ms step_avg:169.90ms\nstep:9244/10000 train_loss:3.7420 train_time:1568878ms step_avg:169.90ms\nstep:9245/10000 train_loss:3.7378 train_time:1569047ms step_avg:169.90ms\nstep:9246/10000 train_loss:3.6811 train_time:1569218ms step_avg:169.90ms\nstep:9247/10000 train_loss:3.7265 train_time:1569387ms step_avg:169.90ms\nstep:9248/10000 train_loss:3.5855 train_time:1569556ms step_avg:169.90ms\nstep:9249/10000 train_loss:3.7412 train_time:1569724ms step_avg:169.90ms\nstep:9250/10000 train_loss:3.7073 train_time:1569892ms step_avg:169.90ms\nstep:9251/10000 train_loss:3.7332 train_time:1570061ms step_avg:169.90ms\nstep:9252/10000 train_loss:3.7890 train_time:1570232ms step_avg:169.90ms\nstep:9253/10000 train_loss:3.7405 train_time:1570400ms step_avg:169.90ms\nstep:9254/10000 train_loss:3.8576 train_time:1570570ms step_avg:169.90ms\nstep:9255/10000 train_loss:3.7685 train_time:1570739ms step_avg:169.90ms\nstep:9256/10000 train_loss:3.6403 train_time:1570908ms step_avg:169.90ms\nstep:9257/10000 train_loss:3.8864 train_time:1571076ms step_avg:169.90ms\nstep:9258/10000 train_loss:3.7768 train_time:1571245ms step_avg:169.90ms\nstep:9259/10000 train_loss:3.9276 train_time:1571423ms step_avg:169.90ms\nstep:9260/10000 train_loss:3.8245 train_time:1571588ms step_avg:169.90ms\nstep:9261/10000 train_loss:3.6795 train_time:1571760ms step_avg:169.90ms\nstep:9262/10000 train_loss:3.6583 train_time:1571931ms step_avg:169.90ms\nstep:9263/10000 train_loss:3.6418 train_time:1572098ms step_avg:169.90ms\nstep:9264/10000 train_loss:3.7937 train_time:1572268ms step_avg:169.90ms\nstep:9265/10000 train_loss:3.7203 train_time:1572440ms step_avg:169.90ms\nstep:9266/10000 train_loss:3.7691 train_time:1572610ms step_avg:169.90ms\nstep:9267/10000 train_loss:3.8322 train_time:1572781ms step_avg:169.90ms\nstep:9268/10000 train_loss:3.7153 train_time:1572953ms step_avg:169.90ms\nstep:9269/10000 train_loss:3.6655 train_time:1573120ms step_avg:169.90ms\nstep:9270/10000 train_loss:3.8840 train_time:1573288ms step_avg:169.90ms\nstep:9271/10000 train_loss:3.8377 train_time:1573458ms step_avg:169.90ms\nstep:9272/10000 train_loss:3.7384 train_time:1573629ms step_avg:169.90ms\nstep:9273/10000 train_loss:3.7403 train_time:1573798ms step_avg:169.90ms\nstep:9274/10000 train_loss:3.7272 train_time:1573971ms step_avg:169.90ms\nstep:9275/10000 train_loss:3.8237 train_time:1574138ms step_avg:169.90ms\nstep:9276/10000 train_loss:3.6340 train_time:1574307ms step_avg:169.90ms\nstep:9277/10000 train_loss:3.7407 train_time:1574475ms step_avg:169.90ms\nstep:9278/10000 train_loss:3.6264 train_time:1574644ms step_avg:169.90ms\nstep:9279/10000 train_loss:3.6579 train_time:1574815ms step_avg:169.90ms\nstep:9280/10000 train_loss:3.9542 train_time:1574984ms step_avg:169.90ms\nstep:9281/10000 train_loss:3.7930 train_time:1575154ms step_avg:169.90ms\nstep:9282/10000 train_loss:3.7310 train_time:1575321ms step_avg:169.90ms\nstep:9283/10000 train_loss:3.7838 train_time:1575489ms step_avg:169.90ms\nstep:9284/10000 train_loss:3.8195 train_time:1575658ms step_avg:169.90ms\nstep:9285/10000 train_loss:3.8661 train_time:1575828ms step_avg:169.90ms\nstep:9286/10000 train_loss:3.6120 train_time:1575996ms step_avg:169.90ms\nstep:9287/10000 train_loss:3.7715 train_time:1576168ms step_avg:169.90ms\nstep:9288/10000 train_loss:3.6696 train_time:1576338ms step_avg:169.90ms\nstep:9289/10000 train_loss:3.8238 train_time:1576507ms step_avg:169.90ms\nstep:9290/10000 train_loss:3.7091 train_time:1576676ms step_avg:169.90ms\nstep:9291/10000 train_loss:3.8192 train_time:1576846ms step_avg:169.90ms\nstep:9292/10000 train_loss:3.6328 train_time:1577017ms step_avg:169.90ms\nstep:9293/10000 train_loss:3.7145 train_time:1577185ms step_avg:169.90ms\nstep:9294/10000 train_loss:3.7554 train_time:1577353ms step_avg:169.90ms\nstep:9295/10000 train_loss:3.7174 train_time:1577521ms step_avg:169.90ms\nstep:9296/10000 train_loss:3.6875 train_time:1577688ms step_avg:169.90ms\nstep:9297/10000 train_loss:3.7617 train_time:1577855ms step_avg:169.90ms\nstep:9298/10000 train_loss:3.5480 train_time:1578025ms step_avg:169.90ms\nstep:9299/10000 train_loss:3.7413 train_time:1578195ms step_avg:169.90ms\nstep:9300/10000 train_loss:3.7344 train_time:1578363ms step_avg:169.90ms\nstep:9301/10000 train_loss:4.1584 train_time:1578532ms step_avg:169.90ms\nstep:9302/10000 train_loss:3.7206 train_time:1578702ms step_avg:169.90ms\nstep:9303/10000 train_loss:3.8085 train_time:1578871ms step_avg:169.90ms\nstep:9304/10000 train_loss:3.6934 train_time:1579039ms step_avg:169.90ms\nstep:9305/10000 train_loss:3.7854 train_time:1579210ms step_avg:169.90ms\nstep:9306/10000 train_loss:3.8002 train_time:1579381ms step_avg:169.90ms\nstep:9307/10000 train_loss:3.7381 train_time:1579551ms step_avg:169.90ms\nstep:9308/10000 train_loss:3.9605 train_time:1579719ms step_avg:169.90ms\nstep:9309/10000 train_loss:3.6738 train_time:1579887ms step_avg:169.90ms\nstep:9310/10000 train_loss:3.6775 train_time:1580055ms step_avg:169.90ms\nstep:9311/10000 train_loss:3.6694 train_time:1580226ms step_avg:169.90ms\nstep:9312/10000 train_loss:3.9227 train_time:1580395ms step_avg:169.90ms\nstep:9313/10000 train_loss:3.7865 train_time:1580564ms step_avg:169.90ms\nstep:9314/10000 train_loss:3.7174 train_time:1580734ms step_avg:169.90ms\nstep:9315/10000 train_loss:3.6905 train_time:1580902ms step_avg:169.90ms\nstep:9316/10000 train_loss:3.5494 train_time:1581073ms step_avg:169.90ms\nstep:9317/10000 train_loss:3.6287 train_time:1581242ms step_avg:169.90ms\nstep:9318/10000 train_loss:3.6513 train_time:1581411ms step_avg:169.90ms\nstep:9319/10000 train_loss:3.8950 train_time:1581583ms step_avg:169.90ms\nstep:9320/10000 train_loss:3.7599 train_time:1581753ms step_avg:169.90ms\nstep:9321/10000 train_loss:3.6517 train_time:1581921ms step_avg:169.90ms\nstep:9322/10000 train_loss:3.7711 train_time:1582091ms step_avg:169.90ms\nstep:9323/10000 train_loss:3.7231 train_time:1582261ms step_avg:169.90ms\nstep:9324/10000 train_loss:3.6241 train_time:1582433ms step_avg:169.90ms\nstep:9325/10000 train_loss:3.7679 train_time:1582601ms step_avg:169.90ms\nstep:9326/10000 train_loss:3.7907 train_time:1582770ms step_avg:169.90ms\nstep:9327/10000 train_loss:3.9233 train_time:1582940ms step_avg:169.90ms\nstep:9328/10000 train_loss:3.7716 train_time:1583109ms step_avg:169.90ms\nstep:9329/10000 train_loss:3.7037 train_time:1583278ms step_avg:169.90ms\nstep:9330/10000 train_loss:3.5709 train_time:1583447ms step_avg:169.90ms\nstep:9331/10000 train_loss:3.7113 train_time:1583619ms step_avg:169.90ms\nstep:9332/10000 train_loss:3.7535 train_time:1583787ms step_avg:169.90ms\nstep:9333/10000 train_loss:3.8025 train_time:1583955ms step_avg:169.90ms\nstep:9334/10000 train_loss:3.6930 train_time:1584123ms step_avg:169.90ms\nstep:9335/10000 train_loss:3.6703 train_time:1584295ms step_avg:169.90ms\nstep:9336/10000 train_loss:3.6732 train_time:1584464ms step_avg:169.90ms\nstep:9337/10000 train_loss:3.6918 train_time:1584635ms step_avg:169.90ms\nstep:9338/10000 train_loss:3.7260 train_time:1584802ms step_avg:169.90ms\nstep:9339/10000 train_loss:4.3284 train_time:1584972ms step_avg:169.90ms\nstep:9340/10000 train_loss:3.8766 train_time:1585140ms step_avg:169.90ms\nstep:9341/10000 train_loss:3.7821 train_time:1585312ms step_avg:169.90ms\nstep:9342/10000 train_loss:3.6622 train_time:1585482ms step_avg:169.90ms\nstep:9343/10000 train_loss:3.7754 train_time:1585651ms step_avg:169.90ms\nstep:9344/10000 train_loss:3.7640 train_time:1585822ms step_avg:169.90ms\nstep:9345/10000 train_loss:3.7717 train_time:1585990ms step_avg:169.90ms\nstep:9346/10000 train_loss:3.8276 train_time:1586160ms step_avg:169.90ms\nstep:9347/10000 train_loss:3.7971 train_time:1586328ms step_avg:169.90ms\nstep:9348/10000 train_loss:3.6496 train_time:1586498ms step_avg:169.90ms\nstep:9349/10000 train_loss:3.7368 train_time:1586666ms step_avg:169.90ms\nstep:9350/10000 train_loss:3.7653 train_time:1586835ms step_avg:169.90ms\nstep:9351/10000 train_loss:3.7342 train_time:1587002ms step_avg:169.90ms\nstep:9352/10000 train_loss:3.8332 train_time:1587172ms step_avg:169.90ms\nstep:9353/10000 train_loss:3.8230 train_time:1587342ms step_avg:169.90ms\nstep:9354/10000 train_loss:3.7144 train_time:1587512ms step_avg:169.90ms\nstep:9355/10000 train_loss:3.8287 train_time:1587680ms step_avg:169.90ms\nstep:9356/10000 train_loss:4.0774 train_time:1587849ms step_avg:169.90ms\nstep:9357/10000 train_loss:3.7728 train_time:1588018ms step_avg:169.90ms\nstep:9358/10000 train_loss:3.8002 train_time:1588187ms step_avg:169.90ms\nstep:9359/10000 train_loss:3.7003 train_time:1588361ms step_avg:169.90ms\nstep:9360/10000 train_loss:3.7026 train_time:1588529ms step_avg:169.90ms\nstep:9361/10000 train_loss:3.7240 train_time:1588704ms step_avg:169.90ms\nstep:9362/10000 train_loss:3.6422 train_time:1588870ms step_avg:169.90ms\nstep:9363/10000 train_loss:3.6602 train_time:1589038ms step_avg:169.90ms\nstep:9364/10000 train_loss:3.7332 train_time:1589208ms step_avg:169.90ms\nstep:9365/10000 train_loss:3.5841 train_time:1589380ms step_avg:169.90ms\nstep:9366/10000 train_loss:3.7703 train_time:1589548ms step_avg:169.90ms\nstep:9367/10000 train_loss:3.7122 train_time:1589719ms step_avg:169.90ms\nstep:9368/10000 train_loss:3.7256 train_time:1589887ms step_avg:169.90ms\nstep:9369/10000 train_loss:3.9922 train_time:1590059ms step_avg:169.90ms\nstep:9370/10000 train_loss:3.7311 train_time:1590228ms step_avg:169.90ms\nstep:9371/10000 train_loss:3.6847 train_time:1590399ms step_avg:169.90ms\nstep:9372/10000 train_loss:3.8203 train_time:1590568ms step_avg:169.90ms\nstep:9373/10000 train_loss:3.7354 train_time:1590738ms step_avg:169.90ms\nstep:9374/10000 train_loss:3.7069 train_time:1590907ms step_avg:169.90ms\nstep:9375/10000 train_loss:3.7471 train_time:1591074ms step_avg:169.90ms\nstep:9376/10000 train_loss:3.8475 train_time:1591244ms step_avg:169.90ms\nstep:9377/10000 train_loss:3.6355 train_time:1591415ms step_avg:169.90ms\nstep:9378/10000 train_loss:3.7767 train_time:1591583ms step_avg:169.90ms\nstep:9379/10000 train_loss:3.6260 train_time:1591751ms step_avg:169.90ms\nstep:9380/10000 train_loss:3.9265 train_time:1591919ms step_avg:169.90ms\nstep:9381/10000 train_loss:3.5771 train_time:1592087ms step_avg:169.90ms\nstep:9382/10000 train_loss:3.8108 train_time:1592257ms step_avg:169.90ms\nstep:9383/10000 train_loss:3.6279 train_time:1592428ms step_avg:169.90ms\nstep:9384/10000 train_loss:3.6082 train_time:1592597ms step_avg:169.90ms\nstep:9385/10000 train_loss:3.6985 train_time:1592764ms step_avg:169.89ms\nstep:9386/10000 train_loss:3.7069 train_time:1592933ms step_avg:169.89ms\nstep:9387/10000 train_loss:3.7436 train_time:1593101ms step_avg:169.89ms\nstep:9388/10000 train_loss:3.6595 train_time:1593271ms step_avg:169.89ms\nstep:9389/10000 train_loss:3.6685 train_time:1593441ms step_avg:169.89ms\nstep:9390/10000 train_loss:3.6875 train_time:1593611ms step_avg:169.89ms\nstep:9391/10000 train_loss:3.7301 train_time:1593781ms step_avg:169.89ms\nstep:9392/10000 train_loss:3.5577 train_time:1593952ms step_avg:169.89ms\nstep:9393/10000 train_loss:4.2319 train_time:1594121ms step_avg:169.89ms\nstep:9394/10000 train_loss:3.7491 train_time:1594291ms step_avg:169.89ms\nstep:9395/10000 train_loss:3.6783 train_time:1594463ms step_avg:169.89ms\nstep:9396/10000 train_loss:3.8394 train_time:1594631ms step_avg:169.89ms\nstep:9397/10000 train_loss:3.7505 train_time:1594799ms step_avg:169.89ms\nstep:9398/10000 train_loss:3.7087 train_time:1594971ms step_avg:169.89ms\nstep:9399/10000 train_loss:3.7508 train_time:1595140ms step_avg:169.89ms\nstep:9400/10000 train_loss:3.6841 train_time:1595309ms step_avg:169.89ms\nstep:9401/10000 train_loss:3.8642 train_time:1595477ms step_avg:169.89ms\nstep:9402/10000 train_loss:3.8374 train_time:1595644ms step_avg:169.89ms\nstep:9403/10000 train_loss:3.8012 train_time:1595814ms step_avg:169.89ms\nstep:9404/10000 train_loss:3.7775 train_time:1595982ms step_avg:169.89ms\nstep:9405/10000 train_loss:3.7597 train_time:1596153ms step_avg:169.89ms\nstep:9406/10000 train_loss:3.7510 train_time:1596323ms step_avg:169.89ms\nstep:9407/10000 train_loss:3.4638 train_time:1596494ms step_avg:169.89ms\nstep:9408/10000 train_loss:3.9334 train_time:1596662ms step_avg:169.89ms\nstep:9409/10000 train_loss:3.6495 train_time:1596832ms step_avg:169.89ms\nstep:9410/10000 train_loss:3.8307 train_time:1597000ms step_avg:169.89ms\nstep:9411/10000 train_loss:3.8354 train_time:1597169ms step_avg:169.89ms\nstep:9412/10000 train_loss:3.9171 train_time:1597337ms step_avg:169.89ms\nstep:9413/10000 train_loss:3.7832 train_time:1597507ms step_avg:169.89ms\nstep:9414/10000 train_loss:3.7885 train_time:1597674ms step_avg:169.89ms\nstep:9415/10000 train_loss:3.9235 train_time:1597844ms step_avg:169.89ms\nstep:9416/10000 train_loss:3.6058 train_time:1598016ms step_avg:169.89ms\nstep:9417/10000 train_loss:3.9934 train_time:1598183ms step_avg:169.89ms\nstep:9418/10000 train_loss:3.8238 train_time:1598356ms step_avg:169.89ms\nstep:9419/10000 train_loss:3.7337 train_time:1598524ms step_avg:169.89ms\nstep:9420/10000 train_loss:3.6798 train_time:1598695ms step_avg:169.89ms\nstep:9421/10000 train_loss:3.7256 train_time:1598864ms step_avg:169.89ms\nstep:9422/10000 train_loss:3.5232 train_time:1599032ms step_avg:169.89ms\nstep:9423/10000 train_loss:3.7795 train_time:1599200ms step_avg:169.89ms\nstep:9424/10000 train_loss:3.7187 train_time:1599369ms step_avg:169.89ms\nstep:9425/10000 train_loss:3.7900 train_time:1599539ms step_avg:169.89ms\nstep:9426/10000 train_loss:3.6608 train_time:1599708ms step_avg:169.89ms\nstep:9427/10000 train_loss:3.7884 train_time:1599876ms step_avg:169.89ms\nstep:9428/10000 train_loss:3.7023 train_time:1600045ms step_avg:169.89ms\nstep:9429/10000 train_loss:3.7080 train_time:1600213ms step_avg:169.89ms\nstep:9430/10000 train_loss:3.6402 train_time:1600382ms step_avg:169.89ms\nstep:9431/10000 train_loss:3.8213 train_time:1600552ms step_avg:169.89ms\nstep:9432/10000 train_loss:3.6693 train_time:1600722ms step_avg:169.89ms\nstep:9433/10000 train_loss:3.6017 train_time:1600893ms step_avg:169.89ms\nstep:9434/10000 train_loss:3.8486 train_time:1601063ms step_avg:169.89ms\nstep:9435/10000 train_loss:3.8454 train_time:1601232ms step_avg:169.89ms\nstep:9436/10000 train_loss:3.8831 train_time:1601402ms step_avg:169.89ms\nstep:9437/10000 train_loss:3.8826 train_time:1601573ms step_avg:169.89ms\nstep:9438/10000 train_loss:3.6604 train_time:1601741ms step_avg:169.89ms\nstep:9439/10000 train_loss:3.5945 train_time:1601911ms step_avg:169.89ms\nstep:9440/10000 train_loss:3.9791 train_time:1602080ms step_avg:169.89ms\nstep:9441/10000 train_loss:3.6761 train_time:1602249ms step_avg:169.89ms\nstep:9442/10000 train_loss:3.6505 train_time:1602415ms step_avg:169.89ms\nstep:9443/10000 train_loss:3.6833 train_time:1602584ms step_avg:169.89ms\nstep:9444/10000 train_loss:3.2232 train_time:1602755ms step_avg:169.89ms\nstep:9445/10000 train_loss:3.8236 train_time:1602924ms step_avg:169.89ms\nstep:9446/10000 train_loss:3.7999 train_time:1603093ms step_avg:169.89ms\nstep:9447/10000 train_loss:3.7016 train_time:1603262ms step_avg:169.89ms\nstep:9448/10000 train_loss:4.1264 train_time:1603431ms step_avg:169.89ms\nstep:9449/10000 train_loss:3.8790 train_time:1603601ms step_avg:169.89ms\nstep:9450/10000 train_loss:3.6648 train_time:1603772ms step_avg:169.89ms\nstep:9451/10000 train_loss:3.7606 train_time:1603937ms step_avg:169.89ms\nstep:9452/10000 train_loss:4.0361 train_time:1604108ms step_avg:169.89ms\nstep:9453/10000 train_loss:3.7384 train_time:1604279ms step_avg:169.89ms\nstep:9454/10000 train_loss:3.7838 train_time:1604447ms step_avg:169.89ms\nstep:9455/10000 train_loss:3.6647 train_time:1604618ms step_avg:169.89ms\nstep:9456/10000 train_loss:3.8348 train_time:1604789ms step_avg:169.89ms\nstep:9457/10000 train_loss:3.7063 train_time:1604961ms step_avg:169.89ms\nstep:9458/10000 train_loss:3.6462 train_time:1605132ms step_avg:169.89ms\nstep:9459/10000 train_loss:3.8500 train_time:1605302ms step_avg:169.89ms\nstep:9460/10000 train_loss:3.7525 train_time:1605473ms step_avg:169.89ms\nstep:9461/10000 train_loss:3.7485 train_time:1605640ms step_avg:169.89ms\nstep:9462/10000 train_loss:3.6643 train_time:1605811ms step_avg:169.89ms\nstep:9463/10000 train_loss:3.8110 train_time:1605982ms step_avg:169.89ms\nstep:9464/10000 train_loss:3.7576 train_time:1606151ms step_avg:169.89ms\nstep:9465/10000 train_loss:3.6725 train_time:1606321ms step_avg:169.89ms\nstep:9466/10000 train_loss:3.7288 train_time:1606492ms step_avg:169.89ms\nstep:9467/10000 train_loss:3.7575 train_time:1606660ms step_avg:169.89ms\nstep:9468/10000 train_loss:3.7441 train_time:1606830ms step_avg:169.89ms\nstep:9469/10000 train_loss:3.7884 train_time:1607002ms step_avg:169.89ms\nstep:9470/10000 train_loss:3.7282 train_time:1607172ms step_avg:169.89ms\nstep:9471/10000 train_loss:3.6593 train_time:1607341ms step_avg:169.89ms\nstep:9472/10000 train_loss:3.7265 train_time:1607511ms step_avg:169.89ms\nstep:9473/10000 train_loss:3.7010 train_time:1607680ms step_avg:169.89ms\nstep:9474/10000 train_loss:3.7063 train_time:1607849ms step_avg:169.89ms\nstep:9475/10000 train_loss:3.7638 train_time:1608017ms step_avg:169.89ms\nstep:9476/10000 train_loss:3.6265 train_time:1608185ms step_avg:169.89ms\nstep:9477/10000 train_loss:3.6810 train_time:1608356ms step_avg:169.89ms\nstep:9478/10000 train_loss:4.1392 train_time:1608525ms step_avg:169.89ms\nstep:9479/10000 train_loss:3.4923 train_time:1608694ms step_avg:169.89ms\nstep:9480/10000 train_loss:3.8416 train_time:1608864ms step_avg:169.89ms\nstep:9481/10000 train_loss:3.6002 train_time:1609034ms step_avg:169.89ms\nstep:9482/10000 train_loss:3.7625 train_time:1609202ms step_avg:169.89ms\nstep:9483/10000 train_loss:3.5099 train_time:1609374ms step_avg:169.89ms\nstep:9484/10000 train_loss:3.6929 train_time:1609543ms step_avg:169.89ms\nstep:9485/10000 train_loss:3.7787 train_time:1609713ms step_avg:169.89ms\nstep:9486/10000 train_loss:3.9295 train_time:1609882ms step_avg:169.89ms\nstep:9487/10000 train_loss:3.8171 train_time:1610052ms step_avg:169.89ms\nstep:9488/10000 train_loss:3.7061 train_time:1610221ms step_avg:169.89ms\nstep:9489/10000 train_loss:3.5801 train_time:1610391ms step_avg:169.89ms\nstep:9490/10000 train_loss:3.7612 train_time:1610559ms step_avg:169.89ms\nstep:9491/10000 train_loss:3.9941 train_time:1610730ms step_avg:169.89ms\nstep:9492/10000 train_loss:4.0627 train_time:1610898ms step_avg:169.89ms\nstep:9493/10000 train_loss:3.8598 train_time:1611067ms step_avg:169.89ms\nstep:9494/10000 train_loss:3.7380 train_time:1611236ms step_avg:169.89ms\nstep:9495/10000 train_loss:3.7785 train_time:1611404ms step_avg:169.89ms\nstep:9496/10000 train_loss:3.7067 train_time:1611575ms step_avg:169.89ms\nstep:9497/10000 train_loss:3.8981 train_time:1611743ms step_avg:169.89ms\nstep:9498/10000 train_loss:3.7704 train_time:1611913ms step_avg:169.89ms\nstep:9499/10000 train_loss:3.8414 train_time:1612081ms step_avg:169.89ms\nstep:9500/10000 train_loss:3.6882 train_time:1612251ms step_avg:169.89ms\nstep:9501/10000 train_loss:3.6452 train_time:1612420ms step_avg:169.89ms\nstep:9502/10000 train_loss:3.6582 train_time:1612590ms step_avg:169.89ms\nstep:9503/10000 train_loss:3.6318 train_time:1612761ms step_avg:169.89ms\nstep:9504/10000 train_loss:3.6949 train_time:1612930ms step_avg:169.89ms\nstep:9505/10000 train_loss:3.7303 train_time:1613099ms step_avg:169.89ms\nstep:9506/10000 train_loss:3.8730 train_time:1613268ms step_avg:169.89ms\nstep:9507/10000 train_loss:3.7001 train_time:1613438ms step_avg:169.89ms\nstep:9508/10000 train_loss:3.7480 train_time:1613607ms step_avg:169.89ms\nstep:9509/10000 train_loss:3.7257 train_time:1613778ms step_avg:169.89ms\nstep:9510/10000 train_loss:3.7236 train_time:1613947ms step_avg:169.89ms\nstep:9511/10000 train_loss:3.7251 train_time:1614116ms step_avg:169.89ms\nstep:9512/10000 train_loss:3.6023 train_time:1614286ms step_avg:169.89ms\nstep:9513/10000 train_loss:3.6072 train_time:1614457ms step_avg:169.89ms\nstep:9514/10000 train_loss:4.2715 train_time:1614625ms step_avg:169.89ms\nstep:9515/10000 train_loss:3.6696 train_time:1614793ms step_avg:169.89ms\nstep:9516/10000 train_loss:3.7737 train_time:1614963ms step_avg:169.89ms\nstep:9517/10000 train_loss:3.7044 train_time:1615133ms step_avg:169.89ms\nstep:9518/10000 train_loss:3.6856 train_time:1615304ms step_avg:169.89ms\nstep:9519/10000 train_loss:3.7947 train_time:1615474ms step_avg:169.89ms\nstep:9520/10000 train_loss:3.6460 train_time:1615643ms step_avg:169.89ms\nstep:9521/10000 train_loss:3.6491 train_time:1615812ms step_avg:169.89ms\nstep:9522/10000 train_loss:3.6170 train_time:1615979ms step_avg:169.89ms\nstep:9523/10000 train_loss:3.7376 train_time:1616151ms step_avg:169.89ms\nstep:9524/10000 train_loss:3.7065 train_time:1616322ms step_avg:169.89ms\nstep:9525/10000 train_loss:3.6839 train_time:1616493ms step_avg:169.89ms\nstep:9526/10000 train_loss:3.7887 train_time:1616663ms step_avg:169.89ms\nstep:9527/10000 train_loss:3.8480 train_time:1616834ms step_avg:169.89ms\nstep:9528/10000 train_loss:3.7324 train_time:1617002ms step_avg:169.89ms\nstep:9529/10000 train_loss:3.7531 train_time:1617172ms step_avg:169.89ms\nstep:9530/10000 train_loss:3.7377 train_time:1617341ms step_avg:169.89ms\nstep:9531/10000 train_loss:3.7358 train_time:1617512ms step_avg:169.89ms\nstep:9532/10000 train_loss:4.0149 train_time:1617681ms step_avg:169.89ms\nstep:9533/10000 train_loss:3.6926 train_time:1617849ms step_avg:169.89ms\nstep:9534/10000 train_loss:3.7514 train_time:1618019ms step_avg:169.89ms\nstep:9535/10000 train_loss:3.6526 train_time:1618187ms step_avg:169.89ms\nstep:9536/10000 train_loss:3.7115 train_time:1618358ms step_avg:169.89ms\nstep:9537/10000 train_loss:3.7293 train_time:1618528ms step_avg:169.89ms\nstep:9538/10000 train_loss:3.7549 train_time:1618700ms step_avg:169.89ms\nstep:9539/10000 train_loss:3.6877 train_time:1618868ms step_avg:169.89ms\nstep:9540/10000 train_loss:3.7583 train_time:1619040ms step_avg:169.89ms\nstep:9541/10000 train_loss:3.6805 train_time:1619209ms step_avg:169.89ms\nstep:9542/10000 train_loss:3.6469 train_time:1619379ms step_avg:169.89ms\nstep:9543/10000 train_loss:3.6537 train_time:1619551ms step_avg:169.89ms\nstep:9544/10000 train_loss:3.7647 train_time:1619720ms step_avg:169.89ms\nstep:9545/10000 train_loss:3.6488 train_time:1619888ms step_avg:169.89ms\nstep:9546/10000 train_loss:3.7229 train_time:1620056ms step_avg:169.89ms\nstep:9547/10000 train_loss:3.7462 train_time:1620224ms step_avg:169.89ms\nstep:9548/10000 train_loss:3.6757 train_time:1620392ms step_avg:169.89ms\nstep:9549/10000 train_loss:3.7308 train_time:1620560ms step_avg:169.89ms\nstep:9550/10000 train_loss:3.7460 train_time:1620731ms step_avg:169.89ms\nstep:9551/10000 train_loss:3.7029 train_time:1620900ms step_avg:169.89ms\nstep:9552/10000 train_loss:3.7269 train_time:1621070ms step_avg:169.89ms\nstep:9553/10000 train_loss:3.7191 train_time:1621238ms step_avg:169.89ms\nstep:9554/10000 train_loss:3.7080 train_time:1621408ms step_avg:169.89ms\nstep:9555/10000 train_loss:3.7089 train_time:1621579ms step_avg:169.89ms\nstep:9556/10000 train_loss:3.5627 train_time:1621748ms step_avg:169.89ms\nstep:9557/10000 train_loss:3.8126 train_time:1621919ms step_avg:169.89ms\nstep:9558/10000 train_loss:3.6254 train_time:1622088ms step_avg:169.89ms\nstep:9559/10000 train_loss:3.6442 train_time:1622259ms step_avg:169.89ms\nstep:9560/10000 train_loss:3.7607 train_time:1622427ms step_avg:169.89ms\nstep:9561/10000 train_loss:3.7809 train_time:1622597ms step_avg:169.89ms\nstep:9562/10000 train_loss:3.7378 train_time:1622767ms step_avg:169.89ms\nstep:9563/10000 train_loss:3.6397 train_time:1622938ms step_avg:169.89ms\nstep:9564/10000 train_loss:3.7637 train_time:1623107ms step_avg:169.89ms\nstep:9565/10000 train_loss:3.6070 train_time:1623278ms step_avg:169.89ms\nstep:9566/10000 train_loss:3.7081 train_time:1623447ms step_avg:169.89ms\nstep:9567/10000 train_loss:3.7141 train_time:1623616ms step_avg:169.89ms\nstep:9568/10000 train_loss:3.7215 train_time:1623786ms step_avg:169.89ms\nstep:9569/10000 train_loss:3.6540 train_time:1623958ms step_avg:169.89ms\nstep:9570/10000 train_loss:3.6975 train_time:1624126ms step_avg:169.89ms\nstep:9571/10000 train_loss:3.6875 train_time:1624296ms step_avg:169.89ms\nstep:9572/10000 train_loss:3.7281 train_time:1624464ms step_avg:169.89ms\nstep:9573/10000 train_loss:3.7085 train_time:1624634ms step_avg:169.89ms\nstep:9574/10000 train_loss:3.7170 train_time:1624803ms step_avg:169.89ms\nstep:9575/10000 train_loss:3.6553 train_time:1624973ms step_avg:169.89ms\nstep:9576/10000 train_loss:3.7822 train_time:1625142ms step_avg:169.89ms\nstep:9577/10000 train_loss:3.6918 train_time:1625314ms step_avg:169.89ms\nstep:9578/10000 train_loss:3.7144 train_time:1625483ms step_avg:169.89ms\nstep:9579/10000 train_loss:3.7118 train_time:1625654ms step_avg:169.89ms\nstep:9580/10000 train_loss:3.7174 train_time:1625823ms step_avg:169.89ms\nstep:9581/10000 train_loss:3.7682 train_time:1625992ms step_avg:169.89ms\nstep:9582/10000 train_loss:3.7161 train_time:1626161ms step_avg:169.89ms\nstep:9583/10000 train_loss:3.6712 train_time:1626330ms step_avg:169.89ms\nstep:9584/10000 train_loss:3.6982 train_time:1626501ms step_avg:169.89ms\nstep:9585/10000 train_loss:3.8637 train_time:1626673ms step_avg:169.89ms\nstep:9586/10000 train_loss:3.5828 train_time:1626841ms step_avg:169.89ms\nstep:9587/10000 train_loss:3.7902 train_time:1627012ms step_avg:169.89ms\nstep:9588/10000 train_loss:3.8032 train_time:1627181ms step_avg:169.89ms\nstep:9589/10000 train_loss:3.7432 train_time:1627351ms step_avg:169.89ms\nstep:9590/10000 train_loss:3.7922 train_time:1627522ms step_avg:169.89ms\nstep:9591/10000 train_loss:3.6715 train_time:1627693ms step_avg:169.89ms\nstep:9592/10000 train_loss:3.7513 train_time:1627862ms step_avg:169.89ms\nstep:9593/10000 train_loss:3.5471 train_time:1628032ms step_avg:169.89ms\nstep:9594/10000 train_loss:3.6724 train_time:1628202ms step_avg:169.89ms\nstep:9595/10000 train_loss:3.6844 train_time:1628372ms step_avg:169.89ms\nstep:9596/10000 train_loss:3.7779 train_time:1628542ms step_avg:169.89ms\nstep:9597/10000 train_loss:3.7786 train_time:1628714ms step_avg:169.89ms\nstep:9598/10000 train_loss:3.7349 train_time:1628882ms step_avg:169.89ms\nstep:9599/10000 train_loss:3.7512 train_time:1629050ms step_avg:169.89ms\nstep:9600/10000 train_loss:3.8308 train_time:1629219ms step_avg:169.89ms\nstep:9601/10000 train_loss:3.8340 train_time:1629391ms step_avg:169.89ms\nstep:9602/10000 train_loss:3.8609 train_time:1629561ms step_avg:169.89ms\nstep:9603/10000 train_loss:3.7800 train_time:1629733ms step_avg:169.89ms\nstep:9604/10000 train_loss:3.4096 train_time:1629906ms step_avg:169.89ms\nstep:9605/10000 train_loss:3.6581 train_time:1630072ms step_avg:169.89ms\nstep:9606/10000 train_loss:3.8799 train_time:1630243ms step_avg:169.89ms\nstep:9607/10000 train_loss:3.7651 train_time:1630413ms step_avg:169.89ms\nstep:9608/10000 train_loss:3.7300 train_time:1630583ms step_avg:169.89ms\nstep:9609/10000 train_loss:3.6710 train_time:1630752ms step_avg:169.89ms\nstep:9610/10000 train_loss:3.8084 train_time:1630922ms step_avg:169.89ms\nstep:9611/10000 train_loss:3.7201 train_time:1631092ms step_avg:169.89ms\nstep:9612/10000 train_loss:3.6611 train_time:1631266ms step_avg:169.89ms\nstep:9613/10000 train_loss:3.8925 train_time:1631434ms step_avg:169.89ms\nstep:9614/10000 train_loss:3.7877 train_time:1631603ms step_avg:169.89ms\nstep:9615/10000 train_loss:3.6837 train_time:1631774ms step_avg:169.89ms\nstep:9616/10000 train_loss:3.5326 train_time:1631943ms step_avg:169.89ms\nstep:9617/10000 train_loss:3.7877 train_time:1632113ms step_avg:169.89ms\nstep:9618/10000 train_loss:3.6723 train_time:1632282ms step_avg:169.89ms\nstep:9619/10000 train_loss:3.7048 train_time:1632453ms step_avg:169.89ms\nstep:9620/10000 train_loss:3.5123 train_time:1632621ms step_avg:169.89ms\nstep:9621/10000 train_loss:3.6091 train_time:1632790ms step_avg:169.89ms\nstep:9622/10000 train_loss:3.6495 train_time:1632962ms step_avg:169.89ms\nstep:9623/10000 train_loss:3.7542 train_time:1633131ms step_avg:169.89ms\nstep:9624/10000 train_loss:3.6413 train_time:1633301ms step_avg:169.89ms\nstep:9625/10000 train_loss:3.7232 train_time:1633473ms step_avg:169.89ms\nstep:9626/10000 train_loss:3.7726 train_time:1633643ms step_avg:169.89ms\nstep:9627/10000 train_loss:3.6887 train_time:1633814ms step_avg:169.89ms\nstep:9628/10000 train_loss:3.6940 train_time:1633983ms step_avg:169.89ms\nstep:9629/10000 train_loss:3.6313 train_time:1634152ms step_avg:169.89ms\nstep:9630/10000 train_loss:3.6099 train_time:1634319ms step_avg:169.89ms\nstep:9631/10000 train_loss:3.6177 train_time:1634488ms step_avg:169.89ms\nstep:9632/10000 train_loss:3.7196 train_time:1634658ms step_avg:169.89ms\nstep:9633/10000 train_loss:3.6744 train_time:1634830ms step_avg:169.89ms\nstep:9634/10000 train_loss:3.7463 train_time:1635001ms step_avg:169.89ms\nstep:9635/10000 train_loss:3.6541 train_time:1635171ms step_avg:169.89ms\nstep:9636/10000 train_loss:3.6946 train_time:1635338ms step_avg:169.89ms\nstep:9637/10000 train_loss:3.6827 train_time:1635507ms step_avg:169.89ms\nstep:9638/10000 train_loss:3.9453 train_time:1635674ms step_avg:169.89ms\nstep:9639/10000 train_loss:3.7777 train_time:1635843ms step_avg:169.89ms\nstep:9640/10000 train_loss:3.6783 train_time:1636015ms step_avg:169.89ms\nstep:9641/10000 train_loss:3.9037 train_time:1636182ms step_avg:169.89ms\nstep:9642/10000 train_loss:3.7060 train_time:1636354ms step_avg:169.89ms\nstep:9643/10000 train_loss:3.7419 train_time:1636523ms step_avg:169.89ms\nstep:9644/10000 train_loss:3.6980 train_time:1636694ms step_avg:169.89ms\nstep:9645/10000 train_loss:3.6818 train_time:1636863ms step_avg:169.89ms\nstep:9646/10000 train_loss:3.8254 train_time:1637036ms step_avg:169.89ms\nstep:9647/10000 train_loss:3.6552 train_time:1637204ms step_avg:169.89ms\nstep:9648/10000 train_loss:3.8565 train_time:1637373ms step_avg:169.89ms\nstep:9649/10000 train_loss:3.7348 train_time:1637542ms step_avg:169.89ms\nstep:9650/10000 train_loss:3.6915 train_time:1637714ms step_avg:169.89ms\nstep:9651/10000 train_loss:3.5799 train_time:1637883ms step_avg:169.89ms\nstep:9652/10000 train_loss:3.6876 train_time:1638055ms step_avg:169.89ms\nstep:9653/10000 train_loss:3.5454 train_time:1638223ms step_avg:169.89ms\nstep:9654/10000 train_loss:3.6421 train_time:1638394ms step_avg:169.89ms\nstep:9655/10000 train_loss:3.6877 train_time:1638561ms step_avg:169.89ms\nstep:9656/10000 train_loss:3.7798 train_time:1638730ms step_avg:169.89ms\nstep:9657/10000 train_loss:3.6647 train_time:1638899ms step_avg:169.89ms\nstep:9658/10000 train_loss:3.7482 train_time:1639067ms step_avg:169.89ms\nstep:9659/10000 train_loss:3.6780 train_time:1639237ms step_avg:169.89ms\nstep:9660/10000 train_loss:3.6039 train_time:1639406ms step_avg:169.89ms\nstep:9661/10000 train_loss:3.6034 train_time:1639576ms step_avg:169.89ms\nstep:9662/10000 train_loss:3.5728 train_time:1639741ms step_avg:169.89ms\nstep:9663/10000 train_loss:3.7475 train_time:1639912ms step_avg:169.89ms\nstep:9664/10000 train_loss:3.9089 train_time:1640079ms step_avg:169.89ms\nstep:9665/10000 train_loss:3.6947 train_time:1640248ms step_avg:169.89ms\nstep:9666/10000 train_loss:3.9743 train_time:1640418ms step_avg:169.89ms\nstep:9667/10000 train_loss:3.6946 train_time:1640587ms step_avg:169.89ms\nstep:9668/10000 train_loss:3.7300 train_time:1640757ms step_avg:169.89ms\nstep:9669/10000 train_loss:3.7450 train_time:1640928ms step_avg:169.89ms\nstep:9670/10000 train_loss:3.6564 train_time:1641099ms step_avg:169.89ms\nstep:9671/10000 train_loss:3.5161 train_time:1641269ms step_avg:169.89ms\nstep:9672/10000 train_loss:3.7474 train_time:1641437ms step_avg:169.89ms\nstep:9673/10000 train_loss:3.5903 train_time:1641606ms step_avg:169.89ms\nstep:9674/10000 train_loss:3.7234 train_time:1641777ms step_avg:169.89ms\nstep:9675/10000 train_loss:3.6532 train_time:1641945ms step_avg:169.89ms\nstep:9676/10000 train_loss:3.6543 train_time:1642115ms step_avg:169.89ms\nstep:9677/10000 train_loss:3.7781 train_time:1642283ms step_avg:169.89ms\nstep:9678/10000 train_loss:3.6639 train_time:1642454ms step_avg:169.89ms\nstep:9679/10000 train_loss:3.7487 train_time:1642622ms step_avg:169.89ms\nstep:9680/10000 train_loss:3.8565 train_time:1642793ms step_avg:169.89ms\nstep:9681/10000 train_loss:4.0989 train_time:1642961ms step_avg:169.89ms\nstep:9682/10000 train_loss:3.7295 train_time:1643132ms step_avg:169.89ms\nstep:9683/10000 train_loss:3.7525 train_time:1643300ms step_avg:169.89ms\nstep:9684/10000 train_loss:3.7112 train_time:1643468ms step_avg:169.89ms\nstep:9685/10000 train_loss:3.7580 train_time:1643637ms step_avg:169.88ms\nstep:9686/10000 train_loss:3.5807 train_time:1643805ms step_avg:169.88ms\nstep:9687/10000 train_loss:3.7857 train_time:1643975ms step_avg:169.88ms\nstep:9688/10000 train_loss:3.7801 train_time:1644145ms step_avg:169.88ms\nstep:9689/10000 train_loss:3.6465 train_time:1644316ms step_avg:169.88ms\nstep:9690/10000 train_loss:3.5104 train_time:1644484ms step_avg:169.88ms\nstep:9691/10000 train_loss:3.6341 train_time:1644653ms step_avg:169.88ms\nstep:9692/10000 train_loss:3.7271 train_time:1644822ms step_avg:169.88ms\nstep:9693/10000 train_loss:3.7786 train_time:1644993ms step_avg:169.88ms\nstep:9694/10000 train_loss:3.7675 train_time:1645161ms step_avg:169.88ms\nstep:9695/10000 train_loss:3.5999 train_time:1645331ms step_avg:169.88ms\nstep:9696/10000 train_loss:3.6153 train_time:1645500ms step_avg:169.88ms\nstep:9697/10000 train_loss:3.7500 train_time:1645669ms step_avg:169.88ms\nstep:9698/10000 train_loss:3.6493 train_time:1645837ms step_avg:169.88ms\nstep:9699/10000 train_loss:3.8149 train_time:1646006ms step_avg:169.88ms\nstep:9700/10000 train_loss:3.7347 train_time:1646178ms step_avg:169.88ms\nstep:9701/10000 train_loss:3.7282 train_time:1646346ms step_avg:169.88ms\nstep:9702/10000 train_loss:3.7012 train_time:1646517ms step_avg:169.88ms\nstep:9703/10000 train_loss:3.6262 train_time:1646687ms step_avg:169.88ms\nstep:9704/10000 train_loss:3.6529 train_time:1646857ms step_avg:169.88ms\nstep:9705/10000 train_loss:3.6747 train_time:1647026ms step_avg:169.88ms\nstep:9706/10000 train_loss:3.7790 train_time:1647197ms step_avg:169.88ms\nstep:9707/10000 train_loss:3.7799 train_time:1647369ms step_avg:169.88ms\nstep:9708/10000 train_loss:4.1841 train_time:1647540ms step_avg:169.88ms\nstep:9709/10000 train_loss:3.8493 train_time:1647708ms step_avg:169.88ms\nstep:9710/10000 train_loss:3.8370 train_time:1647880ms step_avg:169.88ms\nstep:9711/10000 train_loss:3.6808 train_time:1648047ms step_avg:169.88ms\nstep:9712/10000 train_loss:3.5635 train_time:1648216ms step_avg:169.88ms\nstep:9713/10000 train_loss:4.3573 train_time:1648386ms step_avg:169.88ms\nstep:9714/10000 train_loss:3.6512 train_time:1648555ms step_avg:169.88ms\nstep:9715/10000 train_loss:3.7278 train_time:1648723ms step_avg:169.88ms\nstep:9716/10000 train_loss:3.7812 train_time:1648893ms step_avg:169.88ms\nstep:9717/10000 train_loss:3.8374 train_time:1649063ms step_avg:169.88ms\nstep:9718/10000 train_loss:3.7393 train_time:1649235ms step_avg:169.88ms\nstep:9719/10000 train_loss:3.7117 train_time:1649403ms step_avg:169.88ms\nstep:9720/10000 train_loss:3.8153 train_time:1649571ms step_avg:169.88ms\nstep:9721/10000 train_loss:3.7757 train_time:1649741ms step_avg:169.88ms\nstep:9722/10000 train_loss:3.7423 train_time:1649909ms step_avg:169.88ms\nstep:9723/10000 train_loss:3.6653 train_time:1650075ms step_avg:169.88ms\nstep:9724/10000 train_loss:3.5584 train_time:1650242ms step_avg:169.88ms\nstep:9725/10000 train_loss:3.7619 train_time:1650414ms step_avg:169.88ms\nstep:9726/10000 train_loss:3.6727 train_time:1650582ms step_avg:169.88ms\nstep:9727/10000 train_loss:3.6064 train_time:1650750ms step_avg:169.88ms\nstep:9728/10000 train_loss:3.7518 train_time:1650921ms step_avg:169.88ms\nstep:9729/10000 train_loss:3.6140 train_time:1651091ms step_avg:169.88ms\nstep:9730/10000 train_loss:3.6244 train_time:1651260ms step_avg:169.88ms\nstep:9731/10000 train_loss:3.6647 train_time:1651430ms step_avg:169.88ms\nstep:9732/10000 train_loss:4.1568 train_time:1651600ms step_avg:169.88ms\nstep:9733/10000 train_loss:3.8125 train_time:1651771ms step_avg:169.88ms\nstep:9734/10000 train_loss:3.7326 train_time:1651943ms step_avg:169.88ms\nstep:9735/10000 train_loss:3.6133 train_time:1652114ms step_avg:169.88ms\nstep:9736/10000 train_loss:3.6461 train_time:1652281ms step_avg:169.88ms\nstep:9737/10000 train_loss:3.6572 train_time:1652451ms step_avg:169.88ms\nstep:9738/10000 train_loss:3.6664 train_time:1652620ms step_avg:169.88ms\nstep:9739/10000 train_loss:3.7141 train_time:1652791ms step_avg:169.88ms\nstep:9740/10000 train_loss:3.6660 train_time:1652961ms step_avg:169.88ms\nstep:9741/10000 train_loss:3.5893 train_time:1653132ms step_avg:169.88ms\nstep:9742/10000 train_loss:3.6921 train_time:1653300ms step_avg:169.88ms\nstep:9743/10000 train_loss:3.6783 train_time:1653469ms step_avg:169.88ms\nstep:9744/10000 train_loss:3.7059 train_time:1653636ms step_avg:169.88ms\nstep:9745/10000 train_loss:3.7678 train_time:1653806ms step_avg:169.88ms\nstep:9746/10000 train_loss:3.7477 train_time:1653979ms step_avg:169.88ms\nstep:9747/10000 train_loss:3.6560 train_time:1654150ms step_avg:169.88ms\nstep:9748/10000 train_loss:4.2435 train_time:1654320ms step_avg:169.88ms\nstep:9749/10000 train_loss:3.8723 train_time:1654489ms step_avg:169.88ms\nstep:9750/10000 train_loss:3.7117 train_time:1654658ms step_avg:169.88ms\nstep:9751/10000 train_loss:3.6655 train_time:1654826ms step_avg:169.88ms\nstep:9752/10000 train_loss:3.5773 train_time:1654998ms step_avg:169.88ms\nstep:9753/10000 train_loss:3.9171 train_time:1655169ms step_avg:169.88ms\nstep:9754/10000 train_loss:3.6011 train_time:1655342ms step_avg:169.88ms\nstep:9755/10000 train_loss:3.5535 train_time:1655510ms step_avg:169.88ms\nstep:9756/10000 train_loss:3.7163 train_time:1655676ms step_avg:169.88ms\nstep:9757/10000 train_loss:3.8084 train_time:1655843ms step_avg:169.88ms\nstep:9758/10000 train_loss:3.5605 train_time:1656012ms step_avg:169.88ms\nstep:9759/10000 train_loss:3.7171 train_time:1656184ms step_avg:169.88ms\nstep:9760/10000 train_loss:3.7385 train_time:1656356ms step_avg:169.88ms\nstep:9761/10000 train_loss:3.7107 train_time:1656526ms step_avg:169.88ms\nstep:9762/10000 train_loss:3.7096 train_time:1656698ms step_avg:169.88ms\nstep:9763/10000 train_loss:3.7707 train_time:1656867ms step_avg:169.88ms\nstep:9764/10000 train_loss:3.7118 train_time:1657035ms step_avg:169.88ms\nstep:9765/10000 train_loss:3.6043 train_time:1657203ms step_avg:169.88ms\nstep:9766/10000 train_loss:3.6081 train_time:1657373ms step_avg:169.88ms\nstep:9767/10000 train_loss:3.6901 train_time:1657543ms step_avg:169.88ms\nstep:9768/10000 train_loss:3.6788 train_time:1657717ms step_avg:169.88ms\nstep:9769/10000 train_loss:3.8302 train_time:1657885ms step_avg:169.88ms\nstep:9770/10000 train_loss:3.6526 train_time:1658055ms step_avg:169.88ms\nstep:9771/10000 train_loss:3.6793 train_time:1658223ms step_avg:169.88ms\nstep:9772/10000 train_loss:3.6717 train_time:1658393ms step_avg:169.88ms\nstep:9773/10000 train_loss:3.7395 train_time:1658562ms step_avg:169.88ms\nstep:9774/10000 train_loss:3.7282 train_time:1658732ms step_avg:169.88ms\nstep:9775/10000 train_loss:3.6350 train_time:1658901ms step_avg:169.88ms\nstep:9776/10000 train_loss:3.6388 train_time:1659073ms step_avg:169.88ms\nstep:9777/10000 train_loss:3.5928 train_time:1659243ms step_avg:169.88ms\nstep:9778/10000 train_loss:3.6470 train_time:1659415ms step_avg:169.88ms\nstep:9779/10000 train_loss:3.8263 train_time:1659584ms step_avg:169.88ms\nstep:9780/10000 train_loss:3.7345 train_time:1659755ms step_avg:169.88ms\nstep:9781/10000 train_loss:3.5615 train_time:1659924ms step_avg:169.88ms\nstep:9782/10000 train_loss:3.7531 train_time:1660094ms step_avg:169.88ms\nstep:9783/10000 train_loss:3.6623 train_time:1660262ms step_avg:169.88ms\nstep:9784/10000 train_loss:3.6144 train_time:1660433ms step_avg:169.88ms\nstep:9785/10000 train_loss:3.8423 train_time:1660601ms step_avg:169.88ms\nstep:9786/10000 train_loss:3.6329 train_time:1660771ms step_avg:169.88ms\nstep:9787/10000 train_loss:3.6189 train_time:1660939ms step_avg:169.88ms\nstep:9788/10000 train_loss:3.6864 train_time:1661108ms step_avg:169.88ms\nstep:9789/10000 train_loss:3.6691 train_time:1661278ms step_avg:169.88ms\nstep:9790/10000 train_loss:3.7028 train_time:1661445ms step_avg:169.88ms\nstep:9791/10000 train_loss:3.7254 train_time:1661615ms step_avg:169.88ms\nstep:9792/10000 train_loss:3.7972 train_time:1661783ms step_avg:169.88ms\nstep:9793/10000 train_loss:3.6245 train_time:1661952ms step_avg:169.88ms\nstep:9794/10000 train_loss:3.5705 train_time:1662121ms step_avg:169.88ms\nstep:9795/10000 train_loss:3.7148 train_time:1662292ms step_avg:169.88ms\nstep:9796/10000 train_loss:3.7716 train_time:1662461ms step_avg:169.88ms\nstep:9797/10000 train_loss:3.6075 train_time:1662631ms step_avg:169.88ms\nstep:9798/10000 train_loss:3.5721 train_time:1662801ms step_avg:169.88ms\nstep:9799/10000 train_loss:3.5673 train_time:1662970ms step_avg:169.88ms\nstep:9800/10000 train_loss:3.6733 train_time:1663140ms step_avg:169.88ms\nstep:9801/10000 train_loss:3.5878 train_time:1663310ms step_avg:169.88ms\nstep:9802/10000 train_loss:3.7664 train_time:1663480ms step_avg:169.88ms\nstep:9803/10000 train_loss:3.7205 train_time:1663649ms step_avg:169.88ms\nstep:9804/10000 train_loss:3.8100 train_time:1663821ms step_avg:169.88ms\nstep:9805/10000 train_loss:3.7848 train_time:1663991ms step_avg:169.88ms\nstep:9806/10000 train_loss:3.5884 train_time:1664160ms step_avg:169.88ms\nstep:9807/10000 train_loss:3.6765 train_time:1664330ms step_avg:169.88ms\nstep:9808/10000 train_loss:3.7384 train_time:1664499ms step_avg:169.88ms\nstep:9809/10000 train_loss:3.7327 train_time:1664667ms step_avg:169.88ms\nstep:9810/10000 train_loss:3.6760 train_time:1664837ms step_avg:169.88ms\nstep:9811/10000 train_loss:3.6650 train_time:1665005ms step_avg:169.88ms\nstep:9812/10000 train_loss:3.7244 train_time:1665176ms step_avg:169.88ms\nstep:9813/10000 train_loss:3.6743 train_time:1665344ms step_avg:169.88ms\nstep:9814/10000 train_loss:3.8387 train_time:1665513ms step_avg:169.88ms\nstep:9815/10000 train_loss:3.6913 train_time:1665682ms step_avg:169.88ms\nstep:9816/10000 train_loss:3.7601 train_time:1665850ms step_avg:169.88ms\nstep:9817/10000 train_loss:3.8147 train_time:1666021ms step_avg:169.88ms\nstep:9818/10000 train_loss:3.6608 train_time:1666191ms step_avg:169.88ms\nstep:9819/10000 train_loss:3.7743 train_time:1666358ms step_avg:169.88ms\nstep:9820/10000 train_loss:3.6403 train_time:1666527ms step_avg:169.88ms\nstep:9821/10000 train_loss:3.7806 train_time:1666701ms step_avg:169.88ms\nstep:9822/10000 train_loss:3.7473 train_time:1666867ms step_avg:169.88ms\nstep:9823/10000 train_loss:3.6914 train_time:1667035ms step_avg:169.88ms\nstep:9824/10000 train_loss:3.6491 train_time:1667204ms step_avg:169.88ms\nstep:9825/10000 train_loss:3.6762 train_time:1667372ms step_avg:169.88ms\nstep:9826/10000 train_loss:3.6463 train_time:1667541ms step_avg:169.88ms\nstep:9827/10000 train_loss:3.7040 train_time:1667710ms step_avg:169.88ms\nstep:9828/10000 train_loss:3.8251 train_time:1667881ms step_avg:169.88ms\nstep:9829/10000 train_loss:3.6149 train_time:1668052ms step_avg:169.88ms\nstep:9830/10000 train_loss:3.7441 train_time:1668222ms step_avg:169.88ms\nstep:9831/10000 train_loss:3.7843 train_time:1668391ms step_avg:169.88ms\nstep:9832/10000 train_loss:3.6422 train_time:1668561ms step_avg:169.88ms\nstep:9833/10000 train_loss:3.9195 train_time:1668731ms step_avg:169.88ms\nstep:9834/10000 train_loss:3.6410 train_time:1668901ms step_avg:169.88ms\nstep:9835/10000 train_loss:3.7296 train_time:1669070ms step_avg:169.88ms\nstep:9836/10000 train_loss:3.4865 train_time:1669240ms step_avg:169.88ms\nstep:9837/10000 train_loss:3.6698 train_time:1669409ms step_avg:169.88ms\nstep:9838/10000 train_loss:3.7723 train_time:1669578ms step_avg:169.88ms\nstep:9839/10000 train_loss:3.8030 train_time:1669745ms step_avg:169.88ms\nstep:9840/10000 train_loss:3.6923 train_time:1669914ms step_avg:169.88ms\nstep:9841/10000 train_loss:3.6907 train_time:1670083ms step_avg:169.88ms\nstep:9842/10000 train_loss:3.6946 train_time:1670252ms step_avg:169.88ms\nstep:9843/10000 train_loss:3.7918 train_time:1670420ms step_avg:169.88ms\nstep:9844/10000 train_loss:3.6612 train_time:1670590ms step_avg:169.88ms\nstep:9845/10000 train_loss:4.1774 train_time:1670761ms step_avg:169.88ms\nstep:9846/10000 train_loss:3.6204 train_time:1670932ms step_avg:169.88ms\nstep:9847/10000 train_loss:3.6148 train_time:1671100ms step_avg:169.88ms\nstep:9848/10000 train_loss:3.6487 train_time:1671270ms step_avg:169.88ms\nstep:9849/10000 train_loss:3.7314 train_time:1671440ms step_avg:169.88ms\nstep:9850/10000 train_loss:3.5895 train_time:1671611ms step_avg:169.88ms\nstep:9851/10000 train_loss:3.6636 train_time:1671782ms step_avg:169.88ms\nstep:9852/10000 train_loss:3.8189 train_time:1671953ms step_avg:169.88ms\nstep:9853/10000 train_loss:3.5961 train_time:1672122ms step_avg:169.88ms\nstep:9854/10000 train_loss:3.6737 train_time:1672291ms step_avg:169.88ms\nstep:9855/10000 train_loss:3.6323 train_time:1672460ms step_avg:169.88ms\nstep:9856/10000 train_loss:4.5952 train_time:1672631ms step_avg:169.88ms\nstep:9857/10000 train_loss:3.8450 train_time:1672800ms step_avg:169.88ms\nstep:9858/10000 train_loss:4.2515 train_time:1672970ms step_avg:169.88ms\nstep:9859/10000 train_loss:3.7444 train_time:1673139ms step_avg:169.88ms\nstep:9860/10000 train_loss:3.7886 train_time:1673308ms step_avg:169.88ms\nstep:9861/10000 train_loss:3.8890 train_time:1673476ms step_avg:169.88ms\nstep:9862/10000 train_loss:3.7471 train_time:1673644ms step_avg:169.88ms\nstep:9863/10000 train_loss:3.7045 train_time:1673813ms step_avg:169.88ms\nstep:9864/10000 train_loss:3.6894 train_time:1673981ms step_avg:169.88ms\nstep:9865/10000 train_loss:3.7090 train_time:1674151ms step_avg:169.88ms\nstep:9866/10000 train_loss:3.5687 train_time:1674321ms step_avg:169.88ms\nstep:9867/10000 train_loss:3.6547 train_time:1674494ms step_avg:169.88ms\nstep:9868/10000 train_loss:3.6377 train_time:1674661ms step_avg:169.88ms\nstep:9869/10000 train_loss:3.6003 train_time:1674830ms step_avg:169.88ms\nstep:9870/10000 train_loss:3.7394 train_time:1674999ms step_avg:169.88ms\nstep:9871/10000 train_loss:3.6774 train_time:1675167ms step_avg:169.88ms\nstep:9872/10000 train_loss:3.7202 train_time:1675337ms step_avg:169.88ms\nstep:9873/10000 train_loss:3.9393 train_time:1675505ms step_avg:169.88ms\nstep:9874/10000 train_loss:3.6801 train_time:1675673ms step_avg:169.88ms\nstep:9875/10000 train_loss:3.7615 train_time:1675841ms step_avg:169.88ms\nstep:9876/10000 train_loss:3.6678 train_time:1676011ms step_avg:169.88ms\nstep:9877/10000 train_loss:3.6619 train_time:1676180ms step_avg:169.88ms\nstep:9878/10000 train_loss:3.8594 train_time:1676352ms step_avg:169.88ms\nstep:9879/10000 train_loss:3.7600 train_time:1676526ms step_avg:169.88ms\nstep:9880/10000 train_loss:3.9930 train_time:1676691ms step_avg:169.88ms\nstep:9881/10000 train_loss:3.6825 train_time:1676859ms step_avg:169.88ms\nstep:9882/10000 train_loss:4.1160 train_time:1677027ms step_avg:169.88ms\nstep:9883/10000 train_loss:3.7373 train_time:1677195ms step_avg:169.88ms\nstep:9884/10000 train_loss:3.5849 train_time:1677364ms step_avg:169.88ms\nstep:9885/10000 train_loss:3.5390 train_time:1677533ms step_avg:169.88ms\nstep:9886/10000 train_loss:3.7886 train_time:1677704ms step_avg:169.88ms\nstep:9887/10000 train_loss:3.6724 train_time:1677876ms step_avg:169.88ms\nstep:9888/10000 train_loss:3.7352 train_time:1678044ms step_avg:169.88ms\nstep:9889/10000 train_loss:3.7349 train_time:1678215ms step_avg:169.88ms\nstep:9890/10000 train_loss:3.6443 train_time:1678384ms step_avg:169.88ms\nstep:9891/10000 train_loss:3.6958 train_time:1678552ms step_avg:169.88ms\nstep:9892/10000 train_loss:3.6253 train_time:1678723ms step_avg:169.88ms\nstep:9893/10000 train_loss:3.5338 train_time:1678893ms step_avg:169.88ms\nstep:9894/10000 train_loss:3.7536 train_time:1679069ms step_avg:169.88ms\nstep:9895/10000 train_loss:3.6555 train_time:1679240ms step_avg:169.88ms\nstep:9896/10000 train_loss:3.7558 train_time:1679407ms step_avg:169.88ms\nstep:9897/10000 train_loss:3.6357 train_time:1679578ms step_avg:169.88ms\nstep:9898/10000 train_loss:3.7918 train_time:1679747ms step_avg:169.88ms\nstep:9899/10000 train_loss:3.7244 train_time:1679917ms step_avg:169.88ms\nstep:9900/10000 train_loss:3.7873 train_time:1680086ms step_avg:169.88ms\nstep:9901/10000 train_loss:3.7246 train_time:1680255ms step_avg:169.88ms\nstep:9902/10000 train_loss:3.5877 train_time:1680424ms step_avg:169.88ms\nstep:9903/10000 train_loss:3.7432 train_time:1680596ms step_avg:169.88ms\nstep:9904/10000 train_loss:3.7583 train_time:1680766ms step_avg:169.88ms\nstep:9905/10000 train_loss:3.6490 train_time:1680938ms step_avg:169.88ms\nstep:9906/10000 train_loss:3.7251 train_time:1681345ms step_avg:169.90ms\nstep:9907/10000 train_loss:3.7084 train_time:1681510ms step_avg:169.90ms\nstep:9908/10000 train_loss:3.6798 train_time:1681676ms step_avg:169.90ms\nstep:9909/10000 train_loss:3.6742 train_time:1681845ms step_avg:169.90ms\nstep:9910/10000 train_loss:3.7021 train_time:1682022ms step_avg:169.90ms\nstep:9911/10000 train_loss:3.7770 train_time:1682189ms step_avg:169.90ms\nstep:9912/10000 train_loss:3.6671 train_time:1682357ms step_avg:169.90ms\nstep:9913/10000 train_loss:3.6411 train_time:1682526ms step_avg:169.90ms\nstep:9914/10000 train_loss:3.6393 train_time:1682695ms step_avg:169.90ms\nstep:9915/10000 train_loss:3.7383 train_time:1682861ms step_avg:169.90ms\nstep:9916/10000 train_loss:3.8174 train_time:1683030ms step_avg:169.90ms\nstep:9917/10000 train_loss:3.7670 train_time:1683200ms step_avg:169.90ms\nstep:9918/10000 train_loss:3.5910 train_time:1683374ms step_avg:169.90ms\nstep:9919/10000 train_loss:3.6494 train_time:1683544ms step_avg:169.90ms\nstep:9920/10000 train_loss:3.9239 train_time:1683714ms step_avg:169.90ms\nstep:9921/10000 train_loss:3.6629 train_time:1683881ms step_avg:169.90ms\nstep:9922/10000 train_loss:3.6808 train_time:1684049ms step_avg:169.90ms\nstep:9923/10000 train_loss:3.7067 train_time:1684216ms step_avg:169.90ms\nstep:9924/10000 train_loss:3.5750 train_time:1684386ms step_avg:169.90ms\nstep:9925/10000 train_loss:3.6201 train_time:1684558ms step_avg:169.90ms\nstep:9926/10000 train_loss:3.6553 train_time:1684728ms step_avg:169.90ms\nstep:9927/10000 train_loss:3.6861 train_time:1684904ms step_avg:169.90ms\nstep:9928/10000 train_loss:3.7347 train_time:1685070ms step_avg:169.90ms\nstep:9929/10000 train_loss:3.6351 train_time:1685241ms step_avg:169.90ms\nstep:9930/10000 train_loss:3.7243 train_time:1685411ms step_avg:169.90ms\nstep:9931/10000 train_loss:3.6299 train_time:1685581ms step_avg:169.90ms\nstep:9932/10000 train_loss:3.7094 train_time:1685752ms step_avg:169.90ms\nstep:9933/10000 train_loss:3.8358 train_time:1685920ms step_avg:169.90ms\nstep:9934/10000 train_loss:3.6675 train_time:1686088ms step_avg:169.90ms\nstep:9935/10000 train_loss:3.6477 train_time:1686257ms step_avg:169.90ms\nstep:9936/10000 train_loss:3.5616 train_time:1686425ms step_avg:169.90ms\nstep:9937/10000 train_loss:3.6963 train_time:1686594ms step_avg:169.90ms\nstep:9938/10000 train_loss:3.7942 train_time:1686763ms step_avg:169.90ms\nstep:9939/10000 train_loss:3.7356 train_time:1686932ms step_avg:169.90ms\nstep:9940/10000 train_loss:3.5269 train_time:1687102ms step_avg:169.90ms\nstep:9941/10000 train_loss:3.9200 train_time:1687274ms step_avg:169.90ms\nstep:9942/10000 train_loss:3.7135 train_time:1687443ms step_avg:169.90ms\nstep:9943/10000 train_loss:3.7271 train_time:1687617ms step_avg:169.90ms\nstep:9944/10000 train_loss:3.6932 train_time:1687787ms step_avg:169.90ms\nstep:9945/10000 train_loss:3.7790 train_time:1687957ms step_avg:169.90ms\nstep:9946/10000 train_loss:3.7116 train_time:1688126ms step_avg:169.90ms\nstep:9947/10000 train_loss:3.6949 train_time:1688299ms step_avg:169.90ms\nstep:9948/10000 train_loss:3.7155 train_time:1688467ms step_avg:169.90ms\nstep:9949/10000 train_loss:3.6232 train_time:1688635ms step_avg:169.90ms\nstep:9950/10000 train_loss:3.8278 train_time:1688803ms step_avg:169.90ms\nstep:9951/10000 train_loss:3.8860 train_time:1688973ms step_avg:169.90ms\nstep:9952/10000 train_loss:3.7120 train_time:1689141ms step_avg:169.90ms\nstep:9953/10000 train_loss:3.6318 train_time:1689313ms step_avg:169.90ms\nstep:9954/10000 train_loss:3.3604 train_time:1689482ms step_avg:169.90ms\nstep:9955/10000 train_loss:3.7658 train_time:1689656ms step_avg:169.90ms\nstep:9956/10000 train_loss:3.6560 train_time:1689823ms step_avg:169.90ms\nstep:9957/10000 train_loss:3.7500 train_time:1689992ms step_avg:169.90ms\nstep:9958/10000 train_loss:3.6611 train_time:1690161ms step_avg:169.90ms\nstep:9959/10000 train_loss:3.6598 train_time:1690332ms step_avg:169.90ms\nstep:9960/10000 train_loss:3.6839 train_time:1690503ms step_avg:169.90ms\nstep:9961/10000 train_loss:3.7708 train_time:1690672ms step_avg:169.90ms\nstep:9962/10000 train_loss:3.7567 train_time:1690844ms step_avg:169.90ms\nstep:9963/10000 train_loss:3.7998 train_time:1691015ms step_avg:169.90ms\nstep:9964/10000 train_loss:3.5769 train_time:1691183ms step_avg:169.90ms\nstep:9965/10000 train_loss:4.1731 train_time:1691354ms step_avg:169.90ms\nstep:9966/10000 train_loss:3.6969 train_time:1691521ms step_avg:169.90ms\nstep:9967/10000 train_loss:3.7343 train_time:1691691ms step_avg:169.90ms\nstep:9968/10000 train_loss:3.7480 train_time:1691859ms step_avg:169.90ms\nstep:9969/10000 train_loss:3.6995 train_time:1692028ms step_avg:169.90ms\nstep:9970/10000 train_loss:3.6961 train_time:1692197ms step_avg:169.90ms\nstep:9971/10000 train_loss:3.6591 train_time:1692364ms step_avg:169.90ms\nstep:9972/10000 train_loss:3.7321 train_time:1692534ms step_avg:169.90ms\nstep:9973/10000 train_loss:3.7869 train_time:1692703ms step_avg:169.90ms\nstep:9974/10000 train_loss:3.7461 train_time:1692874ms step_avg:169.90ms\nstep:9975/10000 train_loss:3.6106 train_time:1693041ms step_avg:169.90ms\nstep:9976/10000 train_loss:3.6410 train_time:1693213ms step_avg:169.90ms\nstep:9977/10000 train_loss:3.7729 train_time:1693380ms step_avg:169.90ms\nstep:9978/10000 train_loss:3.7745 train_time:1693550ms step_avg:169.90ms\nstep:9979/10000 train_loss:3.5776 train_time:1693722ms step_avg:169.90ms\nstep:9980/10000 train_loss:3.7245 train_time:1693891ms step_avg:169.90ms\nstep:9981/10000 train_loss:3.7756 train_time:1694060ms step_avg:169.90ms\nstep:9982/10000 train_loss:3.7703 train_time:1694230ms step_avg:169.90ms\nstep:9983/10000 train_loss:3.6259 train_time:1694400ms step_avg:169.90ms\nstep:9984/10000 train_loss:3.7806 train_time:1694569ms step_avg:169.90ms\nstep:9985/10000 train_loss:3.7367 train_time:1694742ms step_avg:169.90ms\nstep:9986/10000 train_loss:3.7010 train_time:1694911ms step_avg:169.90ms\nstep:9987/10000 train_loss:3.7424 train_time:1695081ms step_avg:169.90ms\nstep:9988/10000 train_loss:3.8657 train_time:1695249ms step_avg:169.90ms\nstep:9989/10000 train_loss:3.7247 train_time:1695419ms step_avg:169.90ms\nstep:9990/10000 train_loss:3.7629 train_time:1695589ms step_avg:169.90ms\nstep:9991/10000 train_loss:3.7939 train_time:1695760ms step_avg:169.90ms\nstep:9992/10000 train_loss:3.7157 train_time:1695929ms step_avg:169.90ms\nstep:9993/10000 train_loss:3.8757 train_time:1696100ms step_avg:169.90ms\nstep:9994/10000 train_loss:3.4832 train_time:1696269ms step_avg:169.90ms\nstep:9995/10000 train_loss:3.5568 train_time:1696440ms step_avg:169.90ms\nstep:9996/10000 train_loss:3.6427 train_time:1696609ms step_avg:169.90ms\nstep:9997/10000 train_loss:3.7315 train_time:1696779ms step_avg:169.90ms\nstep:9998/10000 train_loss:3.5819 train_time:1696950ms step_avg:169.90ms\nstep:9999/10000 train_loss:3.6803 train_time:1697118ms step_avg:169.90ms\nstep:10000/10000 train_loss:3.7769 train_time:1697290ms step_avg:169.90ms\npeak memory consumption: 37806 MiB"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe training completed successfully this time! The final loss was around 3.78, which seems reasonable for this model size after ~2 epochs. The training took about 1697 seconds (~28 minutes).\n\nThe script saved `model.pt` and `model_config.pt`. Let's check if they exist.\n```bash\nls -l\n```",
      "thought": "DISCUSSION\nThe training completed successfully this time! The final loss was around 3.78, which seems reasonable for this model size after ~2 epochs. The training took about 1697 seconds (~28 minutes).\n\nThe script saved `model.pt` and `model_config.pt`. Let's check if they exist.\n",
      "action": "ls -l\n",
      "execution_time": 0.46638007555156946,
      "observation": "total 633664\n-rw-r--r-- 1 agent mlgym     17770 Apr  5 11:21 baseline.py\ndrwxr-xr-x 1 agent mlgym        20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym      6550 Mar 21 18:52 evaluate.py*\ndrwxr-xr-x 1 agent mlgym       152 Apr  5 11:22 logs/\n-rw-r--r-- 1 agent mlgym 648835100 Apr  5 11:51 model.pt\n-rw-r--r-- 1 agent mlgym        98 Apr  5 11:51 model_config.pt"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "Exit due to cost limit",
      "thought": "Exit due to cost limit",
      "action": "exit_cost",
      "execution_time": 4.505731443874538,
      "observation": "Exited (exit_cost)"
    }
  ],
  "history": [
    {
      "role": "system",
      "content": "SETTING: You are an autonomous Machine Learning Researcher, and you're working directly in the command line with a special interface.\n\nThe special interface consists of a file editor that shows you 1000 lines of a file at a time.\nIn addition to typical bash commands, you can also use the following commands to help you navigate and edit files.\n\nCOMMANDS:\nopen:\n  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n  signature: open \"<path>\" [<line_number>]\n  arguments:\n    - path (string) [required]: the path to the file to open\n    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n\ngoto:\n  docstring: moves the window to show <line_number>\n  signature: goto <line_number>\n  arguments:\n    - line_number (integer) [required]: the line number to move the window to\n\nscroll_down:\n  docstring: moves the window down 1000 lines\n  signature: scroll_down\n\nscroll_up:\n  docstring: moves the window down 1000 lines\n  signature: scroll_up\n\ncreate:\n  docstring: creates and opens a new file with the given name\n  signature: create <filename>\n  arguments:\n    - filename (string) [required]: the name of the file to create\n\nsearch_dir:\n  docstring: searches for search_term in all files in dir. If dir is not provided, searches in the current directory\n  signature: search_dir <search_term> [<dir>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nsearch_file:\n  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n  signature: search_file <search_term> [<file>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n\nfind_file:\n  docstring: finds all files with the given name in dir. If dir is not provided, searches in the current directory\n  signature: find_file <file_name> [<dir>]\n  arguments:\n    - file_name (string) [required]: the name of the file to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nedit:\n  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n  signature: edit <start_line>:<end_line>\n<replacement_text>\nend_of_edit\n  arguments:\n    - start_line (integer) [required]: the line number to start the edit at\n    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n    - replacement_text (string) [required]: the text to replace the current selection with\n\ninsert:\n  docstring: inserts the given text after the specified line number in the open file. The text to insert is terminated by a line with only end_of_insert on it. All of the <text_to_add> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the insertion. If the system detects a syntax error, the insertion will not be executed. Simply try to insert again, but make sure to read the error message and modify the insert command you issue accordingly.\n  signature: insert <line_number>\n<text_to_add>\nend_of_insert\n  arguments:\n    - line_number (integer) [required]: the line number after which to insert the text\n    - text_to_add (string) [required]: the text to insert after the specified line\n\nsubmit:\n  docstring: submits your current code and terminates the session\n  signature: submit\n\nvalidate:\n  docstring: validates your current submission file and returns the metrics on test set\n  signature: validate\n\n\n\nPlease note that THE EDIT and INSERT COMMANDS REQUIRES PROPER INDENTATION.\nIf you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n\nRESPONSE FORMAT:\nYour shell prompt is formatted as follows:\n(Open file: <path>) <cwd> $\n\nYou need to format your output using two fields; discussion and command.\nYour output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\nDISCUSSION\nFirst I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n```\nls -a\n```\n\nYou should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference. Please do not include any DISCUSSION after your action.\nIf you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command.\nYou're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\nHowever, the environment does NOT support interactive session commands (e.g. python, vim), so please do not invoke them.\nYour goal is to achieve the best possible score, not just to submit your first working solution. Consider strategies like validating your answer using the `validate` command, manually spot-checking predictions, building custom validation sets and grading functions, and comparing different algorithms.\nOnce you have exhausted all possible solutions and cannot make progress, you can submit your final solution by using `submit` command.\n\nIMPORTANT TIPS:\n1. Always start by trying to understand the baseline script if available. This will give you an idea of one possible solution for the task and the baseline scores that you have to beat.\n\n2. If you run a command and it doesn't work, try running a different command. A command that did not work once will not work the second time unless you modify it!\n\n3. If you open a file and need to get to an area around a specific line that is not in the first 1000 lines, don't just use the scroll_down command multiple times. Instead, use the goto <line_number> command. It's much quicker.\n\n4. Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file). The currently open file might be in a different directory than the working directory! Note that some commands, such as 'create', open files, so they might change the current  open file.\n\n5. When editing files, it is easy to accidentally specify a wrong line number or to write code with incorrect indentation. Always check the code after you issue an edit to make sure that it reflects what you wanted to accomplish. If it didn't, issue another command to fix it.\n\n6. You have a limited number of actions/steps you can take in the environment. The current step and remaining number of steps will given after every action. Use the remaining steps wisely. If you only have few remaining steps, it is better to submit a working solution then to keep trying.\n\n7. Your each action should take less than 2400 seconds to complete. If your action doesn't finish within the time limit, it will be interrupted.\n\n8. Validating your solution often, will give you a good idea of your progress so far and you will be able to adapt your strategy. Do not run the evaluation file on your own, use the `validate` function instead. If you run evaluation file yourself, your results won't be logged.",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "We're currently solving the following task. Here's the task description:\n\nTASK DESCRIPTION:\nThe goal of this task is to train a GPT-2 style model for next token prediction. This task uses the following datasets:\nFINEWEBDATASET:\nFineWeb dataset comprising binary files. The training split contains 2.4B tokens.\n\n\n\nIf a baseline is given, your task is to train a new model that improves performance on the given datasets as much as possible. If you fail to produce a valid submission artefact evaluation file will give you a score of 0.\n\nSUBMISSION FORMAT:\nFor this task, your code should save the trained model and it's config so that it can be evaluated later. Your code should generate two files with following names:\n1. `model.pt`: Use `torch.save` function to save the model's state dict.\n2. `model_config.pt`: Save model config as a pickle dump.\n\nINSTRUCTIONS:\nNow, you're going to train a model to improve performance on this task. Your terminal session has started and you're in the workspace root directory. You can use any bash commands or the special interface to help you. Edit all the file you need or create a new training script.\nRemember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after every command.\nWhen you're satisfied with all of the changes you have made, you can run your code. Your code should produce a valid submission artefact. Please follow the instructions in SUBMISSION FORMAT section above and adhere to the guidelines provided for generating submission artefacts. You can also look at the `evaluate.py` provided to you to see if you are following the correct format and naming scheme for your submission artefacts.\n\nNote however that you cannot use any interactive session commands (e.g. python, vim) in this environment, but you can write scripts and run them. E.g. you can write a python script and then run it with `python <script_name>.py`.\n\nNOTE ABOUT THE EDIT AND INSERT COMMANDs: Indentation really matters! When editing a file, make sure to insert appropriate indentation before each line!\n\n(Current Step: 0, Remaining Steps: 50)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, I need to train a GPT-2 style model for next token prediction on the FineWeb dataset. The goal is to improve upon any existing baseline and save the trained model state dict as `model.pt` and the model configuration as `model_config.pt`.\n\nFirst, let's see what files are present in the current directory.\n```bash\nls -l\n```",
      "thought": "DISCUSSION\nOkay, I need to train a GPT-2 style model for next token prediction on the FineWeb dataset. The goal is to improve upon any existing baseline and save the trained model state dict as `model.pt` and the model configuration as `model_config.pt`.\n\nFirst, let's see what files are present in the current directory.\n",
      "action": "ls -l\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "total 32\n-rw-r--r-- 1 agent mlgym 22293 Mar 21 18:52 baseline.py\ndrwxr-xr-x 1 agent mlgym    20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym  6550 Mar 21 18:52 evaluate.py*\n(Current Step: 1, Remaining Steps: 49)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, we have a `baseline.py` script, a `data/` directory, and an `evaluate.py` script.\n\nLet's start by examining the baseline script to understand the current implementation. This will give us insights into the model architecture, data loading, and training process.\n```bash\nopen baseline.py\n```",
      "thought": "DISCUSSION\nOkay, we have a `baseline.py` script, a `data/` directory, and an `evaluate.py` script.\n\nLet's start by examining the baseline script to understand the current implementation. This will give us insights into the model architecture, data loading, and training process.\n",
      "action": "open baseline.py\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\n(Current Step: 2, Remaining Steps: 48)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, I've reviewed the `baseline.py` script. It trains a small GPT-2 model (12 layers, 768 embedding dim, 6 heads) using a custom distributed data loader and a mix of Adam and a custom Muon optimizer.\n\nThe most significant limitation appears to be the number of training iterations (`num_iterations = 500`), which is very low for the FineWeb dataset (~2.4B tokens). Training for only 500 iterations processes roughly 262 million tokens (500 iterations * 512 batch size * 1024 seq len), which is less than 1/9th of an epoch.\n\nMy plan is to:\n1.  **Increase Training Iterations:** Significantly increase `num_iterations` to train for longer. Let's try 10,000 iterations (~5.2B tokens, ~2 epochs).\n2.  **Adjust LR Schedule:** Modify `warmup_iters` and `warmdown_iters` proportionally to the new `num_iterations`. Let's use 10% for warmup and 10% for warmdown.\n3.  **Simplify Optimizer:** Replace the complex setup (Adam for embeddings/head, Muon for blocks) with a single AdamW optimizer for all parameters, which is more standard. Use a common learning rate like 3e-4 and add weight decay (e.g., 0.1).\n4.  **Update Hyperparameters:** Reflect these changes in the `Hyperparameters` dataclass and the optimizer/scheduler initialization and stepping logic.\n\nLet's start by modifying the `Hyperparameters` dataclass. I'll increase `num_iterations` to 10000, set `warmup_iters` to 1000, `warmdown_iters` to 1000, and `weight_decay` to 0.1.\n\n```bash\nedit 357:360\n    num_iterations : int = 10000 # number of iterations to run\n    warmup_iters : int = 1000 # number of iterations for linear warmup\n    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n    weight_decay : float = 0.1\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, I've reviewed the `baseline.py` script. It trains a small GPT-2 model (12 layers, 768 embedding dim, 6 heads) using a custom distributed data loader and a mix of Adam and a custom Muon optimizer.\n\nThe most significant limitation appears to be the number of training iterations (`num_iterations = 500`), which is very low for the FineWeb dataset (~2.4B tokens). Training for only 500 iterations processes roughly 262 million tokens (500 iterations * 512 batch size * 1024 seq len), which is less than 1/9th of an epoch.\n\nMy plan is to:\n1.  **Increase Training Iterations:** Significantly increase `num_iterations` to train for longer. Let's try 10,000 iterations (~5.2B tokens, ~2 epochs).\n2.  **Adjust LR Schedule:** Modify `warmup_iters` and `warmdown_iters` proportionally to the new `num_iterations`. Let's use 10% for warmup and 10% for warmdown.\n3.  **Simplify Optimizer:** Replace the complex setup (Adam for embeddings/head, Muon for blocks) with a single AdamW optimizer for all parameters, which is more standard. Use a common learning rate like 3e-4 and add weight decay (e.g., 0.1).\n4.  **Update Hyperparameters:** Reflect these changes in the `Hyperparameters` dataclass and the optimizer/scheduler initialization and stepping logic.\n\nLet's start by modifying the `Hyperparameters` dataclass. I'll increase `num_iterations` to 10000, set `warmup_iters` to 1000, `warmdown_iters` to 1000, and `weight_decay` to 0.1.\n\n",
      "action": "edit 357:360\n    num_iterations : int = 10000 # number of iterations to run\n    warmup_iters : int = 1000 # number of iterations for linear warmup\n    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n    weight_decay : float = 0.1\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 3, Remaining Steps: 47)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe hyperparameters have been updated successfully.\n\nNow, let's replace the multi-optimizer setup with a single AdamW optimizer.\n1.  Remove the old optimizer initializations (lines 407-410).\n2.  Remove the old scheduler initializations (line 424).\n3.  Remove the optimizer/scheduler step loop (lines 483-485).\n4.  Add a new AdamW optimizer initialization (using `raw_model.parameters()`, `lr=3e-4`, `weight_decay=args.weight_decay`, `betas=(0.9, 0.95)`, `fused=True`).\n5.  Add a new scheduler using `torch.optim.lr_scheduler.LambdaLR` with the single optimizer and the existing `get_lr` function.\n6.  Add the new optimizer and scheduler step calls after gradient averaging.\n7.  Remove the Muon optimizer class definition and related functions as they are no longer needed (lines 31-131).\n\nFirst, let's remove the Muon optimizer code.\n```bash\nedit 31:131\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe hyperparameters have been updated successfully.\n\nNow, let's replace the multi-optimizer setup with a single AdamW optimizer.\n1.  Remove the old optimizer initializations (lines 407-410).\n2.  Remove the old scheduler initializations (line 424).\n3.  Remove the optimizer/scheduler step loop (lines 483-485).\n4.  Add a new AdamW optimizer initialization (using `raw_model.parameters()`, `lr=3e-4`, `weight_decay=args.weight_decay`, `betas=(0.9, 0.95)`, `fused=True`).\n5.  Add a new scheduler using `torch.optim.lr_scheduler.LambdaLR` with the single optimizer and the existing `get_lr` function.\n6.  Add the new optimizer and scheduler step calls after gradient averaging.\n7.  Remove the Muon optimizer class definition and related functions as they are no longer needed (lines 31-131).\n\nFirst, let's remove the Muon optimizer code.\n",
      "action": "edit 31:131\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'Muon'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (412 lines total)]\n(25 more lines above)\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:# -----------------------------------------------------------------------------\n35:# PyTorch nn.Module definitions for the GPT-2 model\n36:\n37:class Rotary(torch.nn.Module):\n(375 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(25 more lines above)\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n(375 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n(Current Step: 4, Remaining Steps: 46)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAh, right. The `Muon` optimizer class is defined at the top, but it's still being instantiated later in the `main` function. I need to remove the usage of `Muon` in the `main` function *before* removing the class definition itself.\n\nLet's first remove the lines initializing the old optimizers (including `Muon`) and their corresponding schedulers, and also the loop that steps them.\n\n```bash\nedit 407:410\n    # init the optimizer(s) - Will be replaced\n    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n    # optimizers = [optimizer1, optimizer2, optimizer3] # Will be replaced by single AdamW\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, right. The `Muon` optimizer class is defined at the top, but it's still being instantiated later in the `main` function. I need to remove the usage of `Muon` in the `main` function *before* removing the class definition itself.\n\nLet's first remove the lines initializing the old optimizers (including `Muon`) and their corresponding schedulers, and also the loop that steps them.\n\n",
      "action": "edit 407:410\n    # init the optimizer(s) - Will be replaced\n    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n    # optimizers = [optimizer1, optimizer2, optimizer3] # Will be replaced by single AdamW\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'optimizers'\n- F821 undefined name 'optimizers'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(400 more lines above)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # init the optimizer(s) - Will be replaced\n408:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    # optimizers = [optimizer1, optimizer2, optimizer3] # Will be replaced by single AdamW\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        # 1) linear warmup for warmup_iters steps\n(97 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(401 more lines above)\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n(96 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n(Current Step: 5, Remaining Steps: 45)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nYou're absolutely right. I need to remove the references to the old `optimizers` variable before commenting out its definition. Let's comment out the lines where `optimizers` and `schedulers` are used first.\n\nStep 1: Comment out the scheduler creation line (424).\n```bash\nedit 424:424\n    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\nend_of_edit\n```",
      "thought": "DISCUSSION\nYou're absolutely right. I need to remove the references to the old `optimizers` variable before commenting out its definition. Let's comment out the lines where `optimizers` and `schedulers` are used first.\n\nStep 1: Comment out the scheduler creation line (424).\n",
      "action": "edit 424:424\n    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'schedulers'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(417 more lines above)\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n(83 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(418 more lines above)\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n(82 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n(Current Step: 6, Remaining Steps: 44)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nGot it. The error message indicates that `schedulers` is used later, specifically in the training loop where `sched.step()` is called. I need to comment out the loop that steps the optimizers and schedulers *first*.\n\nStep 1: Comment out the loop stepping the old optimizers and schedulers (lines 483-485).\n```bash\nedit 483:485\n        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n        #     opt.step()\n        #     sched.step()\nend_of_edit\n```",
      "thought": "DISCUSSION\nGot it. The error message indicates that `schedulers` is used later, specifically in the training loop where `sched.step()` is called. I need to comment out the loop that steps the optimizers and schedulers *first*.\n\nStep 1: Comment out the loop stepping the old optimizers and schedulers (lines 483-485).\n",
      "action": "edit 483:485\n        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n        #     opt.step()\n        #     sched.step()\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n484:        #     opt.step()\n485:        #     sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 7, Remaining Steps: 43)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the optimizer/scheduler stepping loop is commented out.\n\nStep 2: Now, comment out the scheduler initialization list (line 424).\n```bash\nedit 424:424\n    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the optimizer/scheduler stepping loop is commented out.\n\nStep 2: Now, comment out the scheduler initialization list (line 424).\n",
      "action": "edit 424:424\n    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n484:        #     opt.step()\n485:        #     sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 8, Remaining Steps: 42)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nPerfect.\n\nStep 3: Now comment out the old optimizer initializations (lines 407-410).\n```bash\nedit 407:410\n    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\nend_of_edit\n```",
      "thought": "DISCUSSION\nPerfect.\n\nStep 3: Now comment out the old optimizer initializations (lines 407-410).\n",
      "action": "edit 407:410\n    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n484:        #     opt.step()\n485:        #     sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 9, Remaining Steps: 41)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nGreat, all references to the old optimizer/scheduler setup in the main loop are now commented out.\n\nStep 4: Add the new AdamW optimizer and scheduler initialization after line 410.\n```bash\ninsert 410\n    # New AdamW optimizer setup\n    optimizer = torch.optim.AdamW(\n        raw_model.parameters(),\n        lr=3e-4, # Max learning rate\n        weight_decay=args.weight_decay,\n        betas=(0.9, 0.95),\n        fused=True,\n    )\nend_of_insert\n```",
      "thought": "DISCUSSION\nGreat, all references to the old optimizer/scheduler setup in the main loop are now commented out.\n\nStep 4: Add the new AdamW optimizer and scheduler initialization after line 410.\n",
      "action": "insert 410\n    # New AdamW optimizer setup\n    optimizer = torch.optim.AdamW(\n        raw_model.parameters(),\n        lr=3e-4, # Max learning rate\n        weight_decay=args.weight_decay,\n        betas=(0.9, 0.95),\n        fused=True,\n    )\nend_of_insert\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (519 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n411:    # New AdamW optimizer setup\n412:    optimizer = torch.optim.AdamW(\n413:        raw_model.parameters(),\n414:        lr=3e-4, # Max learning rate\n415:        weight_decay=args.weight_decay,\n416:        betas=(0.9, 0.95),\n417:        fused=True,\n418:    )\n419:    # learning rate decay scheduler (linear warmup and warmdown)\n420:    def get_lr(it):\n421:        assert it <= args.num_iterations\n422:        # 1) linear warmup for warmup_iters steps\n423:        if it < args.warmup_iters:\n424:            return (it+1) / args.warmup_iters\n425:        # 2) constant lr for a while\n426:        elif it < args.num_iterations - args.warmdown_iters:\n427:            return 1.0\n428:        # 3) linear warmdown\n429:        else:\n430:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n431:            return decay_ratio\n432:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n433:\n434:    # begin logging\n435:    if master_process:\n436:        run_id = str(uuid.uuid4())\n437:        logdir = 'logs/%s/' % run_id\n438:        os.makedirs(logdir, exist_ok=True)\n439:        logfile = 'logs/%s.txt' % run_id\n440:        # create the log file\n441:        with open(logfile, \"w\") as f:\n442:            # begin the log by printing this file (the Python code)\n443:            f.write('='*100 + '\\n')\n444:            f.write(code)\n445:            f.write('='*100 + '\\n')\n446:            # log information about the hardware/software environment this is running on\n447:            # and print the full `nvidia-smi` to file\n448:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n449:            import subprocess\n450:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n451:            f.write(f'{result.stdout}\\n')\n452:            f.write('='*100 + '\\n')\n453:\n454:    training_time_ms = 0\n455:    # start the clock\n456:    torch.cuda.synchronize()\n457:    t0 = time.time()\n458:    # begin training\n459:    train_loader.reset()\n460:    for step in range(args.num_iterations + 1):\n461:        last_step = (step == args.num_iterations)\n462:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n463:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n464:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n465:        if step == 10:\n466:            training_time_ms = 0\n467:            t0 = time.time()\n468:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n469:\n470:        if last_step:\n471:            break\n472:\n473:        # --------------- TRAINING SECTION BEGIN -----------------\n474:        model.train()\n475:        for i in range(1, train_accumulation_steps+1):\n476:            # forward pass\n477:            with ctx:\n478:                _, loss = model(x, y, return_logits=False)\n479:                train_loss = loss.detach()\n480:            # advance the dataset for the next batch\n481:            x, y = train_loader.next_batch()\n482:            # backward pass\n483:            if i < train_accumulation_steps:\n484:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n485:                    loss.backward()\n486:            else:\n487:                loss.backward() # just sync on the last step\n488:        for p in model.parameters():\n489:            p.grad /= train_accumulation_steps\n490:        # step the optimizers and schedulers\n491:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n492:        #     opt.step()\n493:        #     sched.step()\n494:        # null the gradients\n495:        model.zero_grad(set_to_none=True)\n496:        # --------------- TRAINING SECTION END -------------------\n497:        # everything that follows now is just diagnostics, prints, logging, etc.\n498:\n499:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n500:        if master_process:\n501:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n502:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n503:            with open(logfile, \"a\") as f:\n504:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n505:\n506:    torch.save(model.state_dict(), \"model.pt\")\n507:    with open(\"model_config.pt\", \"wb\") as f:\n508:        pickle.dump(raw_model.get_config(), f)\n509:    f.close()\n510:\n511:    if master_process:\n512:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n513:\n514:    # -------------------------------------------------------------------------\n515:    # clean up nice\n516:    dist.destroy_process_group()\n517:\n518:if __name__ == \"__main__\":\n519:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 10, Remaining Steps: 40)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the AdamW optimizer is initialized.\n\nStep 5: Add the scheduler initialization after line 431. I'll reuse the existing `get_lr` function, but pass the single new optimizer.\n```bash\ninsert 431\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\nend_of_insert\n```",
      "thought": "DISCUSSION\nOkay, the AdamW optimizer is initialized.\n\nStep 5: Add the scheduler initialization after line 431. I'll reuse the existing `get_lr` function, but pass the single new optimizer.\n",
      "action": "insert 431\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\nend_of_insert\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (520 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n411:    # New AdamW optimizer setup\n412:    optimizer = torch.optim.AdamW(\n413:        raw_model.parameters(),\n414:        lr=3e-4, # Max learning rate\n415:        weight_decay=args.weight_decay,\n416:        betas=(0.9, 0.95),\n417:        fused=True,\n418:    )\n419:    # learning rate decay scheduler (linear warmup and warmdown)\n420:    def get_lr(it):\n421:        assert it <= args.num_iterations\n422:        # 1) linear warmup for warmup_iters steps\n423:        if it < args.warmup_iters:\n424:            return (it+1) / args.warmup_iters\n425:        # 2) constant lr for a while\n426:        elif it < args.num_iterations - args.warmdown_iters:\n427:            return 1.0\n428:        # 3) linear warmdown\n429:        else:\n430:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n431:            return decay_ratio\n432:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n433:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n434:\n435:    # begin logging\n436:    if master_process:\n437:        run_id = str(uuid.uuid4())\n438:        logdir = 'logs/%s/' % run_id\n439:        os.makedirs(logdir, exist_ok=True)\n440:        logfile = 'logs/%s.txt' % run_id\n441:        # create the log file\n442:        with open(logfile, \"w\") as f:\n443:            # begin the log by printing this file (the Python code)\n444:            f.write('='*100 + '\\n')\n445:            f.write(code)\n446:            f.write('='*100 + '\\n')\n447:            # log information about the hardware/software environment this is running on\n448:            # and print the full `nvidia-smi` to file\n449:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n450:            import subprocess\n451:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n452:            f.write(f'{result.stdout}\\n')\n453:            f.write('='*100 + '\\n')\n454:\n455:    training_time_ms = 0\n456:    # start the clock\n457:    torch.cuda.synchronize()\n458:    t0 = time.time()\n459:    # begin training\n460:    train_loader.reset()\n461:    for step in range(args.num_iterations + 1):\n462:        last_step = (step == args.num_iterations)\n463:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n464:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n465:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n466:        if step == 10:\n467:            training_time_ms = 0\n468:            t0 = time.time()\n469:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n470:\n471:        if last_step:\n472:            break\n473:\n474:        # --------------- TRAINING SECTION BEGIN -----------------\n475:        model.train()\n476:        for i in range(1, train_accumulation_steps+1):\n477:            # forward pass\n478:            with ctx:\n479:                _, loss = model(x, y, return_logits=False)\n480:                train_loss = loss.detach()\n481:            # advance the dataset for the next batch\n482:            x, y = train_loader.next_batch()\n483:            # backward pass\n484:            if i < train_accumulation_steps:\n485:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n486:                    loss.backward()\n487:            else:\n488:                loss.backward() # just sync on the last step\n489:        for p in model.parameters():\n490:            p.grad /= train_accumulation_steps\n491:        # step the optimizers and schedulers\n492:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n493:        #     opt.step()\n494:        #     sched.step()\n495:        # null the gradients\n496:        model.zero_grad(set_to_none=True)\n497:        # --------------- TRAINING SECTION END -------------------\n498:        # everything that follows now is just diagnostics, prints, logging, etc.\n499:\n500:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n501:        if master_process:\n502:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n503:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n504:            with open(logfile, \"a\") as f:\n505:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n506:\n507:    torch.save(model.state_dict(), \"model.pt\")\n508:    with open(\"model_config.pt\", \"wb\") as f:\n509:        pickle.dump(raw_model.get_config(), f)\n510:    f.close()\n511:\n512:    if master_process:\n513:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n514:\n515:    # -------------------------------------------------------------------------\n516:    # clean up nice\n517:    dist.destroy_process_group()\n518:\n519:if __name__ == \"__main__\":\n520:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 11, Remaining Steps: 39)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the scheduler is initialized.\n\nStep 6: Add the new optimizer and scheduler step calls in the training loop after gradient averaging (after line 490).\n```bash\ninsert 490\n        # Step the new optimizer and scheduler\n        optimizer.step()\n        scheduler.step()\nend_of_insert\n```",
      "thought": "DISCUSSION\nOkay, the scheduler is initialized.\n\nStep 6: Add the new optimizer and scheduler step calls in the training loop after gradient averaging (after line 490).\n",
      "action": "insert 490\n        # Step the new optimizer and scheduler\n        optimizer.step()\n        scheduler.step()\nend_of_insert\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (523 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 1000 # number of iterations for linear warmup\n359:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n411:    # New AdamW optimizer setup\n412:    optimizer = torch.optim.AdamW(\n413:        raw_model.parameters(),\n414:        lr=3e-4, # Max learning rate\n415:        weight_decay=args.weight_decay,\n416:        betas=(0.9, 0.95),\n417:        fused=True,\n418:    )\n419:    # learning rate decay scheduler (linear warmup and warmdown)\n420:    def get_lr(it):\n421:        assert it <= args.num_iterations\n422:        # 1) linear warmup for warmup_iters steps\n423:        if it < args.warmup_iters:\n424:            return (it+1) / args.warmup_iters\n425:        # 2) constant lr for a while\n426:        elif it < args.num_iterations - args.warmdown_iters:\n427:            return 1.0\n428:        # 3) linear warmdown\n429:        else:\n430:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n431:            return decay_ratio\n432:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n433:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n434:\n435:    # begin logging\n436:    if master_process:\n437:        run_id = str(uuid.uuid4())\n438:        logdir = 'logs/%s/' % run_id\n439:        os.makedirs(logdir, exist_ok=True)\n440:        logfile = 'logs/%s.txt' % run_id\n441:        # create the log file\n442:        with open(logfile, \"w\") as f:\n443:            # begin the log by printing this file (the Python code)\n444:            f.write('='*100 + '\\n')\n445:            f.write(code)\n446:            f.write('='*100 + '\\n')\n447:            # log information about the hardware/software environment this is running on\n448:            # and print the full `nvidia-smi` to file\n449:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n450:            import subprocess\n451:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n452:            f.write(f'{result.stdout}\\n')\n453:            f.write('='*100 + '\\n')\n454:\n455:    training_time_ms = 0\n456:    # start the clock\n457:    torch.cuda.synchronize()\n458:    t0 = time.time()\n459:    # begin training\n460:    train_loader.reset()\n461:    for step in range(args.num_iterations + 1):\n462:        last_step = (step == args.num_iterations)\n463:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n464:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n465:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n466:        if step == 10:\n467:            training_time_ms = 0\n468:            t0 = time.time()\n469:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n470:\n471:        if last_step:\n472:            break\n473:\n474:        # --------------- TRAINING SECTION BEGIN -----------------\n475:        model.train()\n476:        for i in range(1, train_accumulation_steps+1):\n477:            # forward pass\n478:            with ctx:\n479:                _, loss = model(x, y, return_logits=False)\n480:                train_loss = loss.detach()\n481:            # advance the dataset for the next batch\n482:            x, y = train_loader.next_batch()\n483:            # backward pass\n484:            if i < train_accumulation_steps:\n485:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n486:                    loss.backward()\n487:            else:\n488:                loss.backward() # just sync on the last step\n489:        for p in model.parameters():\n490:            p.grad /= train_accumulation_steps\n491:        # Step the new optimizer and scheduler\n492:        optimizer.step()\n493:        scheduler.step()\n494:        # step the optimizers and schedulers\n495:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n496:        #     opt.step()\n497:        #     sched.step()\n498:        # null the gradients\n499:        model.zero_grad(set_to_none=True)\n500:        # --------------- TRAINING SECTION END -------------------\n501:        # everything that follows now is just diagnostics, prints, logging, etc.\n502:\n503:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n504:        if master_process:\n505:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n506:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n507:            with open(logfile, \"a\") as f:\n508:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n509:\n510:    torch.save(model.state_dict(), \"model.pt\")\n511:    with open(\"model_config.pt\", \"wb\") as f:\n512:        pickle.dump(raw_model.get_config(), f)\n513:    f.close()\n514:\n515:    if master_process:\n516:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n517:\n518:    # -------------------------------------------------------------------------\n519:    # clean up nice\n520:    dist.destroy_process_group()\n521:\n522:if __name__ == \"__main__\":\n523:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 12, Remaining Steps: 38)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nLooks correct. The new optimizer and scheduler steps are inserted at the right place, before zeroing the gradients.\n\nStep 7: Finally, remove the Muon optimizer class definition and related functions, as they are no longer needed (lines 31-131).\n```bash\nedit 31:131\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n```",
      "thought": "DISCUSSION\nLooks correct. The new optimizer and scheduler steps are inserted at the right place, before zeroing the gradients.\n\nStep 7: Finally, remove the Muon optimizer class definition and related functions, as they are no longer needed (lines 31-131).\n",
      "action": "edit 31:131\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (424 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:# -----------------------------------------------------------------------------\n35:# PyTorch nn.Module definitions for the GPT-2 model\n36:\n37:class Rotary(torch.nn.Module):\n38:\n39:    def __init__(self, dim, base=10000):\n40:        super().__init__()\n41:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n42:        self.seq_len_cached = None\n43:        self.cos_cached = None\n44:        self.sin_cached = None\n45:\n46:    def forward(self, x):\n47:        seq_len = x.shape[1]\n48:        if seq_len != self.seq_len_cached:\n49:            self.seq_len_cached = seq_len\n50:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n51:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n52:            self.cos_cached = freqs.cos().bfloat16()\n53:            self.sin_cached = freqs.sin().bfloat16()\n54:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n55:\n56:def apply_rotary_emb(x, cos, sin):\n57:    assert x.ndim == 4 # multihead attention\n58:    d = x.shape[3]//2\n59:    x1 = x[..., :d]\n60:    x2 = x[..., d:]\n61:    y1 = x1 * cos + x2 * sin\n62:    y2 = x1 * (-sin) + x2 * cos\n63:    return torch.cat([y1, y2], 3).type_as(x)\n64:\n65:class CausalSelfAttention(nn.Module):\n66:\n67:    def __init__(self, config):\n68:        super().__init__()\n69:        self.n_head = config.n_head\n70:        self.n_embd = config.n_embd\n71:        self.head_dim = self.n_embd // self.n_head\n72:        assert self.n_embd % self.n_head == 0\n73:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n74:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n76:        # output projection\n77:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n78:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n79:        self.rotary = Rotary(self.head_dim)\n80:\n81:    def forward(self, x):\n82:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n83:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n84:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n85:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n86:        cos, sin = self.rotary(q)\n87:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n88:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n89:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n90:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n91:        y = self.c_proj(y)\n92:        return y\n93:\n94:class MLP(nn.Module):\n95:\n96:    def __init__(self, config):\n97:        super().__init__()\n98:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n99:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n100:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n101:\n102:    def forward(self, x):\n103:        x = self.c_fc(x)\n104:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n105:        x = self.c_proj(x)\n106:        return x\n107:\n108:class Block(nn.Module):\n109:\n110:    def __init__(self, config):\n111:        super().__init__()\n112:        self.attn = CausalSelfAttention(config)\n113:        self.mlp = MLP(config)\n114:\n115:    def forward(self, x):\n116:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n117:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n118:        return x\n119:\n120:# -----------------------------------------------------------------------------\n121:# The main GPT-2 model\n122:\n123:@dataclass\n124:class ModelConfig:\n125:    vocab_size : int = 50304\n126:    n_layer : int = 12\n127:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n128:    n_embd : int = 768\n129:\n130:class Model(nn.Module):\n131:\n132:    def __init__(self, config):\n133:        super().__init__()\n134:        self.config = config\n135:\n136:        self.transformer = nn.ModuleDict(dict(\n137:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n138:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n139:        ))\n140:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n141:        self.lm_head.weight.data.zero_() # @Grad62304977\n142:\n143:    def forward(self, idx, targets=None, return_logits=True):\n144:\n145:        # forward the GPT model itself\n146:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n147:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n148:        for block in self.transformer.h:\n149:            x = block(x)\n150:        x = F.rms_norm(x, (x.size(-1),))\n151:\n152:        if targets is not None:\n153:            # if we are given some desired targets also calculate the loss\n154:            logits = self.lm_head(x)\n155:            logits = logits.float() # use tf32/fp32 for logits\n156:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n157:        else:\n158:            # inference-time mini-optimization: only forward the lm_head on the very last position\n159:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n160:            logits = logits.float() # use tf32/fp32 for logits\n161:            loss = None\n162:\n163:        # there are performance reasons why not returning logits is prudent, if not needed\n164:        if not return_logits:\n165:            logits = None\n166:\n167:        return logits, loss\n168:    \n169:    def get_config(self):\n170:        return self.config\n171:\n172:# -----------------------------------------------------------------------------\n173:# Our own simple Distributed Data Loader\n174:\n175:def _peek_data_shard(filename):\n176:    # only reads the header, returns header data\n177:    with open(filename, \"rb\") as f:\n178:        # first read the header, which is 256 int32 integers (4 bytes each)\n179:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n180:    if header[0] != 20240520:\n181:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n182:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n183:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n184:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n185:        exit(1)\n186:    assert header[1] == 1, \"unsupported version\"\n187:    ntok = header[2] # number of tokens (claimed)\n188:    return ntok # for now just return the number of tokens\n189:\n190:def _load_data_shard(filename):\n191:    with open(filename, \"rb\") as f:\n192:        # first read the header, which is 256 int32 integers (4 bytes each)\n193:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n194:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n195:        assert header[1] == 1, \"unsupported version\"\n196:        ntok = header[2] # number of tokens (claimed)\n197:        # the rest of it are tokens, stored as uint16\n198:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n199:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n200:    return tokens\n201:\n202:class DistributedDataLoader:\n203:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n204:        self.process_rank = process_rank\n205:        self.num_processes = num_processes\n206:        self.B = B\n207:        self.T = T\n208:\n209:        # glob files that match the pattern\n210:        self.files = sorted(glob.glob(filename_pattern))\n211:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n212:\n213:        # load and validate all data shards, count number of tokens in total\n214:        ntok_total = 0\n215:        for fname in self.files:\n216:            shard_ntok = _peek_data_shard(fname)\n217:            assert shard_ntok >= num_processes * B * T + 1\n218:            ntok_total += int(shard_ntok)\n219:        self.ntok_total = ntok_total\n220:\n221:        # kick things off\n222:        self.reset()\n223:\n224:    def reset(self):\n225:        self.current_shard = 0\n226:        self.current_position = self.process_rank * self.B * self.T\n227:        self.tokens = _load_data_shard(self.files[self.current_shard])\n228:\n229:    def advance(self): # advance to next data shard\n230:        self.current_shard = (self.current_shard + 1) % len(self.files)\n231:        self.current_position = self.process_rank * self.B * self.T\n232:        self.tokens = _load_data_shard(self.files[self.current_shard])\n233:\n234:    def next_batch(self):\n235:        B = self.B\n236:        T = self.T\n237:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n238:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n239:        x = (buf[:-1]).view(B, T) # inputs\n240:        y = (buf[1:]).view(B, T) # targets\n241:        # advance current position and load next shard if necessary\n242:        self.current_position += B * T * self.num_processes\n243:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n244:            self.advance()\n245:        return x.cuda(), y.cuda()\n246:\n247:# -----------------------------------------------------------------------------\n248:# int main\n249:\n250:@dataclass\n251:class Hyperparameters:\n252:    # data hyperparams\n253:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n254:    # optimization hyperparams\n255:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n256:    device_batch_size : int = 64 # batch size, in sequences, per device\n257:    sequence_length : int = 1024 # sequence length, in tokens\n258:    num_iterations : int = 10000 # number of iterations to run\n259:    warmup_iters : int = 1000 # number of iterations for linear warmup\n260:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n261:    weight_decay : float = 0.1\n262:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n263:\n264:\n265:def main():\n266:    with open(sys.argv[0]) as f:\n267:        code = f.read() # read the code of this file ASAP, for logging\n268:    args = Hyperparameters()\n269:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n270:\n271:    # set up DDP (distributed data parallel). torchrun sets this env variable\n272:    assert torch.cuda.is_available()\n273:    dist.init_process_group(backend='nccl')\n274:    ddp_rank = int(os.environ['RANK'])\n275:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n276:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n277:    device = f'cuda:{ddp_local_rank}'\n278:    torch.cuda.set_device(device)\n279:    print(f\"using device: {device}\")\n280:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n281:\n282:    # convenience variables\n283:    B, T = args.device_batch_size, args.sequence_length\n284:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n285:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n286:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n287:\n288:    # load tokens\n289:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n290:    if master_process:\n291:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n292:    x, y = train_loader.next_batch()\n293:\n294:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n295:    # this originates from Karpathy's experiments.\n296:    num_vocab = 50304\n297:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n298:    model = model.cuda()\n299:    if hasattr(config, \"coordinate_descent_tuning\"):\n300:        config.coordinate_descent_tuning = True # suggested by @Chillee\n301:    model = torch.compile(model)\n302:    # here we wrap model into DDP container\n303:    model = DDP(model, device_ids=[ddp_local_rank])\n304:    raw_model = model.module # always contains the \"raw\" unwrapped model\n305:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n306:\n307:    # init the optimizer(s)\n308:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n309:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n310:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n311:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n312:    # New AdamW optimizer setup\n313:    optimizer = torch.optim.AdamW(\n314:        raw_model.parameters(),\n315:        lr=3e-4, # Max learning rate\n316:        weight_decay=args.weight_decay,\n317:        betas=(0.9, 0.95),\n318:        fused=True,\n319:    )\n320:    # learning rate decay scheduler (linear warmup and warmdown)\n321:    def get_lr(it):\n322:        assert it <= args.num_iterations\n323:        # 1) linear warmup for warmup_iters steps\n324:        if it < args.warmup_iters:\n325:            return (it+1) / args.warmup_iters\n326:        # 2) constant lr for a while\n327:        elif it < args.num_iterations - args.warmdown_iters:\n328:            return 1.0\n329:        # 3) linear warmdown\n330:        else:\n331:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n332:            return decay_ratio\n333:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n334:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n335:\n336:    # begin logging\n337:    if master_process:\n338:        run_id = str(uuid.uuid4())\n339:        logdir = 'logs/%s/' % run_id\n340:        os.makedirs(logdir, exist_ok=True)\n341:        logfile = 'logs/%s.txt' % run_id\n342:        # create the log file\n343:        with open(logfile, \"w\") as f:\n344:            # begin the log by printing this file (the Python code)\n345:            f.write('='*100 + '\\n')\n346:            f.write(code)\n347:            f.write('='*100 + '\\n')\n348:            # log information about the hardware/software environment this is running on\n349:            # and print the full `nvidia-smi` to file\n350:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n351:            import subprocess\n352:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n353:            f.write(f'{result.stdout}\\n')\n354:            f.write('='*100 + '\\n')\n355:\n356:    training_time_ms = 0\n357:    # start the clock\n358:    torch.cuda.synchronize()\n359:    t0 = time.time()\n360:    # begin training\n361:    train_loader.reset()\n362:    for step in range(args.num_iterations + 1):\n363:        last_step = (step == args.num_iterations)\n364:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n365:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n366:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n367:        if step == 10:\n368:            training_time_ms = 0\n369:            t0 = time.time()\n370:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n371:\n372:        if last_step:\n373:            break\n374:\n375:        # --------------- TRAINING SECTION BEGIN -----------------\n376:        model.train()\n377:        for i in range(1, train_accumulation_steps+1):\n378:            # forward pass\n379:            with ctx:\n380:                _, loss = model(x, y, return_logits=False)\n381:                train_loss = loss.detach()\n382:            # advance the dataset for the next batch\n383:            x, y = train_loader.next_batch()\n384:            # backward pass\n385:            if i < train_accumulation_steps:\n386:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n387:                    loss.backward()\n388:            else:\n389:                loss.backward() # just sync on the last step\n390:        for p in model.parameters():\n391:            p.grad /= train_accumulation_steps\n392:        # Step the new optimizer and scheduler\n393:        optimizer.step()\n394:        scheduler.step()\n395:        # step the optimizers and schedulers\n396:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n397:        #     opt.step()\n398:        #     sched.step()\n399:        # null the gradients\n400:        model.zero_grad(set_to_none=True)\n401:        # --------------- TRAINING SECTION END -------------------\n402:        # everything that follows now is just diagnostics, prints, logging, etc.\n403:\n404:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n405:        if master_process:\n406:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n407:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n408:            with open(logfile, \"a\") as f:\n409:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n410:\n411:    torch.save(model.state_dict(), \"model.pt\")\n412:    with open(\"model_config.pt\", \"wb\") as f:\n413:        pickle.dump(raw_model.get_config(), f)\n414:    f.close()\n415:\n416:    if master_process:\n417:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n418:\n419:    # -------------------------------------------------------------------------\n420:    # clean up nice\n421:    dist.destroy_process_group()\n422:\n423:if __name__ == \"__main__\":\n424:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 13, Remaining Steps: 37)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the Muon optimizer code has been removed. There are some redundant comment lines left over (lines 30-35). Let's clean those up.\n```bash\nedit 30:35\n\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the Muon optimizer code has been removed. There are some redundant comment lines left over (lines 30-35). Let's clean those up.\n",
      "action": "edit 30:35\n\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (421 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:# The main GPT-2 model\n119:\n120:@dataclass\n121:class ModelConfig:\n122:    vocab_size : int = 50304\n123:    n_layer : int = 12\n124:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n125:    n_embd : int = 768\n126:\n127:class Model(nn.Module):\n128:\n129:    def __init__(self, config):\n130:        super().__init__()\n131:        self.config = config\n132:\n133:        self.transformer = nn.ModuleDict(dict(\n134:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n135:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n136:        ))\n137:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n138:        self.lm_head.weight.data.zero_() # @Grad62304977\n139:\n140:    def forward(self, idx, targets=None, return_logits=True):\n141:\n142:        # forward the GPT model itself\n143:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n144:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n145:        for block in self.transformer.h:\n146:            x = block(x)\n147:        x = F.rms_norm(x, (x.size(-1),))\n148:\n149:        if targets is not None:\n150:            # if we are given some desired targets also calculate the loss\n151:            logits = self.lm_head(x)\n152:            logits = logits.float() # use tf32/fp32 for logits\n153:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n154:        else:\n155:            # inference-time mini-optimization: only forward the lm_head on the very last position\n156:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n157:            logits = logits.float() # use tf32/fp32 for logits\n158:            loss = None\n159:\n160:        # there are performance reasons why not returning logits is prudent, if not needed\n161:        if not return_logits:\n162:            logits = None\n163:\n164:        return logits, loss\n165:    \n166:    def get_config(self):\n167:        return self.config\n168:\n169:# -----------------------------------------------------------------------------\n170:# Our own simple Distributed Data Loader\n171:\n172:def _peek_data_shard(filename):\n173:    # only reads the header, returns header data\n174:    with open(filename, \"rb\") as f:\n175:        # first read the header, which is 256 int32 integers (4 bytes each)\n176:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n177:    if header[0] != 20240520:\n178:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n179:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n180:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n181:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n182:        exit(1)\n183:    assert header[1] == 1, \"unsupported version\"\n184:    ntok = header[2] # number of tokens (claimed)\n185:    return ntok # for now just return the number of tokens\n186:\n187:def _load_data_shard(filename):\n188:    with open(filename, \"rb\") as f:\n189:        # first read the header, which is 256 int32 integers (4 bytes each)\n190:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n191:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n192:        assert header[1] == 1, \"unsupported version\"\n193:        ntok = header[2] # number of tokens (claimed)\n194:        # the rest of it are tokens, stored as uint16\n195:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n196:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n197:    return tokens\n198:\n199:class DistributedDataLoader:\n200:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n201:        self.process_rank = process_rank\n202:        self.num_processes = num_processes\n203:        self.B = B\n204:        self.T = T\n205:\n206:        # glob files that match the pattern\n207:        self.files = sorted(glob.glob(filename_pattern))\n208:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n209:\n210:        # load and validate all data shards, count number of tokens in total\n211:        ntok_total = 0\n212:        for fname in self.files:\n213:            shard_ntok = _peek_data_shard(fname)\n214:            assert shard_ntok >= num_processes * B * T + 1\n215:            ntok_total += int(shard_ntok)\n216:        self.ntok_total = ntok_total\n217:\n218:        # kick things off\n219:        self.reset()\n220:\n221:    def reset(self):\n222:        self.current_shard = 0\n223:        self.current_position = self.process_rank * self.B * self.T\n224:        self.tokens = _load_data_shard(self.files[self.current_shard])\n225:\n226:    def advance(self): # advance to next data shard\n227:        self.current_shard = (self.current_shard + 1) % len(self.files)\n228:        self.current_position = self.process_rank * self.B * self.T\n229:        self.tokens = _load_data_shard(self.files[self.current_shard])\n230:\n231:    def next_batch(self):\n232:        B = self.B\n233:        T = self.T\n234:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n235:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n236:        x = (buf[:-1]).view(B, T) # inputs\n237:        y = (buf[1:]).view(B, T) # targets\n238:        # advance current position and load next shard if necessary\n239:        self.current_position += B * T * self.num_processes\n240:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n241:            self.advance()\n242:        return x.cuda(), y.cuda()\n243:\n244:# -----------------------------------------------------------------------------\n245:# int main\n246:\n247:@dataclass\n248:class Hyperparameters:\n249:    # data hyperparams\n250:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n251:    # optimization hyperparams\n252:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n253:    device_batch_size : int = 64 # batch size, in sequences, per device\n254:    sequence_length : int = 1024 # sequence length, in tokens\n255:    num_iterations : int = 10000 # number of iterations to run\n256:    warmup_iters : int = 1000 # number of iterations for linear warmup\n257:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n258:    weight_decay : float = 0.1\n259:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n260:\n261:\n262:def main():\n263:    with open(sys.argv[0]) as f:\n264:        code = f.read() # read the code of this file ASAP, for logging\n265:    args = Hyperparameters()\n266:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n267:\n268:    # set up DDP (distributed data parallel). torchrun sets this env variable\n269:    assert torch.cuda.is_available()\n270:    dist.init_process_group(backend='nccl')\n271:    ddp_rank = int(os.environ['RANK'])\n272:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n273:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n274:    device = f'cuda:{ddp_local_rank}'\n275:    torch.cuda.set_device(device)\n276:    print(f\"using device: {device}\")\n277:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n278:\n279:    # convenience variables\n280:    B, T = args.device_batch_size, args.sequence_length\n281:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n282:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n283:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n284:\n285:    # load tokens\n286:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n287:    if master_process:\n288:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n289:    x, y = train_loader.next_batch()\n290:\n291:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n292:    # this originates from Karpathy's experiments.\n293:    num_vocab = 50304\n294:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n295:    model = model.cuda()\n296:    if hasattr(config, \"coordinate_descent_tuning\"):\n297:        config.coordinate_descent_tuning = True # suggested by @Chillee\n298:    model = torch.compile(model)\n299:    # here we wrap model into DDP container\n300:    model = DDP(model, device_ids=[ddp_local_rank])\n301:    raw_model = model.module # always contains the \"raw\" unwrapped model\n302:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n303:\n304:    # init the optimizer(s)\n305:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n306:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n307:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n308:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n309:    # New AdamW optimizer setup\n310:    optimizer = torch.optim.AdamW(\n311:        raw_model.parameters(),\n312:        lr=3e-4, # Max learning rate\n313:        weight_decay=args.weight_decay,\n314:        betas=(0.9, 0.95),\n315:        fused=True,\n316:    )\n317:    # learning rate decay scheduler (linear warmup and warmdown)\n318:    def get_lr(it):\n319:        assert it <= args.num_iterations\n320:        # 1) linear warmup for warmup_iters steps\n321:        if it < args.warmup_iters:\n322:            return (it+1) / args.warmup_iters\n323:        # 2) constant lr for a while\n324:        elif it < args.num_iterations - args.warmdown_iters:\n325:            return 1.0\n326:        # 3) linear warmdown\n327:        else:\n328:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n329:            return decay_ratio\n330:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n331:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n332:\n333:    # begin logging\n334:    if master_process:\n335:        run_id = str(uuid.uuid4())\n336:        logdir = 'logs/%s/' % run_id\n337:        os.makedirs(logdir, exist_ok=True)\n338:        logfile = 'logs/%s.txt' % run_id\n339:        # create the log file\n340:        with open(logfile, \"w\") as f:\n341:            # begin the log by printing this file (the Python code)\n342:            f.write('='*100 + '\\n')\n343:            f.write(code)\n344:            f.write('='*100 + '\\n')\n345:            # log information about the hardware/software environment this is running on\n346:            # and print the full `nvidia-smi` to file\n347:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n348:            import subprocess\n349:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n350:            f.write(f'{result.stdout}\\n')\n351:            f.write('='*100 + '\\n')\n352:\n353:    training_time_ms = 0\n354:    # start the clock\n355:    torch.cuda.synchronize()\n356:    t0 = time.time()\n357:    # begin training\n358:    train_loader.reset()\n359:    for step in range(args.num_iterations + 1):\n360:        last_step = (step == args.num_iterations)\n361:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n362:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n363:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n364:        if step == 10:\n365:            training_time_ms = 0\n366:            t0 = time.time()\n367:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n368:\n369:        if last_step:\n370:            break\n371:\n372:        # --------------- TRAINING SECTION BEGIN -----------------\n373:        model.train()\n374:        for i in range(1, train_accumulation_steps+1):\n375:            # forward pass\n376:            with ctx:\n377:                _, loss = model(x, y, return_logits=False)\n378:                train_loss = loss.detach()\n379:            # advance the dataset for the next batch\n380:            x, y = train_loader.next_batch()\n381:            # backward pass\n382:            if i < train_accumulation_steps:\n383:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n384:                    loss.backward()\n385:            else:\n386:                loss.backward() # just sync on the last step\n387:        for p in model.parameters():\n388:            p.grad /= train_accumulation_steps\n389:        # Step the new optimizer and scheduler\n390:        optimizer.step()\n391:        scheduler.step()\n392:        # step the optimizers and schedulers\n393:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n394:        #     opt.step()\n395:        #     sched.step()\n396:        # null the gradients\n397:        model.zero_grad(set_to_none=True)\n398:        # --------------- TRAINING SECTION END -------------------\n399:        # everything that follows now is just diagnostics, prints, logging, etc.\n400:\n401:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n402:        if master_process:\n403:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n404:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n405:            with open(logfile, \"a\") as f:\n406:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n407:\n408:    torch.save(model.state_dict(), \"model.pt\")\n409:    with open(\"model_config.pt\", \"wb\") as f:\n410:        pickle.dump(raw_model.get_config(), f)\n411:    f.close()\n412:\n413:    if master_process:\n414:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n415:\n416:    # -------------------------------------------------------------------------\n417:    # clean up nice\n418:    dist.destroy_process_group()\n419:\n420:if __name__ == \"__main__\":\n421:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 14, Remaining Steps: 36)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe script looks much cleaner now. The main changes were:\n- Increased `num_iterations` from 500 to 10000.\n- Adjusted `warmup_iters` and `warmdown_iters` to 1000 each.\n- Replaced the complex multi-optimizer setup with a single AdamW optimizer with `lr=3e-4`, `weight_decay=0.1`, `betas=(0.9, 0.95)`.\n- Updated the scheduler to work with the new optimizer.\n- Removed the unused Muon optimizer code.\n\nNow, I'm ready to run the training. The script uses `torchrun`, so I need to determine the number of available GPUs first.\n```bash\nnvidia-smi --query-gpu=count --format=csv,noheader\n```",
      "thought": "DISCUSSION\nThe script looks much cleaner now. The main changes were:\n- Increased `num_iterations` from 500 to 10000.\n- Adjusted `warmup_iters` and `warmdown_iters` to 1000 each.\n- Replaced the complex multi-optimizer setup with a single AdamW optimizer with `lr=3e-4`, `weight_decay=0.1`, `betas=(0.9, 0.95)`.\n- Updated the scheduler to work with the new optimizer.\n- Removed the unused Muon optimizer code.\n\nNow, I'm ready to run the training. The script uses `torchrun`, so I need to determine the number of available GPUs first.\n",
      "action": "nvidia-smi --query-gpu=count --format=csv,noheader\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "2\n2\n(Current Step: 15, Remaining Steps: 35)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nIt seems the command outputted the number twice. There are 2 GPUs available. I'll use `torchrun --nproc_per_node=2` to launch the script. Training might take a while with 10000 iterations.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nIt seems the command outputted the number twice. There are 2 GPUs available. I'll use `torchrun --nproc_per_node=2` to launch the script. Training might take a while with 10000 iterations.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "W0405 11:21:42.609000 768 site-packages/torch/distributed/run.py:793] \nW0405 11:21:42.609000 768 site-packages/torch/distributed/run.py:793] *****************************************\nW0405 11:21:42.609000 768 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0405 11:21:42.609000 768 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 421, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 286, in main\n[rank1]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 208, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 421, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 286, in main\n[rank0]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 208, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\n[rank0]:[W405 11:21:45.035800547 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0405 11:21:45.448000 768 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 835 closing signal SIGTERM\nE0405 11:21:45.562000 768 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 834) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-05_11:21:45\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 834)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n(Current Step: 16, Remaining Steps: 34)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe training failed because the script couldn't find the data files at the expected location: `./data/fineweb10B/fineweb_train_*.bin`.\n\nThe `AssertionError` comes from the `DistributedDataLoader` which uses `glob.glob` to find the training shards. This means either the directory `data/fineweb10B` doesn't exist, or it doesn't contain files matching `fineweb_train_*.bin`.\n\nLet's check the contents of the `data` directory to see what's inside.\n```bash\nls -l data/\n```",
      "thought": "DISCUSSION\nThe training failed because the script couldn't find the data files at the expected location: `./data/fineweb10B/fineweb_train_*.bin`.\n\nThe `AssertionError` comes from the `DistributedDataLoader` which uses `glob.glob` to find the training shards. This means either the directory `data/fineweb10B` doesn't exist, or it doesn't contain files matching `fineweb_train_*.bin`.\n\nLet's check the contents of the `data` directory to see what's inside.\n",
      "action": "ls -l data/\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "total 0\ndr-xr-xr-x 1 agent mlgym 908 Mar 26 02:59 fineweb108/\n(Current Step: 17, Remaining Steps: 33)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAh, the directory name is `fineweb108/`, not `fineweb10B/`. That's a simple typo in the script's `Hyperparameters`.\n\nLet's correct the `input_bin` path in the `Hyperparameters` dataclass (line 250).\n```bash\nedit 250:250\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, the directory name is `fineweb108/`, not `fineweb10B/`. That's a simple typo in the script's `Hyperparameters`.\n\nLet's correct the `input_bin` path in the `Hyperparameters` dataclass (line 250).\n",
      "action": "edit 250:250\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (421 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:# The main GPT-2 model\n119:\n120:@dataclass\n121:class ModelConfig:\n122:    vocab_size : int = 50304\n123:    n_layer : int = 12\n124:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n125:    n_embd : int = 768\n126:\n127:class Model(nn.Module):\n128:\n129:    def __init__(self, config):\n130:        super().__init__()\n131:        self.config = config\n132:\n133:        self.transformer = nn.ModuleDict(dict(\n134:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n135:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n136:        ))\n137:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n138:        self.lm_head.weight.data.zero_() # @Grad62304977\n139:\n140:    def forward(self, idx, targets=None, return_logits=True):\n141:\n142:        # forward the GPT model itself\n143:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n144:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n145:        for block in self.transformer.h:\n146:            x = block(x)\n147:        x = F.rms_norm(x, (x.size(-1),))\n148:\n149:        if targets is not None:\n150:            # if we are given some desired targets also calculate the loss\n151:            logits = self.lm_head(x)\n152:            logits = logits.float() # use tf32/fp32 for logits\n153:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n154:        else:\n155:            # inference-time mini-optimization: only forward the lm_head on the very last position\n156:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n157:            logits = logits.float() # use tf32/fp32 for logits\n158:            loss = None\n159:\n160:        # there are performance reasons why not returning logits is prudent, if not needed\n161:        if not return_logits:\n162:            logits = None\n163:\n164:        return logits, loss\n165:    \n166:    def get_config(self):\n167:        return self.config\n168:\n169:# -----------------------------------------------------------------------------\n170:# Our own simple Distributed Data Loader\n171:\n172:def _peek_data_shard(filename):\n173:    # only reads the header, returns header data\n174:    with open(filename, \"rb\") as f:\n175:        # first read the header, which is 256 int32 integers (4 bytes each)\n176:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n177:    if header[0] != 20240520:\n178:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n179:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n180:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n181:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n182:        exit(1)\n183:    assert header[1] == 1, \"unsupported version\"\n184:    ntok = header[2] # number of tokens (claimed)\n185:    return ntok # for now just return the number of tokens\n186:\n187:def _load_data_shard(filename):\n188:    with open(filename, \"rb\") as f:\n189:        # first read the header, which is 256 int32 integers (4 bytes each)\n190:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n191:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n192:        assert header[1] == 1, \"unsupported version\"\n193:        ntok = header[2] # number of tokens (claimed)\n194:        # the rest of it are tokens, stored as uint16\n195:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n196:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n197:    return tokens\n198:\n199:class DistributedDataLoader:\n200:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n201:        self.process_rank = process_rank\n202:        self.num_processes = num_processes\n203:        self.B = B\n204:        self.T = T\n205:\n206:        # glob files that match the pattern\n207:        self.files = sorted(glob.glob(filename_pattern))\n208:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n209:\n210:        # load and validate all data shards, count number of tokens in total\n211:        ntok_total = 0\n212:        for fname in self.files:\n213:            shard_ntok = _peek_data_shard(fname)\n214:            assert shard_ntok >= num_processes * B * T + 1\n215:            ntok_total += int(shard_ntok)\n216:        self.ntok_total = ntok_total\n217:\n218:        # kick things off\n219:        self.reset()\n220:\n221:    def reset(self):\n222:        self.current_shard = 0\n223:        self.current_position = self.process_rank * self.B * self.T\n224:        self.tokens = _load_data_shard(self.files[self.current_shard])\n225:\n226:    def advance(self): # advance to next data shard\n227:        self.current_shard = (self.current_shard + 1) % len(self.files)\n228:        self.current_position = self.process_rank * self.B * self.T\n229:        self.tokens = _load_data_shard(self.files[self.current_shard])\n230:\n231:    def next_batch(self):\n232:        B = self.B\n233:        T = self.T\n234:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n235:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n236:        x = (buf[:-1]).view(B, T) # inputs\n237:        y = (buf[1:]).view(B, T) # targets\n238:        # advance current position and load next shard if necessary\n239:        self.current_position += B * T * self.num_processes\n240:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n241:            self.advance()\n242:        return x.cuda(), y.cuda()\n243:\n244:# -----------------------------------------------------------------------------\n245:# int main\n246:\n247:@dataclass\n248:class Hyperparameters:\n249:    # data hyperparams\n250:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n251:    # optimization hyperparams\n252:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n253:    device_batch_size : int = 64 # batch size, in sequences, per device\n254:    sequence_length : int = 1024 # sequence length, in tokens\n255:    num_iterations : int = 10000 # number of iterations to run\n256:    warmup_iters : int = 1000 # number of iterations for linear warmup\n257:    warmdown_iters : int = 1000 # number of iterations of linear warmdown\n258:    weight_decay : float = 0.1\n259:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n260:\n261:\n262:def main():\n263:    with open(sys.argv[0]) as f:\n264:        code = f.read() # read the code of this file ASAP, for logging\n265:    args = Hyperparameters()\n266:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n267:\n268:    # set up DDP (distributed data parallel). torchrun sets this env variable\n269:    assert torch.cuda.is_available()\n270:    dist.init_process_group(backend='nccl')\n271:    ddp_rank = int(os.environ['RANK'])\n272:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n273:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n274:    device = f'cuda:{ddp_local_rank}'\n275:    torch.cuda.set_device(device)\n276:    print(f\"using device: {device}\")\n277:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n278:\n279:    # convenience variables\n280:    B, T = args.device_batch_size, args.sequence_length\n281:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n282:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n283:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n284:\n285:    # load tokens\n286:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n287:    if master_process:\n288:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n289:    x, y = train_loader.next_batch()\n290:\n291:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n292:    # this originates from Karpathy's experiments.\n293:    num_vocab = 50304\n294:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n295:    model = model.cuda()\n296:    if hasattr(config, \"coordinate_descent_tuning\"):\n297:        config.coordinate_descent_tuning = True # suggested by @Chillee\n298:    model = torch.compile(model)\n299:    # here we wrap model into DDP container\n300:    model = DDP(model, device_ids=[ddp_local_rank])\n301:    raw_model = model.module # always contains the \"raw\" unwrapped model\n302:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n303:\n304:    # init the optimizer(s)\n305:    # optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n306:    # optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n307:    # optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n308:    # optimizers = [optimizer1, optimizer2, optimizer3] # Old setup, commented out\n309:    # New AdamW optimizer setup\n310:    optimizer = torch.optim.AdamW(\n311:        raw_model.parameters(),\n312:        lr=3e-4, # Max learning rate\n313:        weight_decay=args.weight_decay,\n314:        betas=(0.9, 0.95),\n315:        fused=True,\n316:    )\n317:    # learning rate decay scheduler (linear warmup and warmdown)\n318:    def get_lr(it):\n319:        assert it <= args.num_iterations\n320:        # 1) linear warmup for warmup_iters steps\n321:        if it < args.warmup_iters:\n322:            return (it+1) / args.warmup_iters\n323:        # 2) constant lr for a while\n324:        elif it < args.num_iterations - args.warmdown_iters:\n325:            return 1.0\n326:        # 3) linear warmdown\n327:        else:\n328:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n329:            return decay_ratio\n330:    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)\n331:    # schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers] # Old setup, commented out\n332:\n333:    # begin logging\n334:    if master_process:\n335:        run_id = str(uuid.uuid4())\n336:        logdir = 'logs/%s/' % run_id\n337:        os.makedirs(logdir, exist_ok=True)\n338:        logfile = 'logs/%s.txt' % run_id\n339:        # create the log file\n340:        with open(logfile, \"w\") as f:\n341:            # begin the log by printing this file (the Python code)\n342:            f.write('='*100 + '\\n')\n343:            f.write(code)\n344:            f.write('='*100 + '\\n')\n345:            # log information about the hardware/software environment this is running on\n346:            # and print the full `nvidia-smi` to file\n347:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n348:            import subprocess\n349:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n350:            f.write(f'{result.stdout}\\n')\n351:            f.write('='*100 + '\\n')\n352:\n353:    training_time_ms = 0\n354:    # start the clock\n355:    torch.cuda.synchronize()\n356:    t0 = time.time()\n357:    # begin training\n358:    train_loader.reset()\n359:    for step in range(args.num_iterations + 1):\n360:        last_step = (step == args.num_iterations)\n361:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n362:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n363:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n364:        if step == 10:\n365:            training_time_ms = 0\n366:            t0 = time.time()\n367:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n368:\n369:        if last_step:\n370:            break\n371:\n372:        # --------------- TRAINING SECTION BEGIN -----------------\n373:        model.train()\n374:        for i in range(1, train_accumulation_steps+1):\n375:            # forward pass\n376:            with ctx:\n377:                _, loss = model(x, y, return_logits=False)\n378:                train_loss = loss.detach()\n379:            # advance the dataset for the next batch\n380:            x, y = train_loader.next_batch()\n381:            # backward pass\n382:            if i < train_accumulation_steps:\n383:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n384:                    loss.backward()\n385:            else:\n386:                loss.backward() # just sync on the last step\n387:        for p in model.parameters():\n388:            p.grad /= train_accumulation_steps\n389:        # Step the new optimizer and scheduler\n390:        optimizer.step()\n391:        scheduler.step()\n392:        # step the optimizers and schedulers\n393:        # for opt, sched in zip(optimizers, schedulers): # Old setup, commented out\n394:        #     opt.step()\n395:        #     sched.step()\n396:        # null the gradients\n397:        model.zero_grad(set_to_none=True)\n398:        # --------------- TRAINING SECTION END -------------------\n399:        # everything that follows now is just diagnostics, prints, logging, etc.\n400:\n401:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n402:        if master_process:\n403:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n404:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n405:            with open(logfile, \"a\") as f:\n406:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n407:\n408:    torch.save(model.state_dict(), \"model.pt\")\n409:    with open(\"model_config.pt\", \"wb\") as f:\n410:        pickle.dump(raw_model.get_config(), f)\n411:    f.close()\n412:\n413:    if master_process:\n414:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n415:\n416:    # -------------------------------------------------------------------------\n417:    # clean up nice\n418:    dist.destroy_process_group()\n419:\n420:if __name__ == \"__main__\":\n421:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 18, Remaining Steps: 32)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe data path has been corrected to `fineweb108`. Let's try running the training script again.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThe data path has been corrected to `fineweb108`. Let's try running the training script again.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "W0405 11:22:01.150000 905 site-packages/torch/distributed/run.py:793] \nW0405 11:22:01.150000 905 site-packages/torch/distributed/run.py:793] *****************************************\nW0405 11:22:01.150000 905 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0405 11:22:01.150000 905 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nusing device: cuda:1\nstep:1/10000 train_loss:10.8258 train_time:58187ms step_avg:nanms\nstep:2/10000 train_loss:10.8258 train_time:69616ms step_avg:nanms\nstep:3/10000 train_loss:10.8257 train_time:69682ms step_avg:nanms\nstep:4/10000 train_loss:10.8257 train_time:69847ms step_avg:nanms\nstep:5/10000 train_loss:10.8255 train_time:70014ms step_avg:nanms\nstep:6/10000 train_loss:10.8253 train_time:70182ms step_avg:nanms\nstep:7/10000 train_loss:10.8250 train_time:70346ms step_avg:nanms\nstep:8/10000 train_loss:10.8247 train_time:70513ms step_avg:nanms\nstep:9/10000 train_loss:10.8243 train_time:70688ms step_avg:nanms\nstep:10/10000 train_loss:10.8239 train_time:70855ms step_avg:nanms\nstep:11/10000 train_loss:10.8234 train_time:65ms step_avg:nanms\nstep:12/10000 train_loss:10.8227 train_time:231ms step_avg:nanms\nstep:13/10000 train_loss:10.8219 train_time:403ms step_avg:134.25ms\nstep:14/10000 train_loss:10.8208 train_time:570ms step_avg:142.45ms\nstep:15/10000 train_loss:10.8197 train_time:739ms step_avg:147.70ms\nstep:16/10000 train_loss:10.8180 train_time:903ms step_avg:150.49ms\nstep:17/10000 train_loss:10.8169 train_time:1071ms step_avg:153.01ms\nstep:18/10000 train_loss:10.8145 train_time:1238ms step_avg:154.70ms\nstep:19/10000 train_loss:10.8119 train_time:1405ms step_avg:156.11ms\nstep:20/10000 train_loss:10.8092 train_time:1573ms step_avg:157.34ms\nstep:21/10000 train_loss:10.8058 train_time:1743ms step_avg:158.48ms\nstep:22/10000 train_loss:10.8013 train_time:1911ms step_avg:159.27ms\nstep:23/10000 train_loss:10.7969 train_time:2081ms step_avg:160.10ms\nstep:24/10000 train_loss:10.7920 train_time:2248ms step_avg:160.58ms\nstep:25/10000 train_loss:10.7867 train_time:2417ms step_avg:161.13ms\nstep:26/10000 train_loss:10.7829 train_time:2585ms step_avg:161.55ms\nstep:27/10000 train_loss:10.7733 train_time:2755ms step_avg:162.07ms\nstep:28/10000 train_loss:10.7662 train_time:2922ms step_avg:162.34ms\nstep:29/10000 train_loss:10.7569 train_time:3088ms step_avg:162.55ms\nstep:30/10000 train_loss:10.7500 train_time:3255ms step_avg:162.75ms\nstep:31/10000 train_loss:10.7384 train_time:3422ms step_avg:162.94ms\nstep:32/10000 train_loss:10.7251 train_time:3589ms step_avg:163.13ms\nstep:33/10000 train_loss:10.7192 train_time:3759ms step_avg:163.42ms\nstep:34/10000 train_loss:10.7067 train_time:3926ms step_avg:163.57ms\nstep:35/10000 train_loss:10.6962 train_time:4091ms step_avg:163.62ms\nstep:36/10000 train_loss:10.6863 train_time:4256ms step_avg:163.69ms\nstep:37/10000 train_loss:10.6705 train_time:4422ms step_avg:163.78ms\nstep:38/10000 train_loss:10.6609 train_time:4588ms step_avg:163.86ms\nstep:39/10000 train_loss:10.6467 train_time:4760ms step_avg:164.14ms\nstep:40/10000 train_loss:10.6361 train_time:4926ms step_avg:164.21ms\nstep:41/10000 train_loss:10.6237 train_time:5093ms step_avg:164.29ms\nstep:42/10000 train_loss:10.6083 train_time:5260ms step_avg:164.37ms\nstep:43/10000 train_loss:10.5940 train_time:5425ms step_avg:164.40ms\nstep:44/10000 train_loss:10.5801 train_time:5590ms step_avg:164.43ms\nstep:45/10000 train_loss:10.5610 train_time:5761ms step_avg:164.60ms\nstep:46/10000 train_loss:10.5472 train_time:5928ms step_avg:164.67ms\nstep:47/10000 train_loss:10.5366 train_time:6095ms step_avg:164.73ms\nstep:48/10000 train_loss:10.5206 train_time:6259ms step_avg:164.72ms\nstep:49/10000 train_loss:10.5054 train_time:6426ms step_avg:164.78ms\nstep:50/10000 train_loss:10.4898 train_time:6595ms step_avg:164.88ms\nstep:51/10000 train_loss:10.4853 train_time:6763ms step_avg:164.95ms\nstep:52/10000 train_loss:10.4564 train_time:6928ms step_avg:164.95ms\nstep:53/10000 train_loss:10.4459 train_time:7097ms step_avg:165.04ms\nstep:54/10000 train_loss:10.4247 train_time:7266ms step_avg:165.13ms\nstep:55/10000 train_loss:10.3941 train_time:7433ms step_avg:165.18ms\nstep:56/10000 train_loss:10.3902 train_time:7603ms step_avg:165.28ms\nstep:57/10000 train_loss:10.3717 train_time:7771ms step_avg:165.35ms\nstep:58/10000 train_loss:10.3555 train_time:7941ms step_avg:165.44ms\nstep:59/10000 train_loss:10.3365 train_time:8109ms step_avg:165.50ms\nstep:60/10000 train_loss:10.3256 train_time:8276ms step_avg:165.53ms\nstep:61/10000 train_loss:10.3095 train_time:8445ms step_avg:165.59ms\nstep:62/10000 train_loss:10.2865 train_time:8611ms step_avg:165.60ms\nstep:63/10000 train_loss:10.2643 train_time:8780ms step_avg:165.65ms\nstep:64/10000 train_loss:10.2437 train_time:8947ms step_avg:165.69ms\nstep:65/10000 train_loss:10.2466 train_time:9115ms step_avg:165.73ms\nstep:66/10000 train_loss:10.2246 train_time:9278ms step_avg:165.69ms\nstep:67/10000 train_loss:10.1781 train_time:9443ms step_avg:165.67ms\nstep:68/10000 train_loss:10.1662 train_time:9612ms step_avg:165.73ms\nstep:69/10000 train_loss:10.1576 train_time:9777ms step_avg:165.72ms\nstep:70/10000 train_loss:10.1373 train_time:9944ms step_avg:165.73ms\nstep:71/10000 train_loss:10.1276 train_time:10112ms step_avg:165.77ms\nstep:72/10000 train_loss:10.1453 train_time:10278ms step_avg:165.77ms\nstep:73/10000 train_loss:10.0778 train_time:10442ms step_avg:165.75ms\nstep:74/10000 train_loss:10.0507 train_time:10610ms step_avg:165.79ms\nstep:75/10000 train_loss:10.0577 train_time:10779ms step_avg:165.83ms\nstep:76/10000 train_loss:10.0162 train_time:10948ms step_avg:165.88ms\nstep:77/10000 train_loss:9.9929 train_time:11115ms step_avg:165.89ms\nstep:78/10000 train_loss:9.9442 train_time:11280ms step_avg:165.89ms\nstep:79/10000 train_loss:9.9599 train_time:11448ms step_avg:165.91ms\nstep:80/10000 train_loss:9.9566 train_time:11615ms step_avg:165.93ms\nstep:81/10000 train_loss:9.9067 train_time:11784ms step_avg:165.97ms\nstep:82/10000 train_loss:9.9186 train_time:11951ms step_avg:165.98ms\nstep:83/10000 train_loss:9.8642 train_time:12118ms step_avg:166.01ms\nstep:84/10000 train_loss:9.8384 train_time:12283ms step_avg:165.99ms\nstep:85/10000 train_loss:9.8524 train_time:12448ms step_avg:165.97ms\nstep:86/10000 train_loss:9.8724 train_time:12615ms step_avg:165.99ms\nstep:87/10000 train_loss:9.8215 train_time:12784ms step_avg:166.02ms\nstep:88/10000 train_loss:9.7628 train_time:12951ms step_avg:166.04ms\nstep:89/10000 train_loss:9.7468 train_time:13119ms step_avg:166.06ms\nstep:90/10000 train_loss:9.7395 train_time:13284ms step_avg:166.04ms\nstep:91/10000 train_loss:9.7169 train_time:13450ms step_avg:166.05ms\nstep:92/10000 train_loss:9.6740 train_time:13616ms step_avg:166.05ms\nstep:93/10000 train_loss:9.6391 train_time:13783ms step_avg:166.06ms\nstep:94/10000 train_loss:9.6638 train_time:13949ms step_avg:166.06ms\nstep:95/10000 train_loss:9.6241 train_time:14115ms step_avg:166.05ms\nstep:96/10000 train_loss:9.5974 train_time:14281ms step_avg:166.05ms\nstep:97/10000 train_loss:9.5609 train_time:14446ms step_avg:166.04ms\nstep:98/10000 train_loss:9.5467 train_time:14613ms step_avg:166.05ms\nstep:99/10000 train_loss:9.5048 train_time:14783ms step_avg:166.11ms\nstep:100/10000 train_loss:9.4662 train_time:14951ms step_avg:166.12ms\nstep:101/10000 train_loss:9.4789 train_time:15119ms step_avg:166.14ms\nstep:102/10000 train_loss:9.4379 train_time:15285ms step_avg:166.14ms\nstep:103/10000 train_loss:9.4368 train_time:15452ms step_avg:166.15ms\nstep:104/10000 train_loss:9.4268 train_time:15621ms step_avg:166.18ms\nstep:105/10000 train_loss:9.3905 train_time:15788ms step_avg:166.19ms\nstep:106/10000 train_loss:9.4114 train_time:15956ms step_avg:166.21ms\nstep:107/10000 train_loss:9.3594 train_time:16123ms step_avg:166.22ms\nstep:108/10000 train_loss:9.3250 train_time:16292ms step_avg:166.25ms\nstep:109/10000 train_loss:9.3342 train_time:16462ms step_avg:166.28ms\nstep:110/10000 train_loss:9.2295 train_time:16629ms step_avg:166.29ms\nstep:111/10000 train_loss:9.2496 train_time:16795ms step_avg:166.29ms\nstep:112/10000 train_loss:9.2576 train_time:16961ms step_avg:166.28ms\nstep:113/10000 train_loss:9.2485 train_time:17126ms step_avg:166.28ms\nstep:114/10000 train_loss:9.2123 train_time:17292ms step_avg:166.27ms\nstep:115/10000 train_loss:9.1947 train_time:17460ms step_avg:166.28ms\nstep:116/10000 train_loss:9.1418 train_time:17629ms step_avg:166.31ms\nstep:117/10000 train_loss:9.1199 train_time:17797ms step_avg:166.33ms\nstep:118/10000 train_loss:9.1061 train_time:17964ms step_avg:166.33ms\nstep:119/10000 train_loss:9.1124 train_time:18128ms step_avg:166.31ms\nstep:120/10000 train_loss:9.0947 train_time:18294ms step_avg:166.31ms\nstep:121/10000 train_loss:9.0352 train_time:18462ms step_avg:166.33ms\nstep:122/10000 train_loss:8.9998 train_time:18630ms step_avg:166.34ms\nstep:123/10000 train_loss:8.9820 train_time:18797ms step_avg:166.34ms\nstep:124/10000 train_loss:9.1255 train_time:18964ms step_avg:166.35ms\nstep:125/10000 train_loss:8.9571 train_time:19128ms step_avg:166.33ms\nstep:126/10000 train_loss:8.8639 train_time:19297ms step_avg:166.36ms\nstep:127/10000 train_loss:8.9042 train_time:19463ms step_avg:166.35ms\nstep:128/10000 train_loss:8.8907 train_time:19628ms step_avg:166.34ms\nstep:129/10000 train_loss:8.9119 train_time:19795ms step_avg:166.34ms\nstep:130/10000 train_loss:8.9511 train_time:19962ms step_avg:166.35ms\nstep:131/10000 train_loss:8.8363 train_time:20128ms step_avg:166.35ms\nstep:132/10000 train_loss:8.7903 train_time:20294ms step_avg:166.34ms\nstep:133/10000 train_loss:8.7930 train_time:20460ms step_avg:166.34ms\nstep:134/10000 train_loss:8.7665 train_time:20626ms step_avg:166.34ms\nstep:135/10000 train_loss:8.7693 train_time:20795ms step_avg:166.36ms\nstep:136/10000 train_loss:8.7126 train_time:20961ms step_avg:166.36ms\nstep:137/10000 train_loss:8.6994 train_time:21127ms step_avg:166.36ms\nstep:138/10000 train_loss:8.7590 train_time:21295ms step_avg:166.36ms\nstep:139/10000 train_loss:8.6623 train_time:21459ms step_avg:166.35ms\nstep:140/10000 train_loss:8.6985 train_time:21628ms step_avg:166.37ms\nstep:141/10000 train_loss:8.6165 train_time:21795ms step_avg:166.38ms\nstep:142/10000 train_loss:8.6876 train_time:21961ms step_avg:166.37ms\nstep:143/10000 train_loss:8.5715 train_time:22125ms step_avg:166.36ms\nstep:144/10000 train_loss:8.6398 train_time:22292ms step_avg:166.36ms\nstep:145/10000 train_loss:8.5854 train_time:22461ms step_avg:166.38ms\nstep:146/10000 train_loss:8.5394 train_time:22629ms step_avg:166.39ms\nstep:147/10000 train_loss:8.4559 train_time:22798ms step_avg:166.41ms\nstep:148/10000 train_loss:8.5206 train_time:22965ms step_avg:166.41ms\nstep:149/10000 train_loss:8.3594 train_time:23134ms step_avg:166.43ms\nstep:150/10000 train_loss:8.4247 train_time:23301ms step_avg:166.44ms\nstep:151/10000 train_loss:8.4716 train_time:23471ms step_avg:166.46ms\nstep:152/10000 train_loss:8.4228 train_time:23639ms step_avg:166.47ms\nstep:153/10000 train_loss:8.5074 train_time:23809ms step_avg:166.50ms\nstep:154/10000 train_loss:8.4176 train_time:23972ms step_avg:166.47ms\nstep:155/10000 train_loss:8.3024 train_time:24141ms step_avg:166.49ms\nstep:156/10000 train_loss:8.3885 train_time:24306ms step_avg:166.48ms\nstep:157/10000 train_loss:8.3717 train_time:24474ms step_avg:166.49ms\nstep:158/10000 train_loss:8.2864 train_time:24645ms step_avg:166.52ms\nstep:159/10000 train_loss:8.3146 train_time:24813ms step_avg:166.53ms\nstep:160/10000 train_loss:8.2299 train_time:24981ms step_avg:166.54ms\nstep:161/10000 train_loss:8.3074 train_time:25149ms step_avg:166.55ms\nstep:162/10000 train_loss:8.2358 train_time:25317ms step_avg:166.56ms\nstep:163/10000 train_loss:8.1615 train_time:25483ms step_avg:166.55ms\nstep:164/10000 train_loss:8.2305 train_time:25652ms step_avg:166.57ms\nstep:165/10000 train_loss:8.1453 train_time:25820ms step_avg:166.58ms\nstep:166/10000 train_loss:8.1063 train_time:25985ms step_avg:166.57ms\nstep:167/10000 train_loss:8.0659 train_time:26155ms step_avg:166.59ms\nstep:168/10000 train_loss:8.1540 train_time:26326ms step_avg:166.62ms\nstep:169/10000 train_loss:8.2223 train_time:26496ms step_avg:166.64ms\nstep:170/10000 train_loss:8.0874 train_time:26664ms step_avg:166.65ms\nstep:171/10000 train_loss:8.2121 train_time:26833ms step_avg:166.66ms\nstep:172/10000 train_loss:8.0946 train_time:27000ms step_avg:166.66ms\nstep:173/10000 train_loss:8.0925 train_time:27168ms step_avg:166.67ms\nstep:174/10000 train_loss:8.0526 train_time:27337ms step_avg:166.69ms\nstep:175/10000 train_loss:8.0023 train_time:27503ms step_avg:166.69ms\nstep:176/10000 train_loss:8.0202 train_time:27671ms step_avg:166.69ms\nstep:177/10000 train_loss:7.9986 train_time:27840ms step_avg:166.70ms\nstep:178/10000 train_loss:8.1132 train_time:28006ms step_avg:166.70ms\nstep:179/10000 train_loss:8.0905 train_time:28171ms step_avg:166.69ms\nstep:180/10000 train_loss:7.9346 train_time:28338ms step_avg:166.69ms\nstep:181/10000 train_loss:7.9026 train_time:28505ms step_avg:166.70ms\nstep:182/10000 train_loss:7.9845 train_time:28674ms step_avg:166.71ms\nstep:183/10000 train_loss:7.9359 train_time:28845ms step_avg:166.73ms\nstep:184/10000 train_loss:7.9084 train_time:29012ms step_avg:166.74ms\nstep:185/10000 train_loss:7.9590 train_time:29182ms step_avg:166.75ms\nstep:186/10000 train_loss:7.8302 train_time:29352ms step_avg:166.77ms\nstep:187/10000 train_loss:8.0111 train_time:29520ms step_avg:166.78ms\nstep:188/10000 train_loss:7.8398 train_time:29686ms step_avg:166.78ms\nstep:189/10000 train_loss:7.9427 train_time:29856ms step_avg:166.79ms\nstep:190/10000 train_loss:7.9542 train_time:30023ms step_avg:166.80ms\nstep:191/10000 train_loss:7.8261 train_time:30190ms step_avg:166.80ms\nstep:192/10000 train_loss:7.9067 train_time:30359ms step_avg:166.81ms\nstep:193/10000 train_loss:7.7897 train_time:30528ms step_avg:166.82ms\nstep:194/10000 train_loss:7.8232 train_time:30703ms step_avg:166.87ms\nstep:195/10000 train_loss:7.8049 train_time:30864ms step_avg:166.83ms\nstep:196/10000 train_loss:7.6681 train_time:31033ms step_avg:166.84ms\nstep:197/10000 train_loss:7.7980 train_time:31203ms step_avg:166.86ms\nstep:198/10000 train_loss:7.8840 train_time:31372ms step_avg:166.87ms\nstep:199/10000 train_loss:7.7462 train_time:31539ms step_avg:166.87ms\nstep:200/10000 train_loss:7.7686 train_time:31706ms step_avg:166.87ms\nstep:201/10000 train_loss:7.7451 train_time:31874ms step_avg:166.88ms\nstep:202/10000 train_loss:7.7666 train_time:32044ms step_avg:166.90ms\nstep:203/10000 train_loss:7.4963 train_time:32212ms step_avg:166.90ms\nstep:204/10000 train_loss:7.8131 train_time:32382ms step_avg:166.92ms\nstep:205/10000 train_loss:7.7289 train_time:32550ms step_avg:166.93ms\nstep:206/10000 train_loss:7.6306 train_time:32718ms step_avg:166.93ms\nstep:207/10000 train_loss:7.6296 train_time:32886ms step_avg:166.94ms\nstep:208/10000 train_loss:7.7572 train_time:33053ms step_avg:166.93ms\nstep:209/10000 train_loss:7.7191 train_time:33221ms step_avg:166.94ms\nstep:210/10000 train_loss:7.7078 train_time:33389ms step_avg:166.95ms\nstep:211/10000 train_loss:7.7047 train_time:33559ms step_avg:166.96ms\nstep:212/10000 train_loss:7.6963 train_time:33726ms step_avg:166.96ms\nstep:213/10000 train_loss:7.8143 train_time:33894ms step_avg:166.97ms\nstep:214/10000 train_loss:7.6375 train_time:34063ms step_avg:166.97ms\nstep:215/10000 train_loss:7.6354 train_time:34229ms step_avg:166.97ms\nstep:216/10000 train_loss:7.6793 train_time:34397ms step_avg:166.98ms\nstep:217/10000 train_loss:7.5229 train_time:34566ms step_avg:166.99ms\nstep:218/10000 train_loss:7.8361 train_time:34734ms step_avg:166.99ms\nstep:219/10000 train_loss:7.7099 train_time:34902ms step_avg:166.99ms\nstep:220/10000 train_loss:7.6065 train_time:35070ms step_avg:167.00ms\nstep:221/10000 train_loss:7.6132 train_time:35239ms step_avg:167.01ms\nstep:222/10000 train_loss:7.6160 train_time:35413ms step_avg:167.04ms\nstep:223/10000 train_loss:7.5672 train_time:35585ms step_avg:167.06ms\nstep:224/10000 train_loss:7.5580 train_time:35753ms step_avg:167.07ms\nstep:225/10000 train_loss:7.6061 train_time:35920ms step_avg:167.07ms\nstep:226/10000 train_loss:7.5200 train_time:36085ms step_avg:167.06ms\nstep:227/10000 train_loss:7.6737 train_time:36254ms step_avg:167.07ms\nstep:228/10000 train_loss:7.5745 train_time:36424ms step_avg:167.08ms\nstep:229/10000 train_loss:7.6067 train_time:36593ms step_avg:167.09ms\nstep:230/10000 train_loss:7.6394 train_time:36762ms step_avg:167.10ms\nstep:231/10000 train_loss:7.7118 train_time:36931ms step_avg:167.11ms\nstep:232/10000 train_loss:7.5725 train_time:37101ms step_avg:167.12ms\nstep:233/10000 train_loss:7.5057 train_time:37270ms step_avg:167.13ms\nstep:234/10000 train_loss:7.5424 train_time:37440ms step_avg:167.14ms\nstep:235/10000 train_loss:7.5780 train_time:37610ms step_avg:167.16ms\nstep:236/10000 train_loss:7.6337 train_time:37778ms step_avg:167.16ms\nstep:237/10000 train_loss:7.6051 train_time:37944ms step_avg:167.16ms\nstep:238/10000 train_loss:7.5887 train_time:38114ms step_avg:167.17ms\nstep:239/10000 train_loss:7.6406 train_time:38282ms step_avg:167.17ms\nstep:240/10000 train_loss:7.5015 train_time:38448ms step_avg:167.17ms\nstep:241/10000 train_loss:7.6204 train_time:38615ms step_avg:167.17ms\nstep:242/10000 train_loss:7.4702 train_time:38786ms step_avg:167.18ms\nstep:243/10000 train_loss:7.4723 train_time:38956ms step_avg:167.19ms\nstep:244/10000 train_loss:7.5216 train_time:39125ms step_avg:167.20ms\nstep:245/10000 train_loss:7.5417 train_time:39293ms step_avg:167.20ms\nstep:246/10000 train_loss:7.5334 train_time:39461ms step_avg:167.21ms\nstep:247/10000 train_loss:7.4484 train_time:39629ms step_avg:167.21ms\nstep:248/10000 train_loss:7.5540 train_time:39798ms step_avg:167.22ms\nstep:249/10000 train_loss:7.6389 train_time:39967ms step_avg:167.23ms\nstep:250/10000 train_loss:7.4639 train_time:40139ms step_avg:167.25ms\nstep:251/10000 train_loss:7.4686 train_time:40312ms step_avg:167.27ms\nstep:252/10000 train_loss:7.4763 train_time:40476ms step_avg:167.26ms\nstep:253/10000 train_loss:7.5593 train_time:40642ms step_avg:167.25ms\nstep:254/10000 train_loss:8.2568 train_time:40813ms step_avg:167.27ms\nstep:255/10000 train_loss:7.4853 train_time:40983ms step_avg:167.28ms\nstep:256/10000 train_loss:7.4326 train_time:41152ms step_avg:167.28ms\nstep:257/10000 train_loss:7.5519 train_time:41321ms step_avg:167.29ms\nstep:258/10000 train_loss:7.3595 train_time:41492ms step_avg:167.30ms\nstep:259/10000 train_loss:7.5035 train_time:41661ms step_avg:167.31ms\nstep:260/10000 train_loss:7.2991 train_time:41829ms step_avg:167.32ms\nstep:261/10000 train_loss:7.4638 train_time:41998ms step_avg:167.32ms\nstep:262/10000 train_loss:7.4112 train_time:42168ms step_avg:167.33ms\nstep:263/10000 train_loss:7.3786 train_time:42337ms step_avg:167.34ms\nstep:264/10000 train_loss:7.2964 train_time:42506ms step_avg:167.35ms\nstep:265/10000 train_loss:7.4579 train_time:42673ms step_avg:167.34ms\nstep:266/10000 train_loss:7.3409 train_time:42844ms step_avg:167.36ms\nstep:267/10000 train_loss:7.3692 train_time:43013ms step_avg:167.37ms\nstep:268/10000 train_loss:7.4883 train_time:43183ms step_avg:167.38ms\nstep:269/10000 train_loss:7.4737 train_time:43354ms step_avg:167.39ms\nstep:270/10000 train_loss:7.5778 train_time:43524ms step_avg:167.40ms\nstep:271/10000 train_loss:7.2181 train_time:43693ms step_avg:167.41ms\nstep:272/10000 train_loss:7.4487 train_time:43863ms step_avg:167.42ms\nstep:273/10000 train_loss:7.5158 train_time:44031ms step_avg:167.42ms\nstep:274/10000 train_loss:7.2445 train_time:44204ms step_avg:167.44ms\nstep:275/10000 train_loss:7.2852 train_time:44375ms step_avg:167.45ms\nstep:276/10000 train_loss:7.4682 train_time:44545ms step_avg:167.46ms\nstep:277/10000 train_loss:7.3742 train_time:44714ms step_avg:167.47ms\nstep:278/10000 train_loss:7.1799 train_time:44885ms step_avg:167.48ms\nstep:279/10000 train_loss:7.3561 train_time:45053ms step_avg:167.48ms\nstep:280/10000 train_loss:7.2933 train_time:45225ms step_avg:167.50ms\nstep:281/10000 train_loss:7.4282 train_time:45393ms step_avg:167.50ms\nstep:282/10000 train_loss:7.4696 train_time:45563ms step_avg:167.51ms\nstep:283/10000 train_loss:7.2973 train_time:45733ms step_avg:167.52ms\nstep:284/10000 train_loss:7.4449 train_time:45905ms step_avg:167.54ms\nstep:285/10000 train_loss:7.5393 train_time:46074ms step_avg:167.54ms\nstep:286/10000 train_loss:7.3573 train_time:46245ms step_avg:167.56ms\nstep:287/10000 train_loss:7.4436 train_time:46415ms step_avg:167.56ms\nstep:288/10000 train_loss:7.3172 train_time:46586ms step_avg:167.58ms\nstep:289/10000 train_loss:7.2529 train_time:46756ms step_avg:167.58ms\nstep:290/10000 train_loss:7.2787 train_time:46925ms step_avg:167.59ms\nstep:291/10000 train_loss:7.3820 train_time:47096ms step_avg:167.60ms\nstep:292/10000 train_loss:7.3582 train_time:47265ms step_avg:167.61ms\nstep:293/10000 train_loss:7.3113 train_time:47432ms step_avg:167.60ms\nstep:294/10000 train_loss:7.3102 train_time:47601ms step_avg:167.61ms\nstep:295/10000 train_loss:7.1259 train_time:47771ms step_avg:167.62ms\nstep:296/10000 train_loss:7.1119 train_time:47941ms step_avg:167.63ms\nstep:297/10000 train_loss:7.3042 train_time:48109ms step_avg:167.63ms\nstep:298/10000 train_loss:7.3442 train_time:48282ms step_avg:167.64ms\nstep:299/10000 train_loss:7.1793 train_time:48449ms step_avg:167.64ms\nstep:300/10000 train_loss:7.1981 train_time:48619ms step_avg:167.65ms\nstep:301/10000 train_loss:7.2623 train_time:48787ms step_avg:167.65ms\nstep:302/10000 train_loss:7.2474 train_time:48959ms step_avg:167.67ms\nstep:303/10000 train_loss:7.0725 train_time:49128ms step_avg:167.67ms\nstep:304/10000 train_loss:7.1548 train_time:49298ms step_avg:167.68ms\nstep:305/10000 train_loss:7.2084 train_time:49468ms step_avg:167.69ms\nstep:306/10000 train_loss:7.2803 train_time:49639ms step_avg:167.70ms\nstep:307/10000 train_loss:7.2788 train_time:49808ms step_avg:167.70ms\nstep:308/10000 train_loss:7.3000 train_time:49980ms step_avg:167.72ms\nstep:309/10000 train_loss:7.1843 train_time:50148ms step_avg:167.72ms\nstep:310/10000 train_loss:7.2388 train_time:50318ms step_avg:167.73ms\nstep:311/10000 train_loss:7.2047 train_time:50488ms step_avg:167.73ms\nstep:312/10000 train_loss:7.1404 train_time:50658ms step_avg:167.74ms\nstep:313/10000 train_loss:7.2225 train_time:50828ms step_avg:167.75ms\nstep:314/10000 train_loss:7.2768 train_time:50998ms step_avg:167.76ms\nstep:315/10000 train_loss:7.0549 train_time:51165ms step_avg:167.75ms\nstep:316/10000 train_loss:6.8123 train_time:51337ms step_avg:167.77ms\nstep:317/10000 train_loss:7.1432 train_time:51506ms step_avg:167.77ms\nstep:318/10000 train_loss:7.2466 train_time:51676ms step_avg:167.78ms\nstep:319/10000 train_loss:7.0085 train_time:51846ms step_avg:167.79ms\nstep:320/10000 train_loss:7.1264 train_time:52016ms step_avg:167.80ms\nstep:321/10000 train_loss:7.1333 train_time:52186ms step_avg:167.80ms\nstep:322/10000 train_loss:7.0246 train_time:52357ms step_avg:167.81ms\nstep:323/10000 train_loss:7.1226 train_time:52528ms step_avg:167.82ms\nstep:324/10000 train_loss:7.2396 train_time:52699ms step_avg:167.83ms\nstep:325/10000 train_loss:7.1082 train_time:52869ms step_avg:167.84ms\nstep:326/10000 train_loss:7.1739 train_time:53040ms step_avg:167.85ms\nstep:327/10000 train_loss:7.1258 train_time:53208ms step_avg:167.85ms\nstep:328/10000 train_loss:7.0837 train_time:53378ms step_avg:167.85ms\nstep:329/10000 train_loss:7.3934 train_time:53546ms step_avg:167.86ms\nstep:330/10000 train_loss:7.1682 train_time:53717ms step_avg:167.86ms\nstep:331/10000 train_loss:6.9654 train_time:53884ms step_avg:167.86ms\nstep:332/10000 train_loss:7.1457 train_time:54055ms step_avg:167.87ms\nstep:333/10000 train_loss:7.1964 train_time:54227ms step_avg:167.89ms\nstep:334/10000 train_loss:7.0287 train_time:54399ms step_avg:167.90ms\nstep:335/10000 train_loss:7.0399 train_time:54568ms step_avg:167.90ms\nstep:336/10000 train_loss:7.0303 train_time:54739ms step_avg:167.91ms\nstep:337/10000 train_loss:7.1525 train_time:54908ms step_avg:167.91ms\nstep:338/10000 train_loss:6.9022 train_time:55076ms step_avg:167.92ms\nstep:339/10000 train_loss:6.9481 train_time:55245ms step_avg:167.92ms\nstep:340/10000 train_loss:7.1018 train_time:55416ms step_avg:167.93ms\nstep:341/10000 train_loss:7.0443 train_time:55586ms step_avg:167.93ms\nstep:342/10000 train_loss:7.0824 train_time:55757ms step_avg:167.94ms\nstep:343/10000 train_loss:7.0366 train_time:55927ms step_avg:167.95ms\nstep:344/10000 train_loss:7.0169 train_time:56098ms step_avg:167.96ms\nstep:345/10000 train_loss:6.9989 train_time:56267ms step_avg:167.96ms\nstep:346/10000 train_loss:7.0127 train_time:56440ms step_avg:167.98ms\nstep:347/10000 train_loss:7.0184 train_time:56610ms step_avg:167.98ms\nstep:348/10000 train_loss:7.0587 train_time:56781ms step_avg:167.99ms\nstep:349/10000 train_loss:6.9592 train_time:56951ms step_avg:168.00ms\nstep:350/10000 train_loss:7.1499 train_time:57121ms step_avg:168.00ms\nstep:351/10000 train_loss:6.9058 train_time:57290ms step_avg:168.01ms\nstep:352/10000 train_loss:6.9129 train_time:57462ms step_avg:168.02ms\nstep:353/10000 train_loss:6.8182 train_time:57632ms step_avg:168.02ms\nstep:354/10000 train_loss:7.0655 train_time:57803ms step_avg:168.03ms\nstep:355/10000 train_loss:6.9352 train_time:57971ms step_avg:168.03ms\nstep:356/10000 train_loss:6.8937 train_time:58142ms step_avg:168.04ms\nstep:357/10000 train_loss:6.6851 train_time:58310ms step_avg:168.04ms\nstep:358/10000 train_loss:6.9790 train_time:58480ms step_avg:168.05ms\nstep:359/10000 train_loss:7.0287 train_time:58648ms step_avg:168.05ms\nstep:360/10000 train_loss:6.7616 train_time:58821ms step_avg:168.06ms\nstep:361/10000 train_loss:6.8191 train_time:58988ms step_avg:168.06ms\nstep:362/10000 train_loss:6.9359 train_time:59160ms step_avg:168.07ms\nstep:363/10000 train_loss:6.8775 train_time:59330ms step_avg:168.07ms\nstep:364/10000 train_loss:6.9500 train_time:59501ms step_avg:168.08ms\nstep:365/10000 train_loss:6.9134 train_time:59669ms step_avg:168.08ms\nstep:366/10000 train_loss:7.0560 train_time:59840ms step_avg:168.09ms\nstep:367/10000 train_loss:7.1302 train_time:60011ms step_avg:168.10ms\nstep:368/10000 train_loss:6.8704 train_time:60179ms step_avg:168.10ms\nstep:369/10000 train_loss:7.0689 train_time:60348ms step_avg:168.10ms\nstep:370/10000 train_loss:6.8913 train_time:60519ms step_avg:168.11ms\nstep:371/10000 train_loss:6.9620 train_time:60688ms step_avg:168.11ms\nstep:372/10000 train_loss:7.1501 train_time:60859ms step_avg:168.12ms\nstep:373/10000 train_loss:6.8472 train_time:61029ms step_avg:168.12ms\nstep:374/10000 train_loss:6.8587 train_time:61200ms step_avg:168.13ms\nstep:375/10000 train_loss:6.8438 train_time:61372ms step_avg:168.14ms\nstep:376/10000 train_loss:6.9669 train_time:61540ms step_avg:168.14ms\nstep:377/10000 train_loss:6.7645 train_time:61710ms step_avg:168.15ms\nstep:378/10000 train_loss:6.8635 train_time:61881ms step_avg:168.16ms\nstep:379/10000 train_loss:6.8679 train_time:62049ms step_avg:168.15ms\nstep:380/10000 train_loss:6.7290 train_time:62218ms step_avg:168.16ms\nstep:381/10000 train_loss:6.9695 train_time:62387ms step_avg:168.16ms\nstep:382/10000 train_loss:6.7211 train_time:62560ms step_avg:168.17ms\nstep:383/10000 train_loss:6.9533 train_time:62728ms step_avg:168.17ms\nstep:384/10000 train_loss:6.8411 train_time:62898ms step_avg:168.18ms\nstep:385/10000 train_loss:6.7023 train_time:63068ms step_avg:168.18ms\nstep:386/10000 train_loss:6.8399 train_time:63239ms step_avg:168.19ms\nstep:387/10000 train_loss:6.7791 train_time:63408ms step_avg:168.19ms\nstep:388/10000 train_loss:6.8876 train_time:63578ms step_avg:168.20ms\nstep:389/10000 train_loss:6.7788 train_time:63751ms step_avg:168.21ms\nstep:390/10000 train_loss:6.6868 train_time:63922ms step_avg:168.22ms\nstep:391/10000 train_loss:6.8651 train_time:64090ms step_avg:168.22ms\nstep:392/10000 train_loss:6.6932 train_time:64262ms step_avg:168.23ms\nstep:393/10000 train_loss:6.7410 train_time:64432ms step_avg:168.23ms\nstep:394/10000 train_loss:6.8726 train_time:64602ms step_avg:168.23ms\nstep:395/10000 train_loss:6.6526 train_time:64773ms step_avg:168.24ms\nstep:396/10000 train_loss:6.6615 train_time:64943ms step_avg:168.25ms\nstep:397/10000 train_loss:6.8125 train_time:65113ms step_avg:168.25ms\nstep:398/10000 train_loss:6.8048 train_time:65282ms step_avg:168.25ms\nstep:399/10000 train_loss:6.6620 train_time:65452ms step_avg:168.26ms\nstep:400/10000 train_loss:6.6425 train_time:65625ms step_avg:168.27ms\nstep:401/10000 train_loss:6.5177 train_time:65795ms step_avg:168.27ms\nstep:402/10000 train_loss:6.6970 train_time:65966ms step_avg:168.28ms\nstep:403/10000 train_loss:6.9328 train_time:66142ms step_avg:168.30ms\nstep:404/10000 train_loss:6.5564 train_time:66306ms step_avg:168.29ms\nstep:405/10000 train_loss:6.7998 train_time:66477ms step_avg:168.30ms\nstep:406/10000 train_loss:6.7690 train_time:66648ms step_avg:168.30ms\nstep:407/10000 train_loss:6.7258 train_time:66819ms step_avg:168.31ms\nstep:408/10000 train_loss:6.6874 train_time:66986ms step_avg:168.31ms\nstep:409/10000 train_loss:6.8648 train_time:67155ms step_avg:168.31ms\nstep:410/10000 train_loss:6.8110 train_time:67324ms step_avg:168.31ms\nstep:411/10000 train_loss:6.7880 train_time:67495ms step_avg:168.32ms\nstep:412/10000 train_loss:6.5079 train_time:67664ms step_avg:168.32ms\nstep:413/10000 train_loss:6.7214 train_time:67837ms step_avg:168.33ms\nstep:414/10000 train_loss:6.6028 train_time:68006ms step_avg:168.33ms\nstep:415/10000 train_loss:7.2109 train_time:68176ms step_avg:168.34ms\nstep:416/10000 train_loss:6.6984 train_time:68345ms step_avg:168.34ms\nstep:417/10000 train_loss:6.4837 train_time:68515ms step_avg:168.34ms\nstep:418/10000 train_loss:6.7472 train_time:68685ms step_avg:168.35ms\nstep:419/10000 train_loss:6.7550 train_time:68860ms step_avg:168.36ms\nstep:420/10000 train_loss:6.7296 train_time:69027ms step_avg:168.36ms\nstep:421/10000 train_loss:6.5965 train_time:69197ms step_avg:168.36ms\nstep:422/10000 train_loss:6.9654 train_time:69366ms step_avg:168.36ms\nstep:423/10000 train_loss:6.8252 train_time:69538ms step_avg:168.37ms\nstep:424/10000 train_loss:6.8711 train_time:69705ms step_avg:168.37ms\nstep:425/10000 train_loss:6.6871 train_time:69878ms step_avg:168.38ms\nstep:426/10000 train_loss:6.6987 train_time:70048ms step_avg:168.39ms\nstep:427/10000 train_loss:6.8529 train_time:70220ms step_avg:168.39ms\nstep:428/10000 train_loss:6.6170 train_time:70389ms step_avg:168.39ms\nstep:429/10000 train_loss:6.6915 train_time:70560ms step_avg:168.40ms\nstep:430/10000 train_loss:6.3602 train_time:70730ms step_avg:168.41ms\nstep:431/10000 train_loss:6.5525 train_time:70900ms step_avg:168.41ms\nstep:432/10000 train_loss:6.5320 train_time:71070ms step_avg:168.41ms\nstep:433/10000 train_loss:6.6896 train_time:71241ms step_avg:168.42ms\nstep:434/10000 train_loss:6.6525 train_time:71410ms step_avg:168.42ms\nstep:435/10000 train_loss:6.5476 train_time:71581ms step_avg:168.43ms\nstep:436/10000 train_loss:6.7381 train_time:71752ms step_avg:168.43ms\nstep:437/10000 train_loss:6.5958 train_time:71922ms step_avg:168.44ms\nstep:438/10000 train_loss:6.6320 train_time:72091ms step_avg:168.44ms\nstep:439/10000 train_loss:6.6065 train_time:72264ms step_avg:168.45ms\nstep:440/10000 train_loss:6.7156 train_time:72433ms step_avg:168.45ms\nstep:441/10000 train_loss:6.7151 train_time:72601ms step_avg:168.45ms\nstep:442/10000 train_loss:6.6319 train_time:72771ms step_avg:168.45ms\nstep:443/10000 train_loss:6.5641 train_time:72942ms step_avg:168.46ms\nstep:444/10000 train_loss:6.5882 train_time:73114ms step_avg:168.47ms\nstep:445/10000 train_loss:6.6348 train_time:73286ms step_avg:168.47ms\nstep:446/10000 train_loss:6.6311 train_time:73455ms step_avg:168.48ms\nstep:447/10000 train_loss:6.7024 train_time:73622ms step_avg:168.47ms\nstep:448/10000 train_loss:6.5984 train_time:73790ms step_avg:168.47ms\nstep:449/10000 train_loss:6.6501 train_time:73965ms step_avg:168.48ms\nstep:450/10000 train_loss:6.6197 train_time:74137ms step_avg:168.49ms\nstep:451/10000 train_loss:6.6539 train_time:74307ms step_avg:168.50ms\nstep:452/10000 train_loss:6.3810 train_time:74478ms step_avg:168.50ms\nstep:453/10000 train_loss:6.7334 train_time:74648ms step_avg:168.51ms\nstep:454/10000 train_loss:6.4230 train_time:74818ms step_avg:168.51ms\nstep:455/10000 train_loss:6.5764 train_time:74986ms step_avg:168.51ms\nstep:456/10000 train_loss:6.6508 train_time:75160ms step_avg:168.52ms\nstep:457/10000 train_loss:6.5255 train_time:75327ms step_avg:168.52ms\nstep:458/10000 train_loss:6.6755 train_time:75498ms step_avg:168.52ms\nstep:459/10000 train_loss:6.6873 train_time:75667ms step_avg:168.52ms\nstep:460/10000 train_loss:6.5881 train_time:75838ms step_avg:168.53ms\nstep:461/10000 train_loss:6.5405 train_time:76009ms step_avg:168.53ms\nstep:462/10000 train_loss:6.4653 train_time:76180ms step_avg:168.54ms\nstep:463/10000 train_loss:6.6149 train_time:76350ms step_avg:168.54ms\nstep:464/10000 train_loss:6.7045 train_time:76520ms step_avg:168.55ms\nstep:465/10000 train_loss:6.6119 train_time:76689ms step_avg:168.55ms\nstep:466/10000 train_loss:6.4772 train_time:76860ms step_avg:168.55ms\nstep:467/10000 train_loss:6.6446 train_time:77028ms step_avg:168.55ms\nstep:468/10000 train_loss:6.4902 train_time:77200ms step_avg:168.56ms\nstep:469/10000 train_loss:6.5290 train_time:77370ms step_avg:168.56ms\nstep:470/10000 train_loss:6.4978 train_time:77542ms step_avg:168.57ms\nstep:471/10000 train_loss:6.6125 train_time:77711ms step_avg:168.57ms\nstep:472/10000 train_loss:6.5048 train_time:77884ms step_avg:168.58ms\nstep:473/10000 train_loss:6.3988 train_time:78055ms step_avg:168.59ms\nstep:474/10000 train_loss:6.6073 train_time:78227ms step_avg:168.59ms\nstep:475/10000 train_loss:6.5602 train_time:78397ms step_avg:168.60ms\nstep:476/10000 train_loss:6.5770 train_time:78568ms step_avg:168.60ms\nstep:477/10000 train_loss:6.6216 train_time:78739ms step_avg:168.61ms\nstep:478/10000 train_loss:6.5364 train_time:78907ms step_avg:168.60ms\nstep:479/10000 train_loss:6.4828 train_time:79085ms step_avg:168.62ms\nstep:480/10000 train_loss:6.6127 train_time:79247ms step_avg:168.61ms\nstep:481/10000 train_loss:6.4573 train_time:79418ms step_avg:168.62ms\nstep:482/10000 train_loss:6.4123 train_time:79587ms step_avg:168.62ms\nstep:483/10000 train_loss:6.4896 train_time:79755ms step_avg:168.62ms\nstep:484/10000 train_loss:6.5274 train_time:79923ms step_avg:168.61ms\nstep:485/10000 train_loss:6.5514 train_time:80094ms step_avg:168.62ms\nstep:486/10000 train_loss:6.4261 train_time:80266ms step_avg:168.63ms\nstep:487/10000 train_loss:6.4290 train_time:80437ms step_avg:168.63ms\nstep:488/10000 train_loss:6.4388 train_time:80608ms step_avg:168.64ms\nstep:489/10000 train_loss:6.3047 train_time:80774ms step_avg:168.63ms\nstep:490/10000 train_loss:6.3258 train_time:80942ms step_avg:168.63ms\nstep:491/10000 train_loss:6.4424 train_time:81113ms step_avg:168.63ms\nstep:492/10000 train_loss:6.5604 train_time:81285ms step_avg:168.64ms\nstep:493/10000 train_loss:6.5491 train_time:81457ms step_avg:168.65ms\nstep:494/10000 train_loss:6.6433 train_time:81626ms step_avg:168.65ms\nstep:495/10000 train_loss:6.3473 train_time:81795ms step_avg:168.65ms\nstep:496/10000 train_loss:6.4277 train_time:81965ms step_avg:168.65ms\nstep:497/10000 train_loss:6.4566 train_time:82134ms step_avg:168.65ms\nstep:498/10000 train_loss:6.4104 train_time:82306ms step_avg:168.66ms\nstep:499/10000 train_loss:6.4683 train_time:82480ms step_avg:168.67ms\nstep:500/10000 train_loss:6.2488 train_time:82647ms step_avg:168.67ms\nstep:501/10000 train_loss:6.3707 train_time:82818ms step_avg:168.67ms\nstep:502/10000 train_loss:6.4739 train_time:82993ms step_avg:168.69ms\nstep:503/10000 train_loss:6.3393 train_time:83166ms step_avg:168.69ms\nstep:504/10000 train_loss:6.3795 train_time:83337ms step_avg:168.70ms\nstep:505/10000 train_loss:6.4447 train_time:83511ms step_avg:168.71ms\nstep:506/10000 train_loss:6.5727 train_time:83680ms step_avg:168.71ms\nstep:507/10000 train_loss:6.6073 train_time:83851ms step_avg:168.72ms\nstep:508/10000 train_loss:6.4582 train_time:84022ms step_avg:168.72ms\nstep:509/10000 train_loss:6.5986 train_time:84193ms step_avg:168.72ms\nstep:510/10000 train_loss:6.3719 train_time:84362ms step_avg:168.72ms\nstep:511/10000 train_loss:6.3884 train_time:84536ms step_avg:168.73ms\nstep:512/10000 train_loss:6.4900 train_time:84704ms step_avg:168.73ms\nstep:513/10000 train_loss:6.4351 train_time:84872ms step_avg:168.73ms\nstep:514/10000 train_loss:6.5960 train_time:85039ms step_avg:168.73ms\nstep:515/10000 train_loss:6.3844 train_time:85208ms step_avg:168.73ms\nstep:516/10000 train_loss:6.4561 train_time:85380ms step_avg:168.73ms\nstep:517/10000 train_loss:6.3068 train_time:85548ms step_avg:168.73ms\nstep:518/10000 train_loss:6.3999 train_time:85718ms step_avg:168.74ms\nstep:519/10000 train_loss:6.2037 train_time:85887ms step_avg:168.74ms\nstep:520/10000 train_loss:6.5331 train_time:86058ms step_avg:168.74ms\nstep:521/10000 train_loss:6.2795 train_time:86227ms step_avg:168.74ms\nstep:522/10000 train_loss:6.5101 train_time:86399ms step_avg:168.75ms\nstep:523/10000 train_loss:6.3356 train_time:86568ms step_avg:168.75ms\nstep:524/10000 train_loss:6.5665 train_time:86744ms step_avg:168.76ms\nstep:525/10000 train_loss:6.7174 train_time:86908ms step_avg:168.75ms\nstep:526/10000 train_loss:6.3513 train_time:87078ms step_avg:168.76ms\nstep:527/10000 train_loss:6.3516 train_time:87247ms step_avg:168.76ms\nstep:528/10000 train_loss:6.3043 train_time:87420ms step_avg:168.76ms\nstep:529/10000 train_loss:6.3346 train_time:87590ms step_avg:168.77ms\nstep:530/10000 train_loss:6.3758 train_time:87761ms step_avg:168.77ms\nstep:531/10000 train_loss:6.3629 train_time:87931ms step_avg:168.77ms\nstep:532/10000 train_loss:6.2835 train_time:88102ms step_avg:168.78ms\nstep:533/10000 train_loss:6.3251 train_time:88270ms step_avg:168.78ms\nstep:534/10000 train_loss:6.3709 train_time:88441ms step_avg:168.78ms\nstep:535/10000 train_loss:6.3214 train_time:88611ms step_avg:168.78ms\nstep:536/10000 train_loss:6.3444 train_time:88782ms step_avg:168.79ms\nstep:537/10000 train_loss:6.4914 train_time:88951ms step_avg:168.79ms\nstep:538/10000 train_loss:6.3377 train_time:89121ms step_avg:168.79ms\nstep:539/10000 train_loss:6.4394 train_time:89290ms step_avg:168.79ms\nstep:540/10000 train_loss:6.2752 train_time:89461ms step_avg:168.79ms\nstep:541/10000 train_loss:6.2014 train_time:89631ms step_avg:168.80ms\nstep:542/10000 train_loss:6.2024 train_time:89804ms step_avg:168.80ms\nstep:543/10000 train_loss:6.3954 train_time:89973ms step_avg:168.80ms\nstep:544/10000 train_loss:6.2192 train_time:90146ms step_avg:168.81ms\nstep:545/10000 train_loss:6.3048 train_time:90318ms step_avg:168.82ms\nstep:546/10000 train_loss:6.2900 train_time:90487ms step_avg:168.82ms\nstep:547/10000 train_loss:6.1857 train_time:90657ms step_avg:168.82ms\nstep:548/10000 train_loss:6.3341 train_time:90826ms step_avg:168.82ms\nstep:549/10000 train_loss:6.2435 train_time:90998ms step_avg:168.83ms\nstep:550/10000 train_loss:6.3128 train_time:91168ms step_avg:168.83ms\nstep:551/10000 train_loss:6.3179 train_time:91339ms step_avg:168.83ms\nstep:552/10000 train_loss:6.3003 train_time:91508ms step_avg:168.83ms\nstep:553/10000 train_loss:6.2241 train_time:91678ms step_avg:168.84ms\nstep:554/10000 train_loss:6.3439 train_time:91847ms step_avg:168.84ms\nstep:555/10000 train_loss:6.2054 train_time:92017ms step_avg:168.84ms\nstep:556/10000 train_loss:6.2599 train_time:92187ms step_avg:168.84ms\nstep:557/10000 train_loss:6.2888 train_time:92358ms step_avg:168.84ms\nstep:558/10000 train_loss:6.2408 train_time:92529ms step_avg:168.85ms\nstep:559/10000 train_loss:6.2462 train_time:92700ms step_avg:168.85ms\nstep:560/10000 train_loss:6.1565 train_time:92868ms step_avg:168.85ms\nstep:561/10000 train_loss:6.4076 train_time:93039ms step_avg:168.85ms\nstep:562/10000 train_loss:6.2027 train_time:93208ms step_avg:168.86ms\nstep:563/10000 train_loss:6.0967 train_time:93379ms step_avg:168.86ms\nstep:564/10000 train_loss:6.1854 train_time:93548ms step_avg:168.86ms\nstep:565/10000 train_loss:6.3853 train_time:93719ms step_avg:168.86ms\nstep:566/10000 train_loss:6.3394 train_time:93888ms step_avg:168.86ms\nstep:567/10000 train_loss:6.4779 train_time:94058ms step_avg:168.86ms\nstep:568/10000 train_loss:6.1435 train_time:94228ms step_avg:168.87ms\nstep:569/10000 train_loss:6.3676 train_time:94400ms step_avg:168.87ms\nstep:570/10000 train_loss:6.0644 train_time:94571ms step_avg:168.88ms\nstep:571/10000 train_loss:6.2416 train_time:94742ms step_avg:168.88ms\nstep:572/10000 train_loss:6.3869 train_time:94911ms step_avg:168.88ms\nstep:573/10000 train_loss:6.3176 train_time:95081ms step_avg:168.88ms\nstep:574/10000 train_loss:6.2552 train_time:95249ms step_avg:168.88ms\nstep:575/10000 train_loss:6.1423 train_time:95420ms step_avg:168.89ms\nstep:576/10000 train_loss:6.6669 train_time:95589ms step_avg:168.89ms\nstep:577/10000 train_loss:6.2072 train_time:95761ms step_avg:168.89ms\nstep:578/10000 train_loss:6.1888 train_time:95931ms step_avg:168.89ms\nstep:579/10000 train_loss:6.2518 train_time:96102ms step_avg:168.90ms\nstep:580/10000 train_loss:6.2017 train_time:96271ms step_avg:168.90ms\nstep:581/10000 train_loss:6.2617 train_time:96441ms step_avg:168.90ms\nstep:582/10000 train_loss:6.2254 train_time:96612ms step_avg:168.90ms\nstep:583/10000 train_loss:6.2537 train_time:96783ms step_avg:168.91ms\nstep:584/10000 train_loss:6.2903 train_time:96951ms step_avg:168.90ms\nstep:585/10000 train_loss:6.0920 train_time:97121ms step_avg:168.91ms\nstep:586/10000 train_loss:6.2150 train_time:97291ms step_avg:168.91ms\nstep:587/10000 train_loss:6.1503 train_time:97463ms step_avg:168.91ms\nstep:588/10000 train_loss:6.2285 train_time:97635ms step_avg:168.92ms\nstep:589/10000 train_loss:6.1331 train_time:97804ms step_avg:168.92ms\nstep:590/10000 train_loss:6.1725 train_time:97972ms step_avg:168.92ms\nstep:591/10000 train_loss:6.1568 train_time:98143ms step_avg:168.92ms\nstep:592/10000 train_loss:5.9639 train_time:98314ms step_avg:168.93ms\nstep:593/10000 train_loss:6.2207 train_time:98488ms step_avg:168.93ms\nstep:594/10000 train_loss:6.3059 train_time:98659ms step_avg:168.94ms\nstep:595/10000 train_loss:6.0638 train_time:98829ms step_avg:168.94ms\nstep:596/10000 train_loss:6.4167 train_time:99001ms step_avg:168.94ms\nstep:597/10000 train_loss:6.2410 train_time:99171ms step_avg:168.94ms\nstep:598/10000 train_loss:6.3111 train_time:99344ms step_avg:168.95ms\nstep:599/10000 train_loss:6.2860 train_time:99513ms step_avg:168.95ms\nstep:600/10000 train_loss:6.3419 train_time:99685ms step_avg:168.96ms\nstep:601/10000 train_loss:6.2753 train_time:99853ms step_avg:168.96ms\nstep:602/10000 train_loss:6.2075 train_time:100025ms step_avg:168.96ms\nstep:603/10000 train_loss:6.3178 train_time:100195ms step_avg:168.96ms\nstep:604/10000 train_loss:6.1003 train_time:100366ms step_avg:168.97ms\nstep:605/10000 train_loss:6.0923 train_time:100537ms step_avg:168.97ms\nstep:606/10000 train_loss:6.1698 train_time:100707ms step_avg:168.97ms\nstep:607/10000 train_loss:6.1482 train_time:100877ms step_avg:168.97ms\nstep:608/10000 train_loss:6.2569 train_time:101046ms step_avg:168.97ms\nstep:609/10000 train_loss:6.1344 train_time:101217ms step_avg:168.98ms\nstep:610/10000 train_loss:6.1976 train_time:101386ms step_avg:168.98ms\nstep:611/10000 train_loss:6.3180 train_time:101558ms step_avg:168.98ms\nstep:612/10000 train_loss:6.0746 train_time:101727ms step_avg:168.98ms\nstep:613/10000 train_loss:5.9059 train_time:101897ms step_avg:168.98ms\nstep:614/10000 train_loss:6.1178 train_time:102067ms step_avg:168.99ms\nstep:615/10000 train_loss:6.1345 train_time:102237ms step_avg:168.99ms\nstep:616/10000 train_loss:6.0795 train_time:102415ms step_avg:169.00ms\nstep:617/10000 train_loss:6.1868 train_time:102582ms step_avg:169.00ms\nstep:618/10000 train_loss:6.2240 train_time:102750ms step_avg:169.00ms\nstep:619/10000 train_loss:6.2684 train_time:102920ms step_avg:169.00ms\nstep:620/10000 train_loss:6.0752 train_time:103089ms step_avg:169.00ms\nstep:621/10000 train_loss:6.1919 train_time:103260ms step_avg:169.00ms\nstep:622/10000 train_loss:6.1199 train_time:103429ms step_avg:169.00ms\nstep:623/10000 train_loss:6.0882 train_time:103599ms step_avg:169.00ms\nstep:624/10000 train_loss:6.1815 train_time:103770ms step_avg:169.01ms\nstep:625/10000 train_loss:6.2572 train_time:103941ms step_avg:169.01ms\nstep:626/10000 train_loss:6.2888 train_time:104111ms step_avg:169.01ms\nstep:627/10000 train_loss:6.0334 train_time:104281ms step_avg:169.01ms\nstep:628/10000 train_loss:6.1209 train_time:104451ms step_avg:169.01ms\nstep:629/10000 train_loss:6.1415 train_time:104622ms step_avg:169.02ms\nstep:630/10000 train_loss:6.0435 train_time:104792ms step_avg:169.02ms\nstep:631/10000 train_loss:6.0194 train_time:104962ms step_avg:169.02ms\nstep:632/10000 train_loss:6.1796 train_time:105131ms step_avg:169.02ms\nstep:633/10000 train_loss:6.0358 train_time:105301ms step_avg:169.02ms\nstep:634/10000 train_loss:6.1138 train_time:105470ms step_avg:169.02ms\nstep:635/10000 train_loss:6.2280 train_time:105643ms step_avg:169.03ms\nstep:636/10000 train_loss:6.0340 train_time:105812ms step_avg:169.03ms\nstep:637/10000 train_loss:5.9950 train_time:105984ms step_avg:169.03ms\nstep:638/10000 train_loss:6.0582 train_time:106154ms step_avg:169.04ms\nstep:639/10000 train_loss:6.2869 train_time:106325ms step_avg:169.04ms\nstep:640/10000 train_loss:6.0540 train_time:106497ms step_avg:169.04ms\nstep:641/10000 train_loss:6.0411 train_time:106667ms step_avg:169.05ms\nstep:642/10000 train_loss:6.0593 train_time:106840ms step_avg:169.05ms\nstep:643/10000 train_loss:6.0806 train_time:107010ms step_avg:169.05ms\nstep:644/10000 train_loss:6.1073 train_time:107180ms step_avg:169.05ms\nstep:645/10000 train_loss:5.9218 train_time:107350ms step_avg:169.06ms\nstep:646/10000 train_loss:6.1389 train_time:107521ms step_avg:169.06ms\nstep:647/10000 train_loss:6.1419 train_time:107691ms step_avg:169.06ms\nstep:648/10000 train_loss:6.1221 train_time:107863ms step_avg:169.06ms\nstep:649/10000 train_loss:6.0335 train_time:108031ms step_avg:169.06ms\nstep:650/10000 train_loss:6.1288 train_time:108201ms step_avg:169.06ms\nstep:651/10000 train_loss:6.3008 train_time:108370ms step_avg:169.06ms\nstep:652/10000 train_loss:6.0201 train_time:108541ms step_avg:169.07ms\nstep:653/10000 train_loss:5.9885 train_time:108711ms step_avg:169.07ms\nstep:654/10000 train_loss:5.9582 train_time:108884ms step_avg:169.07ms\nstep:655/10000 train_loss:6.1265 train_time:109053ms step_avg:169.07ms\nstep:656/10000 train_loss:5.9316 train_time:109225ms step_avg:169.08ms\nstep:657/10000 train_loss:5.9297 train_time:109395ms step_avg:169.08ms\nstep:658/10000 train_loss:6.0897 train_time:109567ms step_avg:169.08ms\nstep:659/10000 train_loss:6.0934 train_time:109737ms step_avg:169.09ms\nstep:660/10000 train_loss:6.0538 train_time:109908ms step_avg:169.09ms\nstep:661/10000 train_loss:5.8645 train_time:110080ms step_avg:169.09ms\nstep:662/10000 train_loss:6.2346 train_time:110251ms step_avg:169.10ms\nstep:663/10000 train_loss:5.9947 train_time:110420ms step_avg:169.10ms\nstep:664/10000 train_loss:6.1529 train_time:110588ms step_avg:169.10ms\nstep:665/10000 train_loss:6.0986 train_time:110759ms step_avg:169.10ms\nstep:666/10000 train_loss:6.0121 train_time:110927ms step_avg:169.10ms\nstep:667/10000 train_loss:6.0206 train_time:111099ms step_avg:169.10ms\nstep:668/10000 train_loss:5.9314 train_time:111268ms step_avg:169.10ms\nstep:669/10000 train_loss:5.7876 train_time:111439ms step_avg:169.10ms\nstep:670/10000 train_loss:6.0488 train_time:111608ms step_avg:169.10ms\nstep:671/10000 train_loss:6.0927 train_time:111781ms step_avg:169.11ms\nstep:672/10000 train_loss:6.0108 train_time:111947ms step_avg:169.10ms\nstep:673/10000 train_loss:5.9563 train_time:112118ms step_avg:169.11ms\nstep:674/10000 train_loss:5.9387 train_time:112287ms step_avg:169.11ms\nstep:675/10000 train_loss:5.9098 train_time:112458ms step_avg:169.11ms\nstep:676/10000 train_loss:6.0254 train_time:112626ms step_avg:169.11ms\nstep:677/10000 train_loss:6.0062 train_time:112802ms step_avg:169.12ms\nstep:678/10000 train_loss:6.0398 train_time:112970ms step_avg:169.12ms\nstep:679/10000 train_loss:6.0502 train_time:113141ms step_avg:169.12ms\nstep:680/10000 train_loss:5.9999 train_time:113311ms step_avg:169.12ms\nstep:681/10000 train_loss:5.9922 train_time:113482ms step_avg:169.12ms\nstep:682/10000 train_loss:5.9876 train_time:113650ms step_avg:169.12ms\nstep:683/10000 train_loss:6.1611 train_time:113820ms step_avg:169.12ms\nstep:684/10000 train_loss:5.2970 train_time:113990ms step_avg:169.12ms\nstep:685/10000 train_loss:5.9163 train_time:114161ms step_avg:169.13ms\nstep:686/10000 train_loss:5.9471 train_time:114331ms step_avg:169.13ms\nstep:687/10000 train_loss:6.1273 train_time:114504ms step_avg:169.13ms\nstep:688/10000 train_loss:5.9373 train_time:114673ms step_avg:169.13ms\nstep:689/10000 train_loss:6.1976 train_time:114842ms step_avg:169.13ms\nstep:690/10000 train_loss:6.0348 train_time:115013ms step_avg:169.14ms\nstep:691/10000 train_loss:6.0725 train_time:115185ms step_avg:169.14ms\nstep:692/10000 train_loss:5.8661 train_time:115354ms step_avg:169.14ms\nstep:693/10000 train_loss:5.8513 train_time:115524ms step_avg:169.14ms\nstep:694/10000 train_loss:6.1166 train_time:115695ms step_avg:169.15ms\nstep:695/10000 train_loss:5.8585 train_time:115864ms step_avg:169.14ms\nstep:696/10000 train_loss:6.0722 train_time:116036ms step_avg:169.15ms\nstep:697/10000 train_loss:6.0199 train_time:116208ms step_avg:169.15ms\nstep:698/10000 train_loss:6.0406 train_time:116378ms step_avg:169.15ms\nstep:699/10000 train_loss:5.8900 train_time:116547ms step_avg:169.15ms\nstep:700/10000 train_loss:6.1145 train_time:116718ms step_avg:169.16ms\nstep:701/10000 train_loss:6.1656 train_time:116887ms step_avg:169.16ms\nstep:702/10000 train_loss:5.8737 train_time:117058ms step_avg:169.16ms\nstep:703/10000 train_loss:6.0358 train_time:117227ms step_avg:169.16ms\nstep:704/10000 train_loss:5.9680 train_time:117398ms step_avg:169.16ms\nstep:705/10000 train_loss:5.9137 train_time:117569ms step_avg:169.16ms\nstep:706/10000 train_loss:5.8402 train_time:117741ms step_avg:169.17ms\nstep:707/10000 train_loss:5.7907 train_time:117911ms step_avg:169.17ms\nstep:708/10000 train_loss:6.1568 train_time:118084ms step_avg:169.17ms\nstep:709/10000 train_loss:5.9387 train_time:118255ms step_avg:169.18ms\nstep:710/10000 train_loss:5.8322 train_time:118427ms step_avg:169.18ms\nstep:711/10000 train_loss:5.7381 train_time:118597ms step_avg:169.18ms\nstep:712/10000 train_loss:5.8779 train_time:118767ms step_avg:169.18ms\nstep:713/10000 train_loss:6.1893 train_time:118938ms step_avg:169.19ms\nstep:714/10000 train_loss:6.0095 train_time:119108ms step_avg:169.19ms\nstep:715/10000 train_loss:6.0558 train_time:119280ms step_avg:169.19ms\nstep:716/10000 train_loss:5.8901 train_time:119449ms step_avg:169.19ms\nstep:717/10000 train_loss:5.8890 train_time:119622ms step_avg:169.20ms\nstep:718/10000 train_loss:5.9166 train_time:119791ms step_avg:169.20ms\nstep:719/10000 train_loss:6.0312 train_time:119962ms step_avg:169.20ms\nstep:720/10000 train_loss:5.9650 train_time:120131ms step_avg:169.20ms\nstep:721/10000 train_loss:5.9716 train_time:120304ms step_avg:169.20ms\nstep:722/10000 train_loss:5.9633 train_time:120472ms step_avg:169.20ms\nstep:723/10000 train_loss:5.9279 train_time:120644ms step_avg:169.21ms\nstep:724/10000 train_loss:6.0448 train_time:120814ms step_avg:169.21ms\nstep:725/10000 train_loss:5.7870 train_time:120986ms step_avg:169.21ms\nstep:726/10000 train_loss:5.9474 train_time:121158ms step_avg:169.21ms\nstep:727/10000 train_loss:5.8947 train_time:121329ms step_avg:169.22ms\nstep:728/10000 train_loss:5.7332 train_time:121498ms step_avg:169.22ms\nstep:729/10000 train_loss:5.9991 train_time:121666ms step_avg:169.22ms\nstep:730/10000 train_loss:5.8322 train_time:121842ms step_avg:169.23ms\nstep:731/10000 train_loss:5.7592 train_time:122008ms step_avg:169.22ms\nstep:732/10000 train_loss:5.9850 train_time:122179ms step_avg:169.22ms\nstep:733/10000 train_loss:5.9091 train_time:122346ms step_avg:169.22ms\nstep:734/10000 train_loss:5.8048 train_time:122516ms step_avg:169.22ms\nstep:735/10000 train_loss:5.8156 train_time:122685ms step_avg:169.22ms\nstep:736/10000 train_loss:6.1307 train_time:122855ms step_avg:169.22ms\nstep:737/10000 train_loss:5.7791 train_time:123027ms step_avg:169.22ms\nstep:738/10000 train_loss:5.9773 train_time:123198ms step_avg:169.23ms\nstep:739/10000 train_loss:5.7357 train_time:123367ms step_avg:169.23ms\nstep:740/10000 train_loss:5.8667 train_time:123538ms step_avg:169.23ms\nstep:741/10000 train_loss:5.8514 train_time:123705ms step_avg:169.23ms\nstep:742/10000 train_loss:5.7912 train_time:123873ms step_avg:169.23ms\nstep:743/10000 train_loss:5.8280 train_time:124045ms step_avg:169.23ms\nstep:744/10000 train_loss:5.9076 train_time:124215ms step_avg:169.23ms\nstep:745/10000 train_loss:5.7710 train_time:124386ms step_avg:169.23ms\nstep:746/10000 train_loss:6.1591 train_time:124556ms step_avg:169.23ms\nstep:747/10000 train_loss:5.8944 train_time:124726ms step_avg:169.23ms\nstep:748/10000 train_loss:5.9177 train_time:124896ms step_avg:169.24ms\nstep:749/10000 train_loss:6.0384 train_time:125067ms step_avg:169.24ms\nstep:750/10000 train_loss:5.8620 train_time:125238ms step_avg:169.24ms\nstep:751/10000 train_loss:5.7677 train_time:125414ms step_avg:169.25ms\nstep:752/10000 train_loss:5.9793 train_time:125582ms step_avg:169.25ms\nstep:753/10000 train_loss:5.8996 train_time:125751ms step_avg:169.25ms\nstep:754/10000 train_loss:5.8010 train_time:125919ms step_avg:169.25ms\nstep:755/10000 train_loss:5.9318 train_time:126086ms step_avg:169.24ms\nstep:756/10000 train_loss:5.4083 train_time:126258ms step_avg:169.25ms\nstep:757/10000 train_loss:5.3073 train_time:126428ms step_avg:169.25ms\nstep:758/10000 train_loss:6.0077 train_time:126603ms step_avg:169.26ms\nstep:759/10000 train_loss:5.8886 train_time:126777ms step_avg:169.26ms\nstep:760/10000 train_loss:5.7759 train_time:126944ms step_avg:169.26ms\nstep:761/10000 train_loss:5.7154 train_time:127114ms step_avg:169.26ms\nstep:762/10000 train_loss:5.9818 train_time:127515ms step_avg:169.57ms\nstep:763/10000 train_loss:5.8402 train_time:127674ms step_avg:169.55ms\nstep:764/10000 train_loss:5.7729 train_time:127840ms step_avg:169.55ms\nstep:765/10000 train_loss:5.6996 train_time:128008ms step_avg:169.55ms\nstep:766/10000 train_loss:5.9463 train_time:128176ms step_avg:169.54ms\nstep:767/10000 train_loss:5.8321 train_time:128344ms step_avg:169.54ms\nstep:768/10000 train_loss:5.7383 train_time:128525ms step_avg:169.56ms\nstep:769/10000 train_loss:5.8956 train_time:128698ms step_avg:169.56ms\nstep:770/10000 train_loss:5.8330 train_time:128867ms step_avg:169.56ms\nstep:771/10000 train_loss:5.9090 train_time:129036ms step_avg:169.56ms\nstep:772/10000 train_loss:5.8429 train_time:129204ms step_avg:169.56ms\nstep:773/10000 train_loss:5.6153 train_time:129373ms step_avg:169.56ms\nstep:774/10000 train_loss:5.8469 train_time:129546ms step_avg:169.56ms\nstep:775/10000 train_loss:5.8885 train_time:129718ms step_avg:169.57ms\nstep:776/10000 train_loss:5.8614 train_time:129887ms step_avg:169.57ms\nstep:777/10000 train_loss:5.9117 train_time:130058ms step_avg:169.57ms\nstep:778/10000 train_loss:5.8401 train_time:130228ms step_avg:169.57ms\nstep:779/10000 train_loss:5.9480 train_time:130399ms step_avg:169.57ms\nstep:780/10000 train_loss:6.0650 train_time:130570ms step_avg:169.57ms\nstep:781/10000 train_loss:5.7858 train_time:130743ms step_avg:169.58ms\nstep:782/10000 train_loss:5.8020 train_time:130913ms step_avg:169.58ms\nstep:783/10000 train_loss:5.7664 train_time:131083ms step_avg:169.58ms\nstep:784/10000 train_loss:5.9267 train_time:131253ms step_avg:169.58ms\nstep:785/10000 train_loss:5.7158 train_time:131424ms step_avg:169.58ms\nstep:786/10000 train_loss:5.8980 train_time:131595ms step_avg:169.58ms\nstep:787/10000 train_loss:5.8551 train_time:131765ms step_avg:169.58ms\nstep:788/10000 train_loss:5.7471 train_time:131935ms step_avg:169.58ms\nstep:789/10000 train_loss:5.6970 train_time:132103ms step_avg:169.58ms\nstep:790/10000 train_loss:5.6954 train_time:132275ms step_avg:169.58ms\nstep:791/10000 train_loss:5.9914 train_time:132446ms step_avg:169.59ms\nstep:792/10000 train_loss:5.7427 train_time:132620ms step_avg:169.59ms\nstep:793/10000 train_loss:5.6800 train_time:132789ms step_avg:169.59ms\nstep:794/10000 train_loss:5.9039 train_time:132961ms step_avg:169.59ms\nstep:795/10000 train_loss:5.7769 train_time:133128ms step_avg:169.59ms\nstep:796/10000 train_loss:5.7180 train_time:133299ms step_avg:169.59ms\nstep:797/10000 train_loss:5.6999 train_time:133468ms step_avg:169.59ms\nstep:798/10000 train_loss:5.7939 train_time:133641ms step_avg:169.60ms\nstep:799/10000 train_loss:5.9894 train_time:133811ms step_avg:169.60ms\nstep:800/10000 train_loss:5.7839 train_time:133981ms step_avg:169.60ms\nstep:801/10000 train_loss:5.6975 train_time:134149ms step_avg:169.59ms\nstep:802/10000 train_loss:6.4363 train_time:134318ms step_avg:169.59ms\nstep:803/10000 train_loss:5.9255 train_time:134487ms step_avg:169.59ms\nstep:804/10000 train_loss:5.7698 train_time:134659ms step_avg:169.60ms\nstep:805/10000 train_loss:5.8819 train_time:134829ms step_avg:169.60ms\nstep:806/10000 train_loss:5.9746 train_time:134998ms step_avg:169.60ms\nstep:807/10000 train_loss:5.7399 train_time:135167ms step_avg:169.60ms\nstep:808/10000 train_loss:5.8761 train_time:135339ms step_avg:169.60ms\nstep:809/10000 train_loss:5.9271 train_time:135508ms step_avg:169.60ms\nstep:810/10000 train_loss:5.7625 train_time:135679ms step_avg:169.60ms\nstep:811/10000 train_loss:5.7149 train_time:135848ms step_avg:169.60ms\nstep:812/10000 train_loss:5.8400 train_time:136020ms step_avg:169.60ms\nstep:813/10000 train_loss:5.9493 train_time:136189ms step_avg:169.60ms\nstep:814/10000 train_loss:5.7271 train_time:136360ms step_avg:169.60ms\nstep:815/10000 train_loss:5.8049 train_time:136530ms step_avg:169.60ms\nstep:816/10000 train_loss:5.7144 train_time:136701ms step_avg:169.60ms\nstep:817/10000 train_loss:5.7114 train_time:136872ms step_avg:169.61ms\nstep:818/10000 train_loss:5.7279 train_time:137044ms step_avg:169.61ms\nstep:819/10000 train_loss:5.8172 train_time:137213ms step_avg:169.61ms\nstep:820/10000 train_loss:5.9371 train_time:137386ms step_avg:169.61ms\nstep:821/10000 train_loss:5.7715 train_time:137555ms step_avg:169.61ms\nstep:822/10000 train_loss:5.7266 train_time:137724ms step_avg:169.61ms\nstep:823/10000 train_loss:5.7643 train_time:137892ms step_avg:169.61ms\nstep:824/10000 train_loss:5.5972 train_time:138062ms step_avg:169.61ms\nstep:825/10000 train_loss:5.7212 train_time:138234ms step_avg:169.61ms\nstep:826/10000 train_loss:5.8209 train_time:138404ms step_avg:169.61ms\nstep:827/10000 train_loss:5.8373 train_time:138575ms step_avg:169.61ms\nstep:828/10000 train_loss:5.7496 train_time:138747ms step_avg:169.62ms\nstep:829/10000 train_loss:5.6320 train_time:138917ms step_avg:169.62ms\nstep:830/10000 train_loss:5.6275 train_time:139086ms step_avg:169.62ms\nstep:831/10000 train_loss:5.6482 train_time:139257ms step_avg:169.62ms\nstep:832/10000 train_loss:5.7153 train_time:139428ms step_avg:169.62ms\nstep:833/10000 train_loss:5.7781 train_time:139601ms step_avg:169.62ms\nstep:834/10000 train_loss:5.7408 train_time:139768ms step_avg:169.62ms\nstep:835/10000 train_loss:5.4698 train_time:139939ms step_avg:169.62ms\nstep:836/10000 train_loss:5.8617 train_time:140109ms step_avg:169.62ms\nstep:837/10000 train_loss:5.6951 train_time:140279ms step_avg:169.62ms\nstep:838/10000 train_loss:5.6514 train_time:140448ms step_avg:169.62ms\nstep:839/10000 train_loss:5.7666 train_time:140620ms step_avg:169.63ms\nstep:840/10000 train_loss:5.6868 train_time:140790ms step_avg:169.63ms\nstep:841/10000 train_loss:5.8226 train_time:140962ms step_avg:169.63ms\nstep:842/10000 train_loss:5.6876 train_time:141132ms step_avg:169.63ms\nstep:843/10000 train_loss:5.7827 train_time:141303ms step_avg:169.63ms\nstep:844/10000 train_loss:5.6467 train_time:141473ms step_avg:169.63ms\nstep:845/10000 train_loss:5.7361 train_time:141645ms step_avg:169.63ms\nstep:846/10000 train_loss:5.6546 train_time:141817ms step_avg:169.64ms\nstep:847/10000 train_loss:5.7462 train_time:141986ms step_avg:169.64ms\nstep:848/10000 train_loss:5.9467 train_time:142156ms step_avg:169.64ms\nstep:849/10000 train_loss:5.6738 train_time:142324ms step_avg:169.64ms\nstep:850/10000 train_loss:5.6328 train_time:142491ms step_avg:169.63ms\nstep:851/10000 train_loss:5.9518 train_time:142663ms step_avg:169.64ms\nstep:852/10000 train_loss:5.6015 train_time:142834ms step_avg:169.64ms\nstep:853/10000 train_loss:5.7700 train_time:143004ms step_avg:169.64ms\nstep:854/10000 train_loss:5.7150 train_time:143174ms step_avg:169.64ms\nstep:855/10000 train_loss:5.8469 train_time:143347ms step_avg:169.64ms\nstep:856/10000 train_loss:5.7240 train_time:143518ms step_avg:169.64ms\nstep:857/10000 train_loss:5.5440 train_time:143689ms step_avg:169.64ms\nstep:858/10000 train_loss:5.7248 train_time:143860ms step_avg:169.65ms\nstep:859/10000 train_loss:5.8581 train_time:144030ms step_avg:169.65ms\nstep:860/10000 train_loss:5.6335 train_time:144201ms step_avg:169.65ms\nstep:861/10000 train_loss:5.9516 train_time:144371ms step_avg:169.65ms\nstep:862/10000 train_loss:5.6058 train_time:144542ms step_avg:169.65ms\nstep:863/10000 train_loss:5.7272 train_time:144711ms step_avg:169.65ms\nstep:864/10000 train_loss:5.7512 train_time:144882ms step_avg:169.65ms\nstep:865/10000 train_loss:5.5735 train_time:145052ms step_avg:169.65ms\nstep:866/10000 train_loss:5.7142 train_time:145222ms step_avg:169.65ms\nstep:867/10000 train_loss:5.5474 train_time:145392ms step_avg:169.65ms\nstep:868/10000 train_loss:5.6691 train_time:145564ms step_avg:169.65ms\nstep:869/10000 train_loss:5.5632 train_time:145734ms step_avg:169.66ms\nstep:870/10000 train_loss:5.6409 train_time:145903ms step_avg:169.66ms\nstep:871/10000 train_loss:5.6693 train_time:146074ms step_avg:169.66ms\nstep:872/10000 train_loss:5.5849 train_time:146246ms step_avg:169.66ms\nstep:873/10000 train_loss:5.5146 train_time:146415ms step_avg:169.66ms\nstep:874/10000 train_loss:5.4451 train_time:146584ms step_avg:169.66ms\nstep:875/10000 train_loss:5.7203 train_time:146754ms step_avg:169.66ms\nstep:876/10000 train_loss:5.6382 train_time:146923ms step_avg:169.66ms\nstep:877/10000 train_loss:5.7194 train_time:147094ms step_avg:169.66ms\nstep:878/10000 train_loss:5.5977 train_time:147265ms step_avg:169.66ms\nstep:879/10000 train_loss:5.4414 train_time:147436ms step_avg:169.66ms\nstep:880/10000 train_loss:5.6120 train_time:147606ms step_avg:169.66ms\nstep:881/10000 train_loss:5.5612 train_time:147777ms step_avg:169.66ms\nstep:882/10000 train_loss:5.7308 train_time:147945ms step_avg:169.66ms\nstep:883/10000 train_loss:5.5802 train_time:148117ms step_avg:169.66ms\nstep:884/10000 train_loss:5.7307 train_time:148288ms step_avg:169.67ms\nstep:885/10000 train_loss:5.5689 train_time:148462ms step_avg:169.67ms\nstep:886/10000 train_loss:5.6955 train_time:148632ms step_avg:169.67ms\nstep:887/10000 train_loss:5.6063 train_time:148803ms step_avg:169.67ms\nstep:888/10000 train_loss:5.7586 train_time:148971ms step_avg:169.67ms\nstep:889/10000 train_loss:5.7033 train_time:149144ms step_avg:169.67ms\nstep:890/10000 train_loss:5.6592 train_time:149314ms step_avg:169.68ms\nstep:891/10000 train_loss:5.5903 train_time:149487ms step_avg:169.68ms\nstep:892/10000 train_loss:5.7529 train_time:149657ms step_avg:169.68ms\nstep:893/10000 train_loss:5.6733 train_time:149828ms step_avg:169.68ms\nstep:894/10000 train_loss:5.6274 train_time:149998ms step_avg:169.68ms\nstep:895/10000 train_loss:5.7539 train_time:150166ms step_avg:169.68ms\nstep:896/10000 train_loss:5.7190 train_time:150338ms step_avg:169.68ms\nstep:897/10000 train_loss:5.5210 train_time:150506ms step_avg:169.68ms\nstep:898/10000 train_loss:5.6552 train_time:150676ms step_avg:169.68ms\nstep:899/10000 train_loss:5.7519 train_time:150844ms step_avg:169.68ms\nstep:900/10000 train_loss:5.3915 train_time:151013ms step_avg:169.68ms\nstep:901/10000 train_loss:5.6392 train_time:151185ms step_avg:169.68ms\nstep:902/10000 train_loss:5.5095 train_time:151358ms step_avg:169.68ms\nstep:903/10000 train_loss:5.7031 train_time:151526ms step_avg:169.68ms\nstep:904/10000 train_loss:5.5256 train_time:151697ms step_avg:169.68ms\nstep:905/10000 train_loss:5.7219 train_time:151868ms step_avg:169.68ms\nstep:906/10000 train_loss:5.5115 train_time:152038ms step_avg:169.69ms\nstep:907/10000 train_loss:5.5567 train_time:152206ms step_avg:169.68ms\nstep:908/10000 train_loss:5.4858 train_time:152377ms step_avg:169.68ms\nstep:909/10000 train_loss:5.6558 train_time:152546ms step_avg:169.68ms\nstep:910/10000 train_loss:5.5558 train_time:152715ms step_avg:169.68ms\nstep:911/10000 train_loss:5.7175 train_time:152883ms step_avg:169.68ms\nstep:912/10000 train_loss:5.6454 train_time:153055ms step_avg:169.68ms\nstep:913/10000 train_loss:5.6490 train_time:153225ms step_avg:169.68ms\nstep:914/10000 train_loss:5.6745 train_time:153396ms step_avg:169.69ms\nstep:915/10000 train_loss:5.6410 train_time:153564ms step_avg:169.68ms\nstep:916/10000 train_loss:5.5833 train_time:153734ms step_avg:169.68ms\nstep:917/10000 train_loss:5.6021 train_time:153901ms step_avg:169.68ms\nstep:918/10000 train_loss:5.6151 train_time:154073ms step_avg:169.68ms\nstep:919/10000 train_loss:5.8182 train_time:154244ms step_avg:169.69ms\nstep:920/10000 train_loss:5.6998 train_time:154417ms step_avg:169.69ms\nstep:921/10000 train_loss:5.6446 train_time:154589ms step_avg:169.69ms\nstep:922/10000 train_loss:5.5826 train_time:154761ms step_avg:169.69ms\nstep:923/10000 train_loss:5.4451 train_time:154931ms step_avg:169.69ms\nstep:924/10000 train_loss:5.5826 train_time:155103ms step_avg:169.70ms\nstep:925/10000 train_loss:5.4133 train_time:155274ms step_avg:169.70ms\nstep:926/10000 train_loss:5.7047 train_time:155447ms step_avg:169.70ms\nstep:927/10000 train_loss:5.5093 train_time:155620ms step_avg:169.71ms\nstep:928/10000 train_loss:5.4143 train_time:155792ms step_avg:169.71ms\nstep:929/10000 train_loss:5.6595 train_time:155958ms step_avg:169.70ms\nstep:930/10000 train_loss:5.4778 train_time:156126ms step_avg:169.70ms\nstep:931/10000 train_loss:5.5254 train_time:156298ms step_avg:169.70ms\nstep:932/10000 train_loss:5.6844 train_time:156469ms step_avg:169.71ms\nstep:933/10000 train_loss:5.7057 train_time:156641ms step_avg:169.71ms\nstep:934/10000 train_loss:5.4966 train_time:156811ms step_avg:169.71ms\nstep:935/10000 train_loss:5.5365 train_time:156983ms step_avg:169.71ms\nstep:936/10000 train_loss:5.5180 train_time:157152ms step_avg:169.71ms\nstep:937/10000 train_loss:5.5714 train_time:157322ms step_avg:169.71ms\nstep:938/10000 train_loss:5.7371 train_time:157493ms step_avg:169.71ms\nstep:939/10000 train_loss:5.7401 train_time:157666ms step_avg:169.72ms\nstep:940/10000 train_loss:5.3562 train_time:157839ms step_avg:169.72ms\nstep:941/10000 train_loss:5.7265 train_time:158009ms step_avg:169.72ms\nstep:942/10000 train_loss:5.5906 train_time:158178ms step_avg:169.72ms\nstep:943/10000 train_loss:5.4399 train_time:158347ms step_avg:169.72ms\nstep:944/10000 train_loss:5.7523 train_time:158518ms step_avg:169.72ms\nstep:945/10000 train_loss:5.4121 train_time:158687ms step_avg:169.72ms\nstep:946/10000 train_loss:5.7316 train_time:158860ms step_avg:169.72ms\nstep:947/10000 train_loss:5.6787 train_time:159029ms step_avg:169.72ms\nstep:948/10000 train_loss:5.5291 train_time:159200ms step_avg:169.72ms\nstep:949/10000 train_loss:5.2988 train_time:159368ms step_avg:169.72ms\nstep:950/10000 train_loss:5.6312 train_time:159538ms step_avg:169.72ms\nstep:951/10000 train_loss:5.6124 train_time:159707ms step_avg:169.72ms\nstep:952/10000 train_loss:5.4973 train_time:159878ms step_avg:169.72ms\nstep:953/10000 train_loss:5.7332 train_time:160049ms step_avg:169.72ms\nstep:954/10000 train_loss:5.5021 train_time:160219ms step_avg:169.72ms\nstep:955/10000 train_loss:5.5838 train_time:160390ms step_avg:169.72ms\nstep:956/10000 train_loss:5.6351 train_time:160563ms step_avg:169.73ms\nstep:957/10000 train_loss:5.4276 train_time:160732ms step_avg:169.73ms\nstep:958/10000 train_loss:5.5805 train_time:160902ms step_avg:169.73ms\nstep:959/10000 train_loss:5.4246 train_time:161070ms step_avg:169.73ms\nstep:960/10000 train_loss:5.6414 train_time:161240ms step_avg:169.73ms\nstep:961/10000 train_loss:5.4204 train_time:161407ms step_avg:169.72ms\nstep:962/10000 train_loss:5.8388 train_time:161579ms step_avg:169.73ms\nstep:963/10000 train_loss:5.6323 train_time:161749ms step_avg:169.73ms\nstep:964/10000 train_loss:5.5198 train_time:161920ms step_avg:169.73ms\nstep:965/10000 train_loss:5.6219 train_time:162088ms step_avg:169.73ms\nstep:966/10000 train_loss:5.5050 train_time:162258ms step_avg:169.73ms\nstep:967/10000 train_loss:5.6343 train_time:162428ms step_avg:169.73ms\nstep:968/10000 train_loss:5.6999 train_time:162599ms step_avg:169.73ms\nstep:969/10000 train_loss:5.6477 train_time:162769ms step_avg:169.73ms\nstep:970/10000 train_loss:5.8625 train_time:162943ms step_avg:169.73ms\nstep:971/10000 train_loss:5.6475 train_time:163113ms step_avg:169.73ms\nstep:972/10000 train_loss:5.4743 train_time:163287ms step_avg:169.74ms\nstep:973/10000 train_loss:5.4149 train_time:163455ms step_avg:169.74ms\nstep:974/10000 train_loss:5.4347 train_time:163626ms step_avg:169.74ms\nstep:975/10000 train_loss:5.4129 train_time:163798ms step_avg:169.74ms\nstep:976/10000 train_loss:5.4416 train_time:163967ms step_avg:169.74ms\nstep:977/10000 train_loss:5.5580 train_time:164136ms step_avg:169.74ms\nstep:978/10000 train_loss:5.4919 train_time:164305ms step_avg:169.74ms\nstep:979/10000 train_loss:5.6466 train_time:164476ms step_avg:169.74ms\nstep:980/10000 train_loss:5.5318 train_time:164642ms step_avg:169.73ms\nstep:981/10000 train_loss:5.5445 train_time:164810ms step_avg:169.73ms\nstep:982/10000 train_loss:5.5194 train_time:164982ms step_avg:169.73ms\nstep:983/10000 train_loss:5.5528 train_time:165151ms step_avg:169.73ms\nstep:984/10000 train_loss:5.5087 train_time:165320ms step_avg:169.73ms\nstep:985/10000 train_loss:5.4620 train_time:165489ms step_avg:169.73ms\nstep:986/10000 train_loss:5.5109 train_time:165661ms step_avg:169.73ms\nstep:987/10000 train_loss:5.6526 train_time:165830ms step_avg:169.73ms\nstep:988/10000 train_loss:5.4778 train_time:166001ms step_avg:169.74ms\nstep:989/10000 train_loss:5.2952 train_time:166170ms step_avg:169.73ms\nstep:990/10000 train_loss:5.5482 train_time:166341ms step_avg:169.74ms\nstep:991/10000 train_loss:5.5151 train_time:166511ms step_avg:169.74ms\nstep:992/10000 train_loss:5.7066 train_time:166683ms step_avg:169.74ms\nstep:993/10000 train_loss:5.4356 train_time:166852ms step_avg:169.74ms\nstep:994/10000 train_loss:5.3565 train_time:167024ms step_avg:169.74ms\nstep:995/10000 train_loss:5.6073 train_time:167192ms step_avg:169.74ms\nstep:996/10000 train_loss:5.3853 train_time:167360ms step_avg:169.74ms\nstep:997/10000 train_loss:5.5392 train_time:167530ms step_avg:169.74ms\nstep:998/10000 train_loss:5.5417 train_time:167702ms step_avg:169.74ms\nstep:999/10000 train_loss:5.4557 train_time:167873ms step_avg:169.74ms\nstep:1000/10000 train_loss:5.3967 train_time:168044ms step_avg:169.74ms\nstep:1001/10000 train_loss:5.4464 train_time:168216ms step_avg:169.74ms\nstep:1002/10000 train_loss:5.4620 train_time:168387ms step_avg:169.75ms\nstep:1003/10000 train_loss:5.2475 train_time:168557ms step_avg:169.75ms\nstep:1004/10000 train_loss:5.5405 train_time:168728ms step_avg:169.75ms\nstep:1005/10000 train_loss:5.5685 train_time:168899ms step_avg:169.75ms\nstep:1006/10000 train_loss:5.3493 train_time:169075ms step_avg:169.75ms\nstep:1007/10000 train_loss:5.7162 train_time:169238ms step_avg:169.75ms\nstep:1008/10000 train_loss:5.4517 train_time:169407ms step_avg:169.75ms\nstep:1009/10000 train_loss:5.4440 train_time:169579ms step_avg:169.75ms\nstep:1010/10000 train_loss:5.3165 train_time:169749ms step_avg:169.75ms\nstep:1011/10000 train_loss:5.4909 train_time:169920ms step_avg:169.75ms\nstep:1012/10000 train_loss:5.4328 train_time:170088ms step_avg:169.75ms\nstep:1013/10000 train_loss:5.3507 train_time:170258ms step_avg:169.75ms\nstep:1014/10000 train_loss:5.4266 train_time:170427ms step_avg:169.75ms\nstep:1015/10000 train_loss:5.4410 train_time:170599ms step_avg:169.75ms\nstep:1016/10000 train_loss:5.4505 train_time:170768ms step_avg:169.75ms\nstep:1017/10000 train_loss:5.4051 train_time:170939ms step_avg:169.75ms\nstep:1018/10000 train_loss:5.4623 train_time:171108ms step_avg:169.75ms\nstep:1019/10000 train_loss:5.2891 train_time:171277ms step_avg:169.75ms\nstep:1020/10000 train_loss:5.5137 train_time:171446ms step_avg:169.75ms\nstep:1021/10000 train_loss:5.5881 train_time:171618ms step_avg:169.75ms\nstep:1022/10000 train_loss:5.3127 train_time:171786ms step_avg:169.75ms\nstep:1023/10000 train_loss:5.4635 train_time:171957ms step_avg:169.75ms\nstep:1024/10000 train_loss:5.5693 train_time:172127ms step_avg:169.75ms\nstep:1025/10000 train_loss:5.4481 train_time:172299ms step_avg:169.75ms\nstep:1026/10000 train_loss:5.4569 train_time:172467ms step_avg:169.75ms\nstep:1027/10000 train_loss:5.3197 train_time:172636ms step_avg:169.75ms\nstep:1028/10000 train_loss:5.4897 train_time:172806ms step_avg:169.75ms\nstep:1029/10000 train_loss:5.4547 train_time:172976ms step_avg:169.75ms\nstep:1030/10000 train_loss:5.6044 train_time:173145ms step_avg:169.75ms\nstep:1031/10000 train_loss:5.4015 train_time:173316ms step_avg:169.75ms\nstep:1032/10000 train_loss:5.3539 train_time:173485ms step_avg:169.75ms\nstep:1033/10000 train_loss:5.6661 train_time:173654ms step_avg:169.75ms\nstep:1034/10000 train_loss:5.2931 train_time:173824ms step_avg:169.75ms\nstep:1035/10000 train_loss:5.3319 train_time:173994ms step_avg:169.75ms\nstep:1036/10000 train_loss:5.3749 train_time:174167ms step_avg:169.75ms\nstep:1037/10000 train_loss:5.3380 train_time:174337ms step_avg:169.75ms\nstep:1038/10000 train_loss:5.4218 train_time:174507ms step_avg:169.75ms\nstep:1039/10000 train_loss:5.3704 train_time:174678ms step_avg:169.76ms\nstep:1040/10000 train_loss:5.4163 train_time:174846ms step_avg:169.75ms\nstep:1041/10000 train_loss:5.5033 train_time:175015ms step_avg:169.75ms\nstep:1042/10000 train_loss:5.3088 train_time:175187ms step_avg:169.76ms\nstep:1043/10000 train_loss:5.4795 train_time:175361ms step_avg:169.76ms\nstep:1044/10000 train_loss:5.4540 train_time:175529ms step_avg:169.76ms\nstep:1045/10000 train_loss:5.2079 train_time:175699ms step_avg:169.76ms\nstep:1046/10000 train_loss:5.3545 train_time:175867ms step_avg:169.76ms\nstep:1047/10000 train_loss:5.2682 train_time:176036ms step_avg:169.76ms\nstep:1048/10000 train_loss:5.4195 train_time:176208ms step_avg:169.76ms\nstep:1049/10000 train_loss:5.5996 train_time:176380ms step_avg:169.76ms\nstep:1050/10000 train_loss:5.4375 train_time:176550ms step_avg:169.76ms\nstep:1051/10000 train_loss:5.4976 train_time:176719ms step_avg:169.76ms\nstep:1052/10000 train_loss:5.3721 train_time:176888ms step_avg:169.76ms\nstep:1053/10000 train_loss:5.4440 train_time:177060ms step_avg:169.76ms\nstep:1054/10000 train_loss:5.4800 train_time:177231ms step_avg:169.76ms\nstep:1055/10000 train_loss:5.3949 train_time:177404ms step_avg:169.76ms\nstep:1056/10000 train_loss:5.3599 train_time:177573ms step_avg:169.76ms\nstep:1057/10000 train_loss:5.4610 train_time:177742ms step_avg:169.76ms\nstep:1058/10000 train_loss:5.8349 train_time:177910ms step_avg:169.76ms\nstep:1059/10000 train_loss:5.3308 train_time:178080ms step_avg:169.76ms\nstep:1060/10000 train_loss:5.2157 train_time:178248ms step_avg:169.76ms\nstep:1061/10000 train_loss:5.1435 train_time:178417ms step_avg:169.76ms\nstep:1062/10000 train_loss:5.4316 train_time:178587ms step_avg:169.76ms\nstep:1063/10000 train_loss:5.5777 train_time:178758ms step_avg:169.76ms\nstep:1064/10000 train_loss:5.2823 train_time:178927ms step_avg:169.76ms\nstep:1065/10000 train_loss:5.2718 train_time:179096ms step_avg:169.76ms\nstep:1066/10000 train_loss:5.4901 train_time:179264ms step_avg:169.76ms\nstep:1067/10000 train_loss:5.4935 train_time:179435ms step_avg:169.76ms\nstep:1068/10000 train_loss:5.3529 train_time:179604ms step_avg:169.76ms\nstep:1069/10000 train_loss:5.1998 train_time:179776ms step_avg:169.76ms\nstep:1070/10000 train_loss:5.3088 train_time:179945ms step_avg:169.76ms\nstep:1071/10000 train_loss:5.4159 train_time:180116ms step_avg:169.76ms\nstep:1072/10000 train_loss:5.2276 train_time:180285ms step_avg:169.76ms\nstep:1073/10000 train_loss:5.4487 train_time:180454ms step_avg:169.76ms\nstep:1074/10000 train_loss:5.2503 train_time:180624ms step_avg:169.76ms\nstep:1075/10000 train_loss:5.2038 train_time:180794ms step_avg:169.76ms\nstep:1076/10000 train_loss:5.3095 train_time:180965ms step_avg:169.76ms\nstep:1077/10000 train_loss:5.3777 train_time:181136ms step_avg:169.76ms\nstep:1078/10000 train_loss:5.4128 train_time:181305ms step_avg:169.76ms\nstep:1079/10000 train_loss:5.2581 train_time:181477ms step_avg:169.76ms\nstep:1080/10000 train_loss:5.4599 train_time:181646ms step_avg:169.76ms\nstep:1081/10000 train_loss:5.1964 train_time:181816ms step_avg:169.76ms\nstep:1082/10000 train_loss:5.4455 train_time:181983ms step_avg:169.76ms\nstep:1083/10000 train_loss:5.3605 train_time:182152ms step_avg:169.76ms\nstep:1084/10000 train_loss:5.4146 train_time:182323ms step_avg:169.76ms\nstep:1085/10000 train_loss:5.4262 train_time:182494ms step_avg:169.76ms\nstep:1086/10000 train_loss:5.3929 train_time:182664ms step_avg:169.76ms\nstep:1087/10000 train_loss:5.3847 train_time:182837ms step_avg:169.77ms\nstep:1088/10000 train_loss:5.2050 train_time:183006ms step_avg:169.76ms\nstep:1089/10000 train_loss:5.2836 train_time:183177ms step_avg:169.77ms\nstep:1090/10000 train_loss:5.2473 train_time:183349ms step_avg:169.77ms\nstep:1091/10000 train_loss:5.3818 train_time:183519ms step_avg:169.77ms\nstep:1092/10000 train_loss:5.2691 train_time:183687ms step_avg:169.77ms\nstep:1093/10000 train_loss:5.3747 train_time:183855ms step_avg:169.76ms\nstep:1094/10000 train_loss:5.5883 train_time:184023ms step_avg:169.76ms\nstep:1095/10000 train_loss:5.2543 train_time:184194ms step_avg:169.76ms\nstep:1096/10000 train_loss:5.4361 train_time:184362ms step_avg:169.76ms\nstep:1097/10000 train_loss:5.4289 train_time:184533ms step_avg:169.76ms\nstep:1098/10000 train_loss:5.2321 train_time:184703ms step_avg:169.76ms\nstep:1099/10000 train_loss:5.1629 train_time:184875ms step_avg:169.77ms\nstep:1100/10000 train_loss:5.4051 train_time:185047ms step_avg:169.77ms\nstep:1101/10000 train_loss:5.1737 train_time:185217ms step_avg:169.77ms\nstep:1102/10000 train_loss:5.3896 train_time:185388ms step_avg:169.77ms\nstep:1103/10000 train_loss:5.3591 train_time:185557ms step_avg:169.77ms\nstep:1104/10000 train_loss:5.6114 train_time:185726ms step_avg:169.77ms\nstep:1105/10000 train_loss:5.3237 train_time:185896ms step_avg:169.77ms\nstep:1106/10000 train_loss:5.4006 train_time:186064ms step_avg:169.77ms\nstep:1107/10000 train_loss:5.2422 train_time:186234ms step_avg:169.77ms\nstep:1108/10000 train_loss:5.3435 train_time:186406ms step_avg:169.77ms\nstep:1109/10000 train_loss:5.3394 train_time:186576ms step_avg:169.77ms\nstep:1110/10000 train_loss:5.2297 train_time:186743ms step_avg:169.77ms\nstep:1111/10000 train_loss:5.3141 train_time:186912ms step_avg:169.77ms\nstep:1112/10000 train_loss:5.5234 train_time:187082ms step_avg:169.77ms\nstep:1113/10000 train_loss:5.2192 train_time:187250ms step_avg:169.76ms\nstep:1114/10000 train_loss:5.5232 train_time:187427ms step_avg:169.77ms\nstep:1115/10000 train_loss:5.2732 train_time:187596ms step_avg:169.77ms\nstep:1116/10000 train_loss:5.3276 train_time:187764ms step_avg:169.77ms\nstep:1117/10000 train_loss:5.2134 train_time:187934ms step_avg:169.77ms\nstep:1118/10000 train_loss:5.3274 train_time:188108ms step_avg:169.77ms\nstep:1119/10000 train_loss:5.3526 train_time:188274ms step_avg:169.77ms\nstep:1120/10000 train_loss:5.2203 train_time:188445ms step_avg:169.77ms\nstep:1121/10000 train_loss:5.4266 train_time:188618ms step_avg:169.77ms\nstep:1122/10000 train_loss:5.2702 train_time:188787ms step_avg:169.77ms\nstep:1123/10000 train_loss:5.1984 train_time:188956ms step_avg:169.77ms\nstep:1124/10000 train_loss:5.3165 train_time:189126ms step_avg:169.77ms\nstep:1125/10000 train_loss:5.2765 train_time:189296ms step_avg:169.77ms\nstep:1126/10000 train_loss:5.3116 train_time:189465ms step_avg:169.77ms\nstep:1127/10000 train_loss:5.3514 train_time:189637ms step_avg:169.77ms\nstep:1128/10000 train_loss:5.4979 train_time:189806ms step_avg:169.77ms\nstep:1129/10000 train_loss:5.3268 train_time:189978ms step_avg:169.77ms\nstep:1130/10000 train_loss:5.4102 train_time:190146ms step_avg:169.77ms\nstep:1131/10000 train_loss:5.2783 train_time:190318ms step_avg:169.78ms\nstep:1132/10000 train_loss:5.2419 train_time:190488ms step_avg:169.78ms\nstep:1133/10000 train_loss:5.2503 train_time:190661ms step_avg:169.78ms\nstep:1134/10000 train_loss:5.3306 train_time:190830ms step_avg:169.78ms\nstep:1135/10000 train_loss:5.2900 train_time:190999ms step_avg:169.78ms\nstep:1136/10000 train_loss:5.3964 train_time:191168ms step_avg:169.78ms\nstep:1137/10000 train_loss:5.1530 train_time:191338ms step_avg:169.78ms\nstep:1138/10000 train_loss:5.2176 train_time:191504ms step_avg:169.77ms\nstep:1139/10000 train_loss:5.3997 train_time:191675ms step_avg:169.77ms\nstep:1140/10000 train_loss:5.3560 train_time:191847ms step_avg:169.78ms\nstep:1141/10000 train_loss:5.3563 train_time:192022ms step_avg:169.78ms\nstep:1142/10000 train_loss:5.3333 train_time:192190ms step_avg:169.78ms\nstep:1143/10000 train_loss:5.2864 train_time:192358ms step_avg:169.78ms\nstep:1144/10000 train_loss:5.4116 train_time:192526ms step_avg:169.78ms\nstep:1145/10000 train_loss:5.3199 train_time:192697ms step_avg:169.78ms\nstep:1146/10000 train_loss:5.2662 train_time:192867ms step_avg:169.78ms\nstep:1147/10000 train_loss:5.2752 train_time:193038ms step_avg:169.78ms\nstep:1148/10000 train_loss:5.3003 train_time:193206ms step_avg:169.78ms\nstep:1149/10000 train_loss:5.2701 train_time:193376ms step_avg:169.78ms\nstep:1150/10000 train_loss:5.2999 train_time:193545ms step_avg:169.78ms\nstep:1151/10000 train_loss:5.2271 train_time:193717ms step_avg:169.78ms\nstep:1152/10000 train_loss:5.3156 train_time:193886ms step_avg:169.78ms\nstep:1153/10000 train_loss:5.3012 train_time:194057ms step_avg:169.78ms\nstep:1154/10000 train_loss:5.4343 train_time:194227ms step_avg:169.78ms\nstep:1155/10000 train_loss:5.3181 train_time:194396ms step_avg:169.78ms\nstep:1156/10000 train_loss:5.1582 train_time:194565ms step_avg:169.78ms\nstep:1157/10000 train_loss:5.0074 train_time:194736ms step_avg:169.78ms\nstep:1158/10000 train_loss:5.4052 train_time:194905ms step_avg:169.78ms\nstep:1159/10000 train_loss:5.1587 train_time:195076ms step_avg:169.78ms\nstep:1160/10000 train_loss:5.1822 train_time:195244ms step_avg:169.78ms\nstep:1161/10000 train_loss:5.3974 train_time:195415ms step_avg:169.78ms\nstep:1162/10000 train_loss:5.2256 train_time:195584ms step_avg:169.78ms\nstep:1163/10000 train_loss:5.2525 train_time:195756ms step_avg:169.78ms\nstep:1164/10000 train_loss:5.3510 train_time:195924ms step_avg:169.78ms\nstep:1165/10000 train_loss:5.2565 train_time:196095ms step_avg:169.78ms\nstep:1166/10000 train_loss:5.1601 train_time:196266ms step_avg:169.78ms\nstep:1167/10000 train_loss:5.2840 train_time:196435ms step_avg:169.78ms\nstep:1168/10000 train_loss:5.1986 train_time:196604ms step_avg:169.78ms\nstep:1169/10000 train_loss:5.3138 train_time:196778ms step_avg:169.78ms\nstep:1170/10000 train_loss:5.3338 train_time:196946ms step_avg:169.78ms\nstep:1171/10000 train_loss:5.2418 train_time:197117ms step_avg:169.78ms\nstep:1172/10000 train_loss:5.2453 train_time:197284ms step_avg:169.78ms\nstep:1173/10000 train_loss:5.3001 train_time:197456ms step_avg:169.78ms\nstep:1174/10000 train_loss:5.1571 train_time:197624ms step_avg:169.78ms\nstep:1175/10000 train_loss:5.1298 train_time:197793ms step_avg:169.78ms\nstep:1176/10000 train_loss:5.2074 train_time:197961ms step_avg:169.78ms\nstep:1177/10000 train_loss:5.4705 train_time:198131ms step_avg:169.78ms\nstep:1178/10000 train_loss:5.3123 train_time:198298ms step_avg:169.78ms\nstep:1179/10000 train_loss:5.4230 train_time:198467ms step_avg:169.78ms\nstep:1180/10000 train_loss:5.1280 train_time:198637ms step_avg:169.78ms\nstep:1181/10000 train_loss:5.0112 train_time:198805ms step_avg:169.77ms\nstep:1182/10000 train_loss:5.3790 train_time:198975ms step_avg:169.77ms\nstep:1183/10000 train_loss:5.3665 train_time:199146ms step_avg:169.77ms\nstep:1184/10000 train_loss:5.0770 train_time:199317ms step_avg:169.78ms\nstep:1185/10000 train_loss:5.2676 train_time:199487ms step_avg:169.78ms\nstep:1186/10000 train_loss:5.1862 train_time:199657ms step_avg:169.78ms\nstep:1187/10000 train_loss:5.3446 train_time:199828ms step_avg:169.78ms\nstep:1188/10000 train_loss:5.1312 train_time:199999ms step_avg:169.78ms\nstep:1189/10000 train_loss:5.4316 train_time:200169ms step_avg:169.78ms\nstep:1190/10000 train_loss:5.3741 train_time:200340ms step_avg:169.78ms\nstep:1191/10000 train_loss:5.2378 train_time:200509ms step_avg:169.78ms\nstep:1192/10000 train_loss:5.2115 train_time:200679ms step_avg:169.78ms\nstep:1193/10000 train_loss:5.3623 train_time:200848ms step_avg:169.78ms\nstep:1194/10000 train_loss:5.2003 train_time:201021ms step_avg:169.78ms\nstep:1195/10000 train_loss:5.1802 train_time:201190ms step_avg:169.78ms\nstep:1196/10000 train_loss:5.1192 train_time:201362ms step_avg:169.78ms\nstep:1197/10000 train_loss:5.1911 train_time:201531ms step_avg:169.78ms\nstep:1198/10000 train_loss:5.4808 train_time:201701ms step_avg:169.78ms\nstep:1199/10000 train_loss:5.0982 train_time:201870ms step_avg:169.78ms\nstep:1200/10000 train_loss:5.3273 train_time:202040ms step_avg:169.78ms\nstep:1201/10000 train_loss:5.2969 train_time:202210ms step_avg:169.78ms\nstep:1202/10000 train_loss:5.2007 train_time:202382ms step_avg:169.78ms\nstep:1203/10000 train_loss:5.1707 train_time:202550ms step_avg:169.78ms\nstep:1204/10000 train_loss:5.2517 train_time:202718ms step_avg:169.78ms\nstep:1205/10000 train_loss:5.1736 train_time:202888ms step_avg:169.78ms\nstep:1206/10000 train_loss:5.1980 train_time:203061ms step_avg:169.78ms\nstep:1207/10000 train_loss:5.2638 train_time:203234ms step_avg:169.79ms\nstep:1208/10000 train_loss:5.1608 train_time:203407ms step_avg:169.79ms\nstep:1209/10000 train_loss:5.2824 train_time:203575ms step_avg:169.79ms\nstep:1210/10000 train_loss:5.2032 train_time:203743ms step_avg:169.79ms\nstep:1211/10000 train_loss:5.2129 train_time:203912ms step_avg:169.78ms\nstep:1212/10000 train_loss:5.2572 train_time:204081ms step_avg:169.78ms\nstep:1213/10000 train_loss:5.1884 train_time:204251ms step_avg:169.78ms\nstep:1214/10000 train_loss:5.2629 train_time:204421ms step_avg:169.78ms\nstep:1215/10000 train_loss:5.1439 train_time:204590ms step_avg:169.78ms\nstep:1216/10000 train_loss:5.2225 train_time:204761ms step_avg:169.79ms\nstep:1217/10000 train_loss:5.2305 train_time:204930ms step_avg:169.78ms\nstep:1218/10000 train_loss:5.2071 train_time:205101ms step_avg:169.79ms\nstep:1219/10000 train_loss:5.2359 train_time:205270ms step_avg:169.79ms\nstep:1220/10000 train_loss:5.6340 train_time:205442ms step_avg:169.79ms\nstep:1221/10000 train_loss:5.0486 train_time:205611ms step_avg:169.79ms\nstep:1222/10000 train_loss:5.2992 train_time:205788ms step_avg:169.79ms\nstep:1223/10000 train_loss:4.9330 train_time:205952ms step_avg:169.79ms\nstep:1224/10000 train_loss:5.1626 train_time:206123ms step_avg:169.79ms\nstep:1225/10000 train_loss:5.1358 train_time:206294ms step_avg:169.79ms\nstep:1226/10000 train_loss:5.2157 train_time:206466ms step_avg:169.79ms\nstep:1227/10000 train_loss:5.3050 train_time:206637ms step_avg:169.79ms\nstep:1228/10000 train_loss:5.0716 train_time:206808ms step_avg:169.79ms\nstep:1229/10000 train_loss:5.2246 train_time:206978ms step_avg:169.79ms\nstep:1230/10000 train_loss:5.1906 train_time:207147ms step_avg:169.79ms\nstep:1231/10000 train_loss:5.2200 train_time:207318ms step_avg:169.79ms\nstep:1232/10000 train_loss:5.3020 train_time:207489ms step_avg:169.79ms\nstep:1233/10000 train_loss:5.0526 train_time:207658ms step_avg:169.79ms\nstep:1234/10000 train_loss:5.3505 train_time:207827ms step_avg:169.79ms\nstep:1235/10000 train_loss:5.0927 train_time:207997ms step_avg:169.79ms\nstep:1236/10000 train_loss:5.0230 train_time:208167ms step_avg:169.79ms\nstep:1237/10000 train_loss:5.0846 train_time:208338ms step_avg:169.79ms\nstep:1238/10000 train_loss:5.1482 train_time:208508ms step_avg:169.79ms\nstep:1239/10000 train_loss:5.3596 train_time:208680ms step_avg:169.80ms\nstep:1240/10000 train_loss:5.2745 train_time:208850ms step_avg:169.80ms\nstep:1241/10000 train_loss:5.2362 train_time:209022ms step_avg:169.80ms\nstep:1242/10000 train_loss:5.2114 train_time:209192ms step_avg:169.80ms\nstep:1243/10000 train_loss:5.2869 train_time:209361ms step_avg:169.80ms\nstep:1244/10000 train_loss:5.2492 train_time:209532ms step_avg:169.80ms\nstep:1245/10000 train_loss:5.2323 train_time:209701ms step_avg:169.80ms\nstep:1246/10000 train_loss:5.1934 train_time:209869ms step_avg:169.80ms\nstep:1247/10000 train_loss:5.3041 train_time:210038ms step_avg:169.80ms\nstep:1248/10000 train_loss:5.1419 train_time:210207ms step_avg:169.80ms\nstep:1249/10000 train_loss:5.1845 train_time:210378ms step_avg:169.80ms\nstep:1250/10000 train_loss:5.0721 train_time:210546ms step_avg:169.80ms\nstep:1251/10000 train_loss:5.1587 train_time:210717ms step_avg:169.80ms\nstep:1252/10000 train_loss:5.3477 train_time:210886ms step_avg:169.80ms\nstep:1253/10000 train_loss:5.2632 train_time:211058ms step_avg:169.80ms\nstep:1254/10000 train_loss:5.0953 train_time:211227ms step_avg:169.80ms\nstep:1255/10000 train_loss:5.2744 train_time:211401ms step_avg:169.80ms\nstep:1256/10000 train_loss:5.3850 train_time:211572ms step_avg:169.80ms\nstep:1257/10000 train_loss:5.1420 train_time:211743ms step_avg:169.80ms\nstep:1258/10000 train_loss:5.2910 train_time:211911ms step_avg:169.80ms\nstep:1259/10000 train_loss:5.1985 train_time:212082ms step_avg:169.80ms\nstep:1260/10000 train_loss:5.2308 train_time:212252ms step_avg:169.80ms\nstep:1261/10000 train_loss:5.2676 train_time:212424ms step_avg:169.80ms\nstep:1262/10000 train_loss:4.8892 train_time:212596ms step_avg:169.80ms\nstep:1263/10000 train_loss:5.2621 train_time:212767ms step_avg:169.81ms\nstep:1264/10000 train_loss:5.1458 train_time:212936ms step_avg:169.81ms\nstep:1265/10000 train_loss:5.2115 train_time:213105ms step_avg:169.80ms\nstep:1266/10000 train_loss:5.1779 train_time:213275ms step_avg:169.81ms\nstep:1267/10000 train_loss:5.1246 train_time:213448ms step_avg:169.81ms\nstep:1268/10000 train_loss:5.1475 train_time:213619ms step_avg:169.81ms\nstep:1269/10000 train_loss:5.0396 train_time:213788ms step_avg:169.81ms\nstep:1270/10000 train_loss:5.1536 train_time:213962ms step_avg:169.81ms\nstep:1271/10000 train_loss:5.1111 train_time:214126ms step_avg:169.81ms\nstep:1272/10000 train_loss:5.0791 train_time:214295ms step_avg:169.81ms\nstep:1273/10000 train_loss:4.9759 train_time:214464ms step_avg:169.81ms\nstep:1274/10000 train_loss:5.0424 train_time:214636ms step_avg:169.81ms\nstep:1275/10000 train_loss:5.0943 train_time:214806ms step_avg:169.81ms\nstep:1276/10000 train_loss:5.0300 train_time:214975ms step_avg:169.81ms\nstep:1277/10000 train_loss:5.2799 train_time:215143ms step_avg:169.81ms\nstep:1278/10000 train_loss:5.1318 train_time:215315ms step_avg:169.81ms\nstep:1279/10000 train_loss:5.2064 train_time:215481ms step_avg:169.80ms\nstep:1280/10000 train_loss:5.0873 train_time:215649ms step_avg:169.80ms\nstep:1281/10000 train_loss:5.2226 train_time:215820ms step_avg:169.80ms\nstep:1282/10000 train_loss:5.0459 train_time:215993ms step_avg:169.81ms\nstep:1283/10000 train_loss:5.1364 train_time:216165ms step_avg:169.81ms\nstep:1284/10000 train_loss:5.1465 train_time:216334ms step_avg:169.81ms\nstep:1285/10000 train_loss:5.0087 train_time:216503ms step_avg:169.81ms\nstep:1286/10000 train_loss:5.1733 train_time:216673ms step_avg:169.81ms\nstep:1287/10000 train_loss:5.2310 train_time:216843ms step_avg:169.81ms\nstep:1288/10000 train_loss:5.0953 train_time:217013ms step_avg:169.81ms\nstep:1289/10000 train_loss:5.1568 train_time:217184ms step_avg:169.81ms\nstep:1290/10000 train_loss:5.0452 train_time:217353ms step_avg:169.81ms\nstep:1291/10000 train_loss:5.1161 train_time:217523ms step_avg:169.81ms\nstep:1292/10000 train_loss:5.1967 train_time:217691ms step_avg:169.81ms\nstep:1293/10000 train_loss:5.1583 train_time:217861ms step_avg:169.81ms\nstep:1294/10000 train_loss:5.1539 train_time:218029ms step_avg:169.80ms\nstep:1295/10000 train_loss:5.1850 train_time:218201ms step_avg:169.81ms\nstep:1296/10000 train_loss:5.2818 train_time:218370ms step_avg:169.81ms\nstep:1297/10000 train_loss:5.0709 train_time:218538ms step_avg:169.80ms\nstep:1298/10000 train_loss:5.1183 train_time:218706ms step_avg:169.80ms\nstep:1299/10000 train_loss:5.1296 train_time:218884ms step_avg:169.81ms\nstep:1300/10000 train_loss:5.2648 train_time:219048ms step_avg:169.80ms\nstep:1301/10000 train_loss:5.2233 train_time:219220ms step_avg:169.81ms\nstep:1302/10000 train_loss:5.0552 train_time:219385ms step_avg:169.80ms\nstep:1303/10000 train_loss:5.2650 train_time:219560ms step_avg:169.81ms\nstep:1304/10000 train_loss:5.1269 train_time:219730ms step_avg:169.81ms\nstep:1305/10000 train_loss:5.1446 train_time:219899ms step_avg:169.81ms\nstep:1306/10000 train_loss:5.1054 train_time:220069ms step_avg:169.81ms\nstep:1307/10000 train_loss:5.2302 train_time:220240ms step_avg:169.81ms\nstep:1308/10000 train_loss:5.5626 train_time:220407ms step_avg:169.81ms\nstep:1309/10000 train_loss:4.9739 train_time:220578ms step_avg:169.81ms\nstep:1310/10000 train_loss:5.1745 train_time:220748ms step_avg:169.81ms\nstep:1311/10000 train_loss:5.2568 train_time:220920ms step_avg:169.81ms\nstep:1312/10000 train_loss:5.2678 train_time:221087ms step_avg:169.81ms\nstep:1313/10000 train_loss:5.2264 train_time:221257ms step_avg:169.81ms\nstep:1314/10000 train_loss:5.2315 train_time:221426ms step_avg:169.81ms\nstep:1315/10000 train_loss:5.1777 train_time:221597ms step_avg:169.81ms\nstep:1316/10000 train_loss:5.1154 train_time:221768ms step_avg:169.81ms\nstep:1317/10000 train_loss:5.1417 train_time:221937ms step_avg:169.81ms\nstep:1318/10000 train_loss:5.0933 train_time:222107ms step_avg:169.81ms\nstep:1319/10000 train_loss:5.3351 train_time:222277ms step_avg:169.81ms\nstep:1320/10000 train_loss:5.1001 train_time:222452ms step_avg:169.81ms\nstep:1321/10000 train_loss:5.1967 train_time:222619ms step_avg:169.81ms\nstep:1322/10000 train_loss:5.0925 train_time:222785ms step_avg:169.81ms\nstep:1323/10000 train_loss:5.0551 train_time:222955ms step_avg:169.81ms\nstep:1324/10000 train_loss:5.1484 train_time:223125ms step_avg:169.81ms\nstep:1325/10000 train_loss:5.0352 train_time:223294ms step_avg:169.81ms\nstep:1326/10000 train_loss:5.6154 train_time:223465ms step_avg:169.81ms\nstep:1327/10000 train_loss:5.0477 train_time:223639ms step_avg:169.81ms\nstep:1328/10000 train_loss:5.0544 train_time:223806ms step_avg:169.81ms\nstep:1329/10000 train_loss:5.0201 train_time:223977ms step_avg:169.81ms\nstep:1330/10000 train_loss:5.0257 train_time:224149ms step_avg:169.81ms\nstep:1331/10000 train_loss:4.9895 train_time:224317ms step_avg:169.81ms\nstep:1332/10000 train_loss:5.0882 train_time:224486ms step_avg:169.81ms\nstep:1333/10000 train_loss:5.0232 train_time:224660ms step_avg:169.81ms\nstep:1334/10000 train_loss:5.0184 train_time:224826ms step_avg:169.81ms\nstep:1335/10000 train_loss:4.9950 train_time:224997ms step_avg:169.81ms\nstep:1336/10000 train_loss:5.0155 train_time:225166ms step_avg:169.81ms\nstep:1337/10000 train_loss:5.0080 train_time:225340ms step_avg:169.81ms\nstep:1338/10000 train_loss:5.1026 train_time:225507ms step_avg:169.81ms\nstep:1339/10000 train_loss:5.1291 train_time:225677ms step_avg:169.81ms\nstep:1340/10000 train_loss:5.2419 train_time:225846ms step_avg:169.81ms\nstep:1341/10000 train_loss:5.1084 train_time:226016ms step_avg:169.81ms\nstep:1342/10000 train_loss:5.1379 train_time:226186ms step_avg:169.81ms\nstep:1343/10000 train_loss:5.0083 train_time:226358ms step_avg:169.81ms\nstep:1344/10000 train_loss:5.1555 train_time:226526ms step_avg:169.81ms\nstep:1345/10000 train_loss:4.9493 train_time:226695ms step_avg:169.81ms\nstep:1346/10000 train_loss:5.0717 train_time:226864ms step_avg:169.81ms\nstep:1347/10000 train_loss:4.9697 train_time:227034ms step_avg:169.81ms\nstep:1348/10000 train_loss:5.6516 train_time:227206ms step_avg:169.81ms\nstep:1349/10000 train_loss:6.2686 train_time:227379ms step_avg:169.81ms\nstep:1350/10000 train_loss:5.1189 train_time:227547ms step_avg:169.81ms\nstep:1351/10000 train_loss:5.2453 train_time:227718ms step_avg:169.81ms\nstep:1352/10000 train_loss:5.1245 train_time:227890ms step_avg:169.81ms\nstep:1353/10000 train_loss:5.1307 train_time:228062ms step_avg:169.82ms\nstep:1354/10000 train_loss:5.0549 train_time:228234ms step_avg:169.82ms\nstep:1355/10000 train_loss:5.2227 train_time:228406ms step_avg:169.82ms\nstep:1356/10000 train_loss:5.1457 train_time:228576ms step_avg:169.82ms\nstep:1357/10000 train_loss:5.0345 train_time:228746ms step_avg:169.82ms\nstep:1358/10000 train_loss:5.0349 train_time:228916ms step_avg:169.82ms\nstep:1359/10000 train_loss:5.0750 train_time:229086ms step_avg:169.82ms\nstep:1360/10000 train_loss:5.0249 train_time:229257ms step_avg:169.82ms\nstep:1361/10000 train_loss:5.0602 train_time:229427ms step_avg:169.82ms\nstep:1362/10000 train_loss:4.9567 train_time:229598ms step_avg:169.82ms\nstep:1363/10000 train_loss:4.9866 train_time:229767ms step_avg:169.82ms\nstep:1364/10000 train_loss:5.0132 train_time:229937ms step_avg:169.82ms\nstep:1365/10000 train_loss:5.1291 train_time:230106ms step_avg:169.82ms\nstep:1366/10000 train_loss:5.0488 train_time:230277ms step_avg:169.82ms\nstep:1367/10000 train_loss:5.0019 train_time:230446ms step_avg:169.82ms\nstep:1368/10000 train_loss:4.9947 train_time:230618ms step_avg:169.82ms\nstep:1369/10000 train_loss:5.0637 train_time:230786ms step_avg:169.82ms\nstep:1370/10000 train_loss:4.9956 train_time:230957ms step_avg:169.82ms\nstep:1371/10000 train_loss:5.0622 train_time:231124ms step_avg:169.82ms\nstep:1372/10000 train_loss:5.0689 train_time:231293ms step_avg:169.82ms\nstep:1373/10000 train_loss:4.9805 train_time:231461ms step_avg:169.82ms\nstep:1374/10000 train_loss:5.1746 train_time:231631ms step_avg:169.82ms\nstep:1375/10000 train_loss:5.0642 train_time:231799ms step_avg:169.82ms\nstep:1376/10000 train_loss:5.1306 train_time:231967ms step_avg:169.81ms\nstep:1377/10000 train_loss:4.8012 train_time:232138ms step_avg:169.82ms\nstep:1378/10000 train_loss:5.0206 train_time:232308ms step_avg:169.82ms\nstep:1379/10000 train_loss:5.1701 train_time:232480ms step_avg:169.82ms\nstep:1380/10000 train_loss:5.0622 train_time:232650ms step_avg:169.82ms\nstep:1381/10000 train_loss:4.9846 train_time:232823ms step_avg:169.82ms\nstep:1382/10000 train_loss:4.9959 train_time:232992ms step_avg:169.82ms\nstep:1383/10000 train_loss:4.9739 train_time:233161ms step_avg:169.82ms\nstep:1384/10000 train_loss:5.0086 train_time:233331ms step_avg:169.82ms\nstep:1385/10000 train_loss:4.9342 train_time:233502ms step_avg:169.82ms\nstep:1386/10000 train_loss:5.0988 train_time:233671ms step_avg:169.82ms\nstep:1387/10000 train_loss:4.9620 train_time:233842ms step_avg:169.82ms\nstep:1388/10000 train_loss:5.0799 train_time:234011ms step_avg:169.82ms\nstep:1389/10000 train_loss:5.0923 train_time:234180ms step_avg:169.82ms\nstep:1390/10000 train_loss:4.9685 train_time:234348ms step_avg:169.82ms\nstep:1391/10000 train_loss:5.0053 train_time:234520ms step_avg:169.82ms\nstep:1392/10000 train_loss:5.0148 train_time:234688ms step_avg:169.82ms\nstep:1393/10000 train_loss:5.0801 train_time:234859ms step_avg:169.82ms\nstep:1394/10000 train_loss:5.0681 train_time:235028ms step_avg:169.82ms\nstep:1395/10000 train_loss:4.9131 train_time:235200ms step_avg:169.82ms\nstep:1396/10000 train_loss:4.9255 train_time:235368ms step_avg:169.82ms\nstep:1397/10000 train_loss:4.7717 train_time:235538ms step_avg:169.82ms\nstep:1398/10000 train_loss:5.0402 train_time:235706ms step_avg:169.82ms\nstep:1399/10000 train_loss:4.9119 train_time:235879ms step_avg:169.82ms\nstep:1400/10000 train_loss:4.5601 train_time:236048ms step_avg:169.82ms\nstep:1401/10000 train_loss:4.9988 train_time:236220ms step_avg:169.82ms\nstep:1402/10000 train_loss:5.1874 train_time:236388ms step_avg:169.82ms\nstep:1403/10000 train_loss:4.9432 train_time:236559ms step_avg:169.82ms\nstep:1404/10000 train_loss:4.8876 train_time:236728ms step_avg:169.82ms\nstep:1405/10000 train_loss:5.0549 train_time:236898ms step_avg:169.82ms\nstep:1406/10000 train_loss:4.9400 train_time:237067ms step_avg:169.82ms\nstep:1407/10000 train_loss:5.0367 train_time:237238ms step_avg:169.82ms\nstep:1408/10000 train_loss:5.2917 train_time:237407ms step_avg:169.82ms\nstep:1409/10000 train_loss:4.9548 train_time:237577ms step_avg:169.82ms\nstep:1410/10000 train_loss:4.9867 train_time:237744ms step_avg:169.82ms\nstep:1411/10000 train_loss:4.9541 train_time:237913ms step_avg:169.82ms\nstep:1412/10000 train_loss:4.8652 train_time:238080ms step_avg:169.81ms\nstep:1413/10000 train_loss:5.0236 train_time:238251ms step_avg:169.82ms\nstep:1414/10000 train_loss:4.9988 train_time:238423ms step_avg:169.82ms\nstep:1415/10000 train_loss:4.9996 train_time:238594ms step_avg:169.82ms\nstep:1416/10000 train_loss:5.0273 train_time:238763ms step_avg:169.82ms\nstep:1417/10000 train_loss:4.8831 train_time:238940ms step_avg:169.82ms\nstep:1418/10000 train_loss:4.9465 train_time:239113ms step_avg:169.82ms\nstep:1419/10000 train_loss:4.8919 train_time:239279ms step_avg:169.82ms\nstep:1420/10000 train_loss:5.0018 train_time:239448ms step_avg:169.82ms\nstep:1421/10000 train_loss:5.0401 train_time:239619ms step_avg:169.82ms\nstep:1422/10000 train_loss:5.0544 train_time:239788ms step_avg:169.82ms\nstep:1423/10000 train_loss:5.0447 train_time:239959ms step_avg:169.82ms\nstep:1424/10000 train_loss:5.0665 train_time:240132ms step_avg:169.82ms\nstep:1425/10000 train_loss:4.9134 train_time:240301ms step_avg:169.82ms\nstep:1426/10000 train_loss:4.9418 train_time:240474ms step_avg:169.83ms\nstep:1427/10000 train_loss:4.8239 train_time:240643ms step_avg:169.83ms\nstep:1428/10000 train_loss:5.1477 train_time:240815ms step_avg:169.83ms\nstep:1429/10000 train_loss:4.9744 train_time:240985ms step_avg:169.83ms\nstep:1430/10000 train_loss:5.0253 train_time:241149ms step_avg:169.82ms\nstep:1431/10000 train_loss:5.2216 train_time:241312ms step_avg:169.82ms\nstep:1432/10000 train_loss:4.9743 train_time:241483ms step_avg:169.82ms\nstep:1433/10000 train_loss:4.8658 train_time:241652ms step_avg:169.82ms\nstep:1434/10000 train_loss:5.0422 train_time:241822ms step_avg:169.82ms\nstep:1435/10000 train_loss:5.1144 train_time:241994ms step_avg:169.82ms\nstep:1436/10000 train_loss:5.2810 train_time:242165ms step_avg:169.82ms\nstep:1437/10000 train_loss:4.9785 train_time:242333ms step_avg:169.82ms\nstep:1438/10000 train_loss:4.9206 train_time:242503ms step_avg:169.82ms\nstep:1439/10000 train_loss:5.1965 train_time:242675ms step_avg:169.82ms\nstep:1440/10000 train_loss:4.7987 train_time:242845ms step_avg:169.82ms\nstep:1441/10000 train_loss:4.9382 train_time:243014ms step_avg:169.82ms\nstep:1442/10000 train_loss:5.2507 train_time:243184ms step_avg:169.82ms\nstep:1443/10000 train_loss:5.0967 train_time:243361ms step_avg:169.83ms\nstep:1444/10000 train_loss:5.1808 train_time:243532ms step_avg:169.83ms\nstep:1445/10000 train_loss:5.0081 train_time:243703ms step_avg:169.83ms\nstep:1446/10000 train_loss:5.1359 train_time:243868ms step_avg:169.82ms\nstep:1447/10000 train_loss:4.8127 train_time:244039ms step_avg:169.83ms\nstep:1448/10000 train_loss:5.1419 train_time:244206ms step_avg:169.82ms\nstep:1449/10000 train_loss:5.0547 train_time:244378ms step_avg:169.82ms\nstep:1450/10000 train_loss:5.0454 train_time:244552ms step_avg:169.83ms\nstep:1451/10000 train_loss:5.0826 train_time:244725ms step_avg:169.83ms\nstep:1452/10000 train_loss:4.9496 train_time:244893ms step_avg:169.83ms\nstep:1453/10000 train_loss:4.9557 train_time:245064ms step_avg:169.83ms\nstep:1454/10000 train_loss:4.9233 train_time:245233ms step_avg:169.83ms\nstep:1455/10000 train_loss:4.9446 train_time:245398ms step_avg:169.83ms\nstep:1456/10000 train_loss:4.9141 train_time:245567ms step_avg:169.82ms\nstep:1457/10000 train_loss:5.6065 train_time:245737ms step_avg:169.83ms\nstep:1458/10000 train_loss:5.1041 train_time:245907ms step_avg:169.82ms\nstep:1459/10000 train_loss:4.9828 train_time:246077ms step_avg:169.83ms\nstep:1460/10000 train_loss:5.0393 train_time:246248ms step_avg:169.83ms\nstep:1461/10000 train_loss:4.9725 train_time:246418ms step_avg:169.83ms\nstep:1462/10000 train_loss:5.0021 train_time:246587ms step_avg:169.83ms\nstep:1463/10000 train_loss:5.0777 train_time:246761ms step_avg:169.83ms\nstep:1464/10000 train_loss:4.9587 train_time:246929ms step_avg:169.83ms\nstep:1465/10000 train_loss:4.8919 train_time:247101ms step_avg:169.83ms\nstep:1466/10000 train_loss:4.9641 train_time:247271ms step_avg:169.83ms\nstep:1467/10000 train_loss:4.9852 train_time:247439ms step_avg:169.83ms\nstep:1468/10000 train_loss:4.9497 train_time:247608ms step_avg:169.83ms\nstep:1469/10000 train_loss:4.9330 train_time:247778ms step_avg:169.83ms\nstep:1470/10000 train_loss:4.9479 train_time:247946ms step_avg:169.83ms\nstep:1471/10000 train_loss:4.9546 train_time:248117ms step_avg:169.83ms\nstep:1472/10000 train_loss:4.9509 train_time:248285ms step_avg:169.83ms\nstep:1473/10000 train_loss:4.9144 train_time:248456ms step_avg:169.83ms\nstep:1474/10000 train_loss:5.0250 train_time:248625ms step_avg:169.83ms\nstep:1475/10000 train_loss:4.9523 train_time:248797ms step_avg:169.83ms\nstep:1476/10000 train_loss:4.8219 train_time:248966ms step_avg:169.83ms\nstep:1477/10000 train_loss:4.9792 train_time:249139ms step_avg:169.83ms\nstep:1478/10000 train_loss:4.9235 train_time:249308ms step_avg:169.83ms\nstep:1479/10000 train_loss:4.8774 train_time:249481ms step_avg:169.83ms\nstep:1480/10000 train_loss:4.9728 train_time:249649ms step_avg:169.83ms\nstep:1481/10000 train_loss:4.9349 train_time:249818ms step_avg:169.83ms\nstep:1482/10000 train_loss:4.7965 train_time:249986ms step_avg:169.83ms\nstep:1483/10000 train_loss:5.0469 train_time:250157ms step_avg:169.83ms\nstep:1484/10000 train_loss:5.0183 train_time:250327ms step_avg:169.83ms\nstep:1485/10000 train_loss:5.0337 train_time:250499ms step_avg:169.83ms\nstep:1486/10000 train_loss:4.9226 train_time:250668ms step_avg:169.83ms\nstep:1487/10000 train_loss:4.9392 train_time:250840ms step_avg:169.83ms\nstep:1488/10000 train_loss:4.7944 train_time:251008ms step_avg:169.83ms\nstep:1489/10000 train_loss:4.8846 train_time:251181ms step_avg:169.83ms\nstep:1490/10000 train_loss:4.9211 train_time:251348ms step_avg:169.83ms\nstep:1491/10000 train_loss:5.1246 train_time:251517ms step_avg:169.83ms\nstep:1492/10000 train_loss:4.9699 train_time:251687ms step_avg:169.83ms\nstep:1493/10000 train_loss:4.9260 train_time:251859ms step_avg:169.83ms\nstep:1494/10000 train_loss:4.9935 train_time:252027ms step_avg:169.83ms\nstep:1495/10000 train_loss:4.8710 train_time:252199ms step_avg:169.83ms\nstep:1496/10000 train_loss:4.9237 train_time:252369ms step_avg:169.83ms\nstep:1497/10000 train_loss:5.0166 train_time:252540ms step_avg:169.83ms\nstep:1498/10000 train_loss:4.9320 train_time:252714ms step_avg:169.83ms\nstep:1499/10000 train_loss:4.8254 train_time:252881ms step_avg:169.83ms\nstep:1500/10000 train_loss:4.9273 train_time:253051ms step_avg:169.83ms\nstep:1501/10000 train_loss:4.8863 train_time:253221ms step_avg:169.83ms\nstep:1502/10000 train_loss:4.9738 train_time:253388ms step_avg:169.83ms\nstep:1503/10000 train_loss:4.9462 train_time:253560ms step_avg:169.83ms\nstep:1504/10000 train_loss:4.9741 train_time:253731ms step_avg:169.83ms\nstep:1505/10000 train_loss:5.0075 train_time:253904ms step_avg:169.84ms\nstep:1506/10000 train_loss:4.8685 train_time:254073ms step_avg:169.83ms\nstep:1507/10000 train_loss:4.9267 train_time:254242ms step_avg:169.83ms\nstep:1508/10000 train_loss:4.8507 train_time:254412ms step_avg:169.83ms\nstep:1509/10000 train_loss:4.8394 train_time:254582ms step_avg:169.83ms\nstep:1510/10000 train_loss:4.2224 train_time:254752ms step_avg:169.83ms\nstep:1511/10000 train_loss:4.8767 train_time:254923ms step_avg:169.84ms\nstep:1512/10000 train_loss:4.9358 train_time:255093ms step_avg:169.84ms\nstep:1513/10000 train_loss:5.0915 train_time:255264ms step_avg:169.84ms\nstep:1514/10000 train_loss:4.7616 train_time:255435ms step_avg:169.84ms\nstep:1515/10000 train_loss:4.7676 train_time:255605ms step_avg:169.84ms\nstep:1516/10000 train_loss:5.0447 train_time:255777ms step_avg:169.84ms\nstep:1517/10000 train_loss:5.0007 train_time:255946ms step_avg:169.84ms\nstep:1518/10000 train_loss:4.8344 train_time:256117ms step_avg:169.84ms\nstep:1519/10000 train_loss:5.1314 train_time:256286ms step_avg:169.84ms\nstep:1520/10000 train_loss:5.0587 train_time:256458ms step_avg:169.84ms\nstep:1521/10000 train_loss:4.8990 train_time:256628ms step_avg:169.84ms\nstep:1522/10000 train_loss:4.9389 train_time:256799ms step_avg:169.84ms\nstep:1523/10000 train_loss:5.0246 train_time:256966ms step_avg:169.84ms\nstep:1524/10000 train_loss:4.8314 train_time:257362ms step_avg:169.99ms\nstep:1525/10000 train_loss:4.8576 train_time:257526ms step_avg:169.98ms\nstep:1526/10000 train_loss:5.0175 train_time:257694ms step_avg:169.98ms\nstep:1527/10000 train_loss:5.0618 train_time:257861ms step_avg:169.98ms\nstep:1528/10000 train_loss:4.9388 train_time:258030ms step_avg:169.98ms\nstep:1529/10000 train_loss:5.1590 train_time:258199ms step_avg:169.98ms\nstep:1530/10000 train_loss:5.0361 train_time:258379ms step_avg:169.99ms\nstep:1531/10000 train_loss:4.9363 train_time:258549ms step_avg:169.99ms\nstep:1532/10000 train_loss:4.8617 train_time:258721ms step_avg:169.99ms\nstep:1533/10000 train_loss:4.9485 train_time:258889ms step_avg:169.99ms\nstep:1534/10000 train_loss:4.8285 train_time:259057ms step_avg:169.98ms\nstep:1535/10000 train_loss:4.7905 train_time:259224ms step_avg:169.98ms\nstep:1536/10000 train_loss:4.9423 train_time:259398ms step_avg:169.99ms\nstep:1537/10000 train_loss:5.0059 train_time:259567ms step_avg:169.98ms\nstep:1538/10000 train_loss:4.9233 train_time:259735ms step_avg:169.98ms\nstep:1539/10000 train_loss:4.9008 train_time:259902ms step_avg:169.98ms\nstep:1540/10000 train_loss:4.7904 train_time:260071ms step_avg:169.98ms\nstep:1541/10000 train_loss:5.0161 train_time:260238ms step_avg:169.98ms\nstep:1542/10000 train_loss:4.8704 train_time:260410ms step_avg:169.98ms\nstep:1543/10000 train_loss:4.9399 train_time:260582ms step_avg:169.98ms\nstep:1544/10000 train_loss:4.9581 train_time:260752ms step_avg:169.98ms\nstep:1545/10000 train_loss:5.0103 train_time:260921ms step_avg:169.98ms\nstep:1546/10000 train_loss:4.9939 train_time:261089ms step_avg:169.98ms\nstep:1547/10000 train_loss:4.9064 train_time:261261ms step_avg:169.98ms\nstep:1548/10000 train_loss:4.8268 train_time:261431ms step_avg:169.98ms\nstep:1549/10000 train_loss:4.9349 train_time:261603ms step_avg:169.98ms\nstep:1550/10000 train_loss:5.2177 train_time:261772ms step_avg:169.98ms\nstep:1551/10000 train_loss:4.9045 train_time:261943ms step_avg:169.98ms\nstep:1552/10000 train_loss:4.9546 train_time:262112ms step_avg:169.98ms\nstep:1553/10000 train_loss:5.0109 train_time:262280ms step_avg:169.98ms\nstep:1554/10000 train_loss:4.8886 train_time:262447ms step_avg:169.98ms\nstep:1555/10000 train_loss:4.8199 train_time:262621ms step_avg:169.98ms\nstep:1556/10000 train_loss:4.8959 train_time:262792ms step_avg:169.98ms\nstep:1557/10000 train_loss:4.8949 train_time:262962ms step_avg:169.98ms\nstep:1558/10000 train_loss:4.7724 train_time:263131ms step_avg:169.98ms\nstep:1559/10000 train_loss:4.9241 train_time:263302ms step_avg:169.98ms\nstep:1560/10000 train_loss:4.8960 train_time:263470ms step_avg:169.98ms\nstep:1561/10000 train_loss:4.7384 train_time:263642ms step_avg:169.98ms\nstep:1562/10000 train_loss:4.9658 train_time:263813ms step_avg:169.98ms\nstep:1563/10000 train_loss:4.9719 train_time:263985ms step_avg:169.98ms\nstep:1564/10000 train_loss:4.8467 train_time:264152ms step_avg:169.98ms\nstep:1565/10000 train_loss:4.8840 train_time:264319ms step_avg:169.98ms\nstep:1566/10000 train_loss:4.8739 train_time:264490ms step_avg:169.98ms\nstep:1567/10000 train_loss:4.9200 train_time:264661ms step_avg:169.98ms\nstep:1568/10000 train_loss:4.9376 train_time:264829ms step_avg:169.98ms\nstep:1569/10000 train_loss:4.8693 train_time:264999ms step_avg:169.98ms\nstep:1570/10000 train_loss:4.9551 train_time:265168ms step_avg:169.98ms\nstep:1571/10000 train_loss:5.0561 train_time:265338ms step_avg:169.98ms\nstep:1572/10000 train_loss:4.8038 train_time:265508ms step_avg:169.98ms\nstep:1573/10000 train_loss:4.8958 train_time:265685ms step_avg:169.98ms\nstep:1574/10000 train_loss:4.7990 train_time:265854ms step_avg:169.98ms\nstep:1575/10000 train_loss:4.9709 train_time:266028ms step_avg:169.99ms\nstep:1576/10000 train_loss:4.8797 train_time:266191ms step_avg:169.98ms\nstep:1577/10000 train_loss:4.9286 train_time:266361ms step_avg:169.98ms\nstep:1578/10000 train_loss:4.9078 train_time:266530ms step_avg:169.98ms\nstep:1579/10000 train_loss:4.9528 train_time:266701ms step_avg:169.98ms\nstep:1580/10000 train_loss:4.8234 train_time:266869ms step_avg:169.98ms\nstep:1581/10000 train_loss:4.9594 train_time:267040ms step_avg:169.98ms\nstep:1582/10000 train_loss:4.7402 train_time:267209ms step_avg:169.98ms\nstep:1583/10000 train_loss:4.8640 train_time:267379ms step_avg:169.98ms\nstep:1584/10000 train_loss:4.9052 train_time:267549ms step_avg:169.98ms\nstep:1585/10000 train_loss:4.8255 train_time:267721ms step_avg:169.98ms\nstep:1586/10000 train_loss:4.8391 train_time:267891ms step_avg:169.98ms\nstep:1587/10000 train_loss:4.7152 train_time:268063ms step_avg:169.98ms\nstep:1588/10000 train_loss:4.6499 train_time:268233ms step_avg:169.98ms\nstep:1589/10000 train_loss:4.7669 train_time:268403ms step_avg:169.98ms\nstep:1590/10000 train_loss:5.0421 train_time:268573ms step_avg:169.98ms\nstep:1591/10000 train_loss:5.0993 train_time:268744ms step_avg:169.98ms\nstep:1592/10000 train_loss:4.8197 train_time:268915ms step_avg:169.98ms\nstep:1593/10000 train_loss:5.0377 train_time:269085ms step_avg:169.98ms\nstep:1594/10000 train_loss:4.9421 train_time:269254ms step_avg:169.98ms\nstep:1595/10000 train_loss:4.8275 train_time:269423ms step_avg:169.98ms\nstep:1596/10000 train_loss:4.8428 train_time:269593ms step_avg:169.98ms\nstep:1597/10000 train_loss:4.9493 train_time:269765ms step_avg:169.98ms\nstep:1598/10000 train_loss:4.8584 train_time:269933ms step_avg:169.98ms\nstep:1599/10000 train_loss:4.7963 train_time:270106ms step_avg:169.98ms\nstep:1600/10000 train_loss:4.8404 train_time:270275ms step_avg:169.98ms\nstep:1601/10000 train_loss:4.8598 train_time:270443ms step_avg:169.98ms\nstep:1602/10000 train_loss:4.9786 train_time:270612ms step_avg:169.98ms\nstep:1603/10000 train_loss:4.9358 train_time:270782ms step_avg:169.98ms\nstep:1604/10000 train_loss:4.9391 train_time:270953ms step_avg:169.98ms\nstep:1605/10000 train_loss:4.8303 train_time:271126ms step_avg:169.98ms\nstep:1606/10000 train_loss:4.9298 train_time:271295ms step_avg:169.98ms\nstep:1607/10000 train_loss:4.9366 train_time:271466ms step_avg:169.98ms\nstep:1608/10000 train_loss:4.8344 train_time:271637ms step_avg:169.99ms\nstep:1609/10000 train_loss:4.8349 train_time:271804ms step_avg:169.98ms\nstep:1610/10000 train_loss:4.9079 train_time:271973ms step_avg:169.98ms\nstep:1611/10000 train_loss:4.8866 train_time:272144ms step_avg:169.98ms\nstep:1612/10000 train_loss:5.1096 train_time:272313ms step_avg:169.98ms\nstep:1613/10000 train_loss:4.8774 train_time:272482ms step_avg:169.98ms\nstep:1614/10000 train_loss:5.0034 train_time:272653ms step_avg:169.98ms\nstep:1615/10000 train_loss:4.7746 train_time:272824ms step_avg:169.98ms\nstep:1616/10000 train_loss:5.0660 train_time:272995ms step_avg:169.98ms\nstep:1617/10000 train_loss:4.9019 train_time:273161ms step_avg:169.98ms\nstep:1618/10000 train_loss:4.8623 train_time:273330ms step_avg:169.98ms\nstep:1619/10000 train_loss:4.8300 train_time:273500ms step_avg:169.98ms\nstep:1620/10000 train_loss:4.9145 train_time:273669ms step_avg:169.98ms\nstep:1621/10000 train_loss:4.8768 train_time:273840ms step_avg:169.98ms\nstep:1622/10000 train_loss:4.8215 train_time:274008ms step_avg:169.98ms\nstep:1623/10000 train_loss:4.8532 train_time:274180ms step_avg:169.98ms\nstep:1624/10000 train_loss:4.8981 train_time:274349ms step_avg:169.98ms\nstep:1625/10000 train_loss:5.2298 train_time:274518ms step_avg:169.98ms\nstep:1626/10000 train_loss:5.0994 train_time:274686ms step_avg:169.98ms\nstep:1627/10000 train_loss:4.7916 train_time:274859ms step_avg:169.98ms\nstep:1628/10000 train_loss:4.8221 train_time:275027ms step_avg:169.98ms\nstep:1629/10000 train_loss:4.9439 train_time:275199ms step_avg:169.98ms\nstep:1630/10000 train_loss:4.8629 train_time:275367ms step_avg:169.98ms\nstep:1631/10000 train_loss:4.8932 train_time:275536ms step_avg:169.98ms\nstep:1632/10000 train_loss:4.7502 train_time:275702ms step_avg:169.98ms\nstep:1633/10000 train_loss:5.0607 train_time:275871ms step_avg:169.98ms\nstep:1634/10000 train_loss:4.8599 train_time:276042ms step_avg:169.98ms\nstep:1635/10000 train_loss:4.8576 train_time:276212ms step_avg:169.98ms\nstep:1636/10000 train_loss:4.8991 train_time:276384ms step_avg:169.98ms\nstep:1637/10000 train_loss:5.0026 train_time:276553ms step_avg:169.98ms\nstep:1638/10000 train_loss:4.8803 train_time:276725ms step_avg:169.98ms\nstep:1639/10000 train_loss:4.8010 train_time:276895ms step_avg:169.98ms\nstep:1640/10000 train_loss:4.7666 train_time:277066ms step_avg:169.98ms\nstep:1641/10000 train_loss:4.7752 train_time:277237ms step_avg:169.98ms\nstep:1642/10000 train_loss:4.7794 train_time:277407ms step_avg:169.98ms\nstep:1643/10000 train_loss:4.8601 train_time:277579ms step_avg:169.98ms\nstep:1644/10000 train_loss:4.8522 train_time:277747ms step_avg:169.98ms\nstep:1645/10000 train_loss:4.7964 train_time:277916ms step_avg:169.98ms\nstep:1646/10000 train_loss:4.8136 train_time:278085ms step_avg:169.98ms\nstep:1647/10000 train_loss:4.8066 train_time:278258ms step_avg:169.98ms\nstep:1648/10000 train_loss:4.8056 train_time:278428ms step_avg:169.98ms\nstep:1649/10000 train_loss:4.9121 train_time:278599ms step_avg:169.98ms\nstep:1650/10000 train_loss:5.2136 train_time:278768ms step_avg:169.98ms\nstep:1651/10000 train_loss:4.8535 train_time:278937ms step_avg:169.98ms\nstep:1652/10000 train_loss:4.8195 train_time:279107ms step_avg:169.98ms\nstep:1653/10000 train_loss:4.9602 train_time:279277ms step_avg:169.98ms\nstep:1654/10000 train_loss:4.6854 train_time:279447ms step_avg:169.98ms\nstep:1655/10000 train_loss:4.7625 train_time:279625ms step_avg:169.98ms\nstep:1656/10000 train_loss:4.8485 train_time:279788ms step_avg:169.98ms\nstep:1657/10000 train_loss:4.8887 train_time:279957ms step_avg:169.98ms\nstep:1658/10000 train_loss:5.0119 train_time:280125ms step_avg:169.98ms\nstep:1659/10000 train_loss:4.9481 train_time:280297ms step_avg:169.98ms\nstep:1660/10000 train_loss:4.8030 train_time:280465ms step_avg:169.98ms\nstep:1661/10000 train_loss:4.8300 train_time:280636ms step_avg:169.98ms\nstep:1662/10000 train_loss:4.7786 train_time:280802ms step_avg:169.98ms\nstep:1663/10000 train_loss:4.9080 train_time:280971ms step_avg:169.98ms\nstep:1664/10000 train_loss:4.7615 train_time:281143ms step_avg:169.98ms\nstep:1665/10000 train_loss:5.0013 train_time:281315ms step_avg:169.98ms\nstep:1666/10000 train_loss:4.7984 train_time:281484ms step_avg:169.98ms\nstep:1667/10000 train_loss:4.4962 train_time:281654ms step_avg:169.98ms\nstep:1668/10000 train_loss:4.8912 train_time:281823ms step_avg:169.98ms\nstep:1669/10000 train_loss:4.8803 train_time:281992ms step_avg:169.98ms\nstep:1670/10000 train_loss:4.9235 train_time:282163ms step_avg:169.98ms\nstep:1671/10000 train_loss:4.8107 train_time:282333ms step_avg:169.98ms\nstep:1672/10000 train_loss:4.6338 train_time:282504ms step_avg:169.98ms\nstep:1673/10000 train_loss:4.7703 train_time:282672ms step_avg:169.98ms\nstep:1674/10000 train_loss:4.7023 train_time:282842ms step_avg:169.98ms\nstep:1675/10000 train_loss:4.8867 train_time:283009ms step_avg:169.98ms\nstep:1676/10000 train_loss:4.9608 train_time:283178ms step_avg:169.97ms\nstep:1677/10000 train_loss:4.5559 train_time:283348ms step_avg:169.97ms\nstep:1678/10000 train_loss:4.7767 train_time:283519ms step_avg:169.98ms\nstep:1679/10000 train_loss:4.7674 train_time:283691ms step_avg:169.98ms\nstep:1680/10000 train_loss:4.9704 train_time:283865ms step_avg:169.98ms\nstep:1681/10000 train_loss:4.6792 train_time:284032ms step_avg:169.98ms\nstep:1682/10000 train_loss:4.8124 train_time:284197ms step_avg:169.97ms\nstep:1683/10000 train_loss:4.8512 train_time:284366ms step_avg:169.97ms\nstep:1684/10000 train_loss:4.8248 train_time:284538ms step_avg:169.97ms\nstep:1685/10000 train_loss:4.8350 train_time:284706ms step_avg:169.97ms\nstep:1686/10000 train_loss:4.6434 train_time:284877ms step_avg:169.97ms\nstep:1687/10000 train_loss:4.7193 train_time:285044ms step_avg:169.97ms\nstep:1688/10000 train_loss:4.8814 train_time:285214ms step_avg:169.97ms\nstep:1689/10000 train_loss:4.7823 train_time:285385ms step_avg:169.97ms\nstep:1690/10000 train_loss:4.7045 train_time:285557ms step_avg:169.97ms\nstep:1691/10000 train_loss:4.7336 train_time:285726ms step_avg:169.97ms\nstep:1692/10000 train_loss:4.6612 train_time:285898ms step_avg:169.97ms\nstep:1693/10000 train_loss:4.7424 train_time:286068ms step_avg:169.97ms\nstep:1694/10000 train_loss:4.8434 train_time:286239ms step_avg:169.98ms\nstep:1695/10000 train_loss:4.7792 train_time:286408ms step_avg:169.98ms\nstep:1696/10000 train_loss:4.7024 train_time:286580ms step_avg:169.98ms\nstep:1697/10000 train_loss:4.8271 train_time:286748ms step_avg:169.98ms\nstep:1698/10000 train_loss:4.7165 train_time:286918ms step_avg:169.98ms\nstep:1699/10000 train_loss:4.8546 train_time:287086ms step_avg:169.97ms\nstep:1700/10000 train_loss:4.6663 train_time:287258ms step_avg:169.97ms\nstep:1701/10000 train_loss:4.8201 train_time:287426ms step_avg:169.97ms\nstep:1702/10000 train_loss:4.8455 train_time:287598ms step_avg:169.98ms\nstep:1703/10000 train_loss:4.7219 train_time:287767ms step_avg:169.97ms\nstep:1704/10000 train_loss:4.8622 train_time:287938ms step_avg:169.98ms\nstep:1705/10000 train_loss:4.7356 train_time:288107ms step_avg:169.97ms\nstep:1706/10000 train_loss:4.7263 train_time:288277ms step_avg:169.97ms\nstep:1707/10000 train_loss:4.8708 train_time:288444ms step_avg:169.97ms\nstep:1708/10000 train_loss:4.8345 train_time:288614ms step_avg:169.97ms\nstep:1709/10000 train_loss:4.6870 train_time:288784ms step_avg:169.97ms\nstep:1710/10000 train_loss:4.8733 train_time:288959ms step_avg:169.98ms\nstep:1711/10000 train_loss:4.8166 train_time:289127ms step_avg:169.97ms\nstep:1712/10000 train_loss:4.7020 train_time:289297ms step_avg:169.97ms\nstep:1713/10000 train_loss:4.7563 train_time:289467ms step_avg:169.97ms\nstep:1714/10000 train_loss:4.8458 train_time:289638ms step_avg:169.98ms\nstep:1715/10000 train_loss:4.7889 train_time:289807ms step_avg:169.97ms\nstep:1716/10000 train_loss:4.8072 train_time:289977ms step_avg:169.97ms\nstep:1717/10000 train_loss:4.6705 train_time:290145ms step_avg:169.97ms\nstep:1718/10000 train_loss:4.7306 train_time:290315ms step_avg:169.97ms\nstep:1719/10000 train_loss:4.6587 train_time:290484ms step_avg:169.97ms\nstep:1720/10000 train_loss:4.7109 train_time:290655ms step_avg:169.97ms\nstep:1721/10000 train_loss:4.7204 train_time:290825ms step_avg:169.97ms\nstep:1722/10000 train_loss:4.7184 train_time:290995ms step_avg:169.97ms\nstep:1723/10000 train_loss:4.8800 train_time:291162ms step_avg:169.97ms\nstep:1724/10000 train_loss:4.6274 train_time:291331ms step_avg:169.97ms\nstep:1725/10000 train_loss:4.6583 train_time:291500ms step_avg:169.97ms\nstep:1726/10000 train_loss:4.7440 train_time:291668ms step_avg:169.97ms\nstep:1727/10000 train_loss:5.0331 train_time:291837ms step_avg:169.97ms\nstep:1728/10000 train_loss:4.8305 train_time:292006ms step_avg:169.97ms\nstep:1729/10000 train_loss:5.0227 train_time:292176ms step_avg:169.97ms\nstep:1730/10000 train_loss:4.7419 train_time:292344ms step_avg:169.97ms\nstep:1731/10000 train_loss:4.7550 train_time:292517ms step_avg:169.97ms\nstep:1732/10000 train_loss:4.8615 train_time:292685ms step_avg:169.97ms\nstep:1733/10000 train_loss:4.6411 train_time:292856ms step_avg:169.97ms\nstep:1734/10000 train_loss:4.8249 train_time:293024ms step_avg:169.97ms\nstep:1735/10000 train_loss:4.7386 train_time:293194ms step_avg:169.97ms\nstep:1736/10000 train_loss:4.7975 train_time:293363ms step_avg:169.97ms\nstep:1737/10000 train_loss:4.7926 train_time:293533ms step_avg:169.97ms\nstep:1738/10000 train_loss:4.7716 train_time:293703ms step_avg:169.97ms\nstep:1739/10000 train_loss:4.8082 train_time:293875ms step_avg:169.97ms\nstep:1740/10000 train_loss:4.7116 train_time:294044ms step_avg:169.97ms\nstep:1741/10000 train_loss:4.7942 train_time:294213ms step_avg:169.97ms\nstep:1742/10000 train_loss:4.7751 train_time:294383ms step_avg:169.97ms\nstep:1743/10000 train_loss:4.8538 train_time:294553ms step_avg:169.97ms\nstep:1744/10000 train_loss:4.8147 train_time:294723ms step_avg:169.97ms\nstep:1745/10000 train_loss:4.7343 train_time:294895ms step_avg:169.97ms\nstep:1746/10000 train_loss:4.7999 train_time:295065ms step_avg:169.97ms\nstep:1747/10000 train_loss:4.8573 train_time:295235ms step_avg:169.97ms\nstep:1748/10000 train_loss:4.8606 train_time:295404ms step_avg:169.97ms\nstep:1749/10000 train_loss:4.7699 train_time:295572ms step_avg:169.97ms\nstep:1750/10000 train_loss:4.7248 train_time:295743ms step_avg:169.97ms\nstep:1751/10000 train_loss:5.1269 train_time:295913ms step_avg:169.97ms\nstep:1752/10000 train_loss:4.7830 train_time:296084ms step_avg:169.97ms\nstep:1753/10000 train_loss:4.8416 train_time:296257ms step_avg:169.97ms\nstep:1754/10000 train_loss:4.7045 train_time:296425ms step_avg:169.97ms\nstep:1755/10000 train_loss:4.7453 train_time:296594ms step_avg:169.97ms\nstep:1756/10000 train_loss:4.8198 train_time:296771ms step_avg:169.97ms\nstep:1757/10000 train_loss:4.7871 train_time:296938ms step_avg:169.97ms\nstep:1758/10000 train_loss:4.6390 train_time:297107ms step_avg:169.97ms\nstep:1759/10000 train_loss:4.8085 train_time:297276ms step_avg:169.97ms\nstep:1760/10000 train_loss:4.6826 train_time:297445ms step_avg:169.97ms\nstep:1761/10000 train_loss:4.6894 train_time:297614ms step_avg:169.97ms\nstep:1762/10000 train_loss:4.8456 train_time:297785ms step_avg:169.97ms\nstep:1763/10000 train_loss:4.7578 train_time:297956ms step_avg:169.97ms\nstep:1764/10000 train_loss:4.7026 train_time:298125ms step_avg:169.97ms\nstep:1765/10000 train_loss:4.4966 train_time:298294ms step_avg:169.97ms\nstep:1766/10000 train_loss:4.7684 train_time:298463ms step_avg:169.97ms\nstep:1767/10000 train_loss:4.6633 train_time:298632ms step_avg:169.97ms\nstep:1768/10000 train_loss:4.7544 train_time:298803ms step_avg:169.97ms\nstep:1769/10000 train_loss:4.7357 train_time:298972ms step_avg:169.97ms\nstep:1770/10000 train_loss:4.7361 train_time:299142ms step_avg:169.97ms\nstep:1771/10000 train_loss:4.6881 train_time:299311ms step_avg:169.97ms\nstep:1772/10000 train_loss:4.5965 train_time:299479ms step_avg:169.97ms\nstep:1773/10000 train_loss:4.7815 train_time:299647ms step_avg:169.96ms\nstep:1774/10000 train_loss:4.6248 train_time:299819ms step_avg:169.97ms\nstep:1775/10000 train_loss:4.6544 train_time:299990ms step_avg:169.97ms\nstep:1776/10000 train_loss:4.7864 train_time:300163ms step_avg:169.97ms\nstep:1777/10000 train_loss:4.7092 train_time:300332ms step_avg:169.97ms\nstep:1778/10000 train_loss:4.8727 train_time:300501ms step_avg:169.97ms\nstep:1779/10000 train_loss:4.7933 train_time:300669ms step_avg:169.97ms\nstep:1780/10000 train_loss:4.9086 train_time:300839ms step_avg:169.97ms\nstep:1781/10000 train_loss:4.7349 train_time:301008ms step_avg:169.96ms\nstep:1782/10000 train_loss:4.7674 train_time:301178ms step_avg:169.97ms\nstep:1783/10000 train_loss:4.8838 train_time:301347ms step_avg:169.96ms\nstep:1784/10000 train_loss:4.6157 train_time:301517ms step_avg:169.96ms\nstep:1785/10000 train_loss:4.7045 train_time:301684ms step_avg:169.96ms\nstep:1786/10000 train_loss:4.7692 train_time:301854ms step_avg:169.96ms\nstep:1787/10000 train_loss:4.7359 train_time:302024ms step_avg:169.96ms\nstep:1788/10000 train_loss:4.7733 train_time:302195ms step_avg:169.96ms\nstep:1789/10000 train_loss:4.8430 train_time:302365ms step_avg:169.96ms\nstep:1790/10000 train_loss:4.7077 train_time:302536ms step_avg:169.96ms\nstep:1791/10000 train_loss:4.6596 train_time:302705ms step_avg:169.96ms\nstep:1792/10000 train_loss:4.7587 train_time:302875ms step_avg:169.96ms\nstep:1793/10000 train_loss:4.8301 train_time:303044ms step_avg:169.96ms\nstep:1794/10000 train_loss:4.7671 train_time:303215ms step_avg:169.96ms\nstep:1795/10000 train_loss:4.9592 train_time:303386ms step_avg:169.96ms\nstep:1796/10000 train_loss:4.6398 train_time:303556ms step_avg:169.96ms\nstep:1797/10000 train_loss:4.8369 train_time:303726ms step_avg:169.96ms\nstep:1798/10000 train_loss:4.6119 train_time:303896ms step_avg:169.96ms\nstep:1799/10000 train_loss:4.7959 train_time:304064ms step_avg:169.96ms\nstep:1800/10000 train_loss:4.6597 train_time:304235ms step_avg:169.96ms\nstep:1801/10000 train_loss:4.8748 train_time:304406ms step_avg:169.96ms\nstep:1802/10000 train_loss:4.6190 train_time:304577ms step_avg:169.96ms\nstep:1803/10000 train_loss:4.8048 train_time:304746ms step_avg:169.96ms\nstep:1804/10000 train_loss:4.7387 train_time:304917ms step_avg:169.96ms\nstep:1805/10000 train_loss:4.7208 train_time:305085ms step_avg:169.96ms\nstep:1806/10000 train_loss:4.5523 train_time:305254ms step_avg:169.96ms\nstep:1807/10000 train_loss:4.7465 train_time:305423ms step_avg:169.96ms\nstep:1808/10000 train_loss:5.0213 train_time:305592ms step_avg:169.96ms\nstep:1809/10000 train_loss:4.8092 train_time:305762ms step_avg:169.96ms\nstep:1810/10000 train_loss:4.6679 train_time:305934ms step_avg:169.96ms\nstep:1811/10000 train_loss:4.7300 train_time:306102ms step_avg:169.96ms\nstep:1812/10000 train_loss:4.8998 train_time:306271ms step_avg:169.96ms\nstep:1813/10000 train_loss:4.7641 train_time:306441ms step_avg:169.96ms\nstep:1814/10000 train_loss:4.6126 train_time:306611ms step_avg:169.96ms\nstep:1815/10000 train_loss:4.7079 train_time:306783ms step_avg:169.96ms\nstep:1816/10000 train_loss:4.6529 train_time:306951ms step_avg:169.96ms\nstep:1817/10000 train_loss:4.9361 train_time:307122ms step_avg:169.96ms\nstep:1818/10000 train_loss:4.6440 train_time:307291ms step_avg:169.96ms\nstep:1819/10000 train_loss:4.4998 train_time:307460ms step_avg:169.96ms\nstep:1820/10000 train_loss:4.6374 train_time:307629ms step_avg:169.96ms\nstep:1821/10000 train_loss:4.8674 train_time:307800ms step_avg:169.96ms\nstep:1822/10000 train_loss:4.8089 train_time:307970ms step_avg:169.96ms\nstep:1823/10000 train_loss:4.5864 train_time:308141ms step_avg:169.96ms\nstep:1824/10000 train_loss:4.7709 train_time:308309ms step_avg:169.96ms\nstep:1825/10000 train_loss:4.8133 train_time:308478ms step_avg:169.96ms\nstep:1826/10000 train_loss:4.6466 train_time:308647ms step_avg:169.96ms\nstep:1827/10000 train_loss:4.6855 train_time:308816ms step_avg:169.96ms\nstep:1828/10000 train_loss:4.7622 train_time:308985ms step_avg:169.96ms\nstep:1829/10000 train_loss:4.7388 train_time:309158ms step_avg:169.96ms\nstep:1830/10000 train_loss:4.7667 train_time:309325ms step_avg:169.96ms\nstep:1831/10000 train_loss:4.5977 train_time:309497ms step_avg:169.96ms\nstep:1832/10000 train_loss:4.7576 train_time:309666ms step_avg:169.96ms\nstep:1833/10000 train_loss:4.4744 train_time:309837ms step_avg:169.96ms\nstep:1834/10000 train_loss:4.7676 train_time:310007ms step_avg:169.96ms\nstep:1835/10000 train_loss:4.6532 train_time:310178ms step_avg:169.96ms\nstep:1836/10000 train_loss:4.7812 train_time:310345ms step_avg:169.96ms\nstep:1837/10000 train_loss:4.6796 train_time:310513ms step_avg:169.96ms\nstep:1838/10000 train_loss:4.5532 train_time:310682ms step_avg:169.96ms\nstep:1839/10000 train_loss:4.5704 train_time:310851ms step_avg:169.96ms\nstep:1840/10000 train_loss:4.6872 train_time:311022ms step_avg:169.96ms\nstep:1841/10000 train_loss:4.6691 train_time:311194ms step_avg:169.96ms\nstep:1842/10000 train_loss:4.7127 train_time:311366ms step_avg:169.96ms\nstep:1843/10000 train_loss:4.7408 train_time:311535ms step_avg:169.96ms\nstep:1844/10000 train_loss:4.7425 train_time:311705ms step_avg:169.96ms\nstep:1845/10000 train_loss:4.6848 train_time:311876ms step_avg:169.96ms\nstep:1846/10000 train_loss:4.6817 train_time:312046ms step_avg:169.96ms\nstep:1847/10000 train_loss:4.7752 train_time:312218ms step_avg:169.96ms\nstep:1848/10000 train_loss:4.6505 train_time:312387ms step_avg:169.96ms\nstep:1849/10000 train_loss:4.6569 train_time:312556ms step_avg:169.96ms\nstep:1850/10000 train_loss:4.5888 train_time:312725ms step_avg:169.96ms\nstep:1851/10000 train_loss:4.7007 train_time:312895ms step_avg:169.96ms\nstep:1852/10000 train_loss:4.4833 train_time:313065ms step_avg:169.96ms\nstep:1853/10000 train_loss:4.5470 train_time:313237ms step_avg:169.96ms\nstep:1854/10000 train_loss:4.7439 train_time:313407ms step_avg:169.96ms\nstep:1855/10000 train_loss:4.7019 train_time:313578ms step_avg:169.96ms\nstep:1856/10000 train_loss:4.6697 train_time:313746ms step_avg:169.96ms\nstep:1857/10000 train_loss:4.7574 train_time:313916ms step_avg:169.96ms\nstep:1858/10000 train_loss:4.6364 train_time:314085ms step_avg:169.96ms\nstep:1859/10000 train_loss:4.7579 train_time:314255ms step_avg:169.96ms\nstep:1860/10000 train_loss:4.7692 train_time:314423ms step_avg:169.96ms\nstep:1861/10000 train_loss:4.7471 train_time:314594ms step_avg:169.96ms\nstep:1862/10000 train_loss:4.6434 train_time:314765ms step_avg:169.96ms\nstep:1863/10000 train_loss:4.6784 train_time:314933ms step_avg:169.96ms\nstep:1864/10000 train_loss:4.6317 train_time:315104ms step_avg:169.96ms\nstep:1865/10000 train_loss:4.7264 train_time:315274ms step_avg:169.96ms\nstep:1866/10000 train_loss:4.7881 train_time:315443ms step_avg:169.96ms\nstep:1867/10000 train_loss:4.6697 train_time:315615ms step_avg:169.96ms\nstep:1868/10000 train_loss:4.6563 train_time:315786ms step_avg:169.96ms\nstep:1869/10000 train_loss:4.8040 train_time:315955ms step_avg:169.96ms\nstep:1870/10000 train_loss:4.8002 train_time:316126ms step_avg:169.96ms\nstep:1871/10000 train_loss:4.5926 train_time:316297ms step_avg:169.96ms\nstep:1872/10000 train_loss:4.6227 train_time:316465ms step_avg:169.96ms\nstep:1873/10000 train_loss:4.6502 train_time:316637ms step_avg:169.96ms\nstep:1874/10000 train_loss:4.7198 train_time:316806ms step_avg:169.96ms\nstep:1875/10000 train_loss:4.5993 train_time:316975ms step_avg:169.96ms\nstep:1876/10000 train_loss:4.8233 train_time:317143ms step_avg:169.96ms\nstep:1877/10000 train_loss:4.7154 train_time:317314ms step_avg:169.96ms\nstep:1878/10000 train_loss:4.6484 train_time:317484ms step_avg:169.96ms\nstep:1879/10000 train_loss:4.6585 train_time:317656ms step_avg:169.96ms\nstep:1880/10000 train_loss:4.7141 train_time:317824ms step_avg:169.96ms\nstep:1881/10000 train_loss:4.6778 train_time:317993ms step_avg:169.96ms\nstep:1882/10000 train_loss:4.7425 train_time:318165ms step_avg:169.96ms\nstep:1883/10000 train_loss:4.8218 train_time:318335ms step_avg:169.96ms\nstep:1884/10000 train_loss:4.5078 train_time:318504ms step_avg:169.96ms\nstep:1885/10000 train_loss:4.4473 train_time:318674ms step_avg:169.96ms\nstep:1886/10000 train_loss:4.7520 train_time:318842ms step_avg:169.96ms\nstep:1887/10000 train_loss:5.0249 train_time:319011ms step_avg:169.96ms\nstep:1888/10000 train_loss:4.7193 train_time:319179ms step_avg:169.96ms\nstep:1889/10000 train_loss:4.6570 train_time:319349ms step_avg:169.96ms\nstep:1890/10000 train_loss:4.6804 train_time:319520ms step_avg:169.96ms\nstep:1891/10000 train_loss:4.6538 train_time:319691ms step_avg:169.96ms\nstep:1892/10000 train_loss:4.6270 train_time:319863ms step_avg:169.96ms\nstep:1893/10000 train_loss:4.7160 train_time:320033ms step_avg:169.96ms\nstep:1894/10000 train_loss:4.6491 train_time:320203ms step_avg:169.96ms\nstep:1895/10000 train_loss:4.6975 train_time:320374ms step_avg:169.96ms\nstep:1896/10000 train_loss:4.7484 train_time:320545ms step_avg:169.96ms\nstep:1897/10000 train_loss:4.6181 train_time:320716ms step_avg:169.96ms\nstep:1898/10000 train_loss:4.5281 train_time:320885ms step_avg:169.96ms\nstep:1899/10000 train_loss:4.7837 train_time:321054ms step_avg:169.96ms\nstep:1900/10000 train_loss:4.7139 train_time:321224ms step_avg:169.96ms\nstep:1901/10000 train_loss:4.6771 train_time:321395ms step_avg:169.96ms\nstep:1902/10000 train_loss:4.9656 train_time:321564ms step_avg:169.96ms\nstep:1903/10000 train_loss:4.5767 train_time:321735ms step_avg:169.96ms\nstep:1904/10000 train_loss:4.8989 train_time:321906ms step_avg:169.96ms\nstep:1905/10000 train_loss:4.7447 train_time:322076ms step_avg:169.96ms\nstep:1906/10000 train_loss:4.7025 train_time:322243ms step_avg:169.96ms\nstep:1907/10000 train_loss:4.8606 train_time:322413ms step_avg:169.96ms\nstep:1908/10000 train_loss:4.7073 train_time:322585ms step_avg:169.96ms\nstep:1909/10000 train_loss:4.4007 train_time:322757ms step_avg:169.96ms\nstep:1910/10000 train_loss:4.5562 train_time:322928ms step_avg:169.96ms\nstep:1911/10000 train_loss:4.6285 train_time:323099ms step_avg:169.96ms\nstep:1912/10000 train_loss:4.7497 train_time:323267ms step_avg:169.96ms\nstep:1913/10000 train_loss:4.5963 train_time:323436ms step_avg:169.96ms\nstep:1914/10000 train_loss:4.6593 train_time:323605ms step_avg:169.96ms\nstep:1915/10000 train_loss:4.6037 train_time:323777ms step_avg:169.96ms\nstep:1916/10000 train_loss:4.6503 train_time:323948ms step_avg:169.96ms\nstep:1917/10000 train_loss:4.6490 train_time:324120ms step_avg:169.96ms\nstep:1918/10000 train_loss:4.6565 train_time:324287ms step_avg:169.96ms\nstep:1919/10000 train_loss:4.5966 train_time:324456ms step_avg:169.96ms\nstep:1920/10000 train_loss:4.5372 train_time:324627ms step_avg:169.96ms\nstep:1921/10000 train_loss:4.7922 train_time:324797ms step_avg:169.96ms\nstep:1922/10000 train_loss:4.7896 train_time:324965ms step_avg:169.96ms\nstep:1923/10000 train_loss:4.6245 train_time:325136ms step_avg:169.96ms\nstep:1924/10000 train_loss:4.6693 train_time:325307ms step_avg:169.96ms\nstep:1925/10000 train_loss:4.7839 train_time:325479ms step_avg:169.96ms\nstep:1926/10000 train_loss:4.6390 train_time:325649ms step_avg:169.96ms\nstep:1927/10000 train_loss:4.5503 train_time:325820ms step_avg:169.96ms\nstep:1928/10000 train_loss:4.7772 train_time:325991ms step_avg:169.96ms\nstep:1929/10000 train_loss:4.5771 train_time:326162ms step_avg:169.96ms\nstep:1930/10000 train_loss:4.4371 train_time:326333ms step_avg:169.97ms\nstep:1931/10000 train_loss:4.5650 train_time:326503ms step_avg:169.96ms\nstep:1932/10000 train_loss:4.6209 train_time:326672ms step_avg:169.96ms\nstep:1933/10000 train_loss:4.4805 train_time:326843ms step_avg:169.97ms\nstep:1934/10000 train_loss:4.6834 train_time:327014ms step_avg:169.97ms\nstep:1935/10000 train_loss:4.8372 train_time:327185ms step_avg:169.97ms\nstep:1936/10000 train_loss:4.6979 train_time:327354ms step_avg:169.97ms\nstep:1937/10000 train_loss:4.6230 train_time:327524ms step_avg:169.97ms\nstep:1938/10000 train_loss:4.5834 train_time:327694ms step_avg:169.97ms\nstep:1939/10000 train_loss:4.6883 train_time:327863ms step_avg:169.97ms\nstep:1940/10000 train_loss:4.6477 train_time:328034ms step_avg:169.97ms\nstep:1941/10000 train_loss:4.6967 train_time:328206ms step_avg:169.97ms\nstep:1942/10000 train_loss:4.6197 train_time:328376ms step_avg:169.97ms\nstep:1943/10000 train_loss:4.5650 train_time:328544ms step_avg:169.97ms\nstep:1944/10000 train_loss:4.5051 train_time:328713ms step_avg:169.97ms\nstep:1945/10000 train_loss:4.6960 train_time:328884ms step_avg:169.97ms\nstep:1946/10000 train_loss:4.5579 train_time:329055ms step_avg:169.97ms\nstep:1947/10000 train_loss:4.6410 train_time:329222ms step_avg:169.96ms\nstep:1948/10000 train_loss:4.6790 train_time:329390ms step_avg:169.96ms\nstep:1949/10000 train_loss:4.5975 train_time:329560ms step_avg:169.96ms\nstep:1950/10000 train_loss:4.5348 train_time:329735ms step_avg:169.97ms\nstep:1951/10000 train_loss:4.5883 train_time:329905ms step_avg:169.97ms\nstep:1952/10000 train_loss:4.6584 train_time:330073ms step_avg:169.97ms\nstep:1953/10000 train_loss:4.5637 train_time:330246ms step_avg:169.97ms\nstep:1954/10000 train_loss:4.7698 train_time:330417ms step_avg:169.97ms\nstep:1955/10000 train_loss:4.5987 train_time:330581ms step_avg:169.96ms\nstep:1956/10000 train_loss:4.6055 train_time:330750ms step_avg:169.96ms\nstep:1957/10000 train_loss:4.5678 train_time:330922ms step_avg:169.96ms\nstep:1958/10000 train_loss:4.6208 train_time:331091ms step_avg:169.96ms\nstep:1959/10000 train_loss:4.6480 train_time:331263ms step_avg:169.97ms\nstep:1960/10000 train_loss:4.7821 train_time:331434ms step_avg:169.97ms\nstep:1961/10000 train_loss:4.5738 train_time:331603ms step_avg:169.97ms\nstep:1962/10000 train_loss:4.5302 train_time:331772ms step_avg:169.97ms\nstep:1963/10000 train_loss:4.4729 train_time:331941ms step_avg:169.96ms\nstep:1964/10000 train_loss:4.6006 train_time:332110ms step_avg:169.96ms\nstep:1965/10000 train_loss:4.5828 train_time:332280ms step_avg:169.96ms\nstep:1966/10000 train_loss:4.5172 train_time:332450ms step_avg:169.96ms\nstep:1967/10000 train_loss:4.8485 train_time:332621ms step_avg:169.96ms\nstep:1968/10000 train_loss:4.5476 train_time:332791ms step_avg:169.96ms\nstep:1969/10000 train_loss:4.4980 train_time:332964ms step_avg:169.97ms\nstep:1970/10000 train_loss:4.6473 train_time:333134ms step_avg:169.97ms\nstep:1971/10000 train_loss:4.6698 train_time:333304ms step_avg:169.97ms\nstep:1972/10000 train_loss:4.6851 train_time:333473ms step_avg:169.97ms\nstep:1973/10000 train_loss:4.7591 train_time:333642ms step_avg:169.97ms\nstep:1974/10000 train_loss:4.4724 train_time:333813ms step_avg:169.97ms\nstep:1975/10000 train_loss:4.5396 train_time:333984ms step_avg:169.97ms\nstep:1976/10000 train_loss:4.4604 train_time:334152ms step_avg:169.97ms\nstep:1977/10000 train_loss:4.7117 train_time:334322ms step_avg:169.97ms\nstep:1978/10000 train_loss:4.6125 train_time:334490ms step_avg:169.96ms\nstep:1979/10000 train_loss:4.5516 train_time:334660ms step_avg:169.96ms\nstep:1980/10000 train_loss:4.6385 train_time:334829ms step_avg:169.96ms\nstep:1981/10000 train_loss:4.6132 train_time:335001ms step_avg:169.97ms\nstep:1982/10000 train_loss:4.5154 train_time:335168ms step_avg:169.96ms\nstep:1983/10000 train_loss:4.6440 train_time:335340ms step_avg:169.96ms\nstep:1984/10000 train_loss:4.8052 train_time:335508ms step_avg:169.96ms\nstep:1985/10000 train_loss:4.6778 train_time:335680ms step_avg:169.96ms\nstep:1986/10000 train_loss:4.5532 train_time:335848ms step_avg:169.96ms\nstep:1987/10000 train_loss:4.6618 train_time:336020ms step_avg:169.96ms\nstep:1988/10000 train_loss:4.5905 train_time:336190ms step_avg:169.96ms\nstep:1989/10000 train_loss:4.5954 train_time:336362ms step_avg:169.97ms\nstep:1990/10000 train_loss:4.6296 train_time:336532ms step_avg:169.97ms\nstep:1991/10000 train_loss:4.6058 train_time:336702ms step_avg:169.97ms\nstep:1992/10000 train_loss:4.5424 train_time:336872ms step_avg:169.97ms\nstep:1993/10000 train_loss:5.0338 train_time:337042ms step_avg:169.97ms\nstep:1994/10000 train_loss:4.7220 train_time:337211ms step_avg:169.97ms\nstep:1995/10000 train_loss:4.5925 train_time:337382ms step_avg:169.97ms\nstep:1996/10000 train_loss:4.5594 train_time:337552ms step_avg:169.97ms\nstep:1997/10000 train_loss:4.5183 train_time:337725ms step_avg:169.97ms\nstep:1998/10000 train_loss:4.7666 train_time:337895ms step_avg:169.97ms\nstep:1999/10000 train_loss:4.6929 train_time:338065ms step_avg:169.97ms\nstep:2000/10000 train_loss:4.6935 train_time:338234ms step_avg:169.97ms\nstep:2001/10000 train_loss:4.6653 train_time:338404ms step_avg:169.97ms\nstep:2002/10000 train_loss:4.6458 train_time:338574ms step_avg:169.97ms\nstep:2003/10000 train_loss:4.6487 train_time:338742ms step_avg:169.97ms\nstep:2004/10000 train_loss:4.6251 train_time:338910ms step_avg:169.97ms\nstep:2005/10000 train_loss:4.6096 train_time:339084ms step_avg:169.97ms\nstep:2006/10000 train_loss:4.6320 train_time:339253ms step_avg:169.97ms\nstep:2007/10000 train_loss:4.5826 train_time:339423ms step_avg:169.97ms\nstep:2008/10000 train_loss:4.4884 train_time:339593ms step_avg:169.97ms\nstep:2009/10000 train_loss:5.2223 train_time:339761ms step_avg:169.97ms\nstep:2010/10000 train_loss:4.6196 train_time:339929ms step_avg:169.96ms\nstep:2011/10000 train_loss:4.6306 train_time:340099ms step_avg:169.96ms\nstep:2012/10000 train_loss:4.5720 train_time:340266ms step_avg:169.96ms\nstep:2013/10000 train_loss:4.5860 train_time:340434ms step_avg:169.96ms\nstep:2014/10000 train_loss:4.6222 train_time:340605ms step_avg:169.96ms\nstep:2015/10000 train_loss:4.4859 train_time:340777ms step_avg:169.96ms\nstep:2016/10000 train_loss:4.5077 train_time:340946ms step_avg:169.96ms\nstep:2017/10000 train_loss:4.4875 train_time:341118ms step_avg:169.96ms\nstep:2018/10000 train_loss:4.6352 train_time:341287ms step_avg:169.96ms\nstep:2019/10000 train_loss:4.6762 train_time:341458ms step_avg:169.96ms\nstep:2020/10000 train_loss:4.8728 train_time:341627ms step_avg:169.96ms\nstep:2021/10000 train_loss:4.4071 train_time:341796ms step_avg:169.96ms\nstep:2022/10000 train_loss:4.7507 train_time:341965ms step_avg:169.96ms\nstep:2023/10000 train_loss:4.4191 train_time:342135ms step_avg:169.96ms\nstep:2024/10000 train_loss:4.5970 train_time:342304ms step_avg:169.96ms\nstep:2025/10000 train_loss:4.5576 train_time:342476ms step_avg:169.96ms\nstep:2026/10000 train_loss:4.4651 train_time:342645ms step_avg:169.96ms\nstep:2027/10000 train_loss:4.5209 train_time:342815ms step_avg:169.96ms\nstep:2028/10000 train_loss:4.6361 train_time:342982ms step_avg:169.96ms\nstep:2029/10000 train_loss:4.3977 train_time:343150ms step_avg:169.96ms\nstep:2030/10000 train_loss:4.6290 train_time:343321ms step_avg:169.96ms\nstep:2031/10000 train_loss:4.6282 train_time:343491ms step_avg:169.96ms\nstep:2032/10000 train_loss:4.7987 train_time:343661ms step_avg:169.96ms\nstep:2033/10000 train_loss:4.7013 train_time:343833ms step_avg:169.96ms\nstep:2034/10000 train_loss:4.7130 train_time:344004ms step_avg:169.96ms\nstep:2035/10000 train_loss:4.5087 train_time:344174ms step_avg:169.96ms\nstep:2036/10000 train_loss:4.5044 train_time:344342ms step_avg:169.96ms\nstep:2037/10000 train_loss:4.6235 train_time:344512ms step_avg:169.96ms\nstep:2038/10000 train_loss:4.6466 train_time:344680ms step_avg:169.96ms\nstep:2039/10000 train_loss:4.7016 train_time:344850ms step_avg:169.96ms\nstep:2040/10000 train_loss:4.6193 train_time:345020ms step_avg:169.96ms\nstep:2041/10000 train_loss:4.5781 train_time:345189ms step_avg:169.96ms\nstep:2042/10000 train_loss:4.4694 train_time:345360ms step_avg:169.96ms\nstep:2043/10000 train_loss:4.5379 train_time:345529ms step_avg:169.96ms\nstep:2044/10000 train_loss:4.6191 train_time:345700ms step_avg:169.96ms\nstep:2045/10000 train_loss:4.5680 train_time:345871ms step_avg:169.96ms\nstep:2046/10000 train_loss:4.6572 train_time:346042ms step_avg:169.96ms\nstep:2047/10000 train_loss:4.5796 train_time:346210ms step_avg:169.96ms\nstep:2048/10000 train_loss:4.4471 train_time:346381ms step_avg:169.96ms\nstep:2049/10000 train_loss:4.5164 train_time:346549ms step_avg:169.96ms\nstep:2050/10000 train_loss:4.5708 train_time:346719ms step_avg:169.96ms\nstep:2051/10000 train_loss:4.5692 train_time:346887ms step_avg:169.96ms\nstep:2052/10000 train_loss:4.4380 train_time:347058ms step_avg:169.96ms\nstep:2053/10000 train_loss:4.6387 train_time:347226ms step_avg:169.96ms\nstep:2054/10000 train_loss:4.5848 train_time:347395ms step_avg:169.96ms\nstep:2055/10000 train_loss:4.5306 train_time:347564ms step_avg:169.96ms\nstep:2056/10000 train_loss:4.6626 train_time:347732ms step_avg:169.96ms\nstep:2057/10000 train_loss:4.5365 train_time:347900ms step_avg:169.96ms\nstep:2058/10000 train_loss:4.5927 train_time:348071ms step_avg:169.96ms\nstep:2059/10000 train_loss:4.5097 train_time:348243ms step_avg:169.96ms\nstep:2060/10000 train_loss:4.8375 train_time:348412ms step_avg:169.96ms\nstep:2061/10000 train_loss:4.6880 train_time:348584ms step_avg:169.96ms\nstep:2062/10000 train_loss:4.6636 train_time:348754ms step_avg:169.96ms\nstep:2063/10000 train_loss:4.6010 train_time:348922ms step_avg:169.96ms\nstep:2064/10000 train_loss:4.6307 train_time:349093ms step_avg:169.96ms\nstep:2065/10000 train_loss:4.6448 train_time:349264ms step_avg:169.96ms\nstep:2066/10000 train_loss:5.0104 train_time:349435ms step_avg:169.96ms\nstep:2067/10000 train_loss:4.7237 train_time:349605ms step_avg:169.96ms\nstep:2068/10000 train_loss:4.4807 train_time:349777ms step_avg:169.96ms\nstep:2069/10000 train_loss:4.5936 train_time:349946ms step_avg:169.96ms\nstep:2070/10000 train_loss:4.5707 train_time:350117ms step_avg:169.96ms\nstep:2071/10000 train_loss:4.5923 train_time:350285ms step_avg:169.96ms\nstep:2072/10000 train_loss:4.6638 train_time:350458ms step_avg:169.96ms\nstep:2073/10000 train_loss:4.6264 train_time:350626ms step_avg:169.96ms\nstep:2074/10000 train_loss:4.6860 train_time:350795ms step_avg:169.96ms\nstep:2075/10000 train_loss:4.4186 train_time:350962ms step_avg:169.96ms\nstep:2076/10000 train_loss:4.4134 train_time:351133ms step_avg:169.96ms\nstep:2077/10000 train_loss:4.6733 train_time:351302ms step_avg:169.96ms\nstep:2078/10000 train_loss:4.6286 train_time:351471ms step_avg:169.96ms\nstep:2079/10000 train_loss:4.5067 train_time:351641ms step_avg:169.96ms\nstep:2080/10000 train_loss:4.6227 train_time:351810ms step_avg:169.96ms\nstep:2081/10000 train_loss:4.5521 train_time:351979ms step_avg:169.96ms\nstep:2082/10000 train_loss:4.6034 train_time:352148ms step_avg:169.96ms\nstep:2083/10000 train_loss:4.5173 train_time:352320ms step_avg:169.96ms\nstep:2084/10000 train_loss:4.5089 train_time:352489ms step_avg:169.96ms\nstep:2085/10000 train_loss:4.5436 train_time:352660ms step_avg:169.96ms\nstep:2086/10000 train_loss:4.4551 train_time:352831ms step_avg:169.96ms\nstep:2087/10000 train_loss:4.6232 train_time:353002ms step_avg:169.96ms\nstep:2088/10000 train_loss:4.3868 train_time:353170ms step_avg:169.96ms\nstep:2089/10000 train_loss:4.5979 train_time:353341ms step_avg:169.96ms\nstep:2090/10000 train_loss:4.5218 train_time:353510ms step_avg:169.96ms\nstep:2091/10000 train_loss:4.5767 train_time:353678ms step_avg:169.96ms\nstep:2092/10000 train_loss:4.6441 train_time:353845ms step_avg:169.95ms\nstep:2093/10000 train_loss:4.6787 train_time:354015ms step_avg:169.95ms\nstep:2094/10000 train_loss:5.0957 train_time:354182ms step_avg:169.95ms\nstep:2095/10000 train_loss:4.4484 train_time:354352ms step_avg:169.95ms\nstep:2096/10000 train_loss:4.4370 train_time:354523ms step_avg:169.95ms\nstep:2097/10000 train_loss:4.3989 train_time:354694ms step_avg:169.95ms\nstep:2098/10000 train_loss:4.6143 train_time:354864ms step_avg:169.95ms\nstep:2099/10000 train_loss:4.5247 train_time:355034ms step_avg:169.95ms\nstep:2100/10000 train_loss:4.5126 train_time:355204ms step_avg:169.95ms\nstep:2101/10000 train_loss:4.5444 train_time:355374ms step_avg:169.95ms\nstep:2102/10000 train_loss:4.5622 train_time:355541ms step_avg:169.95ms\nstep:2103/10000 train_loss:4.5125 train_time:355710ms step_avg:169.95ms\nstep:2104/10000 train_loss:4.5497 train_time:355880ms step_avg:169.95ms\nstep:2105/10000 train_loss:4.3378 train_time:356049ms step_avg:169.95ms\nstep:2106/10000 train_loss:4.5448 train_time:356218ms step_avg:169.95ms\nstep:2107/10000 train_loss:4.7069 train_time:356388ms step_avg:169.95ms\nstep:2108/10000 train_loss:4.7675 train_time:356560ms step_avg:169.95ms\nstep:2109/10000 train_loss:4.4444 train_time:356730ms step_avg:169.95ms\nstep:2110/10000 train_loss:4.4785 train_time:356899ms step_avg:169.95ms\nstep:2111/10000 train_loss:4.5561 train_time:357068ms step_avg:169.95ms\nstep:2112/10000 train_loss:4.4998 train_time:357238ms step_avg:169.95ms\nstep:2113/10000 train_loss:4.5698 train_time:357408ms step_avg:169.95ms\nstep:2114/10000 train_loss:4.4763 train_time:357579ms step_avg:169.95ms\nstep:2115/10000 train_loss:4.6574 train_time:357748ms step_avg:169.95ms\nstep:2116/10000 train_loss:4.4807 train_time:357923ms step_avg:169.95ms\nstep:2117/10000 train_loss:4.4996 train_time:358094ms step_avg:169.95ms\nstep:2118/10000 train_loss:4.6288 train_time:358264ms step_avg:169.95ms\nstep:2119/10000 train_loss:4.6510 train_time:358433ms step_avg:169.95ms\nstep:2120/10000 train_loss:4.5287 train_time:358602ms step_avg:169.95ms\nstep:2121/10000 train_loss:4.5734 train_time:358771ms step_avg:169.95ms\nstep:2122/10000 train_loss:4.4837 train_time:358942ms step_avg:169.95ms\nstep:2123/10000 train_loss:4.5478 train_time:359114ms step_avg:169.95ms\nstep:2124/10000 train_loss:4.6407 train_time:359283ms step_avg:169.95ms\nstep:2125/10000 train_loss:4.4553 train_time:359453ms step_avg:169.95ms\nstep:2126/10000 train_loss:4.7437 train_time:359622ms step_avg:169.95ms\nstep:2127/10000 train_loss:4.4200 train_time:359789ms step_avg:169.95ms\nstep:2128/10000 train_loss:4.5281 train_time:359959ms step_avg:169.95ms\nstep:2129/10000 train_loss:4.3445 train_time:360127ms step_avg:169.95ms\nstep:2130/10000 train_loss:4.5437 train_time:360299ms step_avg:169.95ms\nstep:2131/10000 train_loss:4.6443 train_time:360467ms step_avg:169.95ms\nstep:2132/10000 train_loss:4.5451 train_time:360637ms step_avg:169.95ms\nstep:2133/10000 train_loss:4.2889 train_time:360805ms step_avg:169.95ms\nstep:2134/10000 train_loss:4.5343 train_time:360975ms step_avg:169.95ms\nstep:2135/10000 train_loss:4.5879 train_time:361143ms step_avg:169.95ms\nstep:2136/10000 train_loss:4.6266 train_time:361312ms step_avg:169.95ms\nstep:2137/10000 train_loss:4.5552 train_time:361483ms step_avg:169.95ms\nstep:2138/10000 train_loss:4.5334 train_time:361652ms step_avg:169.95ms\nstep:2139/10000 train_loss:4.5895 train_time:361823ms step_avg:169.95ms\nstep:2140/10000 train_loss:4.6205 train_time:361993ms step_avg:169.95ms\nstep:2141/10000 train_loss:4.5488 train_time:362163ms step_avg:169.95ms\nstep:2142/10000 train_loss:4.4702 train_time:362334ms step_avg:169.95ms\nstep:2143/10000 train_loss:4.5119 train_time:362504ms step_avg:169.95ms\nstep:2144/10000 train_loss:4.5675 train_time:362675ms step_avg:169.95ms\nstep:2145/10000 train_loss:4.8358 train_time:362845ms step_avg:169.95ms\nstep:2146/10000 train_loss:4.5586 train_time:363014ms step_avg:169.95ms\nstep:2147/10000 train_loss:4.4062 train_time:363181ms step_avg:169.95ms\nstep:2148/10000 train_loss:4.5643 train_time:363349ms step_avg:169.95ms\nstep:2149/10000 train_loss:4.3897 train_time:363518ms step_avg:169.95ms\nstep:2150/10000 train_loss:4.5608 train_time:363686ms step_avg:169.95ms\nstep:2151/10000 train_loss:4.4411 train_time:363857ms step_avg:169.95ms\nstep:2152/10000 train_loss:4.4293 train_time:364026ms step_avg:169.95ms\nstep:2153/10000 train_loss:4.6078 train_time:364197ms step_avg:169.95ms\nstep:2154/10000 train_loss:4.5076 train_time:364367ms step_avg:169.95ms\nstep:2155/10000 train_loss:4.4181 train_time:364537ms step_avg:169.95ms\nstep:2156/10000 train_loss:4.3919 train_time:364706ms step_avg:169.95ms\nstep:2157/10000 train_loss:4.5982 train_time:364877ms step_avg:169.95ms\nstep:2158/10000 train_loss:4.9496 train_time:365046ms step_avg:169.95ms\nstep:2159/10000 train_loss:4.4466 train_time:365217ms step_avg:169.95ms\nstep:2160/10000 train_loss:4.4703 train_time:365387ms step_avg:169.95ms\nstep:2161/10000 train_loss:4.5644 train_time:365559ms step_avg:169.95ms\nstep:2162/10000 train_loss:4.5266 train_time:365729ms step_avg:169.95ms\nstep:2163/10000 train_loss:4.6444 train_time:365901ms step_avg:169.95ms\nstep:2164/10000 train_loss:4.5029 train_time:366071ms step_avg:169.95ms\nstep:2165/10000 train_loss:4.2269 train_time:366241ms step_avg:169.95ms\nstep:2166/10000 train_loss:4.6007 train_time:366411ms step_avg:169.95ms\nstep:2167/10000 train_loss:5.0447 train_time:366582ms step_avg:169.95ms\nstep:2168/10000 train_loss:4.4538 train_time:366752ms step_avg:169.95ms\nstep:2169/10000 train_loss:4.5297 train_time:366923ms step_avg:169.95ms\nstep:2170/10000 train_loss:4.4863 train_time:367091ms step_avg:169.95ms\nstep:2171/10000 train_loss:4.4313 train_time:367262ms step_avg:169.95ms\nstep:2172/10000 train_loss:4.5669 train_time:367431ms step_avg:169.95ms\nstep:2173/10000 train_loss:4.6899 train_time:367605ms step_avg:169.95ms\nstep:2174/10000 train_loss:4.4891 train_time:367774ms step_avg:169.95ms\nstep:2175/10000 train_loss:4.6209 train_time:367942ms step_avg:169.95ms\nstep:2176/10000 train_loss:4.5360 train_time:368111ms step_avg:169.95ms\nstep:2177/10000 train_loss:4.5779 train_time:368282ms step_avg:169.95ms\nstep:2178/10000 train_loss:4.6112 train_time:368449ms step_avg:169.95ms\nstep:2179/10000 train_loss:4.4502 train_time:368618ms step_avg:169.95ms\nstep:2180/10000 train_loss:4.5887 train_time:368785ms step_avg:169.95ms\nstep:2181/10000 train_loss:4.7176 train_time:368956ms step_avg:169.95ms\nstep:2182/10000 train_loss:4.4679 train_time:369126ms step_avg:169.95ms\nstep:2183/10000 train_loss:4.3774 train_time:369297ms step_avg:169.95ms\nstep:2184/10000 train_loss:4.5669 train_time:369464ms step_avg:169.95ms\nstep:2185/10000 train_loss:4.5158 train_time:369635ms step_avg:169.95ms\nstep:2186/10000 train_loss:4.5206 train_time:369803ms step_avg:169.95ms\nstep:2187/10000 train_loss:4.4404 train_time:369973ms step_avg:169.95ms\nstep:2188/10000 train_loss:4.3826 train_time:370144ms step_avg:169.95ms\nstep:2189/10000 train_loss:4.5151 train_time:370317ms step_avg:169.95ms\nstep:2190/10000 train_loss:4.4958 train_time:370485ms step_avg:169.95ms\nstep:2191/10000 train_loss:4.5559 train_time:370657ms step_avg:169.95ms\nstep:2192/10000 train_loss:4.6537 train_time:370826ms step_avg:169.95ms\nstep:2193/10000 train_loss:4.4474 train_time:370996ms step_avg:169.95ms\nstep:2194/10000 train_loss:4.5150 train_time:371166ms step_avg:169.95ms\nstep:2195/10000 train_loss:4.3580 train_time:371338ms step_avg:169.95ms\nstep:2196/10000 train_loss:4.2369 train_time:371507ms step_avg:169.95ms\nstep:2197/10000 train_loss:4.4988 train_time:371678ms step_avg:169.95ms\nstep:2198/10000 train_loss:4.5465 train_time:371845ms step_avg:169.95ms\nstep:2199/10000 train_loss:4.3936 train_time:372015ms step_avg:169.95ms\nstep:2200/10000 train_loss:4.6101 train_time:372186ms step_avg:169.95ms\nstep:2201/10000 train_loss:4.3725 train_time:372358ms step_avg:169.95ms\nstep:2202/10000 train_loss:4.4094 train_time:372525ms step_avg:169.95ms\nstep:2203/10000 train_loss:4.4579 train_time:372697ms step_avg:169.95ms\nstep:2204/10000 train_loss:4.5209 train_time:372864ms step_avg:169.95ms\nstep:2205/10000 train_loss:4.5341 train_time:373035ms step_avg:169.95ms\nstep:2206/10000 train_loss:4.4691 train_time:373205ms step_avg:169.95ms\nstep:2207/10000 train_loss:4.4253 train_time:373375ms step_avg:169.95ms\nstep:2208/10000 train_loss:4.4340 train_time:373549ms step_avg:169.95ms\nstep:2209/10000 train_loss:4.4540 train_time:373714ms step_avg:169.95ms\nstep:2210/10000 train_loss:4.7715 train_time:373884ms step_avg:169.95ms\nstep:2211/10000 train_loss:4.4821 train_time:374052ms step_avg:169.95ms\nstep:2212/10000 train_loss:4.6548 train_time:374219ms step_avg:169.95ms\nstep:2213/10000 train_loss:4.5784 train_time:374390ms step_avg:169.95ms\nstep:2214/10000 train_loss:4.5698 train_time:374561ms step_avg:169.95ms\nstep:2215/10000 train_loss:4.5323 train_time:374729ms step_avg:169.94ms\nstep:2216/10000 train_loss:4.5186 train_time:374900ms step_avg:169.95ms\nstep:2217/10000 train_loss:4.3735 train_time:375069ms step_avg:169.95ms\nstep:2218/10000 train_loss:4.5501 train_time:375238ms step_avg:169.94ms\nstep:2219/10000 train_loss:4.4798 train_time:375406ms step_avg:169.94ms\nstep:2220/10000 train_loss:4.4670 train_time:375576ms step_avg:169.94ms\nstep:2221/10000 train_loss:4.3399 train_time:375743ms step_avg:169.94ms\nstep:2222/10000 train_loss:4.5913 train_time:375913ms step_avg:169.94ms\nstep:2223/10000 train_loss:4.5645 train_time:376086ms step_avg:169.94ms\nstep:2224/10000 train_loss:4.5725 train_time:376256ms step_avg:169.94ms\nstep:2225/10000 train_loss:4.4236 train_time:376424ms step_avg:169.94ms\nstep:2226/10000 train_loss:4.5123 train_time:376595ms step_avg:169.94ms\nstep:2227/10000 train_loss:4.5070 train_time:376764ms step_avg:169.94ms\nstep:2228/10000 train_loss:4.5368 train_time:376933ms step_avg:169.94ms\nstep:2229/10000 train_loss:4.3855 train_time:377104ms step_avg:169.94ms\nstep:2230/10000 train_loss:4.2908 train_time:377274ms step_avg:169.94ms\nstep:2231/10000 train_loss:4.6314 train_time:377443ms step_avg:169.94ms\nstep:2232/10000 train_loss:4.4151 train_time:377613ms step_avg:169.94ms\nstep:2233/10000 train_loss:4.3713 train_time:377780ms step_avg:169.94ms\nstep:2234/10000 train_loss:4.4170 train_time:377949ms step_avg:169.94ms\nstep:2235/10000 train_loss:4.4419 train_time:378119ms step_avg:169.94ms\nstep:2236/10000 train_loss:4.5259 train_time:378287ms step_avg:169.94ms\nstep:2237/10000 train_loss:4.6185 train_time:378457ms step_avg:169.94ms\nstep:2238/10000 train_loss:4.4947 train_time:378627ms step_avg:169.94ms\nstep:2239/10000 train_loss:4.4677 train_time:378797ms step_avg:169.94ms\nstep:2240/10000 train_loss:4.3941 train_time:378963ms step_avg:169.94ms\nstep:2241/10000 train_loss:4.6138 train_time:379133ms step_avg:169.94ms\nstep:2242/10000 train_loss:4.5240 train_time:379300ms step_avg:169.94ms\nstep:2243/10000 train_loss:4.6695 train_time:379472ms step_avg:169.94ms\nstep:2244/10000 train_loss:4.4833 train_time:379644ms step_avg:169.94ms\nstep:2245/10000 train_loss:4.3559 train_time:379816ms step_avg:169.94ms\nstep:2246/10000 train_loss:4.4041 train_time:379984ms step_avg:169.94ms\nstep:2247/10000 train_loss:4.4790 train_time:380152ms step_avg:169.94ms\nstep:2248/10000 train_loss:4.4603 train_time:380322ms step_avg:169.94ms\nstep:2249/10000 train_loss:4.4633 train_time:380493ms step_avg:169.94ms\nstep:2250/10000 train_loss:4.3595 train_time:380664ms step_avg:169.94ms\nstep:2251/10000 train_loss:4.3775 train_time:380834ms step_avg:169.94ms\nstep:2252/10000 train_loss:4.3294 train_time:381003ms step_avg:169.94ms\nstep:2253/10000 train_loss:4.5565 train_time:381174ms step_avg:169.94ms\nstep:2254/10000 train_loss:4.5590 train_time:381343ms step_avg:169.94ms\nstep:2255/10000 train_loss:4.4193 train_time:381514ms step_avg:169.94ms\nstep:2256/10000 train_loss:4.5283 train_time:381686ms step_avg:169.94ms\nstep:2257/10000 train_loss:4.3959 train_time:381858ms step_avg:169.94ms\nstep:2258/10000 train_loss:4.4055 train_time:382026ms step_avg:169.94ms\nstep:2259/10000 train_loss:4.5396 train_time:382195ms step_avg:169.94ms\nstep:2260/10000 train_loss:4.4600 train_time:382361ms step_avg:169.94ms\nstep:2261/10000 train_loss:4.5556 train_time:382531ms step_avg:169.94ms\nstep:2262/10000 train_loss:4.5114 train_time:382701ms step_avg:169.94ms\nstep:2263/10000 train_loss:4.3717 train_time:382868ms step_avg:169.94ms\nstep:2264/10000 train_loss:4.4297 train_time:383042ms step_avg:169.94ms\nstep:2265/10000 train_loss:4.6171 train_time:383212ms step_avg:169.94ms\nstep:2266/10000 train_loss:4.4673 train_time:383383ms step_avg:169.94ms\nstep:2267/10000 train_loss:4.4171 train_time:383552ms step_avg:169.94ms\nstep:2268/10000 train_loss:4.5455 train_time:383723ms step_avg:169.94ms\nstep:2269/10000 train_loss:4.4202 train_time:383893ms step_avg:169.94ms\nstep:2270/10000 train_loss:4.4073 train_time:384068ms step_avg:169.94ms\nstep:2271/10000 train_loss:4.3776 train_time:384234ms step_avg:169.94ms\nstep:2272/10000 train_loss:4.5417 train_time:384404ms step_avg:169.94ms\nstep:2273/10000 train_loss:4.4847 train_time:384573ms step_avg:169.94ms\nstep:2274/10000 train_loss:4.4850 train_time:384741ms step_avg:169.94ms\nstep:2275/10000 train_loss:4.4786 train_time:384910ms step_avg:169.94ms\nstep:2276/10000 train_loss:4.7114 train_time:385080ms step_avg:169.94ms\nstep:2277/10000 train_loss:4.4879 train_time:385251ms step_avg:169.94ms\nstep:2278/10000 train_loss:4.4704 train_time:385422ms step_avg:169.94ms\nstep:2279/10000 train_loss:4.5811 train_time:385590ms step_avg:169.94ms\nstep:2280/10000 train_loss:4.4487 train_time:385760ms step_avg:169.94ms\nstep:2281/10000 train_loss:4.3187 train_time:385929ms step_avg:169.94ms\nstep:2282/10000 train_loss:4.4833 train_time:386100ms step_avg:169.94ms\nstep:2283/10000 train_loss:4.4993 train_time:386269ms step_avg:169.94ms\nstep:2284/10000 train_loss:4.4785 train_time:386439ms step_avg:169.94ms\nstep:2285/10000 train_loss:4.4539 train_time:386607ms step_avg:169.94ms\nstep:2286/10000 train_loss:4.3793 train_time:386996ms step_avg:170.03ms\nstep:2287/10000 train_loss:4.4624 train_time:387158ms step_avg:170.03ms\nstep:2288/10000 train_loss:4.6076 train_time:387325ms step_avg:170.03ms\nstep:2289/10000 train_loss:4.3899 train_time:387493ms step_avg:170.03ms\nstep:2290/10000 train_loss:4.3096 train_time:387660ms step_avg:170.03ms\nstep:2291/10000 train_loss:4.5061 train_time:387829ms step_avg:170.03ms\nstep:2292/10000 train_loss:4.4619 train_time:388005ms step_avg:170.03ms\nstep:2293/10000 train_loss:4.4447 train_time:388178ms step_avg:170.03ms\nstep:2294/10000 train_loss:4.3906 train_time:388346ms step_avg:170.03ms\nstep:2295/10000 train_loss:4.4442 train_time:388517ms step_avg:170.03ms\nstep:2296/10000 train_loss:4.4696 train_time:388686ms step_avg:170.03ms\nstep:2297/10000 train_loss:4.5677 train_time:388857ms step_avg:170.03ms\nstep:2298/10000 train_loss:4.7773 train_time:389029ms step_avg:170.03ms\nstep:2299/10000 train_loss:4.5349 train_time:389201ms step_avg:170.03ms\nstep:2300/10000 train_loss:4.5773 train_time:389371ms step_avg:170.03ms\nstep:2301/10000 train_loss:4.4255 train_time:389539ms step_avg:170.03ms\nstep:2302/10000 train_loss:4.3644 train_time:389706ms step_avg:170.03ms\nstep:2303/10000 train_loss:4.4077 train_time:389875ms step_avg:170.03ms\nstep:2304/10000 train_loss:4.5459 train_time:390053ms step_avg:170.03ms\nstep:2305/10000 train_loss:4.5082 train_time:390221ms step_avg:170.03ms\nstep:2306/10000 train_loss:4.4536 train_time:390389ms step_avg:170.03ms\nstep:2307/10000 train_loss:4.5250 train_time:390560ms step_avg:170.03ms\nstep:2308/10000 train_loss:4.5414 train_time:390729ms step_avg:170.03ms\nstep:2309/10000 train_loss:4.4302 train_time:390900ms step_avg:170.03ms\nstep:2310/10000 train_loss:4.6256 train_time:391070ms step_avg:170.03ms\nstep:2311/10000 train_loss:4.4548 train_time:391241ms step_avg:170.03ms\nstep:2312/10000 train_loss:4.5088 train_time:391410ms step_avg:170.03ms\nstep:2313/10000 train_loss:4.5651 train_time:391580ms step_avg:170.03ms\nstep:2314/10000 train_loss:4.3799 train_time:391748ms step_avg:170.03ms\nstep:2315/10000 train_loss:4.3346 train_time:391918ms step_avg:170.03ms\nstep:2316/10000 train_loss:4.4529 train_time:392086ms step_avg:170.03ms\nstep:2317/10000 train_loss:4.5567 train_time:392259ms step_avg:170.03ms\nstep:2318/10000 train_loss:4.3820 train_time:392435ms step_avg:170.03ms\nstep:2319/10000 train_loss:4.6925 train_time:392601ms step_avg:170.03ms\nstep:2320/10000 train_loss:4.4236 train_time:392769ms step_avg:170.03ms\nstep:2321/10000 train_loss:4.5667 train_time:392939ms step_avg:170.03ms\nstep:2322/10000 train_loss:4.5388 train_time:393108ms step_avg:170.03ms\nstep:2323/10000 train_loss:4.3678 train_time:393280ms step_avg:170.03ms\nstep:2324/10000 train_loss:4.3925 train_time:393449ms step_avg:170.03ms\nstep:2325/10000 train_loss:4.4801 train_time:393619ms step_avg:170.03ms\nstep:2326/10000 train_loss:4.5104 train_time:393789ms step_avg:170.03ms\nstep:2327/10000 train_loss:4.3967 train_time:393957ms step_avg:170.03ms\nstep:2328/10000 train_loss:4.4225 train_time:394125ms step_avg:170.03ms\nstep:2329/10000 train_loss:4.3624 train_time:394296ms step_avg:170.03ms\nstep:2330/10000 train_loss:4.4951 train_time:394468ms step_avg:170.03ms\nstep:2331/10000 train_loss:4.5459 train_time:394639ms step_avg:170.03ms\nstep:2332/10000 train_loss:4.6528 train_time:394807ms step_avg:170.03ms\nstep:2333/10000 train_loss:4.4364 train_time:394976ms step_avg:170.03ms\nstep:2334/10000 train_loss:4.6433 train_time:395142ms step_avg:170.03ms\nstep:2335/10000 train_loss:4.3322 train_time:395312ms step_avg:170.03ms\nstep:2336/10000 train_loss:4.4006 train_time:395482ms step_avg:170.03ms\nstep:2337/10000 train_loss:4.4055 train_time:395654ms step_avg:170.03ms\nstep:2338/10000 train_loss:4.3817 train_time:395823ms step_avg:170.03ms\nstep:2339/10000 train_loss:4.5079 train_time:395993ms step_avg:170.03ms\nstep:2340/10000 train_loss:4.3555 train_time:396160ms step_avg:170.03ms\nstep:2341/10000 train_loss:4.4198 train_time:396328ms step_avg:170.02ms\nstep:2342/10000 train_loss:4.5482 train_time:396499ms step_avg:170.03ms\nstep:2343/10000 train_loss:4.3482 train_time:396668ms step_avg:170.02ms\nstep:2344/10000 train_loss:4.5245 train_time:396837ms step_avg:170.02ms\nstep:2345/10000 train_loss:4.5027 train_time:397008ms step_avg:170.02ms\nstep:2346/10000 train_loss:4.4658 train_time:397178ms step_avg:170.02ms\nstep:2347/10000 train_loss:4.4159 train_time:397347ms step_avg:170.02ms\nstep:2348/10000 train_loss:4.3523 train_time:397516ms step_avg:170.02ms\nstep:2349/10000 train_loss:4.4079 train_time:397685ms step_avg:170.02ms\nstep:2350/10000 train_loss:4.7967 train_time:397855ms step_avg:170.02ms\nstep:2351/10000 train_loss:4.3422 train_time:398023ms step_avg:170.02ms\nstep:2352/10000 train_loss:4.4465 train_time:398195ms step_avg:170.02ms\nstep:2353/10000 train_loss:4.4777 train_time:398365ms step_avg:170.02ms\nstep:2354/10000 train_loss:4.2760 train_time:398536ms step_avg:170.02ms\nstep:2355/10000 train_loss:4.4040 train_time:398704ms step_avg:170.02ms\nstep:2356/10000 train_loss:4.4851 train_time:398876ms step_avg:170.02ms\nstep:2357/10000 train_loss:4.4054 train_time:399043ms step_avg:170.02ms\nstep:2358/10000 train_loss:4.6479 train_time:399215ms step_avg:170.02ms\nstep:2359/10000 train_loss:4.4428 train_time:399384ms step_avg:170.02ms\nstep:2360/10000 train_loss:4.7956 train_time:399553ms step_avg:170.02ms\nstep:2361/10000 train_loss:4.5110 train_time:399724ms step_avg:170.02ms\nstep:2362/10000 train_loss:4.5049 train_time:399894ms step_avg:170.02ms\nstep:2363/10000 train_loss:4.6175 train_time:400063ms step_avg:170.02ms\nstep:2364/10000 train_loss:4.5496 train_time:400233ms step_avg:170.02ms\nstep:2365/10000 train_loss:4.4390 train_time:400404ms step_avg:170.02ms\nstep:2366/10000 train_loss:4.2989 train_time:400573ms step_avg:170.02ms\nstep:2367/10000 train_loss:4.3374 train_time:400744ms step_avg:170.02ms\nstep:2368/10000 train_loss:4.3008 train_time:400914ms step_avg:170.02ms\nstep:2369/10000 train_loss:4.6059 train_time:401082ms step_avg:170.02ms\nstep:2370/10000 train_loss:4.3656 train_time:401254ms step_avg:170.02ms\nstep:2371/10000 train_loss:4.5549 train_time:401426ms step_avg:170.02ms\nstep:2372/10000 train_loss:4.3154 train_time:401596ms step_avg:170.02ms\nstep:2373/10000 train_loss:4.4287 train_time:401763ms step_avg:170.02ms\nstep:2374/10000 train_loss:4.5307 train_time:401933ms step_avg:170.02ms\nstep:2375/10000 train_loss:4.5418 train_time:402101ms step_avg:170.02ms\nstep:2376/10000 train_loss:4.3489 train_time:402270ms step_avg:170.02ms\nstep:2377/10000 train_loss:4.3544 train_time:402441ms step_avg:170.02ms\nstep:2378/10000 train_loss:4.4061 train_time:402609ms step_avg:170.02ms\nstep:2379/10000 train_loss:4.4713 train_time:402778ms step_avg:170.02ms\nstep:2380/10000 train_loss:4.4114 train_time:402949ms step_avg:170.02ms\nstep:2381/10000 train_loss:4.3565 train_time:403117ms step_avg:170.02ms\nstep:2382/10000 train_loss:4.4071 train_time:403286ms step_avg:170.02ms\nstep:2383/10000 train_loss:4.8019 train_time:403460ms step_avg:170.02ms\nstep:2384/10000 train_loss:4.6677 train_time:403630ms step_avg:170.02ms\nstep:2385/10000 train_loss:4.4421 train_time:403801ms step_avg:170.02ms\nstep:2386/10000 train_loss:4.4473 train_time:403970ms step_avg:170.02ms\nstep:2387/10000 train_loss:4.3209 train_time:404139ms step_avg:170.02ms\nstep:2388/10000 train_loss:4.4461 train_time:404307ms step_avg:170.02ms\nstep:2389/10000 train_loss:4.5254 train_time:404478ms step_avg:170.02ms\nstep:2390/10000 train_loss:4.4819 train_time:404647ms step_avg:170.02ms\nstep:2391/10000 train_loss:4.4214 train_time:404817ms step_avg:170.02ms\nstep:2392/10000 train_loss:4.3867 train_time:404987ms step_avg:170.02ms\nstep:2393/10000 train_loss:4.4300 train_time:405158ms step_avg:170.02ms\nstep:2394/10000 train_loss:4.5063 train_time:405324ms step_avg:170.02ms\nstep:2395/10000 train_loss:4.4610 train_time:405495ms step_avg:170.02ms\nstep:2396/10000 train_loss:4.4248 train_time:405662ms step_avg:170.02ms\nstep:2397/10000 train_loss:4.4440 train_time:405831ms step_avg:170.02ms\nstep:2398/10000 train_loss:4.5574 train_time:405998ms step_avg:170.02ms\nstep:2399/10000 train_loss:4.4087 train_time:406168ms step_avg:170.02ms\nstep:2400/10000 train_loss:4.2580 train_time:406338ms step_avg:170.02ms\nstep:2401/10000 train_loss:4.4192 train_time:406507ms step_avg:170.02ms\nstep:2402/10000 train_loss:4.4245 train_time:406680ms step_avg:170.02ms\nstep:2403/10000 train_loss:4.3684 train_time:406849ms step_avg:170.02ms\nstep:2404/10000 train_loss:4.3815 train_time:407016ms step_avg:170.02ms\nstep:2405/10000 train_loss:4.4841 train_time:407186ms step_avg:170.02ms\nstep:2406/10000 train_loss:4.3174 train_time:407358ms step_avg:170.02ms\nstep:2407/10000 train_loss:4.3179 train_time:407527ms step_avg:170.02ms\nstep:2408/10000 train_loss:4.3666 train_time:407696ms step_avg:170.02ms\nstep:2409/10000 train_loss:4.3170 train_time:407866ms step_avg:170.01ms\nstep:2410/10000 train_loss:4.4836 train_time:408035ms step_avg:170.01ms\nstep:2411/10000 train_loss:4.4395 train_time:408203ms step_avg:170.01ms\nstep:2412/10000 train_loss:4.3953 train_time:408373ms step_avg:170.01ms\nstep:2413/10000 train_loss:4.3082 train_time:408543ms step_avg:170.01ms\nstep:2414/10000 train_loss:4.3339 train_time:408713ms step_avg:170.01ms\nstep:2415/10000 train_loss:4.4670 train_time:408884ms step_avg:170.01ms\nstep:2416/10000 train_loss:4.5306 train_time:409055ms step_avg:170.01ms\nstep:2417/10000 train_loss:4.4236 train_time:409225ms step_avg:170.01ms\nstep:2418/10000 train_loss:4.4356 train_time:409395ms step_avg:170.01ms\nstep:2419/10000 train_loss:4.5409 train_time:409566ms step_avg:170.01ms\nstep:2420/10000 train_loss:4.4736 train_time:409734ms step_avg:170.01ms\nstep:2421/10000 train_loss:4.4441 train_time:409904ms step_avg:170.01ms\nstep:2422/10000 train_loss:4.6998 train_time:410078ms step_avg:170.02ms\nstep:2423/10000 train_loss:4.5294 train_time:410245ms step_avg:170.01ms\nstep:2424/10000 train_loss:4.3836 train_time:410417ms step_avg:170.02ms\nstep:2425/10000 train_loss:4.3023 train_time:410589ms step_avg:170.02ms\nstep:2426/10000 train_loss:4.4887 train_time:410761ms step_avg:170.02ms\nstep:2427/10000 train_loss:4.2443 train_time:410929ms step_avg:170.02ms\nstep:2428/10000 train_loss:4.2877 train_time:411099ms step_avg:170.02ms\nstep:2429/10000 train_loss:4.3722 train_time:411264ms step_avg:170.01ms\nstep:2430/10000 train_loss:4.5046 train_time:411434ms step_avg:170.01ms\nstep:2431/10000 train_loss:4.4067 train_time:411604ms step_avg:170.01ms\nstep:2432/10000 train_loss:4.5206 train_time:411775ms step_avg:170.01ms\nstep:2433/10000 train_loss:4.3410 train_time:411946ms step_avg:170.01ms\nstep:2434/10000 train_loss:4.4389 train_time:412118ms step_avg:170.02ms\nstep:2435/10000 train_loss:4.3922 train_time:412287ms step_avg:170.02ms\nstep:2436/10000 train_loss:4.3820 train_time:412460ms step_avg:170.02ms\nstep:2437/10000 train_loss:4.3949 train_time:412629ms step_avg:170.02ms\nstep:2438/10000 train_loss:4.3328 train_time:412799ms step_avg:170.02ms\nstep:2439/10000 train_loss:4.3575 train_time:412969ms step_avg:170.02ms\nstep:2440/10000 train_loss:4.3972 train_time:413140ms step_avg:170.02ms\nstep:2441/10000 train_loss:4.4302 train_time:413309ms step_avg:170.02ms\nstep:2442/10000 train_loss:4.4090 train_time:413480ms step_avg:170.02ms\nstep:2443/10000 train_loss:4.4025 train_time:413648ms step_avg:170.02ms\nstep:2444/10000 train_loss:4.4988 train_time:413819ms step_avg:170.02ms\nstep:2445/10000 train_loss:4.3396 train_time:413988ms step_avg:170.02ms\nstep:2446/10000 train_loss:4.3565 train_time:414158ms step_avg:170.02ms\nstep:2447/10000 train_loss:4.4083 train_time:414327ms step_avg:170.02ms\nstep:2448/10000 train_loss:4.4107 train_time:414496ms step_avg:170.01ms\nstep:2449/10000 train_loss:4.4863 train_time:414666ms step_avg:170.01ms\nstep:2450/10000 train_loss:4.5301 train_time:414838ms step_avg:170.02ms\nstep:2451/10000 train_loss:4.3177 train_time:415007ms step_avg:170.02ms\nstep:2452/10000 train_loss:4.3653 train_time:415178ms step_avg:170.02ms\nstep:2453/10000 train_loss:4.1198 train_time:415348ms step_avg:170.02ms\nstep:2454/10000 train_loss:4.3291 train_time:415518ms step_avg:170.02ms\nstep:2455/10000 train_loss:4.3843 train_time:415685ms step_avg:170.01ms\nstep:2456/10000 train_loss:4.5334 train_time:415855ms step_avg:170.01ms\nstep:2457/10000 train_loss:4.2931 train_time:416024ms step_avg:170.01ms\nstep:2458/10000 train_loss:4.4294 train_time:416197ms step_avg:170.02ms\nstep:2459/10000 train_loss:4.5259 train_time:416362ms step_avg:170.01ms\nstep:2460/10000 train_loss:4.5219 train_time:416533ms step_avg:170.01ms\nstep:2461/10000 train_loss:4.4294 train_time:416702ms step_avg:170.01ms\nstep:2462/10000 train_loss:4.3639 train_time:416872ms step_avg:170.01ms\nstep:2463/10000 train_loss:4.5267 train_time:417039ms step_avg:170.01ms\nstep:2464/10000 train_loss:4.4718 train_time:417209ms step_avg:170.01ms\nstep:2465/10000 train_loss:4.3928 train_time:417382ms step_avg:170.01ms\nstep:2466/10000 train_loss:4.4176 train_time:417550ms step_avg:170.01ms\nstep:2467/10000 train_loss:4.3557 train_time:417719ms step_avg:170.01ms\nstep:2468/10000 train_loss:4.3269 train_time:417888ms step_avg:170.01ms\nstep:2469/10000 train_loss:4.4862 train_time:418060ms step_avg:170.01ms\nstep:2470/10000 train_loss:4.4593 train_time:418228ms step_avg:170.01ms\nstep:2471/10000 train_loss:4.3379 train_time:418398ms step_avg:170.01ms\nstep:2472/10000 train_loss:4.3407 train_time:418567ms step_avg:170.01ms\nstep:2473/10000 train_loss:4.4901 train_time:418736ms step_avg:170.01ms\nstep:2474/10000 train_loss:4.2546 train_time:418905ms step_avg:170.01ms\nstep:2475/10000 train_loss:4.3989 train_time:419077ms step_avg:170.01ms\nstep:2476/10000 train_loss:4.4099 train_time:419246ms step_avg:170.01ms\nstep:2477/10000 train_loss:4.4774 train_time:419417ms step_avg:170.01ms\nstep:2478/10000 train_loss:4.5307 train_time:419584ms step_avg:170.01ms\nstep:2479/10000 train_loss:4.4017 train_time:419752ms step_avg:170.01ms\nstep:2480/10000 train_loss:4.3529 train_time:419921ms step_avg:170.01ms\nstep:2481/10000 train_loss:4.3629 train_time:420091ms step_avg:170.01ms\nstep:2482/10000 train_loss:4.4362 train_time:420262ms step_avg:170.01ms\nstep:2483/10000 train_loss:4.3195 train_time:420431ms step_avg:170.01ms\nstep:2484/10000 train_loss:4.3693 train_time:420599ms step_avg:170.01ms\nstep:2485/10000 train_loss:4.3832 train_time:420766ms step_avg:170.01ms\nstep:2486/10000 train_loss:4.4236 train_time:420936ms step_avg:170.01ms\nstep:2487/10000 train_loss:4.4220 train_time:421107ms step_avg:170.01ms\nstep:2488/10000 train_loss:4.6056 train_time:421278ms step_avg:170.01ms\nstep:2489/10000 train_loss:4.5955 train_time:421446ms step_avg:170.01ms\nstep:2490/10000 train_loss:4.2637 train_time:421615ms step_avg:170.01ms\nstep:2491/10000 train_loss:4.4221 train_time:421784ms step_avg:170.01ms\nstep:2492/10000 train_loss:4.3611 train_time:421952ms step_avg:170.00ms\nstep:2493/10000 train_loss:4.4784 train_time:422122ms step_avg:170.00ms\nstep:2494/10000 train_loss:4.4127 train_time:422296ms step_avg:170.01ms\nstep:2495/10000 train_loss:4.4472 train_time:422467ms step_avg:170.01ms\nstep:2496/10000 train_loss:4.4141 train_time:422637ms step_avg:170.01ms\nstep:2497/10000 train_loss:4.4382 train_time:422803ms step_avg:170.01ms\nstep:2498/10000 train_loss:4.4784 train_time:422972ms step_avg:170.00ms\nstep:2499/10000 train_loss:4.2347 train_time:423142ms step_avg:170.00ms\nstep:2500/10000 train_loss:4.4401 train_time:423313ms step_avg:170.01ms\nstep:2501/10000 train_loss:4.4648 train_time:423486ms step_avg:170.01ms\nstep:2502/10000 train_loss:4.3762 train_time:423657ms step_avg:170.01ms\nstep:2503/10000 train_loss:4.4039 train_time:423826ms step_avg:170.01ms\nstep:2504/10000 train_loss:4.5252 train_time:423997ms step_avg:170.01ms\nstep:2505/10000 train_loss:4.3529 train_time:424166ms step_avg:170.01ms\nstep:2506/10000 train_loss:4.3666 train_time:424340ms step_avg:170.01ms\nstep:2507/10000 train_loss:4.4388 train_time:424507ms step_avg:170.01ms\nstep:2508/10000 train_loss:4.5058 train_time:424679ms step_avg:170.01ms\nstep:2509/10000 train_loss:4.3464 train_time:424848ms step_avg:170.01ms\nstep:2510/10000 train_loss:4.2769 train_time:425018ms step_avg:170.01ms\nstep:2511/10000 train_loss:4.3839 train_time:425186ms step_avg:170.01ms\nstep:2512/10000 train_loss:4.4657 train_time:425355ms step_avg:170.01ms\nstep:2513/10000 train_loss:4.5363 train_time:425524ms step_avg:170.01ms\nstep:2514/10000 train_loss:4.4409 train_time:425693ms step_avg:170.01ms\nstep:2515/10000 train_loss:4.2998 train_time:425862ms step_avg:170.00ms\nstep:2516/10000 train_loss:4.5240 train_time:426032ms step_avg:170.00ms\nstep:2517/10000 train_loss:4.3503 train_time:426203ms step_avg:170.01ms\nstep:2518/10000 train_loss:4.2681 train_time:426374ms step_avg:170.01ms\nstep:2519/10000 train_loss:4.3568 train_time:426543ms step_avg:170.01ms\nstep:2520/10000 train_loss:4.3119 train_time:426711ms step_avg:170.00ms\nstep:2521/10000 train_loss:4.4016 train_time:426880ms step_avg:170.00ms\nstep:2522/10000 train_loss:4.4430 train_time:427050ms step_avg:170.00ms\nstep:2523/10000 train_loss:4.4816 train_time:427220ms step_avg:170.00ms\nstep:2524/10000 train_loss:4.4643 train_time:427389ms step_avg:170.00ms\nstep:2525/10000 train_loss:4.3383 train_time:427558ms step_avg:170.00ms\nstep:2526/10000 train_loss:4.2870 train_time:427726ms step_avg:170.00ms\nstep:2527/10000 train_loss:4.3575 train_time:427895ms step_avg:170.00ms\nstep:2528/10000 train_loss:4.5182 train_time:428064ms step_avg:170.00ms\nstep:2529/10000 train_loss:4.3703 train_time:428235ms step_avg:170.00ms\nstep:2530/10000 train_loss:4.4029 train_time:428403ms step_avg:170.00ms\nstep:2531/10000 train_loss:4.3374 train_time:428572ms step_avg:170.00ms\nstep:2532/10000 train_loss:4.4154 train_time:428739ms step_avg:170.00ms\nstep:2533/10000 train_loss:4.2782 train_time:428908ms step_avg:170.00ms\nstep:2534/10000 train_loss:4.5426 train_time:429077ms step_avg:170.00ms\nstep:2535/10000 train_loss:4.4980 train_time:429249ms step_avg:170.00ms\nstep:2536/10000 train_loss:4.2947 train_time:429421ms step_avg:170.00ms\nstep:2537/10000 train_loss:4.5376 train_time:429588ms step_avg:170.00ms\nstep:2538/10000 train_loss:4.3869 train_time:429760ms step_avg:170.00ms\nstep:2539/10000 train_loss:4.4509 train_time:429929ms step_avg:170.00ms\nstep:2540/10000 train_loss:4.3903 train_time:430102ms step_avg:170.00ms\nstep:2541/10000 train_loss:4.3297 train_time:430267ms step_avg:170.00ms\nstep:2542/10000 train_loss:4.2863 train_time:430439ms step_avg:170.00ms\nstep:2543/10000 train_loss:4.2374 train_time:430608ms step_avg:170.00ms\nstep:2544/10000 train_loss:4.6554 train_time:430776ms step_avg:170.00ms\nstep:2545/10000 train_loss:4.3844 train_time:430945ms step_avg:170.00ms\nstep:2546/10000 train_loss:4.3543 train_time:431115ms step_avg:170.00ms\nstep:2547/10000 train_loss:4.4426 train_time:431282ms step_avg:170.00ms\nstep:2548/10000 train_loss:4.2630 train_time:431451ms step_avg:170.00ms\nstep:2549/10000 train_loss:4.3210 train_time:431621ms step_avg:170.00ms\nstep:2550/10000 train_loss:4.2751 train_time:431790ms step_avg:170.00ms\nstep:2551/10000 train_loss:4.4966 train_time:431959ms step_avg:170.00ms\nstep:2552/10000 train_loss:4.1297 train_time:432127ms step_avg:169.99ms\nstep:2553/10000 train_loss:4.4027 train_time:432299ms step_avg:170.00ms\nstep:2554/10000 train_loss:4.3175 train_time:432468ms step_avg:170.00ms\nstep:2555/10000 train_loss:4.3150 train_time:432639ms step_avg:170.00ms\nstep:2556/10000 train_loss:4.3218 train_time:432807ms step_avg:169.99ms\nstep:2557/10000 train_loss:4.3764 train_time:432976ms step_avg:169.99ms\nstep:2558/10000 train_loss:4.3830 train_time:433146ms step_avg:169.99ms\nstep:2559/10000 train_loss:4.4513 train_time:433317ms step_avg:169.99ms\nstep:2560/10000 train_loss:4.3146 train_time:433486ms step_avg:169.99ms\nstep:2561/10000 train_loss:4.2398 train_time:433657ms step_avg:169.99ms\nstep:2562/10000 train_loss:4.2878 train_time:433824ms step_avg:169.99ms\nstep:2563/10000 train_loss:4.4554 train_time:433997ms step_avg:169.99ms\nstep:2564/10000 train_loss:4.3479 train_time:434166ms step_avg:169.99ms\nstep:2565/10000 train_loss:4.4349 train_time:434339ms step_avg:170.00ms\nstep:2566/10000 train_loss:4.3134 train_time:434508ms step_avg:170.00ms\nstep:2567/10000 train_loss:4.3867 train_time:434680ms step_avg:170.00ms\nstep:2568/10000 train_loss:4.3392 train_time:434851ms step_avg:170.00ms\nstep:2569/10000 train_loss:4.4162 train_time:435022ms step_avg:170.00ms\nstep:2570/10000 train_loss:4.4672 train_time:435194ms step_avg:170.00ms\nstep:2571/10000 train_loss:4.3435 train_time:435363ms step_avg:170.00ms\nstep:2572/10000 train_loss:4.3548 train_time:435532ms step_avg:170.00ms\nstep:2573/10000 train_loss:4.1974 train_time:435701ms step_avg:170.00ms\nstep:2574/10000 train_loss:4.1709 train_time:435870ms step_avg:170.00ms\nstep:2575/10000 train_loss:4.4123 train_time:436041ms step_avg:170.00ms\nstep:2576/10000 train_loss:4.3982 train_time:436210ms step_avg:170.00ms\nstep:2577/10000 train_loss:4.3505 train_time:436382ms step_avg:170.00ms\nstep:2578/10000 train_loss:4.3318 train_time:436551ms step_avg:170.00ms\nstep:2579/10000 train_loss:4.5194 train_time:436720ms step_avg:170.00ms\nstep:2580/10000 train_loss:4.3017 train_time:436887ms step_avg:169.99ms\nstep:2581/10000 train_loss:4.4742 train_time:437057ms step_avg:169.99ms\nstep:2582/10000 train_loss:4.2928 train_time:437226ms step_avg:169.99ms\nstep:2583/10000 train_loss:4.4431 train_time:437397ms step_avg:169.99ms\nstep:2584/10000 train_loss:4.5109 train_time:437565ms step_avg:169.99ms\nstep:2585/10000 train_loss:4.3016 train_time:437737ms step_avg:169.99ms\nstep:2586/10000 train_loss:4.4467 train_time:437905ms step_avg:169.99ms\nstep:2587/10000 train_loss:4.3109 train_time:438076ms step_avg:169.99ms\nstep:2588/10000 train_loss:4.3944 train_time:438246ms step_avg:169.99ms\nstep:2589/10000 train_loss:4.3660 train_time:438417ms step_avg:169.99ms\nstep:2590/10000 train_loss:4.3593 train_time:438585ms step_avg:169.99ms\nstep:2591/10000 train_loss:4.3139 train_time:438757ms step_avg:170.00ms\nstep:2592/10000 train_loss:4.4119 train_time:438927ms step_avg:169.99ms\nstep:2593/10000 train_loss:4.3477 train_time:439098ms step_avg:170.00ms\nstep:2594/10000 train_loss:4.6276 train_time:439265ms step_avg:169.99ms\nstep:2595/10000 train_loss:4.6656 train_time:439435ms step_avg:169.99ms\nstep:2596/10000 train_loss:4.3894 train_time:439604ms step_avg:169.99ms\nstep:2597/10000 train_loss:4.4807 train_time:439773ms step_avg:169.99ms\nstep:2598/10000 train_loss:4.3950 train_time:439943ms step_avg:169.99ms\nstep:2599/10000 train_loss:4.3694 train_time:440113ms step_avg:169.99ms\nstep:2600/10000 train_loss:4.4992 train_time:440281ms step_avg:169.99ms\nstep:2601/10000 train_loss:4.4140 train_time:440452ms step_avg:169.99ms\nstep:2602/10000 train_loss:4.2742 train_time:440623ms step_avg:169.99ms\nstep:2603/10000 train_loss:4.4484 train_time:440793ms step_avg:169.99ms\nstep:2604/10000 train_loss:4.3652 train_time:440963ms step_avg:169.99ms\nstep:2605/10000 train_loss:4.4874 train_time:441136ms step_avg:169.99ms\nstep:2606/10000 train_loss:4.2590 train_time:441305ms step_avg:169.99ms\nstep:2607/10000 train_loss:4.3651 train_time:441475ms step_avg:169.99ms\nstep:2608/10000 train_loss:4.4730 train_time:441646ms step_avg:169.99ms\nstep:2609/10000 train_loss:4.2431 train_time:441816ms step_avg:169.99ms\nstep:2610/10000 train_loss:4.3864 train_time:441984ms step_avg:169.99ms\nstep:2611/10000 train_loss:4.3212 train_time:442152ms step_avg:169.99ms\nstep:2612/10000 train_loss:4.3349 train_time:442323ms step_avg:169.99ms\nstep:2613/10000 train_loss:4.2982 train_time:442493ms step_avg:169.99ms\nstep:2614/10000 train_loss:4.3408 train_time:442664ms step_avg:169.99ms\nstep:2615/10000 train_loss:4.3363 train_time:442834ms step_avg:169.99ms\nstep:2616/10000 train_loss:4.3971 train_time:443002ms step_avg:169.99ms\nstep:2617/10000 train_loss:4.3402 train_time:443172ms step_avg:169.99ms\nstep:2618/10000 train_loss:4.4800 train_time:443340ms step_avg:169.99ms\nstep:2619/10000 train_loss:4.2970 train_time:443511ms step_avg:169.99ms\nstep:2620/10000 train_loss:4.1657 train_time:443679ms step_avg:169.99ms\nstep:2621/10000 train_loss:4.4525 train_time:443849ms step_avg:169.99ms\nstep:2622/10000 train_loss:4.5227 train_time:444020ms step_avg:169.99ms\nstep:2623/10000 train_loss:4.5068 train_time:444188ms step_avg:169.99ms\nstep:2624/10000 train_loss:4.3018 train_time:444359ms step_avg:169.99ms\nstep:2625/10000 train_loss:4.2030 train_time:444526ms step_avg:169.99ms\nstep:2626/10000 train_loss:4.4017 train_time:444695ms step_avg:169.99ms\nstep:2627/10000 train_loss:4.3953 train_time:444864ms step_avg:169.99ms\nstep:2628/10000 train_loss:4.2967 train_time:445035ms step_avg:169.99ms\nstep:2629/10000 train_loss:4.4144 train_time:445203ms step_avg:169.99ms\nstep:2630/10000 train_loss:4.3497 train_time:445376ms step_avg:169.99ms\nstep:2631/10000 train_loss:4.3702 train_time:445544ms step_avg:169.99ms\nstep:2632/10000 train_loss:4.2651 train_time:445714ms step_avg:169.99ms\nstep:2633/10000 train_loss:4.1128 train_time:445882ms step_avg:169.99ms\nstep:2634/10000 train_loss:4.2375 train_time:446052ms step_avg:169.99ms\nstep:2635/10000 train_loss:4.4008 train_time:446222ms step_avg:169.99ms\nstep:2636/10000 train_loss:4.4255 train_time:446391ms step_avg:169.99ms\nstep:2637/10000 train_loss:4.2990 train_time:446558ms step_avg:169.99ms\nstep:2638/10000 train_loss:4.3763 train_time:446726ms step_avg:169.99ms\nstep:2639/10000 train_loss:4.3395 train_time:446896ms step_avg:169.99ms\nstep:2640/10000 train_loss:4.3318 train_time:447065ms step_avg:169.99ms\nstep:2641/10000 train_loss:4.2781 train_time:447237ms step_avg:169.99ms\nstep:2642/10000 train_loss:4.4705 train_time:447407ms step_avg:169.99ms\nstep:2643/10000 train_loss:4.4170 train_time:447578ms step_avg:169.99ms\nstep:2644/10000 train_loss:4.3834 train_time:447744ms step_avg:169.99ms\nstep:2645/10000 train_loss:4.3639 train_time:447914ms step_avg:169.99ms\nstep:2646/10000 train_loss:4.3974 train_time:448082ms step_avg:169.99ms\nstep:2647/10000 train_loss:4.3878 train_time:448251ms step_avg:169.99ms\nstep:2648/10000 train_loss:4.4610 train_time:448422ms step_avg:169.99ms\nstep:2649/10000 train_loss:4.4646 train_time:448599ms step_avg:169.99ms\nstep:2650/10000 train_loss:4.2479 train_time:448763ms step_avg:169.99ms\nstep:2651/10000 train_loss:4.4354 train_time:448934ms step_avg:169.99ms\nstep:2652/10000 train_loss:4.3899 train_time:449106ms step_avg:169.99ms\nstep:2653/10000 train_loss:4.3097 train_time:449276ms step_avg:169.99ms\nstep:2654/10000 train_loss:4.3758 train_time:449446ms step_avg:169.99ms\nstep:2655/10000 train_loss:4.3363 train_time:449615ms step_avg:169.99ms\nstep:2656/10000 train_loss:4.2729 train_time:449782ms step_avg:169.99ms\nstep:2657/10000 train_loss:4.3011 train_time:449955ms step_avg:169.99ms\nstep:2658/10000 train_loss:4.3238 train_time:450124ms step_avg:169.99ms\nstep:2659/10000 train_loss:4.2920 train_time:450290ms step_avg:169.99ms\nstep:2660/10000 train_loss:4.3552 train_time:450462ms step_avg:169.99ms\nstep:2661/10000 train_loss:4.3085 train_time:450629ms step_avg:169.98ms\nstep:2662/10000 train_loss:4.2535 train_time:450797ms step_avg:169.98ms\nstep:2663/10000 train_loss:4.3686 train_time:450965ms step_avg:169.98ms\nstep:2664/10000 train_loss:4.2148 train_time:451136ms step_avg:169.98ms\nstep:2665/10000 train_loss:4.5084 train_time:451305ms step_avg:169.98ms\nstep:2666/10000 train_loss:4.3929 train_time:451476ms step_avg:169.98ms\nstep:2667/10000 train_loss:4.2439 train_time:451647ms step_avg:169.98ms\nstep:2668/10000 train_loss:4.5100 train_time:451818ms step_avg:169.98ms\nstep:2669/10000 train_loss:4.2652 train_time:451986ms step_avg:169.98ms\nstep:2670/10000 train_loss:4.4401 train_time:452157ms step_avg:169.98ms\nstep:2671/10000 train_loss:4.3929 train_time:452325ms step_avg:169.98ms\nstep:2672/10000 train_loss:4.3645 train_time:452496ms step_avg:169.98ms\nstep:2673/10000 train_loss:4.3462 train_time:452665ms step_avg:169.98ms\nstep:2674/10000 train_loss:4.4450 train_time:452833ms step_avg:169.98ms\nstep:2675/10000 train_loss:4.4379 train_time:453004ms step_avg:169.98ms\nstep:2676/10000 train_loss:4.3190 train_time:453175ms step_avg:169.98ms\nstep:2677/10000 train_loss:4.4817 train_time:453343ms step_avg:169.98ms\nstep:2678/10000 train_loss:4.2719 train_time:453513ms step_avg:169.98ms\nstep:2679/10000 train_loss:4.3495 train_time:453684ms step_avg:169.98ms\nstep:2680/10000 train_loss:4.1980 train_time:453852ms step_avg:169.98ms\nstep:2681/10000 train_loss:4.3183 train_time:454022ms step_avg:169.98ms\nstep:2682/10000 train_loss:4.3798 train_time:454197ms step_avg:169.98ms\nstep:2683/10000 train_loss:4.3169 train_time:454363ms step_avg:169.98ms\nstep:2684/10000 train_loss:4.3073 train_time:454532ms step_avg:169.98ms\nstep:2685/10000 train_loss:4.3027 train_time:454700ms step_avg:169.98ms\nstep:2686/10000 train_loss:4.3399 train_time:454869ms step_avg:169.98ms\nstep:2687/10000 train_loss:4.5523 train_time:455039ms step_avg:169.98ms\nstep:2688/10000 train_loss:4.2635 train_time:455208ms step_avg:169.98ms\nstep:2689/10000 train_loss:4.2842 train_time:455378ms step_avg:169.98ms\nstep:2690/10000 train_loss:4.1589 train_time:455549ms step_avg:169.98ms\nstep:2691/10000 train_loss:4.4451 train_time:455722ms step_avg:169.98ms\nstep:2692/10000 train_loss:4.3089 train_time:455889ms step_avg:169.98ms\nstep:2693/10000 train_loss:4.3337 train_time:456060ms step_avg:169.98ms\nstep:2694/10000 train_loss:4.6053 train_time:456229ms step_avg:169.98ms\nstep:2695/10000 train_loss:4.2925 train_time:456396ms step_avg:169.98ms\nstep:2696/10000 train_loss:4.5640 train_time:456563ms step_avg:169.98ms\nstep:2697/10000 train_loss:4.2524 train_time:456735ms step_avg:169.98ms\nstep:2698/10000 train_loss:4.4523 train_time:456903ms step_avg:169.98ms\nstep:2699/10000 train_loss:4.3001 train_time:457072ms step_avg:169.98ms\nstep:2700/10000 train_loss:4.3876 train_time:457240ms step_avg:169.98ms\nstep:2701/10000 train_loss:4.3036 train_time:457410ms step_avg:169.98ms\nstep:2702/10000 train_loss:4.3643 train_time:457579ms step_avg:169.98ms\nstep:2703/10000 train_loss:4.2204 train_time:457748ms step_avg:169.98ms\nstep:2704/10000 train_loss:4.4037 train_time:457918ms step_avg:169.98ms\nstep:2705/10000 train_loss:4.3040 train_time:458087ms step_avg:169.98ms\nstep:2706/10000 train_loss:4.3660 train_time:458257ms step_avg:169.98ms\nstep:2707/10000 train_loss:4.3189 train_time:458425ms step_avg:169.98ms\nstep:2708/10000 train_loss:4.1886 train_time:458595ms step_avg:169.98ms\nstep:2709/10000 train_loss:4.3443 train_time:458765ms step_avg:169.98ms\nstep:2710/10000 train_loss:4.3320 train_time:458935ms step_avg:169.98ms\nstep:2711/10000 train_loss:4.3339 train_time:459104ms step_avg:169.98ms\nstep:2712/10000 train_loss:4.2810 train_time:459275ms step_avg:169.98ms\nstep:2713/10000 train_loss:4.2678 train_time:459447ms step_avg:169.98ms\nstep:2714/10000 train_loss:4.2221 train_time:459618ms step_avg:169.98ms\nstep:2715/10000 train_loss:4.3600 train_time:459785ms step_avg:169.98ms\nstep:2716/10000 train_loss:4.2420 train_time:459954ms step_avg:169.98ms\nstep:2717/10000 train_loss:4.1448 train_time:460123ms step_avg:169.98ms\nstep:2718/10000 train_loss:4.3758 train_time:460291ms step_avg:169.97ms\nstep:2719/10000 train_loss:4.3954 train_time:460461ms step_avg:169.97ms\nstep:2720/10000 train_loss:4.4066 train_time:460630ms step_avg:169.97ms\nstep:2721/10000 train_loss:4.1861 train_time:460799ms step_avg:169.97ms\nstep:2722/10000 train_loss:4.3792 train_time:460969ms step_avg:169.97ms\nstep:2723/10000 train_loss:4.2512 train_time:461140ms step_avg:169.97ms\nstep:2724/10000 train_loss:4.3067 train_time:461308ms step_avg:169.97ms\nstep:2725/10000 train_loss:4.3157 train_time:461477ms step_avg:169.97ms\nstep:2726/10000 train_loss:4.4873 train_time:461645ms step_avg:169.97ms\nstep:2727/10000 train_loss:4.4015 train_time:461817ms step_avg:169.97ms\nstep:2728/10000 train_loss:4.3298 train_time:461985ms step_avg:169.97ms\nstep:2729/10000 train_loss:4.2731 train_time:462155ms step_avg:169.97ms\nstep:2730/10000 train_loss:4.2789 train_time:462325ms step_avg:169.97ms\nstep:2731/10000 train_loss:4.2770 train_time:462494ms step_avg:169.97ms\nstep:2732/10000 train_loss:4.4076 train_time:462664ms step_avg:169.97ms\nstep:2733/10000 train_loss:4.4489 train_time:462836ms step_avg:169.97ms\nstep:2734/10000 train_loss:4.3425 train_time:463007ms step_avg:169.97ms\nstep:2735/10000 train_loss:4.2857 train_time:463177ms step_avg:169.97ms\nstep:2736/10000 train_loss:4.4627 train_time:463345ms step_avg:169.97ms\nstep:2737/10000 train_loss:4.8917 train_time:463513ms step_avg:169.97ms\nstep:2738/10000 train_loss:4.3399 train_time:463683ms step_avg:169.97ms\nstep:2739/10000 train_loss:4.1535 train_time:463853ms step_avg:169.97ms\nstep:2740/10000 train_loss:4.3462 train_time:464021ms step_avg:169.97ms\nstep:2741/10000 train_loss:4.5655 train_time:464192ms step_avg:169.97ms\nstep:2742/10000 train_loss:4.2789 train_time:464361ms step_avg:169.97ms\nstep:2743/10000 train_loss:4.3937 train_time:464529ms step_avg:169.97ms\nstep:2744/10000 train_loss:4.4341 train_time:464701ms step_avg:169.97ms\nstep:2745/10000 train_loss:4.3395 train_time:464870ms step_avg:169.97ms\nstep:2746/10000 train_loss:4.4156 train_time:465039ms step_avg:169.97ms\nstep:2747/10000 train_loss:4.3069 train_time:465208ms step_avg:169.97ms\nstep:2748/10000 train_loss:4.3711 train_time:465378ms step_avg:169.97ms\nstep:2749/10000 train_loss:4.2698 train_time:465545ms step_avg:169.97ms\nstep:2750/10000 train_loss:4.3159 train_time:465717ms step_avg:169.97ms\nstep:2751/10000 train_loss:4.4476 train_time:465885ms step_avg:169.97ms\nstep:2752/10000 train_loss:4.3868 train_time:466057ms step_avg:169.97ms\nstep:2753/10000 train_loss:4.2617 train_time:466225ms step_avg:169.97ms\nstep:2754/10000 train_loss:4.2285 train_time:466397ms step_avg:169.97ms\nstep:2755/10000 train_loss:4.2728 train_time:466568ms step_avg:169.97ms\nstep:2756/10000 train_loss:4.1152 train_time:466741ms step_avg:169.97ms\nstep:2757/10000 train_loss:4.2574 train_time:466910ms step_avg:169.97ms\nstep:2758/10000 train_loss:4.3651 train_time:467082ms step_avg:169.97ms\nstep:2759/10000 train_loss:4.5072 train_time:467251ms step_avg:169.97ms\nstep:2760/10000 train_loss:4.1691 train_time:467423ms step_avg:169.97ms\nstep:2761/10000 train_loss:4.2630 train_time:467593ms step_avg:169.97ms\nstep:2762/10000 train_loss:4.1827 train_time:467766ms step_avg:169.97ms\nstep:2763/10000 train_loss:4.2912 train_time:467936ms step_avg:169.97ms\nstep:2764/10000 train_loss:4.2746 train_time:468106ms step_avg:169.97ms\nstep:2765/10000 train_loss:4.3694 train_time:468275ms step_avg:169.97ms\nstep:2766/10000 train_loss:4.3814 train_time:468445ms step_avg:169.97ms\nstep:2767/10000 train_loss:4.2736 train_time:468617ms step_avg:169.97ms\nstep:2768/10000 train_loss:4.1922 train_time:468784ms step_avg:169.97ms\nstep:2769/10000 train_loss:4.3319 train_time:468952ms step_avg:169.97ms\nstep:2770/10000 train_loss:4.3798 train_time:469123ms step_avg:169.97ms\nstep:2771/10000 train_loss:4.2889 train_time:469293ms step_avg:169.97ms\nstep:2772/10000 train_loss:4.4002 train_time:469464ms step_avg:169.97ms\nstep:2773/10000 train_loss:4.2878 train_time:469635ms step_avg:169.97ms\nstep:2774/10000 train_loss:4.1731 train_time:469807ms step_avg:169.97ms\nstep:2775/10000 train_loss:4.2995 train_time:469977ms step_avg:169.97ms\nstep:2776/10000 train_loss:4.3916 train_time:470146ms step_avg:169.97ms\nstep:2777/10000 train_loss:4.1294 train_time:470316ms step_avg:169.97ms\nstep:2778/10000 train_loss:4.3693 train_time:470484ms step_avg:169.97ms\nstep:2779/10000 train_loss:4.3536 train_time:470653ms step_avg:169.97ms\nstep:2780/10000 train_loss:4.2680 train_time:470825ms step_avg:169.97ms\nstep:2781/10000 train_loss:4.2280 train_time:470996ms step_avg:169.97ms\nstep:2782/10000 train_loss:4.3373 train_time:471165ms step_avg:169.97ms\nstep:2783/10000 train_loss:4.3697 train_time:471339ms step_avg:169.97ms\nstep:2784/10000 train_loss:4.2490 train_time:471507ms step_avg:169.97ms\nstep:2785/10000 train_loss:4.2318 train_time:471676ms step_avg:169.97ms\nstep:2786/10000 train_loss:4.0523 train_time:471843ms step_avg:169.97ms\nstep:2787/10000 train_loss:4.2910 train_time:472012ms step_avg:169.97ms\nstep:2788/10000 train_loss:4.5404 train_time:472182ms step_avg:169.97ms\nstep:2789/10000 train_loss:4.3449 train_time:472353ms step_avg:169.97ms\nstep:2790/10000 train_loss:4.4461 train_time:472525ms step_avg:169.97ms\nstep:2791/10000 train_loss:4.3534 train_time:472695ms step_avg:169.97ms\nstep:2792/10000 train_loss:4.3505 train_time:472864ms step_avg:169.97ms\nstep:2793/10000 train_loss:4.1842 train_time:473035ms step_avg:169.97ms\nstep:2794/10000 train_loss:4.3099 train_time:473202ms step_avg:169.97ms\nstep:2795/10000 train_loss:4.2452 train_time:473371ms step_avg:169.97ms\nstep:2796/10000 train_loss:4.3186 train_time:473542ms step_avg:169.97ms\nstep:2797/10000 train_loss:4.1621 train_time:473711ms step_avg:169.97ms\nstep:2798/10000 train_loss:4.2813 train_time:473879ms step_avg:169.97ms\nstep:2799/10000 train_loss:4.2554 train_time:474048ms step_avg:169.97ms\nstep:2800/10000 train_loss:4.5259 train_time:474218ms step_avg:169.97ms\nstep:2801/10000 train_loss:4.2939 train_time:474385ms step_avg:169.97ms\nstep:2802/10000 train_loss:4.2471 train_time:474555ms step_avg:169.97ms\nstep:2803/10000 train_loss:4.5701 train_time:474726ms step_avg:169.97ms\nstep:2804/10000 train_loss:4.2776 train_time:474895ms step_avg:169.97ms\nstep:2805/10000 train_loss:4.1605 train_time:475063ms step_avg:169.97ms\nstep:2806/10000 train_loss:4.2581 train_time:475232ms step_avg:169.97ms\nstep:2807/10000 train_loss:4.1647 train_time:475401ms step_avg:169.97ms\nstep:2808/10000 train_loss:4.1984 train_time:475570ms step_avg:169.97ms\nstep:2809/10000 train_loss:4.3932 train_time:475739ms step_avg:169.97ms\nstep:2810/10000 train_loss:3.8385 train_time:475907ms step_avg:169.97ms\nstep:2811/10000 train_loss:4.3853 train_time:476077ms step_avg:169.97ms\nstep:2812/10000 train_loss:4.2342 train_time:476245ms step_avg:169.97ms\nstep:2813/10000 train_loss:4.2903 train_time:476418ms step_avg:169.97ms\nstep:2814/10000 train_loss:4.3000 train_time:476585ms step_avg:169.97ms\nstep:2815/10000 train_loss:4.2767 train_time:476755ms step_avg:169.97ms\nstep:2816/10000 train_loss:4.1497 train_time:476923ms step_avg:169.97ms\nstep:2817/10000 train_loss:4.2683 train_time:477093ms step_avg:169.97ms\nstep:2818/10000 train_loss:4.1664 train_time:477264ms step_avg:169.97ms\nstep:2819/10000 train_loss:4.3002 train_time:477435ms step_avg:169.97ms\nstep:2820/10000 train_loss:4.2595 train_time:477605ms step_avg:169.97ms\nstep:2821/10000 train_loss:4.4099 train_time:477778ms step_avg:169.97ms\nstep:2822/10000 train_loss:4.3895 train_time:477947ms step_avg:169.97ms\nstep:2823/10000 train_loss:4.3888 train_time:478117ms step_avg:169.97ms\nstep:2824/10000 train_loss:4.2750 train_time:478285ms step_avg:169.97ms\nstep:2825/10000 train_loss:4.3579 train_time:478455ms step_avg:169.97ms\nstep:2826/10000 train_loss:4.2418 train_time:478624ms step_avg:169.97ms\nstep:2827/10000 train_loss:4.2874 train_time:478795ms step_avg:169.97ms\nstep:2828/10000 train_loss:4.2914 train_time:478964ms step_avg:169.97ms\nstep:2829/10000 train_loss:4.1750 train_time:479134ms step_avg:169.97ms\nstep:2830/10000 train_loss:4.3389 train_time:479304ms step_avg:169.97ms\nstep:2831/10000 train_loss:4.2837 train_time:479474ms step_avg:169.97ms\nstep:2832/10000 train_loss:4.3492 train_time:479640ms step_avg:169.96ms\nstep:2833/10000 train_loss:4.2838 train_time:479810ms step_avg:169.96ms\nstep:2834/10000 train_loss:4.2882 train_time:479979ms step_avg:169.96ms\nstep:2835/10000 train_loss:4.7248 train_time:480147ms step_avg:169.96ms\nstep:2836/10000 train_loss:4.2882 train_time:480318ms step_avg:169.96ms\nstep:2837/10000 train_loss:4.2156 train_time:480486ms step_avg:169.96ms\nstep:2838/10000 train_loss:4.3719 train_time:480656ms step_avg:169.96ms\nstep:2839/10000 train_loss:4.3315 train_time:480824ms step_avg:169.96ms\nstep:2840/10000 train_loss:4.2589 train_time:480995ms step_avg:169.96ms\nstep:2841/10000 train_loss:4.3424 train_time:481163ms step_avg:169.96ms\nstep:2842/10000 train_loss:4.3426 train_time:481335ms step_avg:169.96ms\nstep:2843/10000 train_loss:4.2496 train_time:481506ms step_avg:169.96ms\nstep:2844/10000 train_loss:4.2830 train_time:481676ms step_avg:169.96ms\nstep:2845/10000 train_loss:4.3392 train_time:481845ms step_avg:169.96ms\nstep:2846/10000 train_loss:4.3125 train_time:482015ms step_avg:169.96ms\nstep:2847/10000 train_loss:4.4234 train_time:482181ms step_avg:169.96ms\nstep:2848/10000 train_loss:4.2977 train_time:482352ms step_avg:169.96ms\nstep:2849/10000 train_loss:4.2784 train_time:482523ms step_avg:169.96ms\nstep:2850/10000 train_loss:4.2447 train_time:482695ms step_avg:169.96ms\nstep:2851/10000 train_loss:4.3511 train_time:482863ms step_avg:169.96ms\nstep:2852/10000 train_loss:4.3439 train_time:483031ms step_avg:169.96ms\nstep:2853/10000 train_loss:4.2781 train_time:483204ms step_avg:169.96ms\nstep:2854/10000 train_loss:4.2586 train_time:483375ms step_avg:169.96ms\nstep:2855/10000 train_loss:4.2072 train_time:483544ms step_avg:169.96ms\nstep:2856/10000 train_loss:4.4226 train_time:483714ms step_avg:169.96ms\nstep:2857/10000 train_loss:4.3088 train_time:483886ms step_avg:169.96ms\nstep:2858/10000 train_loss:4.3040 train_time:484061ms step_avg:169.97ms\nstep:2859/10000 train_loss:4.4580 train_time:484222ms step_avg:169.96ms\nstep:2860/10000 train_loss:4.2615 train_time:484391ms step_avg:169.96ms\nstep:2861/10000 train_loss:4.2424 train_time:484563ms step_avg:169.96ms\nstep:2862/10000 train_loss:4.1375 train_time:484733ms step_avg:169.96ms\nstep:2863/10000 train_loss:4.2097 train_time:484903ms step_avg:169.96ms\nstep:2864/10000 train_loss:4.2812 train_time:485074ms step_avg:169.96ms\nstep:2865/10000 train_loss:4.2463 train_time:485243ms step_avg:169.96ms\nstep:2866/10000 train_loss:4.5276 train_time:485412ms step_avg:169.96ms\nstep:2867/10000 train_loss:4.4824 train_time:485580ms step_avg:169.96ms\nstep:2868/10000 train_loss:4.3050 train_time:485749ms step_avg:169.96ms\nstep:2869/10000 train_loss:4.0876 train_time:485917ms step_avg:169.96ms\nstep:2870/10000 train_loss:4.4129 train_time:486085ms step_avg:169.96ms\nstep:2871/10000 train_loss:4.2469 train_time:486253ms step_avg:169.96ms\nstep:2872/10000 train_loss:4.4304 train_time:486422ms step_avg:169.96ms\nstep:2873/10000 train_loss:4.2627 train_time:486592ms step_avg:169.96ms\nstep:2874/10000 train_loss:4.4041 train_time:486763ms step_avg:169.96ms\nstep:2875/10000 train_loss:4.2980 train_time:486933ms step_avg:169.96ms\nstep:2876/10000 train_loss:4.2865 train_time:487104ms step_avg:169.96ms\nstep:2877/10000 train_loss:4.3414 train_time:487273ms step_avg:169.96ms\nstep:2878/10000 train_loss:4.3729 train_time:487442ms step_avg:169.96ms\nstep:2879/10000 train_loss:4.3158 train_time:487611ms step_avg:169.96ms\nstep:2880/10000 train_loss:4.3372 train_time:487780ms step_avg:169.96ms\nstep:2881/10000 train_loss:4.1713 train_time:487951ms step_avg:169.96ms\nstep:2882/10000 train_loss:4.3160 train_time:488122ms step_avg:169.96ms\nstep:2883/10000 train_loss:4.1610 train_time:488292ms step_avg:169.96ms\nstep:2884/10000 train_loss:4.4310 train_time:488461ms step_avg:169.96ms\nstep:2885/10000 train_loss:4.4400 train_time:488630ms step_avg:169.96ms\nstep:2886/10000 train_loss:4.1940 train_time:488800ms step_avg:169.96ms\nstep:2887/10000 train_loss:4.2500 train_time:488968ms step_avg:169.96ms\nstep:2888/10000 train_loss:4.1479 train_time:489137ms step_avg:169.96ms\nstep:2889/10000 train_loss:4.3702 train_time:489306ms step_avg:169.96ms\nstep:2890/10000 train_loss:4.2926 train_time:489476ms step_avg:169.96ms\nstep:2891/10000 train_loss:4.4837 train_time:489644ms step_avg:169.96ms\nstep:2892/10000 train_loss:4.4207 train_time:489815ms step_avg:169.96ms\nstep:2893/10000 train_loss:4.2594 train_time:489983ms step_avg:169.96ms\nstep:2894/10000 train_loss:4.2740 train_time:490151ms step_avg:169.96ms\nstep:2895/10000 train_loss:4.4330 train_time:490320ms step_avg:169.95ms\nstep:2896/10000 train_loss:4.4597 train_time:490489ms step_avg:169.95ms\nstep:2897/10000 train_loss:4.3210 train_time:490659ms step_avg:169.95ms\nstep:2898/10000 train_loss:4.3042 train_time:490828ms step_avg:169.95ms\nstep:2899/10000 train_loss:4.0790 train_time:490999ms step_avg:169.95ms\nstep:2900/10000 train_loss:4.2480 train_time:491166ms step_avg:169.95ms\nstep:2901/10000 train_loss:4.2537 train_time:491338ms step_avg:169.95ms\nstep:2902/10000 train_loss:4.2979 train_time:491507ms step_avg:169.95ms\nstep:2903/10000 train_loss:4.3280 train_time:491679ms step_avg:169.95ms\nstep:2904/10000 train_loss:4.3326 train_time:491850ms step_avg:169.95ms\nstep:2905/10000 train_loss:4.2163 train_time:492021ms step_avg:169.96ms\nstep:2906/10000 train_loss:4.1824 train_time:492189ms step_avg:169.95ms\nstep:2907/10000 train_loss:4.1204 train_time:492360ms step_avg:169.96ms\nstep:2908/10000 train_loss:4.2519 train_time:492528ms step_avg:169.95ms\nstep:2909/10000 train_loss:4.2030 train_time:492699ms step_avg:169.95ms\nstep:2910/10000 train_loss:4.3335 train_time:492868ms step_avg:169.95ms\nstep:2911/10000 train_loss:4.3419 train_time:493036ms step_avg:169.95ms\nstep:2912/10000 train_loss:4.2018 train_time:493204ms step_avg:169.95ms\nstep:2913/10000 train_loss:4.3215 train_time:493375ms step_avg:169.95ms\nstep:2914/10000 train_loss:4.2707 train_time:493545ms step_avg:169.95ms\nstep:2915/10000 train_loss:4.2636 train_time:493714ms step_avg:169.95ms\nstep:2916/10000 train_loss:4.4035 train_time:493883ms step_avg:169.95ms\nstep:2917/10000 train_loss:4.2679 train_time:494054ms step_avg:169.95ms\nstep:2918/10000 train_loss:4.0673 train_time:494222ms step_avg:169.95ms\nstep:2919/10000 train_loss:4.1146 train_time:494392ms step_avg:169.95ms\nstep:2920/10000 train_loss:4.4088 train_time:494563ms step_avg:169.95ms\nstep:2921/10000 train_loss:4.3033 train_time:494731ms step_avg:169.95ms\nstep:2922/10000 train_loss:4.2017 train_time:494900ms step_avg:169.95ms\nstep:2923/10000 train_loss:4.3321 train_time:495068ms step_avg:169.95ms\nstep:2924/10000 train_loss:4.3953 train_time:495241ms step_avg:169.95ms\nstep:2925/10000 train_loss:4.3014 train_time:495411ms step_avg:169.95ms\nstep:2926/10000 train_loss:4.3330 train_time:495583ms step_avg:169.95ms\nstep:2927/10000 train_loss:4.3423 train_time:495753ms step_avg:169.95ms\nstep:2928/10000 train_loss:4.2410 train_time:495924ms step_avg:169.95ms\nstep:2929/10000 train_loss:4.3259 train_time:496094ms step_avg:169.95ms\nstep:2930/10000 train_loss:4.3050 train_time:496263ms step_avg:169.95ms\nstep:2931/10000 train_loss:4.3320 train_time:496432ms step_avg:169.95ms\nstep:2932/10000 train_loss:4.2925 train_time:496603ms step_avg:169.95ms\nstep:2933/10000 train_loss:4.2125 train_time:496773ms step_avg:169.95ms\nstep:2934/10000 train_loss:4.3882 train_time:496945ms step_avg:169.95ms\nstep:2935/10000 train_loss:4.4380 train_time:497115ms step_avg:169.95ms\nstep:2936/10000 train_loss:4.5165 train_time:497284ms step_avg:169.95ms\nstep:2937/10000 train_loss:4.1968 train_time:497454ms step_avg:169.95ms\nstep:2938/10000 train_loss:4.2126 train_time:497624ms step_avg:169.95ms\nstep:2939/10000 train_loss:4.3332 train_time:497794ms step_avg:169.95ms\nstep:2940/10000 train_loss:4.2255 train_time:497965ms step_avg:169.95ms\nstep:2941/10000 train_loss:4.3263 train_time:498136ms step_avg:169.95ms\nstep:2942/10000 train_loss:4.3305 train_time:498305ms step_avg:169.95ms\nstep:2943/10000 train_loss:4.2553 train_time:498475ms step_avg:169.95ms\nstep:2944/10000 train_loss:4.3408 train_time:498642ms step_avg:169.95ms\nstep:2945/10000 train_loss:4.3348 train_time:498810ms step_avg:169.95ms\nstep:2946/10000 train_loss:4.3192 train_time:498979ms step_avg:169.95ms\nstep:2947/10000 train_loss:4.2519 train_time:499149ms step_avg:169.95ms\nstep:2948/10000 train_loss:4.4370 train_time:499318ms step_avg:169.95ms\nstep:2949/10000 train_loss:4.1704 train_time:499488ms step_avg:169.95ms\nstep:2950/10000 train_loss:4.3618 train_time:499658ms step_avg:169.95ms\nstep:2951/10000 train_loss:4.1749 train_time:499825ms step_avg:169.95ms\nstep:2952/10000 train_loss:4.3129 train_time:499995ms step_avg:169.95ms\nstep:2953/10000 train_loss:4.2704 train_time:500163ms step_avg:169.95ms\nstep:2954/10000 train_loss:4.2882 train_time:500332ms step_avg:169.95ms\nstep:2955/10000 train_loss:4.3521 train_time:500503ms step_avg:169.95ms\nstep:2956/10000 train_loss:4.2911 train_time:500674ms step_avg:169.95ms\nstep:2957/10000 train_loss:4.2243 train_time:500842ms step_avg:169.95ms\nstep:2958/10000 train_loss:4.3177 train_time:501011ms step_avg:169.95ms\nstep:2959/10000 train_loss:4.2449 train_time:501179ms step_avg:169.95ms\nstep:2960/10000 train_loss:4.1904 train_time:501348ms step_avg:169.95ms\nstep:2961/10000 train_loss:4.2929 train_time:501517ms step_avg:169.95ms\nstep:2962/10000 train_loss:4.3251 train_time:501686ms step_avg:169.95ms\nstep:2963/10000 train_loss:4.2006 train_time:501856ms step_avg:169.95ms\nstep:2964/10000 train_loss:4.8142 train_time:502026ms step_avg:169.95ms\nstep:2965/10000 train_loss:4.3620 train_time:502196ms step_avg:169.95ms\nstep:2966/10000 train_loss:4.3137 train_time:502364ms step_avg:169.95ms\nstep:2967/10000 train_loss:4.3216 train_time:502535ms step_avg:169.95ms\nstep:2968/10000 train_loss:4.2073 train_time:502705ms step_avg:169.95ms\nstep:2969/10000 train_loss:4.3319 train_time:502875ms step_avg:169.95ms\nstep:2970/10000 train_loss:4.3322 train_time:503042ms step_avg:169.95ms\nstep:2971/10000 train_loss:4.3611 train_time:503211ms step_avg:169.95ms\nstep:2972/10000 train_loss:4.3132 train_time:503383ms step_avg:169.95ms\nstep:2973/10000 train_loss:4.4668 train_time:503552ms step_avg:169.95ms\nstep:2974/10000 train_loss:4.2114 train_time:503723ms step_avg:169.95ms\nstep:2975/10000 train_loss:4.2801 train_time:503894ms step_avg:169.95ms\nstep:2976/10000 train_loss:4.2552 train_time:504063ms step_avg:169.95ms\nstep:2977/10000 train_loss:4.3290 train_time:504236ms step_avg:169.95ms\nstep:2978/10000 train_loss:4.4486 train_time:504405ms step_avg:169.95ms\nstep:2979/10000 train_loss:4.2383 train_time:504575ms step_avg:169.95ms\nstep:2980/10000 train_loss:4.3047 train_time:504744ms step_avg:169.95ms\nstep:2981/10000 train_loss:4.1988 train_time:504915ms step_avg:169.95ms\nstep:2982/10000 train_loss:4.2234 train_time:505083ms step_avg:169.95ms\nstep:2983/10000 train_loss:4.0365 train_time:505254ms step_avg:169.95ms\nstep:2984/10000 train_loss:4.3648 train_time:505425ms step_avg:169.95ms\nstep:2985/10000 train_loss:4.2417 train_time:505598ms step_avg:169.95ms\nstep:2986/10000 train_loss:4.2681 train_time:505767ms step_avg:169.95ms\nstep:2987/10000 train_loss:4.1603 train_time:505938ms step_avg:169.95ms\nstep:2988/10000 train_loss:4.2849 train_time:506104ms step_avg:169.95ms\nstep:2989/10000 train_loss:4.3192 train_time:506274ms step_avg:169.95ms\nstep:2990/10000 train_loss:4.3861 train_time:506445ms step_avg:169.95ms\nstep:2991/10000 train_loss:4.2185 train_time:506618ms step_avg:169.95ms\nstep:2992/10000 train_loss:4.3067 train_time:506786ms step_avg:169.95ms\nstep:2993/10000 train_loss:4.2933 train_time:506958ms step_avg:169.95ms\nstep:2994/10000 train_loss:4.2476 train_time:507127ms step_avg:169.95ms\nstep:2995/10000 train_loss:4.2801 train_time:507296ms step_avg:169.95ms\nstep:2996/10000 train_loss:4.2201 train_time:507465ms step_avg:169.95ms\nstep:2997/10000 train_loss:4.3717 train_time:507637ms step_avg:169.95ms\nstep:2998/10000 train_loss:4.3301 train_time:507805ms step_avg:169.95ms\nstep:2999/10000 train_loss:4.4090 train_time:507974ms step_avg:169.95ms\nstep:3000/10000 train_loss:4.2477 train_time:508144ms step_avg:169.95ms\nstep:3001/10000 train_loss:4.1624 train_time:508314ms step_avg:169.95ms\nstep:3002/10000 train_loss:4.2017 train_time:508485ms step_avg:169.95ms\nstep:3003/10000 train_loss:4.3176 train_time:508658ms step_avg:169.95ms\nstep:3004/10000 train_loss:4.2844 train_time:508826ms step_avg:169.95ms\nstep:3005/10000 train_loss:4.2382 train_time:508995ms step_avg:169.95ms\nstep:3006/10000 train_loss:4.3407 train_time:509164ms step_avg:169.95ms\nstep:3007/10000 train_loss:4.1619 train_time:509333ms step_avg:169.95ms\nstep:3008/10000 train_loss:4.2436 train_time:509506ms step_avg:169.95ms\nstep:3009/10000 train_loss:4.3645 train_time:509677ms step_avg:169.95ms\nstep:3010/10000 train_loss:4.1727 train_time:509846ms step_avg:169.95ms\nstep:3011/10000 train_loss:4.2065 train_time:510016ms step_avg:169.95ms\nstep:3012/10000 train_loss:4.2540 train_time:510185ms step_avg:169.95ms\nstep:3013/10000 train_loss:4.2469 train_time:510354ms step_avg:169.95ms\nstep:3014/10000 train_loss:4.2386 train_time:510523ms step_avg:169.95ms\nstep:3015/10000 train_loss:4.2896 train_time:510698ms step_avg:169.95ms\nstep:3016/10000 train_loss:4.3086 train_time:510868ms step_avg:169.95ms\nstep:3017/10000 train_loss:4.2162 train_time:511037ms step_avg:169.95ms\nstep:3018/10000 train_loss:4.2091 train_time:511205ms step_avg:169.95ms\nstep:3019/10000 train_loss:4.3764 train_time:511368ms step_avg:169.95ms\nstep:3020/10000 train_loss:4.2422 train_time:511539ms step_avg:169.95ms\nstep:3021/10000 train_loss:4.4180 train_time:511707ms step_avg:169.95ms\nstep:3022/10000 train_loss:4.2583 train_time:511878ms step_avg:169.95ms\nstep:3023/10000 train_loss:4.1157 train_time:512046ms step_avg:169.95ms\nstep:3024/10000 train_loss:4.3432 train_time:512217ms step_avg:169.95ms\nstep:3025/10000 train_loss:4.3123 train_time:512387ms step_avg:169.95ms\nstep:3026/10000 train_loss:4.1658 train_time:512558ms step_avg:169.95ms\nstep:3027/10000 train_loss:4.3499 train_time:512728ms step_avg:169.95ms\nstep:3028/10000 train_loss:4.1846 train_time:512897ms step_avg:169.95ms\nstep:3029/10000 train_loss:4.1776 train_time:513065ms step_avg:169.95ms\nstep:3030/10000 train_loss:4.4337 train_time:513234ms step_avg:169.95ms\nstep:3031/10000 train_loss:4.3808 train_time:513402ms step_avg:169.94ms\nstep:3032/10000 train_loss:4.3852 train_time:513572ms step_avg:169.94ms\nstep:3033/10000 train_loss:4.1990 train_time:513741ms step_avg:169.94ms\nstep:3034/10000 train_loss:4.5147 train_time:513910ms step_avg:169.94ms\nstep:3035/10000 train_loss:3.9883 train_time:514081ms step_avg:169.94ms\nstep:3036/10000 train_loss:4.3041 train_time:514249ms step_avg:169.94ms\nstep:3037/10000 train_loss:4.2621 train_time:514420ms step_avg:169.94ms\nstep:3038/10000 train_loss:4.2543 train_time:514588ms step_avg:169.94ms\nstep:3039/10000 train_loss:4.2140 train_time:514762ms step_avg:169.94ms\nstep:3040/10000 train_loss:4.2167 train_time:514933ms step_avg:169.94ms\nstep:3041/10000 train_loss:4.2585 train_time:515102ms step_avg:169.94ms\nstep:3042/10000 train_loss:4.2003 train_time:515271ms step_avg:169.94ms\nstep:3043/10000 train_loss:4.3046 train_time:515439ms step_avg:169.94ms\nstep:3044/10000 train_loss:4.3273 train_time:515606ms step_avg:169.94ms\nstep:3045/10000 train_loss:4.2325 train_time:515775ms step_avg:169.94ms\nstep:3046/10000 train_loss:4.3834 train_time:515942ms step_avg:169.94ms\nstep:3047/10000 train_loss:4.3090 train_time:516114ms step_avg:169.94ms\nstep:3048/10000 train_loss:4.2894 train_time:516502ms step_avg:170.01ms\nstep:3049/10000 train_loss:4.2675 train_time:516666ms step_avg:170.01ms\nstep:3050/10000 train_loss:4.0875 train_time:516833ms step_avg:170.01ms\nstep:3051/10000 train_loss:4.2253 train_time:517000ms step_avg:170.01ms\nstep:3052/10000 train_loss:4.1814 train_time:517167ms step_avg:170.01ms\nstep:3053/10000 train_loss:4.1875 train_time:517336ms step_avg:170.01ms\nstep:3054/10000 train_loss:4.2536 train_time:517510ms step_avg:170.01ms\nstep:3055/10000 train_loss:4.3669 train_time:517681ms step_avg:170.01ms\nstep:3056/10000 train_loss:4.0571 train_time:517850ms step_avg:170.01ms\nstep:3057/10000 train_loss:4.2458 train_time:518019ms step_avg:170.01ms\nstep:3058/10000 train_loss:4.3504 train_time:518188ms step_avg:170.01ms\nstep:3059/10000 train_loss:4.2372 train_time:518358ms step_avg:170.01ms\nstep:3060/10000 train_loss:4.2423 train_time:518527ms step_avg:170.01ms\nstep:3061/10000 train_loss:4.2974 train_time:518699ms step_avg:170.01ms\nstep:3062/10000 train_loss:4.0190 train_time:518866ms step_avg:170.01ms\nstep:3063/10000 train_loss:4.2708 train_time:519037ms step_avg:170.01ms\nstep:3064/10000 train_loss:4.2416 train_time:519205ms step_avg:170.01ms\nstep:3065/10000 train_loss:4.2171 train_time:519374ms step_avg:170.01ms\nstep:3066/10000 train_loss:4.4163 train_time:519542ms step_avg:170.01ms\nstep:3067/10000 train_loss:4.2425 train_time:519714ms step_avg:170.01ms\nstep:3068/10000 train_loss:5.2186 train_time:519883ms step_avg:170.01ms\nstep:3069/10000 train_loss:4.1879 train_time:520053ms step_avg:170.01ms\nstep:3070/10000 train_loss:4.3291 train_time:520223ms step_avg:170.01ms\nstep:3071/10000 train_loss:4.2711 train_time:520392ms step_avg:170.01ms\nstep:3072/10000 train_loss:4.2024 train_time:520562ms step_avg:170.01ms\nstep:3073/10000 train_loss:4.2614 train_time:520731ms step_avg:170.01ms\nstep:3074/10000 train_loss:4.2263 train_time:520900ms step_avg:170.01ms\nstep:3075/10000 train_loss:4.4586 train_time:521068ms step_avg:170.01ms\nstep:3076/10000 train_loss:4.2993 train_time:521237ms step_avg:170.01ms\nstep:3077/10000 train_loss:4.3076 train_time:521407ms step_avg:170.01ms\nstep:3078/10000 train_loss:4.2542 train_time:521578ms step_avg:170.01ms\nstep:3079/10000 train_loss:4.3184 train_time:521746ms step_avg:170.01ms\nstep:3080/10000 train_loss:4.3015 train_time:521916ms step_avg:170.01ms\nstep:3081/10000 train_loss:4.1743 train_time:522082ms step_avg:170.00ms\nstep:3082/10000 train_loss:4.2927 train_time:522250ms step_avg:170.00ms\nstep:3083/10000 train_loss:4.2484 train_time:522419ms step_avg:170.00ms\nstep:3084/10000 train_loss:4.8263 train_time:522587ms step_avg:170.00ms\nstep:3085/10000 train_loss:4.1142 train_time:522755ms step_avg:170.00ms\nstep:3086/10000 train_loss:4.3470 train_time:522924ms step_avg:170.00ms\nstep:3087/10000 train_loss:4.0548 train_time:523094ms step_avg:170.00ms\nstep:3088/10000 train_loss:4.2965 train_time:523264ms step_avg:170.00ms\nstep:3089/10000 train_loss:4.3658 train_time:523436ms step_avg:170.00ms\nstep:3090/10000 train_loss:4.5486 train_time:523606ms step_avg:170.00ms\nstep:3091/10000 train_loss:4.2842 train_time:523778ms step_avg:170.00ms\nstep:3092/10000 train_loss:4.2493 train_time:523947ms step_avg:170.00ms\nstep:3093/10000 train_loss:4.2883 train_time:524117ms step_avg:170.00ms\nstep:3094/10000 train_loss:4.1236 train_time:524284ms step_avg:170.00ms\nstep:3095/10000 train_loss:4.1436 train_time:524455ms step_avg:170.00ms\nstep:3096/10000 train_loss:4.2922 train_time:524623ms step_avg:170.00ms\nstep:3097/10000 train_loss:4.2458 train_time:524791ms step_avg:170.00ms\nstep:3098/10000 train_loss:4.3686 train_time:524959ms step_avg:170.00ms\nstep:3099/10000 train_loss:4.2006 train_time:525127ms step_avg:170.00ms\nstep:3100/10000 train_loss:4.4127 train_time:525296ms step_avg:170.00ms\nstep:3101/10000 train_loss:4.1165 train_time:525467ms step_avg:170.00ms\nstep:3102/10000 train_loss:4.2980 train_time:525639ms step_avg:170.00ms\nstep:3103/10000 train_loss:4.5922 train_time:525807ms step_avg:170.00ms\nstep:3104/10000 train_loss:4.2161 train_time:525976ms step_avg:170.00ms\nstep:3105/10000 train_loss:4.0512 train_time:526143ms step_avg:170.00ms\nstep:3106/10000 train_loss:4.2598 train_time:526312ms step_avg:170.00ms\nstep:3107/10000 train_loss:4.1058 train_time:526482ms step_avg:170.00ms\nstep:3108/10000 train_loss:4.2833 train_time:526658ms step_avg:170.00ms\nstep:3109/10000 train_loss:4.2697 train_time:526825ms step_avg:170.00ms\nstep:3110/10000 train_loss:4.1358 train_time:526994ms step_avg:170.00ms\nstep:3111/10000 train_loss:4.2390 train_time:527162ms step_avg:170.00ms\nstep:3112/10000 train_loss:4.2876 train_time:527330ms step_avg:170.00ms\nstep:3113/10000 train_loss:4.2113 train_time:527499ms step_avg:170.00ms\nstep:3114/10000 train_loss:4.0836 train_time:527669ms step_avg:170.00ms\nstep:3115/10000 train_loss:4.3199 train_time:527840ms step_avg:170.00ms\nstep:3116/10000 train_loss:4.2580 train_time:528009ms step_avg:170.00ms\nstep:3117/10000 train_loss:4.1241 train_time:528180ms step_avg:170.00ms\nstep:3118/10000 train_loss:4.1409 train_time:528349ms step_avg:170.00ms\nstep:3119/10000 train_loss:4.3105 train_time:528521ms step_avg:170.00ms\nstep:3120/10000 train_loss:4.2278 train_time:528689ms step_avg:170.00ms\nstep:3121/10000 train_loss:4.3292 train_time:528857ms step_avg:170.00ms\nstep:3122/10000 train_loss:4.2080 train_time:529026ms step_avg:170.00ms\nstep:3123/10000 train_loss:4.2411 train_time:529196ms step_avg:170.00ms\nstep:3124/10000 train_loss:4.0804 train_time:529365ms step_avg:170.00ms\nstep:3125/10000 train_loss:4.2632 train_time:529536ms step_avg:170.00ms\nstep:3126/10000 train_loss:4.2704 train_time:529704ms step_avg:169.99ms\nstep:3127/10000 train_loss:4.2260 train_time:529873ms step_avg:169.99ms\nstep:3128/10000 train_loss:4.2279 train_time:530041ms step_avg:169.99ms\nstep:3129/10000 train_loss:4.2513 train_time:530210ms step_avg:169.99ms\nstep:3130/10000 train_loss:4.3226 train_time:530379ms step_avg:169.99ms\nstep:3131/10000 train_loss:4.0483 train_time:530548ms step_avg:169.99ms\nstep:3132/10000 train_loss:4.2458 train_time:530718ms step_avg:169.99ms\nstep:3133/10000 train_loss:4.1884 train_time:530888ms step_avg:169.99ms\nstep:3134/10000 train_loss:4.1815 train_time:531058ms step_avg:169.99ms\nstep:3135/10000 train_loss:4.1789 train_time:531226ms step_avg:169.99ms\nstep:3136/10000 train_loss:4.2166 train_time:531396ms step_avg:169.99ms\nstep:3137/10000 train_loss:4.2924 train_time:531565ms step_avg:169.99ms\nstep:3138/10000 train_loss:4.2625 train_time:531735ms step_avg:169.99ms\nstep:3139/10000 train_loss:4.1851 train_time:531905ms step_avg:169.99ms\nstep:3140/10000 train_loss:4.1789 train_time:532076ms step_avg:169.99ms\nstep:3141/10000 train_loss:4.1566 train_time:532243ms step_avg:169.99ms\nstep:3142/10000 train_loss:4.1919 train_time:532412ms step_avg:169.99ms\nstep:3143/10000 train_loss:4.1225 train_time:532583ms step_avg:169.99ms\nstep:3144/10000 train_loss:4.3034 train_time:532751ms step_avg:169.99ms\nstep:3145/10000 train_loss:4.2538 train_time:532921ms step_avg:169.99ms\nstep:3146/10000 train_loss:4.2208 train_time:533090ms step_avg:169.99ms\nstep:3147/10000 train_loss:4.2291 train_time:533262ms step_avg:169.99ms\nstep:3148/10000 train_loss:4.1750 train_time:533432ms step_avg:169.99ms\nstep:3149/10000 train_loss:4.0386 train_time:533602ms step_avg:169.99ms\nstep:3150/10000 train_loss:4.1214 train_time:533769ms step_avg:169.99ms\nstep:3151/10000 train_loss:4.2818 train_time:533940ms step_avg:169.99ms\nstep:3152/10000 train_loss:4.2424 train_time:534110ms step_avg:169.99ms\nstep:3153/10000 train_loss:4.3258 train_time:534279ms step_avg:169.99ms\nstep:3154/10000 train_loss:4.3860 train_time:534447ms step_avg:169.99ms\nstep:3155/10000 train_loss:4.2034 train_time:534615ms step_avg:169.99ms\nstep:3156/10000 train_loss:4.2500 train_time:534781ms step_avg:169.99ms\nstep:3157/10000 train_loss:4.2623 train_time:534951ms step_avg:169.99ms\nstep:3158/10000 train_loss:4.1673 train_time:535120ms step_avg:169.99ms\nstep:3159/10000 train_loss:4.1514 train_time:535290ms step_avg:169.99ms\nstep:3160/10000 train_loss:4.1973 train_time:535460ms step_avg:169.99ms\nstep:3161/10000 train_loss:4.2037 train_time:535631ms step_avg:169.99ms\nstep:3162/10000 train_loss:4.2399 train_time:535802ms step_avg:169.99ms\nstep:3163/10000 train_loss:4.2267 train_time:535970ms step_avg:169.99ms\nstep:3164/10000 train_loss:4.2105 train_time:536138ms step_avg:169.99ms\nstep:3165/10000 train_loss:4.6095 train_time:536307ms step_avg:169.99ms\nstep:3166/10000 train_loss:4.2803 train_time:536478ms step_avg:169.99ms\nstep:3167/10000 train_loss:4.1378 train_time:536647ms step_avg:169.99ms\nstep:3168/10000 train_loss:4.2722 train_time:536819ms step_avg:169.99ms\nstep:3169/10000 train_loss:4.2780 train_time:536989ms step_avg:169.99ms\nstep:3170/10000 train_loss:4.3962 train_time:537158ms step_avg:169.99ms\nstep:3171/10000 train_loss:4.4290 train_time:537326ms step_avg:169.99ms\nstep:3172/10000 train_loss:4.2865 train_time:537497ms step_avg:169.99ms\nstep:3173/10000 train_loss:4.1642 train_time:537666ms step_avg:169.99ms\nstep:3174/10000 train_loss:4.4303 train_time:537836ms step_avg:169.99ms\nstep:3175/10000 train_loss:4.4032 train_time:538005ms step_avg:169.99ms\nstep:3176/10000 train_loss:4.2645 train_time:538174ms step_avg:169.99ms\nstep:3177/10000 train_loss:4.3501 train_time:538344ms step_avg:169.99ms\nstep:3178/10000 train_loss:4.1963 train_time:538515ms step_avg:169.99ms\nstep:3179/10000 train_loss:4.1190 train_time:538683ms step_avg:169.99ms\nstep:3180/10000 train_loss:4.2115 train_time:538857ms step_avg:169.99ms\nstep:3181/10000 train_loss:4.2465 train_time:539023ms step_avg:169.99ms\nstep:3182/10000 train_loss:4.2463 train_time:539194ms step_avg:169.99ms\nstep:3183/10000 train_loss:4.1999 train_time:539365ms step_avg:169.99ms\nstep:3184/10000 train_loss:4.2964 train_time:539536ms step_avg:169.99ms\nstep:3185/10000 train_loss:5.2757 train_time:539704ms step_avg:169.99ms\nstep:3186/10000 train_loss:4.3811 train_time:539876ms step_avg:169.99ms\nstep:3187/10000 train_loss:4.2299 train_time:540045ms step_avg:169.99ms\nstep:3188/10000 train_loss:4.2873 train_time:540215ms step_avg:169.99ms\nstep:3189/10000 train_loss:4.2389 train_time:540383ms step_avg:169.99ms\nstep:3190/10000 train_loss:4.4604 train_time:540554ms step_avg:169.99ms\nstep:3191/10000 train_loss:4.6472 train_time:540722ms step_avg:169.98ms\nstep:3192/10000 train_loss:4.0808 train_time:540892ms step_avg:169.98ms\nstep:3193/10000 train_loss:4.2224 train_time:541062ms step_avg:169.98ms\nstep:3194/10000 train_loss:4.1916 train_time:541232ms step_avg:169.98ms\nstep:3195/10000 train_loss:4.3724 train_time:541404ms step_avg:169.99ms\nstep:3196/10000 train_loss:4.1260 train_time:541575ms step_avg:169.99ms\nstep:3197/10000 train_loss:4.2302 train_time:541745ms step_avg:169.99ms\nstep:3198/10000 train_loss:4.3252 train_time:541917ms step_avg:169.99ms\nstep:3199/10000 train_loss:4.3667 train_time:542086ms step_avg:169.99ms\nstep:3200/10000 train_loss:4.9740 train_time:542257ms step_avg:169.99ms\nstep:3201/10000 train_loss:4.1777 train_time:542428ms step_avg:169.99ms\nstep:3202/10000 train_loss:4.2292 train_time:542597ms step_avg:169.99ms\nstep:3203/10000 train_loss:4.3508 train_time:542766ms step_avg:169.99ms\nstep:3204/10000 train_loss:4.3552 train_time:542937ms step_avg:169.99ms\nstep:3205/10000 train_loss:4.2338 train_time:543107ms step_avg:169.99ms\nstep:3206/10000 train_loss:4.2451 train_time:543277ms step_avg:169.99ms\nstep:3207/10000 train_loss:4.1547 train_time:543445ms step_avg:169.99ms\nstep:3208/10000 train_loss:4.2360 train_time:543616ms step_avg:169.99ms\nstep:3209/10000 train_loss:4.3050 train_time:543787ms step_avg:169.99ms\nstep:3210/10000 train_loss:4.3368 train_time:543960ms step_avg:169.99ms\nstep:3211/10000 train_loss:4.4537 train_time:544130ms step_avg:169.99ms\nstep:3212/10000 train_loss:4.2753 train_time:544302ms step_avg:169.99ms\nstep:3213/10000 train_loss:4.4128 train_time:544469ms step_avg:169.99ms\nstep:3214/10000 train_loss:4.1653 train_time:544639ms step_avg:169.99ms\nstep:3215/10000 train_loss:4.2440 train_time:544807ms step_avg:169.99ms\nstep:3216/10000 train_loss:4.2630 train_time:544975ms step_avg:169.99ms\nstep:3217/10000 train_loss:4.1863 train_time:545146ms step_avg:169.99ms\nstep:3218/10000 train_loss:4.2236 train_time:545315ms step_avg:169.99ms\nstep:3219/10000 train_loss:4.2146 train_time:545483ms step_avg:169.99ms\nstep:3220/10000 train_loss:4.1769 train_time:545651ms step_avg:169.98ms\nstep:3221/10000 train_loss:4.1834 train_time:545820ms step_avg:169.98ms\nstep:3222/10000 train_loss:4.3026 train_time:545988ms step_avg:169.98ms\nstep:3223/10000 train_loss:4.1904 train_time:546160ms step_avg:169.98ms\nstep:3224/10000 train_loss:4.0945 train_time:546331ms step_avg:169.98ms\nstep:3225/10000 train_loss:4.1952 train_time:546500ms step_avg:169.98ms\nstep:3226/10000 train_loss:4.1955 train_time:546669ms step_avg:169.98ms\nstep:3227/10000 train_loss:4.3400 train_time:546840ms step_avg:169.98ms\nstep:3228/10000 train_loss:4.2218 train_time:547009ms step_avg:169.98ms\nstep:3229/10000 train_loss:4.2967 train_time:547179ms step_avg:169.98ms\nstep:3230/10000 train_loss:4.1856 train_time:547349ms step_avg:169.98ms\nstep:3231/10000 train_loss:4.1365 train_time:547517ms step_avg:169.98ms\nstep:3232/10000 train_loss:4.2573 train_time:547686ms step_avg:169.98ms\nstep:3233/10000 train_loss:4.3245 train_time:547856ms step_avg:169.98ms\nstep:3234/10000 train_loss:4.3044 train_time:548024ms step_avg:169.98ms\nstep:3235/10000 train_loss:4.1290 train_time:548195ms step_avg:169.98ms\nstep:3236/10000 train_loss:4.4820 train_time:548367ms step_avg:169.98ms\nstep:3237/10000 train_loss:4.2245 train_time:548538ms step_avg:169.98ms\nstep:3238/10000 train_loss:4.2154 train_time:548706ms step_avg:169.98ms\nstep:3239/10000 train_loss:4.2435 train_time:548874ms step_avg:169.98ms\nstep:3240/10000 train_loss:4.3245 train_time:549042ms step_avg:169.98ms\nstep:3241/10000 train_loss:4.3130 train_time:549212ms step_avg:169.98ms\nstep:3242/10000 train_loss:4.1253 train_time:549383ms step_avg:169.98ms\nstep:3243/10000 train_loss:4.6797 train_time:549556ms step_avg:169.98ms\nstep:3244/10000 train_loss:4.2118 train_time:549724ms step_avg:169.98ms\nstep:3245/10000 train_loss:4.2895 train_time:549896ms step_avg:169.98ms\nstep:3246/10000 train_loss:4.2330 train_time:550065ms step_avg:169.98ms\nstep:3247/10000 train_loss:4.1681 train_time:550236ms step_avg:169.98ms\nstep:3248/10000 train_loss:4.2596 train_time:550404ms step_avg:169.98ms\nstep:3249/10000 train_loss:4.2701 train_time:550575ms step_avg:169.98ms\nstep:3250/10000 train_loss:4.2702 train_time:550743ms step_avg:169.98ms\nstep:3251/10000 train_loss:4.2683 train_time:550914ms step_avg:169.98ms\nstep:3252/10000 train_loss:4.2333 train_time:551084ms step_avg:169.98ms\nstep:3253/10000 train_loss:4.4294 train_time:551255ms step_avg:169.98ms\nstep:3254/10000 train_loss:4.2312 train_time:551424ms step_avg:169.98ms\nstep:3255/10000 train_loss:4.2736 train_time:551595ms step_avg:169.98ms\nstep:3256/10000 train_loss:4.2595 train_time:551763ms step_avg:169.98ms\nstep:3257/10000 train_loss:4.2096 train_time:551932ms step_avg:169.98ms\nstep:3258/10000 train_loss:4.2923 train_time:552102ms step_avg:169.98ms\nstep:3259/10000 train_loss:4.2253 train_time:552273ms step_avg:169.98ms\nstep:3260/10000 train_loss:4.0837 train_time:552444ms step_avg:169.98ms\nstep:3261/10000 train_loss:4.4492 train_time:552616ms step_avg:169.98ms\nstep:3262/10000 train_loss:4.2389 train_time:552785ms step_avg:169.98ms\nstep:3263/10000 train_loss:4.4338 train_time:552954ms step_avg:169.98ms\nstep:3264/10000 train_loss:4.4291 train_time:553122ms step_avg:169.98ms\nstep:3265/10000 train_loss:4.1939 train_time:553292ms step_avg:169.98ms\nstep:3266/10000 train_loss:4.2161 train_time:553463ms step_avg:169.98ms\nstep:3267/10000 train_loss:4.1730 train_time:553636ms step_avg:169.98ms\nstep:3268/10000 train_loss:4.1764 train_time:553804ms step_avg:169.98ms\nstep:3269/10000 train_loss:4.2852 train_time:553975ms step_avg:169.98ms\nstep:3270/10000 train_loss:4.2673 train_time:554143ms step_avg:169.98ms\nstep:3271/10000 train_loss:4.1609 train_time:554313ms step_avg:169.98ms\nstep:3272/10000 train_loss:4.1738 train_time:554483ms step_avg:169.98ms\nstep:3273/10000 train_loss:4.1615 train_time:554653ms step_avg:169.98ms\nstep:3274/10000 train_loss:4.3692 train_time:554823ms step_avg:169.98ms\nstep:3275/10000 train_loss:4.1742 train_time:554993ms step_avg:169.98ms\nstep:3276/10000 train_loss:4.1670 train_time:555164ms step_avg:169.98ms\nstep:3277/10000 train_loss:4.2252 train_time:555334ms step_avg:169.98ms\nstep:3278/10000 train_loss:4.2009 train_time:555505ms step_avg:169.98ms\nstep:3279/10000 train_loss:4.1971 train_time:555676ms step_avg:169.98ms\nstep:3280/10000 train_loss:4.2338 train_time:555844ms step_avg:169.98ms\nstep:3281/10000 train_loss:4.1655 train_time:556014ms step_avg:169.98ms\nstep:3282/10000 train_loss:4.3922 train_time:556183ms step_avg:169.98ms\nstep:3283/10000 train_loss:4.2453 train_time:556353ms step_avg:169.98ms\nstep:3284/10000 train_loss:4.0442 train_time:556523ms step_avg:169.98ms\nstep:3285/10000 train_loss:4.1185 train_time:556695ms step_avg:169.98ms\nstep:3286/10000 train_loss:4.1666 train_time:556862ms step_avg:169.98ms\nstep:3287/10000 train_loss:4.2520 train_time:557032ms step_avg:169.98ms\nstep:3288/10000 train_loss:4.2086 train_time:557203ms step_avg:169.98ms\nstep:3289/10000 train_loss:4.1465 train_time:557372ms step_avg:169.98ms\nstep:3290/10000 train_loss:4.1789 train_time:557543ms step_avg:169.98ms\nstep:3291/10000 train_loss:4.2086 train_time:557712ms step_avg:169.98ms\nstep:3292/10000 train_loss:4.3013 train_time:557884ms step_avg:169.98ms\nstep:3293/10000 train_loss:4.1160 train_time:558052ms step_avg:169.98ms\nstep:3294/10000 train_loss:4.2745 train_time:558222ms step_avg:169.98ms\nstep:3295/10000 train_loss:4.1898 train_time:558392ms step_avg:169.98ms\nstep:3296/10000 train_loss:4.0082 train_time:558562ms step_avg:169.98ms\nstep:3297/10000 train_loss:4.1594 train_time:558731ms step_avg:169.98ms\nstep:3298/10000 train_loss:4.2269 train_time:558901ms step_avg:169.98ms\nstep:3299/10000 train_loss:4.1725 train_time:559072ms step_avg:169.98ms\nstep:3300/10000 train_loss:4.2161 train_time:559242ms step_avg:169.98ms\nstep:3301/10000 train_loss:4.1542 train_time:559412ms step_avg:169.98ms\nstep:3302/10000 train_loss:4.1547 train_time:559583ms step_avg:169.98ms\nstep:3303/10000 train_loss:4.2267 train_time:559752ms step_avg:169.98ms\nstep:3304/10000 train_loss:4.3495 train_time:559922ms step_avg:169.98ms\nstep:3305/10000 train_loss:4.1873 train_time:560092ms step_avg:169.98ms\nstep:3306/10000 train_loss:4.2352 train_time:560261ms step_avg:169.98ms\nstep:3307/10000 train_loss:4.1784 train_time:560430ms step_avg:169.98ms\nstep:3308/10000 train_loss:4.1232 train_time:560602ms step_avg:169.98ms\nstep:3309/10000 train_loss:4.0429 train_time:560770ms step_avg:169.98ms\nstep:3310/10000 train_loss:4.1697 train_time:560940ms step_avg:169.98ms\nstep:3311/10000 train_loss:4.2769 train_time:561111ms step_avg:169.98ms\nstep:3312/10000 train_loss:4.1764 train_time:561279ms step_avg:169.98ms\nstep:3313/10000 train_loss:4.1688 train_time:561449ms step_avg:169.98ms\nstep:3314/10000 train_loss:4.3756 train_time:561620ms step_avg:169.98ms\nstep:3315/10000 train_loss:4.1722 train_time:561788ms step_avg:169.98ms\nstep:3316/10000 train_loss:4.2054 train_time:561959ms step_avg:169.98ms\nstep:3317/10000 train_loss:4.1738 train_time:562129ms step_avg:169.98ms\nstep:3318/10000 train_loss:4.2574 train_time:562301ms step_avg:169.98ms\nstep:3319/10000 train_loss:4.2254 train_time:562471ms step_avg:169.98ms\nstep:3320/10000 train_loss:4.2671 train_time:562640ms step_avg:169.98ms\nstep:3321/10000 train_loss:4.1110 train_time:562809ms step_avg:169.98ms\nstep:3322/10000 train_loss:4.3716 train_time:562980ms step_avg:169.98ms\nstep:3323/10000 train_loss:4.1744 train_time:563150ms step_avg:169.98ms\nstep:3324/10000 train_loss:4.0761 train_time:563320ms step_avg:169.98ms\nstep:3325/10000 train_loss:4.2302 train_time:563489ms step_avg:169.98ms\nstep:3326/10000 train_loss:4.1657 train_time:563661ms step_avg:169.98ms\nstep:3327/10000 train_loss:4.2575 train_time:563829ms step_avg:169.98ms\nstep:3328/10000 train_loss:4.2856 train_time:564002ms step_avg:169.98ms\nstep:3329/10000 train_loss:4.2039 train_time:564173ms step_avg:169.98ms\nstep:3330/10000 train_loss:4.2127 train_time:564343ms step_avg:169.98ms\nstep:3331/10000 train_loss:4.4643 train_time:564513ms step_avg:169.98ms\nstep:3332/10000 train_loss:4.2586 train_time:564680ms step_avg:169.98ms\nstep:3333/10000 train_loss:4.1743 train_time:564852ms step_avg:169.98ms\nstep:3334/10000 train_loss:4.1689 train_time:565017ms step_avg:169.98ms\nstep:3335/10000 train_loss:4.2587 train_time:565187ms step_avg:169.98ms\nstep:3336/10000 train_loss:4.2503 train_time:565359ms step_avg:169.98ms\nstep:3337/10000 train_loss:4.2119 train_time:565528ms step_avg:169.98ms\nstep:3338/10000 train_loss:4.2696 train_time:565697ms step_avg:169.98ms\nstep:3339/10000 train_loss:4.3407 train_time:565864ms step_avg:169.98ms\nstep:3340/10000 train_loss:4.1323 train_time:566034ms step_avg:169.98ms\nstep:3341/10000 train_loss:4.3240 train_time:566204ms step_avg:169.98ms\nstep:3342/10000 train_loss:4.1838 train_time:566376ms step_avg:169.98ms\nstep:3343/10000 train_loss:4.1898 train_time:566547ms step_avg:169.98ms\nstep:3344/10000 train_loss:4.3547 train_time:566717ms step_avg:169.98ms\nstep:3345/10000 train_loss:4.1115 train_time:566885ms step_avg:169.98ms\nstep:3346/10000 train_loss:4.1118 train_time:567053ms step_avg:169.98ms\nstep:3347/10000 train_loss:4.1183 train_time:567224ms step_avg:169.98ms\nstep:3348/10000 train_loss:4.0863 train_time:567394ms step_avg:169.98ms\nstep:3349/10000 train_loss:4.2370 train_time:567563ms step_avg:169.98ms\nstep:3350/10000 train_loss:4.3821 train_time:567734ms step_avg:169.98ms\nstep:3351/10000 train_loss:4.1403 train_time:567904ms step_avg:169.98ms\nstep:3352/10000 train_loss:4.0421 train_time:568073ms step_avg:169.98ms\nstep:3353/10000 train_loss:3.9956 train_time:568241ms step_avg:169.98ms\nstep:3354/10000 train_loss:4.0897 train_time:568411ms step_avg:169.98ms\nstep:3355/10000 train_loss:4.2324 train_time:568580ms step_avg:169.98ms\nstep:3356/10000 train_loss:4.2490 train_time:568749ms step_avg:169.98ms\nstep:3357/10000 train_loss:4.1803 train_time:568921ms step_avg:169.98ms\nstep:3358/10000 train_loss:4.2231 train_time:569088ms step_avg:169.98ms\nstep:3359/10000 train_loss:4.1518 train_time:569257ms step_avg:169.98ms\nstep:3360/10000 train_loss:4.1846 train_time:569426ms step_avg:169.98ms\nstep:3361/10000 train_loss:4.1214 train_time:569597ms step_avg:169.98ms\nstep:3362/10000 train_loss:4.0913 train_time:569765ms step_avg:169.98ms\nstep:3363/10000 train_loss:4.2165 train_time:569936ms step_avg:169.98ms\nstep:3364/10000 train_loss:4.2336 train_time:570104ms step_avg:169.98ms\nstep:3365/10000 train_loss:4.2333 train_time:570273ms step_avg:169.98ms\nstep:3366/10000 train_loss:4.1247 train_time:570442ms step_avg:169.98ms\nstep:3367/10000 train_loss:4.3001 train_time:570612ms step_avg:169.98ms\nstep:3368/10000 train_loss:4.1612 train_time:570780ms step_avg:169.98ms\nstep:3369/10000 train_loss:4.1555 train_time:570950ms step_avg:169.98ms\nstep:3370/10000 train_loss:4.2092 train_time:571122ms step_avg:169.98ms\nstep:3371/10000 train_loss:4.2135 train_time:571290ms step_avg:169.98ms\nstep:3372/10000 train_loss:4.1386 train_time:571460ms step_avg:169.98ms\nstep:3373/10000 train_loss:4.2561 train_time:571628ms step_avg:169.98ms\nstep:3374/10000 train_loss:4.1959 train_time:571796ms step_avg:169.97ms\nstep:3375/10000 train_loss:4.0491 train_time:571965ms step_avg:169.97ms\nstep:3376/10000 train_loss:4.1401 train_time:572137ms step_avg:169.98ms\nstep:3377/10000 train_loss:4.1808 train_time:572306ms step_avg:169.98ms\nstep:3378/10000 train_loss:4.2216 train_time:572477ms step_avg:169.98ms\nstep:3379/10000 train_loss:4.3117 train_time:572646ms step_avg:169.98ms\nstep:3380/10000 train_loss:4.2103 train_time:572816ms step_avg:169.98ms\nstep:3381/10000 train_loss:3.9590 train_time:572984ms step_avg:169.97ms\nstep:3382/10000 train_loss:4.0601 train_time:573154ms step_avg:169.97ms\nstep:3383/10000 train_loss:4.2144 train_time:573323ms step_avg:169.97ms\nstep:3384/10000 train_loss:4.1791 train_time:573492ms step_avg:169.97ms\nstep:3385/10000 train_loss:4.2750 train_time:573661ms step_avg:169.97ms\nstep:3386/10000 train_loss:4.3043 train_time:573830ms step_avg:169.97ms\nstep:3387/10000 train_loss:4.1503 train_time:574000ms step_avg:169.97ms\nstep:3388/10000 train_loss:4.2293 train_time:574169ms step_avg:169.97ms\nstep:3389/10000 train_loss:4.2190 train_time:574340ms step_avg:169.97ms\nstep:3390/10000 train_loss:4.1722 train_time:574509ms step_avg:169.97ms\nstep:3391/10000 train_loss:4.2286 train_time:574681ms step_avg:169.97ms\nstep:3392/10000 train_loss:4.1782 train_time:574849ms step_avg:169.97ms\nstep:3393/10000 train_loss:4.1522 train_time:575018ms step_avg:169.97ms\nstep:3394/10000 train_loss:4.1363 train_time:575186ms step_avg:169.97ms\nstep:3395/10000 train_loss:4.2587 train_time:575356ms step_avg:169.97ms\nstep:3396/10000 train_loss:4.1024 train_time:575525ms step_avg:169.97ms\nstep:3397/10000 train_loss:4.2619 train_time:575695ms step_avg:169.97ms\nstep:3398/10000 train_loss:4.2456 train_time:575864ms step_avg:169.97ms\nstep:3399/10000 train_loss:4.2938 train_time:576032ms step_avg:169.97ms\nstep:3400/10000 train_loss:4.2500 train_time:576200ms step_avg:169.97ms\nstep:3401/10000 train_loss:4.2120 train_time:576368ms step_avg:169.97ms\nstep:3402/10000 train_loss:4.1227 train_time:576539ms step_avg:169.97ms\nstep:3403/10000 train_loss:4.2331 train_time:576708ms step_avg:169.97ms\nstep:3404/10000 train_loss:4.1195 train_time:576877ms step_avg:169.97ms\nstep:3405/10000 train_loss:4.2292 train_time:577046ms step_avg:169.97ms\nstep:3406/10000 train_loss:4.0987 train_time:577216ms step_avg:169.97ms\nstep:3407/10000 train_loss:4.4898 train_time:577385ms step_avg:169.97ms\nstep:3408/10000 train_loss:4.2433 train_time:577556ms step_avg:169.97ms\nstep:3409/10000 train_loss:4.3123 train_time:577724ms step_avg:169.97ms\nstep:3410/10000 train_loss:4.4153 train_time:577896ms step_avg:169.97ms\nstep:3411/10000 train_loss:4.1952 train_time:578066ms step_avg:169.97ms\nstep:3412/10000 train_loss:3.9758 train_time:578234ms step_avg:169.97ms\nstep:3413/10000 train_loss:4.3540 train_time:578403ms step_avg:169.97ms\nstep:3414/10000 train_loss:4.1183 train_time:578572ms step_avg:169.97ms\nstep:3415/10000 train_loss:4.1971 train_time:578741ms step_avg:169.97ms\nstep:3416/10000 train_loss:4.3008 train_time:578909ms step_avg:169.97ms\nstep:3417/10000 train_loss:4.1177 train_time:579080ms step_avg:169.97ms\nstep:3418/10000 train_loss:4.2085 train_time:579249ms step_avg:169.97ms\nstep:3419/10000 train_loss:4.1912 train_time:579418ms step_avg:169.97ms\nstep:3420/10000 train_loss:4.1640 train_time:579587ms step_avg:169.97ms\nstep:3421/10000 train_loss:4.2202 train_time:579755ms step_avg:169.97ms\nstep:3422/10000 train_loss:4.2924 train_time:579923ms step_avg:169.97ms\nstep:3423/10000 train_loss:4.1877 train_time:580095ms step_avg:169.97ms\nstep:3424/10000 train_loss:3.9459 train_time:580265ms step_avg:169.97ms\nstep:3425/10000 train_loss:4.2184 train_time:580434ms step_avg:169.97ms\nstep:3426/10000 train_loss:4.2061 train_time:580603ms step_avg:169.97ms\nstep:3427/10000 train_loss:4.1165 train_time:580771ms step_avg:169.97ms\nstep:3428/10000 train_loss:4.2519 train_time:580940ms step_avg:169.96ms\nstep:3429/10000 train_loss:4.2530 train_time:581109ms step_avg:169.96ms\nstep:3430/10000 train_loss:4.1754 train_time:581278ms step_avg:169.96ms\nstep:3431/10000 train_loss:4.1896 train_time:581448ms step_avg:169.96ms\nstep:3432/10000 train_loss:4.3245 train_time:581619ms step_avg:169.96ms\nstep:3433/10000 train_loss:4.1931 train_time:581787ms step_avg:169.96ms\nstep:3434/10000 train_loss:4.2132 train_time:581961ms step_avg:169.97ms\nstep:3435/10000 train_loss:4.2093 train_time:582128ms step_avg:169.96ms\nstep:3436/10000 train_loss:4.0702 train_time:582298ms step_avg:169.96ms\nstep:3437/10000 train_loss:4.1953 train_time:582465ms step_avg:169.96ms\nstep:3438/10000 train_loss:4.1837 train_time:582636ms step_avg:169.96ms\nstep:3439/10000 train_loss:4.0333 train_time:582804ms step_avg:169.96ms\nstep:3440/10000 train_loss:4.1730 train_time:582976ms step_avg:169.96ms\nstep:3441/10000 train_loss:4.4972 train_time:583145ms step_avg:169.96ms\nstep:3442/10000 train_loss:4.2612 train_time:583315ms step_avg:169.96ms\nstep:3443/10000 train_loss:4.0700 train_time:583482ms step_avg:169.96ms\nstep:3444/10000 train_loss:4.3043 train_time:583651ms step_avg:169.96ms\nstep:3445/10000 train_loss:4.3252 train_time:583820ms step_avg:169.96ms\nstep:3446/10000 train_loss:4.2434 train_time:583990ms step_avg:169.96ms\nstep:3447/10000 train_loss:4.2087 train_time:584160ms step_avg:169.96ms\nstep:3448/10000 train_loss:4.0304 train_time:584328ms step_avg:169.96ms\nstep:3449/10000 train_loss:4.2259 train_time:584498ms step_avg:169.96ms\nstep:3450/10000 train_loss:4.3084 train_time:584666ms step_avg:169.96ms\nstep:3451/10000 train_loss:4.0472 train_time:584836ms step_avg:169.96ms\nstep:3452/10000 train_loss:4.1446 train_time:585010ms step_avg:169.96ms\nstep:3453/10000 train_loss:4.1479 train_time:585179ms step_avg:169.96ms\nstep:3454/10000 train_loss:4.1939 train_time:585350ms step_avg:169.96ms\nstep:3455/10000 train_loss:4.1106 train_time:585521ms step_avg:169.96ms\nstep:3456/10000 train_loss:4.4393 train_time:585689ms step_avg:169.96ms\nstep:3457/10000 train_loss:4.1757 train_time:585859ms step_avg:169.96ms\nstep:3458/10000 train_loss:4.2031 train_time:586028ms step_avg:169.96ms\nstep:3459/10000 train_loss:4.1642 train_time:586197ms step_avg:169.96ms\nstep:3460/10000 train_loss:4.4228 train_time:586366ms step_avg:169.96ms\nstep:3461/10000 train_loss:4.1478 train_time:586537ms step_avg:169.96ms\nstep:3462/10000 train_loss:4.0867 train_time:586705ms step_avg:169.96ms\nstep:3463/10000 train_loss:4.3156 train_time:586877ms step_avg:169.96ms\nstep:3464/10000 train_loss:4.2074 train_time:587045ms step_avg:169.96ms\nstep:3465/10000 train_loss:4.3320 train_time:587216ms step_avg:169.96ms\nstep:3466/10000 train_loss:4.2262 train_time:587383ms step_avg:169.96ms\nstep:3467/10000 train_loss:4.0710 train_time:587554ms step_avg:169.96ms\nstep:3468/10000 train_loss:4.1532 train_time:587721ms step_avg:169.96ms\nstep:3469/10000 train_loss:4.4756 train_time:587891ms step_avg:169.96ms\nstep:3470/10000 train_loss:4.5335 train_time:588058ms step_avg:169.96ms\nstep:3471/10000 train_loss:4.1398 train_time:588228ms step_avg:169.96ms\nstep:3472/10000 train_loss:4.4089 train_time:588396ms step_avg:169.96ms\nstep:3473/10000 train_loss:4.1444 train_time:588565ms step_avg:169.96ms\nstep:3474/10000 train_loss:4.0776 train_time:588736ms step_avg:169.96ms\nstep:3475/10000 train_loss:4.2976 train_time:588905ms step_avg:169.96ms\nstep:3476/10000 train_loss:4.1101 train_time:589076ms step_avg:169.96ms\nstep:3477/10000 train_loss:4.1739 train_time:589244ms step_avg:169.96ms\nstep:3478/10000 train_loss:4.2518 train_time:589413ms step_avg:169.96ms\nstep:3479/10000 train_loss:4.1655 train_time:589582ms step_avg:169.96ms\nstep:3480/10000 train_loss:4.0227 train_time:589754ms step_avg:169.96ms\nstep:3481/10000 train_loss:4.1659 train_time:589924ms step_avg:169.96ms\nstep:3482/10000 train_loss:4.2545 train_time:590093ms step_avg:169.96ms\nstep:3483/10000 train_loss:4.2652 train_time:590264ms step_avg:169.96ms\nstep:3484/10000 train_loss:4.2573 train_time:590436ms step_avg:169.96ms\nstep:3485/10000 train_loss:4.2748 train_time:590604ms step_avg:169.96ms\nstep:3486/10000 train_loss:4.0701 train_time:590775ms step_avg:169.96ms\nstep:3487/10000 train_loss:4.2749 train_time:590943ms step_avg:169.96ms\nstep:3488/10000 train_loss:4.1951 train_time:591112ms step_avg:169.96ms\nstep:3489/10000 train_loss:4.1013 train_time:591284ms step_avg:169.96ms\nstep:3490/10000 train_loss:4.0873 train_time:591454ms step_avg:169.96ms\nstep:3491/10000 train_loss:4.1285 train_time:591624ms step_avg:169.96ms\nstep:3492/10000 train_loss:4.1296 train_time:591797ms step_avg:169.96ms\nstep:3493/10000 train_loss:4.2833 train_time:591966ms step_avg:169.96ms\nstep:3494/10000 train_loss:4.2920 train_time:592134ms step_avg:169.96ms\nstep:3495/10000 train_loss:4.1951 train_time:592304ms step_avg:169.96ms\nstep:3496/10000 train_loss:4.2052 train_time:592474ms step_avg:169.96ms\nstep:3497/10000 train_loss:4.2491 train_time:592644ms step_avg:169.96ms\nstep:3498/10000 train_loss:4.0800 train_time:592813ms step_avg:169.96ms\nstep:3499/10000 train_loss:4.1938 train_time:592984ms step_avg:169.96ms\nstep:3500/10000 train_loss:4.1279 train_time:593154ms step_avg:169.96ms\nstep:3501/10000 train_loss:4.2379 train_time:593321ms step_avg:169.96ms\nstep:3502/10000 train_loss:3.6788 train_time:593494ms step_avg:169.96ms\nstep:3503/10000 train_loss:4.1757 train_time:593664ms step_avg:169.96ms\nstep:3504/10000 train_loss:4.2563 train_time:593834ms step_avg:169.96ms\nstep:3505/10000 train_loss:4.2071 train_time:594004ms step_avg:169.96ms\nstep:3506/10000 train_loss:4.2606 train_time:594175ms step_avg:169.96ms\nstep:3507/10000 train_loss:4.2012 train_time:594343ms step_avg:169.96ms\nstep:3508/10000 train_loss:3.9971 train_time:594515ms step_avg:169.96ms\nstep:3509/10000 train_loss:4.1973 train_time:594682ms step_avg:169.96ms\nstep:3510/10000 train_loss:4.1086 train_time:594853ms step_avg:169.96ms\nstep:3511/10000 train_loss:3.9703 train_time:595021ms step_avg:169.96ms\nstep:3512/10000 train_loss:4.2895 train_time:595192ms step_avg:169.96ms\nstep:3513/10000 train_loss:4.4565 train_time:595360ms step_avg:169.96ms\nstep:3514/10000 train_loss:4.2700 train_time:595530ms step_avg:169.96ms\nstep:3515/10000 train_loss:4.1498 train_time:595701ms step_avg:169.96ms\nstep:3516/10000 train_loss:4.1291 train_time:595870ms step_avg:169.96ms\nstep:3517/10000 train_loss:4.1150 train_time:596041ms step_avg:169.96ms\nstep:3518/10000 train_loss:4.1429 train_time:596210ms step_avg:169.96ms\nstep:3519/10000 train_loss:4.2112 train_time:596382ms step_avg:169.96ms\nstep:3520/10000 train_loss:4.4960 train_time:596551ms step_avg:169.96ms\nstep:3521/10000 train_loss:4.2140 train_time:596722ms step_avg:169.96ms\nstep:3522/10000 train_loss:4.2901 train_time:596892ms step_avg:169.96ms\nstep:3523/10000 train_loss:4.0205 train_time:597062ms step_avg:169.96ms\nstep:3524/10000 train_loss:4.2249 train_time:597231ms step_avg:169.96ms\nstep:3525/10000 train_loss:4.1292 train_time:597402ms step_avg:169.96ms\nstep:3526/10000 train_loss:3.9721 train_time:597570ms step_avg:169.96ms\nstep:3527/10000 train_loss:4.0987 train_time:597739ms step_avg:169.96ms\nstep:3528/10000 train_loss:4.0361 train_time:597908ms step_avg:169.96ms\nstep:3529/10000 train_loss:4.0809 train_time:598078ms step_avg:169.96ms\nstep:3530/10000 train_loss:4.1404 train_time:598247ms step_avg:169.96ms\nstep:3531/10000 train_loss:4.2541 train_time:598418ms step_avg:169.96ms\nstep:3532/10000 train_loss:4.3268 train_time:598588ms step_avg:169.96ms\nstep:3533/10000 train_loss:4.1282 train_time:598755ms step_avg:169.96ms\nstep:3534/10000 train_loss:4.3345 train_time:598923ms step_avg:169.96ms\nstep:3535/10000 train_loss:4.1695 train_time:599093ms step_avg:169.96ms\nstep:3536/10000 train_loss:4.0490 train_time:599264ms step_avg:169.96ms\nstep:3537/10000 train_loss:4.1882 train_time:599435ms step_avg:169.96ms\nstep:3538/10000 train_loss:4.4832 train_time:599604ms step_avg:169.96ms\nstep:3539/10000 train_loss:4.0629 train_time:599775ms step_avg:169.96ms\nstep:3540/10000 train_loss:4.2769 train_time:599946ms step_avg:169.96ms\nstep:3541/10000 train_loss:4.0699 train_time:600115ms step_avg:169.96ms\nstep:3542/10000 train_loss:4.2212 train_time:600285ms step_avg:169.96ms\nstep:3543/10000 train_loss:4.1477 train_time:600455ms step_avg:169.96ms\nstep:3544/10000 train_loss:4.1523 train_time:600624ms step_avg:169.96ms\nstep:3545/10000 train_loss:4.0538 train_time:600792ms step_avg:169.96ms\nstep:3546/10000 train_loss:4.1099 train_time:600961ms step_avg:169.96ms\nstep:3547/10000 train_loss:4.0777 train_time:601131ms step_avg:169.96ms\nstep:3548/10000 train_loss:4.3240 train_time:601302ms step_avg:169.96ms\nstep:3549/10000 train_loss:4.1550 train_time:601473ms step_avg:169.96ms\nstep:3550/10000 train_loss:4.2410 train_time:601643ms step_avg:169.96ms\nstep:3551/10000 train_loss:4.1802 train_time:601815ms step_avg:169.96ms\nstep:3552/10000 train_loss:4.1946 train_time:601984ms step_avg:169.96ms\nstep:3553/10000 train_loss:4.0715 train_time:602155ms step_avg:169.96ms\nstep:3554/10000 train_loss:4.6810 train_time:602323ms step_avg:169.96ms\nstep:3555/10000 train_loss:4.3474 train_time:602492ms step_avg:169.96ms\nstep:3556/10000 train_loss:4.2551 train_time:602658ms step_avg:169.95ms\nstep:3557/10000 train_loss:4.0532 train_time:602826ms step_avg:169.95ms\nstep:3558/10000 train_loss:4.4329 train_time:602996ms step_avg:169.95ms\nstep:3559/10000 train_loss:4.1451 train_time:603164ms step_avg:169.95ms\nstep:3560/10000 train_loss:4.2132 train_time:603338ms step_avg:169.95ms\nstep:3561/10000 train_loss:4.2329 train_time:603508ms step_avg:169.95ms\nstep:3562/10000 train_loss:4.1670 train_time:603677ms step_avg:169.95ms\nstep:3563/10000 train_loss:4.0238 train_time:603842ms step_avg:169.95ms\nstep:3564/10000 train_loss:4.0329 train_time:604011ms step_avg:169.95ms\nstep:3565/10000 train_loss:4.0465 train_time:604181ms step_avg:169.95ms\nstep:3566/10000 train_loss:4.1723 train_time:604352ms step_avg:169.95ms\nstep:3567/10000 train_loss:3.9368 train_time:604524ms step_avg:169.95ms\nstep:3568/10000 train_loss:4.1922 train_time:604695ms step_avg:169.95ms\nstep:3569/10000 train_loss:4.0800 train_time:604864ms step_avg:169.95ms\nstep:3570/10000 train_loss:3.9867 train_time:605032ms step_avg:169.95ms\nstep:3571/10000 train_loss:4.0135 train_time:605203ms step_avg:169.95ms\nstep:3572/10000 train_loss:4.0960 train_time:605375ms step_avg:169.95ms\nstep:3573/10000 train_loss:4.2616 train_time:605547ms step_avg:169.95ms\nstep:3574/10000 train_loss:4.2941 train_time:605717ms step_avg:169.95ms\nstep:3575/10000 train_loss:4.3278 train_time:605886ms step_avg:169.95ms\nstep:3576/10000 train_loss:4.1559 train_time:606055ms step_avg:169.95ms\nstep:3577/10000 train_loss:3.9961 train_time:606223ms step_avg:169.95ms\nstep:3578/10000 train_loss:4.0699 train_time:606397ms step_avg:169.95ms\nstep:3579/10000 train_loss:4.1332 train_time:606567ms step_avg:169.95ms\nstep:3580/10000 train_loss:3.9882 train_time:606739ms step_avg:169.95ms\nstep:3581/10000 train_loss:4.0650 train_time:606906ms step_avg:169.95ms\nstep:3582/10000 train_loss:4.4120 train_time:607074ms step_avg:169.95ms\nstep:3583/10000 train_loss:4.1758 train_time:607244ms step_avg:169.95ms\nstep:3584/10000 train_loss:3.9894 train_time:607416ms step_avg:169.95ms\nstep:3585/10000 train_loss:4.2925 train_time:607585ms step_avg:169.95ms\nstep:3586/10000 train_loss:4.3360 train_time:607756ms step_avg:169.95ms\nstep:3587/10000 train_loss:4.0891 train_time:607925ms step_avg:169.95ms\nstep:3588/10000 train_loss:4.0803 train_time:608096ms step_avg:169.95ms\nstep:3589/10000 train_loss:4.0550 train_time:608266ms step_avg:169.95ms\nstep:3590/10000 train_loss:4.1365 train_time:608437ms step_avg:169.95ms\nstep:3591/10000 train_loss:4.1266 train_time:608610ms step_avg:169.96ms\nstep:3592/10000 train_loss:4.2682 train_time:608778ms step_avg:169.95ms\nstep:3593/10000 train_loss:4.1736 train_time:608945ms step_avg:169.95ms\nstep:3594/10000 train_loss:4.1492 train_time:609114ms step_avg:169.95ms\nstep:3595/10000 train_loss:4.1813 train_time:609282ms step_avg:169.95ms\nstep:3596/10000 train_loss:4.1540 train_time:609454ms step_avg:169.95ms\nstep:3597/10000 train_loss:4.1844 train_time:609625ms step_avg:169.95ms\nstep:3598/10000 train_loss:4.2031 train_time:609795ms step_avg:169.95ms\nstep:3599/10000 train_loss:4.1265 train_time:609962ms step_avg:169.95ms\nstep:3600/10000 train_loss:4.1614 train_time:610133ms step_avg:169.95ms\nstep:3601/10000 train_loss:4.7281 train_time:610301ms step_avg:169.95ms\nstep:3602/10000 train_loss:4.1059 train_time:610471ms step_avg:169.95ms\nstep:3603/10000 train_loss:4.1060 train_time:610641ms step_avg:169.95ms\nstep:3604/10000 train_loss:4.3575 train_time:610810ms step_avg:169.95ms\nstep:3605/10000 train_loss:4.0767 train_time:610980ms step_avg:169.95ms\nstep:3606/10000 train_loss:4.0503 train_time:611149ms step_avg:169.95ms\nstep:3607/10000 train_loss:4.1405 train_time:611318ms step_avg:169.95ms\nstep:3608/10000 train_loss:4.1682 train_time:611486ms step_avg:169.95ms\nstep:3609/10000 train_loss:4.1172 train_time:611656ms step_avg:169.95ms\nstep:3610/10000 train_loss:4.2633 train_time:611825ms step_avg:169.95ms\nstep:3611/10000 train_loss:4.1077 train_time:611996ms step_avg:169.95ms\nstep:3612/10000 train_loss:4.0635 train_time:612164ms step_avg:169.95ms\nstep:3613/10000 train_loss:4.1795 train_time:612336ms step_avg:169.95ms\nstep:3614/10000 train_loss:4.2706 train_time:612504ms step_avg:169.95ms\nstep:3615/10000 train_loss:4.5459 train_time:612672ms step_avg:169.95ms\nstep:3616/10000 train_loss:4.3215 train_time:612839ms step_avg:169.95ms\nstep:3617/10000 train_loss:4.1409 train_time:613009ms step_avg:169.95ms\nstep:3618/10000 train_loss:4.1585 train_time:613178ms step_avg:169.95ms\nstep:3619/10000 train_loss:4.1191 train_time:613347ms step_avg:169.95ms\nstep:3620/10000 train_loss:4.2490 train_time:613517ms step_avg:169.95ms\nstep:3621/10000 train_loss:4.1469 train_time:613685ms step_avg:169.95ms\nstep:3622/10000 train_loss:4.1356 train_time:613855ms step_avg:169.95ms\nstep:3623/10000 train_loss:4.1972 train_time:614026ms step_avg:169.95ms\nstep:3624/10000 train_loss:4.1552 train_time:614198ms step_avg:169.95ms\nstep:3625/10000 train_loss:4.1049 train_time:614366ms step_avg:169.95ms\nstep:3626/10000 train_loss:4.1267 train_time:614537ms step_avg:169.95ms\nstep:3627/10000 train_loss:4.0186 train_time:614706ms step_avg:169.95ms\nstep:3628/10000 train_loss:4.0914 train_time:614876ms step_avg:169.95ms\nstep:3629/10000 train_loss:4.1097 train_time:615046ms step_avg:169.95ms\nstep:3630/10000 train_loss:4.3620 train_time:615216ms step_avg:169.95ms\nstep:3631/10000 train_loss:4.1424 train_time:615383ms step_avg:169.95ms\nstep:3632/10000 train_loss:4.3863 train_time:615554ms step_avg:169.95ms\nstep:3633/10000 train_loss:4.1675 train_time:615723ms step_avg:169.95ms\nstep:3634/10000 train_loss:4.1381 train_time:615894ms step_avg:169.95ms\nstep:3635/10000 train_loss:4.2481 train_time:616063ms step_avg:169.95ms\nstep:3636/10000 train_loss:4.1089 train_time:616236ms step_avg:169.95ms\nstep:3637/10000 train_loss:4.0872 train_time:616405ms step_avg:169.95ms\nstep:3638/10000 train_loss:4.1552 train_time:616574ms step_avg:169.95ms\nstep:3639/10000 train_loss:4.1368 train_time:616742ms step_avg:169.95ms\nstep:3640/10000 train_loss:4.2428 train_time:616910ms step_avg:169.95ms\nstep:3641/10000 train_loss:4.1450 train_time:617081ms step_avg:169.95ms\nstep:3642/10000 train_loss:4.0765 train_time:617249ms step_avg:169.95ms\nstep:3643/10000 train_loss:4.2418 train_time:617420ms step_avg:169.95ms\nstep:3644/10000 train_loss:4.4776 train_time:617589ms step_avg:169.95ms\nstep:3645/10000 train_loss:4.2402 train_time:617759ms step_avg:169.95ms\nstep:3646/10000 train_loss:4.1829 train_time:617927ms step_avg:169.95ms\nstep:3647/10000 train_loss:4.1198 train_time:618098ms step_avg:169.95ms\nstep:3648/10000 train_loss:4.0130 train_time:618267ms step_avg:169.95ms\nstep:3649/10000 train_loss:4.1201 train_time:618438ms step_avg:169.95ms\nstep:3650/10000 train_loss:4.2077 train_time:618607ms step_avg:169.95ms\nstep:3651/10000 train_loss:4.0726 train_time:618777ms step_avg:169.95ms\nstep:3652/10000 train_loss:4.2731 train_time:618944ms step_avg:169.95ms\nstep:3653/10000 train_loss:4.1436 train_time:619114ms step_avg:169.95ms\nstep:3654/10000 train_loss:4.2404 train_time:619283ms step_avg:169.95ms\nstep:3655/10000 train_loss:4.0635 train_time:619455ms step_avg:169.95ms\nstep:3656/10000 train_loss:4.2027 train_time:619623ms step_avg:169.95ms\nstep:3657/10000 train_loss:4.1826 train_time:619794ms step_avg:169.95ms\nstep:3658/10000 train_loss:4.1433 train_time:619961ms step_avg:169.95ms\nstep:3659/10000 train_loss:4.1799 train_time:620128ms step_avg:169.94ms\nstep:3660/10000 train_loss:4.1654 train_time:620298ms step_avg:169.94ms\nstep:3661/10000 train_loss:3.9858 train_time:620466ms step_avg:169.94ms\nstep:3662/10000 train_loss:4.3472 train_time:620635ms step_avg:169.94ms\nstep:3663/10000 train_loss:4.1737 train_time:620805ms step_avg:169.94ms\nstep:3664/10000 train_loss:4.3489 train_time:620974ms step_avg:169.94ms\nstep:3665/10000 train_loss:4.1651 train_time:621143ms step_avg:169.94ms\nstep:3666/10000 train_loss:4.1838 train_time:621313ms step_avg:169.94ms\nstep:3667/10000 train_loss:4.0713 train_time:621483ms step_avg:169.94ms\nstep:3668/10000 train_loss:4.0926 train_time:621653ms step_avg:169.94ms\nstep:3669/10000 train_loss:4.3503 train_time:621823ms step_avg:169.94ms\nstep:3670/10000 train_loss:4.2521 train_time:621993ms step_avg:169.94ms\nstep:3671/10000 train_loss:4.2137 train_time:622161ms step_avg:169.94ms\nstep:3672/10000 train_loss:4.0725 train_time:622330ms step_avg:169.94ms\nstep:3673/10000 train_loss:4.6161 train_time:622499ms step_avg:169.94ms\nstep:3674/10000 train_loss:4.2506 train_time:622669ms step_avg:169.94ms\nstep:3675/10000 train_loss:4.0873 train_time:622841ms step_avg:169.94ms\nstep:3676/10000 train_loss:4.1395 train_time:623010ms step_avg:169.94ms\nstep:3677/10000 train_loss:4.1427 train_time:623181ms step_avg:169.94ms\nstep:3678/10000 train_loss:5.3595 train_time:623350ms step_avg:169.94ms\nstep:3679/10000 train_loss:4.1067 train_time:623522ms step_avg:169.94ms\nstep:3680/10000 train_loss:4.0772 train_time:623693ms step_avg:169.94ms\nstep:3681/10000 train_loss:3.8213 train_time:623864ms step_avg:169.94ms\nstep:3682/10000 train_loss:4.1988 train_time:624034ms step_avg:169.94ms\nstep:3683/10000 train_loss:3.9584 train_time:624204ms step_avg:169.94ms\nstep:3684/10000 train_loss:4.1310 train_time:624373ms step_avg:169.94ms\nstep:3685/10000 train_loss:4.2370 train_time:624543ms step_avg:169.94ms\nstep:3686/10000 train_loss:4.1728 train_time:624714ms step_avg:169.94ms\nstep:3687/10000 train_loss:4.1989 train_time:624884ms step_avg:169.94ms\nstep:3688/10000 train_loss:4.0812 train_time:625054ms step_avg:169.94ms\nstep:3689/10000 train_loss:4.1378 train_time:625223ms step_avg:169.94ms\nstep:3690/10000 train_loss:4.1343 train_time:625392ms step_avg:169.94ms\nstep:3691/10000 train_loss:4.0508 train_time:625562ms step_avg:169.94ms\nstep:3692/10000 train_loss:4.1786 train_time:625732ms step_avg:169.94ms\nstep:3693/10000 train_loss:4.1485 train_time:625903ms step_avg:169.94ms\nstep:3694/10000 train_loss:4.2204 train_time:626073ms step_avg:169.94ms\nstep:3695/10000 train_loss:4.1092 train_time:626242ms step_avg:169.94ms\nstep:3696/10000 train_loss:4.1786 train_time:626411ms step_avg:169.94ms\nstep:3697/10000 train_loss:4.1161 train_time:626581ms step_avg:169.94ms\nstep:3698/10000 train_loss:4.2015 train_time:626753ms step_avg:169.94ms\nstep:3699/10000 train_loss:4.0599 train_time:626924ms step_avg:169.94ms\nstep:3700/10000 train_loss:4.0518 train_time:627095ms step_avg:169.94ms\nstep:3701/10000 train_loss:4.1338 train_time:627263ms step_avg:169.94ms\nstep:3702/10000 train_loss:4.2773 train_time:627431ms step_avg:169.94ms\nstep:3703/10000 train_loss:4.0314 train_time:627601ms step_avg:169.94ms\nstep:3704/10000 train_loss:4.1534 train_time:627771ms step_avg:169.94ms\nstep:3705/10000 train_loss:4.0852 train_time:627942ms step_avg:169.94ms\nstep:3706/10000 train_loss:4.2726 train_time:628114ms step_avg:169.94ms\nstep:3707/10000 train_loss:4.1265 train_time:628283ms step_avg:169.94ms\nstep:3708/10000 train_loss:4.0731 train_time:628452ms step_avg:169.94ms\nstep:3709/10000 train_loss:3.9526 train_time:628622ms step_avg:169.94ms\nstep:3710/10000 train_loss:4.1559 train_time:628790ms step_avg:169.94ms\nstep:3711/10000 train_loss:4.0488 train_time:628961ms step_avg:169.94ms\nstep:3712/10000 train_loss:4.2462 train_time:629131ms step_avg:169.94ms\nstep:3713/10000 train_loss:4.2794 train_time:629302ms step_avg:169.94ms\nstep:3714/10000 train_loss:4.1583 train_time:629474ms step_avg:169.94ms\nstep:3715/10000 train_loss:3.9823 train_time:629642ms step_avg:169.94ms\nstep:3716/10000 train_loss:4.1311 train_time:629813ms step_avg:169.94ms\nstep:3717/10000 train_loss:4.0564 train_time:629985ms step_avg:169.94ms\nstep:3718/10000 train_loss:4.3301 train_time:630156ms step_avg:169.94ms\nstep:3719/10000 train_loss:4.1932 train_time:630325ms step_avg:169.94ms\nstep:3720/10000 train_loss:3.9621 train_time:630496ms step_avg:169.94ms\nstep:3721/10000 train_loss:4.1739 train_time:630667ms step_avg:169.95ms\nstep:3722/10000 train_loss:4.2200 train_time:630836ms step_avg:169.94ms\nstep:3723/10000 train_loss:4.2376 train_time:631005ms step_avg:169.94ms\nstep:3724/10000 train_loss:4.3222 train_time:631176ms step_avg:169.95ms\nstep:3725/10000 train_loss:4.3361 train_time:631344ms step_avg:169.94ms\nstep:3726/10000 train_loss:3.9783 train_time:631514ms step_avg:169.94ms\nstep:3727/10000 train_loss:4.2380 train_time:631682ms step_avg:169.94ms\nstep:3728/10000 train_loss:4.0695 train_time:631851ms step_avg:169.94ms\nstep:3729/10000 train_loss:4.1277 train_time:632020ms step_avg:169.94ms\nstep:3730/10000 train_loss:4.1168 train_time:632192ms step_avg:169.94ms\nstep:3731/10000 train_loss:4.2053 train_time:632362ms step_avg:169.94ms\nstep:3732/10000 train_loss:4.0349 train_time:632531ms step_avg:169.94ms\nstep:3733/10000 train_loss:4.1236 train_time:632703ms step_avg:169.94ms\nstep:3734/10000 train_loss:4.2991 train_time:632871ms step_avg:169.94ms\nstep:3735/10000 train_loss:4.3979 train_time:633041ms step_avg:169.94ms\nstep:3736/10000 train_loss:4.1703 train_time:633210ms step_avg:169.94ms\nstep:3737/10000 train_loss:4.1371 train_time:633381ms step_avg:169.94ms\nstep:3738/10000 train_loss:4.0586 train_time:633551ms step_avg:169.94ms\nstep:3739/10000 train_loss:4.0589 train_time:633721ms step_avg:169.94ms\nstep:3740/10000 train_loss:4.1947 train_time:633889ms step_avg:169.94ms\nstep:3741/10000 train_loss:4.0922 train_time:634063ms step_avg:169.94ms\nstep:3742/10000 train_loss:4.1562 train_time:634233ms step_avg:169.94ms\nstep:3743/10000 train_loss:4.3096 train_time:634404ms step_avg:169.94ms\nstep:3744/10000 train_loss:4.0407 train_time:634572ms step_avg:169.94ms\nstep:3745/10000 train_loss:4.2975 train_time:634742ms step_avg:169.94ms\nstep:3746/10000 train_loss:4.0993 train_time:634913ms step_avg:169.94ms\nstep:3747/10000 train_loss:4.0452 train_time:635084ms step_avg:169.94ms\nstep:3748/10000 train_loss:4.0115 train_time:635254ms step_avg:169.94ms\nstep:3749/10000 train_loss:4.3840 train_time:635423ms step_avg:169.94ms\nstep:3750/10000 train_loss:4.1271 train_time:635594ms step_avg:169.94ms\nstep:3751/10000 train_loss:4.4031 train_time:635763ms step_avg:169.94ms\nstep:3752/10000 train_loss:4.1276 train_time:635934ms step_avg:169.94ms\nstep:3753/10000 train_loss:4.0899 train_time:636104ms step_avg:169.94ms\nstep:3754/10000 train_loss:4.2181 train_time:636274ms step_avg:169.95ms\nstep:3755/10000 train_loss:4.2228 train_time:636442ms step_avg:169.94ms\nstep:3756/10000 train_loss:3.9442 train_time:636611ms step_avg:169.94ms\nstep:3757/10000 train_loss:4.1042 train_time:636780ms step_avg:169.94ms\nstep:3758/10000 train_loss:4.1424 train_time:636951ms step_avg:169.94ms\nstep:3759/10000 train_loss:4.1080 train_time:637121ms step_avg:169.94ms\nstep:3760/10000 train_loss:4.1892 train_time:637289ms step_avg:169.94ms\nstep:3761/10000 train_loss:4.1815 train_time:637461ms step_avg:169.94ms\nstep:3762/10000 train_loss:4.3286 train_time:637630ms step_avg:169.94ms\nstep:3763/10000 train_loss:4.0647 train_time:637800ms step_avg:169.94ms\nstep:3764/10000 train_loss:4.0436 train_time:637969ms step_avg:169.94ms\nstep:3765/10000 train_loss:4.1906 train_time:638140ms step_avg:169.94ms\nstep:3766/10000 train_loss:4.1154 train_time:638309ms step_avg:169.94ms\nstep:3767/10000 train_loss:4.0406 train_time:638481ms step_avg:169.94ms\nstep:3768/10000 train_loss:4.0048 train_time:638648ms step_avg:169.94ms\nstep:3769/10000 train_loss:4.1109 train_time:638819ms step_avg:169.94ms\nstep:3770/10000 train_loss:4.1662 train_time:638987ms step_avg:169.94ms\nstep:3771/10000 train_loss:4.7245 train_time:639158ms step_avg:169.94ms\nstep:3772/10000 train_loss:4.1558 train_time:639327ms step_avg:169.94ms\nstep:3773/10000 train_loss:4.1560 train_time:639498ms step_avg:169.94ms\nstep:3774/10000 train_loss:3.9914 train_time:639667ms step_avg:169.94ms\nstep:3775/10000 train_loss:3.9803 train_time:639838ms step_avg:169.94ms\nstep:3776/10000 train_loss:4.0544 train_time:640006ms step_avg:169.94ms\nstep:3777/10000 train_loss:4.3253 train_time:640177ms step_avg:169.94ms\nstep:3778/10000 train_loss:4.2846 train_time:640345ms step_avg:169.94ms\nstep:3779/10000 train_loss:4.3847 train_time:640518ms step_avg:169.94ms\nstep:3780/10000 train_loss:4.0921 train_time:640688ms step_avg:169.94ms\nstep:3781/10000 train_loss:4.1423 train_time:640858ms step_avg:169.94ms\nstep:3782/10000 train_loss:4.0197 train_time:641026ms step_avg:169.94ms\nstep:3783/10000 train_loss:4.0957 train_time:641195ms step_avg:169.94ms\nstep:3784/10000 train_loss:4.3640 train_time:641365ms step_avg:169.94ms\nstep:3785/10000 train_loss:4.2012 train_time:641536ms step_avg:169.94ms\nstep:3786/10000 train_loss:4.0536 train_time:641703ms step_avg:169.94ms\nstep:3787/10000 train_loss:4.3016 train_time:641875ms step_avg:169.94ms\nstep:3788/10000 train_loss:4.1359 train_time:642045ms step_avg:169.94ms\nstep:3789/10000 train_loss:4.0588 train_time:642213ms step_avg:169.94ms\nstep:3790/10000 train_loss:5.5672 train_time:642379ms step_avg:169.94ms\nstep:3791/10000 train_loss:4.0647 train_time:642550ms step_avg:169.94ms\nstep:3792/10000 train_loss:4.2071 train_time:642720ms step_avg:169.94ms\nstep:3793/10000 train_loss:4.1954 train_time:642890ms step_avg:169.94ms\nstep:3794/10000 train_loss:4.1942 train_time:643061ms step_avg:169.94ms\nstep:3795/10000 train_loss:4.4887 train_time:643231ms step_avg:169.94ms\nstep:3796/10000 train_loss:4.3822 train_time:643402ms step_avg:169.94ms\nstep:3797/10000 train_loss:4.1790 train_time:643571ms step_avg:169.94ms\nstep:3798/10000 train_loss:4.1145 train_time:643742ms step_avg:169.94ms\nstep:3799/10000 train_loss:4.1433 train_time:643912ms step_avg:169.94ms\nstep:3800/10000 train_loss:3.9781 train_time:644081ms step_avg:169.94ms\nstep:3801/10000 train_loss:4.1642 train_time:644249ms step_avg:169.94ms\nstep:3802/10000 train_loss:4.0185 train_time:644420ms step_avg:169.94ms\nstep:3803/10000 train_loss:4.0837 train_time:644590ms step_avg:169.94ms\nstep:3804/10000 train_loss:4.0715 train_time:644760ms step_avg:169.94ms\nstep:3805/10000 train_loss:4.0466 train_time:644933ms step_avg:169.94ms\nstep:3806/10000 train_loss:4.0714 train_time:645104ms step_avg:169.94ms\nstep:3807/10000 train_loss:3.9936 train_time:645273ms step_avg:169.94ms\nstep:3808/10000 train_loss:4.3513 train_time:645443ms step_avg:169.94ms\nstep:3809/10000 train_loss:4.1302 train_time:645612ms step_avg:169.94ms\nstep:3810/10000 train_loss:4.2959 train_time:646002ms step_avg:170.00ms\nstep:3811/10000 train_loss:4.1656 train_time:646164ms step_avg:170.00ms\nstep:3812/10000 train_loss:4.0278 train_time:646332ms step_avg:170.00ms\nstep:3813/10000 train_loss:4.0883 train_time:646496ms step_avg:170.00ms\nstep:3814/10000 train_loss:4.1960 train_time:646665ms step_avg:170.00ms\nstep:3815/10000 train_loss:4.1335 train_time:646834ms step_avg:170.00ms\nstep:3816/10000 train_loss:4.0916 train_time:647011ms step_avg:170.00ms\nstep:3817/10000 train_loss:4.1180 train_time:647184ms step_avg:170.00ms\nstep:3818/10000 train_loss:3.8870 train_time:647352ms step_avg:170.00ms\nstep:3819/10000 train_loss:4.0704 train_time:647520ms step_avg:170.00ms\nstep:3820/10000 train_loss:4.1427 train_time:647688ms step_avg:170.00ms\nstep:3821/10000 train_loss:4.0912 train_time:647858ms step_avg:170.00ms\nstep:3822/10000 train_loss:4.0866 train_time:648028ms step_avg:170.00ms\nstep:3823/10000 train_loss:4.0496 train_time:648199ms step_avg:170.00ms\nstep:3824/10000 train_loss:4.0092 train_time:648369ms step_avg:170.00ms\nstep:3825/10000 train_loss:4.1764 train_time:648538ms step_avg:170.00ms\nstep:3826/10000 train_loss:4.0611 train_time:648705ms step_avg:170.00ms\nstep:3827/10000 train_loss:4.0762 train_time:648875ms step_avg:170.00ms\nstep:3828/10000 train_loss:4.0949 train_time:649044ms step_avg:170.00ms\nstep:3829/10000 train_loss:4.0716 train_time:649215ms step_avg:170.00ms\nstep:3830/10000 train_loss:4.0832 train_time:649384ms step_avg:170.00ms\nstep:3831/10000 train_loss:4.1734 train_time:649552ms step_avg:170.00ms\nstep:3832/10000 train_loss:4.0392 train_time:649722ms step_avg:170.00ms\nstep:3833/10000 train_loss:4.6363 train_time:649891ms step_avg:170.00ms\nstep:3834/10000 train_loss:4.0836 train_time:650060ms step_avg:169.99ms\nstep:3835/10000 train_loss:4.0406 train_time:650232ms step_avg:170.00ms\nstep:3836/10000 train_loss:4.1091 train_time:650400ms step_avg:169.99ms\nstep:3837/10000 train_loss:4.1559 train_time:650569ms step_avg:169.99ms\nstep:3838/10000 train_loss:4.2503 train_time:650738ms step_avg:169.99ms\nstep:3839/10000 train_loss:4.1150 train_time:650908ms step_avg:169.99ms\nstep:3840/10000 train_loss:4.0956 train_time:651077ms step_avg:169.99ms\nstep:3841/10000 train_loss:4.3528 train_time:651248ms step_avg:169.99ms\nstep:3842/10000 train_loss:4.2928 train_time:651419ms step_avg:169.99ms\nstep:3843/10000 train_loss:4.0101 train_time:651588ms step_avg:169.99ms\nstep:3844/10000 train_loss:4.1167 train_time:651757ms step_avg:169.99ms\nstep:3845/10000 train_loss:4.3685 train_time:651926ms step_avg:169.99ms\nstep:3846/10000 train_loss:4.2237 train_time:652095ms step_avg:169.99ms\nstep:3847/10000 train_loss:4.1521 train_time:652264ms step_avg:169.99ms\nstep:3848/10000 train_loss:3.9767 train_time:652435ms step_avg:169.99ms\nstep:3849/10000 train_loss:4.2751 train_time:652602ms step_avg:169.99ms\nstep:3850/10000 train_loss:4.1190 train_time:652771ms step_avg:169.99ms\nstep:3851/10000 train_loss:4.1380 train_time:652939ms step_avg:169.99ms\nstep:3852/10000 train_loss:4.2804 train_time:653108ms step_avg:169.99ms\nstep:3853/10000 train_loss:4.1191 train_time:653277ms step_avg:169.99ms\nstep:3854/10000 train_loss:3.9830 train_time:653450ms step_avg:169.99ms\nstep:3855/10000 train_loss:4.0892 train_time:653622ms step_avg:169.99ms\nstep:3856/10000 train_loss:4.2148 train_time:653790ms step_avg:169.99ms\nstep:3857/10000 train_loss:4.1405 train_time:653957ms step_avg:169.99ms\nstep:3858/10000 train_loss:4.0859 train_time:654123ms step_avg:169.99ms\nstep:3859/10000 train_loss:4.0405 train_time:654293ms step_avg:169.99ms\nstep:3860/10000 train_loss:4.0596 train_time:654462ms step_avg:169.99ms\nstep:3861/10000 train_loss:4.1453 train_time:654633ms step_avg:169.99ms\nstep:3862/10000 train_loss:4.1896 train_time:654804ms step_avg:169.99ms\nstep:3863/10000 train_loss:4.2439 train_time:654973ms step_avg:169.99ms\nstep:3864/10000 train_loss:4.0438 train_time:655141ms step_avg:169.99ms\nstep:3865/10000 train_loss:3.9846 train_time:655312ms step_avg:169.99ms\nstep:3866/10000 train_loss:4.2395 train_time:655482ms step_avg:169.99ms\nstep:3867/10000 train_loss:4.0876 train_time:655651ms step_avg:169.99ms\nstep:3868/10000 train_loss:4.0850 train_time:655818ms step_avg:169.99ms\nstep:3869/10000 train_loss:4.1246 train_time:655986ms step_avg:169.99ms\nstep:3870/10000 train_loss:4.1058 train_time:656154ms step_avg:169.99ms\nstep:3871/10000 train_loss:4.0896 train_time:656325ms step_avg:169.99ms\nstep:3872/10000 train_loss:4.2878 train_time:656497ms step_avg:169.99ms\nstep:3873/10000 train_loss:4.1686 train_time:656667ms step_avg:169.99ms\nstep:3874/10000 train_loss:4.1713 train_time:656836ms step_avg:169.99ms\nstep:3875/10000 train_loss:4.0136 train_time:657005ms step_avg:169.99ms\nstep:3876/10000 train_loss:4.1416 train_time:657175ms step_avg:169.99ms\nstep:3877/10000 train_loss:4.1379 train_time:657344ms step_avg:169.99ms\nstep:3878/10000 train_loss:4.1462 train_time:657516ms step_avg:169.99ms\nstep:3879/10000 train_loss:4.1782 train_time:657685ms step_avg:169.99ms\nstep:3880/10000 train_loss:4.1429 train_time:657857ms step_avg:169.99ms\nstep:3881/10000 train_loss:4.0370 train_time:658026ms step_avg:169.99ms\nstep:3882/10000 train_loss:4.1711 train_time:658198ms step_avg:169.99ms\nstep:3883/10000 train_loss:4.0405 train_time:658367ms step_avg:169.99ms\nstep:3884/10000 train_loss:4.1506 train_time:658539ms step_avg:169.99ms\nstep:3885/10000 train_loss:4.1108 train_time:658708ms step_avg:169.99ms\nstep:3886/10000 train_loss:4.0457 train_time:658879ms step_avg:169.99ms\nstep:3887/10000 train_loss:4.0051 train_time:659047ms step_avg:169.99ms\nstep:3888/10000 train_loss:4.0022 train_time:659218ms step_avg:169.99ms\nstep:3889/10000 train_loss:4.2565 train_time:659386ms step_avg:169.99ms\nstep:3890/10000 train_loss:3.9592 train_time:659556ms step_avg:169.99ms\nstep:3891/10000 train_loss:4.1503 train_time:659723ms step_avg:169.99ms\nstep:3892/10000 train_loss:4.1388 train_time:659895ms step_avg:169.99ms\nstep:3893/10000 train_loss:4.1166 train_time:660064ms step_avg:169.99ms\nstep:3894/10000 train_loss:4.1408 train_time:660236ms step_avg:169.99ms\nstep:3895/10000 train_loss:4.0186 train_time:660405ms step_avg:169.99ms\nstep:3896/10000 train_loss:4.1023 train_time:660576ms step_avg:169.99ms\nstep:3897/10000 train_loss:4.0595 train_time:660745ms step_avg:169.99ms\nstep:3898/10000 train_loss:3.9794 train_time:660914ms step_avg:169.99ms\nstep:3899/10000 train_loss:3.9667 train_time:661083ms step_avg:169.99ms\nstep:3900/10000 train_loss:4.1589 train_time:661257ms step_avg:169.99ms\nstep:3901/10000 train_loss:4.0863 train_time:661425ms step_avg:169.99ms\nstep:3902/10000 train_loss:4.1168 train_time:661596ms step_avg:169.99ms\nstep:3903/10000 train_loss:4.1875 train_time:661764ms step_avg:169.99ms\nstep:3904/10000 train_loss:4.2427 train_time:661934ms step_avg:169.99ms\nstep:3905/10000 train_loss:3.9945 train_time:662103ms step_avg:169.99ms\nstep:3906/10000 train_loss:4.1792 train_time:662274ms step_avg:169.99ms\nstep:3907/10000 train_loss:4.0743 train_time:662445ms step_avg:169.99ms\nstep:3908/10000 train_loss:4.0820 train_time:662617ms step_avg:169.99ms\nstep:3909/10000 train_loss:4.0840 train_time:662786ms step_avg:169.99ms\nstep:3910/10000 train_loss:4.0608 train_time:662956ms step_avg:169.99ms\nstep:3911/10000 train_loss:4.4217 train_time:663124ms step_avg:169.99ms\nstep:3912/10000 train_loss:4.2813 train_time:663296ms step_avg:169.99ms\nstep:3913/10000 train_loss:4.0617 train_time:663466ms step_avg:169.99ms\nstep:3914/10000 train_loss:4.0978 train_time:663636ms step_avg:169.99ms\nstep:3915/10000 train_loss:4.2565 train_time:663804ms step_avg:169.99ms\nstep:3916/10000 train_loss:4.1674 train_time:663974ms step_avg:169.99ms\nstep:3917/10000 train_loss:4.0327 train_time:664143ms step_avg:169.99ms\nstep:3918/10000 train_loss:4.1627 train_time:664314ms step_avg:169.99ms\nstep:3919/10000 train_loss:4.1455 train_time:664484ms step_avg:169.99ms\nstep:3920/10000 train_loss:4.0249 train_time:664655ms step_avg:169.99ms\nstep:3921/10000 train_loss:4.1861 train_time:664823ms step_avg:169.99ms\nstep:3922/10000 train_loss:3.9830 train_time:664991ms step_avg:169.99ms\nstep:3923/10000 train_loss:4.0246 train_time:665160ms step_avg:169.99ms\nstep:3924/10000 train_loss:4.3179 train_time:665329ms step_avg:169.99ms\nstep:3925/10000 train_loss:4.1547 train_time:665500ms step_avg:169.99ms\nstep:3926/10000 train_loss:4.0872 train_time:665668ms step_avg:169.99ms\nstep:3927/10000 train_loss:4.0652 train_time:665835ms step_avg:169.99ms\nstep:3928/10000 train_loss:4.0218 train_time:666004ms step_avg:169.99ms\nstep:3929/10000 train_loss:4.2186 train_time:666176ms step_avg:169.99ms\nstep:3930/10000 train_loss:4.0479 train_time:666345ms step_avg:169.99ms\nstep:3931/10000 train_loss:4.1445 train_time:666515ms step_avg:169.99ms\nstep:3932/10000 train_loss:4.1786 train_time:666685ms step_avg:169.99ms\nstep:3933/10000 train_loss:4.1041 train_time:666856ms step_avg:169.99ms\nstep:3934/10000 train_loss:4.0846 train_time:667024ms step_avg:169.99ms\nstep:3935/10000 train_loss:4.0965 train_time:667193ms step_avg:169.99ms\nstep:3936/10000 train_loss:4.1672 train_time:667364ms step_avg:169.99ms\nstep:3937/10000 train_loss:4.1236 train_time:667533ms step_avg:169.99ms\nstep:3938/10000 train_loss:4.2043 train_time:667702ms step_avg:169.99ms\nstep:3939/10000 train_loss:3.9554 train_time:667873ms step_avg:169.99ms\nstep:3940/10000 train_loss:4.2181 train_time:668041ms step_avg:169.99ms\nstep:3941/10000 train_loss:4.0983 train_time:668209ms step_avg:169.98ms\nstep:3942/10000 train_loss:4.0801 train_time:668379ms step_avg:169.98ms\nstep:3943/10000 train_loss:4.0536 train_time:668547ms step_avg:169.98ms\nstep:3944/10000 train_loss:4.1082 train_time:668719ms step_avg:169.98ms\nstep:3945/10000 train_loss:4.1912 train_time:668888ms step_avg:169.98ms\nstep:3946/10000 train_loss:4.0753 train_time:669058ms step_avg:169.98ms\nstep:3947/10000 train_loss:3.9913 train_time:669226ms step_avg:169.98ms\nstep:3948/10000 train_loss:4.2031 train_time:669394ms step_avg:169.98ms\nstep:3949/10000 train_loss:4.1583 train_time:669563ms step_avg:169.98ms\nstep:3950/10000 train_loss:4.0974 train_time:669735ms step_avg:169.98ms\nstep:3951/10000 train_loss:4.5541 train_time:669905ms step_avg:169.98ms\nstep:3952/10000 train_loss:4.0234 train_time:670075ms step_avg:169.98ms\nstep:3953/10000 train_loss:4.1914 train_time:670243ms step_avg:169.98ms\nstep:3954/10000 train_loss:4.2064 train_time:670413ms step_avg:169.98ms\nstep:3955/10000 train_loss:4.0127 train_time:670581ms step_avg:169.98ms\nstep:3956/10000 train_loss:4.0991 train_time:670750ms step_avg:169.98ms\nstep:3957/10000 train_loss:4.2590 train_time:670921ms step_avg:169.98ms\nstep:3958/10000 train_loss:3.9651 train_time:671091ms step_avg:169.98ms\nstep:3959/10000 train_loss:4.1921 train_time:671262ms step_avg:169.98ms\nstep:3960/10000 train_loss:4.0855 train_time:671432ms step_avg:169.98ms\nstep:3961/10000 train_loss:4.1499 train_time:671601ms step_avg:169.98ms\nstep:3962/10000 train_loss:4.0011 train_time:671770ms step_avg:169.98ms\nstep:3963/10000 train_loss:3.9285 train_time:671940ms step_avg:169.98ms\nstep:3964/10000 train_loss:4.0510 train_time:672109ms step_avg:169.98ms\nstep:3965/10000 train_loss:4.0746 train_time:672277ms step_avg:169.98ms\nstep:3966/10000 train_loss:4.1647 train_time:672446ms step_avg:169.98ms\nstep:3967/10000 train_loss:4.0291 train_time:672618ms step_avg:169.98ms\nstep:3968/10000 train_loss:4.1996 train_time:672784ms step_avg:169.98ms\nstep:3969/10000 train_loss:4.0294 train_time:672955ms step_avg:169.98ms\nstep:3970/10000 train_loss:4.0984 train_time:673124ms step_avg:169.98ms\nstep:3971/10000 train_loss:4.0978 train_time:673291ms step_avg:169.98ms\nstep:3972/10000 train_loss:4.0865 train_time:673459ms step_avg:169.98ms\nstep:3973/10000 train_loss:4.1226 train_time:673629ms step_avg:169.98ms\nstep:3974/10000 train_loss:4.1214 train_time:673798ms step_avg:169.98ms\nstep:3975/10000 train_loss:4.0116 train_time:673967ms step_avg:169.98ms\nstep:3976/10000 train_loss:4.0380 train_time:674139ms step_avg:169.98ms\nstep:3977/10000 train_loss:4.1587 train_time:674309ms step_avg:169.98ms\nstep:3978/10000 train_loss:4.0758 train_time:674478ms step_avg:169.98ms\nstep:3979/10000 train_loss:4.1387 train_time:674646ms step_avg:169.98ms\nstep:3980/10000 train_loss:4.1318 train_time:674817ms step_avg:169.98ms\nstep:3981/10000 train_loss:4.0153 train_time:674986ms step_avg:169.98ms\nstep:3982/10000 train_loss:4.0231 train_time:675157ms step_avg:169.98ms\nstep:3983/10000 train_loss:4.3743 train_time:675324ms step_avg:169.98ms\nstep:3984/10000 train_loss:4.1935 train_time:675496ms step_avg:169.98ms\nstep:3985/10000 train_loss:4.2206 train_time:675665ms step_avg:169.98ms\nstep:3986/10000 train_loss:4.0660 train_time:675836ms step_avg:169.98ms\nstep:3987/10000 train_loss:4.1682 train_time:676005ms step_avg:169.98ms\nstep:3988/10000 train_loss:4.1439 train_time:676176ms step_avg:169.98ms\nstep:3989/10000 train_loss:4.0472 train_time:676345ms step_avg:169.98ms\nstep:3990/10000 train_loss:4.0654 train_time:676515ms step_avg:169.98ms\nstep:3991/10000 train_loss:4.0295 train_time:676684ms step_avg:169.98ms\nstep:3992/10000 train_loss:4.0409 train_time:676854ms step_avg:169.98ms\nstep:3993/10000 train_loss:3.8764 train_time:677024ms step_avg:169.98ms\nstep:3994/10000 train_loss:4.1953 train_time:677196ms step_avg:169.98ms\nstep:3995/10000 train_loss:4.1096 train_time:677365ms step_avg:169.98ms\nstep:3996/10000 train_loss:4.4368 train_time:677537ms step_avg:169.98ms\nstep:3997/10000 train_loss:4.0131 train_time:677707ms step_avg:169.98ms\nstep:3998/10000 train_loss:4.0128 train_time:677877ms step_avg:169.98ms\nstep:3999/10000 train_loss:4.1265 train_time:678045ms step_avg:169.98ms\nstep:4000/10000 train_loss:4.0525 train_time:678215ms step_avg:169.98ms\nstep:4001/10000 train_loss:4.1548 train_time:678384ms step_avg:169.98ms\nstep:4002/10000 train_loss:4.2389 train_time:678556ms step_avg:169.98ms\nstep:4003/10000 train_loss:4.1392 train_time:678724ms step_avg:169.98ms\nstep:4004/10000 train_loss:4.1665 train_time:678893ms step_avg:169.98ms\nstep:4005/10000 train_loss:4.1328 train_time:679061ms step_avg:169.98ms\nstep:4006/10000 train_loss:4.1691 train_time:679232ms step_avg:169.98ms\nstep:4007/10000 train_loss:4.2230 train_time:679403ms step_avg:169.98ms\nstep:4008/10000 train_loss:4.0418 train_time:679575ms step_avg:169.98ms\nstep:4009/10000 train_loss:4.3105 train_time:679743ms step_avg:169.98ms\nstep:4010/10000 train_loss:4.1457 train_time:679911ms step_avg:169.98ms\nstep:4011/10000 train_loss:4.1762 train_time:680082ms step_avg:169.98ms\nstep:4012/10000 train_loss:4.1278 train_time:680252ms step_avg:169.98ms\nstep:4013/10000 train_loss:4.0853 train_time:680423ms step_avg:169.98ms\nstep:4014/10000 train_loss:4.1224 train_time:680594ms step_avg:169.98ms\nstep:4015/10000 train_loss:4.0895 train_time:680764ms step_avg:169.98ms\nstep:4016/10000 train_loss:3.9918 train_time:680935ms step_avg:169.98ms\nstep:4017/10000 train_loss:4.1614 train_time:681103ms step_avg:169.98ms\nstep:4018/10000 train_loss:4.1016 train_time:681271ms step_avg:169.98ms\nstep:4019/10000 train_loss:4.1517 train_time:681443ms step_avg:169.98ms\nstep:4020/10000 train_loss:4.1807 train_time:681615ms step_avg:169.98ms\nstep:4021/10000 train_loss:4.0410 train_time:681784ms step_avg:169.98ms\nstep:4022/10000 train_loss:4.1117 train_time:681953ms step_avg:169.98ms\nstep:4023/10000 train_loss:4.3288 train_time:682122ms step_avg:169.98ms\nstep:4024/10000 train_loss:4.2256 train_time:682290ms step_avg:169.98ms\nstep:4025/10000 train_loss:4.0950 train_time:682461ms step_avg:169.98ms\nstep:4026/10000 train_loss:4.0696 train_time:682630ms step_avg:169.98ms\nstep:4027/10000 train_loss:4.2880 train_time:682803ms step_avg:169.98ms\nstep:4028/10000 train_loss:4.0008 train_time:682974ms step_avg:169.98ms\nstep:4029/10000 train_loss:4.0107 train_time:683143ms step_avg:169.98ms\nstep:4030/10000 train_loss:4.0649 train_time:683310ms step_avg:169.98ms\nstep:4031/10000 train_loss:4.1695 train_time:683480ms step_avg:169.98ms\nstep:4032/10000 train_loss:4.0793 train_time:683650ms step_avg:169.98ms\nstep:4033/10000 train_loss:4.0380 train_time:683819ms step_avg:169.98ms\nstep:4034/10000 train_loss:4.0714 train_time:683988ms step_avg:169.98ms\nstep:4035/10000 train_loss:4.0461 train_time:684157ms step_avg:169.98ms\nstep:4036/10000 train_loss:4.0548 train_time:684326ms step_avg:169.98ms\nstep:4037/10000 train_loss:4.3210 train_time:684496ms step_avg:169.98ms\nstep:4038/10000 train_loss:4.0681 train_time:684664ms step_avg:169.98ms\nstep:4039/10000 train_loss:4.0969 train_time:684835ms step_avg:169.98ms\nstep:4040/10000 train_loss:4.1763 train_time:685003ms step_avg:169.98ms\nstep:4041/10000 train_loss:4.0539 train_time:685174ms step_avg:169.98ms\nstep:4042/10000 train_loss:4.3494 train_time:685343ms step_avg:169.98ms\nstep:4043/10000 train_loss:4.1594 train_time:685514ms step_avg:169.98ms\nstep:4044/10000 train_loss:4.2536 train_time:685682ms step_avg:169.98ms\nstep:4045/10000 train_loss:4.1322 train_time:685851ms step_avg:169.98ms\nstep:4046/10000 train_loss:4.2523 train_time:686018ms step_avg:169.97ms\nstep:4047/10000 train_loss:4.1999 train_time:686186ms step_avg:169.97ms\nstep:4048/10000 train_loss:4.1526 train_time:686357ms step_avg:169.97ms\nstep:4049/10000 train_loss:4.2234 train_time:686528ms step_avg:169.97ms\nstep:4050/10000 train_loss:4.0134 train_time:686698ms step_avg:169.97ms\nstep:4051/10000 train_loss:4.1473 train_time:686867ms step_avg:169.97ms\nstep:4052/10000 train_loss:4.1017 train_time:687038ms step_avg:169.97ms\nstep:4053/10000 train_loss:4.1090 train_time:687207ms step_avg:169.97ms\nstep:4054/10000 train_loss:4.0622 train_time:687377ms step_avg:169.97ms\nstep:4055/10000 train_loss:4.4294 train_time:687545ms step_avg:169.97ms\nstep:4056/10000 train_loss:3.9752 train_time:687716ms step_avg:169.97ms\nstep:4057/10000 train_loss:4.1825 train_time:687883ms step_avg:169.97ms\nstep:4058/10000 train_loss:4.0575 train_time:688052ms step_avg:169.97ms\nstep:4059/10000 train_loss:4.0867 train_time:688221ms step_avg:169.97ms\nstep:4060/10000 train_loss:4.1271 train_time:688391ms step_avg:169.97ms\nstep:4061/10000 train_loss:4.1506 train_time:688562ms step_avg:169.97ms\nstep:4062/10000 train_loss:4.1185 train_time:688732ms step_avg:169.97ms\nstep:4063/10000 train_loss:4.1729 train_time:688904ms step_avg:169.97ms\nstep:4064/10000 train_loss:4.2511 train_time:689073ms step_avg:169.97ms\nstep:4065/10000 train_loss:3.9923 train_time:689242ms step_avg:169.97ms\nstep:4066/10000 train_loss:4.1988 train_time:689411ms step_avg:169.97ms\nstep:4067/10000 train_loss:4.0306 train_time:689580ms step_avg:169.97ms\nstep:4068/10000 train_loss:3.9418 train_time:689752ms step_avg:169.97ms\nstep:4069/10000 train_loss:4.3361 train_time:689923ms step_avg:169.97ms\nstep:4070/10000 train_loss:4.0223 train_time:690092ms step_avg:169.97ms\nstep:4071/10000 train_loss:4.0587 train_time:690260ms step_avg:169.97ms\nstep:4072/10000 train_loss:4.0603 train_time:690429ms step_avg:169.97ms\nstep:4073/10000 train_loss:4.0832 train_time:690603ms step_avg:169.97ms\nstep:4074/10000 train_loss:4.0835 train_time:690770ms step_avg:169.97ms\nstep:4075/10000 train_loss:4.1025 train_time:690938ms step_avg:169.97ms\nstep:4076/10000 train_loss:4.0952 train_time:691105ms step_avg:169.97ms\nstep:4077/10000 train_loss:4.0913 train_time:691280ms step_avg:169.97ms\nstep:4078/10000 train_loss:4.0512 train_time:691445ms step_avg:169.97ms\nstep:4079/10000 train_loss:4.0961 train_time:691617ms step_avg:169.97ms\nstep:4080/10000 train_loss:3.9915 train_time:691784ms step_avg:169.97ms\nstep:4081/10000 train_loss:4.0001 train_time:691954ms step_avg:169.97ms\nstep:4082/10000 train_loss:4.2228 train_time:692122ms step_avg:169.97ms\nstep:4083/10000 train_loss:4.0959 train_time:692293ms step_avg:169.97ms\nstep:4084/10000 train_loss:4.1443 train_time:692465ms step_avg:169.97ms\nstep:4085/10000 train_loss:4.4444 train_time:692638ms step_avg:169.97ms\nstep:4086/10000 train_loss:3.9905 train_time:692808ms step_avg:169.97ms\nstep:4087/10000 train_loss:4.1223 train_time:692977ms step_avg:169.97ms\nstep:4088/10000 train_loss:4.0794 train_time:693146ms step_avg:169.97ms\nstep:4089/10000 train_loss:4.0718 train_time:693315ms step_avg:169.97ms\nstep:4090/10000 train_loss:4.3381 train_time:693484ms step_avg:169.97ms\nstep:4091/10000 train_loss:4.1119 train_time:693655ms step_avg:169.97ms\nstep:4092/10000 train_loss:3.9968 train_time:693829ms step_avg:169.97ms\nstep:4093/10000 train_loss:4.0458 train_time:693996ms step_avg:169.97ms\nstep:4094/10000 train_loss:4.1967 train_time:694163ms step_avg:169.97ms\nstep:4095/10000 train_loss:4.1532 train_time:694336ms step_avg:169.97ms\nstep:4096/10000 train_loss:4.0879 train_time:694511ms step_avg:169.97ms\nstep:4097/10000 train_loss:4.0839 train_time:694678ms step_avg:169.97ms\nstep:4098/10000 train_loss:3.9359 train_time:694851ms step_avg:169.97ms\nstep:4099/10000 train_loss:4.1749 train_time:695019ms step_avg:169.97ms\nstep:4100/10000 train_loss:4.1205 train_time:695184ms step_avg:169.97ms\nstep:4101/10000 train_loss:4.1842 train_time:695355ms step_avg:169.97ms\nstep:4102/10000 train_loss:4.0987 train_time:695525ms step_avg:169.97ms\nstep:4103/10000 train_loss:4.1032 train_time:695694ms step_avg:169.97ms\nstep:4104/10000 train_loss:4.0745 train_time:695864ms step_avg:169.97ms\nstep:4105/10000 train_loss:4.1046 train_time:696034ms step_avg:169.97ms\nstep:4106/10000 train_loss:4.0574 train_time:696204ms step_avg:169.97ms\nstep:4107/10000 train_loss:4.1516 train_time:696373ms step_avg:169.97ms\nstep:4108/10000 train_loss:4.1332 train_time:696542ms step_avg:169.97ms\nstep:4109/10000 train_loss:4.0081 train_time:696717ms step_avg:169.97ms\nstep:4110/10000 train_loss:4.0942 train_time:696883ms step_avg:169.97ms\nstep:4111/10000 train_loss:4.2179 train_time:697054ms step_avg:169.97ms\nstep:4112/10000 train_loss:4.2711 train_time:697224ms step_avg:169.97ms\nstep:4113/10000 train_loss:3.9915 train_time:697395ms step_avg:169.97ms\nstep:4114/10000 train_loss:4.0549 train_time:697564ms step_avg:169.97ms\nstep:4115/10000 train_loss:4.1297 train_time:697733ms step_avg:169.97ms\nstep:4116/10000 train_loss:3.9338 train_time:697903ms step_avg:169.97ms\nstep:4117/10000 train_loss:4.2441 train_time:698073ms step_avg:169.97ms\nstep:4118/10000 train_loss:3.9741 train_time:698241ms step_avg:169.97ms\nstep:4119/10000 train_loss:4.1243 train_time:698412ms step_avg:169.97ms\nstep:4120/10000 train_loss:3.9618 train_time:698582ms step_avg:169.97ms\nstep:4121/10000 train_loss:4.0920 train_time:698750ms step_avg:169.97ms\nstep:4122/10000 train_loss:4.0676 train_time:698920ms step_avg:169.97ms\nstep:4123/10000 train_loss:3.9722 train_time:699088ms step_avg:169.97ms\nstep:4124/10000 train_loss:3.9844 train_time:699256ms step_avg:169.97ms\nstep:4125/10000 train_loss:4.1458 train_time:699425ms step_avg:169.97ms\nstep:4126/10000 train_loss:4.4929 train_time:699597ms step_avg:169.97ms\nstep:4127/10000 train_loss:3.9917 train_time:699767ms step_avg:169.97ms\nstep:4128/10000 train_loss:4.0798 train_time:699938ms step_avg:169.97ms\nstep:4129/10000 train_loss:4.0132 train_time:700106ms step_avg:169.97ms\nstep:4130/10000 train_loss:4.0270 train_time:700278ms step_avg:169.97ms\nstep:4131/10000 train_loss:4.1066 train_time:700445ms step_avg:169.97ms\nstep:4132/10000 train_loss:4.1104 train_time:700616ms step_avg:169.97ms\nstep:4133/10000 train_loss:3.9034 train_time:700784ms step_avg:169.97ms\nstep:4134/10000 train_loss:4.0155 train_time:700954ms step_avg:169.97ms\nstep:4135/10000 train_loss:4.0829 train_time:701122ms step_avg:169.97ms\nstep:4136/10000 train_loss:4.2672 train_time:701293ms step_avg:169.97ms\nstep:4137/10000 train_loss:4.1179 train_time:701464ms step_avg:169.97ms\nstep:4138/10000 train_loss:4.1017 train_time:701634ms step_avg:169.97ms\nstep:4139/10000 train_loss:4.1691 train_time:701800ms step_avg:169.97ms\nstep:4140/10000 train_loss:4.0944 train_time:701969ms step_avg:169.97ms\nstep:4141/10000 train_loss:4.0563 train_time:702136ms step_avg:169.97ms\nstep:4142/10000 train_loss:4.1109 train_time:702306ms step_avg:169.97ms\nstep:4143/10000 train_loss:4.1719 train_time:702478ms step_avg:169.97ms\nstep:4144/10000 train_loss:4.0786 train_time:702647ms step_avg:169.97ms\nstep:4145/10000 train_loss:4.1154 train_time:702815ms step_avg:169.97ms\nstep:4146/10000 train_loss:3.9578 train_time:702984ms step_avg:169.97ms\nstep:4147/10000 train_loss:4.2502 train_time:703153ms step_avg:169.97ms\nstep:4148/10000 train_loss:4.0150 train_time:703324ms step_avg:169.97ms\nstep:4149/10000 train_loss:4.0503 train_time:703495ms step_avg:169.97ms\nstep:4150/10000 train_loss:3.9331 train_time:703663ms step_avg:169.97ms\nstep:4151/10000 train_loss:4.0934 train_time:703834ms step_avg:169.97ms\nstep:4152/10000 train_loss:4.1164 train_time:704003ms step_avg:169.97ms\nstep:4153/10000 train_loss:4.0259 train_time:704175ms step_avg:169.97ms\nstep:4154/10000 train_loss:4.1512 train_time:704346ms step_avg:169.97ms\nstep:4155/10000 train_loss:4.2269 train_time:704519ms step_avg:169.97ms\nstep:4156/10000 train_loss:4.4354 train_time:704686ms step_avg:169.97ms\nstep:4157/10000 train_loss:4.0565 train_time:704857ms step_avg:169.97ms\nstep:4158/10000 train_loss:3.9527 train_time:705025ms step_avg:169.97ms\nstep:4159/10000 train_loss:4.1715 train_time:705194ms step_avg:169.97ms\nstep:4160/10000 train_loss:4.2189 train_time:705365ms step_avg:169.97ms\nstep:4161/10000 train_loss:4.1061 train_time:705536ms step_avg:169.97ms\nstep:4162/10000 train_loss:3.9924 train_time:705705ms step_avg:169.97ms\nstep:4163/10000 train_loss:4.0466 train_time:705875ms step_avg:169.97ms\nstep:4164/10000 train_loss:4.1127 train_time:706044ms step_avg:169.97ms\nstep:4165/10000 train_loss:3.9911 train_time:706214ms step_avg:169.97ms\nstep:4166/10000 train_loss:4.0964 train_time:706382ms step_avg:169.97ms\nstep:4167/10000 train_loss:4.1055 train_time:706553ms step_avg:169.97ms\nstep:4168/10000 train_loss:4.0228 train_time:706722ms step_avg:169.97ms\nstep:4169/10000 train_loss:4.1009 train_time:706894ms step_avg:169.97ms\nstep:4170/10000 train_loss:3.9825 train_time:707063ms step_avg:169.97ms\nstep:4171/10000 train_loss:4.1574 train_time:707233ms step_avg:169.97ms\nstep:4172/10000 train_loss:4.0611 train_time:707401ms step_avg:169.97ms\nstep:4173/10000 train_loss:4.2772 train_time:707570ms step_avg:169.97ms\nstep:4174/10000 train_loss:4.0141 train_time:707739ms step_avg:169.97ms\nstep:4175/10000 train_loss:4.0165 train_time:707908ms step_avg:169.97ms\nstep:4176/10000 train_loss:4.1256 train_time:708077ms step_avg:169.97ms\nstep:4177/10000 train_loss:4.0949 train_time:708244ms step_avg:169.96ms\nstep:4178/10000 train_loss:4.1178 train_time:708412ms step_avg:169.96ms\nstep:4179/10000 train_loss:4.2698 train_time:708580ms step_avg:169.96ms\nstep:4180/10000 train_loss:4.0430 train_time:708749ms step_avg:169.96ms\nstep:4181/10000 train_loss:4.0568 train_time:708919ms step_avg:169.96ms\nstep:4182/10000 train_loss:4.2525 train_time:709090ms step_avg:169.96ms\nstep:4183/10000 train_loss:4.1814 train_time:709260ms step_avg:169.96ms\nstep:4184/10000 train_loss:4.0503 train_time:709430ms step_avg:169.96ms\nstep:4185/10000 train_loss:4.0209 train_time:709599ms step_avg:169.96ms\nstep:4186/10000 train_loss:3.9279 train_time:709768ms step_avg:169.96ms\nstep:4187/10000 train_loss:4.0338 train_time:709936ms step_avg:169.96ms\nstep:4188/10000 train_loss:4.1526 train_time:710104ms step_avg:169.96ms\nstep:4189/10000 train_loss:4.1260 train_time:710275ms step_avg:169.96ms\nstep:4190/10000 train_loss:4.2849 train_time:710443ms step_avg:169.96ms\nstep:4191/10000 train_loss:4.0595 train_time:710616ms step_avg:169.96ms\nstep:4192/10000 train_loss:4.0570 train_time:710785ms step_avg:169.96ms\nstep:4193/10000 train_loss:4.0693 train_time:710961ms step_avg:169.96ms\nstep:4194/10000 train_loss:3.9804 train_time:711125ms step_avg:169.96ms\nstep:4195/10000 train_loss:4.0692 train_time:711293ms step_avg:169.96ms\nstep:4196/10000 train_loss:4.2892 train_time:711464ms step_avg:169.96ms\nstep:4197/10000 train_loss:4.0523 train_time:711636ms step_avg:169.96ms\nstep:4198/10000 train_loss:4.5364 train_time:711803ms step_avg:169.96ms\nstep:4199/10000 train_loss:4.0779 train_time:711976ms step_avg:169.96ms\nstep:4200/10000 train_loss:4.1036 train_time:712144ms step_avg:169.96ms\nstep:4201/10000 train_loss:3.9123 train_time:712316ms step_avg:169.96ms\nstep:4202/10000 train_loss:4.1193 train_time:712483ms step_avg:169.96ms\nstep:4203/10000 train_loss:4.1061 train_time:712653ms step_avg:169.96ms\nstep:4204/10000 train_loss:4.0521 train_time:712821ms step_avg:169.96ms\nstep:4205/10000 train_loss:4.1290 train_time:712990ms step_avg:169.96ms\nstep:4206/10000 train_loss:4.0240 train_time:713160ms step_avg:169.96ms\nstep:4207/10000 train_loss:4.0416 train_time:713331ms step_avg:169.96ms\nstep:4208/10000 train_loss:3.9957 train_time:713503ms step_avg:169.96ms\nstep:4209/10000 train_loss:4.0807 train_time:713672ms step_avg:169.96ms\nstep:4210/10000 train_loss:3.9869 train_time:713843ms step_avg:169.96ms\nstep:4211/10000 train_loss:4.0674 train_time:714012ms step_avg:169.96ms\nstep:4212/10000 train_loss:4.1188 train_time:714182ms step_avg:169.96ms\nstep:4213/10000 train_loss:4.0912 train_time:714350ms step_avg:169.96ms\nstep:4214/10000 train_loss:4.1001 train_time:714522ms step_avg:169.96ms\nstep:4215/10000 train_loss:4.1402 train_time:714691ms step_avg:169.96ms\nstep:4216/10000 train_loss:4.0157 train_time:714862ms step_avg:169.96ms\nstep:4217/10000 train_loss:4.0564 train_time:715030ms step_avg:169.96ms\nstep:4218/10000 train_loss:4.0596 train_time:715199ms step_avg:169.96ms\nstep:4219/10000 train_loss:4.1410 train_time:715370ms step_avg:169.96ms\nstep:4220/10000 train_loss:4.3231 train_time:715540ms step_avg:169.96ms\nstep:4221/10000 train_loss:4.1213 train_time:715708ms step_avg:169.96ms\nstep:4222/10000 train_loss:4.0653 train_time:715878ms step_avg:169.96ms\nstep:4223/10000 train_loss:3.9985 train_time:716047ms step_avg:169.96ms\nstep:4224/10000 train_loss:4.1341 train_time:716216ms step_avg:169.96ms\nstep:4225/10000 train_loss:4.1622 train_time:716384ms step_avg:169.96ms\nstep:4226/10000 train_loss:4.0114 train_time:716554ms step_avg:169.96ms\nstep:4227/10000 train_loss:3.9727 train_time:716724ms step_avg:169.96ms\nstep:4228/10000 train_loss:4.0128 train_time:716895ms step_avg:169.96ms\nstep:4229/10000 train_loss:3.9924 train_time:717065ms step_avg:169.96ms\nstep:4230/10000 train_loss:4.0422 train_time:717234ms step_avg:169.96ms\nstep:4231/10000 train_loss:4.0728 train_time:717404ms step_avg:169.96ms\nstep:4232/10000 train_loss:4.1106 train_time:717574ms step_avg:169.96ms\nstep:4233/10000 train_loss:3.9744 train_time:717743ms step_avg:169.96ms\nstep:4234/10000 train_loss:4.1232 train_time:717914ms step_avg:169.96ms\nstep:4235/10000 train_loss:3.9686 train_time:718082ms step_avg:169.96ms\nstep:4236/10000 train_loss:4.1729 train_time:718250ms step_avg:169.96ms\nstep:4237/10000 train_loss:4.0542 train_time:718422ms step_avg:169.96ms\nstep:4238/10000 train_loss:4.2528 train_time:718593ms step_avg:169.96ms\nstep:4239/10000 train_loss:4.0144 train_time:718763ms step_avg:169.96ms\nstep:4240/10000 train_loss:3.9101 train_time:718934ms step_avg:169.96ms\nstep:4241/10000 train_loss:4.0179 train_time:719104ms step_avg:169.96ms\nstep:4242/10000 train_loss:4.0539 train_time:719273ms step_avg:169.96ms\nstep:4243/10000 train_loss:4.0658 train_time:719442ms step_avg:169.96ms\nstep:4244/10000 train_loss:4.0729 train_time:719613ms step_avg:169.96ms\nstep:4245/10000 train_loss:4.1119 train_time:719784ms step_avg:169.96ms\nstep:4246/10000 train_loss:4.2727 train_time:719955ms step_avg:169.96ms\nstep:4247/10000 train_loss:4.1071 train_time:720123ms step_avg:169.96ms\nstep:4248/10000 train_loss:4.0662 train_time:720293ms step_avg:169.96ms\nstep:4249/10000 train_loss:4.0544 train_time:720463ms step_avg:169.96ms\nstep:4250/10000 train_loss:3.9776 train_time:720633ms step_avg:169.96ms\nstep:4251/10000 train_loss:3.8861 train_time:720806ms step_avg:169.96ms\nstep:4252/10000 train_loss:4.0316 train_time:720977ms step_avg:169.96ms\nstep:4253/10000 train_loss:4.2103 train_time:721145ms step_avg:169.96ms\nstep:4254/10000 train_loss:4.0145 train_time:721314ms step_avg:169.96ms\nstep:4255/10000 train_loss:4.1440 train_time:721485ms step_avg:169.96ms\nstep:4256/10000 train_loss:3.9999 train_time:721652ms step_avg:169.96ms\nstep:4257/10000 train_loss:4.0188 train_time:721824ms step_avg:169.96ms\nstep:4258/10000 train_loss:4.1856 train_time:721996ms step_avg:169.96ms\nstep:4259/10000 train_loss:4.0730 train_time:722162ms step_avg:169.96ms\nstep:4260/10000 train_loss:4.1193 train_time:722333ms step_avg:169.96ms\nstep:4261/10000 train_loss:4.0083 train_time:722502ms step_avg:169.96ms\nstep:4262/10000 train_loss:4.0931 train_time:722672ms step_avg:169.96ms\nstep:4263/10000 train_loss:4.0506 train_time:722843ms step_avg:169.96ms\nstep:4264/10000 train_loss:4.0230 train_time:723016ms step_avg:169.96ms\nstep:4265/10000 train_loss:4.1350 train_time:723186ms step_avg:169.96ms\nstep:4266/10000 train_loss:4.2130 train_time:723356ms step_avg:169.96ms\nstep:4267/10000 train_loss:4.1813 train_time:723524ms step_avg:169.96ms\nstep:4268/10000 train_loss:3.9737 train_time:723694ms step_avg:169.96ms\nstep:4269/10000 train_loss:4.1328 train_time:723865ms step_avg:169.96ms\nstep:4270/10000 train_loss:3.7619 train_time:724036ms step_avg:169.96ms\nstep:4271/10000 train_loss:4.1958 train_time:724204ms step_avg:169.96ms\nstep:4272/10000 train_loss:3.9803 train_time:724375ms step_avg:169.96ms\nstep:4273/10000 train_loss:4.1469 train_time:724544ms step_avg:169.96ms\nstep:4274/10000 train_loss:3.9757 train_time:724714ms step_avg:169.96ms\nstep:4275/10000 train_loss:4.0577 train_time:724885ms step_avg:169.96ms\nstep:4276/10000 train_loss:3.9396 train_time:725057ms step_avg:169.96ms\nstep:4277/10000 train_loss:4.1053 train_time:725224ms step_avg:169.96ms\nstep:4278/10000 train_loss:4.0787 train_time:725393ms step_avg:169.96ms\nstep:4279/10000 train_loss:4.0127 train_time:725562ms step_avg:169.96ms\nstep:4280/10000 train_loss:4.0118 train_time:725730ms step_avg:169.96ms\nstep:4281/10000 train_loss:4.0014 train_time:725903ms step_avg:169.96ms\nstep:4282/10000 train_loss:4.0746 train_time:726076ms step_avg:169.96ms\nstep:4283/10000 train_loss:4.2690 train_time:726243ms step_avg:169.96ms\nstep:4284/10000 train_loss:3.9311 train_time:726413ms step_avg:169.96ms\nstep:4285/10000 train_loss:4.1400 train_time:726582ms step_avg:169.96ms\nstep:4286/10000 train_loss:4.0260 train_time:726749ms step_avg:169.96ms\nstep:4287/10000 train_loss:4.1485 train_time:726919ms step_avg:169.96ms\nstep:4288/10000 train_loss:4.1261 train_time:727089ms step_avg:169.96ms\nstep:4289/10000 train_loss:4.1991 train_time:727257ms step_avg:169.96ms\nstep:4290/10000 train_loss:3.9603 train_time:727426ms step_avg:169.96ms\nstep:4291/10000 train_loss:4.0169 train_time:727595ms step_avg:169.96ms\nstep:4292/10000 train_loss:4.0995 train_time:727765ms step_avg:169.96ms\nstep:4293/10000 train_loss:3.9997 train_time:727937ms step_avg:169.96ms\nstep:4294/10000 train_loss:4.0189 train_time:728105ms step_avg:169.96ms\nstep:4295/10000 train_loss:4.0184 train_time:728273ms step_avg:169.96ms\nstep:4296/10000 train_loss:4.0611 train_time:728441ms step_avg:169.96ms\nstep:4297/10000 train_loss:4.0117 train_time:728611ms step_avg:169.96ms\nstep:4298/10000 train_loss:4.1031 train_time:728778ms step_avg:169.96ms\nstep:4299/10000 train_loss:4.1555 train_time:728948ms step_avg:169.96ms\nstep:4300/10000 train_loss:4.1138 train_time:729122ms step_avg:169.96ms\nstep:4301/10000 train_loss:3.8501 train_time:729291ms step_avg:169.96ms\nstep:4302/10000 train_loss:3.9743 train_time:729459ms step_avg:169.96ms\nstep:4303/10000 train_loss:3.9678 train_time:729628ms step_avg:169.96ms\nstep:4304/10000 train_loss:4.1006 train_time:729796ms step_avg:169.96ms\nstep:4305/10000 train_loss:4.1964 train_time:729963ms step_avg:169.96ms\nstep:4306/10000 train_loss:4.1846 train_time:730132ms step_avg:169.96ms\nstep:4307/10000 train_loss:3.9801 train_time:730302ms step_avg:169.96ms\nstep:4308/10000 train_loss:4.0011 train_time:730472ms step_avg:169.96ms\nstep:4309/10000 train_loss:4.1309 train_time:730638ms step_avg:169.96ms\nstep:4310/10000 train_loss:4.0757 train_time:730807ms step_avg:169.96ms\nstep:4311/10000 train_loss:4.0979 train_time:730976ms step_avg:169.95ms\nstep:4312/10000 train_loss:4.3728 train_time:731144ms step_avg:169.95ms\nstep:4313/10000 train_loss:4.1747 train_time:731314ms step_avg:169.95ms\nstep:4314/10000 train_loss:4.0519 train_time:731485ms step_avg:169.95ms\nstep:4315/10000 train_loss:4.0794 train_time:731655ms step_avg:169.95ms\nstep:4316/10000 train_loss:4.1368 train_time:731821ms step_avg:169.95ms\nstep:4317/10000 train_loss:3.9430 train_time:731990ms step_avg:169.95ms\nstep:4318/10000 train_loss:4.0335 train_time:732163ms step_avg:169.95ms\nstep:4319/10000 train_loss:4.0739 train_time:732335ms step_avg:169.95ms\nstep:4320/10000 train_loss:3.8249 train_time:732505ms step_avg:169.95ms\nstep:4321/10000 train_loss:4.0137 train_time:732676ms step_avg:169.95ms\nstep:4322/10000 train_loss:4.1179 train_time:732845ms step_avg:169.95ms\nstep:4323/10000 train_loss:4.0946 train_time:733014ms step_avg:169.95ms\nstep:4324/10000 train_loss:4.1282 train_time:733184ms step_avg:169.95ms\nstep:4325/10000 train_loss:4.0605 train_time:733355ms step_avg:169.95ms\nstep:4326/10000 train_loss:4.1317 train_time:733524ms step_avg:169.95ms\nstep:4327/10000 train_loss:4.0295 train_time:733695ms step_avg:169.95ms\nstep:4328/10000 train_loss:4.0554 train_time:733862ms step_avg:169.95ms\nstep:4329/10000 train_loss:4.0773 train_time:734035ms step_avg:169.95ms\nstep:4330/10000 train_loss:4.0732 train_time:734203ms step_avg:169.95ms\nstep:4331/10000 train_loss:3.9772 train_time:734374ms step_avg:169.95ms\nstep:4332/10000 train_loss:4.1377 train_time:734540ms step_avg:169.95ms\nstep:4333/10000 train_loss:3.9617 train_time:734709ms step_avg:169.95ms\nstep:4334/10000 train_loss:4.0308 train_time:734880ms step_avg:169.95ms\nstep:4335/10000 train_loss:3.9946 train_time:735049ms step_avg:169.95ms\nstep:4336/10000 train_loss:4.2266 train_time:735223ms step_avg:169.95ms\nstep:4337/10000 train_loss:3.9152 train_time:735393ms step_avg:169.95ms\nstep:4338/10000 train_loss:3.8334 train_time:735563ms step_avg:169.95ms\nstep:4339/10000 train_loss:3.9866 train_time:735732ms step_avg:169.95ms\nstep:4340/10000 train_loss:4.1100 train_time:735902ms step_avg:169.95ms\nstep:4341/10000 train_loss:4.0132 train_time:736070ms step_avg:169.95ms\nstep:4342/10000 train_loss:3.9872 train_time:736240ms step_avg:169.95ms\nstep:4343/10000 train_loss:3.9656 train_time:736410ms step_avg:169.95ms\nstep:4344/10000 train_loss:4.1153 train_time:736580ms step_avg:169.95ms\nstep:4345/10000 train_loss:4.1171 train_time:736752ms step_avg:169.95ms\nstep:4346/10000 train_loss:4.1090 train_time:736922ms step_avg:169.95ms\nstep:4347/10000 train_loss:4.0018 train_time:737090ms step_avg:169.95ms\nstep:4348/10000 train_loss:4.0437 train_time:737261ms step_avg:169.95ms\nstep:4349/10000 train_loss:4.0853 train_time:737430ms step_avg:169.95ms\nstep:4350/10000 train_loss:3.9593 train_time:737603ms step_avg:169.95ms\nstep:4351/10000 train_loss:4.0504 train_time:737773ms step_avg:169.95ms\nstep:4352/10000 train_loss:4.2495 train_time:737941ms step_avg:169.95ms\nstep:4353/10000 train_loss:4.1189 train_time:738110ms step_avg:169.95ms\nstep:4354/10000 train_loss:3.9147 train_time:738280ms step_avg:169.95ms\nstep:4355/10000 train_loss:4.0943 train_time:738447ms step_avg:169.95ms\nstep:4356/10000 train_loss:4.1747 train_time:738614ms step_avg:169.95ms\nstep:4357/10000 train_loss:4.0972 train_time:738781ms step_avg:169.95ms\nstep:4358/10000 train_loss:4.0593 train_time:738951ms step_avg:169.95ms\nstep:4359/10000 train_loss:3.9022 train_time:739122ms step_avg:169.95ms\nstep:4360/10000 train_loss:3.9478 train_time:739295ms step_avg:169.95ms\nstep:4361/10000 train_loss:3.9821 train_time:739465ms step_avg:169.95ms\nstep:4362/10000 train_loss:4.0968 train_time:739635ms step_avg:169.95ms\nstep:4363/10000 train_loss:3.9975 train_time:739804ms step_avg:169.95ms\nstep:4364/10000 train_loss:4.0002 train_time:739973ms step_avg:169.95ms\nstep:4365/10000 train_loss:4.0187 train_time:740142ms step_avg:169.95ms\nstep:4366/10000 train_loss:4.1070 train_time:740314ms step_avg:169.95ms\nstep:4367/10000 train_loss:4.0047 train_time:740485ms step_avg:169.95ms\nstep:4368/10000 train_loss:4.1163 train_time:740655ms step_avg:169.95ms\nstep:4369/10000 train_loss:4.0723 train_time:740823ms step_avg:169.95ms\nstep:4370/10000 train_loss:4.0847 train_time:740992ms step_avg:169.95ms\nstep:4371/10000 train_loss:3.9934 train_time:741161ms step_avg:169.95ms\nstep:4372/10000 train_loss:3.9075 train_time:741331ms step_avg:169.95ms\nstep:4373/10000 train_loss:4.0200 train_time:741501ms step_avg:169.95ms\nstep:4374/10000 train_loss:3.9711 train_time:741671ms step_avg:169.95ms\nstep:4375/10000 train_loss:4.0409 train_time:741839ms step_avg:169.95ms\nstep:4376/10000 train_loss:4.1180 train_time:742007ms step_avg:169.95ms\nstep:4377/10000 train_loss:3.9882 train_time:742176ms step_avg:169.95ms\nstep:4378/10000 train_loss:3.9689 train_time:742346ms step_avg:169.95ms\nstep:4379/10000 train_loss:4.0302 train_time:742516ms step_avg:169.95ms\nstep:4380/10000 train_loss:4.2268 train_time:742686ms step_avg:169.95ms\nstep:4381/10000 train_loss:4.0702 train_time:742855ms step_avg:169.95ms\nstep:4382/10000 train_loss:4.2117 train_time:743024ms step_avg:169.95ms\nstep:4383/10000 train_loss:4.0296 train_time:743194ms step_avg:169.95ms\nstep:4384/10000 train_loss:4.0758 train_time:743363ms step_avg:169.95ms\nstep:4385/10000 train_loss:4.2107 train_time:743532ms step_avg:169.95ms\nstep:4386/10000 train_loss:4.0062 train_time:743700ms step_avg:169.95ms\nstep:4387/10000 train_loss:3.9801 train_time:743869ms step_avg:169.95ms\nstep:4388/10000 train_loss:4.0298 train_time:744040ms step_avg:169.95ms\nstep:4389/10000 train_loss:4.1136 train_time:744208ms step_avg:169.95ms\nstep:4390/10000 train_loss:3.9914 train_time:744379ms step_avg:169.95ms\nstep:4391/10000 train_loss:4.0767 train_time:744548ms step_avg:169.95ms\nstep:4392/10000 train_loss:4.0588 train_time:744717ms step_avg:169.95ms\nstep:4393/10000 train_loss:4.1287 train_time:744885ms step_avg:169.95ms\nstep:4394/10000 train_loss:4.0180 train_time:745054ms step_avg:169.95ms\nstep:4395/10000 train_loss:3.9614 train_time:745222ms step_avg:169.95ms\nstep:4396/10000 train_loss:4.1093 train_time:745394ms step_avg:169.95ms\nstep:4397/10000 train_loss:4.0651 train_time:745566ms step_avg:169.95ms\nstep:4398/10000 train_loss:4.0806 train_time:745735ms step_avg:169.95ms\nstep:4399/10000 train_loss:4.0848 train_time:745906ms step_avg:169.95ms\nstep:4400/10000 train_loss:4.1539 train_time:746077ms step_avg:169.95ms\nstep:4401/10000 train_loss:3.9569 train_time:746249ms step_avg:169.95ms\nstep:4402/10000 train_loss:4.1474 train_time:746421ms step_avg:169.95ms\nstep:4403/10000 train_loss:4.0161 train_time:746590ms step_avg:169.95ms\nstep:4404/10000 train_loss:4.1058 train_time:746764ms step_avg:169.95ms\nstep:4405/10000 train_loss:4.0230 train_time:746929ms step_avg:169.95ms\nstep:4406/10000 train_loss:4.0059 train_time:747099ms step_avg:169.95ms\nstep:4407/10000 train_loss:3.9499 train_time:747268ms step_avg:169.95ms\nstep:4408/10000 train_loss:4.0715 train_time:747439ms step_avg:169.95ms\nstep:4409/10000 train_loss:4.0818 train_time:747607ms step_avg:169.95ms\nstep:4410/10000 train_loss:4.0602 train_time:747777ms step_avg:169.95ms\nstep:4411/10000 train_loss:3.9761 train_time:747945ms step_avg:169.95ms\nstep:4412/10000 train_loss:3.9152 train_time:748114ms step_avg:169.95ms\nstep:4413/10000 train_loss:4.0890 train_time:748283ms step_avg:169.95ms\nstep:4414/10000 train_loss:3.9901 train_time:748457ms step_avg:169.95ms\nstep:4415/10000 train_loss:4.1906 train_time:748623ms step_avg:169.95ms\nstep:4416/10000 train_loss:4.0210 train_time:748795ms step_avg:169.95ms\nstep:4417/10000 train_loss:4.1278 train_time:748964ms step_avg:169.95ms\nstep:4418/10000 train_loss:3.9487 train_time:749136ms step_avg:169.95ms\nstep:4419/10000 train_loss:4.3292 train_time:749302ms step_avg:169.95ms\nstep:4420/10000 train_loss:4.0517 train_time:749476ms step_avg:169.95ms\nstep:4421/10000 train_loss:4.1106 train_time:749641ms step_avg:169.95ms\nstep:4422/10000 train_loss:4.0777 train_time:749810ms step_avg:169.95ms\nstep:4423/10000 train_loss:4.0625 train_time:749981ms step_avg:169.95ms\nstep:4424/10000 train_loss:3.9440 train_time:750153ms step_avg:169.95ms\nstep:4425/10000 train_loss:4.2859 train_time:750323ms step_avg:169.95ms\nstep:4426/10000 train_loss:4.1665 train_time:750492ms step_avg:169.95ms\nstep:4427/10000 train_loss:4.3141 train_time:750661ms step_avg:169.95ms\nstep:4428/10000 train_loss:4.6691 train_time:750832ms step_avg:169.95ms\nstep:4429/10000 train_loss:4.0692 train_time:751002ms step_avg:169.95ms\nstep:4430/10000 train_loss:4.0007 train_time:751172ms step_avg:169.95ms\nstep:4431/10000 train_loss:4.1298 train_time:751342ms step_avg:169.95ms\nstep:4432/10000 train_loss:3.8533 train_time:751511ms step_avg:169.95ms\nstep:4433/10000 train_loss:4.1089 train_time:751677ms step_avg:169.95ms\nstep:4434/10000 train_loss:4.2362 train_time:751845ms step_avg:169.95ms\nstep:4435/10000 train_loss:4.1298 train_time:752016ms step_avg:169.95ms\nstep:4436/10000 train_loss:4.1704 train_time:752185ms step_avg:169.95ms\nstep:4437/10000 train_loss:4.1209 train_time:752358ms step_avg:169.95ms\nstep:4438/10000 train_loss:3.9540 train_time:752527ms step_avg:169.95ms\nstep:4439/10000 train_loss:4.2604 train_time:752695ms step_avg:169.95ms\nstep:4440/10000 train_loss:3.9865 train_time:752863ms step_avg:169.95ms\nstep:4441/10000 train_loss:3.9980 train_time:753033ms step_avg:169.95ms\nstep:4442/10000 train_loss:4.0833 train_time:753200ms step_avg:169.95ms\nstep:4443/10000 train_loss:4.1770 train_time:753370ms step_avg:169.95ms\nstep:4444/10000 train_loss:4.0920 train_time:753544ms step_avg:169.95ms\nstep:4445/10000 train_loss:3.9895 train_time:753714ms step_avg:169.95ms\nstep:4446/10000 train_loss:4.0982 train_time:753883ms step_avg:169.95ms\nstep:4447/10000 train_loss:3.9486 train_time:754051ms step_avg:169.95ms\nstep:4448/10000 train_loss:4.0567 train_time:754221ms step_avg:169.95ms\nstep:4449/10000 train_loss:3.9997 train_time:754393ms step_avg:169.95ms\nstep:4450/10000 train_loss:4.0634 train_time:754562ms step_avg:169.95ms\nstep:4451/10000 train_loss:3.9954 train_time:754730ms step_avg:169.95ms\nstep:4452/10000 train_loss:4.0758 train_time:754896ms step_avg:169.95ms\nstep:4453/10000 train_loss:3.8703 train_time:755065ms step_avg:169.94ms\nstep:4454/10000 train_loss:4.0444 train_time:755235ms step_avg:169.94ms\nstep:4455/10000 train_loss:3.9118 train_time:755404ms step_avg:169.94ms\nstep:4456/10000 train_loss:4.1730 train_time:755574ms step_avg:169.94ms\nstep:4457/10000 train_loss:4.1341 train_time:755741ms step_avg:169.94ms\nstep:4458/10000 train_loss:3.9467 train_time:755911ms step_avg:169.94ms\nstep:4459/10000 train_loss:4.0617 train_time:756081ms step_avg:169.94ms\nstep:4460/10000 train_loss:4.0426 train_time:756252ms step_avg:169.94ms\nstep:4461/10000 train_loss:3.9998 train_time:756421ms step_avg:169.94ms\nstep:4462/10000 train_loss:4.1414 train_time:756592ms step_avg:169.94ms\nstep:4463/10000 train_loss:4.1546 train_time:756762ms step_avg:169.94ms\nstep:4464/10000 train_loss:3.9622 train_time:756931ms step_avg:169.94ms\nstep:4465/10000 train_loss:4.1206 train_time:757101ms step_avg:169.94ms\nstep:4466/10000 train_loss:4.1312 train_time:757272ms step_avg:169.94ms\nstep:4467/10000 train_loss:4.2684 train_time:757443ms step_avg:169.94ms\nstep:4468/10000 train_loss:3.9050 train_time:757613ms step_avg:169.94ms\nstep:4469/10000 train_loss:4.0545 train_time:757782ms step_avg:169.94ms\nstep:4470/10000 train_loss:3.9610 train_time:757953ms step_avg:169.94ms\nstep:4471/10000 train_loss:3.9728 train_time:758124ms step_avg:169.94ms\nstep:4472/10000 train_loss:4.0489 train_time:758293ms step_avg:169.94ms\nstep:4473/10000 train_loss:4.1481 train_time:758461ms step_avg:169.94ms\nstep:4474/10000 train_loss:4.2135 train_time:758629ms step_avg:169.94ms\nstep:4475/10000 train_loss:3.9252 train_time:758799ms step_avg:169.94ms\nstep:4476/10000 train_loss:4.1995 train_time:758967ms step_avg:169.94ms\nstep:4477/10000 train_loss:4.0154 train_time:759140ms step_avg:169.94ms\nstep:4478/10000 train_loss:4.0472 train_time:759309ms step_avg:169.94ms\nstep:4479/10000 train_loss:4.1691 train_time:759480ms step_avg:169.94ms\nstep:4480/10000 train_loss:4.2564 train_time:759649ms step_avg:169.94ms\nstep:4481/10000 train_loss:4.0689 train_time:759819ms step_avg:169.94ms\nstep:4482/10000 train_loss:4.0476 train_time:759988ms step_avg:169.94ms\nstep:4483/10000 train_loss:4.0528 train_time:760161ms step_avg:169.94ms\nstep:4484/10000 train_loss:4.0779 train_time:760331ms step_avg:169.94ms\nstep:4485/10000 train_loss:4.1459 train_time:760502ms step_avg:169.94ms\nstep:4486/10000 train_loss:4.1194 train_time:760673ms step_avg:169.94ms\nstep:4487/10000 train_loss:4.0101 train_time:760841ms step_avg:169.94ms\nstep:4488/10000 train_loss:4.1077 train_time:761008ms step_avg:169.94ms\nstep:4489/10000 train_loss:3.9773 train_time:761178ms step_avg:169.94ms\nstep:4490/10000 train_loss:4.3034 train_time:761347ms step_avg:169.94ms\nstep:4491/10000 train_loss:4.1386 train_time:761516ms step_avg:169.94ms\nstep:4492/10000 train_loss:4.0104 train_time:761684ms step_avg:169.94ms\nstep:4493/10000 train_loss:4.1198 train_time:761855ms step_avg:169.94ms\nstep:4494/10000 train_loss:4.0085 train_time:762024ms step_avg:169.94ms\nstep:4495/10000 train_loss:3.9982 train_time:762194ms step_avg:169.94ms\nstep:4496/10000 train_loss:4.0542 train_time:762363ms step_avg:169.94ms\nstep:4497/10000 train_loss:4.0720 train_time:762535ms step_avg:169.94ms\nstep:4498/10000 train_loss:4.0057 train_time:762705ms step_avg:169.94ms\nstep:4499/10000 train_loss:4.0398 train_time:762876ms step_avg:169.94ms\nstep:4500/10000 train_loss:4.2035 train_time:763044ms step_avg:169.94ms\nstep:4501/10000 train_loss:3.9739 train_time:763214ms step_avg:169.94ms\nstep:4502/10000 train_loss:4.0176 train_time:763384ms step_avg:169.94ms\nstep:4503/10000 train_loss:4.0958 train_time:763554ms step_avg:169.94ms\nstep:4504/10000 train_loss:3.9865 train_time:763723ms step_avg:169.94ms\nstep:4505/10000 train_loss:4.1242 train_time:763893ms step_avg:169.94ms\nstep:4506/10000 train_loss:4.0688 train_time:764060ms step_avg:169.94ms\nstep:4507/10000 train_loss:4.0063 train_time:764230ms step_avg:169.94ms\nstep:4508/10000 train_loss:3.8836 train_time:764403ms step_avg:169.94ms\nstep:4509/10000 train_loss:4.1005 train_time:764569ms step_avg:169.94ms\nstep:4510/10000 train_loss:4.1604 train_time:764747ms step_avg:169.94ms\nstep:4511/10000 train_loss:4.2414 train_time:764913ms step_avg:169.94ms\nstep:4512/10000 train_loss:4.0951 train_time:765082ms step_avg:169.94ms\nstep:4513/10000 train_loss:3.9630 train_time:765250ms step_avg:169.94ms\nstep:4514/10000 train_loss:3.9339 train_time:765419ms step_avg:169.94ms\nstep:4515/10000 train_loss:4.0210 train_time:765586ms step_avg:169.94ms\nstep:4516/10000 train_loss:4.3044 train_time:765757ms step_avg:169.94ms\nstep:4517/10000 train_loss:4.0300 train_time:765927ms step_avg:169.94ms\nstep:4518/10000 train_loss:3.9910 train_time:766097ms step_avg:169.94ms\nstep:4519/10000 train_loss:4.2007 train_time:766266ms step_avg:169.94ms\nstep:4520/10000 train_loss:3.8441 train_time:766436ms step_avg:169.94ms\nstep:4521/10000 train_loss:3.9231 train_time:766604ms step_avg:169.94ms\nstep:4522/10000 train_loss:4.0923 train_time:766774ms step_avg:169.94ms\nstep:4523/10000 train_loss:4.0162 train_time:766942ms step_avg:169.94ms\nstep:4524/10000 train_loss:4.1581 train_time:767114ms step_avg:169.94ms\nstep:4525/10000 train_loss:4.0868 train_time:767279ms step_avg:169.94ms\nstep:4526/10000 train_loss:3.9530 train_time:767450ms step_avg:169.94ms\nstep:4527/10000 train_loss:4.0620 train_time:767619ms step_avg:169.94ms\nstep:4528/10000 train_loss:4.0098 train_time:767790ms step_avg:169.94ms\nstep:4529/10000 train_loss:4.0198 train_time:767966ms step_avg:169.94ms\nstep:4530/10000 train_loss:4.1085 train_time:768130ms step_avg:169.94ms\nstep:4531/10000 train_loss:3.8949 train_time:768300ms step_avg:169.94ms\nstep:4532/10000 train_loss:4.0150 train_time:768468ms step_avg:169.94ms\nstep:4533/10000 train_loss:4.1842 train_time:768638ms step_avg:169.94ms\nstep:4534/10000 train_loss:4.0294 train_time:768806ms step_avg:169.94ms\nstep:4535/10000 train_loss:3.9134 train_time:768977ms step_avg:169.94ms\nstep:4536/10000 train_loss:3.9944 train_time:769146ms step_avg:169.94ms\nstep:4537/10000 train_loss:4.1145 train_time:769317ms step_avg:169.94ms\nstep:4538/10000 train_loss:4.0434 train_time:769486ms step_avg:169.94ms\nstep:4539/10000 train_loss:4.0304 train_time:769655ms step_avg:169.94ms\nstep:4540/10000 train_loss:4.1674 train_time:769825ms step_avg:169.94ms\nstep:4541/10000 train_loss:4.0134 train_time:769994ms step_avg:169.94ms\nstep:4542/10000 train_loss:3.9906 train_time:770164ms step_avg:169.94ms\nstep:4543/10000 train_loss:3.9290 train_time:770335ms step_avg:169.94ms\nstep:4544/10000 train_loss:4.0870 train_time:770504ms step_avg:169.94ms\nstep:4545/10000 train_loss:4.1374 train_time:770674ms step_avg:169.94ms\nstep:4546/10000 train_loss:4.2557 train_time:770843ms step_avg:169.94ms\nstep:4547/10000 train_loss:3.9850 train_time:771012ms step_avg:169.94ms\nstep:4548/10000 train_loss:4.1072 train_time:771180ms step_avg:169.94ms\nstep:4549/10000 train_loss:4.1404 train_time:771351ms step_avg:169.94ms\nstep:4550/10000 train_loss:4.1371 train_time:771522ms step_avg:169.94ms\nstep:4551/10000 train_loss:4.0702 train_time:771697ms step_avg:169.94ms\nstep:4552/10000 train_loss:4.1320 train_time:771863ms step_avg:169.94ms\nstep:4553/10000 train_loss:3.9591 train_time:772031ms step_avg:169.94ms\nstep:4554/10000 train_loss:3.9641 train_time:772200ms step_avg:169.94ms\nstep:4555/10000 train_loss:3.9370 train_time:772368ms step_avg:169.94ms\nstep:4556/10000 train_loss:4.0725 train_time:772540ms step_avg:169.94ms\nstep:4557/10000 train_loss:4.0922 train_time:772709ms step_avg:169.94ms\nstep:4558/10000 train_loss:4.0939 train_time:772878ms step_avg:169.94ms\nstep:4559/10000 train_loss:4.0395 train_time:773046ms step_avg:169.94ms\nstep:4560/10000 train_loss:3.9767 train_time:773217ms step_avg:169.94ms\nstep:4561/10000 train_loss:3.9229 train_time:773385ms step_avg:169.94ms\nstep:4562/10000 train_loss:4.0070 train_time:773554ms step_avg:169.94ms\nstep:4563/10000 train_loss:4.0111 train_time:773722ms step_avg:169.94ms\nstep:4564/10000 train_loss:4.2551 train_time:773892ms step_avg:169.94ms\nstep:4565/10000 train_loss:3.9593 train_time:774062ms step_avg:169.94ms\nstep:4566/10000 train_loss:4.0400 train_time:774230ms step_avg:169.94ms\nstep:4567/10000 train_loss:4.0976 train_time:774399ms step_avg:169.94ms\nstep:4568/10000 train_loss:4.0712 train_time:774567ms step_avg:169.94ms\nstep:4569/10000 train_loss:3.9205 train_time:774739ms step_avg:169.94ms\nstep:4570/10000 train_loss:4.0804 train_time:774910ms step_avg:169.94ms\nstep:4571/10000 train_loss:3.9533 train_time:775080ms step_avg:169.94ms\nstep:4572/10000 train_loss:4.1754 train_time:775474ms step_avg:169.99ms\nstep:4573/10000 train_loss:4.0032 train_time:775637ms step_avg:169.98ms\nstep:4574/10000 train_loss:4.1260 train_time:775804ms step_avg:169.98ms\nstep:4575/10000 train_loss:3.9940 train_time:775974ms step_avg:169.98ms\nstep:4576/10000 train_loss:4.1870 train_time:776142ms step_avg:169.98ms\nstep:4577/10000 train_loss:4.1543 train_time:776309ms step_avg:169.98ms\nstep:4578/10000 train_loss:4.0777 train_time:776482ms step_avg:169.98ms\nstep:4579/10000 train_loss:4.0291 train_time:776657ms step_avg:169.98ms\nstep:4580/10000 train_loss:3.9043 train_time:776826ms step_avg:169.98ms\nstep:4581/10000 train_loss:4.1443 train_time:776995ms step_avg:169.98ms\nstep:4582/10000 train_loss:3.9944 train_time:777165ms step_avg:169.98ms\nstep:4583/10000 train_loss:4.0354 train_time:777335ms step_avg:169.98ms\nstep:4584/10000 train_loss:4.1456 train_time:777505ms step_avg:169.98ms\nstep:4585/10000 train_loss:3.9869 train_time:777675ms step_avg:169.98ms\nstep:4586/10000 train_loss:4.0857 train_time:777844ms step_avg:169.98ms\nstep:4587/10000 train_loss:3.9409 train_time:778014ms step_avg:169.98ms\nstep:4588/10000 train_loss:3.9563 train_time:778183ms step_avg:169.98ms\nstep:4589/10000 train_loss:4.0883 train_time:778353ms step_avg:169.98ms\nstep:4590/10000 train_loss:4.2122 train_time:778522ms step_avg:169.98ms\nstep:4591/10000 train_loss:3.9951 train_time:778693ms step_avg:169.98ms\nstep:4592/10000 train_loss:3.9986 train_time:778862ms step_avg:169.98ms\nstep:4593/10000 train_loss:4.0657 train_time:779032ms step_avg:169.98ms\nstep:4594/10000 train_loss:4.1342 train_time:779200ms step_avg:169.98ms\nstep:4595/10000 train_loss:4.3572 train_time:779369ms step_avg:169.98ms\nstep:4596/10000 train_loss:4.0858 train_time:779540ms step_avg:169.98ms\nstep:4597/10000 train_loss:4.0248 train_time:779709ms step_avg:169.98ms\nstep:4598/10000 train_loss:4.0634 train_time:779878ms step_avg:169.98ms\nstep:4599/10000 train_loss:4.1753 train_time:780046ms step_avg:169.98ms\nstep:4600/10000 train_loss:4.1279 train_time:780216ms step_avg:169.98ms\nstep:4601/10000 train_loss:3.9995 train_time:780384ms step_avg:169.98ms\nstep:4602/10000 train_loss:4.1054 train_time:780552ms step_avg:169.98ms\nstep:4603/10000 train_loss:4.1630 train_time:780723ms step_avg:169.98ms\nstep:4604/10000 train_loss:4.0919 train_time:780894ms step_avg:169.98ms\nstep:4605/10000 train_loss:4.0433 train_time:781061ms step_avg:169.98ms\nstep:4606/10000 train_loss:3.9682 train_time:781231ms step_avg:169.98ms\nstep:4607/10000 train_loss:3.9920 train_time:781399ms step_avg:169.98ms\nstep:4608/10000 train_loss:4.0098 train_time:781567ms step_avg:169.98ms\nstep:4609/10000 train_loss:3.9484 train_time:781738ms step_avg:169.98ms\nstep:4610/10000 train_loss:3.9625 train_time:781906ms step_avg:169.98ms\nstep:4611/10000 train_loss:4.1145 train_time:782137ms step_avg:169.99ms\nstep:4612/10000 train_loss:3.9460 train_time:782304ms step_avg:169.99ms\nstep:4613/10000 train_loss:4.0651 train_time:782472ms step_avg:169.99ms\nstep:4614/10000 train_loss:3.9562 train_time:782641ms step_avg:169.99ms\nstep:4615/10000 train_loss:4.0097 train_time:782809ms step_avg:169.99ms\nstep:4616/10000 train_loss:4.2238 train_time:782980ms step_avg:169.99ms\nstep:4617/10000 train_loss:3.8480 train_time:783156ms step_avg:169.99ms\nstep:4618/10000 train_loss:4.1559 train_time:783326ms step_avg:169.99ms\nstep:4619/10000 train_loss:4.1703 train_time:783497ms step_avg:169.99ms\nstep:4620/10000 train_loss:4.2109 train_time:783662ms step_avg:169.99ms\nstep:4621/10000 train_loss:4.2794 train_time:783829ms step_avg:169.99ms\nstep:4622/10000 train_loss:4.0852 train_time:783996ms step_avg:169.99ms\nstep:4623/10000 train_loss:3.9180 train_time:784166ms step_avg:169.99ms\nstep:4624/10000 train_loss:3.9064 train_time:784336ms step_avg:169.99ms\nstep:4625/10000 train_loss:4.1175 train_time:784507ms step_avg:169.99ms\nstep:4626/10000 train_loss:4.1151 train_time:784679ms step_avg:169.99ms\nstep:4627/10000 train_loss:3.9365 train_time:784848ms step_avg:169.99ms\nstep:4628/10000 train_loss:3.9167 train_time:785017ms step_avg:169.99ms\nstep:4629/10000 train_loss:4.1246 train_time:785186ms step_avg:169.99ms\nstep:4630/10000 train_loss:4.0168 train_time:785357ms step_avg:169.99ms\nstep:4631/10000 train_loss:4.0352 train_time:785525ms step_avg:169.99ms\nstep:4632/10000 train_loss:4.0549 train_time:785696ms step_avg:169.99ms\nstep:4633/10000 train_loss:4.0246 train_time:785865ms step_avg:169.99ms\nstep:4634/10000 train_loss:4.1345 train_time:786037ms step_avg:169.99ms\nstep:4635/10000 train_loss:4.1099 train_time:786204ms step_avg:169.99ms\nstep:4636/10000 train_loss:4.1135 train_time:786376ms step_avg:169.99ms\nstep:4637/10000 train_loss:4.1341 train_time:786546ms step_avg:169.99ms\nstep:4638/10000 train_loss:4.1122 train_time:786716ms step_avg:169.99ms\nstep:4639/10000 train_loss:3.9482 train_time:786884ms step_avg:169.99ms\nstep:4640/10000 train_loss:3.8429 train_time:787053ms step_avg:169.99ms\nstep:4641/10000 train_loss:4.1271 train_time:787221ms step_avg:169.99ms\nstep:4642/10000 train_loss:3.9752 train_time:787390ms step_avg:169.99ms\nstep:4643/10000 train_loss:4.3848 train_time:787562ms step_avg:169.99ms\nstep:4644/10000 train_loss:3.9136 train_time:787730ms step_avg:169.99ms\nstep:4645/10000 train_loss:4.0924 train_time:787900ms step_avg:169.99ms\nstep:4646/10000 train_loss:3.9268 train_time:788069ms step_avg:169.99ms\nstep:4647/10000 train_loss:4.1153 train_time:788238ms step_avg:169.99ms\nstep:4648/10000 train_loss:3.9219 train_time:788408ms step_avg:169.99ms\nstep:4649/10000 train_loss:3.9508 train_time:788577ms step_avg:169.99ms\nstep:4650/10000 train_loss:3.9256 train_time:788745ms step_avg:169.99ms\nstep:4651/10000 train_loss:3.9699 train_time:788913ms step_avg:169.99ms\nstep:4652/10000 train_loss:4.0942 train_time:789082ms step_avg:169.99ms\nstep:4653/10000 train_loss:4.0645 train_time:789250ms step_avg:169.99ms\nstep:4654/10000 train_loss:3.7578 train_time:789419ms step_avg:169.99ms\nstep:4655/10000 train_loss:3.9491 train_time:789588ms step_avg:169.99ms\nstep:4656/10000 train_loss:3.8889 train_time:789760ms step_avg:169.99ms\nstep:4657/10000 train_loss:3.9158 train_time:789929ms step_avg:169.99ms\nstep:4658/10000 train_loss:4.0729 train_time:790095ms step_avg:169.99ms\nstep:4659/10000 train_loss:4.0475 train_time:790262ms step_avg:169.99ms\nstep:4660/10000 train_loss:4.0326 train_time:790432ms step_avg:169.99ms\nstep:4661/10000 train_loss:4.1027 train_time:790602ms step_avg:169.99ms\nstep:4662/10000 train_loss:4.0317 train_time:790774ms step_avg:169.99ms\nstep:4663/10000 train_loss:3.8836 train_time:790944ms step_avg:169.99ms\nstep:4664/10000 train_loss:4.0947 train_time:791114ms step_avg:169.99ms\nstep:4665/10000 train_loss:4.0621 train_time:791282ms step_avg:169.99ms\nstep:4666/10000 train_loss:3.8962 train_time:791452ms step_avg:169.99ms\nstep:4667/10000 train_loss:4.1909 train_time:791619ms step_avg:169.98ms\nstep:4668/10000 train_loss:4.0430 train_time:791795ms step_avg:169.99ms\nstep:4669/10000 train_loss:3.9695 train_time:791960ms step_avg:169.99ms\nstep:4670/10000 train_loss:3.9203 train_time:792129ms step_avg:169.98ms\nstep:4671/10000 train_loss:4.0865 train_time:792299ms step_avg:169.98ms\nstep:4672/10000 train_loss:3.9929 train_time:792468ms step_avg:169.98ms\nstep:4673/10000 train_loss:4.0526 train_time:792639ms step_avg:169.98ms\nstep:4674/10000 train_loss:3.9836 train_time:792809ms step_avg:169.98ms\nstep:4675/10000 train_loss:4.0399 train_time:792979ms step_avg:169.98ms\nstep:4676/10000 train_loss:4.0023 train_time:793148ms step_avg:169.98ms\nstep:4677/10000 train_loss:4.1087 train_time:793316ms step_avg:169.98ms\nstep:4678/10000 train_loss:3.9621 train_time:793484ms step_avg:169.98ms\nstep:4679/10000 train_loss:3.9949 train_time:793654ms step_avg:169.98ms\nstep:4680/10000 train_loss:4.1870 train_time:793823ms step_avg:169.98ms\nstep:4681/10000 train_loss:4.0246 train_time:793992ms step_avg:169.98ms\nstep:4682/10000 train_loss:4.2938 train_time:794159ms step_avg:169.98ms\nstep:4683/10000 train_loss:4.0306 train_time:794327ms step_avg:169.98ms\nstep:4684/10000 train_loss:3.9970 train_time:794495ms step_avg:169.98ms\nstep:4685/10000 train_loss:4.0673 train_time:794664ms step_avg:169.98ms\nstep:4686/10000 train_loss:4.0040 train_time:794833ms step_avg:169.98ms\nstep:4687/10000 train_loss:3.9766 train_time:795003ms step_avg:169.98ms\nstep:4688/10000 train_loss:4.0796 train_time:795173ms step_avg:169.98ms\nstep:4689/10000 train_loss:3.9556 train_time:795341ms step_avg:169.98ms\nstep:4690/10000 train_loss:4.0276 train_time:795511ms step_avg:169.98ms\nstep:4691/10000 train_loss:4.0202 train_time:795679ms step_avg:169.98ms\nstep:4692/10000 train_loss:4.0598 train_time:795849ms step_avg:169.98ms\nstep:4693/10000 train_loss:4.0501 train_time:796020ms step_avg:169.98ms\nstep:4694/10000 train_loss:3.9650 train_time:796188ms step_avg:169.98ms\nstep:4695/10000 train_loss:3.8930 train_time:796357ms step_avg:169.98ms\nstep:4696/10000 train_loss:3.9356 train_time:796525ms step_avg:169.98ms\nstep:4697/10000 train_loss:3.8632 train_time:796695ms step_avg:169.98ms\nstep:4698/10000 train_loss:3.9976 train_time:796863ms step_avg:169.98ms\nstep:4699/10000 train_loss:4.2065 train_time:797033ms step_avg:169.98ms\nstep:4700/10000 train_loss:4.0370 train_time:797203ms step_avg:169.98ms\nstep:4701/10000 train_loss:4.0534 train_time:797375ms step_avg:169.98ms\nstep:4702/10000 train_loss:4.1615 train_time:797542ms step_avg:169.98ms\nstep:4703/10000 train_loss:4.0370 train_time:797711ms step_avg:169.98ms\nstep:4704/10000 train_loss:4.3699 train_time:797881ms step_avg:169.98ms\nstep:4705/10000 train_loss:4.1404 train_time:798051ms step_avg:169.98ms\nstep:4706/10000 train_loss:4.0012 train_time:798220ms step_avg:169.98ms\nstep:4707/10000 train_loss:3.9619 train_time:798392ms step_avg:169.98ms\nstep:4708/10000 train_loss:3.9844 train_time:798560ms step_avg:169.98ms\nstep:4709/10000 train_loss:4.0389 train_time:798729ms step_avg:169.98ms\nstep:4710/10000 train_loss:4.0335 train_time:798898ms step_avg:169.98ms\nstep:4711/10000 train_loss:3.9952 train_time:799068ms step_avg:169.98ms\nstep:4712/10000 train_loss:3.9319 train_time:799242ms step_avg:169.98ms\nstep:4713/10000 train_loss:3.9855 train_time:799412ms step_avg:169.98ms\nstep:4714/10000 train_loss:4.0757 train_time:799579ms step_avg:169.98ms\nstep:4715/10000 train_loss:4.1278 train_time:799747ms step_avg:169.98ms\nstep:4716/10000 train_loss:3.9453 train_time:799916ms step_avg:169.98ms\nstep:4717/10000 train_loss:4.0744 train_time:800083ms step_avg:169.98ms\nstep:4718/10000 train_loss:4.0507 train_time:800254ms step_avg:169.98ms\nstep:4719/10000 train_loss:4.0875 train_time:800424ms step_avg:169.98ms\nstep:4720/10000 train_loss:4.0462 train_time:800594ms step_avg:169.98ms\nstep:4721/10000 train_loss:4.0016 train_time:800765ms step_avg:169.98ms\nstep:4722/10000 train_loss:4.0976 train_time:800934ms step_avg:169.98ms\nstep:4723/10000 train_loss:3.9741 train_time:801103ms step_avg:169.98ms\nstep:4724/10000 train_loss:4.0909 train_time:801273ms step_avg:169.98ms\nstep:4725/10000 train_loss:3.9466 train_time:801443ms step_avg:169.98ms\nstep:4726/10000 train_loss:4.0275 train_time:801612ms step_avg:169.98ms\nstep:4727/10000 train_loss:4.1418 train_time:801781ms step_avg:169.98ms\nstep:4728/10000 train_loss:4.0160 train_time:801952ms step_avg:169.98ms\nstep:4729/10000 train_loss:3.9780 train_time:802122ms step_avg:169.98ms\nstep:4730/10000 train_loss:4.0477 train_time:802293ms step_avg:169.98ms\nstep:4731/10000 train_loss:4.0382 train_time:802461ms step_avg:169.98ms\nstep:4732/10000 train_loss:3.8825 train_time:802631ms step_avg:169.98ms\nstep:4733/10000 train_loss:3.9841 train_time:802801ms step_avg:169.98ms\nstep:4734/10000 train_loss:3.9615 train_time:802971ms step_avg:169.98ms\nstep:4735/10000 train_loss:4.0491 train_time:803139ms step_avg:169.98ms\nstep:4736/10000 train_loss:4.0076 train_time:803310ms step_avg:169.98ms\nstep:4737/10000 train_loss:3.9343 train_time:803482ms step_avg:169.98ms\nstep:4738/10000 train_loss:4.0283 train_time:803651ms step_avg:169.98ms\nstep:4739/10000 train_loss:4.1168 train_time:803821ms step_avg:169.98ms\nstep:4740/10000 train_loss:4.1474 train_time:803991ms step_avg:169.98ms\nstep:4741/10000 train_loss:4.0615 train_time:804161ms step_avg:169.98ms\nstep:4742/10000 train_loss:3.9579 train_time:804330ms step_avg:169.98ms\nstep:4743/10000 train_loss:3.9994 train_time:804499ms step_avg:169.98ms\nstep:4744/10000 train_loss:4.2783 train_time:804667ms step_avg:169.98ms\nstep:4745/10000 train_loss:3.9400 train_time:804834ms step_avg:169.98ms\nstep:4746/10000 train_loss:4.0970 train_time:805003ms step_avg:169.98ms\nstep:4747/10000 train_loss:3.9556 train_time:805175ms step_avg:169.98ms\nstep:4748/10000 train_loss:4.1194 train_time:805344ms step_avg:169.98ms\nstep:4749/10000 train_loss:3.8738 train_time:805516ms step_avg:169.98ms\nstep:4750/10000 train_loss:4.1612 train_time:805684ms step_avg:169.98ms\nstep:4751/10000 train_loss:4.1371 train_time:805853ms step_avg:169.98ms\nstep:4752/10000 train_loss:3.9925 train_time:806020ms step_avg:169.97ms\nstep:4753/10000 train_loss:3.9473 train_time:806188ms step_avg:169.97ms\nstep:4754/10000 train_loss:3.9387 train_time:806359ms step_avg:169.97ms\nstep:4755/10000 train_loss:4.0373 train_time:806528ms step_avg:169.97ms\nstep:4756/10000 train_loss:3.8709 train_time:806699ms step_avg:169.97ms\nstep:4757/10000 train_loss:4.0079 train_time:806867ms step_avg:169.97ms\nstep:4758/10000 train_loss:4.0108 train_time:807038ms step_avg:169.97ms\nstep:4759/10000 train_loss:4.0661 train_time:807209ms step_avg:169.97ms\nstep:4760/10000 train_loss:3.9653 train_time:807379ms step_avg:169.97ms\nstep:4761/10000 train_loss:3.9804 train_time:807550ms step_avg:169.97ms\nstep:4762/10000 train_loss:4.0484 train_time:807722ms step_avg:169.98ms\nstep:4763/10000 train_loss:4.2470 train_time:807889ms step_avg:169.97ms\nstep:4764/10000 train_loss:4.0126 train_time:808058ms step_avg:169.97ms\nstep:4765/10000 train_loss:3.8887 train_time:808226ms step_avg:169.97ms\nstep:4766/10000 train_loss:3.8865 train_time:808396ms step_avg:169.97ms\nstep:4767/10000 train_loss:4.2041 train_time:808569ms step_avg:169.97ms\nstep:4768/10000 train_loss:4.0588 train_time:808737ms step_avg:169.97ms\nstep:4769/10000 train_loss:4.0543 train_time:808904ms step_avg:169.97ms\nstep:4770/10000 train_loss:4.0669 train_time:809074ms step_avg:169.97ms\nstep:4771/10000 train_loss:3.8943 train_time:809243ms step_avg:169.97ms\nstep:4772/10000 train_loss:4.0321 train_time:809415ms step_avg:169.97ms\nstep:4773/10000 train_loss:4.1737 train_time:809585ms step_avg:169.97ms\nstep:4774/10000 train_loss:4.1889 train_time:809756ms step_avg:169.97ms\nstep:4775/10000 train_loss:4.1914 train_time:809924ms step_avg:169.97ms\nstep:4776/10000 train_loss:4.1213 train_time:810094ms step_avg:169.97ms\nstep:4777/10000 train_loss:4.0110 train_time:810262ms step_avg:169.97ms\nstep:4778/10000 train_loss:4.0129 train_time:810431ms step_avg:169.97ms\nstep:4779/10000 train_loss:3.9803 train_time:810603ms step_avg:169.97ms\nstep:4780/10000 train_loss:3.9353 train_time:810775ms step_avg:169.97ms\nstep:4781/10000 train_loss:4.5196 train_time:810942ms step_avg:169.97ms\nstep:4782/10000 train_loss:3.9052 train_time:811112ms step_avg:169.97ms\nstep:4783/10000 train_loss:3.9650 train_time:811282ms step_avg:169.97ms\nstep:4784/10000 train_loss:3.9828 train_time:811451ms step_avg:169.97ms\nstep:4785/10000 train_loss:3.9876 train_time:811622ms step_avg:169.97ms\nstep:4786/10000 train_loss:4.2043 train_time:811790ms step_avg:169.97ms\nstep:4787/10000 train_loss:3.9473 train_time:811961ms step_avg:169.97ms\nstep:4788/10000 train_loss:3.9516 train_time:812130ms step_avg:169.97ms\nstep:4789/10000 train_loss:3.8558 train_time:812303ms step_avg:169.97ms\nstep:4790/10000 train_loss:3.9828 train_time:812473ms step_avg:169.97ms\nstep:4791/10000 train_loss:4.0059 train_time:812643ms step_avg:169.97ms\nstep:4792/10000 train_loss:4.0211 train_time:812814ms step_avg:169.97ms\nstep:4793/10000 train_loss:3.8244 train_time:812982ms step_avg:169.97ms\nstep:4794/10000 train_loss:3.9774 train_time:813152ms step_avg:169.97ms\nstep:4795/10000 train_loss:4.1145 train_time:813322ms step_avg:169.97ms\nstep:4796/10000 train_loss:4.0200 train_time:813491ms step_avg:169.97ms\nstep:4797/10000 train_loss:4.0264 train_time:813663ms step_avg:169.97ms\nstep:4798/10000 train_loss:3.8879 train_time:813831ms step_avg:169.97ms\nstep:4799/10000 train_loss:3.9076 train_time:814000ms step_avg:169.97ms\nstep:4800/10000 train_loss:3.9562 train_time:814169ms step_avg:169.97ms\nstep:4801/10000 train_loss:3.8642 train_time:814339ms step_avg:169.97ms\nstep:4802/10000 train_loss:3.8672 train_time:814508ms step_avg:169.97ms\nstep:4803/10000 train_loss:3.9314 train_time:814681ms step_avg:169.97ms\nstep:4804/10000 train_loss:4.3651 train_time:814851ms step_avg:169.97ms\nstep:4805/10000 train_loss:4.0089 train_time:815020ms step_avg:169.97ms\nstep:4806/10000 train_loss:3.9465 train_time:815189ms step_avg:169.97ms\nstep:4807/10000 train_loss:4.0632 train_time:815359ms step_avg:169.97ms\nstep:4808/10000 train_loss:3.9241 train_time:815529ms step_avg:169.97ms\nstep:4809/10000 train_loss:3.9553 train_time:815701ms step_avg:169.97ms\nstep:4810/10000 train_loss:4.0084 train_time:815871ms step_avg:169.97ms\nstep:4811/10000 train_loss:3.9272 train_time:816047ms step_avg:169.97ms\nstep:4812/10000 train_loss:4.0032 train_time:816211ms step_avg:169.97ms\nstep:4813/10000 train_loss:3.9088 train_time:816381ms step_avg:169.97ms\nstep:4814/10000 train_loss:4.1241 train_time:816550ms step_avg:169.97ms\nstep:4815/10000 train_loss:4.0335 train_time:816721ms step_avg:169.97ms\nstep:4816/10000 train_loss:3.9875 train_time:816892ms step_avg:169.97ms\nstep:4817/10000 train_loss:3.8268 train_time:817059ms step_avg:169.97ms\nstep:4818/10000 train_loss:4.0672 train_time:817228ms step_avg:169.97ms\nstep:4819/10000 train_loss:3.9199 train_time:817398ms step_avg:169.97ms\nstep:4820/10000 train_loss:4.1069 train_time:817568ms step_avg:169.97ms\nstep:4821/10000 train_loss:3.9634 train_time:817739ms step_avg:169.97ms\nstep:4822/10000 train_loss:4.0425 train_time:817909ms step_avg:169.97ms\nstep:4823/10000 train_loss:3.9285 train_time:818079ms step_avg:169.97ms\nstep:4824/10000 train_loss:4.0873 train_time:818248ms step_avg:169.97ms\nstep:4825/10000 train_loss:4.0995 train_time:818419ms step_avg:169.97ms\nstep:4826/10000 train_loss:3.9876 train_time:818587ms step_avg:169.97ms\nstep:4827/10000 train_loss:3.9035 train_time:818758ms step_avg:169.97ms\nstep:4828/10000 train_loss:4.0419 train_time:818925ms step_avg:169.97ms\nstep:4829/10000 train_loss:4.1921 train_time:819095ms step_avg:169.97ms\nstep:4830/10000 train_loss:4.0148 train_time:819265ms step_avg:169.97ms\nstep:4831/10000 train_loss:4.0962 train_time:819436ms step_avg:169.97ms\nstep:4832/10000 train_loss:3.9639 train_time:819604ms step_avg:169.97ms\nstep:4833/10000 train_loss:4.1482 train_time:819775ms step_avg:169.97ms\nstep:4834/10000 train_loss:4.0637 train_time:819943ms step_avg:169.97ms\nstep:4835/10000 train_loss:3.9284 train_time:820112ms step_avg:169.97ms\nstep:4836/10000 train_loss:4.0023 train_time:820282ms step_avg:169.97ms\nstep:4837/10000 train_loss:3.8517 train_time:820452ms step_avg:169.97ms\nstep:4838/10000 train_loss:4.1242 train_time:820621ms step_avg:169.97ms\nstep:4839/10000 train_loss:4.1845 train_time:820790ms step_avg:169.97ms\nstep:4840/10000 train_loss:3.9439 train_time:820961ms step_avg:169.97ms\nstep:4841/10000 train_loss:3.9548 train_time:821129ms step_avg:169.97ms\nstep:4842/10000 train_loss:3.9985 train_time:821301ms step_avg:169.97ms\nstep:4843/10000 train_loss:3.9535 train_time:821471ms step_avg:169.97ms\nstep:4844/10000 train_loss:4.0265 train_time:821642ms step_avg:169.97ms\nstep:4845/10000 train_loss:3.9453 train_time:821812ms step_avg:169.97ms\nstep:4846/10000 train_loss:3.8547 train_time:821979ms step_avg:169.97ms\nstep:4847/10000 train_loss:3.9311 train_time:822148ms step_avg:169.97ms\nstep:4848/10000 train_loss:4.2703 train_time:822318ms step_avg:169.97ms\nstep:4849/10000 train_loss:3.9221 train_time:822488ms step_avg:169.97ms\nstep:4850/10000 train_loss:3.9064 train_time:822657ms step_avg:169.97ms\nstep:4851/10000 train_loss:4.0467 train_time:822826ms step_avg:169.97ms\nstep:4852/10000 train_loss:3.9284 train_time:822995ms step_avg:169.97ms\nstep:4853/10000 train_loss:4.0240 train_time:823162ms step_avg:169.97ms\nstep:4854/10000 train_loss:4.0111 train_time:823331ms step_avg:169.97ms\nstep:4855/10000 train_loss:3.9959 train_time:823498ms step_avg:169.97ms\nstep:4856/10000 train_loss:4.1932 train_time:823667ms step_avg:169.97ms\nstep:4857/10000 train_loss:3.9108 train_time:823838ms step_avg:169.97ms\nstep:4858/10000 train_loss:4.0308 train_time:824009ms step_avg:169.97ms\nstep:4859/10000 train_loss:3.8993 train_time:824179ms step_avg:169.97ms\nstep:4860/10000 train_loss:4.0463 train_time:824347ms step_avg:169.97ms\nstep:4861/10000 train_loss:3.9695 train_time:824517ms step_avg:169.97ms\nstep:4862/10000 train_loss:4.0595 train_time:824685ms step_avg:169.97ms\nstep:4863/10000 train_loss:4.0851 train_time:824855ms step_avg:169.97ms\nstep:4864/10000 train_loss:4.0576 train_time:825026ms step_avg:169.97ms\nstep:4865/10000 train_loss:4.0039 train_time:825196ms step_avg:169.97ms\nstep:4866/10000 train_loss:4.0174 train_time:825364ms step_avg:169.97ms\nstep:4867/10000 train_loss:4.0478 train_time:825535ms step_avg:169.97ms\nstep:4868/10000 train_loss:4.0371 train_time:825706ms step_avg:169.97ms\nstep:4869/10000 train_loss:3.9677 train_time:825877ms step_avg:169.97ms\nstep:4870/10000 train_loss:4.0228 train_time:826047ms step_avg:169.97ms\nstep:4871/10000 train_loss:3.9484 train_time:826218ms step_avg:169.97ms\nstep:4872/10000 train_loss:4.0400 train_time:826387ms step_avg:169.97ms\nstep:4873/10000 train_loss:4.1797 train_time:826557ms step_avg:169.97ms\nstep:4874/10000 train_loss:3.9591 train_time:826726ms step_avg:169.97ms\nstep:4875/10000 train_loss:3.8360 train_time:826897ms step_avg:169.97ms\nstep:4876/10000 train_loss:4.0815 train_time:827068ms step_avg:169.97ms\nstep:4877/10000 train_loss:3.9840 train_time:827239ms step_avg:169.97ms\nstep:4878/10000 train_loss:3.9919 train_time:827409ms step_avg:169.97ms\nstep:4879/10000 train_loss:4.0537 train_time:827580ms step_avg:169.97ms\nstep:4880/10000 train_loss:3.9964 train_time:827749ms step_avg:169.97ms\nstep:4881/10000 train_loss:3.9025 train_time:827918ms step_avg:169.97ms\nstep:4882/10000 train_loss:4.0494 train_time:828087ms step_avg:169.97ms\nstep:4883/10000 train_loss:4.0268 train_time:828258ms step_avg:169.97ms\nstep:4884/10000 train_loss:4.0431 train_time:828427ms step_avg:169.97ms\nstep:4885/10000 train_loss:3.9768 train_time:828597ms step_avg:169.97ms\nstep:4886/10000 train_loss:3.8837 train_time:828764ms step_avg:169.97ms\nstep:4887/10000 train_loss:4.0486 train_time:828932ms step_avg:169.97ms\nstep:4888/10000 train_loss:4.0342 train_time:829101ms step_avg:169.97ms\nstep:4889/10000 train_loss:4.0972 train_time:829272ms step_avg:169.97ms\nstep:4890/10000 train_loss:4.0942 train_time:829442ms step_avg:169.97ms\nstep:4891/10000 train_loss:3.9349 train_time:829615ms step_avg:169.97ms\nstep:4892/10000 train_loss:4.1089 train_time:829786ms step_avg:169.97ms\nstep:4893/10000 train_loss:4.0304 train_time:829956ms step_avg:169.97ms\nstep:4894/10000 train_loss:4.1308 train_time:830125ms step_avg:169.97ms\nstep:4895/10000 train_loss:3.9282 train_time:830294ms step_avg:169.97ms\nstep:4896/10000 train_loss:4.0574 train_time:830463ms step_avg:169.97ms\nstep:4897/10000 train_loss:4.0077 train_time:830633ms step_avg:169.97ms\nstep:4898/10000 train_loss:4.0765 train_time:830802ms step_avg:169.97ms\nstep:4899/10000 train_loss:4.1068 train_time:830971ms step_avg:169.97ms\nstep:4900/10000 train_loss:4.0057 train_time:831140ms step_avg:169.97ms\nstep:4901/10000 train_loss:4.1911 train_time:831311ms step_avg:169.97ms\nstep:4902/10000 train_loss:3.8947 train_time:831483ms step_avg:169.97ms\nstep:4903/10000 train_loss:4.0223 train_time:831652ms step_avg:169.97ms\nstep:4904/10000 train_loss:3.9475 train_time:831821ms step_avg:169.97ms\nstep:4905/10000 train_loss:4.2243 train_time:831991ms step_avg:169.97ms\nstep:4906/10000 train_loss:3.9218 train_time:832163ms step_avg:169.97ms\nstep:4907/10000 train_loss:4.1825 train_time:832333ms step_avg:169.97ms\nstep:4908/10000 train_loss:4.0611 train_time:832501ms step_avg:169.97ms\nstep:4909/10000 train_loss:4.0660 train_time:832669ms step_avg:169.97ms\nstep:4910/10000 train_loss:3.9999 train_time:832840ms step_avg:169.97ms\nstep:4911/10000 train_loss:4.0252 train_time:833010ms step_avg:169.97ms\nstep:4912/10000 train_loss:3.8583 train_time:833181ms step_avg:169.97ms\nstep:4913/10000 train_loss:3.9867 train_time:833351ms step_avg:169.97ms\nstep:4914/10000 train_loss:3.9652 train_time:833522ms step_avg:169.97ms\nstep:4915/10000 train_loss:4.0529 train_time:833691ms step_avg:169.97ms\nstep:4916/10000 train_loss:4.2121 train_time:833859ms step_avg:169.97ms\nstep:4917/10000 train_loss:4.2007 train_time:834027ms step_avg:169.97ms\nstep:4918/10000 train_loss:3.8901 train_time:834197ms step_avg:169.97ms\nstep:4919/10000 train_loss:3.9799 train_time:834366ms step_avg:169.97ms\nstep:4920/10000 train_loss:3.9518 train_time:834536ms step_avg:169.97ms\nstep:4921/10000 train_loss:3.9743 train_time:834705ms step_avg:169.97ms\nstep:4922/10000 train_loss:4.0732 train_time:834873ms step_avg:169.97ms\nstep:4923/10000 train_loss:4.0780 train_time:835042ms step_avg:169.97ms\nstep:4924/10000 train_loss:3.9556 train_time:835213ms step_avg:169.97ms\nstep:4925/10000 train_loss:3.8452 train_time:835384ms step_avg:169.97ms\nstep:4926/10000 train_loss:3.9858 train_time:835553ms step_avg:169.97ms\nstep:4927/10000 train_loss:4.0028 train_time:835722ms step_avg:169.97ms\nstep:4928/10000 train_loss:4.1282 train_time:835891ms step_avg:169.97ms\nstep:4929/10000 train_loss:3.8063 train_time:836061ms step_avg:169.97ms\nstep:4930/10000 train_loss:4.0560 train_time:836233ms step_avg:169.97ms\nstep:4931/10000 train_loss:3.8430 train_time:836403ms step_avg:169.97ms\nstep:4932/10000 train_loss:3.8607 train_time:836573ms step_avg:169.97ms\nstep:4933/10000 train_loss:4.0191 train_time:836739ms step_avg:169.97ms\nstep:4934/10000 train_loss:4.2088 train_time:836909ms step_avg:169.97ms\nstep:4935/10000 train_loss:4.0489 train_time:837078ms step_avg:169.97ms\nstep:4936/10000 train_loss:3.9710 train_time:837246ms step_avg:169.96ms\nstep:4937/10000 train_loss:3.9244 train_time:837415ms step_avg:169.96ms\nstep:4938/10000 train_loss:4.1134 train_time:837585ms step_avg:169.96ms\nstep:4939/10000 train_loss:4.1706 train_time:837755ms step_avg:169.96ms\nstep:4940/10000 train_loss:3.9800 train_time:837923ms step_avg:169.96ms\nstep:4941/10000 train_loss:3.9648 train_time:838093ms step_avg:169.96ms\nstep:4942/10000 train_loss:4.0335 train_time:838261ms step_avg:169.96ms\nstep:4943/10000 train_loss:3.7614 train_time:838429ms step_avg:169.96ms\nstep:4944/10000 train_loss:4.0428 train_time:838596ms step_avg:169.96ms\nstep:4945/10000 train_loss:3.9042 train_time:838765ms step_avg:169.96ms\nstep:4946/10000 train_loss:4.1897 train_time:838936ms step_avg:169.96ms\nstep:4947/10000 train_loss:3.9020 train_time:839104ms step_avg:169.96ms\nstep:4948/10000 train_loss:3.9498 train_time:839276ms step_avg:169.96ms\nstep:4949/10000 train_loss:3.8730 train_time:839444ms step_avg:169.96ms\nstep:4950/10000 train_loss:3.9816 train_time:839614ms step_avg:169.96ms\nstep:4951/10000 train_loss:4.0077 train_time:839782ms step_avg:169.96ms\nstep:4952/10000 train_loss:3.9645 train_time:839951ms step_avg:169.96ms\nstep:4953/10000 train_loss:4.0013 train_time:840123ms step_avg:169.96ms\nstep:4954/10000 train_loss:3.9352 train_time:840295ms step_avg:169.96ms\nstep:4955/10000 train_loss:4.0732 train_time:840466ms step_avg:169.96ms\nstep:4956/10000 train_loss:4.1959 train_time:840635ms step_avg:169.96ms\nstep:4957/10000 train_loss:4.0060 train_time:840805ms step_avg:169.96ms\nstep:4958/10000 train_loss:3.8705 train_time:840977ms step_avg:169.96ms\nstep:4959/10000 train_loss:4.1508 train_time:841144ms step_avg:169.96ms\nstep:4960/10000 train_loss:3.9401 train_time:841316ms step_avg:169.96ms\nstep:4961/10000 train_loss:3.8593 train_time:841485ms step_avg:169.96ms\nstep:4962/10000 train_loss:4.0021 train_time:841654ms step_avg:169.96ms\nstep:4963/10000 train_loss:4.1134 train_time:841824ms step_avg:169.96ms\nstep:4964/10000 train_loss:3.8376 train_time:841996ms step_avg:169.96ms\nstep:4965/10000 train_loss:3.9343 train_time:842163ms step_avg:169.96ms\nstep:4966/10000 train_loss:3.9482 train_time:842334ms step_avg:169.96ms\nstep:4967/10000 train_loss:4.0806 train_time:842503ms step_avg:169.96ms\nstep:4968/10000 train_loss:4.1747 train_time:842674ms step_avg:169.96ms\nstep:4969/10000 train_loss:4.0213 train_time:842843ms step_avg:169.96ms\nstep:4970/10000 train_loss:3.9991 train_time:843014ms step_avg:169.96ms\nstep:4971/10000 train_loss:4.0055 train_time:843184ms step_avg:169.96ms\nstep:4972/10000 train_loss:3.8022 train_time:843356ms step_avg:169.96ms\nstep:4973/10000 train_loss:3.9446 train_time:843524ms step_avg:169.96ms\nstep:4974/10000 train_loss:4.0421 train_time:843693ms step_avg:169.96ms\nstep:4975/10000 train_loss:4.0102 train_time:843862ms step_avg:169.96ms\nstep:4976/10000 train_loss:4.0471 train_time:844033ms step_avg:169.96ms\nstep:4977/10000 train_loss:4.0289 train_time:844201ms step_avg:169.96ms\nstep:4978/10000 train_loss:3.9074 train_time:844372ms step_avg:169.96ms\nstep:4979/10000 train_loss:3.9259 train_time:844541ms step_avg:169.96ms\nstep:4980/10000 train_loss:4.2675 train_time:844710ms step_avg:169.96ms\nstep:4981/10000 train_loss:4.2076 train_time:844881ms step_avg:169.96ms\nstep:4982/10000 train_loss:4.0005 train_time:845050ms step_avg:169.96ms\nstep:4983/10000 train_loss:4.1101 train_time:845219ms step_avg:169.96ms\nstep:4984/10000 train_loss:3.9193 train_time:845389ms step_avg:169.96ms\nstep:4985/10000 train_loss:3.9688 train_time:845559ms step_avg:169.96ms\nstep:4986/10000 train_loss:4.0208 train_time:845729ms step_avg:169.96ms\nstep:4987/10000 train_loss:4.1215 train_time:845902ms step_avg:169.96ms\nstep:4988/10000 train_loss:3.9823 train_time:846071ms step_avg:169.96ms\nstep:4989/10000 train_loss:3.9498 train_time:846240ms step_avg:169.96ms\nstep:4990/10000 train_loss:4.0193 train_time:846410ms step_avg:169.96ms\nstep:4991/10000 train_loss:4.0128 train_time:846579ms step_avg:169.96ms\nstep:4992/10000 train_loss:4.1063 train_time:846749ms step_avg:169.96ms\nstep:4993/10000 train_loss:4.0197 train_time:846920ms step_avg:169.96ms\nstep:4994/10000 train_loss:3.8590 train_time:847088ms step_avg:169.96ms\nstep:4995/10000 train_loss:4.0366 train_time:847258ms step_avg:169.96ms\nstep:4996/10000 train_loss:3.9366 train_time:847428ms step_avg:169.96ms\nstep:4997/10000 train_loss:4.0065 train_time:847598ms step_avg:169.96ms\nstep:4998/10000 train_loss:4.0146 train_time:847766ms step_avg:169.96ms\nstep:4999/10000 train_loss:3.9939 train_time:847937ms step_avg:169.96ms\nstep:5000/10000 train_loss:3.8946 train_time:848104ms step_avg:169.96ms\nstep:5001/10000 train_loss:4.0122 train_time:848274ms step_avg:169.96ms\nstep:5002/10000 train_loss:4.0708 train_time:848443ms step_avg:169.96ms\nstep:5003/10000 train_loss:3.9965 train_time:848614ms step_avg:169.96ms\nstep:5004/10000 train_loss:3.9278 train_time:848782ms step_avg:169.96ms\nstep:5005/10000 train_loss:3.9900 train_time:848952ms step_avg:169.96ms\nstep:5006/10000 train_loss:4.0222 train_time:849121ms step_avg:169.96ms\nstep:5007/10000 train_loss:4.0367 train_time:849293ms step_avg:169.96ms\nstep:5008/10000 train_loss:3.7900 train_time:849463ms step_avg:169.96ms\nstep:5009/10000 train_loss:3.9561 train_time:849634ms step_avg:169.96ms\nstep:5010/10000 train_loss:3.9084 train_time:849806ms step_avg:169.96ms\nstep:5011/10000 train_loss:3.8733 train_time:849970ms step_avg:169.96ms\nstep:5012/10000 train_loss:4.0597 train_time:850137ms step_avg:169.96ms\nstep:5013/10000 train_loss:3.9092 train_time:850305ms step_avg:169.96ms\nstep:5014/10000 train_loss:4.1381 train_time:850476ms step_avg:169.96ms\nstep:5015/10000 train_loss:3.8996 train_time:850645ms step_avg:169.96ms\nstep:5016/10000 train_loss:4.1488 train_time:850822ms step_avg:169.96ms\nstep:5017/10000 train_loss:3.8908 train_time:850990ms step_avg:169.96ms\nstep:5018/10000 train_loss:4.0262 train_time:851161ms step_avg:169.96ms\nstep:5019/10000 train_loss:3.9995 train_time:851323ms step_avg:169.96ms\nstep:5020/10000 train_loss:3.9143 train_time:851493ms step_avg:169.96ms\nstep:5021/10000 train_loss:3.9693 train_time:851667ms step_avg:169.96ms\nstep:5022/10000 train_loss:3.9103 train_time:851833ms step_avg:169.96ms\nstep:5023/10000 train_loss:4.0623 train_time:852002ms step_avg:169.96ms\nstep:5024/10000 train_loss:4.0877 train_time:852171ms step_avg:169.96ms\nstep:5025/10000 train_loss:4.0132 train_time:852342ms step_avg:169.96ms\nstep:5026/10000 train_loss:4.0653 train_time:852513ms step_avg:169.96ms\nstep:5027/10000 train_loss:3.9811 train_time:852682ms step_avg:169.96ms\nstep:5028/10000 train_loss:3.8357 train_time:852851ms step_avg:169.96ms\nstep:5029/10000 train_loss:4.1016 train_time:853022ms step_avg:169.96ms\nstep:5030/10000 train_loss:4.1246 train_time:853191ms step_avg:169.96ms\nstep:5031/10000 train_loss:4.0428 train_time:853362ms step_avg:169.96ms\nstep:5032/10000 train_loss:4.0310 train_time:853532ms step_avg:169.96ms\nstep:5033/10000 train_loss:3.8709 train_time:853702ms step_avg:169.96ms\nstep:5034/10000 train_loss:3.9462 train_time:853873ms step_avg:169.96ms\nstep:5035/10000 train_loss:4.0428 train_time:854045ms step_avg:169.96ms\nstep:5036/10000 train_loss:4.1513 train_time:854215ms step_avg:169.96ms\nstep:5037/10000 train_loss:3.9882 train_time:854385ms step_avg:169.96ms\nstep:5038/10000 train_loss:3.9393 train_time:854556ms step_avg:169.96ms\nstep:5039/10000 train_loss:3.9098 train_time:854724ms step_avg:169.96ms\nstep:5040/10000 train_loss:3.9412 train_time:854894ms step_avg:169.96ms\nstep:5041/10000 train_loss:4.1280 train_time:855061ms step_avg:169.96ms\nstep:5042/10000 train_loss:3.9745 train_time:855231ms step_avg:169.96ms\nstep:5043/10000 train_loss:4.8098 train_time:855400ms step_avg:169.96ms\nstep:5044/10000 train_loss:4.0219 train_time:855572ms step_avg:169.96ms\nstep:5045/10000 train_loss:4.0254 train_time:855742ms step_avg:169.96ms\nstep:5046/10000 train_loss:3.9847 train_time:855913ms step_avg:169.96ms\nstep:5047/10000 train_loss:4.0043 train_time:856083ms step_avg:169.96ms\nstep:5048/10000 train_loss:4.1435 train_time:856254ms step_avg:169.96ms\nstep:5049/10000 train_loss:3.9721 train_time:856422ms step_avg:169.96ms\nstep:5050/10000 train_loss:3.8229 train_time:856591ms step_avg:169.96ms\nstep:5051/10000 train_loss:4.1026 train_time:856761ms step_avg:169.96ms\nstep:5052/10000 train_loss:3.8504 train_time:856931ms step_avg:169.96ms\nstep:5053/10000 train_loss:4.0112 train_time:857102ms step_avg:169.96ms\nstep:5054/10000 train_loss:3.9020 train_time:857271ms step_avg:169.96ms\nstep:5055/10000 train_loss:3.9966 train_time:857441ms step_avg:169.96ms\nstep:5056/10000 train_loss:4.0613 train_time:857609ms step_avg:169.96ms\nstep:5057/10000 train_loss:4.1042 train_time:857778ms step_avg:169.96ms\nstep:5058/10000 train_loss:4.0307 train_time:857946ms step_avg:169.96ms\nstep:5059/10000 train_loss:4.1778 train_time:858116ms step_avg:169.96ms\nstep:5060/10000 train_loss:3.9745 train_time:858283ms step_avg:169.96ms\nstep:5061/10000 train_loss:4.0343 train_time:858453ms step_avg:169.96ms\nstep:5062/10000 train_loss:3.8220 train_time:858620ms step_avg:169.96ms\nstep:5063/10000 train_loss:4.0318 train_time:858787ms step_avg:169.96ms\nstep:5064/10000 train_loss:3.9924 train_time:858955ms step_avg:169.96ms\nstep:5065/10000 train_loss:4.1602 train_time:859123ms step_avg:169.96ms\nstep:5066/10000 train_loss:4.0664 train_time:859294ms step_avg:169.96ms\nstep:5067/10000 train_loss:3.9642 train_time:859464ms step_avg:169.96ms\nstep:5068/10000 train_loss:3.9080 train_time:859635ms step_avg:169.96ms\nstep:5069/10000 train_loss:3.9293 train_time:859802ms step_avg:169.95ms\nstep:5070/10000 train_loss:4.0491 train_time:859973ms step_avg:169.96ms\nstep:5071/10000 train_loss:4.0968 train_time:860141ms step_avg:169.95ms\nstep:5072/10000 train_loss:3.9201 train_time:860314ms step_avg:169.96ms\nstep:5073/10000 train_loss:3.8640 train_time:860483ms step_avg:169.96ms\nstep:5074/10000 train_loss:4.0458 train_time:860650ms step_avg:169.95ms\nstep:5075/10000 train_loss:3.8900 train_time:860819ms step_avg:169.95ms\nstep:5076/10000 train_loss:4.0227 train_time:860987ms step_avg:169.95ms\nstep:5077/10000 train_loss:4.0606 train_time:861157ms step_avg:169.95ms\nstep:5078/10000 train_loss:3.9829 train_time:861327ms step_avg:169.95ms\nstep:5079/10000 train_loss:3.7967 train_time:861499ms step_avg:169.95ms\nstep:5080/10000 train_loss:3.9079 train_time:861669ms step_avg:169.95ms\nstep:5081/10000 train_loss:4.0001 train_time:861837ms step_avg:169.95ms\nstep:5082/10000 train_loss:4.0308 train_time:862006ms step_avg:169.95ms\nstep:5083/10000 train_loss:3.9207 train_time:862176ms step_avg:169.95ms\nstep:5084/10000 train_loss:3.9969 train_time:862342ms step_avg:169.95ms\nstep:5085/10000 train_loss:3.9085 train_time:862512ms step_avg:169.95ms\nstep:5086/10000 train_loss:3.8932 train_time:862680ms step_avg:169.95ms\nstep:5087/10000 train_loss:4.0431 train_time:862850ms step_avg:169.95ms\nstep:5088/10000 train_loss:3.9931 train_time:863019ms step_avg:169.95ms\nstep:5089/10000 train_loss:3.9269 train_time:863188ms step_avg:169.95ms\nstep:5090/10000 train_loss:3.7366 train_time:863359ms step_avg:169.95ms\nstep:5091/10000 train_loss:3.9731 train_time:863529ms step_avg:169.95ms\nstep:5092/10000 train_loss:4.0093 train_time:863697ms step_avg:169.95ms\nstep:5093/10000 train_loss:4.1708 train_time:863865ms step_avg:169.95ms\nstep:5094/10000 train_loss:3.9961 train_time:864033ms step_avg:169.95ms\nstep:5095/10000 train_loss:3.9541 train_time:864201ms step_avg:169.95ms\nstep:5096/10000 train_loss:3.9223 train_time:864370ms step_avg:169.95ms\nstep:5097/10000 train_loss:3.9502 train_time:864539ms step_avg:169.95ms\nstep:5098/10000 train_loss:3.9674 train_time:864708ms step_avg:169.95ms\nstep:5099/10000 train_loss:4.0199 train_time:864879ms step_avg:169.95ms\nstep:5100/10000 train_loss:4.0493 train_time:865047ms step_avg:169.95ms\nstep:5101/10000 train_loss:3.9889 train_time:865217ms step_avg:169.95ms\nstep:5102/10000 train_loss:4.0395 train_time:865387ms step_avg:169.95ms\nstep:5103/10000 train_loss:4.1186 train_time:865559ms step_avg:169.95ms\nstep:5104/10000 train_loss:3.9529 train_time:865728ms step_avg:169.95ms\nstep:5105/10000 train_loss:4.0223 train_time:865896ms step_avg:169.95ms\nstep:5106/10000 train_loss:3.9491 train_time:866064ms step_avg:169.95ms\nstep:5107/10000 train_loss:3.9612 train_time:866234ms step_avg:169.95ms\nstep:5108/10000 train_loss:4.0332 train_time:866402ms step_avg:169.95ms\nstep:5109/10000 train_loss:4.0198 train_time:866580ms step_avg:169.95ms\nstep:5110/10000 train_loss:4.0583 train_time:866740ms step_avg:169.95ms\nstep:5111/10000 train_loss:3.8518 train_time:866910ms step_avg:169.95ms\nstep:5112/10000 train_loss:3.9986 train_time:867080ms step_avg:169.95ms\nstep:5113/10000 train_loss:3.8835 train_time:867249ms step_avg:169.95ms\nstep:5114/10000 train_loss:4.0864 train_time:867418ms step_avg:169.95ms\nstep:5115/10000 train_loss:4.0386 train_time:867588ms step_avg:169.95ms\nstep:5116/10000 train_loss:3.9333 train_time:867758ms step_avg:169.95ms\nstep:5117/10000 train_loss:4.0796 train_time:867927ms step_avg:169.95ms\nstep:5118/10000 train_loss:4.0420 train_time:868100ms step_avg:169.95ms\nstep:5119/10000 train_loss:3.9562 train_time:868266ms step_avg:169.95ms\nstep:5120/10000 train_loss:3.8622 train_time:868438ms step_avg:169.95ms\nstep:5121/10000 train_loss:3.9173 train_time:868610ms step_avg:169.95ms\nstep:5122/10000 train_loss:4.0319 train_time:868778ms step_avg:169.95ms\nstep:5123/10000 train_loss:4.0360 train_time:868947ms step_avg:169.95ms\nstep:5124/10000 train_loss:4.0206 train_time:869119ms step_avg:169.95ms\nstep:5125/10000 train_loss:3.9657 train_time:869287ms step_avg:169.95ms\nstep:5126/10000 train_loss:3.8478 train_time:869458ms step_avg:169.95ms\nstep:5127/10000 train_loss:3.7846 train_time:869626ms step_avg:169.95ms\nstep:5128/10000 train_loss:3.9222 train_time:869797ms step_avg:169.95ms\nstep:5129/10000 train_loss:4.1095 train_time:869965ms step_avg:169.95ms\nstep:5130/10000 train_loss:4.0020 train_time:870136ms step_avg:169.95ms\nstep:5131/10000 train_loss:4.0194 train_time:870305ms step_avg:169.95ms\nstep:5132/10000 train_loss:4.1213 train_time:870475ms step_avg:169.95ms\nstep:5133/10000 train_loss:3.9928 train_time:870643ms step_avg:169.95ms\nstep:5134/10000 train_loss:4.0559 train_time:870813ms step_avg:169.95ms\nstep:5135/10000 train_loss:3.9570 train_time:870985ms step_avg:169.95ms\nstep:5136/10000 train_loss:3.9518 train_time:871154ms step_avg:169.95ms\nstep:5137/10000 train_loss:4.0769 train_time:871322ms step_avg:169.95ms\nstep:5138/10000 train_loss:4.0770 train_time:871496ms step_avg:169.95ms\nstep:5139/10000 train_loss:3.9246 train_time:871666ms step_avg:169.95ms\nstep:5140/10000 train_loss:4.0615 train_time:871835ms step_avg:169.95ms\nstep:5141/10000 train_loss:3.9700 train_time:872003ms step_avg:169.95ms\nstep:5142/10000 train_loss:3.9700 train_time:872175ms step_avg:169.95ms\nstep:5143/10000 train_loss:3.8690 train_time:872344ms step_avg:169.95ms\nstep:5144/10000 train_loss:3.9244 train_time:872513ms step_avg:169.95ms\nstep:5145/10000 train_loss:3.9705 train_time:872684ms step_avg:169.95ms\nstep:5146/10000 train_loss:4.0189 train_time:872854ms step_avg:169.95ms\nstep:5147/10000 train_loss:3.8618 train_time:873023ms step_avg:169.95ms\nstep:5148/10000 train_loss:4.0264 train_time:873193ms step_avg:169.95ms\nstep:5149/10000 train_loss:3.8761 train_time:873362ms step_avg:169.95ms\nstep:5150/10000 train_loss:3.9743 train_time:873531ms step_avg:169.95ms\nstep:5151/10000 train_loss:4.0664 train_time:873702ms step_avg:169.95ms\nstep:5152/10000 train_loss:4.0421 train_time:873875ms step_avg:169.95ms\nstep:5153/10000 train_loss:4.0534 train_time:874044ms step_avg:169.95ms\nstep:5154/10000 train_loss:3.9582 train_time:874216ms step_avg:169.95ms\nstep:5155/10000 train_loss:3.9459 train_time:874384ms step_avg:169.95ms\nstep:5156/10000 train_loss:3.9702 train_time:874553ms step_avg:169.95ms\nstep:5157/10000 train_loss:4.1518 train_time:874722ms step_avg:169.95ms\nstep:5158/10000 train_loss:4.0614 train_time:874890ms step_avg:169.95ms\nstep:5159/10000 train_loss:4.2285 train_time:875059ms step_avg:169.95ms\nstep:5160/10000 train_loss:3.9898 train_time:875231ms step_avg:169.95ms\nstep:5161/10000 train_loss:3.8919 train_time:875403ms step_avg:169.95ms\nstep:5162/10000 train_loss:3.9378 train_time:875571ms step_avg:169.95ms\nstep:5163/10000 train_loss:3.9793 train_time:875741ms step_avg:169.95ms\nstep:5164/10000 train_loss:4.1306 train_time:875910ms step_avg:169.95ms\nstep:5165/10000 train_loss:3.8698 train_time:876078ms step_avg:169.95ms\nstep:5166/10000 train_loss:3.9961 train_time:876247ms step_avg:169.95ms\nstep:5167/10000 train_loss:3.8009 train_time:876418ms step_avg:169.95ms\nstep:5168/10000 train_loss:3.9783 train_time:876586ms step_avg:169.95ms\nstep:5169/10000 train_loss:4.0609 train_time:876757ms step_avg:169.95ms\nstep:5170/10000 train_loss:4.0711 train_time:876926ms step_avg:169.95ms\nstep:5171/10000 train_loss:4.0183 train_time:877095ms step_avg:169.95ms\nstep:5172/10000 train_loss:3.7458 train_time:877264ms step_avg:169.95ms\nstep:5173/10000 train_loss:3.8468 train_time:877436ms step_avg:169.95ms\nstep:5174/10000 train_loss:4.0042 train_time:877604ms step_avg:169.95ms\nstep:5175/10000 train_loss:3.9313 train_time:877773ms step_avg:169.95ms\nstep:5176/10000 train_loss:3.9849 train_time:877949ms step_avg:169.95ms\nstep:5177/10000 train_loss:4.0760 train_time:878116ms step_avg:169.95ms\nstep:5178/10000 train_loss:4.0940 train_time:878287ms step_avg:169.95ms\nstep:5179/10000 train_loss:3.9583 train_time:878451ms step_avg:169.95ms\nstep:5180/10000 train_loss:4.0006 train_time:878618ms step_avg:169.95ms\nstep:5181/10000 train_loss:4.1698 train_time:878787ms step_avg:169.95ms\nstep:5182/10000 train_loss:4.1183 train_time:878958ms step_avg:169.95ms\nstep:5183/10000 train_loss:3.9623 train_time:879127ms step_avg:169.95ms\nstep:5184/10000 train_loss:4.0331 train_time:879301ms step_avg:169.95ms\nstep:5185/10000 train_loss:4.1400 train_time:879471ms step_avg:169.95ms\nstep:5186/10000 train_loss:4.1016 train_time:879642ms step_avg:169.95ms\nstep:5187/10000 train_loss:3.9658 train_time:879812ms step_avg:169.95ms\nstep:5188/10000 train_loss:4.0468 train_time:879978ms step_avg:169.95ms\nstep:5189/10000 train_loss:3.9235 train_time:880148ms step_avg:169.95ms\nstep:5190/10000 train_loss:3.9021 train_time:880318ms step_avg:169.95ms\nstep:5191/10000 train_loss:4.0918 train_time:880488ms step_avg:169.95ms\nstep:5192/10000 train_loss:4.0153 train_time:880659ms step_avg:169.95ms\nstep:5193/10000 train_loss:4.0631 train_time:880836ms step_avg:169.95ms\nstep:5194/10000 train_loss:3.9713 train_time:881000ms step_avg:169.95ms\nstep:5195/10000 train_loss:4.1042 train_time:881168ms step_avg:169.95ms\nstep:5196/10000 train_loss:4.0848 train_time:881338ms step_avg:169.95ms\nstep:5197/10000 train_loss:3.9726 train_time:881508ms step_avg:169.95ms\nstep:5198/10000 train_loss:3.8634 train_time:881676ms step_avg:169.95ms\nstep:5199/10000 train_loss:4.1650 train_time:881845ms step_avg:169.95ms\nstep:5200/10000 train_loss:3.9842 train_time:882013ms step_avg:169.94ms\nstep:5201/10000 train_loss:3.8810 train_time:882182ms step_avg:169.94ms\nstep:5202/10000 train_loss:3.9728 train_time:882351ms step_avg:169.94ms\nstep:5203/10000 train_loss:3.9278 train_time:882522ms step_avg:169.94ms\nstep:5204/10000 train_loss:4.1059 train_time:882693ms step_avg:169.94ms\nstep:5205/10000 train_loss:4.0766 train_time:882862ms step_avg:169.94ms\nstep:5206/10000 train_loss:3.9916 train_time:883030ms step_avg:169.94ms\nstep:5207/10000 train_loss:3.9329 train_time:883198ms step_avg:169.94ms\nstep:5208/10000 train_loss:4.0338 train_time:883365ms step_avg:169.94ms\nstep:5209/10000 train_loss:3.8717 train_time:883536ms step_avg:169.94ms\nstep:5210/10000 train_loss:4.0285 train_time:883703ms step_avg:169.94ms\nstep:5211/10000 train_loss:3.9041 train_time:883875ms step_avg:169.94ms\nstep:5212/10000 train_loss:4.0170 train_time:884043ms step_avg:169.94ms\nstep:5213/10000 train_loss:4.0966 train_time:884213ms step_avg:169.94ms\nstep:5214/10000 train_loss:4.0364 train_time:884383ms step_avg:169.94ms\nstep:5215/10000 train_loss:4.1434 train_time:884553ms step_avg:169.94ms\nstep:5216/10000 train_loss:3.9449 train_time:884723ms step_avg:169.94ms\nstep:5217/10000 train_loss:3.9190 train_time:884895ms step_avg:169.94ms\nstep:5218/10000 train_loss:4.1536 train_time:885064ms step_avg:169.94ms\nstep:5219/10000 train_loss:3.9749 train_time:885235ms step_avg:169.94ms\nstep:5220/10000 train_loss:4.0822 train_time:885405ms step_avg:169.94ms\nstep:5221/10000 train_loss:3.9658 train_time:885576ms step_avg:169.94ms\nstep:5222/10000 train_loss:3.9663 train_time:885744ms step_avg:169.94ms\nstep:5223/10000 train_loss:3.9513 train_time:885913ms step_avg:169.94ms\nstep:5224/10000 train_loss:4.1266 train_time:886084ms step_avg:169.94ms\nstep:5225/10000 train_loss:4.0130 train_time:886257ms step_avg:169.94ms\nstep:5226/10000 train_loss:4.1719 train_time:886426ms step_avg:169.94ms\nstep:5227/10000 train_loss:3.9653 train_time:886597ms step_avg:169.94ms\nstep:5228/10000 train_loss:3.9144 train_time:886765ms step_avg:169.94ms\nstep:5229/10000 train_loss:3.9504 train_time:886933ms step_avg:169.94ms\nstep:5230/10000 train_loss:3.9096 train_time:887104ms step_avg:169.94ms\nstep:5231/10000 train_loss:3.9398 train_time:887277ms step_avg:169.94ms\nstep:5232/10000 train_loss:3.9229 train_time:887444ms step_avg:169.94ms\nstep:5233/10000 train_loss:3.9957 train_time:887616ms step_avg:169.94ms\nstep:5234/10000 train_loss:4.0471 train_time:887783ms step_avg:169.94ms\nstep:5235/10000 train_loss:3.9766 train_time:887953ms step_avg:169.94ms\nstep:5236/10000 train_loss:3.9227 train_time:888122ms step_avg:169.94ms\nstep:5237/10000 train_loss:3.9983 train_time:888292ms step_avg:169.94ms\nstep:5238/10000 train_loss:4.0665 train_time:888462ms step_avg:169.94ms\nstep:5239/10000 train_loss:3.8877 train_time:888632ms step_avg:169.94ms\nstep:5240/10000 train_loss:3.9387 train_time:888801ms step_avg:169.94ms\nstep:5241/10000 train_loss:3.9330 train_time:888971ms step_avg:169.94ms\nstep:5242/10000 train_loss:3.9666 train_time:889138ms step_avg:169.94ms\nstep:5243/10000 train_loss:4.0045 train_time:889308ms step_avg:169.94ms\nstep:5244/10000 train_loss:3.9003 train_time:889480ms step_avg:169.94ms\nstep:5245/10000 train_loss:3.9764 train_time:889650ms step_avg:169.94ms\nstep:5246/10000 train_loss:3.8700 train_time:889821ms step_avg:169.94ms\nstep:5247/10000 train_loss:4.0555 train_time:889989ms step_avg:169.94ms\nstep:5248/10000 train_loss:4.0768 train_time:890159ms step_avg:169.94ms\nstep:5249/10000 train_loss:4.2615 train_time:890328ms step_avg:169.94ms\nstep:5250/10000 train_loss:3.9370 train_time:890499ms step_avg:169.94ms\nstep:5251/10000 train_loss:3.8235 train_time:890666ms step_avg:169.94ms\nstep:5252/10000 train_loss:3.9890 train_time:890837ms step_avg:169.94ms\nstep:5253/10000 train_loss:3.9783 train_time:891005ms step_avg:169.94ms\nstep:5254/10000 train_loss:3.8888 train_time:891174ms step_avg:169.94ms\nstep:5255/10000 train_loss:4.0632 train_time:891344ms step_avg:169.94ms\nstep:5256/10000 train_loss:4.0478 train_time:891519ms step_avg:169.94ms\nstep:5257/10000 train_loss:3.9042 train_time:891681ms step_avg:169.94ms\nstep:5258/10000 train_loss:4.0383 train_time:891849ms step_avg:169.94ms\nstep:5259/10000 train_loss:4.0067 train_time:892024ms step_avg:169.94ms\nstep:5260/10000 train_loss:4.0294 train_time:892192ms step_avg:169.94ms\nstep:5261/10000 train_loss:3.9420 train_time:892356ms step_avg:169.94ms\nstep:5262/10000 train_loss:3.9653 train_time:892526ms step_avg:169.94ms\nstep:5263/10000 train_loss:3.9508 train_time:892698ms step_avg:169.94ms\nstep:5264/10000 train_loss:4.0215 train_time:892866ms step_avg:169.94ms\nstep:5265/10000 train_loss:3.8464 train_time:893036ms step_avg:169.94ms\nstep:5266/10000 train_loss:3.7029 train_time:893204ms step_avg:169.94ms\nstep:5267/10000 train_loss:3.8832 train_time:893374ms step_avg:169.94ms\nstep:5268/10000 train_loss:3.8763 train_time:893543ms step_avg:169.94ms\nstep:5269/10000 train_loss:3.8785 train_time:893713ms step_avg:169.94ms\nstep:5270/10000 train_loss:3.9273 train_time:893883ms step_avg:169.94ms\nstep:5271/10000 train_loss:4.0216 train_time:894052ms step_avg:169.94ms\nstep:5272/10000 train_loss:4.1122 train_time:894222ms step_avg:169.94ms\nstep:5273/10000 train_loss:3.8813 train_time:894394ms step_avg:169.94ms\nstep:5274/10000 train_loss:3.9862 train_time:894563ms step_avg:169.94ms\nstep:5275/10000 train_loss:4.0758 train_time:894736ms step_avg:169.94ms\nstep:5276/10000 train_loss:4.0477 train_time:894905ms step_avg:169.94ms\nstep:5277/10000 train_loss:4.0719 train_time:895075ms step_avg:169.94ms\nstep:5278/10000 train_loss:4.0111 train_time:895245ms step_avg:169.94ms\nstep:5279/10000 train_loss:3.8434 train_time:895417ms step_avg:169.94ms\nstep:5280/10000 train_loss:3.9693 train_time:895584ms step_avg:169.94ms\nstep:5281/10000 train_loss:4.0484 train_time:895754ms step_avg:169.94ms\nstep:5282/10000 train_loss:3.8581 train_time:895923ms step_avg:169.94ms\nstep:5283/10000 train_loss:3.9546 train_time:896093ms step_avg:169.94ms\nstep:5284/10000 train_loss:4.0574 train_time:896262ms step_avg:169.94ms\nstep:5285/10000 train_loss:3.9068 train_time:896433ms step_avg:169.94ms\nstep:5286/10000 train_loss:4.0292 train_time:896604ms step_avg:169.94ms\nstep:5287/10000 train_loss:3.8762 train_time:896774ms step_avg:169.94ms\nstep:5288/10000 train_loss:4.0128 train_time:896943ms step_avg:169.94ms\nstep:5289/10000 train_loss:4.0417 train_time:897114ms step_avg:169.94ms\nstep:5290/10000 train_loss:4.0717 train_time:897285ms step_avg:169.94ms\nstep:5291/10000 train_loss:3.9690 train_time:897457ms step_avg:169.94ms\nstep:5292/10000 train_loss:3.9362 train_time:897625ms step_avg:169.94ms\nstep:5293/10000 train_loss:4.1054 train_time:897796ms step_avg:169.94ms\nstep:5294/10000 train_loss:3.9616 train_time:897964ms step_avg:169.94ms\nstep:5295/10000 train_loss:3.7201 train_time:898133ms step_avg:169.94ms\nstep:5296/10000 train_loss:4.1588 train_time:898303ms step_avg:169.94ms\nstep:5297/10000 train_loss:4.0887 train_time:898474ms step_avg:169.94ms\nstep:5298/10000 train_loss:3.9401 train_time:898643ms step_avg:169.94ms\nstep:5299/10000 train_loss:4.0223 train_time:898814ms step_avg:169.94ms\nstep:5300/10000 train_loss:4.0557 train_time:898983ms step_avg:169.94ms\nstep:5301/10000 train_loss:3.7726 train_time:899152ms step_avg:169.94ms\nstep:5302/10000 train_loss:3.9818 train_time:899322ms step_avg:169.94ms\nstep:5303/10000 train_loss:3.8697 train_time:899494ms step_avg:169.94ms\nstep:5304/10000 train_loss:4.0730 train_time:899664ms step_avg:169.94ms\nstep:5305/10000 train_loss:3.9481 train_time:899834ms step_avg:169.94ms\nstep:5306/10000 train_loss:4.0581 train_time:900004ms step_avg:169.94ms\nstep:5307/10000 train_loss:3.9615 train_time:900174ms step_avg:169.94ms\nstep:5308/10000 train_loss:4.1131 train_time:900343ms step_avg:169.94ms\nstep:5309/10000 train_loss:4.0457 train_time:900513ms step_avg:169.94ms\nstep:5310/10000 train_loss:4.1179 train_time:900683ms step_avg:169.94ms\nstep:5311/10000 train_loss:3.9024 train_time:900853ms step_avg:169.94ms\nstep:5312/10000 train_loss:3.8855 train_time:901020ms step_avg:169.94ms\nstep:5313/10000 train_loss:3.9103 train_time:901189ms step_avg:169.94ms\nstep:5314/10000 train_loss:3.9023 train_time:901360ms step_avg:169.94ms\nstep:5315/10000 train_loss:4.0200 train_time:901531ms step_avg:169.94ms\nstep:5316/10000 train_loss:3.9350 train_time:901698ms step_avg:169.94ms\nstep:5317/10000 train_loss:3.9316 train_time:901867ms step_avg:169.94ms\nstep:5318/10000 train_loss:3.7998 train_time:902036ms step_avg:169.94ms\nstep:5319/10000 train_loss:4.0145 train_time:902203ms step_avg:169.94ms\nstep:5320/10000 train_loss:4.0160 train_time:902373ms step_avg:169.94ms\nstep:5321/10000 train_loss:3.9561 train_time:902545ms step_avg:169.94ms\nstep:5322/10000 train_loss:4.0680 train_time:902716ms step_avg:169.94ms\nstep:5323/10000 train_loss:4.0298 train_time:902884ms step_avg:169.94ms\nstep:5324/10000 train_loss:2.8755 train_time:903053ms step_avg:169.94ms\nstep:5325/10000 train_loss:3.9620 train_time:903225ms step_avg:169.94ms\nstep:5326/10000 train_loss:3.9858 train_time:903394ms step_avg:169.94ms\nstep:5327/10000 train_loss:3.8998 train_time:903564ms step_avg:169.94ms\nstep:5328/10000 train_loss:3.9873 train_time:903735ms step_avg:169.94ms\nstep:5329/10000 train_loss:4.2670 train_time:903904ms step_avg:169.94ms\nstep:5330/10000 train_loss:4.0698 train_time:904072ms step_avg:169.94ms\nstep:5331/10000 train_loss:3.9576 train_time:904243ms step_avg:169.94ms\nstep:5332/10000 train_loss:3.9669 train_time:904414ms step_avg:169.94ms\nstep:5333/10000 train_loss:3.7663 train_time:904584ms step_avg:169.94ms\nstep:5334/10000 train_loss:3.9143 train_time:904966ms step_avg:169.98ms\nstep:5335/10000 train_loss:3.9525 train_time:905130ms step_avg:169.98ms\nstep:5336/10000 train_loss:4.0049 train_time:905299ms step_avg:169.98ms\nstep:5337/10000 train_loss:4.0103 train_time:905468ms step_avg:169.98ms\nstep:5338/10000 train_loss:3.9335 train_time:905638ms step_avg:169.98ms\nstep:5339/10000 train_loss:3.9423 train_time:905806ms step_avg:169.98ms\nstep:5340/10000 train_loss:4.3545 train_time:905983ms step_avg:169.98ms\nstep:5341/10000 train_loss:4.0893 train_time:906152ms step_avg:169.98ms\nstep:5342/10000 train_loss:3.9107 train_time:906321ms step_avg:169.98ms\nstep:5343/10000 train_loss:3.9525 train_time:906488ms step_avg:169.98ms\nstep:5344/10000 train_loss:4.0521 train_time:906656ms step_avg:169.98ms\nstep:5345/10000 train_loss:4.0554 train_time:906822ms step_avg:169.98ms\nstep:5346/10000 train_loss:4.0188 train_time:906992ms step_avg:169.98ms\nstep:5347/10000 train_loss:3.9315 train_time:907162ms step_avg:169.98ms\nstep:5348/10000 train_loss:3.9651 train_time:907332ms step_avg:169.98ms\nstep:5349/10000 train_loss:3.9362 train_time:907500ms step_avg:169.98ms\nstep:5350/10000 train_loss:3.9442 train_time:907669ms step_avg:169.98ms\nstep:5351/10000 train_loss:3.9857 train_time:907836ms step_avg:169.97ms\nstep:5352/10000 train_loss:3.9424 train_time:908005ms step_avg:169.97ms\nstep:5353/10000 train_loss:4.4251 train_time:908176ms step_avg:169.97ms\nstep:5354/10000 train_loss:4.0220 train_time:908347ms step_avg:169.98ms\nstep:5355/10000 train_loss:4.0043 train_time:908517ms step_avg:169.98ms\nstep:5356/10000 train_loss:3.9253 train_time:908686ms step_avg:169.97ms\nstep:5357/10000 train_loss:4.0120 train_time:908858ms step_avg:169.98ms\nstep:5358/10000 train_loss:3.9305 train_time:909026ms step_avg:169.97ms\nstep:5359/10000 train_loss:3.9556 train_time:909196ms step_avg:169.98ms\nstep:5360/10000 train_loss:4.1812 train_time:909365ms step_avg:169.97ms\nstep:5361/10000 train_loss:3.9248 train_time:909536ms step_avg:169.97ms\nstep:5362/10000 train_loss:3.8495 train_time:909706ms step_avg:169.97ms\nstep:5363/10000 train_loss:4.1506 train_time:909875ms step_avg:169.97ms\nstep:5364/10000 train_loss:4.0830 train_time:910045ms step_avg:169.97ms\nstep:5365/10000 train_loss:3.9580 train_time:910217ms step_avg:169.98ms\nstep:5366/10000 train_loss:3.8968 train_time:910384ms step_avg:169.97ms\nstep:5367/10000 train_loss:3.8282 train_time:910554ms step_avg:169.97ms\nstep:5368/10000 train_loss:3.9517 train_time:910722ms step_avg:169.97ms\nstep:5369/10000 train_loss:3.9173 train_time:910891ms step_avg:169.97ms\nstep:5370/10000 train_loss:4.0163 train_time:911061ms step_avg:169.97ms\nstep:5371/10000 train_loss:3.5655 train_time:911229ms step_avg:169.97ms\nstep:5372/10000 train_loss:3.9216 train_time:911399ms step_avg:169.97ms\nstep:5373/10000 train_loss:3.8319 train_time:911569ms step_avg:169.97ms\nstep:5374/10000 train_loss:3.9702 train_time:911737ms step_avg:169.97ms\nstep:5375/10000 train_loss:3.9844 train_time:911905ms step_avg:169.97ms\nstep:5376/10000 train_loss:4.2309 train_time:912075ms step_avg:169.97ms\nstep:5377/10000 train_loss:3.9026 train_time:912244ms step_avg:169.97ms\nstep:5378/10000 train_loss:4.1014 train_time:912414ms step_avg:169.97ms\nstep:5379/10000 train_loss:3.9551 train_time:912583ms step_avg:169.97ms\nstep:5380/10000 train_loss:4.0010 train_time:912752ms step_avg:169.97ms\nstep:5381/10000 train_loss:4.4050 train_time:912919ms step_avg:169.97ms\nstep:5382/10000 train_loss:3.9777 train_time:913089ms step_avg:169.97ms\nstep:5383/10000 train_loss:3.9323 train_time:913260ms step_avg:169.97ms\nstep:5384/10000 train_loss:3.9863 train_time:913429ms step_avg:169.97ms\nstep:5385/10000 train_loss:4.0242 train_time:913598ms step_avg:169.97ms\nstep:5386/10000 train_loss:3.9453 train_time:913768ms step_avg:169.97ms\nstep:5387/10000 train_loss:3.9788 train_time:913937ms step_avg:169.97ms\nstep:5388/10000 train_loss:4.0472 train_time:914106ms step_avg:169.97ms\nstep:5389/10000 train_loss:3.9560 train_time:914277ms step_avg:169.97ms\nstep:5390/10000 train_loss:3.9319 train_time:914446ms step_avg:169.97ms\nstep:5391/10000 train_loss:3.9148 train_time:914622ms step_avg:169.97ms\nstep:5392/10000 train_loss:4.0021 train_time:914784ms step_avg:169.97ms\nstep:5393/10000 train_loss:3.8355 train_time:914956ms step_avg:169.97ms\nstep:5394/10000 train_loss:3.8955 train_time:915123ms step_avg:169.97ms\nstep:5395/10000 train_loss:4.0701 train_time:915294ms step_avg:169.97ms\nstep:5396/10000 train_loss:3.9101 train_time:915463ms step_avg:169.97ms\nstep:5397/10000 train_loss:3.9556 train_time:915634ms step_avg:169.97ms\nstep:5398/10000 train_loss:3.8669 train_time:915801ms step_avg:169.97ms\nstep:5399/10000 train_loss:4.0413 train_time:915972ms step_avg:169.97ms\nstep:5400/10000 train_loss:3.8053 train_time:916142ms step_avg:169.97ms\nstep:5401/10000 train_loss:3.8911 train_time:916314ms step_avg:169.97ms\nstep:5402/10000 train_loss:4.1511 train_time:916483ms step_avg:169.97ms\nstep:5403/10000 train_loss:3.9919 train_time:916655ms step_avg:169.97ms\nstep:5404/10000 train_loss:3.9330 train_time:916820ms step_avg:169.97ms\nstep:5405/10000 train_loss:3.9963 train_time:916988ms step_avg:169.97ms\nstep:5406/10000 train_loss:3.8604 train_time:917160ms step_avg:169.97ms\nstep:5407/10000 train_loss:3.9767 train_time:917329ms step_avg:169.97ms\nstep:5408/10000 train_loss:3.8890 train_time:917498ms step_avg:169.97ms\nstep:5409/10000 train_loss:3.8640 train_time:917667ms step_avg:169.97ms\nstep:5410/10000 train_loss:3.9655 train_time:917836ms step_avg:169.97ms\nstep:5411/10000 train_loss:3.9041 train_time:918005ms step_avg:169.97ms\nstep:5412/10000 train_loss:4.0188 train_time:918174ms step_avg:169.97ms\nstep:5413/10000 train_loss:3.8787 train_time:918344ms step_avg:169.97ms\nstep:5414/10000 train_loss:4.0614 train_time:918513ms step_avg:169.97ms\nstep:5415/10000 train_loss:4.0218 train_time:918682ms step_avg:169.97ms\nstep:5416/10000 train_loss:3.8599 train_time:918852ms step_avg:169.97ms\nstep:5417/10000 train_loss:4.0846 train_time:919021ms step_avg:169.97ms\nstep:5418/10000 train_loss:4.0073 train_time:919191ms step_avg:169.97ms\nstep:5419/10000 train_loss:4.0419 train_time:919362ms step_avg:169.97ms\nstep:5420/10000 train_loss:3.9138 train_time:919530ms step_avg:169.97ms\nstep:5421/10000 train_loss:3.8281 train_time:919701ms step_avg:169.97ms\nstep:5422/10000 train_loss:3.9221 train_time:919869ms step_avg:169.97ms\nstep:5423/10000 train_loss:3.9568 train_time:920039ms step_avg:169.97ms\nstep:5424/10000 train_loss:4.0551 train_time:920209ms step_avg:169.97ms\nstep:5425/10000 train_loss:4.0779 train_time:920382ms step_avg:169.97ms\nstep:5426/10000 train_loss:3.9235 train_time:920551ms step_avg:169.97ms\nstep:5427/10000 train_loss:3.8911 train_time:920717ms step_avg:169.97ms\nstep:5428/10000 train_loss:3.8723 train_time:920886ms step_avg:169.97ms\nstep:5429/10000 train_loss:3.8779 train_time:921058ms step_avg:169.97ms\nstep:5430/10000 train_loss:4.0406 train_time:921227ms step_avg:169.97ms\nstep:5431/10000 train_loss:3.9224 train_time:921395ms step_avg:169.97ms\nstep:5432/10000 train_loss:3.7985 train_time:921565ms step_avg:169.97ms\nstep:5433/10000 train_loss:3.9091 train_time:921737ms step_avg:169.97ms\nstep:5434/10000 train_loss:3.9143 train_time:921907ms step_avg:169.97ms\nstep:5435/10000 train_loss:3.9313 train_time:922078ms step_avg:169.97ms\nstep:5436/10000 train_loss:4.1468 train_time:922246ms step_avg:169.97ms\nstep:5437/10000 train_loss:4.0293 train_time:922415ms step_avg:169.97ms\nstep:5438/10000 train_loss:3.9883 train_time:922584ms step_avg:169.97ms\nstep:5439/10000 train_loss:3.9091 train_time:922753ms step_avg:169.97ms\nstep:5440/10000 train_loss:4.0886 train_time:922923ms step_avg:169.97ms\nstep:5441/10000 train_loss:4.1415 train_time:923095ms step_avg:169.97ms\nstep:5442/10000 train_loss:4.1296 train_time:923262ms step_avg:169.97ms\nstep:5443/10000 train_loss:3.9431 train_time:923435ms step_avg:169.97ms\nstep:5444/10000 train_loss:3.8020 train_time:923603ms step_avg:169.97ms\nstep:5445/10000 train_loss:3.9978 train_time:923773ms step_avg:169.97ms\nstep:5446/10000 train_loss:4.1268 train_time:923944ms step_avg:169.97ms\nstep:5447/10000 train_loss:4.0533 train_time:924121ms step_avg:169.97ms\nstep:5448/10000 train_loss:4.0969 train_time:924286ms step_avg:169.97ms\nstep:5449/10000 train_loss:3.9333 train_time:924457ms step_avg:169.97ms\nstep:5450/10000 train_loss:3.8882 train_time:924625ms step_avg:169.97ms\nstep:5451/10000 train_loss:3.9579 train_time:924795ms step_avg:169.97ms\nstep:5452/10000 train_loss:3.9527 train_time:924965ms step_avg:169.97ms\nstep:5453/10000 train_loss:4.0446 train_time:925135ms step_avg:169.97ms\nstep:5454/10000 train_loss:3.9081 train_time:925304ms step_avg:169.97ms\nstep:5455/10000 train_loss:3.7671 train_time:925475ms step_avg:169.97ms\nstep:5456/10000 train_loss:3.8851 train_time:925643ms step_avg:169.97ms\nstep:5457/10000 train_loss:3.9523 train_time:925814ms step_avg:169.97ms\nstep:5458/10000 train_loss:3.9243 train_time:925983ms step_avg:169.97ms\nstep:5459/10000 train_loss:3.9703 train_time:926157ms step_avg:169.97ms\nstep:5460/10000 train_loss:3.9954 train_time:926326ms step_avg:169.97ms\nstep:5461/10000 train_loss:3.9980 train_time:926498ms step_avg:169.97ms\nstep:5462/10000 train_loss:3.8649 train_time:926665ms step_avg:169.97ms\nstep:5463/10000 train_loss:4.0181 train_time:926835ms step_avg:169.97ms\nstep:5464/10000 train_loss:3.8859 train_time:927003ms step_avg:169.97ms\nstep:5465/10000 train_loss:4.0621 train_time:927175ms step_avg:169.97ms\nstep:5466/10000 train_loss:3.8033 train_time:927343ms step_avg:169.97ms\nstep:5467/10000 train_loss:3.8286 train_time:927514ms step_avg:169.97ms\nstep:5468/10000 train_loss:3.9436 train_time:927682ms step_avg:169.97ms\nstep:5469/10000 train_loss:4.0274 train_time:927850ms step_avg:169.97ms\nstep:5470/10000 train_loss:3.8286 train_time:928019ms step_avg:169.97ms\nstep:5471/10000 train_loss:3.9657 train_time:928190ms step_avg:169.97ms\nstep:5472/10000 train_loss:3.9450 train_time:928360ms step_avg:169.97ms\nstep:5473/10000 train_loss:3.9613 train_time:928532ms step_avg:169.97ms\nstep:5474/10000 train_loss:3.9145 train_time:928700ms step_avg:169.97ms\nstep:5475/10000 train_loss:3.8767 train_time:928868ms step_avg:169.97ms\nstep:5476/10000 train_loss:4.0397 train_time:929037ms step_avg:169.97ms\nstep:5477/10000 train_loss:3.9287 train_time:929205ms step_avg:169.97ms\nstep:5478/10000 train_loss:3.9732 train_time:929377ms step_avg:169.97ms\nstep:5479/10000 train_loss:3.9451 train_time:929546ms step_avg:169.97ms\nstep:5480/10000 train_loss:4.0226 train_time:929715ms step_avg:169.97ms\nstep:5481/10000 train_loss:4.1155 train_time:929883ms step_avg:169.97ms\nstep:5482/10000 train_loss:3.9568 train_time:930055ms step_avg:169.97ms\nstep:5483/10000 train_loss:3.9792 train_time:930223ms step_avg:169.97ms\nstep:5484/10000 train_loss:3.9942 train_time:930396ms step_avg:169.97ms\nstep:5485/10000 train_loss:3.9297 train_time:930566ms step_avg:169.97ms\nstep:5486/10000 train_loss:4.1371 train_time:930735ms step_avg:169.97ms\nstep:5487/10000 train_loss:3.9588 train_time:930905ms step_avg:169.97ms\nstep:5488/10000 train_loss:3.8380 train_time:931074ms step_avg:169.97ms\nstep:5489/10000 train_loss:3.8550 train_time:931245ms step_avg:169.97ms\nstep:5490/10000 train_loss:3.9510 train_time:931414ms step_avg:169.97ms\nstep:5491/10000 train_loss:3.8869 train_time:931581ms step_avg:169.97ms\nstep:5492/10000 train_loss:4.2028 train_time:931750ms step_avg:169.97ms\nstep:5493/10000 train_loss:3.9013 train_time:931920ms step_avg:169.97ms\nstep:5494/10000 train_loss:3.8138 train_time:932088ms step_avg:169.97ms\nstep:5495/10000 train_loss:4.4417 train_time:932259ms step_avg:169.97ms\nstep:5496/10000 train_loss:3.9772 train_time:932429ms step_avg:169.97ms\nstep:5497/10000 train_loss:3.9312 train_time:932601ms step_avg:169.97ms\nstep:5498/10000 train_loss:3.8345 train_time:932771ms step_avg:169.97ms\nstep:5499/10000 train_loss:3.9137 train_time:932943ms step_avg:169.97ms\nstep:5500/10000 train_loss:3.9921 train_time:933112ms step_avg:169.97ms\nstep:5501/10000 train_loss:3.8922 train_time:933285ms step_avg:169.97ms\nstep:5502/10000 train_loss:4.0607 train_time:933455ms step_avg:169.97ms\nstep:5503/10000 train_loss:4.0443 train_time:933626ms step_avg:169.97ms\nstep:5504/10000 train_loss:3.9713 train_time:933797ms step_avg:169.97ms\nstep:5505/10000 train_loss:3.8532 train_time:933966ms step_avg:169.97ms\nstep:5506/10000 train_loss:3.9635 train_time:934135ms step_avg:169.97ms\nstep:5507/10000 train_loss:3.9641 train_time:934303ms step_avg:169.97ms\nstep:5508/10000 train_loss:3.8122 train_time:934474ms step_avg:169.97ms\nstep:5509/10000 train_loss:3.9229 train_time:934642ms step_avg:169.97ms\nstep:5510/10000 train_loss:4.0395 train_time:934811ms step_avg:169.97ms\nstep:5511/10000 train_loss:3.9118 train_time:934982ms step_avg:169.97ms\nstep:5512/10000 train_loss:4.2055 train_time:935152ms step_avg:169.97ms\nstep:5513/10000 train_loss:4.0329 train_time:935320ms step_avg:169.97ms\nstep:5514/10000 train_loss:3.8643 train_time:935491ms step_avg:169.97ms\nstep:5515/10000 train_loss:3.9605 train_time:935663ms step_avg:169.97ms\nstep:5516/10000 train_loss:3.9633 train_time:935833ms step_avg:169.97ms\nstep:5517/10000 train_loss:3.9366 train_time:936004ms step_avg:169.97ms\nstep:5518/10000 train_loss:3.9483 train_time:936173ms step_avg:169.97ms\nstep:5519/10000 train_loss:4.1613 train_time:936340ms step_avg:169.97ms\nstep:5520/10000 train_loss:4.0979 train_time:936511ms step_avg:169.97ms\nstep:5521/10000 train_loss:3.9641 train_time:936681ms step_avg:169.97ms\nstep:5522/10000 train_loss:3.8943 train_time:936850ms step_avg:169.97ms\nstep:5523/10000 train_loss:3.8962 train_time:937019ms step_avg:169.97ms\nstep:5524/10000 train_loss:4.1526 train_time:937187ms step_avg:169.97ms\nstep:5525/10000 train_loss:3.8278 train_time:937357ms step_avg:169.97ms\nstep:5526/10000 train_loss:3.9206 train_time:937527ms step_avg:169.96ms\nstep:5527/10000 train_loss:3.9214 train_time:937696ms step_avg:169.96ms\nstep:5528/10000 train_loss:3.8163 train_time:937864ms step_avg:169.96ms\nstep:5529/10000 train_loss:3.8753 train_time:938035ms step_avg:169.96ms\nstep:5530/10000 train_loss:4.1266 train_time:938203ms step_avg:169.96ms\nstep:5531/10000 train_loss:3.7757 train_time:938375ms step_avg:169.96ms\nstep:5532/10000 train_loss:3.9567 train_time:938542ms step_avg:169.96ms\nstep:5533/10000 train_loss:3.8578 train_time:938714ms step_avg:169.96ms\nstep:5534/10000 train_loss:3.8623 train_time:938883ms step_avg:169.96ms\nstep:5535/10000 train_loss:3.7449 train_time:939053ms step_avg:169.96ms\nstep:5536/10000 train_loss:4.4261 train_time:939222ms step_avg:169.96ms\nstep:5537/10000 train_loss:3.9559 train_time:939390ms step_avg:169.96ms\nstep:5538/10000 train_loss:3.8817 train_time:939559ms step_avg:169.96ms\nstep:5539/10000 train_loss:3.9748 train_time:939728ms step_avg:169.96ms\nstep:5540/10000 train_loss:3.8892 train_time:939898ms step_avg:169.96ms\nstep:5541/10000 train_loss:4.0214 train_time:940066ms step_avg:169.96ms\nstep:5542/10000 train_loss:3.9668 train_time:940236ms step_avg:169.96ms\nstep:5543/10000 train_loss:3.8618 train_time:940404ms step_avg:169.96ms\nstep:5544/10000 train_loss:4.0233 train_time:940574ms step_avg:169.96ms\nstep:5545/10000 train_loss:3.7227 train_time:940744ms step_avg:169.96ms\nstep:5546/10000 train_loss:3.9445 train_time:940914ms step_avg:169.96ms\nstep:5547/10000 train_loss:4.2160 train_time:941082ms step_avg:169.96ms\nstep:5548/10000 train_loss:4.0943 train_time:941251ms step_avg:169.96ms\nstep:5549/10000 train_loss:3.8682 train_time:941424ms step_avg:169.96ms\nstep:5550/10000 train_loss:3.8985 train_time:941594ms step_avg:169.96ms\nstep:5551/10000 train_loss:3.9198 train_time:941764ms step_avg:169.96ms\nstep:5552/10000 train_loss:3.9883 train_time:941935ms step_avg:169.96ms\nstep:5553/10000 train_loss:3.9555 train_time:942104ms step_avg:169.96ms\nstep:5554/10000 train_loss:3.8480 train_time:942274ms step_avg:169.96ms\nstep:5555/10000 train_loss:3.8023 train_time:942448ms step_avg:169.96ms\nstep:5556/10000 train_loss:4.0246 train_time:942613ms step_avg:169.96ms\nstep:5557/10000 train_loss:4.0288 train_time:942782ms step_avg:169.96ms\nstep:5558/10000 train_loss:3.9893 train_time:942951ms step_avg:169.96ms\nstep:5559/10000 train_loss:4.0192 train_time:943120ms step_avg:169.96ms\nstep:5560/10000 train_loss:3.8227 train_time:943290ms step_avg:169.96ms\nstep:5561/10000 train_loss:3.9958 train_time:943467ms step_avg:169.96ms\nstep:5562/10000 train_loss:4.0150 train_time:943632ms step_avg:169.96ms\nstep:5563/10000 train_loss:3.8994 train_time:943800ms step_avg:169.96ms\nstep:5564/10000 train_loss:3.9660 train_time:943969ms step_avg:169.96ms\nstep:5565/10000 train_loss:3.8541 train_time:944140ms step_avg:169.96ms\nstep:5566/10000 train_loss:4.0425 train_time:944310ms step_avg:169.96ms\nstep:5567/10000 train_loss:3.9730 train_time:944482ms step_avg:169.96ms\nstep:5568/10000 train_loss:3.9202 train_time:944653ms step_avg:169.96ms\nstep:5569/10000 train_loss:3.8639 train_time:944823ms step_avg:169.96ms\nstep:5570/10000 train_loss:4.1120 train_time:944990ms step_avg:169.96ms\nstep:5571/10000 train_loss:4.0311 train_time:945159ms step_avg:169.96ms\nstep:5572/10000 train_loss:4.2151 train_time:945327ms step_avg:169.96ms\nstep:5573/10000 train_loss:3.9194 train_time:945495ms step_avg:169.96ms\nstep:5574/10000 train_loss:3.9968 train_time:945664ms step_avg:169.96ms\nstep:5575/10000 train_loss:3.8618 train_time:945833ms step_avg:169.96ms\nstep:5576/10000 train_loss:3.9161 train_time:946001ms step_avg:169.96ms\nstep:5577/10000 train_loss:4.0752 train_time:946171ms step_avg:169.96ms\nstep:5578/10000 train_loss:4.0174 train_time:946342ms step_avg:169.96ms\nstep:5579/10000 train_loss:3.9285 train_time:946513ms step_avg:169.96ms\nstep:5580/10000 train_loss:3.9116 train_time:946681ms step_avg:169.96ms\nstep:5581/10000 train_loss:3.8914 train_time:946852ms step_avg:169.96ms\nstep:5582/10000 train_loss:3.9846 train_time:947022ms step_avg:169.96ms\nstep:5583/10000 train_loss:3.9981 train_time:947193ms step_avg:169.96ms\nstep:5584/10000 train_loss:3.9060 train_time:947363ms step_avg:169.96ms\nstep:5585/10000 train_loss:3.9950 train_time:947535ms step_avg:169.96ms\nstep:5586/10000 train_loss:3.7843 train_time:947704ms step_avg:169.96ms\nstep:5587/10000 train_loss:3.9317 train_time:947874ms step_avg:169.96ms\nstep:5588/10000 train_loss:4.1651 train_time:948042ms step_avg:169.96ms\nstep:5589/10000 train_loss:4.0993 train_time:948211ms step_avg:169.96ms\nstep:5590/10000 train_loss:4.0569 train_time:948379ms step_avg:169.96ms\nstep:5591/10000 train_loss:3.9022 train_time:948548ms step_avg:169.96ms\nstep:5592/10000 train_loss:4.0618 train_time:948717ms step_avg:169.96ms\nstep:5593/10000 train_loss:3.9863 train_time:948884ms step_avg:169.96ms\nstep:5594/10000 train_loss:3.9336 train_time:949053ms step_avg:169.96ms\nstep:5595/10000 train_loss:4.0724 train_time:949224ms step_avg:169.96ms\nstep:5596/10000 train_loss:4.0500 train_time:949392ms step_avg:169.96ms\nstep:5597/10000 train_loss:3.9346 train_time:949559ms step_avg:169.96ms\nstep:5598/10000 train_loss:3.8469 train_time:949734ms step_avg:169.96ms\nstep:5599/10000 train_loss:3.9402 train_time:949900ms step_avg:169.96ms\nstep:5600/10000 train_loss:3.9003 train_time:950069ms step_avg:169.96ms\nstep:5601/10000 train_loss:4.0193 train_time:950238ms step_avg:169.96ms\nstep:5602/10000 train_loss:3.7825 train_time:950408ms step_avg:169.96ms\nstep:5603/10000 train_loss:4.0254 train_time:950580ms step_avg:169.96ms\nstep:5604/10000 train_loss:3.9132 train_time:950749ms step_avg:169.96ms\nstep:5605/10000 train_loss:4.0391 train_time:950917ms step_avg:169.96ms\nstep:5606/10000 train_loss:3.7735 train_time:951086ms step_avg:169.96ms\nstep:5607/10000 train_loss:3.9687 train_time:951256ms step_avg:169.96ms\nstep:5608/10000 train_loss:3.9771 train_time:951424ms step_avg:169.96ms\nstep:5609/10000 train_loss:4.0850 train_time:951593ms step_avg:169.96ms\nstep:5610/10000 train_loss:4.1264 train_time:951763ms step_avg:169.96ms\nstep:5611/10000 train_loss:3.7273 train_time:951934ms step_avg:169.96ms\nstep:5612/10000 train_loss:3.9169 train_time:952102ms step_avg:169.96ms\nstep:5613/10000 train_loss:3.9714 train_time:952272ms step_avg:169.96ms\nstep:5614/10000 train_loss:4.1663 train_time:952443ms step_avg:169.96ms\nstep:5615/10000 train_loss:3.9363 train_time:952612ms step_avg:169.96ms\nstep:5616/10000 train_loss:3.9338 train_time:952780ms step_avg:169.96ms\nstep:5617/10000 train_loss:3.7601 train_time:952948ms step_avg:169.96ms\nstep:5618/10000 train_loss:4.0152 train_time:953117ms step_avg:169.96ms\nstep:5619/10000 train_loss:4.0086 train_time:953286ms step_avg:169.96ms\nstep:5620/10000 train_loss:3.9261 train_time:953457ms step_avg:169.96ms\nstep:5621/10000 train_loss:3.9156 train_time:953626ms step_avg:169.96ms\nstep:5622/10000 train_loss:3.9153 train_time:953798ms step_avg:169.96ms\nstep:5623/10000 train_loss:3.9456 train_time:953966ms step_avg:169.96ms\nstep:5624/10000 train_loss:4.1408 train_time:954135ms step_avg:169.96ms\nstep:5625/10000 train_loss:3.9827 train_time:954304ms step_avg:169.96ms\nstep:5626/10000 train_loss:4.0169 train_time:954475ms step_avg:169.96ms\nstep:5627/10000 train_loss:3.9712 train_time:954644ms step_avg:169.96ms\nstep:5628/10000 train_loss:3.8847 train_time:954815ms step_avg:169.96ms\nstep:5629/10000 train_loss:3.9044 train_time:954984ms step_avg:169.96ms\nstep:5630/10000 train_loss:3.8855 train_time:955154ms step_avg:169.96ms\nstep:5631/10000 train_loss:3.9018 train_time:955322ms step_avg:169.96ms\nstep:5632/10000 train_loss:3.9541 train_time:955491ms step_avg:169.96ms\nstep:5633/10000 train_loss:3.9550 train_time:955662ms step_avg:169.96ms\nstep:5634/10000 train_loss:3.7506 train_time:955834ms step_avg:169.96ms\nstep:5635/10000 train_loss:3.9772 train_time:956002ms step_avg:169.96ms\nstep:5636/10000 train_loss:3.9585 train_time:956172ms step_avg:169.96ms\nstep:5637/10000 train_loss:4.1378 train_time:956343ms step_avg:169.96ms\nstep:5638/10000 train_loss:3.8921 train_time:956511ms step_avg:169.96ms\nstep:5639/10000 train_loss:3.8802 train_time:956681ms step_avg:169.96ms\nstep:5640/10000 train_loss:3.8308 train_time:956849ms step_avg:169.96ms\nstep:5641/10000 train_loss:4.0417 train_time:957019ms step_avg:169.96ms\nstep:5642/10000 train_loss:3.9746 train_time:957188ms step_avg:169.96ms\nstep:5643/10000 train_loss:3.9302 train_time:957359ms step_avg:169.96ms\nstep:5644/10000 train_loss:3.9074 train_time:957527ms step_avg:169.96ms\nstep:5645/10000 train_loss:4.0263 train_time:957695ms step_avg:169.95ms\nstep:5646/10000 train_loss:4.2890 train_time:957862ms step_avg:169.95ms\nstep:5647/10000 train_loss:3.9324 train_time:958031ms step_avg:169.95ms\nstep:5648/10000 train_loss:3.8970 train_time:958199ms step_avg:169.95ms\nstep:5649/10000 train_loss:3.9723 train_time:958367ms step_avg:169.95ms\nstep:5650/10000 train_loss:4.0869 train_time:958535ms step_avg:169.95ms\nstep:5651/10000 train_loss:3.9580 train_time:958705ms step_avg:169.95ms\nstep:5652/10000 train_loss:3.8859 train_time:958876ms step_avg:169.95ms\nstep:5653/10000 train_loss:3.9696 train_time:959046ms step_avg:169.95ms\nstep:5654/10000 train_loss:4.0942 train_time:959216ms step_avg:169.95ms\nstep:5655/10000 train_loss:3.9291 train_time:959385ms step_avg:169.95ms\nstep:5656/10000 train_loss:4.0067 train_time:959553ms step_avg:169.95ms\nstep:5657/10000 train_loss:4.0246 train_time:959721ms step_avg:169.95ms\nstep:5658/10000 train_loss:4.0969 train_time:959890ms step_avg:169.95ms\nstep:5659/10000 train_loss:3.9159 train_time:960063ms step_avg:169.95ms\nstep:5660/10000 train_loss:3.8892 train_time:960234ms step_avg:169.95ms\nstep:5661/10000 train_loss:3.9653 train_time:960402ms step_avg:169.95ms\nstep:5662/10000 train_loss:3.8946 train_time:960570ms step_avg:169.95ms\nstep:5663/10000 train_loss:4.0193 train_time:960739ms step_avg:169.95ms\nstep:5664/10000 train_loss:3.9809 train_time:960908ms step_avg:169.95ms\nstep:5665/10000 train_loss:3.9419 train_time:961078ms step_avg:169.95ms\nstep:5666/10000 train_loss:3.9731 train_time:961246ms step_avg:169.95ms\nstep:5667/10000 train_loss:4.0072 train_time:961417ms step_avg:169.95ms\nstep:5668/10000 train_loss:3.9882 train_time:961588ms step_avg:169.95ms\nstep:5669/10000 train_loss:3.7948 train_time:961759ms step_avg:169.95ms\nstep:5670/10000 train_loss:3.7604 train_time:961927ms step_avg:169.95ms\nstep:5671/10000 train_loss:3.9275 train_time:962098ms step_avg:169.95ms\nstep:5672/10000 train_loss:4.0481 train_time:962268ms step_avg:169.95ms\nstep:5673/10000 train_loss:3.9827 train_time:962442ms step_avg:169.95ms\nstep:5674/10000 train_loss:3.4009 train_time:962608ms step_avg:169.95ms\nstep:5675/10000 train_loss:3.9686 train_time:962781ms step_avg:169.95ms\nstep:5676/10000 train_loss:3.8489 train_time:962957ms step_avg:169.95ms\nstep:5677/10000 train_loss:4.0186 train_time:963124ms step_avg:169.95ms\nstep:5678/10000 train_loss:3.9800 train_time:963295ms step_avg:169.95ms\nstep:5679/10000 train_loss:3.9570 train_time:963464ms step_avg:169.95ms\nstep:5680/10000 train_loss:3.9088 train_time:963635ms step_avg:169.95ms\nstep:5681/10000 train_loss:4.1423 train_time:963803ms step_avg:169.95ms\nstep:5682/10000 train_loss:4.0208 train_time:963973ms step_avg:169.95ms\nstep:5683/10000 train_loss:4.0324 train_time:964143ms step_avg:169.95ms\nstep:5684/10000 train_loss:3.9981 train_time:964315ms step_avg:169.95ms\nstep:5685/10000 train_loss:4.0269 train_time:964485ms step_avg:169.95ms\nstep:5686/10000 train_loss:3.9331 train_time:964656ms step_avg:169.95ms\nstep:5687/10000 train_loss:3.8592 train_time:964825ms step_avg:169.95ms\nstep:5688/10000 train_loss:4.0821 train_time:964994ms step_avg:169.95ms\nstep:5689/10000 train_loss:4.1710 train_time:965165ms step_avg:169.95ms\nstep:5690/10000 train_loss:3.9963 train_time:965336ms step_avg:169.95ms\nstep:5691/10000 train_loss:3.9987 train_time:965505ms step_avg:169.95ms\nstep:5692/10000 train_loss:4.0053 train_time:965678ms step_avg:169.95ms\nstep:5693/10000 train_loss:3.8754 train_time:965847ms step_avg:169.95ms\nstep:5694/10000 train_loss:3.9892 train_time:966016ms step_avg:169.95ms\nstep:5695/10000 train_loss:3.9541 train_time:966184ms step_avg:169.95ms\nstep:5696/10000 train_loss:3.9748 train_time:966355ms step_avg:169.95ms\nstep:5697/10000 train_loss:3.9012 train_time:966524ms step_avg:169.95ms\nstep:5698/10000 train_loss:3.8342 train_time:966696ms step_avg:169.95ms\nstep:5699/10000 train_loss:3.9397 train_time:966865ms step_avg:169.95ms\nstep:5700/10000 train_loss:3.9991 train_time:967036ms step_avg:169.95ms\nstep:5701/10000 train_loss:3.9868 train_time:967204ms step_avg:169.95ms\nstep:5702/10000 train_loss:3.8499 train_time:967373ms step_avg:169.95ms\nstep:5703/10000 train_loss:4.1922 train_time:967541ms step_avg:169.95ms\nstep:5704/10000 train_loss:3.9928 train_time:967710ms step_avg:169.95ms\nstep:5705/10000 train_loss:3.9701 train_time:967879ms step_avg:169.95ms\nstep:5706/10000 train_loss:4.0751 train_time:968047ms step_avg:169.95ms\nstep:5707/10000 train_loss:3.8547 train_time:968214ms step_avg:169.95ms\nstep:5708/10000 train_loss:3.9565 train_time:968383ms step_avg:169.95ms\nstep:5709/10000 train_loss:3.8657 train_time:968554ms step_avg:169.95ms\nstep:5710/10000 train_loss:3.7939 train_time:968721ms step_avg:169.95ms\nstep:5711/10000 train_loss:4.1243 train_time:968891ms step_avg:169.95ms\nstep:5712/10000 train_loss:3.8218 train_time:969061ms step_avg:169.95ms\nstep:5713/10000 train_loss:4.0182 train_time:969231ms step_avg:169.95ms\nstep:5714/10000 train_loss:3.9857 train_time:969398ms step_avg:169.95ms\nstep:5715/10000 train_loss:3.9189 train_time:969566ms step_avg:169.95ms\nstep:5716/10000 train_loss:3.9618 train_time:969735ms step_avg:169.95ms\nstep:5717/10000 train_loss:4.0830 train_time:969904ms step_avg:169.95ms\nstep:5718/10000 train_loss:4.0374 train_time:970075ms step_avg:169.95ms\nstep:5719/10000 train_loss:3.8516 train_time:970244ms step_avg:169.95ms\nstep:5720/10000 train_loss:3.8591 train_time:970415ms step_avg:169.95ms\nstep:5721/10000 train_loss:3.9202 train_time:970581ms step_avg:169.95ms\nstep:5722/10000 train_loss:3.9406 train_time:970750ms step_avg:169.95ms\nstep:5723/10000 train_loss:3.9725 train_time:970921ms step_avg:169.95ms\nstep:5724/10000 train_loss:3.9835 train_time:971090ms step_avg:169.95ms\nstep:5725/10000 train_loss:4.0617 train_time:971260ms step_avg:169.95ms\nstep:5726/10000 train_loss:3.9391 train_time:971430ms step_avg:169.95ms\nstep:5727/10000 train_loss:3.8600 train_time:971600ms step_avg:169.95ms\nstep:5728/10000 train_loss:3.9596 train_time:971769ms step_avg:169.95ms\nstep:5729/10000 train_loss:3.9629 train_time:971938ms step_avg:169.95ms\nstep:5730/10000 train_loss:3.8607 train_time:972106ms step_avg:169.95ms\nstep:5731/10000 train_loss:3.9346 train_time:972277ms step_avg:169.95ms\nstep:5732/10000 train_loss:3.8904 train_time:972446ms step_avg:169.95ms\nstep:5733/10000 train_loss:3.8793 train_time:972617ms step_avg:169.95ms\nstep:5734/10000 train_loss:3.9871 train_time:972788ms step_avg:169.95ms\nstep:5735/10000 train_loss:3.8678 train_time:972958ms step_avg:169.95ms\nstep:5736/10000 train_loss:3.9230 train_time:973127ms step_avg:169.95ms\nstep:5737/10000 train_loss:3.9040 train_time:973297ms step_avg:169.95ms\nstep:5738/10000 train_loss:3.8983 train_time:973464ms step_avg:169.95ms\nstep:5739/10000 train_loss:3.9562 train_time:973634ms step_avg:169.95ms\nstep:5740/10000 train_loss:3.9952 train_time:973803ms step_avg:169.95ms\nstep:5741/10000 train_loss:3.9108 train_time:973973ms step_avg:169.95ms\nstep:5742/10000 train_loss:4.3831 train_time:974144ms step_avg:169.95ms\nstep:5743/10000 train_loss:3.8915 train_time:974314ms step_avg:169.95ms\nstep:5744/10000 train_loss:3.8290 train_time:974482ms step_avg:169.95ms\nstep:5745/10000 train_loss:3.9996 train_time:974651ms step_avg:169.95ms\nstep:5746/10000 train_loss:3.7754 train_time:974819ms step_avg:169.95ms\nstep:5747/10000 train_loss:3.9067 train_time:974990ms step_avg:169.95ms\nstep:5748/10000 train_loss:3.9516 train_time:975162ms step_avg:169.95ms\nstep:5749/10000 train_loss:3.9162 train_time:975333ms step_avg:169.95ms\nstep:5750/10000 train_loss:3.8853 train_time:975504ms step_avg:169.95ms\nstep:5751/10000 train_loss:3.8088 train_time:975675ms step_avg:169.95ms\nstep:5752/10000 train_loss:3.8205 train_time:975843ms step_avg:169.95ms\nstep:5753/10000 train_loss:3.9103 train_time:976012ms step_avg:169.95ms\nstep:5754/10000 train_loss:3.8920 train_time:976182ms step_avg:169.95ms\nstep:5755/10000 train_loss:3.7419 train_time:976352ms step_avg:169.95ms\nstep:5756/10000 train_loss:3.8229 train_time:976522ms step_avg:169.95ms\nstep:5757/10000 train_loss:3.7625 train_time:976694ms step_avg:169.95ms\nstep:5758/10000 train_loss:3.9131 train_time:976862ms step_avg:169.95ms\nstep:5759/10000 train_loss:3.8492 train_time:977030ms step_avg:169.95ms\nstep:5760/10000 train_loss:4.0356 train_time:977200ms step_avg:169.95ms\nstep:5761/10000 train_loss:3.9766 train_time:977368ms step_avg:169.95ms\nstep:5762/10000 train_loss:3.8574 train_time:977537ms step_avg:169.95ms\nstep:5763/10000 train_loss:3.9622 train_time:977706ms step_avg:169.95ms\nstep:5764/10000 train_loss:3.9762 train_time:977876ms step_avg:169.95ms\nstep:5765/10000 train_loss:3.9542 train_time:978043ms step_avg:169.95ms\nstep:5766/10000 train_loss:4.0106 train_time:978212ms step_avg:169.95ms\nstep:5767/10000 train_loss:3.9344 train_time:978383ms step_avg:169.95ms\nstep:5768/10000 train_loss:3.9376 train_time:978552ms step_avg:169.95ms\nstep:5769/10000 train_loss:4.0163 train_time:978718ms step_avg:169.95ms\nstep:5770/10000 train_loss:3.8625 train_time:978886ms step_avg:169.95ms\nstep:5771/10000 train_loss:3.8877 train_time:979055ms step_avg:169.95ms\nstep:5772/10000 train_loss:3.7918 train_time:979223ms step_avg:169.95ms\nstep:5773/10000 train_loss:3.8007 train_time:979394ms step_avg:169.95ms\nstep:5774/10000 train_loss:3.9976 train_time:979564ms step_avg:169.95ms\nstep:5775/10000 train_loss:4.0214 train_time:979734ms step_avg:169.95ms\nstep:5776/10000 train_loss:3.9858 train_time:979902ms step_avg:169.94ms\nstep:5777/10000 train_loss:3.8476 train_time:980074ms step_avg:169.95ms\nstep:5778/10000 train_loss:3.9410 train_time:980241ms step_avg:169.94ms\nstep:5779/10000 train_loss:3.9090 train_time:980413ms step_avg:169.95ms\nstep:5780/10000 train_loss:4.0461 train_time:980582ms step_avg:169.94ms\nstep:5781/10000 train_loss:4.0630 train_time:980752ms step_avg:169.94ms\nstep:5782/10000 train_loss:3.9093 train_time:980921ms step_avg:169.94ms\nstep:5783/10000 train_loss:3.8008 train_time:981091ms step_avg:169.94ms\nstep:5784/10000 train_loss:4.0670 train_time:981260ms step_avg:169.94ms\nstep:5785/10000 train_loss:4.0030 train_time:981429ms step_avg:169.94ms\nstep:5786/10000 train_loss:4.1177 train_time:981601ms step_avg:169.94ms\nstep:5787/10000 train_loss:3.9886 train_time:981771ms step_avg:169.94ms\nstep:5788/10000 train_loss:4.0824 train_time:981941ms step_avg:169.94ms\nstep:5789/10000 train_loss:3.8750 train_time:982110ms step_avg:169.94ms\nstep:5790/10000 train_loss:3.8361 train_time:982280ms step_avg:169.94ms\nstep:5791/10000 train_loss:3.9924 train_time:982451ms step_avg:169.94ms\nstep:5792/10000 train_loss:3.9557 train_time:982622ms step_avg:169.95ms\nstep:5793/10000 train_loss:3.9769 train_time:982789ms step_avg:169.94ms\nstep:5794/10000 train_loss:3.7357 train_time:982960ms step_avg:169.94ms\nstep:5795/10000 train_loss:3.9809 train_time:983130ms step_avg:169.94ms\nstep:5796/10000 train_loss:3.8500 train_time:983300ms step_avg:169.94ms\nstep:5797/10000 train_loss:3.9560 train_time:983470ms step_avg:169.94ms\nstep:5798/10000 train_loss:4.0910 train_time:983642ms step_avg:169.95ms\nstep:5799/10000 train_loss:3.9738 train_time:983811ms step_avg:169.94ms\nstep:5800/10000 train_loss:3.8988 train_time:983982ms step_avg:169.95ms\nstep:5801/10000 train_loss:3.9718 train_time:984149ms step_avg:169.94ms\nstep:5802/10000 train_loss:3.9369 train_time:984319ms step_avg:169.94ms\nstep:5803/10000 train_loss:3.7411 train_time:984488ms step_avg:169.94ms\nstep:5804/10000 train_loss:3.9366 train_time:984657ms step_avg:169.94ms\nstep:5805/10000 train_loss:3.9891 train_time:984825ms step_avg:169.94ms\nstep:5806/10000 train_loss:3.9340 train_time:984995ms step_avg:169.94ms\nstep:5807/10000 train_loss:4.0132 train_time:985163ms step_avg:169.94ms\nstep:5808/10000 train_loss:4.0466 train_time:985331ms step_avg:169.94ms\nstep:5809/10000 train_loss:3.9182 train_time:985500ms step_avg:169.94ms\nstep:5810/10000 train_loss:3.8322 train_time:985669ms step_avg:169.94ms\nstep:5811/10000 train_loss:3.7420 train_time:985841ms step_avg:169.94ms\nstep:5812/10000 train_loss:4.0558 train_time:986013ms step_avg:169.94ms\nstep:5813/10000 train_loss:3.9546 train_time:986185ms step_avg:169.94ms\nstep:5814/10000 train_loss:3.7806 train_time:986357ms step_avg:169.94ms\nstep:5815/10000 train_loss:4.0777 train_time:986526ms step_avg:169.94ms\nstep:5816/10000 train_loss:3.8670 train_time:986694ms step_avg:169.94ms\nstep:5817/10000 train_loss:4.0155 train_time:986862ms step_avg:169.94ms\nstep:5818/10000 train_loss:3.9015 train_time:987032ms step_avg:169.94ms\nstep:5819/10000 train_loss:3.7484 train_time:987202ms step_avg:169.94ms\nstep:5820/10000 train_loss:3.9593 train_time:987372ms step_avg:169.94ms\nstep:5821/10000 train_loss:3.9144 train_time:987543ms step_avg:169.94ms\nstep:5822/10000 train_loss:3.9089 train_time:987713ms step_avg:169.94ms\nstep:5823/10000 train_loss:4.0071 train_time:987884ms step_avg:169.94ms\nstep:5824/10000 train_loss:3.8618 train_time:988053ms step_avg:169.94ms\nstep:5825/10000 train_loss:3.9343 train_time:988225ms step_avg:169.94ms\nstep:5826/10000 train_loss:3.7882 train_time:988395ms step_avg:169.94ms\nstep:5827/10000 train_loss:3.9170 train_time:988562ms step_avg:169.94ms\nstep:5828/10000 train_loss:3.9002 train_time:988732ms step_avg:169.94ms\nstep:5829/10000 train_loss:4.0488 train_time:988904ms step_avg:169.94ms\nstep:5830/10000 train_loss:4.0136 train_time:989073ms step_avg:169.94ms\nstep:5831/10000 train_loss:3.9737 train_time:989239ms step_avg:169.94ms\nstep:5832/10000 train_loss:3.9575 train_time:989409ms step_avg:169.94ms\nstep:5833/10000 train_loss:4.0294 train_time:989578ms step_avg:169.94ms\nstep:5834/10000 train_loss:4.0246 train_time:989747ms step_avg:169.94ms\nstep:5835/10000 train_loss:3.8307 train_time:989917ms step_avg:169.94ms\nstep:5836/10000 train_loss:3.8593 train_time:990086ms step_avg:169.94ms\nstep:5837/10000 train_loss:3.8714 train_time:990258ms step_avg:169.94ms\nstep:5838/10000 train_loss:3.8720 train_time:990428ms step_avg:169.94ms\nstep:5839/10000 train_loss:4.0015 train_time:990598ms step_avg:169.94ms\nstep:5840/10000 train_loss:3.7929 train_time:990764ms step_avg:169.94ms\nstep:5841/10000 train_loss:3.8685 train_time:990931ms step_avg:169.94ms\nstep:5842/10000 train_loss:3.9538 train_time:991097ms step_avg:169.94ms\nstep:5843/10000 train_loss:4.0367 train_time:991265ms step_avg:169.94ms\nstep:5844/10000 train_loss:4.0008 train_time:991434ms step_avg:169.94ms\nstep:5845/10000 train_loss:4.3483 train_time:991606ms step_avg:169.94ms\nstep:5846/10000 train_loss:3.8781 train_time:991775ms step_avg:169.94ms\nstep:5847/10000 train_loss:3.9049 train_time:991947ms step_avg:169.94ms\nstep:5848/10000 train_loss:3.9066 train_time:992114ms step_avg:169.94ms\nstep:5849/10000 train_loss:3.8669 train_time:992283ms step_avg:169.94ms\nstep:5850/10000 train_loss:3.9494 train_time:992457ms step_avg:169.94ms\nstep:5851/10000 train_loss:3.9446 train_time:992626ms step_avg:169.94ms\nstep:5852/10000 train_loss:3.9393 train_time:992796ms step_avg:169.94ms\nstep:5853/10000 train_loss:3.8220 train_time:992967ms step_avg:169.94ms\nstep:5854/10000 train_loss:3.8934 train_time:993135ms step_avg:169.94ms\nstep:5855/10000 train_loss:3.8610 train_time:993302ms step_avg:169.94ms\nstep:5856/10000 train_loss:4.0865 train_time:993471ms step_avg:169.94ms\nstep:5857/10000 train_loss:3.8684 train_time:993638ms step_avg:169.94ms\nstep:5858/10000 train_loss:3.8904 train_time:993810ms step_avg:169.94ms\nstep:5859/10000 train_loss:3.9583 train_time:993980ms step_avg:169.94ms\nstep:5860/10000 train_loss:3.9029 train_time:994148ms step_avg:169.94ms\nstep:5861/10000 train_loss:3.8240 train_time:994314ms step_avg:169.94ms\nstep:5862/10000 train_loss:3.8206 train_time:994482ms step_avg:169.94ms\nstep:5863/10000 train_loss:3.8293 train_time:994652ms step_avg:169.94ms\nstep:5864/10000 train_loss:4.0524 train_time:994820ms step_avg:169.94ms\nstep:5865/10000 train_loss:3.9144 train_time:994989ms step_avg:169.94ms\nstep:5866/10000 train_loss:4.0254 train_time:995158ms step_avg:169.94ms\nstep:5867/10000 train_loss:4.0522 train_time:995327ms step_avg:169.94ms\nstep:5868/10000 train_loss:3.9534 train_time:995497ms step_avg:169.94ms\nstep:5869/10000 train_loss:3.9675 train_time:995666ms step_avg:169.94ms\nstep:5870/10000 train_loss:3.9828 train_time:995836ms step_avg:169.94ms\nstep:5871/10000 train_loss:3.9337 train_time:996004ms step_avg:169.94ms\nstep:5872/10000 train_loss:3.9748 train_time:996178ms step_avg:169.94ms\nstep:5873/10000 train_loss:4.0396 train_time:996345ms step_avg:169.94ms\nstep:5874/10000 train_loss:3.8748 train_time:996511ms step_avg:169.94ms\nstep:5875/10000 train_loss:3.9125 train_time:996681ms step_avg:169.94ms\nstep:5876/10000 train_loss:3.8944 train_time:996851ms step_avg:169.94ms\nstep:5877/10000 train_loss:3.9556 train_time:997019ms step_avg:169.94ms\nstep:5878/10000 train_loss:3.9004 train_time:997187ms step_avg:169.94ms\nstep:5879/10000 train_loss:3.9839 train_time:997358ms step_avg:169.94ms\nstep:5880/10000 train_loss:4.0741 train_time:997529ms step_avg:169.94ms\nstep:5881/10000 train_loss:3.8825 train_time:997701ms step_avg:169.94ms\nstep:5882/10000 train_loss:3.9527 train_time:997873ms step_avg:169.94ms\nstep:5883/10000 train_loss:3.8744 train_time:998042ms step_avg:169.94ms\nstep:5884/10000 train_loss:3.9118 train_time:998208ms step_avg:169.94ms\nstep:5885/10000 train_loss:3.9715 train_time:998378ms step_avg:169.94ms\nstep:5886/10000 train_loss:3.8051 train_time:998547ms step_avg:169.94ms\nstep:5887/10000 train_loss:3.9087 train_time:998717ms step_avg:169.94ms\nstep:5888/10000 train_loss:3.8630 train_time:998885ms step_avg:169.94ms\nstep:5889/10000 train_loss:3.8685 train_time:999056ms step_avg:169.94ms\nstep:5890/10000 train_loss:3.8106 train_time:999224ms step_avg:169.94ms\nstep:5891/10000 train_loss:3.9514 train_time:999393ms step_avg:169.94ms\nstep:5892/10000 train_loss:3.9321 train_time:999562ms step_avg:169.94ms\nstep:5893/10000 train_loss:3.8903 train_time:999732ms step_avg:169.94ms\nstep:5894/10000 train_loss:3.9860 train_time:999899ms step_avg:169.94ms\nstep:5895/10000 train_loss:3.7021 train_time:1000068ms step_avg:169.94ms\nstep:5896/10000 train_loss:3.8571 train_time:1000235ms step_avg:169.93ms\nstep:5897/10000 train_loss:3.9520 train_time:1000403ms step_avg:169.93ms\nstep:5898/10000 train_loss:3.8953 train_time:1000575ms step_avg:169.93ms\nstep:5899/10000 train_loss:3.8975 train_time:1000744ms step_avg:169.93ms\nstep:5900/10000 train_loss:3.9937 train_time:1000916ms step_avg:169.93ms\nstep:5901/10000 train_loss:3.8984 train_time:1001084ms step_avg:169.93ms\nstep:5902/10000 train_loss:4.0209 train_time:1001254ms step_avg:169.93ms\nstep:5903/10000 train_loss:3.9816 train_time:1001423ms step_avg:169.93ms\nstep:5904/10000 train_loss:4.0327 train_time:1001593ms step_avg:169.93ms\nstep:5905/10000 train_loss:3.9501 train_time:1001763ms step_avg:169.93ms\nstep:5906/10000 train_loss:3.8502 train_time:1001934ms step_avg:169.93ms\nstep:5907/10000 train_loss:3.8785 train_time:1002103ms step_avg:169.93ms\nstep:5908/10000 train_loss:3.9344 train_time:1002274ms step_avg:169.93ms\nstep:5909/10000 train_loss:3.8840 train_time:1002443ms step_avg:169.93ms\nstep:5910/10000 train_loss:3.9471 train_time:1002614ms step_avg:169.93ms\nstep:5911/10000 train_loss:3.8973 train_time:1002784ms step_avg:169.93ms\nstep:5912/10000 train_loss:3.7416 train_time:1002956ms step_avg:169.93ms\nstep:5913/10000 train_loss:3.7451 train_time:1003123ms step_avg:169.93ms\nstep:5914/10000 train_loss:4.0641 train_time:1003292ms step_avg:169.93ms\nstep:5915/10000 train_loss:3.8896 train_time:1003459ms step_avg:169.93ms\nstep:5916/10000 train_loss:3.8595 train_time:1003630ms step_avg:169.93ms\nstep:5917/10000 train_loss:3.8336 train_time:1003800ms step_avg:169.93ms\nstep:5918/10000 train_loss:3.8661 train_time:1003970ms step_avg:169.93ms\nstep:5919/10000 train_loss:4.0473 train_time:1004141ms step_avg:169.93ms\nstep:5920/10000 train_loss:3.8373 train_time:1004310ms step_avg:169.93ms\nstep:5921/10000 train_loss:3.6927 train_time:1004479ms step_avg:169.93ms\nstep:5922/10000 train_loss:3.8417 train_time:1004648ms step_avg:169.93ms\nstep:5923/10000 train_loss:3.8174 train_time:1004817ms step_avg:169.93ms\nstep:5924/10000 train_loss:3.9555 train_time:1004986ms step_avg:169.93ms\nstep:5925/10000 train_loss:3.9328 train_time:1005158ms step_avg:169.93ms\nstep:5926/10000 train_loss:3.8909 train_time:1005327ms step_avg:169.93ms\nstep:5927/10000 train_loss:4.0035 train_time:1005498ms step_avg:169.93ms\nstep:5928/10000 train_loss:4.0130 train_time:1005667ms step_avg:169.93ms\nstep:5929/10000 train_loss:3.8632 train_time:1005837ms step_avg:169.93ms\nstep:5930/10000 train_loss:3.8929 train_time:1006005ms step_avg:169.93ms\nstep:5931/10000 train_loss:3.8301 train_time:1006177ms step_avg:169.93ms\nstep:5932/10000 train_loss:3.8932 train_time:1006345ms step_avg:169.93ms\nstep:5933/10000 train_loss:3.9028 train_time:1006515ms step_avg:169.93ms\nstep:5934/10000 train_loss:4.0648 train_time:1006681ms step_avg:169.93ms\nstep:5935/10000 train_loss:4.0028 train_time:1006853ms step_avg:169.93ms\nstep:5936/10000 train_loss:4.0602 train_time:1007023ms step_avg:169.93ms\nstep:5937/10000 train_loss:3.8929 train_time:1007195ms step_avg:169.93ms\nstep:5938/10000 train_loss:3.7768 train_time:1007364ms step_avg:169.93ms\nstep:5939/10000 train_loss:3.9395 train_time:1007536ms step_avg:169.93ms\nstep:5940/10000 train_loss:4.0051 train_time:1007702ms step_avg:169.93ms\nstep:5941/10000 train_loss:3.8544 train_time:1007872ms step_avg:169.93ms\nstep:5942/10000 train_loss:3.9175 train_time:1008043ms step_avg:169.93ms\nstep:5943/10000 train_loss:3.9299 train_time:1008215ms step_avg:169.93ms\nstep:5944/10000 train_loss:3.8951 train_time:1008386ms step_avg:169.93ms\nstep:5945/10000 train_loss:3.9910 train_time:1008555ms step_avg:169.93ms\nstep:5946/10000 train_loss:4.0122 train_time:1008723ms step_avg:169.93ms\nstep:5947/10000 train_loss:3.8420 train_time:1008893ms step_avg:169.93ms\nstep:5948/10000 train_loss:3.8836 train_time:1009060ms step_avg:169.93ms\nstep:5949/10000 train_loss:4.0032 train_time:1009230ms step_avg:169.93ms\nstep:5950/10000 train_loss:3.9025 train_time:1009400ms step_avg:169.93ms\nstep:5951/10000 train_loss:3.9608 train_time:1009569ms step_avg:169.93ms\nstep:5952/10000 train_loss:3.8507 train_time:1009737ms step_avg:169.93ms\nstep:5953/10000 train_loss:3.9457 train_time:1009905ms step_avg:169.93ms\nstep:5954/10000 train_loss:3.7975 train_time:1010076ms step_avg:169.93ms\nstep:5955/10000 train_loss:3.9995 train_time:1010246ms step_avg:169.93ms\nstep:5956/10000 train_loss:3.8535 train_time:1010417ms step_avg:169.93ms\nstep:5957/10000 train_loss:3.9174 train_time:1010585ms step_avg:169.93ms\nstep:5958/10000 train_loss:3.9179 train_time:1010762ms step_avg:169.93ms\nstep:5959/10000 train_loss:3.9626 train_time:1010930ms step_avg:169.93ms\nstep:5960/10000 train_loss:3.9152 train_time:1011098ms step_avg:169.93ms\nstep:5961/10000 train_loss:4.0870 train_time:1011264ms step_avg:169.93ms\nstep:5962/10000 train_loss:3.7254 train_time:1011436ms step_avg:169.93ms\nstep:5963/10000 train_loss:3.9509 train_time:1011603ms step_avg:169.93ms\nstep:5964/10000 train_loss:3.8228 train_time:1011773ms step_avg:169.93ms\nstep:5965/10000 train_loss:3.9850 train_time:1011941ms step_avg:169.93ms\nstep:5966/10000 train_loss:3.6556 train_time:1012111ms step_avg:169.93ms\nstep:5967/10000 train_loss:4.3467 train_time:1012279ms step_avg:169.93ms\nstep:5968/10000 train_loss:3.9542 train_time:1012450ms step_avg:169.93ms\nstep:5969/10000 train_loss:3.9397 train_time:1012618ms step_avg:169.93ms\nstep:5970/10000 train_loss:3.7674 train_time:1012785ms step_avg:169.93ms\nstep:5971/10000 train_loss:4.0844 train_time:1012955ms step_avg:169.93ms\nstep:5972/10000 train_loss:3.8784 train_time:1013121ms step_avg:169.93ms\nstep:5973/10000 train_loss:3.8471 train_time:1013291ms step_avg:169.93ms\nstep:5974/10000 train_loss:4.1214 train_time:1013461ms step_avg:169.93ms\nstep:5975/10000 train_loss:4.1158 train_time:1013633ms step_avg:169.93ms\nstep:5976/10000 train_loss:3.8285 train_time:1013799ms step_avg:169.93ms\nstep:5977/10000 train_loss:4.0262 train_time:1013967ms step_avg:169.93ms\nstep:5978/10000 train_loss:3.9811 train_time:1014134ms step_avg:169.93ms\nstep:5979/10000 train_loss:3.9686 train_time:1014303ms step_avg:169.93ms\nstep:5980/10000 train_loss:3.8598 train_time:1014474ms step_avg:169.93ms\nstep:5981/10000 train_loss:3.7575 train_time:1014643ms step_avg:169.93ms\nstep:5982/10000 train_loss:3.8789 train_time:1014814ms step_avg:169.93ms\nstep:5983/10000 train_loss:3.9403 train_time:1014981ms step_avg:169.93ms\nstep:5984/10000 train_loss:4.0618 train_time:1015150ms step_avg:169.93ms\nstep:5985/10000 train_loss:3.9050 train_time:1015317ms step_avg:169.93ms\nstep:5986/10000 train_loss:4.0990 train_time:1015486ms step_avg:169.93ms\nstep:5987/10000 train_loss:3.9139 train_time:1015662ms step_avg:169.93ms\nstep:5988/10000 train_loss:3.9162 train_time:1015824ms step_avg:169.93ms\nstep:5989/10000 train_loss:3.8201 train_time:1015993ms step_avg:169.93ms\nstep:5990/10000 train_loss:3.9471 train_time:1016160ms step_avg:169.93ms\nstep:5991/10000 train_loss:3.9172 train_time:1016327ms step_avg:169.93ms\nstep:5992/10000 train_loss:3.6666 train_time:1016494ms step_avg:169.93ms\nstep:5993/10000 train_loss:4.0207 train_time:1016663ms step_avg:169.93ms\nstep:5994/10000 train_loss:3.9110 train_time:1016833ms step_avg:169.93ms\nstep:5995/10000 train_loss:3.8498 train_time:1017001ms step_avg:169.93ms\nstep:5996/10000 train_loss:3.9532 train_time:1017172ms step_avg:169.93ms\nstep:5997/10000 train_loss:3.8496 train_time:1017342ms step_avg:169.93ms\nstep:5998/10000 train_loss:3.8676 train_time:1017512ms step_avg:169.93ms\nstep:5999/10000 train_loss:3.9747 train_time:1017678ms step_avg:169.92ms\nstep:6000/10000 train_loss:3.9204 train_time:1017846ms step_avg:169.92ms\nstep:6001/10000 train_loss:4.0538 train_time:1018016ms step_avg:169.92ms\nstep:6002/10000 train_loss:3.9550 train_time:1018186ms step_avg:169.92ms\nstep:6003/10000 train_loss:3.9349 train_time:1018357ms step_avg:169.92ms\nstep:6004/10000 train_loss:3.9501 train_time:1018525ms step_avg:169.92ms\nstep:6005/10000 train_loss:3.7612 train_time:1018694ms step_avg:169.92ms\nstep:6006/10000 train_loss:3.9149 train_time:1018864ms step_avg:169.92ms\nstep:6007/10000 train_loss:3.8640 train_time:1019035ms step_avg:169.92ms\nstep:6008/10000 train_loss:3.9425 train_time:1019202ms step_avg:169.92ms\nstep:6009/10000 train_loss:3.8334 train_time:1019371ms step_avg:169.92ms\nstep:6010/10000 train_loss:3.8828 train_time:1019540ms step_avg:169.92ms\nstep:6011/10000 train_loss:3.9068 train_time:1019708ms step_avg:169.92ms\nstep:6012/10000 train_loss:3.8791 train_time:1019876ms step_avg:169.92ms\nstep:6013/10000 train_loss:4.0477 train_time:1020045ms step_avg:169.92ms\nstep:6014/10000 train_loss:3.8421 train_time:1020215ms step_avg:169.92ms\nstep:6015/10000 train_loss:3.9714 train_time:1020383ms step_avg:169.92ms\nstep:6016/10000 train_loss:3.7112 train_time:1020553ms step_avg:169.92ms\nstep:6017/10000 train_loss:3.5770 train_time:1020722ms step_avg:169.92ms\nstep:6018/10000 train_loss:3.8247 train_time:1020892ms step_avg:169.92ms\nstep:6019/10000 train_loss:4.1731 train_time:1021063ms step_avg:169.92ms\nstep:6020/10000 train_loss:3.9699 train_time:1021233ms step_avg:169.92ms\nstep:6021/10000 train_loss:3.8175 train_time:1021403ms step_avg:169.92ms\nstep:6022/10000 train_loss:3.9135 train_time:1021570ms step_avg:169.92ms\nstep:6023/10000 train_loss:3.9450 train_time:1021739ms step_avg:169.92ms\nstep:6024/10000 train_loss:3.8588 train_time:1021906ms step_avg:169.92ms\nstep:6025/10000 train_loss:3.9199 train_time:1022075ms step_avg:169.92ms\nstep:6026/10000 train_loss:3.9562 train_time:1022244ms step_avg:169.92ms\nstep:6027/10000 train_loss:3.6103 train_time:1022413ms step_avg:169.92ms\nstep:6028/10000 train_loss:3.8744 train_time:1022581ms step_avg:169.92ms\nstep:6029/10000 train_loss:3.8734 train_time:1022750ms step_avg:169.92ms\nstep:6030/10000 train_loss:3.9079 train_time:1022919ms step_avg:169.92ms\nstep:6031/10000 train_loss:3.7948 train_time:1023089ms step_avg:169.92ms\nstep:6032/10000 train_loss:3.8368 train_time:1023260ms step_avg:169.92ms\nstep:6033/10000 train_loss:3.8826 train_time:1023430ms step_avg:169.92ms\nstep:6034/10000 train_loss:3.8350 train_time:1023599ms step_avg:169.92ms\nstep:6035/10000 train_loss:3.8127 train_time:1023768ms step_avg:169.92ms\nstep:6036/10000 train_loss:3.9147 train_time:1023939ms step_avg:169.92ms\nstep:6037/10000 train_loss:4.0963 train_time:1024107ms step_avg:169.92ms\nstep:6038/10000 train_loss:3.8467 train_time:1024278ms step_avg:169.92ms\nstep:6039/10000 train_loss:3.8585 train_time:1024446ms step_avg:169.92ms\nstep:6040/10000 train_loss:3.8630 train_time:1024614ms step_avg:169.92ms\nstep:6041/10000 train_loss:3.8413 train_time:1024783ms step_avg:169.92ms\nstep:6042/10000 train_loss:3.8841 train_time:1024952ms step_avg:169.92ms\nstep:6043/10000 train_loss:3.8225 train_time:1025122ms step_avg:169.92ms\nstep:6044/10000 train_loss:4.1436 train_time:1025290ms step_avg:169.92ms\nstep:6045/10000 train_loss:3.9840 train_time:1025459ms step_avg:169.92ms\nstep:6046/10000 train_loss:3.8845 train_time:1025628ms step_avg:169.92ms\nstep:6047/10000 train_loss:3.8303 train_time:1025799ms step_avg:169.92ms\nstep:6048/10000 train_loss:3.8701 train_time:1025967ms step_avg:169.92ms\nstep:6049/10000 train_loss:4.0385 train_time:1026138ms step_avg:169.92ms\nstep:6050/10000 train_loss:3.9368 train_time:1026305ms step_avg:169.92ms\nstep:6051/10000 train_loss:3.8337 train_time:1026474ms step_avg:169.92ms\nstep:6052/10000 train_loss:3.8979 train_time:1026643ms step_avg:169.92ms\nstep:6053/10000 train_loss:3.8689 train_time:1026812ms step_avg:169.92ms\nstep:6054/10000 train_loss:3.8240 train_time:1026981ms step_avg:169.92ms\nstep:6055/10000 train_loss:3.9899 train_time:1027153ms step_avg:169.92ms\nstep:6056/10000 train_loss:3.9865 train_time:1027323ms step_avg:169.92ms\nstep:6057/10000 train_loss:3.8789 train_time:1027492ms step_avg:169.92ms\nstep:6058/10000 train_loss:3.9840 train_time:1027660ms step_avg:169.92ms\nstep:6059/10000 train_loss:3.9534 train_time:1027831ms step_avg:169.92ms\nstep:6060/10000 train_loss:4.0304 train_time:1028001ms step_avg:169.92ms\nstep:6061/10000 train_loss:3.9839 train_time:1028170ms step_avg:169.92ms\nstep:6062/10000 train_loss:3.8589 train_time:1028341ms step_avg:169.92ms\nstep:6063/10000 train_loss:3.8555 train_time:1028508ms step_avg:169.92ms\nstep:6064/10000 train_loss:3.8971 train_time:1028677ms step_avg:169.92ms\nstep:6065/10000 train_loss:3.9132 train_time:1028846ms step_avg:169.92ms\nstep:6066/10000 train_loss:3.9037 train_time:1029016ms step_avg:169.92ms\nstep:6067/10000 train_loss:3.9392 train_time:1029184ms step_avg:169.92ms\nstep:6068/10000 train_loss:3.7172 train_time:1029356ms step_avg:169.92ms\nstep:6069/10000 train_loss:3.9961 train_time:1029526ms step_avg:169.92ms\nstep:6070/10000 train_loss:3.8954 train_time:1029695ms step_avg:169.92ms\nstep:6071/10000 train_loss:3.9928 train_time:1029864ms step_avg:169.92ms\nstep:6072/10000 train_loss:3.8481 train_time:1030035ms step_avg:169.92ms\nstep:6073/10000 train_loss:4.0234 train_time:1030204ms step_avg:169.92ms\nstep:6074/10000 train_loss:3.8668 train_time:1030374ms step_avg:169.92ms\nstep:6075/10000 train_loss:3.8856 train_time:1030542ms step_avg:169.92ms\nstep:6076/10000 train_loss:3.8696 train_time:1030713ms step_avg:169.92ms\nstep:6077/10000 train_loss:4.0855 train_time:1030882ms step_avg:169.92ms\nstep:6078/10000 train_loss:3.9558 train_time:1031051ms step_avg:169.92ms\nstep:6079/10000 train_loss:3.9078 train_time:1031220ms step_avg:169.92ms\nstep:6080/10000 train_loss:3.9669 train_time:1031390ms step_avg:169.92ms\nstep:6081/10000 train_loss:3.9479 train_time:1031561ms step_avg:169.92ms\nstep:6082/10000 train_loss:3.8219 train_time:1031732ms step_avg:169.92ms\nstep:6083/10000 train_loss:3.9431 train_time:1031903ms step_avg:169.92ms\nstep:6084/10000 train_loss:3.8588 train_time:1032073ms step_avg:169.92ms\nstep:6085/10000 train_loss:3.9063 train_time:1032241ms step_avg:169.92ms\nstep:6086/10000 train_loss:3.9942 train_time:1032416ms step_avg:169.92ms\nstep:6087/10000 train_loss:3.8625 train_time:1032578ms step_avg:169.92ms\nstep:6088/10000 train_loss:4.1156 train_time:1032748ms step_avg:169.92ms\nstep:6089/10000 train_loss:4.0506 train_time:1032917ms step_avg:169.92ms\nstep:6090/10000 train_loss:3.9931 train_time:1033085ms step_avg:169.92ms\nstep:6091/10000 train_loss:4.0051 train_time:1033261ms step_avg:169.92ms\nstep:6092/10000 train_loss:3.8338 train_time:1033427ms step_avg:169.92ms\nstep:6093/10000 train_loss:3.8607 train_time:1033604ms step_avg:169.92ms\nstep:6094/10000 train_loss:3.9317 train_time:1033765ms step_avg:169.92ms\nstep:6095/10000 train_loss:3.8901 train_time:1033938ms step_avg:169.92ms\nstep:6096/10000 train_loss:3.8903 train_time:1034319ms step_avg:169.95ms\nstep:6097/10000 train_loss:3.8294 train_time:1034483ms step_avg:169.95ms\nstep:6098/10000 train_loss:3.8840 train_time:1034653ms step_avg:169.95ms\nstep:6099/10000 train_loss:3.9562 train_time:1034822ms step_avg:169.95ms\nstep:6100/10000 train_loss:3.9076 train_time:1034990ms step_avg:169.95ms\nstep:6101/10000 train_loss:3.8922 train_time:1035158ms step_avg:169.95ms\nstep:6102/10000 train_loss:3.9884 train_time:1035338ms step_avg:169.95ms\nstep:6103/10000 train_loss:3.8091 train_time:1035505ms step_avg:169.95ms\nstep:6104/10000 train_loss:3.9108 train_time:1035675ms step_avg:169.95ms\nstep:6105/10000 train_loss:3.8764 train_time:1035844ms step_avg:169.95ms\nstep:6106/10000 train_loss:3.8624 train_time:1036012ms step_avg:169.95ms\nstep:6107/10000 train_loss:3.7818 train_time:1036181ms step_avg:169.95ms\nstep:6108/10000 train_loss:3.9832 train_time:1036353ms step_avg:169.95ms\nstep:6109/10000 train_loss:4.0263 train_time:1036524ms step_avg:169.95ms\nstep:6110/10000 train_loss:3.9367 train_time:1036694ms step_avg:169.95ms\nstep:6111/10000 train_loss:3.9759 train_time:1036862ms step_avg:169.95ms\nstep:6112/10000 train_loss:3.8611 train_time:1037030ms step_avg:169.95ms\nstep:6113/10000 train_loss:3.9187 train_time:1037201ms step_avg:169.95ms\nstep:6114/10000 train_loss:3.9314 train_time:1037373ms step_avg:169.95ms\nstep:6115/10000 train_loss:3.9387 train_time:1037542ms step_avg:169.95ms\nstep:6116/10000 train_loss:3.8679 train_time:1037713ms step_avg:169.95ms\nstep:6117/10000 train_loss:3.9272 train_time:1037881ms step_avg:169.95ms\nstep:6118/10000 train_loss:3.9106 train_time:1038051ms step_avg:169.95ms\nstep:6119/10000 train_loss:3.9808 train_time:1038223ms step_avg:169.95ms\nstep:6120/10000 train_loss:3.8653 train_time:1038394ms step_avg:169.95ms\nstep:6121/10000 train_loss:3.9219 train_time:1038563ms step_avg:169.95ms\nstep:6122/10000 train_loss:3.9002 train_time:1038734ms step_avg:169.95ms\nstep:6123/10000 train_loss:3.8777 train_time:1038902ms step_avg:169.95ms\nstep:6124/10000 train_loss:3.9034 train_time:1039071ms step_avg:169.95ms\nstep:6125/10000 train_loss:3.9122 train_time:1039238ms step_avg:169.95ms\nstep:6126/10000 train_loss:4.0441 train_time:1039406ms step_avg:169.95ms\nstep:6127/10000 train_loss:4.0538 train_time:1039578ms step_avg:169.95ms\nstep:6128/10000 train_loss:3.8717 train_time:1039747ms step_avg:169.95ms\nstep:6129/10000 train_loss:3.8675 train_time:1039916ms step_avg:169.95ms\nstep:6130/10000 train_loss:4.0246 train_time:1040083ms step_avg:169.95ms\nstep:6131/10000 train_loss:3.9679 train_time:1040255ms step_avg:169.95ms\nstep:6132/10000 train_loss:3.8641 train_time:1040423ms step_avg:169.95ms\nstep:6133/10000 train_loss:3.9046 train_time:1040594ms step_avg:169.95ms\nstep:6134/10000 train_loss:3.8534 train_time:1040763ms step_avg:169.95ms\nstep:6135/10000 train_loss:3.8721 train_time:1040933ms step_avg:169.95ms\nstep:6136/10000 train_loss:4.0498 train_time:1041101ms step_avg:169.95ms\nstep:6137/10000 train_loss:3.9031 train_time:1041271ms step_avg:169.95ms\nstep:6138/10000 train_loss:3.8385 train_time:1041439ms step_avg:169.95ms\nstep:6139/10000 train_loss:4.0304 train_time:1041610ms step_avg:169.95ms\nstep:6140/10000 train_loss:3.8827 train_time:1041778ms step_avg:169.95ms\nstep:6141/10000 train_loss:3.9734 train_time:1041946ms step_avg:169.95ms\nstep:6142/10000 train_loss:3.9870 train_time:1042114ms step_avg:169.95ms\nstep:6143/10000 train_loss:3.8501 train_time:1042284ms step_avg:169.95ms\nstep:6144/10000 train_loss:3.9444 train_time:1042456ms step_avg:169.95ms\nstep:6145/10000 train_loss:3.8587 train_time:1042624ms step_avg:169.95ms\nstep:6146/10000 train_loss:3.9251 train_time:1042795ms step_avg:169.95ms\nstep:6147/10000 train_loss:3.8499 train_time:1042964ms step_avg:169.95ms\nstep:6148/10000 train_loss:3.8413 train_time:1043134ms step_avg:169.95ms\nstep:6149/10000 train_loss:3.9649 train_time:1043304ms step_avg:169.95ms\nstep:6150/10000 train_loss:3.9379 train_time:1043474ms step_avg:169.95ms\nstep:6151/10000 train_loss:3.9108 train_time:1043644ms step_avg:169.95ms\nstep:6152/10000 train_loss:3.7852 train_time:1043814ms step_avg:169.95ms\nstep:6153/10000 train_loss:3.8298 train_time:1043984ms step_avg:169.95ms\nstep:6154/10000 train_loss:3.8981 train_time:1044155ms step_avg:169.95ms\nstep:6155/10000 train_loss:3.8982 train_time:1044323ms step_avg:169.95ms\nstep:6156/10000 train_loss:3.6891 train_time:1044500ms step_avg:169.95ms\nstep:6157/10000 train_loss:4.1282 train_time:1044664ms step_avg:169.95ms\nstep:6158/10000 train_loss:3.8323 train_time:1044834ms step_avg:169.95ms\nstep:6159/10000 train_loss:3.9380 train_time:1045004ms step_avg:169.95ms\nstep:6160/10000 train_loss:3.8278 train_time:1045173ms step_avg:169.95ms\nstep:6161/10000 train_loss:3.7821 train_time:1045340ms step_avg:169.95ms\nstep:6162/10000 train_loss:3.9265 train_time:1045511ms step_avg:169.95ms\nstep:6163/10000 train_loss:3.9537 train_time:1045681ms step_avg:169.95ms\nstep:6164/10000 train_loss:3.8290 train_time:1045850ms step_avg:169.95ms\nstep:6165/10000 train_loss:3.9300 train_time:1046021ms step_avg:169.95ms\nstep:6166/10000 train_loss:4.0759 train_time:1046189ms step_avg:169.95ms\nstep:6167/10000 train_loss:3.9228 train_time:1046360ms step_avg:169.95ms\nstep:6168/10000 train_loss:3.7402 train_time:1046528ms step_avg:169.95ms\nstep:6169/10000 train_loss:3.8885 train_time:1046699ms step_avg:169.95ms\nstep:6170/10000 train_loss:3.9889 train_time:1046868ms step_avg:169.95ms\nstep:6171/10000 train_loss:4.3045 train_time:1047040ms step_avg:169.95ms\nstep:6172/10000 train_loss:4.0437 train_time:1047209ms step_avg:169.95ms\nstep:6173/10000 train_loss:3.7808 train_time:1047380ms step_avg:169.95ms\nstep:6174/10000 train_loss:3.9103 train_time:1047549ms step_avg:169.95ms\nstep:6175/10000 train_loss:3.8536 train_time:1047720ms step_avg:169.95ms\nstep:6176/10000 train_loss:3.8181 train_time:1047889ms step_avg:169.95ms\nstep:6177/10000 train_loss:3.8682 train_time:1048057ms step_avg:169.95ms\nstep:6178/10000 train_loss:3.8883 train_time:1048226ms step_avg:169.95ms\nstep:6179/10000 train_loss:4.0026 train_time:1048395ms step_avg:169.95ms\nstep:6180/10000 train_loss:3.6684 train_time:1048565ms step_avg:169.95ms\nstep:6181/10000 train_loss:4.0369 train_time:1048734ms step_avg:169.95ms\nstep:6182/10000 train_loss:3.8475 train_time:1048901ms step_avg:169.95ms\nstep:6183/10000 train_loss:3.9713 train_time:1049070ms step_avg:169.94ms\nstep:6184/10000 train_loss:3.8952 train_time:1049240ms step_avg:169.94ms\nstep:6185/10000 train_loss:3.7882 train_time:1049409ms step_avg:169.94ms\nstep:6186/10000 train_loss:3.8534 train_time:1049579ms step_avg:169.94ms\nstep:6187/10000 train_loss:3.9889 train_time:1049748ms step_avg:169.94ms\nstep:6188/10000 train_loss:3.9159 train_time:1049923ms step_avg:169.95ms\nstep:6189/10000 train_loss:3.8975 train_time:1050087ms step_avg:169.94ms\nstep:6190/10000 train_loss:3.9379 train_time:1050260ms step_avg:169.94ms\nstep:6191/10000 train_loss:3.9816 train_time:1050429ms step_avg:169.94ms\nstep:6192/10000 train_loss:3.6869 train_time:1050597ms step_avg:169.94ms\nstep:6193/10000 train_loss:3.8836 train_time:1050766ms step_avg:169.94ms\nstep:6194/10000 train_loss:3.8985 train_time:1050936ms step_avg:169.94ms\nstep:6195/10000 train_loss:3.9603 train_time:1051105ms step_avg:169.94ms\nstep:6196/10000 train_loss:4.1010 train_time:1051275ms step_avg:169.94ms\nstep:6197/10000 train_loss:4.1254 train_time:1051444ms step_avg:169.94ms\nstep:6198/10000 train_loss:3.8525 train_time:1051614ms step_avg:169.94ms\nstep:6199/10000 train_loss:3.8480 train_time:1051782ms step_avg:169.94ms\nstep:6200/10000 train_loss:4.0145 train_time:1051954ms step_avg:169.94ms\nstep:6201/10000 train_loss:3.8459 train_time:1052123ms step_avg:169.94ms\nstep:6202/10000 train_loss:3.8364 train_time:1052294ms step_avg:169.94ms\nstep:6203/10000 train_loss:3.9366 train_time:1052460ms step_avg:169.94ms\nstep:6204/10000 train_loss:3.9347 train_time:1052628ms step_avg:169.94ms\nstep:6205/10000 train_loss:3.9988 train_time:1052798ms step_avg:169.94ms\nstep:6206/10000 train_loss:3.8449 train_time:1052966ms step_avg:169.94ms\nstep:6207/10000 train_loss:4.0141 train_time:1053135ms step_avg:169.94ms\nstep:6208/10000 train_loss:4.0393 train_time:1053311ms step_avg:169.94ms\nstep:6209/10000 train_loss:3.9431 train_time:1053477ms step_avg:169.94ms\nstep:6210/10000 train_loss:3.9711 train_time:1053645ms step_avg:169.94ms\nstep:6211/10000 train_loss:3.9353 train_time:1053815ms step_avg:169.94ms\nstep:6212/10000 train_loss:3.9578 train_time:1053983ms step_avg:169.94ms\nstep:6213/10000 train_loss:3.9723 train_time:1054152ms step_avg:169.94ms\nstep:6214/10000 train_loss:3.8834 train_time:1054323ms step_avg:169.94ms\nstep:6215/10000 train_loss:4.0023 train_time:1054496ms step_avg:169.94ms\nstep:6216/10000 train_loss:3.8514 train_time:1054664ms step_avg:169.94ms\nstep:6217/10000 train_loss:4.0081 train_time:1054835ms step_avg:169.94ms\nstep:6218/10000 train_loss:3.8359 train_time:1055003ms step_avg:169.94ms\nstep:6219/10000 train_loss:3.9568 train_time:1055172ms step_avg:169.94ms\nstep:6220/10000 train_loss:4.0292 train_time:1055342ms step_avg:169.94ms\nstep:6221/10000 train_loss:3.8289 train_time:1055516ms step_avg:169.94ms\nstep:6222/10000 train_loss:3.8236 train_time:1055684ms step_avg:169.94ms\nstep:6223/10000 train_loss:3.9721 train_time:1055895ms step_avg:169.95ms\nstep:6224/10000 train_loss:3.9717 train_time:1056046ms step_avg:169.95ms\nstep:6225/10000 train_loss:3.9420 train_time:1056196ms step_avg:169.94ms\nstep:6226/10000 train_loss:3.8065 train_time:1056366ms step_avg:169.94ms\nstep:6227/10000 train_loss:3.8669 train_time:1056535ms step_avg:169.94ms\nstep:6228/10000 train_loss:3.8857 train_time:1056704ms step_avg:169.94ms\nstep:6229/10000 train_loss:3.8994 train_time:1056876ms step_avg:169.94ms\nstep:6230/10000 train_loss:3.9325 train_time:1057045ms step_avg:169.94ms\nstep:6231/10000 train_loss:3.8600 train_time:1057216ms step_avg:169.94ms\nstep:6232/10000 train_loss:3.8241 train_time:1057384ms step_avg:169.94ms\nstep:6233/10000 train_loss:3.9445 train_time:1057554ms step_avg:169.94ms\nstep:6234/10000 train_loss:3.9428 train_time:1057723ms step_avg:169.94ms\nstep:6235/10000 train_loss:3.8676 train_time:1057893ms step_avg:169.94ms\nstep:6236/10000 train_loss:3.8943 train_time:1058060ms step_avg:169.94ms\nstep:6237/10000 train_loss:3.9971 train_time:1058230ms step_avg:169.94ms\nstep:6238/10000 train_loss:3.8824 train_time:1058400ms step_avg:169.94ms\nstep:6239/10000 train_loss:4.0153 train_time:1058568ms step_avg:169.94ms\nstep:6240/10000 train_loss:3.8494 train_time:1058735ms step_avg:169.94ms\nstep:6241/10000 train_loss:3.9192 train_time:1058905ms step_avg:169.94ms\nstep:6242/10000 train_loss:3.9574 train_time:1059075ms step_avg:169.94ms\nstep:6243/10000 train_loss:3.9762 train_time:1059243ms step_avg:169.94ms\nstep:6244/10000 train_loss:3.9077 train_time:1059414ms step_avg:169.94ms\nstep:6245/10000 train_loss:3.7131 train_time:1059584ms step_avg:169.94ms\nstep:6246/10000 train_loss:3.8845 train_time:1059752ms step_avg:169.94ms\nstep:6247/10000 train_loss:3.8991 train_time:1059921ms step_avg:169.94ms\nstep:6248/10000 train_loss:3.8672 train_time:1060088ms step_avg:169.94ms\nstep:6249/10000 train_loss:3.7359 train_time:1060259ms step_avg:169.94ms\nstep:6250/10000 train_loss:3.8277 train_time:1060428ms step_avg:169.94ms\nstep:6251/10000 train_loss:3.9002 train_time:1060599ms step_avg:169.94ms\nstep:6252/10000 train_loss:3.9868 train_time:1060769ms step_avg:169.94ms\nstep:6253/10000 train_loss:4.0907 train_time:1060940ms step_avg:169.94ms\nstep:6254/10000 train_loss:4.0137 train_time:1061108ms step_avg:169.94ms\nstep:6255/10000 train_loss:3.9181 train_time:1061277ms step_avg:169.94ms\nstep:6256/10000 train_loss:3.8525 train_time:1061448ms step_avg:169.94ms\nstep:6257/10000 train_loss:3.8417 train_time:1061618ms step_avg:169.94ms\nstep:6258/10000 train_loss:3.8687 train_time:1061787ms step_avg:169.94ms\nstep:6259/10000 train_loss:3.8953 train_time:1061958ms step_avg:169.94ms\nstep:6260/10000 train_loss:3.9696 train_time:1062127ms step_avg:169.94ms\nstep:6261/10000 train_loss:3.8995 train_time:1062296ms step_avg:169.94ms\nstep:6262/10000 train_loss:3.8196 train_time:1062464ms step_avg:169.94ms\nstep:6263/10000 train_loss:4.0524 train_time:1062633ms step_avg:169.94ms\nstep:6264/10000 train_loss:3.8179 train_time:1062801ms step_avg:169.94ms\nstep:6265/10000 train_loss:3.9112 train_time:1062971ms step_avg:169.94ms\nstep:6266/10000 train_loss:3.9780 train_time:1063139ms step_avg:169.94ms\nstep:6267/10000 train_loss:3.7876 train_time:1063306ms step_avg:169.94ms\nstep:6268/10000 train_loss:3.9404 train_time:1063474ms step_avg:169.94ms\nstep:6269/10000 train_loss:3.9325 train_time:1063644ms step_avg:169.94ms\nstep:6270/10000 train_loss:3.7749 train_time:1063815ms step_avg:169.94ms\nstep:6271/10000 train_loss:3.9206 train_time:1063984ms step_avg:169.94ms\nstep:6272/10000 train_loss:3.7418 train_time:1064155ms step_avg:169.94ms\nstep:6273/10000 train_loss:3.9278 train_time:1064325ms step_avg:169.94ms\nstep:6274/10000 train_loss:3.8808 train_time:1064495ms step_avg:169.94ms\nstep:6275/10000 train_loss:3.9013 train_time:1064665ms step_avg:169.94ms\nstep:6276/10000 train_loss:3.8250 train_time:1064836ms step_avg:169.94ms\nstep:6277/10000 train_loss:3.8375 train_time:1065004ms step_avg:169.94ms\nstep:6278/10000 train_loss:3.9617 train_time:1065177ms step_avg:169.94ms\nstep:6279/10000 train_loss:3.9029 train_time:1065346ms step_avg:169.94ms\nstep:6280/10000 train_loss:3.7948 train_time:1065516ms step_avg:169.94ms\nstep:6281/10000 train_loss:3.7996 train_time:1065682ms step_avg:169.94ms\nstep:6282/10000 train_loss:3.8744 train_time:1065853ms step_avg:169.94ms\nstep:6283/10000 train_loss:3.8685 train_time:1066022ms step_avg:169.94ms\nstep:6284/10000 train_loss:3.8057 train_time:1066193ms step_avg:169.94ms\nstep:6285/10000 train_loss:3.8725 train_time:1066364ms step_avg:169.94ms\nstep:6286/10000 train_loss:3.8463 train_time:1066535ms step_avg:169.94ms\nstep:6287/10000 train_loss:3.8913 train_time:1066704ms step_avg:169.94ms\nstep:6288/10000 train_loss:3.8198 train_time:1066875ms step_avg:169.94ms\nstep:6289/10000 train_loss:3.9535 train_time:1067044ms step_avg:169.94ms\nstep:6290/10000 train_loss:3.9405 train_time:1067214ms step_avg:169.94ms\nstep:6291/10000 train_loss:3.8674 train_time:1067381ms step_avg:169.94ms\nstep:6292/10000 train_loss:3.8748 train_time:1067550ms step_avg:169.94ms\nstep:6293/10000 train_loss:3.8647 train_time:1067718ms step_avg:169.94ms\nstep:6294/10000 train_loss:3.9737 train_time:1067889ms step_avg:169.94ms\nstep:6295/10000 train_loss:3.7272 train_time:1068059ms step_avg:169.94ms\nstep:6296/10000 train_loss:3.6924 train_time:1068227ms step_avg:169.94ms\nstep:6297/10000 train_loss:3.9304 train_time:1068399ms step_avg:169.94ms\nstep:6298/10000 train_loss:3.7837 train_time:1068567ms step_avg:169.94ms\nstep:6299/10000 train_loss:3.8428 train_time:1068737ms step_avg:169.94ms\nstep:6300/10000 train_loss:3.8880 train_time:1068906ms step_avg:169.94ms\nstep:6301/10000 train_loss:3.8729 train_time:1069076ms step_avg:169.94ms\nstep:6302/10000 train_loss:3.8124 train_time:1069243ms step_avg:169.94ms\nstep:6303/10000 train_loss:3.7822 train_time:1069415ms step_avg:169.94ms\nstep:6304/10000 train_loss:3.8945 train_time:1069583ms step_avg:169.94ms\nstep:6305/10000 train_loss:3.8686 train_time:1069750ms step_avg:169.94ms\nstep:6306/10000 train_loss:3.9329 train_time:1069919ms step_avg:169.94ms\nstep:6307/10000 train_loss:3.8589 train_time:1070091ms step_avg:169.94ms\nstep:6308/10000 train_loss:3.8731 train_time:1070263ms step_avg:169.94ms\nstep:6309/10000 train_loss:3.9457 train_time:1070435ms step_avg:169.94ms\nstep:6310/10000 train_loss:3.7956 train_time:1070604ms step_avg:169.94ms\nstep:6311/10000 train_loss:3.8900 train_time:1070776ms step_avg:169.94ms\nstep:6312/10000 train_loss:3.8834 train_time:1070944ms step_avg:169.94ms\nstep:6313/10000 train_loss:4.0232 train_time:1071116ms step_avg:169.94ms\nstep:6314/10000 train_loss:3.9287 train_time:1071284ms step_avg:169.94ms\nstep:6315/10000 train_loss:3.8484 train_time:1071454ms step_avg:169.94ms\nstep:6316/10000 train_loss:3.8645 train_time:1071623ms step_avg:169.94ms\nstep:6317/10000 train_loss:3.8849 train_time:1071791ms step_avg:169.94ms\nstep:6318/10000 train_loss:3.8448 train_time:1071962ms step_avg:169.94ms\nstep:6319/10000 train_loss:3.9322 train_time:1072132ms step_avg:169.94ms\nstep:6320/10000 train_loss:3.8497 train_time:1072301ms step_avg:169.94ms\nstep:6321/10000 train_loss:4.2219 train_time:1072473ms step_avg:169.94ms\nstep:6322/10000 train_loss:3.9143 train_time:1072644ms step_avg:169.94ms\nstep:6323/10000 train_loss:3.8238 train_time:1072814ms step_avg:169.94ms\nstep:6324/10000 train_loss:4.0380 train_time:1072983ms step_avg:169.94ms\nstep:6325/10000 train_loss:3.9339 train_time:1073153ms step_avg:169.94ms\nstep:6326/10000 train_loss:3.8564 train_time:1073323ms step_avg:169.94ms\nstep:6327/10000 train_loss:3.7399 train_time:1073492ms step_avg:169.94ms\nstep:6328/10000 train_loss:3.9173 train_time:1073660ms step_avg:169.94ms\nstep:6329/10000 train_loss:4.0952 train_time:1073827ms step_avg:169.94ms\nstep:6330/10000 train_loss:3.8311 train_time:1073996ms step_avg:169.94ms\nstep:6331/10000 train_loss:3.8611 train_time:1074167ms step_avg:169.94ms\nstep:6332/10000 train_loss:3.9628 train_time:1074337ms step_avg:169.94ms\nstep:6333/10000 train_loss:3.8985 train_time:1074507ms step_avg:169.94ms\nstep:6334/10000 train_loss:3.8177 train_time:1074679ms step_avg:169.94ms\nstep:6335/10000 train_loss:3.7772 train_time:1074848ms step_avg:169.94ms\nstep:6336/10000 train_loss:3.7433 train_time:1075015ms step_avg:169.94ms\nstep:6337/10000 train_loss:3.9059 train_time:1075185ms step_avg:169.94ms\nstep:6338/10000 train_loss:4.0323 train_time:1075353ms step_avg:169.94ms\nstep:6339/10000 train_loss:3.9378 train_time:1075520ms step_avg:169.94ms\nstep:6340/10000 train_loss:3.8509 train_time:1075690ms step_avg:169.94ms\nstep:6341/10000 train_loss:4.0554 train_time:1075859ms step_avg:169.94ms\nstep:6342/10000 train_loss:3.8247 train_time:1076028ms step_avg:169.93ms\nstep:6343/10000 train_loss:4.0498 train_time:1076199ms step_avg:169.94ms\nstep:6344/10000 train_loss:3.8308 train_time:1076367ms step_avg:169.93ms\nstep:6345/10000 train_loss:3.8521 train_time:1076536ms step_avg:169.93ms\nstep:6346/10000 train_loss:3.9695 train_time:1076704ms step_avg:169.93ms\nstep:6347/10000 train_loss:3.9207 train_time:1076875ms step_avg:169.93ms\nstep:6348/10000 train_loss:3.9956 train_time:1077044ms step_avg:169.93ms\nstep:6349/10000 train_loss:3.8626 train_time:1077217ms step_avg:169.93ms\nstep:6350/10000 train_loss:3.8281 train_time:1077385ms step_avg:169.93ms\nstep:6351/10000 train_loss:3.9005 train_time:1077554ms step_avg:169.93ms\nstep:6352/10000 train_loss:3.8520 train_time:1077721ms step_avg:169.93ms\nstep:6353/10000 train_loss:3.9569 train_time:1077891ms step_avg:169.93ms\nstep:6354/10000 train_loss:3.9701 train_time:1078061ms step_avg:169.93ms\nstep:6355/10000 train_loss:4.1529 train_time:1078230ms step_avg:169.93ms\nstep:6356/10000 train_loss:3.7998 train_time:1078401ms step_avg:169.93ms\nstep:6357/10000 train_loss:3.8417 train_time:1078570ms step_avg:169.93ms\nstep:6358/10000 train_loss:3.7433 train_time:1078740ms step_avg:169.93ms\nstep:6359/10000 train_loss:3.7993 train_time:1078909ms step_avg:169.93ms\nstep:6360/10000 train_loss:3.9405 train_time:1079080ms step_avg:169.93ms\nstep:6361/10000 train_loss:3.8268 train_time:1079249ms step_avg:169.93ms\nstep:6362/10000 train_loss:3.9463 train_time:1079420ms step_avg:169.93ms\nstep:6363/10000 train_loss:3.9294 train_time:1079588ms step_avg:169.93ms\nstep:6364/10000 train_loss:3.8654 train_time:1079756ms step_avg:169.93ms\nstep:6365/10000 train_loss:3.8075 train_time:1079924ms step_avg:169.93ms\nstep:6366/10000 train_loss:3.8487 train_time:1080093ms step_avg:169.93ms\nstep:6367/10000 train_loss:4.0703 train_time:1080261ms step_avg:169.93ms\nstep:6368/10000 train_loss:3.8239 train_time:1080433ms step_avg:169.93ms\nstep:6369/10000 train_loss:3.9276 train_time:1080601ms step_avg:169.93ms\nstep:6370/10000 train_loss:4.0200 train_time:1080770ms step_avg:169.93ms\nstep:6371/10000 train_loss:3.9256 train_time:1080942ms step_avg:169.93ms\nstep:6372/10000 train_loss:3.8111 train_time:1081111ms step_avg:169.93ms\nstep:6373/10000 train_loss:3.8066 train_time:1081281ms step_avg:169.93ms\nstep:6374/10000 train_loss:3.8373 train_time:1081454ms step_avg:169.93ms\nstep:6375/10000 train_loss:3.8971 train_time:1081623ms step_avg:169.93ms\nstep:6376/10000 train_loss:4.2003 train_time:1081792ms step_avg:169.93ms\nstep:6377/10000 train_loss:3.5656 train_time:1081961ms step_avg:169.93ms\nstep:6378/10000 train_loss:3.8505 train_time:1082129ms step_avg:169.93ms\nstep:6379/10000 train_loss:3.9585 train_time:1082297ms step_avg:169.93ms\nstep:6380/10000 train_loss:3.7313 train_time:1082464ms step_avg:169.93ms\nstep:6381/10000 train_loss:3.8138 train_time:1082634ms step_avg:169.93ms\nstep:6382/10000 train_loss:3.9273 train_time:1082805ms step_avg:169.93ms\nstep:6383/10000 train_loss:3.8870 train_time:1082975ms step_avg:169.93ms\nstep:6384/10000 train_loss:3.8169 train_time:1083144ms step_avg:169.93ms\nstep:6385/10000 train_loss:3.8337 train_time:1083315ms step_avg:169.93ms\nstep:6386/10000 train_loss:3.9124 train_time:1083484ms step_avg:169.93ms\nstep:6387/10000 train_loss:3.6776 train_time:1083657ms step_avg:169.93ms\nstep:6388/10000 train_loss:3.9071 train_time:1083824ms step_avg:169.93ms\nstep:6389/10000 train_loss:3.9913 train_time:1083993ms step_avg:169.93ms\nstep:6390/10000 train_loss:3.8909 train_time:1084164ms step_avg:169.93ms\nstep:6391/10000 train_loss:3.8877 train_time:1084333ms step_avg:169.93ms\nstep:6392/10000 train_loss:3.8467 train_time:1084503ms step_avg:169.93ms\nstep:6393/10000 train_loss:3.8321 train_time:1084674ms step_avg:169.93ms\nstep:6394/10000 train_loss:4.0299 train_time:1084843ms step_avg:169.93ms\nstep:6395/10000 train_loss:4.0160 train_time:1085015ms step_avg:169.93ms\nstep:6396/10000 train_loss:3.7115 train_time:1085186ms step_avg:169.93ms\nstep:6397/10000 train_loss:3.9072 train_time:1085356ms step_avg:169.93ms\nstep:6398/10000 train_loss:3.7924 train_time:1085525ms step_avg:169.93ms\nstep:6399/10000 train_loss:3.8935 train_time:1085693ms step_avg:169.93ms\nstep:6400/10000 train_loss:3.8696 train_time:1085863ms step_avg:169.93ms\nstep:6401/10000 train_loss:3.9105 train_time:1086033ms step_avg:169.93ms\nstep:6402/10000 train_loss:3.9163 train_time:1086203ms step_avg:169.93ms\nstep:6403/10000 train_loss:3.9092 train_time:1086373ms step_avg:169.93ms\nstep:6404/10000 train_loss:3.8477 train_time:1086542ms step_avg:169.93ms\nstep:6405/10000 train_loss:3.7722 train_time:1086710ms step_avg:169.93ms\nstep:6406/10000 train_loss:3.9464 train_time:1086878ms step_avg:169.93ms\nstep:6407/10000 train_loss:4.3146 train_time:1087047ms step_avg:169.93ms\nstep:6408/10000 train_loss:3.9024 train_time:1087218ms step_avg:169.93ms\nstep:6409/10000 train_loss:3.8588 train_time:1087387ms step_avg:169.93ms\nstep:6410/10000 train_loss:4.0055 train_time:1087561ms step_avg:169.93ms\nstep:6411/10000 train_loss:3.8043 train_time:1087725ms step_avg:169.93ms\nstep:6412/10000 train_loss:3.8587 train_time:1087894ms step_avg:169.93ms\nstep:6413/10000 train_loss:3.8386 train_time:1088063ms step_avg:169.93ms\nstep:6414/10000 train_loss:3.8382 train_time:1088233ms step_avg:169.93ms\nstep:6415/10000 train_loss:3.8315 train_time:1088401ms step_avg:169.93ms\nstep:6416/10000 train_loss:3.7930 train_time:1088572ms step_avg:169.93ms\nstep:6417/10000 train_loss:3.8796 train_time:1088742ms step_avg:169.93ms\nstep:6418/10000 train_loss:4.1348 train_time:1088914ms step_avg:169.93ms\nstep:6419/10000 train_loss:4.0002 train_time:1089082ms step_avg:169.93ms\nstep:6420/10000 train_loss:3.8575 train_time:1089250ms step_avg:169.93ms\nstep:6421/10000 train_loss:3.9379 train_time:1089421ms step_avg:169.93ms\nstep:6422/10000 train_loss:4.0489 train_time:1089591ms step_avg:169.93ms\nstep:6423/10000 train_loss:3.8933 train_time:1089760ms step_avg:169.93ms\nstep:6424/10000 train_loss:3.7706 train_time:1089927ms step_avg:169.93ms\nstep:6425/10000 train_loss:3.7325 train_time:1090099ms step_avg:169.93ms\nstep:6426/10000 train_loss:3.8671 train_time:1090267ms step_avg:169.93ms\nstep:6427/10000 train_loss:3.8360 train_time:1090437ms step_avg:169.93ms\nstep:6428/10000 train_loss:3.8773 train_time:1090607ms step_avg:169.93ms\nstep:6429/10000 train_loss:3.9010 train_time:1090776ms step_avg:169.93ms\nstep:6430/10000 train_loss:3.8721 train_time:1090944ms step_avg:169.93ms\nstep:6431/10000 train_loss:3.8043 train_time:1091113ms step_avg:169.93ms\nstep:6432/10000 train_loss:3.7635 train_time:1091281ms step_avg:169.93ms\nstep:6433/10000 train_loss:3.9927 train_time:1091455ms step_avg:169.93ms\nstep:6434/10000 train_loss:3.7351 train_time:1091625ms step_avg:169.93ms\nstep:6435/10000 train_loss:3.9263 train_time:1091794ms step_avg:169.93ms\nstep:6436/10000 train_loss:3.8279 train_time:1091961ms step_avg:169.93ms\nstep:6437/10000 train_loss:3.8401 train_time:1092131ms step_avg:169.93ms\nstep:6438/10000 train_loss:4.0814 train_time:1092300ms step_avg:169.93ms\nstep:6439/10000 train_loss:3.9978 train_time:1092473ms step_avg:169.93ms\nstep:6440/10000 train_loss:3.8619 train_time:1092641ms step_avg:169.93ms\nstep:6441/10000 train_loss:4.0048 train_time:1092812ms step_avg:169.93ms\nstep:6442/10000 train_loss:3.8456 train_time:1092979ms step_avg:169.93ms\nstep:6443/10000 train_loss:3.8928 train_time:1093147ms step_avg:169.93ms\nstep:6444/10000 train_loss:3.7086 train_time:1093314ms step_avg:169.93ms\nstep:6445/10000 train_loss:3.7553 train_time:1093485ms step_avg:169.93ms\nstep:6446/10000 train_loss:3.8722 train_time:1093653ms step_avg:169.93ms\nstep:6447/10000 train_loss:3.6638 train_time:1093821ms step_avg:169.93ms\nstep:6448/10000 train_loss:3.8776 train_time:1093990ms step_avg:169.93ms\nstep:6449/10000 train_loss:3.8319 train_time:1094160ms step_avg:169.93ms\nstep:6450/10000 train_loss:3.8211 train_time:1094329ms step_avg:169.93ms\nstep:6451/10000 train_loss:3.8629 train_time:1094498ms step_avg:169.93ms\nstep:6452/10000 train_loss:3.8507 train_time:1094668ms step_avg:169.93ms\nstep:6453/10000 train_loss:3.9120 train_time:1094839ms step_avg:169.93ms\nstep:6454/10000 train_loss:4.4138 train_time:1095007ms step_avg:169.93ms\nstep:6455/10000 train_loss:3.9337 train_time:1095177ms step_avg:169.93ms\nstep:6456/10000 train_loss:3.8810 train_time:1095346ms step_avg:169.93ms\nstep:6457/10000 train_loss:3.8580 train_time:1095515ms step_avg:169.93ms\nstep:6458/10000 train_loss:4.0802 train_time:1095684ms step_avg:169.93ms\nstep:6459/10000 train_loss:3.9265 train_time:1095856ms step_avg:169.93ms\nstep:6460/10000 train_loss:3.9566 train_time:1096024ms step_avg:169.93ms\nstep:6461/10000 train_loss:3.8873 train_time:1096194ms step_avg:169.93ms\nstep:6462/10000 train_loss:4.0218 train_time:1096361ms step_avg:169.93ms\nstep:6463/10000 train_loss:3.9823 train_time:1096530ms step_avg:169.93ms\nstep:6464/10000 train_loss:3.8708 train_time:1096699ms step_avg:169.93ms\nstep:6465/10000 train_loss:3.8202 train_time:1096869ms step_avg:169.93ms\nstep:6466/10000 train_loss:3.8853 train_time:1097040ms step_avg:169.93ms\nstep:6467/10000 train_loss:4.0430 train_time:1097211ms step_avg:169.93ms\nstep:6468/10000 train_loss:3.9212 train_time:1097379ms step_avg:169.93ms\nstep:6469/10000 train_loss:3.8075 train_time:1097548ms step_avg:169.93ms\nstep:6470/10000 train_loss:3.9240 train_time:1097717ms step_avg:169.93ms\nstep:6471/10000 train_loss:3.9038 train_time:1097886ms step_avg:169.93ms\nstep:6472/10000 train_loss:3.8870 train_time:1098059ms step_avg:169.93ms\nstep:6473/10000 train_loss:3.8959 train_time:1098228ms step_avg:169.93ms\nstep:6474/10000 train_loss:3.8881 train_time:1098396ms step_avg:169.93ms\nstep:6475/10000 train_loss:3.7777 train_time:1098562ms step_avg:169.92ms\nstep:6476/10000 train_loss:3.9764 train_time:1098732ms step_avg:169.92ms\nstep:6477/10000 train_loss:3.8111 train_time:1098903ms step_avg:169.92ms\nstep:6478/10000 train_loss:3.8297 train_time:1099073ms step_avg:169.92ms\nstep:6479/10000 train_loss:3.8609 train_time:1099241ms step_avg:169.92ms\nstep:6480/10000 train_loss:3.9969 train_time:1099409ms step_avg:169.92ms\nstep:6481/10000 train_loss:3.8913 train_time:1099579ms step_avg:169.92ms\nstep:6482/10000 train_loss:3.9177 train_time:1099747ms step_avg:169.92ms\nstep:6483/10000 train_loss:3.8813 train_time:1099920ms step_avg:169.92ms\nstep:6484/10000 train_loss:3.8527 train_time:1100089ms step_avg:169.92ms\nstep:6485/10000 train_loss:3.7687 train_time:1100259ms step_avg:169.92ms\nstep:6486/10000 train_loss:3.9084 train_time:1100428ms step_avg:169.92ms\nstep:6487/10000 train_loss:3.7859 train_time:1100598ms step_avg:169.92ms\nstep:6488/10000 train_loss:4.1632 train_time:1100767ms step_avg:169.92ms\nstep:6489/10000 train_loss:4.0043 train_time:1100933ms step_avg:169.92ms\nstep:6490/10000 train_loss:3.7763 train_time:1101102ms step_avg:169.92ms\nstep:6491/10000 train_loss:3.8704 train_time:1101275ms step_avg:169.92ms\nstep:6492/10000 train_loss:3.7737 train_time:1101442ms step_avg:169.92ms\nstep:6493/10000 train_loss:3.7397 train_time:1101613ms step_avg:169.92ms\nstep:6494/10000 train_loss:4.0470 train_time:1101784ms step_avg:169.92ms\nstep:6495/10000 train_loss:3.9530 train_time:1101954ms step_avg:169.92ms\nstep:6496/10000 train_loss:4.1110 train_time:1102121ms step_avg:169.92ms\nstep:6497/10000 train_loss:3.8623 train_time:1102292ms step_avg:169.92ms\nstep:6498/10000 train_loss:3.9047 train_time:1102460ms step_avg:169.92ms\nstep:6499/10000 train_loss:3.9037 train_time:1102631ms step_avg:169.92ms\nstep:6500/10000 train_loss:3.8285 train_time:1102801ms step_avg:169.92ms\nstep:6501/10000 train_loss:3.7786 train_time:1102972ms step_avg:169.92ms\nstep:6502/10000 train_loss:3.7476 train_time:1103141ms step_avg:169.92ms\nstep:6503/10000 train_loss:3.9709 train_time:1103310ms step_avg:169.92ms\nstep:6504/10000 train_loss:3.9218 train_time:1103481ms step_avg:169.92ms\nstep:6505/10000 train_loss:3.7562 train_time:1103652ms step_avg:169.92ms\nstep:6506/10000 train_loss:3.8329 train_time:1103821ms step_avg:169.92ms\nstep:6507/10000 train_loss:3.8495 train_time:1103991ms step_avg:169.92ms\nstep:6508/10000 train_loss:3.9165 train_time:1104160ms step_avg:169.92ms\nstep:6509/10000 train_loss:3.8393 train_time:1104328ms step_avg:169.92ms\nstep:6510/10000 train_loss:3.8437 train_time:1104497ms step_avg:169.92ms\nstep:6511/10000 train_loss:3.8288 train_time:1104666ms step_avg:169.92ms\nstep:6512/10000 train_loss:3.9863 train_time:1104837ms step_avg:169.92ms\nstep:6513/10000 train_loss:3.5847 train_time:1105007ms step_avg:169.92ms\nstep:6514/10000 train_loss:3.8820 train_time:1105181ms step_avg:169.92ms\nstep:6515/10000 train_loss:3.8417 train_time:1105351ms step_avg:169.92ms\nstep:6516/10000 train_loss:3.8306 train_time:1105522ms step_avg:169.92ms\nstep:6517/10000 train_loss:3.9459 train_time:1105690ms step_avg:169.92ms\nstep:6518/10000 train_loss:3.8719 train_time:1105861ms step_avg:169.92ms\nstep:6519/10000 train_loss:3.9132 train_time:1106029ms step_avg:169.92ms\nstep:6520/10000 train_loss:3.8573 train_time:1106197ms step_avg:169.92ms\nstep:6521/10000 train_loss:3.9343 train_time:1106366ms step_avg:169.92ms\nstep:6522/10000 train_loss:3.8099 train_time:1106535ms step_avg:169.92ms\nstep:6523/10000 train_loss:3.9598 train_time:1106704ms step_avg:169.92ms\nstep:6524/10000 train_loss:3.8780 train_time:1106872ms step_avg:169.92ms\nstep:6525/10000 train_loss:3.7493 train_time:1107040ms step_avg:169.92ms\nstep:6526/10000 train_loss:3.9148 train_time:1107209ms step_avg:169.92ms\nstep:6527/10000 train_loss:3.7017 train_time:1107378ms step_avg:169.92ms\nstep:6528/10000 train_loss:3.7514 train_time:1107546ms step_avg:169.92ms\nstep:6529/10000 train_loss:3.8174 train_time:1107719ms step_avg:169.92ms\nstep:6530/10000 train_loss:3.8226 train_time:1107888ms step_avg:169.92ms\nstep:6531/10000 train_loss:3.8835 train_time:1108061ms step_avg:169.92ms\nstep:6532/10000 train_loss:3.8200 train_time:1108229ms step_avg:169.92ms\nstep:6533/10000 train_loss:3.9791 train_time:1108397ms step_avg:169.92ms\nstep:6534/10000 train_loss:3.8593 train_time:1108565ms step_avg:169.92ms\nstep:6535/10000 train_loss:3.8652 train_time:1108736ms step_avg:169.92ms\nstep:6536/10000 train_loss:3.8404 train_time:1108903ms step_avg:169.92ms\nstep:6537/10000 train_loss:4.0270 train_time:1109074ms step_avg:169.92ms\nstep:6538/10000 train_loss:3.7237 train_time:1109242ms step_avg:169.92ms\nstep:6539/10000 train_loss:3.8615 train_time:1109412ms step_avg:169.92ms\nstep:6540/10000 train_loss:3.8626 train_time:1109579ms step_avg:169.92ms\nstep:6541/10000 train_loss:3.8662 train_time:1109748ms step_avg:169.92ms\nstep:6542/10000 train_loss:3.7219 train_time:1109921ms step_avg:169.92ms\nstep:6543/10000 train_loss:3.9621 train_time:1110091ms step_avg:169.92ms\nstep:6544/10000 train_loss:3.9036 train_time:1110262ms step_avg:169.92ms\nstep:6545/10000 train_loss:3.7952 train_time:1110434ms step_avg:169.92ms\nstep:6546/10000 train_loss:3.8874 train_time:1110600ms step_avg:169.92ms\nstep:6547/10000 train_loss:4.0359 train_time:1110772ms step_avg:169.92ms\nstep:6548/10000 train_loss:4.1808 train_time:1110942ms step_avg:169.92ms\nstep:6549/10000 train_loss:3.9126 train_time:1111111ms step_avg:169.92ms\nstep:6550/10000 train_loss:3.8658 train_time:1111283ms step_avg:169.92ms\nstep:6551/10000 train_loss:4.0419 train_time:1111455ms step_avg:169.92ms\nstep:6552/10000 train_loss:3.8104 train_time:1111622ms step_avg:169.92ms\nstep:6553/10000 train_loss:3.7083 train_time:1111791ms step_avg:169.92ms\nstep:6554/10000 train_loss:3.8074 train_time:1111963ms step_avg:169.92ms\nstep:6555/10000 train_loss:3.9269 train_time:1112127ms step_avg:169.92ms\nstep:6556/10000 train_loss:3.7953 train_time:1112298ms step_avg:169.92ms\nstep:6557/10000 train_loss:4.0209 train_time:1112465ms step_avg:169.92ms\nstep:6558/10000 train_loss:3.9062 train_time:1112633ms step_avg:169.92ms\nstep:6559/10000 train_loss:3.9020 train_time:1112801ms step_avg:169.92ms\nstep:6560/10000 train_loss:3.8253 train_time:1112970ms step_avg:169.92ms\nstep:6561/10000 train_loss:3.8418 train_time:1113138ms step_avg:169.92ms\nstep:6562/10000 train_loss:3.9680 train_time:1113308ms step_avg:169.92ms\nstep:6563/10000 train_loss:3.7030 train_time:1113480ms step_avg:169.92ms\nstep:6564/10000 train_loss:3.9594 train_time:1113648ms step_avg:169.92ms\nstep:6565/10000 train_loss:4.0683 train_time:1113816ms step_avg:169.92ms\nstep:6566/10000 train_loss:3.9582 train_time:1113985ms step_avg:169.92ms\nstep:6567/10000 train_loss:3.9090 train_time:1114160ms step_avg:169.92ms\nstep:6568/10000 train_loss:3.8557 train_time:1114331ms step_avg:169.92ms\nstep:6569/10000 train_loss:3.7079 train_time:1114502ms step_avg:169.92ms\nstep:6570/10000 train_loss:4.1401 train_time:1114668ms step_avg:169.92ms\nstep:6571/10000 train_loss:4.0476 train_time:1114834ms step_avg:169.92ms\nstep:6572/10000 train_loss:3.9181 train_time:1115003ms step_avg:169.92ms\nstep:6573/10000 train_loss:3.7448 train_time:1115174ms step_avg:169.92ms\nstep:6574/10000 train_loss:3.9962 train_time:1115344ms step_avg:169.92ms\nstep:6575/10000 train_loss:3.8498 train_time:1115516ms step_avg:169.92ms\nstep:6576/10000 train_loss:3.7700 train_time:1115684ms step_avg:169.92ms\nstep:6577/10000 train_loss:3.7709 train_time:1115853ms step_avg:169.92ms\nstep:6578/10000 train_loss:3.8725 train_time:1116020ms step_avg:169.92ms\nstep:6579/10000 train_loss:3.8949 train_time:1116189ms step_avg:169.92ms\nstep:6580/10000 train_loss:3.8239 train_time:1116360ms step_avg:169.92ms\nstep:6581/10000 train_loss:3.8550 train_time:1116530ms step_avg:169.92ms\nstep:6582/10000 train_loss:3.9458 train_time:1116699ms step_avg:169.92ms\nstep:6583/10000 train_loss:3.9599 train_time:1116869ms step_avg:169.92ms\nstep:6584/10000 train_loss:3.8998 train_time:1117039ms step_avg:169.92ms\nstep:6585/10000 train_loss:3.8082 train_time:1117208ms step_avg:169.92ms\nstep:6586/10000 train_loss:3.8429 train_time:1117377ms step_avg:169.92ms\nstep:6587/10000 train_loss:3.8336 train_time:1117548ms step_avg:169.92ms\nstep:6588/10000 train_loss:3.7842 train_time:1117718ms step_avg:169.92ms\nstep:6589/10000 train_loss:3.9749 train_time:1117888ms step_avg:169.92ms\nstep:6590/10000 train_loss:3.8716 train_time:1118057ms step_avg:169.92ms\nstep:6591/10000 train_loss:4.0233 train_time:1118225ms step_avg:169.92ms\nstep:6592/10000 train_loss:3.8804 train_time:1118394ms step_avg:169.92ms\nstep:6593/10000 train_loss:3.8177 train_time:1118562ms step_avg:169.92ms\nstep:6594/10000 train_loss:3.8756 train_time:1118733ms step_avg:169.92ms\nstep:6595/10000 train_loss:3.8333 train_time:1118902ms step_avg:169.92ms\nstep:6596/10000 train_loss:3.9434 train_time:1119071ms step_avg:169.92ms\nstep:6597/10000 train_loss:3.8336 train_time:1119237ms step_avg:169.92ms\nstep:6598/10000 train_loss:3.6816 train_time:1119406ms step_avg:169.92ms\nstep:6599/10000 train_loss:3.8765 train_time:1119574ms step_avg:169.92ms\nstep:6600/10000 train_loss:3.8332 train_time:1119743ms step_avg:169.92ms\nstep:6601/10000 train_loss:3.7617 train_time:1119918ms step_avg:169.92ms\nstep:6602/10000 train_loss:3.9255 train_time:1120089ms step_avg:169.92ms\nstep:6603/10000 train_loss:3.8534 train_time:1120259ms step_avg:169.92ms\nstep:6604/10000 train_loss:3.9020 train_time:1120422ms step_avg:169.92ms\nstep:6605/10000 train_loss:3.8566 train_time:1120590ms step_avg:169.92ms\nstep:6606/10000 train_loss:3.7709 train_time:1120760ms step_avg:169.92ms\nstep:6607/10000 train_loss:3.8552 train_time:1120931ms step_avg:169.92ms\nstep:6608/10000 train_loss:3.8909 train_time:1121100ms step_avg:169.92ms\nstep:6609/10000 train_loss:4.0587 train_time:1121272ms step_avg:169.92ms\nstep:6610/10000 train_loss:3.8501 train_time:1121439ms step_avg:169.91ms\nstep:6611/10000 train_loss:3.9195 train_time:1121607ms step_avg:169.91ms\nstep:6612/10000 train_loss:3.8504 train_time:1121774ms step_avg:169.91ms\nstep:6613/10000 train_loss:3.9422 train_time:1121943ms step_avg:169.91ms\nstep:6614/10000 train_loss:3.8286 train_time:1122113ms step_avg:169.91ms\nstep:6615/10000 train_loss:3.8464 train_time:1122282ms step_avg:169.91ms\nstep:6616/10000 train_loss:3.9569 train_time:1122453ms step_avg:169.91ms\nstep:6617/10000 train_loss:3.7159 train_time:1122621ms step_avg:169.91ms\nstep:6618/10000 train_loss:3.8857 train_time:1122791ms step_avg:169.91ms\nstep:6619/10000 train_loss:3.9655 train_time:1122959ms step_avg:169.91ms\nstep:6620/10000 train_loss:3.7622 train_time:1123129ms step_avg:169.91ms\nstep:6621/10000 train_loss:3.8246 train_time:1123300ms step_avg:169.91ms\nstep:6622/10000 train_loss:3.8565 train_time:1123472ms step_avg:169.91ms\nstep:6623/10000 train_loss:3.8734 train_time:1123641ms step_avg:169.91ms\nstep:6624/10000 train_loss:3.6205 train_time:1123810ms step_avg:169.91ms\nstep:6625/10000 train_loss:3.8691 train_time:1123982ms step_avg:169.91ms\nstep:6626/10000 train_loss:3.9830 train_time:1124150ms step_avg:169.91ms\nstep:6627/10000 train_loss:3.8040 train_time:1124320ms step_avg:169.91ms\nstep:6628/10000 train_loss:3.8971 train_time:1124490ms step_avg:169.91ms\nstep:6629/10000 train_loss:3.8139 train_time:1124660ms step_avg:169.91ms\nstep:6630/10000 train_loss:3.9364 train_time:1124829ms step_avg:169.91ms\nstep:6631/10000 train_loss:3.8355 train_time:1124998ms step_avg:169.91ms\nstep:6632/10000 train_loss:3.9180 train_time:1125168ms step_avg:169.91ms\nstep:6633/10000 train_loss:3.8917 train_time:1125338ms step_avg:169.91ms\nstep:6634/10000 train_loss:3.7653 train_time:1125513ms step_avg:169.91ms\nstep:6635/10000 train_loss:4.0298 train_time:1125683ms step_avg:169.91ms\nstep:6636/10000 train_loss:3.8403 train_time:1125847ms step_avg:169.91ms\nstep:6637/10000 train_loss:3.8405 train_time:1126016ms step_avg:169.91ms\nstep:6638/10000 train_loss:3.9149 train_time:1126184ms step_avg:169.91ms\nstep:6639/10000 train_loss:3.9009 train_time:1126353ms step_avg:169.91ms\nstep:6640/10000 train_loss:4.0805 train_time:1126523ms step_avg:169.91ms\nstep:6641/10000 train_loss:3.9619 train_time:1126693ms step_avg:169.91ms\nstep:6642/10000 train_loss:3.8281 train_time:1126861ms step_avg:169.91ms\nstep:6643/10000 train_loss:3.9492 train_time:1127032ms step_avg:169.91ms\nstep:6644/10000 train_loss:3.9016 train_time:1127203ms step_avg:169.91ms\nstep:6645/10000 train_loss:3.9224 train_time:1127372ms step_avg:169.91ms\nstep:6646/10000 train_loss:3.8073 train_time:1127541ms step_avg:169.91ms\nstep:6647/10000 train_loss:3.8304 train_time:1127709ms step_avg:169.91ms\nstep:6648/10000 train_loss:3.9152 train_time:1127882ms step_avg:169.91ms\nstep:6649/10000 train_loss:3.9903 train_time:1128050ms step_avg:169.91ms\nstep:6650/10000 train_loss:3.8677 train_time:1128222ms step_avg:169.91ms\nstep:6651/10000 train_loss:3.9564 train_time:1128393ms step_avg:169.91ms\nstep:6652/10000 train_loss:3.9106 train_time:1128560ms step_avg:169.91ms\nstep:6653/10000 train_loss:3.9007 train_time:1128727ms step_avg:169.91ms\nstep:6654/10000 train_loss:3.8917 train_time:1128899ms step_avg:169.91ms\nstep:6655/10000 train_loss:4.0514 train_time:1129069ms step_avg:169.91ms\nstep:6656/10000 train_loss:3.9342 train_time:1129238ms step_avg:169.91ms\nstep:6657/10000 train_loss:3.8315 train_time:1129408ms step_avg:169.91ms\nstep:6658/10000 train_loss:3.8262 train_time:1129583ms step_avg:169.91ms\nstep:6659/10000 train_loss:3.9100 train_time:1129750ms step_avg:169.91ms\nstep:6660/10000 train_loss:3.7984 train_time:1129921ms step_avg:169.91ms\nstep:6661/10000 train_loss:3.9892 train_time:1130090ms step_avg:169.91ms\nstep:6662/10000 train_loss:3.9009 train_time:1130262ms step_avg:169.91ms\nstep:6663/10000 train_loss:3.9986 train_time:1130432ms step_avg:169.91ms\nstep:6664/10000 train_loss:3.8651 train_time:1130602ms step_avg:169.91ms\nstep:6665/10000 train_loss:3.7385 train_time:1130770ms step_avg:169.91ms\nstep:6666/10000 train_loss:3.9233 train_time:1130941ms step_avg:169.91ms\nstep:6667/10000 train_loss:3.9716 train_time:1131109ms step_avg:169.91ms\nstep:6668/10000 train_loss:3.7645 train_time:1131280ms step_avg:169.91ms\nstep:6669/10000 train_loss:3.8654 train_time:1131452ms step_avg:169.91ms\nstep:6670/10000 train_loss:3.9334 train_time:1131622ms step_avg:169.91ms\nstep:6671/10000 train_loss:3.9058 train_time:1131790ms step_avg:169.91ms\nstep:6672/10000 train_loss:3.8425 train_time:1131960ms step_avg:169.91ms\nstep:6673/10000 train_loss:3.8333 train_time:1132128ms step_avg:169.91ms\nstep:6674/10000 train_loss:3.9384 train_time:1132296ms step_avg:169.91ms\nstep:6675/10000 train_loss:3.8670 train_time:1132466ms step_avg:169.91ms\nstep:6676/10000 train_loss:3.8917 train_time:1132637ms step_avg:169.91ms\nstep:6677/10000 train_loss:3.8544 train_time:1132805ms step_avg:169.91ms\nstep:6678/10000 train_loss:4.0371 train_time:1132974ms step_avg:169.91ms\nstep:6679/10000 train_loss:3.9299 train_time:1133142ms step_avg:169.91ms\nstep:6680/10000 train_loss:3.6974 train_time:1133312ms step_avg:169.91ms\nstep:6681/10000 train_loss:3.7635 train_time:1133481ms step_avg:169.91ms\nstep:6682/10000 train_loss:3.7864 train_time:1133650ms step_avg:169.91ms\nstep:6683/10000 train_loss:3.7784 train_time:1133820ms step_avg:169.91ms\nstep:6684/10000 train_loss:3.9170 train_time:1133990ms step_avg:169.91ms\nstep:6685/10000 train_loss:3.9317 train_time:1134162ms step_avg:169.91ms\nstep:6686/10000 train_loss:3.8394 train_time:1134333ms step_avg:169.91ms\nstep:6687/10000 train_loss:3.9424 train_time:1134505ms step_avg:169.91ms\nstep:6688/10000 train_loss:3.7255 train_time:1134672ms step_avg:169.91ms\nstep:6689/10000 train_loss:3.8244 train_time:1134840ms step_avg:169.91ms\nstep:6690/10000 train_loss:3.8048 train_time:1135009ms step_avg:169.91ms\nstep:6691/10000 train_loss:3.9192 train_time:1135177ms step_avg:169.91ms\nstep:6692/10000 train_loss:4.0092 train_time:1135346ms step_avg:169.91ms\nstep:6693/10000 train_loss:3.8539 train_time:1135516ms step_avg:169.91ms\nstep:6694/10000 train_loss:3.7803 train_time:1135686ms step_avg:169.91ms\nstep:6695/10000 train_loss:3.7603 train_time:1135856ms step_avg:169.91ms\nstep:6696/10000 train_loss:3.8867 train_time:1136023ms step_avg:169.91ms\nstep:6697/10000 train_loss:3.8415 train_time:1136193ms step_avg:169.91ms\nstep:6698/10000 train_loss:3.9630 train_time:1136362ms step_avg:169.91ms\nstep:6699/10000 train_loss:3.9925 train_time:1136533ms step_avg:169.91ms\nstep:6700/10000 train_loss:3.8630 train_time:1136702ms step_avg:169.91ms\nstep:6701/10000 train_loss:4.0365 train_time:1136875ms step_avg:169.91ms\nstep:6702/10000 train_loss:4.1404 train_time:1137043ms step_avg:169.91ms\nstep:6703/10000 train_loss:3.7177 train_time:1137214ms step_avg:169.91ms\nstep:6704/10000 train_loss:3.7484 train_time:1137382ms step_avg:169.91ms\nstep:6705/10000 train_loss:3.8791 train_time:1137555ms step_avg:169.91ms\nstep:6706/10000 train_loss:3.8602 train_time:1137724ms step_avg:169.91ms\nstep:6707/10000 train_loss:3.9401 train_time:1137894ms step_avg:169.91ms\nstep:6708/10000 train_loss:3.8242 train_time:1138061ms step_avg:169.91ms\nstep:6709/10000 train_loss:3.7686 train_time:1138232ms step_avg:169.91ms\nstep:6710/10000 train_loss:3.8259 train_time:1138400ms step_avg:169.91ms\nstep:6711/10000 train_loss:3.8897 train_time:1138568ms step_avg:169.91ms\nstep:6712/10000 train_loss:4.1262 train_time:1138739ms step_avg:169.91ms\nstep:6713/10000 train_loss:3.9332 train_time:1138908ms step_avg:169.91ms\nstep:6714/10000 train_loss:3.7415 train_time:1139076ms step_avg:169.91ms\nstep:6715/10000 train_loss:3.8231 train_time:1139247ms step_avg:169.91ms\nstep:6716/10000 train_loss:3.8929 train_time:1139419ms step_avg:169.91ms\nstep:6717/10000 train_loss:3.8277 train_time:1139588ms step_avg:169.91ms\nstep:6718/10000 train_loss:3.9303 train_time:1139759ms step_avg:169.91ms\nstep:6719/10000 train_loss:3.8002 train_time:1139926ms step_avg:169.91ms\nstep:6720/10000 train_loss:3.8443 train_time:1140094ms step_avg:169.91ms\nstep:6721/10000 train_loss:3.9800 train_time:1140263ms step_avg:169.91ms\nstep:6722/10000 train_loss:3.8317 train_time:1140433ms step_avg:169.91ms\nstep:6723/10000 train_loss:4.0726 train_time:1140602ms step_avg:169.91ms\nstep:6724/10000 train_loss:3.9750 train_time:1140773ms step_avg:169.91ms\nstep:6725/10000 train_loss:4.0340 train_time:1140944ms step_avg:169.91ms\nstep:6726/10000 train_loss:3.8534 train_time:1141113ms step_avg:169.91ms\nstep:6727/10000 train_loss:3.8924 train_time:1141282ms step_avg:169.91ms\nstep:6728/10000 train_loss:3.9244 train_time:1141453ms step_avg:169.91ms\nstep:6729/10000 train_loss:3.9799 train_time:1141623ms step_avg:169.91ms\nstep:6730/10000 train_loss:3.8452 train_time:1141792ms step_avg:169.91ms\nstep:6731/10000 train_loss:3.7794 train_time:1141961ms step_avg:169.91ms\nstep:6732/10000 train_loss:3.8129 train_time:1142129ms step_avg:169.91ms\nstep:6733/10000 train_loss:3.8372 train_time:1142297ms step_avg:169.91ms\nstep:6734/10000 train_loss:3.6817 train_time:1142465ms step_avg:169.91ms\nstep:6735/10000 train_loss:3.7987 train_time:1142635ms step_avg:169.91ms\nstep:6736/10000 train_loss:3.9073 train_time:1142806ms step_avg:169.91ms\nstep:6737/10000 train_loss:3.8928 train_time:1142975ms step_avg:169.91ms\nstep:6738/10000 train_loss:3.8343 train_time:1143145ms step_avg:169.91ms\nstep:6739/10000 train_loss:3.8101 train_time:1143316ms step_avg:169.91ms\nstep:6740/10000 train_loss:3.7566 train_time:1143484ms step_avg:169.91ms\nstep:6741/10000 train_loss:3.8436 train_time:1143654ms step_avg:169.91ms\nstep:6742/10000 train_loss:3.8752 train_time:1143822ms step_avg:169.91ms\nstep:6743/10000 train_loss:3.8172 train_time:1143993ms step_avg:169.91ms\nstep:6744/10000 train_loss:3.9001 train_time:1144162ms step_avg:169.91ms\nstep:6745/10000 train_loss:3.8303 train_time:1144332ms step_avg:169.91ms\nstep:6746/10000 train_loss:4.0050 train_time:1144500ms step_avg:169.91ms\nstep:6747/10000 train_loss:3.7190 train_time:1144670ms step_avg:169.91ms\nstep:6748/10000 train_loss:3.7987 train_time:1144841ms step_avg:169.91ms\nstep:6749/10000 train_loss:3.8376 train_time:1145012ms step_avg:169.91ms\nstep:6750/10000 train_loss:3.9223 train_time:1145178ms step_avg:169.91ms\nstep:6751/10000 train_loss:3.9244 train_time:1145348ms step_avg:169.91ms\nstep:6752/10000 train_loss:3.6872 train_time:1145520ms step_avg:169.91ms\nstep:6753/10000 train_loss:3.9063 train_time:1145689ms step_avg:169.91ms\nstep:6754/10000 train_loss:3.8430 train_time:1145858ms step_avg:169.91ms\nstep:6755/10000 train_loss:3.8212 train_time:1146026ms step_avg:169.91ms\nstep:6756/10000 train_loss:4.1082 train_time:1146194ms step_avg:169.91ms\nstep:6757/10000 train_loss:3.8858 train_time:1146364ms step_avg:169.91ms\nstep:6758/10000 train_loss:3.9129 train_time:1146536ms step_avg:169.91ms\nstep:6759/10000 train_loss:3.7756 train_time:1146704ms step_avg:169.91ms\nstep:6760/10000 train_loss:3.8989 train_time:1146874ms step_avg:169.91ms\nstep:6761/10000 train_loss:3.9215 train_time:1147042ms step_avg:169.91ms\nstep:6762/10000 train_loss:3.8040 train_time:1147213ms step_avg:169.91ms\nstep:6763/10000 train_loss:3.8021 train_time:1147383ms step_avg:169.91ms\nstep:6764/10000 train_loss:3.8970 train_time:1147555ms step_avg:169.91ms\nstep:6765/10000 train_loss:3.8381 train_time:1147723ms step_avg:169.91ms\nstep:6766/10000 train_loss:3.9157 train_time:1147895ms step_avg:169.91ms\nstep:6767/10000 train_loss:3.8740 train_time:1148064ms step_avg:169.91ms\nstep:6768/10000 train_loss:3.8305 train_time:1148234ms step_avg:169.91ms\nstep:6769/10000 train_loss:4.0002 train_time:1148403ms step_avg:169.91ms\nstep:6770/10000 train_loss:3.7979 train_time:1148574ms step_avg:169.91ms\nstep:6771/10000 train_loss:3.8578 train_time:1148743ms step_avg:169.91ms\nstep:6772/10000 train_loss:3.7615 train_time:1148914ms step_avg:169.91ms\nstep:6773/10000 train_loss:3.8457 train_time:1149083ms step_avg:169.91ms\nstep:6774/10000 train_loss:3.7574 train_time:1149253ms step_avg:169.91ms\nstep:6775/10000 train_loss:3.8367 train_time:1149425ms step_avg:169.91ms\nstep:6776/10000 train_loss:3.8806 train_time:1149591ms step_avg:169.91ms\nstep:6777/10000 train_loss:3.6768 train_time:1149761ms step_avg:169.91ms\nstep:6778/10000 train_loss:3.8356 train_time:1149930ms step_avg:169.91ms\nstep:6779/10000 train_loss:3.8188 train_time:1150099ms step_avg:169.91ms\nstep:6780/10000 train_loss:3.8329 train_time:1150268ms step_avg:169.91ms\nstep:6781/10000 train_loss:3.8149 train_time:1150437ms step_avg:169.91ms\nstep:6782/10000 train_loss:3.8340 train_time:1150606ms step_avg:169.91ms\nstep:6783/10000 train_loss:3.8789 train_time:1150775ms step_avg:169.91ms\nstep:6784/10000 train_loss:3.8914 train_time:1150944ms step_avg:169.91ms\nstep:6785/10000 train_loss:3.8359 train_time:1151114ms step_avg:169.91ms\nstep:6786/10000 train_loss:3.7718 train_time:1151282ms step_avg:169.91ms\nstep:6787/10000 train_loss:3.9691 train_time:1151454ms step_avg:169.91ms\nstep:6788/10000 train_loss:3.8891 train_time:1151623ms step_avg:169.91ms\nstep:6789/10000 train_loss:3.8115 train_time:1151792ms step_avg:169.91ms\nstep:6790/10000 train_loss:4.2129 train_time:1151961ms step_avg:169.91ms\nstep:6791/10000 train_loss:3.8814 train_time:1152130ms step_avg:169.91ms\nstep:6792/10000 train_loss:3.9377 train_time:1152300ms step_avg:169.91ms\nstep:6793/10000 train_loss:3.8786 train_time:1152472ms step_avg:169.91ms\nstep:6794/10000 train_loss:3.9208 train_time:1152640ms step_avg:169.91ms\nstep:6795/10000 train_loss:3.8481 train_time:1152809ms step_avg:169.91ms\nstep:6796/10000 train_loss:3.8818 train_time:1152979ms step_avg:169.91ms\nstep:6797/10000 train_loss:3.8770 train_time:1153149ms step_avg:169.91ms\nstep:6798/10000 train_loss:3.9022 train_time:1153321ms step_avg:169.91ms\nstep:6799/10000 train_loss:3.7650 train_time:1153490ms step_avg:169.91ms\nstep:6800/10000 train_loss:3.8248 train_time:1153657ms step_avg:169.91ms\nstep:6801/10000 train_loss:3.8639 train_time:1153826ms step_avg:169.91ms\nstep:6802/10000 train_loss:3.8110 train_time:1153994ms step_avg:169.90ms\nstep:6803/10000 train_loss:3.9889 train_time:1154162ms step_avg:169.90ms\nstep:6804/10000 train_loss:3.7502 train_time:1154331ms step_avg:169.90ms\nstep:6805/10000 train_loss:3.9185 train_time:1154501ms step_avg:169.90ms\nstep:6806/10000 train_loss:3.7826 train_time:1154669ms step_avg:169.90ms\nstep:6807/10000 train_loss:3.8200 train_time:1154838ms step_avg:169.90ms\nstep:6808/10000 train_loss:3.7828 train_time:1155007ms step_avg:169.90ms\nstep:6809/10000 train_loss:3.8294 train_time:1155180ms step_avg:169.90ms\nstep:6810/10000 train_loss:3.8490 train_time:1155349ms step_avg:169.90ms\nstep:6811/10000 train_loss:3.8766 train_time:1155519ms step_avg:169.90ms\nstep:6812/10000 train_loss:3.8333 train_time:1155691ms step_avg:169.90ms\nstep:6813/10000 train_loss:3.8635 train_time:1155857ms step_avg:169.90ms\nstep:6814/10000 train_loss:3.7949 train_time:1156025ms step_avg:169.90ms\nstep:6815/10000 train_loss:3.8295 train_time:1156195ms step_avg:169.90ms\nstep:6816/10000 train_loss:3.8447 train_time:1156364ms step_avg:169.90ms\nstep:6817/10000 train_loss:4.0710 train_time:1156535ms step_avg:169.90ms\nstep:6818/10000 train_loss:3.8658 train_time:1156702ms step_avg:169.90ms\nstep:6819/10000 train_loss:3.9011 train_time:1156872ms step_avg:169.90ms\nstep:6820/10000 train_loss:3.8620 train_time:1157041ms step_avg:169.90ms\nstep:6821/10000 train_loss:3.9272 train_time:1157211ms step_avg:169.90ms\nstep:6822/10000 train_loss:3.8021 train_time:1157381ms step_avg:169.90ms\nstep:6823/10000 train_loss:3.9174 train_time:1157551ms step_avg:169.90ms\nstep:6824/10000 train_loss:3.9016 train_time:1157720ms step_avg:169.90ms\nstep:6825/10000 train_loss:3.8410 train_time:1157890ms step_avg:169.90ms\nstep:6826/10000 train_loss:4.0264 train_time:1158059ms step_avg:169.90ms\nstep:6827/10000 train_loss:3.7993 train_time:1158229ms step_avg:169.90ms\nstep:6828/10000 train_loss:3.8075 train_time:1158400ms step_avg:169.90ms\nstep:6829/10000 train_loss:3.8511 train_time:1158570ms step_avg:169.90ms\nstep:6830/10000 train_loss:3.7744 train_time:1158738ms step_avg:169.90ms\nstep:6831/10000 train_loss:3.7808 train_time:1158907ms step_avg:169.90ms\nstep:6832/10000 train_loss:3.9513 train_time:1159079ms step_avg:169.90ms\nstep:6833/10000 train_loss:3.9166 train_time:1159247ms step_avg:169.90ms\nstep:6834/10000 train_loss:3.9896 train_time:1159420ms step_avg:169.90ms\nstep:6835/10000 train_loss:3.8927 train_time:1159589ms step_avg:169.90ms\nstep:6836/10000 train_loss:3.7177 train_time:1159756ms step_avg:169.90ms\nstep:6837/10000 train_loss:3.9064 train_time:1159928ms step_avg:169.90ms\nstep:6838/10000 train_loss:3.8568 train_time:1160098ms step_avg:169.90ms\nstep:6839/10000 train_loss:3.6274 train_time:1160266ms step_avg:169.90ms\nstep:6840/10000 train_loss:3.8476 train_time:1160434ms step_avg:169.90ms\nstep:6841/10000 train_loss:3.9122 train_time:1160601ms step_avg:169.90ms\nstep:6842/10000 train_loss:3.5834 train_time:1160769ms step_avg:169.90ms\nstep:6843/10000 train_loss:3.9544 train_time:1160938ms step_avg:169.90ms\nstep:6844/10000 train_loss:3.7875 train_time:1161107ms step_avg:169.90ms\nstep:6845/10000 train_loss:3.6866 train_time:1161277ms step_avg:169.90ms\nstep:6846/10000 train_loss:3.9576 train_time:1161447ms step_avg:169.90ms\nstep:6847/10000 train_loss:3.8960 train_time:1161620ms step_avg:169.90ms\nstep:6848/10000 train_loss:3.7857 train_time:1161790ms step_avg:169.90ms\nstep:6849/10000 train_loss:3.7486 train_time:1161960ms step_avg:169.90ms\nstep:6850/10000 train_loss:3.8848 train_time:1162126ms step_avg:169.90ms\nstep:6851/10000 train_loss:3.9350 train_time:1162296ms step_avg:169.90ms\nstep:6852/10000 train_loss:4.2785 train_time:1162464ms step_avg:169.90ms\nstep:6853/10000 train_loss:3.9098 train_time:1162638ms step_avg:169.90ms\nstep:6854/10000 train_loss:3.8856 train_time:1162805ms step_avg:169.90ms\nstep:6855/10000 train_loss:4.2439 train_time:1162975ms step_avg:169.90ms\nstep:6856/10000 train_loss:3.8533 train_time:1163143ms step_avg:169.90ms\nstep:6857/10000 train_loss:3.7117 train_time:1163314ms step_avg:169.90ms\nstep:6858/10000 train_loss:3.8543 train_time:1163702ms step_avg:169.93ms\nstep:6859/10000 train_loss:3.8499 train_time:1163867ms step_avg:169.93ms\nstep:6860/10000 train_loss:3.8499 train_time:1164037ms step_avg:169.93ms\nstep:6861/10000 train_loss:3.6932 train_time:1164204ms step_avg:169.93ms\nstep:6862/10000 train_loss:4.0907 train_time:1164372ms step_avg:169.93ms\nstep:6863/10000 train_loss:3.7698 train_time:1164542ms step_avg:169.93ms\nstep:6864/10000 train_loss:3.9014 train_time:1164721ms step_avg:169.93ms\nstep:6865/10000 train_loss:3.8086 train_time:1164892ms step_avg:169.93ms\nstep:6866/10000 train_loss:3.8582 train_time:1165060ms step_avg:169.93ms\nstep:6867/10000 train_loss:3.9691 train_time:1165229ms step_avg:169.93ms\nstep:6868/10000 train_loss:3.9152 train_time:1165398ms step_avg:169.93ms\nstep:6869/10000 train_loss:3.8342 train_time:1165567ms step_avg:169.93ms\nstep:6870/10000 train_loss:3.8703 train_time:1165741ms step_avg:169.93ms\nstep:6871/10000 train_loss:3.8721 train_time:1165911ms step_avg:169.93ms\nstep:6872/10000 train_loss:3.8782 train_time:1166080ms step_avg:169.93ms\nstep:6873/10000 train_loss:3.8090 train_time:1166249ms step_avg:169.93ms\nstep:6874/10000 train_loss:3.9336 train_time:1166420ms step_avg:169.93ms\nstep:6875/10000 train_loss:4.0715 train_time:1166589ms step_avg:169.93ms\nstep:6876/10000 train_loss:3.8133 train_time:1166758ms step_avg:169.93ms\nstep:6877/10000 train_loss:4.0270 train_time:1166926ms step_avg:169.93ms\nstep:6878/10000 train_loss:4.0059 train_time:1167094ms step_avg:169.93ms\nstep:6879/10000 train_loss:3.8117 train_time:1167263ms step_avg:169.93ms\nstep:6880/10000 train_loss:3.6973 train_time:1167433ms step_avg:169.93ms\nstep:6881/10000 train_loss:3.8313 train_time:1167604ms step_avg:169.93ms\nstep:6882/10000 train_loss:4.0179 train_time:1167773ms step_avg:169.93ms\nstep:6883/10000 train_loss:3.6982 train_time:1167941ms step_avg:169.93ms\nstep:6884/10000 train_loss:3.8044 train_time:1168110ms step_avg:169.93ms\nstep:6885/10000 train_loss:3.8861 train_time:1168278ms step_avg:169.93ms\nstep:6886/10000 train_loss:3.8704 train_time:1168449ms step_avg:169.93ms\nstep:6887/10000 train_loss:3.9222 train_time:1168619ms step_avg:169.93ms\nstep:6888/10000 train_loss:3.8418 train_time:1168789ms step_avg:169.93ms\nstep:6889/10000 train_loss:3.8482 train_time:1168960ms step_avg:169.93ms\nstep:6890/10000 train_loss:3.8473 train_time:1169128ms step_avg:169.93ms\nstep:6891/10000 train_loss:3.8423 train_time:1169295ms step_avg:169.93ms\nstep:6892/10000 train_loss:3.7841 train_time:1169462ms step_avg:169.93ms\nstep:6893/10000 train_loss:3.9963 train_time:1169632ms step_avg:169.93ms\nstep:6894/10000 train_loss:3.8343 train_time:1169802ms step_avg:169.93ms\nstep:6895/10000 train_loss:3.7936 train_time:1169973ms step_avg:169.93ms\nstep:6896/10000 train_loss:3.9260 train_time:1170142ms step_avg:169.93ms\nstep:6897/10000 train_loss:3.9098 train_time:1170313ms step_avg:169.93ms\nstep:6898/10000 train_loss:3.4364 train_time:1170480ms step_avg:169.93ms\nstep:6899/10000 train_loss:3.8022 train_time:1170650ms step_avg:169.93ms\nstep:6900/10000 train_loss:3.7663 train_time:1170826ms step_avg:169.93ms\nstep:6901/10000 train_loss:3.8323 train_time:1170993ms step_avg:169.93ms\nstep:6902/10000 train_loss:3.7597 train_time:1171161ms step_avg:169.93ms\nstep:6903/10000 train_loss:3.9524 train_time:1171333ms step_avg:169.93ms\nstep:6904/10000 train_loss:3.8789 train_time:1171502ms step_avg:169.93ms\nstep:6905/10000 train_loss:3.9310 train_time:1171679ms step_avg:169.93ms\nstep:6906/10000 train_loss:3.8893 train_time:1171849ms step_avg:169.93ms\nstep:6907/10000 train_loss:3.9251 train_time:1172016ms step_avg:169.93ms\nstep:6908/10000 train_loss:3.9633 train_time:1172180ms step_avg:169.93ms\nstep:6909/10000 train_loss:4.0022 train_time:1172349ms step_avg:169.93ms\nstep:6910/10000 train_loss:3.8303 train_time:1172519ms step_avg:169.93ms\nstep:6911/10000 train_loss:3.7708 train_time:1172686ms step_avg:169.93ms\nstep:6912/10000 train_loss:3.8201 train_time:1172857ms step_avg:169.93ms\nstep:6913/10000 train_loss:3.7557 train_time:1173027ms step_avg:169.93ms\nstep:6914/10000 train_loss:3.9030 train_time:1173196ms step_avg:169.93ms\nstep:6915/10000 train_loss:4.0550 train_time:1173363ms step_avg:169.93ms\nstep:6916/10000 train_loss:4.0615 train_time:1173532ms step_avg:169.93ms\nstep:6917/10000 train_loss:3.8717 train_time:1173701ms step_avg:169.93ms\nstep:6918/10000 train_loss:3.8440 train_time:1173871ms step_avg:169.93ms\nstep:6919/10000 train_loss:3.7771 train_time:1174040ms step_avg:169.93ms\nstep:6920/10000 train_loss:3.8431 train_time:1174211ms step_avg:169.93ms\nstep:6921/10000 train_loss:3.8982 train_time:1174382ms step_avg:169.93ms\nstep:6922/10000 train_loss:3.8223 train_time:1174551ms step_avg:169.93ms\nstep:6923/10000 train_loss:3.8293 train_time:1174719ms step_avg:169.93ms\nstep:6924/10000 train_loss:3.8901 train_time:1174888ms step_avg:169.93ms\nstep:6925/10000 train_loss:3.9411 train_time:1175059ms step_avg:169.93ms\nstep:6926/10000 train_loss:3.8308 train_time:1175229ms step_avg:169.93ms\nstep:6927/10000 train_loss:3.7933 train_time:1175400ms step_avg:169.93ms\nstep:6928/10000 train_loss:3.8748 train_time:1175570ms step_avg:169.93ms\nstep:6929/10000 train_loss:3.8264 train_time:1175740ms step_avg:169.93ms\nstep:6930/10000 train_loss:4.0118 train_time:1175907ms step_avg:169.93ms\nstep:6931/10000 train_loss:3.9130 train_time:1176077ms step_avg:169.93ms\nstep:6932/10000 train_loss:3.9072 train_time:1176246ms step_avg:169.93ms\nstep:6933/10000 train_loss:4.0828 train_time:1176417ms step_avg:169.93ms\nstep:6934/10000 train_loss:3.7859 train_time:1176584ms step_avg:169.93ms\nstep:6935/10000 train_loss:3.7716 train_time:1176754ms step_avg:169.93ms\nstep:6936/10000 train_loss:4.2806 train_time:1176921ms step_avg:169.93ms\nstep:6937/10000 train_loss:3.8061 train_time:1177091ms step_avg:169.93ms\nstep:6938/10000 train_loss:3.8486 train_time:1177260ms step_avg:169.93ms\nstep:6939/10000 train_loss:4.0893 train_time:1177429ms step_avg:169.93ms\nstep:6940/10000 train_loss:3.8543 train_time:1177601ms step_avg:169.93ms\nstep:6941/10000 train_loss:3.8891 train_time:1177769ms step_avg:169.93ms\nstep:6942/10000 train_loss:3.8416 train_time:1177938ms step_avg:169.93ms\nstep:6943/10000 train_loss:3.8166 train_time:1178105ms step_avg:169.93ms\nstep:6944/10000 train_loss:3.9782 train_time:1178273ms step_avg:169.93ms\nstep:6945/10000 train_loss:3.8413 train_time:1178442ms step_avg:169.93ms\nstep:6946/10000 train_loss:3.8112 train_time:1178611ms step_avg:169.93ms\nstep:6947/10000 train_loss:3.9752 train_time:1178781ms step_avg:169.93ms\nstep:6948/10000 train_loss:3.7630 train_time:1178951ms step_avg:169.93ms\nstep:6949/10000 train_loss:3.9410 train_time:1179123ms step_avg:169.93ms\nstep:6950/10000 train_loss:3.7797 train_time:1179293ms step_avg:169.93ms\nstep:6951/10000 train_loss:3.8381 train_time:1179462ms step_avg:169.93ms\nstep:6952/10000 train_loss:3.7954 train_time:1179633ms step_avg:169.93ms\nstep:6953/10000 train_loss:3.7733 train_time:1179802ms step_avg:169.93ms\nstep:6954/10000 train_loss:3.9562 train_time:1179973ms step_avg:169.93ms\nstep:6955/10000 train_loss:4.7234 train_time:1180141ms step_avg:169.93ms\nstep:6956/10000 train_loss:3.8266 train_time:1180311ms step_avg:169.93ms\nstep:6957/10000 train_loss:3.8781 train_time:1180481ms step_avg:169.93ms\nstep:6958/10000 train_loss:3.8459 train_time:1180654ms step_avg:169.93ms\nstep:6959/10000 train_loss:3.8217 train_time:1180822ms step_avg:169.93ms\nstep:6960/10000 train_loss:3.8092 train_time:1180991ms step_avg:169.93ms\nstep:6961/10000 train_loss:4.1241 train_time:1181158ms step_avg:169.93ms\nstep:6962/10000 train_loss:3.9664 train_time:1181327ms step_avg:169.93ms\nstep:6963/10000 train_loss:3.7962 train_time:1181497ms step_avg:169.93ms\nstep:6964/10000 train_loss:3.9813 train_time:1181665ms step_avg:169.93ms\nstep:6965/10000 train_loss:4.0969 train_time:1181833ms step_avg:169.93ms\nstep:6966/10000 train_loss:3.8597 train_time:1182000ms step_avg:169.93ms\nstep:6967/10000 train_loss:3.8534 train_time:1182170ms step_avg:169.93ms\nstep:6968/10000 train_loss:3.8224 train_time:1182338ms step_avg:169.93ms\nstep:6969/10000 train_loss:3.6582 train_time:1182509ms step_avg:169.93ms\nstep:6970/10000 train_loss:3.9489 train_time:1182675ms step_avg:169.92ms\nstep:6971/10000 train_loss:3.7467 train_time:1182844ms step_avg:169.92ms\nstep:6972/10000 train_loss:3.9033 train_time:1183015ms step_avg:169.92ms\nstep:6973/10000 train_loss:3.8131 train_time:1183184ms step_avg:169.92ms\nstep:6974/10000 train_loss:3.9536 train_time:1183356ms step_avg:169.92ms\nstep:6975/10000 train_loss:3.8109 train_time:1183524ms step_avg:169.92ms\nstep:6976/10000 train_loss:3.9161 train_time:1183692ms step_avg:169.92ms\nstep:6977/10000 train_loss:3.8366 train_time:1183860ms step_avg:169.92ms\nstep:6978/10000 train_loss:3.9240 train_time:1184029ms step_avg:169.92ms\nstep:6979/10000 train_loss:3.7346 train_time:1184200ms step_avg:169.92ms\nstep:6980/10000 train_loss:3.7431 train_time:1184369ms step_avg:169.92ms\nstep:6981/10000 train_loss:3.6910 train_time:1184540ms step_avg:169.92ms\nstep:6982/10000 train_loss:3.9413 train_time:1184708ms step_avg:169.92ms\nstep:6983/10000 train_loss:3.7849 train_time:1184879ms step_avg:169.92ms\nstep:6984/10000 train_loss:3.6125 train_time:1185049ms step_avg:169.92ms\nstep:6985/10000 train_loss:3.9631 train_time:1185220ms step_avg:169.92ms\nstep:6986/10000 train_loss:3.9692 train_time:1185389ms step_avg:169.92ms\nstep:6987/10000 train_loss:3.8154 train_time:1185560ms step_avg:169.92ms\nstep:6988/10000 train_loss:3.5698 train_time:1185731ms step_avg:169.92ms\nstep:6989/10000 train_loss:3.6742 train_time:1185901ms step_avg:169.92ms\nstep:6990/10000 train_loss:3.7979 train_time:1186070ms step_avg:169.92ms\nstep:6991/10000 train_loss:3.9099 train_time:1186241ms step_avg:169.92ms\nstep:6992/10000 train_loss:3.8569 train_time:1186410ms step_avg:169.92ms\nstep:6993/10000 train_loss:3.8601 train_time:1186581ms step_avg:169.92ms\nstep:6994/10000 train_loss:3.7766 train_time:1186749ms step_avg:169.92ms\nstep:6995/10000 train_loss:3.7867 train_time:1186919ms step_avg:169.92ms\nstep:6996/10000 train_loss:3.8678 train_time:1187091ms step_avg:169.92ms\nstep:6997/10000 train_loss:3.9144 train_time:1187257ms step_avg:169.92ms\nstep:6998/10000 train_loss:3.8984 train_time:1187424ms step_avg:169.92ms\nstep:6999/10000 train_loss:3.8258 train_time:1187594ms step_avg:169.92ms\nstep:7000/10000 train_loss:3.8533 train_time:1187762ms step_avg:169.92ms\nstep:7001/10000 train_loss:3.9101 train_time:1187933ms step_avg:169.92ms\nstep:7002/10000 train_loss:3.9277 train_time:1188103ms step_avg:169.92ms\nstep:7003/10000 train_loss:3.8928 train_time:1188275ms step_avg:169.92ms\nstep:7004/10000 train_loss:3.8199 train_time:1188443ms step_avg:169.92ms\nstep:7005/10000 train_loss:3.7969 train_time:1188615ms step_avg:169.92ms\nstep:7006/10000 train_loss:3.8680 train_time:1188784ms step_avg:169.92ms\nstep:7007/10000 train_loss:3.8846 train_time:1188955ms step_avg:169.92ms\nstep:7008/10000 train_loss:4.1085 train_time:1189122ms step_avg:169.92ms\nstep:7009/10000 train_loss:3.8192 train_time:1189292ms step_avg:169.92ms\nstep:7010/10000 train_loss:3.9174 train_time:1189463ms step_avg:169.92ms\nstep:7011/10000 train_loss:4.1449 train_time:1189634ms step_avg:169.92ms\nstep:7012/10000 train_loss:3.8071 train_time:1189803ms step_avg:169.92ms\nstep:7013/10000 train_loss:3.8719 train_time:1189973ms step_avg:169.92ms\nstep:7014/10000 train_loss:3.8290 train_time:1190142ms step_avg:169.92ms\nstep:7015/10000 train_loss:4.0433 train_time:1190312ms step_avg:169.92ms\nstep:7016/10000 train_loss:3.8962 train_time:1190479ms step_avg:169.92ms\nstep:7017/10000 train_loss:3.9350 train_time:1190648ms step_avg:169.92ms\nstep:7018/10000 train_loss:3.8280 train_time:1190818ms step_avg:169.92ms\nstep:7019/10000 train_loss:3.8082 train_time:1190987ms step_avg:169.92ms\nstep:7020/10000 train_loss:3.8525 train_time:1191155ms step_avg:169.92ms\nstep:7021/10000 train_loss:3.8338 train_time:1191323ms step_avg:169.92ms\nstep:7022/10000 train_loss:3.8550 train_time:1191493ms step_avg:169.92ms\nstep:7023/10000 train_loss:4.0080 train_time:1191661ms step_avg:169.92ms\nstep:7024/10000 train_loss:3.5296 train_time:1191831ms step_avg:169.92ms\nstep:7025/10000 train_loss:3.9626 train_time:1192000ms step_avg:169.92ms\nstep:7026/10000 train_loss:3.9952 train_time:1192169ms step_avg:169.92ms\nstep:7027/10000 train_loss:3.9195 train_time:1192341ms step_avg:169.92ms\nstep:7028/10000 train_loss:3.6483 train_time:1192512ms step_avg:169.92ms\nstep:7029/10000 train_loss:3.8575 train_time:1192682ms step_avg:169.92ms\nstep:7030/10000 train_loss:3.8985 train_time:1192853ms step_avg:169.92ms\nstep:7031/10000 train_loss:3.9804 train_time:1193022ms step_avg:169.92ms\nstep:7032/10000 train_loss:3.7407 train_time:1193191ms step_avg:169.92ms\nstep:7033/10000 train_loss:3.9628 train_time:1193362ms step_avg:169.92ms\nstep:7034/10000 train_loss:3.9479 train_time:1193533ms step_avg:169.92ms\nstep:7035/10000 train_loss:3.7703 train_time:1193700ms step_avg:169.92ms\nstep:7036/10000 train_loss:3.6693 train_time:1193871ms step_avg:169.92ms\nstep:7037/10000 train_loss:3.8339 train_time:1194041ms step_avg:169.92ms\nstep:7038/10000 train_loss:3.8632 train_time:1194211ms step_avg:169.92ms\nstep:7039/10000 train_loss:3.8066 train_time:1194380ms step_avg:169.92ms\nstep:7040/10000 train_loss:3.8260 train_time:1194547ms step_avg:169.92ms\nstep:7041/10000 train_loss:3.8078 train_time:1194714ms step_avg:169.92ms\nstep:7042/10000 train_loss:3.8254 train_time:1194881ms step_avg:169.92ms\nstep:7043/10000 train_loss:3.7497 train_time:1195053ms step_avg:169.92ms\nstep:7044/10000 train_loss:3.7244 train_time:1195224ms step_avg:169.92ms\nstep:7045/10000 train_loss:3.8072 train_time:1195394ms step_avg:169.92ms\nstep:7046/10000 train_loss:3.9451 train_time:1195563ms step_avg:169.92ms\nstep:7047/10000 train_loss:3.8449 train_time:1195735ms step_avg:169.92ms\nstep:7048/10000 train_loss:3.6937 train_time:1195905ms step_avg:169.92ms\nstep:7049/10000 train_loss:3.6976 train_time:1196075ms step_avg:169.92ms\nstep:7050/10000 train_loss:3.8258 train_time:1196244ms step_avg:169.92ms\nstep:7051/10000 train_loss:3.9175 train_time:1196416ms step_avg:169.92ms\nstep:7052/10000 train_loss:4.7627 train_time:1196583ms step_avg:169.92ms\nstep:7053/10000 train_loss:3.8397 train_time:1196752ms step_avg:169.92ms\nstep:7054/10000 train_loss:3.8728 train_time:1196922ms step_avg:169.92ms\nstep:7055/10000 train_loss:3.8406 train_time:1197091ms step_avg:169.92ms\nstep:7056/10000 train_loss:3.8178 train_time:1197260ms step_avg:169.92ms\nstep:7057/10000 train_loss:3.8778 train_time:1197430ms step_avg:169.92ms\nstep:7058/10000 train_loss:3.8430 train_time:1197599ms step_avg:169.92ms\nstep:7059/10000 train_loss:3.9609 train_time:1197768ms step_avg:169.92ms\nstep:7060/10000 train_loss:3.8337 train_time:1197937ms step_avg:169.92ms\nstep:7061/10000 train_loss:4.0045 train_time:1198105ms step_avg:169.92ms\nstep:7062/10000 train_loss:3.9618 train_time:1198277ms step_avg:169.92ms\nstep:7063/10000 train_loss:3.7677 train_time:1198445ms step_avg:169.92ms\nstep:7064/10000 train_loss:3.8705 train_time:1198615ms step_avg:169.92ms\nstep:7065/10000 train_loss:3.8479 train_time:1198784ms step_avg:169.92ms\nstep:7066/10000 train_loss:3.8405 train_time:1198954ms step_avg:169.92ms\nstep:7067/10000 train_loss:3.7749 train_time:1199122ms step_avg:169.92ms\nstep:7068/10000 train_loss:3.8826 train_time:1199294ms step_avg:169.92ms\nstep:7069/10000 train_loss:3.8664 train_time:1199463ms step_avg:169.92ms\nstep:7070/10000 train_loss:3.8018 train_time:1199634ms step_avg:169.92ms\nstep:7071/10000 train_loss:3.8745 train_time:1199802ms step_avg:169.92ms\nstep:7072/10000 train_loss:3.7898 train_time:1199971ms step_avg:169.92ms\nstep:7073/10000 train_loss:3.7998 train_time:1200144ms step_avg:169.92ms\nstep:7074/10000 train_loss:3.8720 train_time:1200317ms step_avg:169.92ms\nstep:7075/10000 train_loss:3.9074 train_time:1200486ms step_avg:169.92ms\nstep:7076/10000 train_loss:3.8173 train_time:1200658ms step_avg:169.92ms\nstep:7077/10000 train_loss:3.7679 train_time:1200827ms step_avg:169.92ms\nstep:7078/10000 train_loss:3.8631 train_time:1200997ms step_avg:169.92ms\nstep:7079/10000 train_loss:3.8136 train_time:1201164ms step_avg:169.92ms\nstep:7080/10000 train_loss:3.8658 train_time:1201334ms step_avg:169.92ms\nstep:7081/10000 train_loss:3.7919 train_time:1201503ms step_avg:169.92ms\nstep:7082/10000 train_loss:3.7926 train_time:1201673ms step_avg:169.92ms\nstep:7083/10000 train_loss:3.7332 train_time:1201841ms step_avg:169.92ms\nstep:7084/10000 train_loss:4.1083 train_time:1202011ms step_avg:169.92ms\nstep:7085/10000 train_loss:3.8884 train_time:1202183ms step_avg:169.92ms\nstep:7086/10000 train_loss:3.8794 train_time:1202352ms step_avg:169.92ms\nstep:7087/10000 train_loss:3.8587 train_time:1202518ms step_avg:169.92ms\nstep:7088/10000 train_loss:3.8317 train_time:1202687ms step_avg:169.92ms\nstep:7089/10000 train_loss:3.8332 train_time:1202857ms step_avg:169.92ms\nstep:7090/10000 train_loss:3.8592 train_time:1203029ms step_avg:169.92ms\nstep:7091/10000 train_loss:3.8446 train_time:1203197ms step_avg:169.92ms\nstep:7092/10000 train_loss:3.9072 train_time:1203366ms step_avg:169.92ms\nstep:7093/10000 train_loss:3.9608 train_time:1203537ms step_avg:169.92ms\nstep:7094/10000 train_loss:3.8565 train_time:1203704ms step_avg:169.92ms\nstep:7095/10000 train_loss:4.2150 train_time:1203874ms step_avg:169.92ms\nstep:7096/10000 train_loss:4.2420 train_time:1204044ms step_avg:169.92ms\nstep:7097/10000 train_loss:3.8080 train_time:1204215ms step_avg:169.92ms\nstep:7098/10000 train_loss:3.8314 train_time:1204383ms step_avg:169.92ms\nstep:7099/10000 train_loss:3.8958 train_time:1204553ms step_avg:169.92ms\nstep:7100/10000 train_loss:3.8707 train_time:1204723ms step_avg:169.92ms\nstep:7101/10000 train_loss:3.9046 train_time:1204894ms step_avg:169.92ms\nstep:7102/10000 train_loss:3.8531 train_time:1205062ms step_avg:169.92ms\nstep:7103/10000 train_loss:3.9986 train_time:1205233ms step_avg:169.92ms\nstep:7104/10000 train_loss:3.7412 train_time:1205404ms step_avg:169.92ms\nstep:7105/10000 train_loss:3.8234 train_time:1205575ms step_avg:169.92ms\nstep:7106/10000 train_loss:3.9041 train_time:1205743ms step_avg:169.92ms\nstep:7107/10000 train_loss:3.7467 train_time:1205916ms step_avg:169.92ms\nstep:7108/10000 train_loss:4.0202 train_time:1206084ms step_avg:169.92ms\nstep:7109/10000 train_loss:4.1891 train_time:1206254ms step_avg:169.92ms\nstep:7110/10000 train_loss:3.7994 train_time:1206424ms step_avg:169.92ms\nstep:7111/10000 train_loss:3.7559 train_time:1206594ms step_avg:169.92ms\nstep:7112/10000 train_loss:3.6596 train_time:1206763ms step_avg:169.92ms\nstep:7113/10000 train_loss:3.9198 train_time:1206933ms step_avg:169.92ms\nstep:7114/10000 train_loss:3.9612 train_time:1207103ms step_avg:169.92ms\nstep:7115/10000 train_loss:3.9218 train_time:1207274ms step_avg:169.92ms\nstep:7116/10000 train_loss:3.8759 train_time:1207443ms step_avg:169.92ms\nstep:7117/10000 train_loss:3.9390 train_time:1207614ms step_avg:169.92ms\nstep:7118/10000 train_loss:4.0846 train_time:1207783ms step_avg:169.92ms\nstep:7119/10000 train_loss:3.8069 train_time:1207955ms step_avg:169.92ms\nstep:7120/10000 train_loss:3.9653 train_time:1208123ms step_avg:169.92ms\nstep:7121/10000 train_loss:3.8298 train_time:1208293ms step_avg:169.92ms\nstep:7122/10000 train_loss:3.8081 train_time:1208461ms step_avg:169.92ms\nstep:7123/10000 train_loss:4.0813 train_time:1208630ms step_avg:169.92ms\nstep:7124/10000 train_loss:4.0302 train_time:1208801ms step_avg:169.92ms\nstep:7125/10000 train_loss:4.0411 train_time:1208972ms step_avg:169.92ms\nstep:7126/10000 train_loss:3.8479 train_time:1209138ms step_avg:169.92ms\nstep:7127/10000 train_loss:3.7323 train_time:1209308ms step_avg:169.92ms\nstep:7128/10000 train_loss:3.8650 train_time:1209478ms step_avg:169.92ms\nstep:7129/10000 train_loss:3.9061 train_time:1209647ms step_avg:169.92ms\nstep:7130/10000 train_loss:3.7907 train_time:1209818ms step_avg:169.92ms\nstep:7131/10000 train_loss:4.0311 train_time:1209986ms step_avg:169.92ms\nstep:7132/10000 train_loss:4.1261 train_time:1210155ms step_avg:169.92ms\nstep:7133/10000 train_loss:4.0158 train_time:1210323ms step_avg:169.92ms\nstep:7134/10000 train_loss:3.8252 train_time:1210494ms step_avg:169.92ms\nstep:7135/10000 train_loss:3.8628 train_time:1210660ms step_avg:169.92ms\nstep:7136/10000 train_loss:3.9359 train_time:1210829ms step_avg:169.92ms\nstep:7137/10000 train_loss:3.9333 train_time:1210998ms step_avg:169.92ms\nstep:7138/10000 train_loss:3.8482 train_time:1211167ms step_avg:169.92ms\nstep:7139/10000 train_loss:3.7565 train_time:1211337ms step_avg:169.92ms\nstep:7140/10000 train_loss:3.9411 train_time:1211507ms step_avg:169.92ms\nstep:7141/10000 train_loss:3.8180 train_time:1211676ms step_avg:169.92ms\nstep:7142/10000 train_loss:3.7531 train_time:1211843ms step_avg:169.92ms\nstep:7143/10000 train_loss:3.7631 train_time:1212014ms step_avg:169.92ms\nstep:7144/10000 train_loss:3.7417 train_time:1212184ms step_avg:169.92ms\nstep:7145/10000 train_loss:3.8904 train_time:1212353ms step_avg:169.92ms\nstep:7146/10000 train_loss:3.9896 train_time:1212522ms step_avg:169.92ms\nstep:7147/10000 train_loss:3.9008 train_time:1212694ms step_avg:169.92ms\nstep:7148/10000 train_loss:3.8053 train_time:1212862ms step_avg:169.92ms\nstep:7149/10000 train_loss:3.8900 train_time:1213034ms step_avg:169.92ms\nstep:7150/10000 train_loss:3.9489 train_time:1213204ms step_avg:169.92ms\nstep:7151/10000 train_loss:3.8918 train_time:1213374ms step_avg:169.92ms\nstep:7152/10000 train_loss:3.9216 train_time:1213544ms step_avg:169.92ms\nstep:7153/10000 train_loss:3.9381 train_time:1213714ms step_avg:169.92ms\nstep:7154/10000 train_loss:3.6884 train_time:1213882ms step_avg:169.92ms\nstep:7155/10000 train_loss:3.7666 train_time:1214053ms step_avg:169.92ms\nstep:7156/10000 train_loss:3.8221 train_time:1214222ms step_avg:169.92ms\nstep:7157/10000 train_loss:3.8685 train_time:1214391ms step_avg:169.92ms\nstep:7158/10000 train_loss:3.8642 train_time:1214562ms step_avg:169.92ms\nstep:7159/10000 train_loss:3.9959 train_time:1214732ms step_avg:169.92ms\nstep:7160/10000 train_loss:3.9869 train_time:1214903ms step_avg:169.92ms\nstep:7161/10000 train_loss:3.7291 train_time:1215073ms step_avg:169.92ms\nstep:7162/10000 train_loss:3.8539 train_time:1215242ms step_avg:169.92ms\nstep:7163/10000 train_loss:3.9207 train_time:1215413ms step_avg:169.92ms\nstep:7164/10000 train_loss:3.8384 train_time:1215583ms step_avg:169.92ms\nstep:7165/10000 train_loss:4.1507 train_time:1215753ms step_avg:169.92ms\nstep:7166/10000 train_loss:3.8501 train_time:1215923ms step_avg:169.92ms\nstep:7167/10000 train_loss:3.8597 train_time:1216094ms step_avg:169.92ms\nstep:7168/10000 train_loss:3.7944 train_time:1216264ms step_avg:169.92ms\nstep:7169/10000 train_loss:3.6698 train_time:1216435ms step_avg:169.92ms\nstep:7170/10000 train_loss:3.9602 train_time:1216602ms step_avg:169.92ms\nstep:7171/10000 train_loss:3.8014 train_time:1216771ms step_avg:169.92ms\nstep:7172/10000 train_loss:3.9421 train_time:1216943ms step_avg:169.92ms\nstep:7173/10000 train_loss:3.8165 train_time:1217113ms step_avg:169.92ms\nstep:7174/10000 train_loss:3.9686 train_time:1217282ms step_avg:169.92ms\nstep:7175/10000 train_loss:3.7572 train_time:1217451ms step_avg:169.92ms\nstep:7176/10000 train_loss:3.8188 train_time:1217621ms step_avg:169.92ms\nstep:7177/10000 train_loss:3.7799 train_time:1217789ms step_avg:169.92ms\nstep:7178/10000 train_loss:3.8847 train_time:1217958ms step_avg:169.92ms\nstep:7179/10000 train_loss:3.8284 train_time:1218127ms step_avg:169.92ms\nstep:7180/10000 train_loss:3.7800 train_time:1218296ms step_avg:169.92ms\nstep:7181/10000 train_loss:3.8302 train_time:1218464ms step_avg:169.92ms\nstep:7182/10000 train_loss:3.7692 train_time:1218632ms step_avg:169.92ms\nstep:7183/10000 train_loss:3.9788 train_time:1218799ms step_avg:169.91ms\nstep:7184/10000 train_loss:3.9056 train_time:1218968ms step_avg:169.91ms\nstep:7185/10000 train_loss:3.8086 train_time:1219139ms step_avg:169.91ms\nstep:7186/10000 train_loss:3.9320 train_time:1219308ms step_avg:169.91ms\nstep:7187/10000 train_loss:3.8181 train_time:1219479ms step_avg:169.91ms\nstep:7188/10000 train_loss:3.7870 train_time:1219649ms step_avg:169.91ms\nstep:7189/10000 train_loss:3.7520 train_time:1219818ms step_avg:169.91ms\nstep:7190/10000 train_loss:3.9185 train_time:1219987ms step_avg:169.91ms\nstep:7191/10000 train_loss:3.9340 train_time:1220160ms step_avg:169.91ms\nstep:7192/10000 train_loss:3.7894 train_time:1220331ms step_avg:169.92ms\nstep:7193/10000 train_loss:3.8634 train_time:1220501ms step_avg:169.92ms\nstep:7194/10000 train_loss:3.8512 train_time:1220671ms step_avg:169.92ms\nstep:7195/10000 train_loss:3.8306 train_time:1220839ms step_avg:169.91ms\nstep:7196/10000 train_loss:3.8223 train_time:1221006ms step_avg:169.91ms\nstep:7197/10000 train_loss:3.7976 train_time:1221175ms step_avg:169.91ms\nstep:7198/10000 train_loss:3.9284 train_time:1221343ms step_avg:169.91ms\nstep:7199/10000 train_loss:3.8692 train_time:1221515ms step_avg:169.91ms\nstep:7200/10000 train_loss:3.8313 train_time:1221685ms step_avg:169.91ms\nstep:7201/10000 train_loss:3.9261 train_time:1221854ms step_avg:169.91ms\nstep:7202/10000 train_loss:3.9030 train_time:1222022ms step_avg:169.91ms\nstep:7203/10000 train_loss:4.2705 train_time:1222193ms step_avg:169.91ms\nstep:7204/10000 train_loss:3.9251 train_time:1222361ms step_avg:169.91ms\nstep:7205/10000 train_loss:3.8188 train_time:1222530ms step_avg:169.91ms\nstep:7206/10000 train_loss:3.8860 train_time:1222698ms step_avg:169.91ms\nstep:7207/10000 train_loss:3.8334 train_time:1222867ms step_avg:169.91ms\nstep:7208/10000 train_loss:3.8219 train_time:1223037ms step_avg:169.91ms\nstep:7209/10000 train_loss:3.8358 train_time:1223206ms step_avg:169.91ms\nstep:7210/10000 train_loss:3.8825 train_time:1223378ms step_avg:169.91ms\nstep:7211/10000 train_loss:3.9324 train_time:1223547ms step_avg:169.91ms\nstep:7212/10000 train_loss:3.8883 train_time:1223717ms step_avg:169.91ms\nstep:7213/10000 train_loss:3.8611 train_time:1223886ms step_avg:169.91ms\nstep:7214/10000 train_loss:3.7831 train_time:1224056ms step_avg:169.91ms\nstep:7215/10000 train_loss:4.0685 train_time:1224224ms step_avg:169.91ms\nstep:7216/10000 train_loss:3.8332 train_time:1224396ms step_avg:169.91ms\nstep:7217/10000 train_loss:3.8213 train_time:1224563ms step_avg:169.91ms\nstep:7218/10000 train_loss:3.8108 train_time:1224733ms step_avg:169.91ms\nstep:7219/10000 train_loss:3.7877 train_time:1224900ms step_avg:169.91ms\nstep:7220/10000 train_loss:3.8561 train_time:1225069ms step_avg:169.91ms\nstep:7221/10000 train_loss:3.8094 train_time:1225243ms step_avg:169.91ms\nstep:7222/10000 train_loss:4.0101 train_time:1225416ms step_avg:169.91ms\nstep:7223/10000 train_loss:3.9735 train_time:1225582ms step_avg:169.91ms\nstep:7224/10000 train_loss:3.8193 train_time:1225753ms step_avg:169.91ms\nstep:7225/10000 train_loss:3.8425 train_time:1225921ms step_avg:169.91ms\nstep:7226/10000 train_loss:3.7496 train_time:1226090ms step_avg:169.91ms\nstep:7227/10000 train_loss:3.8625 train_time:1226258ms step_avg:169.91ms\nstep:7228/10000 train_loss:3.7467 train_time:1226427ms step_avg:169.91ms\nstep:7229/10000 train_loss:3.8338 train_time:1226598ms step_avg:169.91ms\nstep:7230/10000 train_loss:3.8303 train_time:1226767ms step_avg:169.91ms\nstep:7231/10000 train_loss:4.0612 train_time:1226934ms step_avg:169.91ms\nstep:7232/10000 train_loss:3.9597 train_time:1227102ms step_avg:169.91ms\nstep:7233/10000 train_loss:3.7933 train_time:1227273ms step_avg:169.91ms\nstep:7234/10000 train_loss:3.8454 train_time:1227443ms step_avg:169.91ms\nstep:7235/10000 train_loss:3.7908 train_time:1227613ms step_avg:169.91ms\nstep:7236/10000 train_loss:3.8799 train_time:1227781ms step_avg:169.91ms\nstep:7237/10000 train_loss:3.8188 train_time:1227950ms step_avg:169.91ms\nstep:7238/10000 train_loss:3.9372 train_time:1228119ms step_avg:169.91ms\nstep:7239/10000 train_loss:3.8746 train_time:1228288ms step_avg:169.91ms\nstep:7240/10000 train_loss:3.8676 train_time:1228459ms step_avg:169.91ms\nstep:7241/10000 train_loss:3.8773 train_time:1228631ms step_avg:169.91ms\nstep:7242/10000 train_loss:3.9561 train_time:1228798ms step_avg:169.91ms\nstep:7243/10000 train_loss:3.7818 train_time:1228967ms step_avg:169.91ms\nstep:7244/10000 train_loss:3.9960 train_time:1229135ms step_avg:169.91ms\nstep:7245/10000 train_loss:3.8619 train_time:1229303ms step_avg:169.91ms\nstep:7246/10000 train_loss:3.7740 train_time:1229472ms step_avg:169.91ms\nstep:7247/10000 train_loss:3.7848 train_time:1229642ms step_avg:169.91ms\nstep:7248/10000 train_loss:3.7914 train_time:1229813ms step_avg:169.91ms\nstep:7249/10000 train_loss:3.9090 train_time:1229982ms step_avg:169.91ms\nstep:7250/10000 train_loss:3.9126 train_time:1230152ms step_avg:169.91ms\nstep:7251/10000 train_loss:3.9672 train_time:1230320ms step_avg:169.91ms\nstep:7252/10000 train_loss:3.9099 train_time:1230489ms step_avg:169.91ms\nstep:7253/10000 train_loss:3.9090 train_time:1230660ms step_avg:169.91ms\nstep:7254/10000 train_loss:3.7666 train_time:1230829ms step_avg:169.91ms\nstep:7255/10000 train_loss:3.8913 train_time:1230997ms step_avg:169.91ms\nstep:7256/10000 train_loss:4.0159 train_time:1231167ms step_avg:169.91ms\nstep:7257/10000 train_loss:3.7816 train_time:1231338ms step_avg:169.91ms\nstep:7258/10000 train_loss:3.8460 train_time:1231506ms step_avg:169.91ms\nstep:7259/10000 train_loss:3.8801 train_time:1231677ms step_avg:169.91ms\nstep:7260/10000 train_loss:3.8891 train_time:1231845ms step_avg:169.91ms\nstep:7261/10000 train_loss:3.7915 train_time:1232013ms step_avg:169.91ms\nstep:7262/10000 train_loss:3.8183 train_time:1232180ms step_avg:169.91ms\nstep:7263/10000 train_loss:3.7562 train_time:1232349ms step_avg:169.91ms\nstep:7264/10000 train_loss:3.9193 train_time:1232519ms step_avg:169.91ms\nstep:7265/10000 train_loss:3.8352 train_time:1232688ms step_avg:169.91ms\nstep:7266/10000 train_loss:3.6943 train_time:1232859ms step_avg:169.91ms\nstep:7267/10000 train_loss:3.8897 train_time:1233029ms step_avg:169.91ms\nstep:7268/10000 train_loss:3.9351 train_time:1233199ms step_avg:169.91ms\nstep:7269/10000 train_loss:3.8442 train_time:1233367ms step_avg:169.91ms\nstep:7270/10000 train_loss:3.8831 train_time:1233535ms step_avg:169.91ms\nstep:7271/10000 train_loss:3.8304 train_time:1233705ms step_avg:169.91ms\nstep:7272/10000 train_loss:3.8920 train_time:1233873ms step_avg:169.91ms\nstep:7273/10000 train_loss:3.8601 train_time:1234041ms step_avg:169.91ms\nstep:7274/10000 train_loss:3.6737 train_time:1234212ms step_avg:169.91ms\nstep:7275/10000 train_loss:3.7894 train_time:1234382ms step_avg:169.91ms\nstep:7276/10000 train_loss:3.9486 train_time:1234551ms step_avg:169.91ms\nstep:7277/10000 train_loss:3.8249 train_time:1234719ms step_avg:169.91ms\nstep:7278/10000 train_loss:3.8478 train_time:1234896ms step_avg:169.91ms\nstep:7279/10000 train_loss:3.9178 train_time:1235063ms step_avg:169.91ms\nstep:7280/10000 train_loss:3.8891 train_time:1235231ms step_avg:169.91ms\nstep:7281/10000 train_loss:3.8509 train_time:1235392ms step_avg:169.91ms\nstep:7282/10000 train_loss:3.8177 train_time:1235563ms step_avg:169.91ms\nstep:7283/10000 train_loss:3.8088 train_time:1235733ms step_avg:169.91ms\nstep:7284/10000 train_loss:3.9273 train_time:1235904ms step_avg:169.91ms\nstep:7285/10000 train_loss:3.7490 train_time:1236073ms step_avg:169.91ms\nstep:7286/10000 train_loss:3.9404 train_time:1236242ms step_avg:169.91ms\nstep:7287/10000 train_loss:3.8501 train_time:1236414ms step_avg:169.91ms\nstep:7288/10000 train_loss:3.8607 train_time:1236581ms step_avg:169.91ms\nstep:7289/10000 train_loss:3.8977 train_time:1236749ms step_avg:169.91ms\nstep:7290/10000 train_loss:3.8224 train_time:1236919ms step_avg:169.91ms\nstep:7291/10000 train_loss:3.8954 train_time:1237088ms step_avg:169.91ms\nstep:7292/10000 train_loss:3.8662 train_time:1237259ms step_avg:169.91ms\nstep:7293/10000 train_loss:3.8779 train_time:1237430ms step_avg:169.91ms\nstep:7294/10000 train_loss:3.8052 train_time:1237600ms step_avg:169.91ms\nstep:7295/10000 train_loss:3.8899 train_time:1237769ms step_avg:169.91ms\nstep:7296/10000 train_loss:3.7926 train_time:1237939ms step_avg:169.91ms\nstep:7297/10000 train_loss:3.7137 train_time:1238107ms step_avg:169.91ms\nstep:7298/10000 train_loss:3.8474 train_time:1238277ms step_avg:169.91ms\nstep:7299/10000 train_loss:3.8099 train_time:1238446ms step_avg:169.91ms\nstep:7300/10000 train_loss:3.8033 train_time:1238618ms step_avg:169.91ms\nstep:7301/10000 train_loss:3.9664 train_time:1238788ms step_avg:169.91ms\nstep:7302/10000 train_loss:3.8036 train_time:1238958ms step_avg:169.91ms\nstep:7303/10000 train_loss:3.7836 train_time:1239125ms step_avg:169.91ms\nstep:7304/10000 train_loss:3.6819 train_time:1239294ms step_avg:169.91ms\nstep:7305/10000 train_loss:3.8140 train_time:1239462ms step_avg:169.91ms\nstep:7306/10000 train_loss:3.7981 train_time:1239631ms step_avg:169.91ms\nstep:7307/10000 train_loss:3.9145 train_time:1239799ms step_avg:169.91ms\nstep:7308/10000 train_loss:3.8497 train_time:1239969ms step_avg:169.91ms\nstep:7309/10000 train_loss:4.0412 train_time:1240139ms step_avg:169.91ms\nstep:7310/10000 train_loss:3.9123 train_time:1240308ms step_avg:169.91ms\nstep:7311/10000 train_loss:3.7482 train_time:1240478ms step_avg:169.91ms\nstep:7312/10000 train_loss:3.9749 train_time:1240647ms step_avg:169.91ms\nstep:7313/10000 train_loss:3.9200 train_time:1240822ms step_avg:169.91ms\nstep:7314/10000 train_loss:3.6858 train_time:1240986ms step_avg:169.90ms\nstep:7315/10000 train_loss:3.9443 train_time:1241155ms step_avg:169.90ms\nstep:7316/10000 train_loss:3.9291 train_time:1241323ms step_avg:169.90ms\nstep:7317/10000 train_loss:4.0299 train_time:1241496ms step_avg:169.91ms\nstep:7318/10000 train_loss:4.2357 train_time:1241663ms step_avg:169.90ms\nstep:7319/10000 train_loss:3.7167 train_time:1241832ms step_avg:169.90ms\nstep:7320/10000 train_loss:3.9193 train_time:1242003ms step_avg:169.90ms\nstep:7321/10000 train_loss:3.8137 train_time:1242174ms step_avg:169.90ms\nstep:7322/10000 train_loss:3.8328 train_time:1242344ms step_avg:169.90ms\nstep:7323/10000 train_loss:3.8736 train_time:1242515ms step_avg:169.90ms\nstep:7324/10000 train_loss:3.7733 train_time:1242686ms step_avg:169.91ms\nstep:7325/10000 train_loss:3.8835 train_time:1242855ms step_avg:169.91ms\nstep:7326/10000 train_loss:3.5275 train_time:1243025ms step_avg:169.90ms\nstep:7327/10000 train_loss:3.8258 train_time:1243196ms step_avg:169.91ms\nstep:7328/10000 train_loss:3.8439 train_time:1243364ms step_avg:169.90ms\nstep:7329/10000 train_loss:3.9257 train_time:1243535ms step_avg:169.91ms\nstep:7330/10000 train_loss:3.8582 train_time:1243703ms step_avg:169.90ms\nstep:7331/10000 train_loss:3.9853 train_time:1243872ms step_avg:169.90ms\nstep:7332/10000 train_loss:3.9269 train_time:1244041ms step_avg:169.90ms\nstep:7333/10000 train_loss:3.7664 train_time:1244212ms step_avg:169.90ms\nstep:7334/10000 train_loss:3.7020 train_time:1244383ms step_avg:169.90ms\nstep:7335/10000 train_loss:3.8643 train_time:1244555ms step_avg:169.91ms\nstep:7336/10000 train_loss:3.7399 train_time:1244723ms step_avg:169.90ms\nstep:7337/10000 train_loss:3.8894 train_time:1244892ms step_avg:169.90ms\nstep:7338/10000 train_loss:3.8770 train_time:1245062ms step_avg:169.90ms\nstep:7339/10000 train_loss:3.8320 train_time:1245233ms step_avg:169.90ms\nstep:7340/10000 train_loss:3.8815 train_time:1245401ms step_avg:169.90ms\nstep:7341/10000 train_loss:3.7639 train_time:1245571ms step_avg:169.90ms\nstep:7342/10000 train_loss:3.9445 train_time:1245741ms step_avg:169.90ms\nstep:7343/10000 train_loss:3.8466 train_time:1245910ms step_avg:169.90ms\nstep:7344/10000 train_loss:3.7460 train_time:1246083ms step_avg:169.90ms\nstep:7345/10000 train_loss:3.9175 train_time:1246254ms step_avg:169.91ms\nstep:7346/10000 train_loss:3.8669 train_time:1246423ms step_avg:169.90ms\nstep:7347/10000 train_loss:3.9614 train_time:1246592ms step_avg:169.90ms\nstep:7348/10000 train_loss:4.0942 train_time:1246760ms step_avg:169.90ms\nstep:7349/10000 train_loss:3.9249 train_time:1246931ms step_avg:169.90ms\nstep:7350/10000 train_loss:3.7431 train_time:1247099ms step_avg:169.90ms\nstep:7351/10000 train_loss:3.7790 train_time:1247268ms step_avg:169.90ms\nstep:7352/10000 train_loss:3.8995 train_time:1247438ms step_avg:169.90ms\nstep:7353/10000 train_loss:3.9134 train_time:1247607ms step_avg:169.90ms\nstep:7354/10000 train_loss:3.9245 train_time:1247778ms step_avg:169.90ms\nstep:7355/10000 train_loss:3.7036 train_time:1247948ms step_avg:169.90ms\nstep:7356/10000 train_loss:3.8798 train_time:1248118ms step_avg:169.90ms\nstep:7357/10000 train_loss:3.8746 train_time:1248286ms step_avg:169.90ms\nstep:7358/10000 train_loss:3.8802 train_time:1248455ms step_avg:169.90ms\nstep:7359/10000 train_loss:3.8630 train_time:1248624ms step_avg:169.90ms\nstep:7360/10000 train_loss:3.9857 train_time:1248793ms step_avg:169.90ms\nstep:7361/10000 train_loss:3.8057 train_time:1248962ms step_avg:169.90ms\nstep:7362/10000 train_loss:3.8152 train_time:1249133ms step_avg:169.90ms\nstep:7363/10000 train_loss:3.8397 train_time:1249302ms step_avg:169.90ms\nstep:7364/10000 train_loss:3.8632 train_time:1249474ms step_avg:169.90ms\nstep:7365/10000 train_loss:3.8814 train_time:1249643ms step_avg:169.90ms\nstep:7366/10000 train_loss:3.7930 train_time:1249820ms step_avg:169.90ms\nstep:7367/10000 train_loss:3.7767 train_time:1249988ms step_avg:169.90ms\nstep:7368/10000 train_loss:3.7215 train_time:1250154ms step_avg:169.90ms\nstep:7369/10000 train_loss:3.8293 train_time:1250327ms step_avg:169.90ms\nstep:7370/10000 train_loss:3.6582 train_time:1250492ms step_avg:169.90ms\nstep:7371/10000 train_loss:3.7615 train_time:1250662ms step_avg:169.90ms\nstep:7372/10000 train_loss:3.8845 train_time:1250833ms step_avg:169.90ms\nstep:7373/10000 train_loss:3.8123 train_time:1251001ms step_avg:169.90ms\nstep:7374/10000 train_loss:3.8778 train_time:1251177ms step_avg:169.90ms\nstep:7375/10000 train_loss:3.9287 train_time:1251339ms step_avg:169.90ms\nstep:7376/10000 train_loss:3.7484 train_time:1251506ms step_avg:169.90ms\nstep:7377/10000 train_loss:3.8216 train_time:1251674ms step_avg:169.90ms\nstep:7378/10000 train_loss:3.9170 train_time:1251845ms step_avg:169.90ms\nstep:7379/10000 train_loss:3.7620 train_time:1252014ms step_avg:169.90ms\nstep:7380/10000 train_loss:3.8501 train_time:1252183ms step_avg:169.90ms\nstep:7381/10000 train_loss:3.8015 train_time:1252354ms step_avg:169.90ms\nstep:7382/10000 train_loss:3.7834 train_time:1252522ms step_avg:169.90ms\nstep:7383/10000 train_loss:3.8159 train_time:1252690ms step_avg:169.90ms\nstep:7384/10000 train_loss:3.9144 train_time:1252861ms step_avg:169.90ms\nstep:7385/10000 train_loss:3.8105 train_time:1253032ms step_avg:169.90ms\nstep:7386/10000 train_loss:3.9061 train_time:1253202ms step_avg:169.90ms\nstep:7387/10000 train_loss:3.6096 train_time:1253374ms step_avg:169.90ms\nstep:7388/10000 train_loss:3.7463 train_time:1253543ms step_avg:169.90ms\nstep:7389/10000 train_loss:3.8967 train_time:1253712ms step_avg:169.90ms\nstep:7390/10000 train_loss:3.6536 train_time:1253883ms step_avg:169.90ms\nstep:7391/10000 train_loss:3.8442 train_time:1254052ms step_avg:169.90ms\nstep:7392/10000 train_loss:3.8161 train_time:1254222ms step_avg:169.90ms\nstep:7393/10000 train_loss:3.7763 train_time:1254393ms step_avg:169.90ms\nstep:7394/10000 train_loss:3.8219 train_time:1254562ms step_avg:169.90ms\nstep:7395/10000 train_loss:3.8285 train_time:1254731ms step_avg:169.90ms\nstep:7396/10000 train_loss:3.8281 train_time:1254899ms step_avg:169.90ms\nstep:7397/10000 train_loss:3.8689 train_time:1255067ms step_avg:169.90ms\nstep:7398/10000 train_loss:3.7815 train_time:1255237ms step_avg:169.90ms\nstep:7399/10000 train_loss:3.8094 train_time:1255406ms step_avg:169.90ms\nstep:7400/10000 train_loss:3.8170 train_time:1255576ms step_avg:169.90ms\nstep:7401/10000 train_loss:3.9224 train_time:1255742ms step_avg:169.90ms\nstep:7402/10000 train_loss:3.8410 train_time:1255911ms step_avg:169.90ms\nstep:7403/10000 train_loss:3.7992 train_time:1256080ms step_avg:169.90ms\nstep:7404/10000 train_loss:3.6957 train_time:1256251ms step_avg:169.90ms\nstep:7405/10000 train_loss:3.8553 train_time:1256419ms step_avg:169.90ms\nstep:7406/10000 train_loss:3.8523 train_time:1256588ms step_avg:169.90ms\nstep:7407/10000 train_loss:3.9000 train_time:1256756ms step_avg:169.90ms\nstep:7408/10000 train_loss:3.9653 train_time:1256924ms step_avg:169.90ms\nstep:7409/10000 train_loss:3.8720 train_time:1257093ms step_avg:169.90ms\nstep:7410/10000 train_loss:3.8528 train_time:1257262ms step_avg:169.90ms\nstep:7411/10000 train_loss:3.8016 train_time:1257432ms step_avg:169.90ms\nstep:7412/10000 train_loss:3.7407 train_time:1257602ms step_avg:169.90ms\nstep:7413/10000 train_loss:3.9196 train_time:1257773ms step_avg:169.90ms\nstep:7414/10000 train_loss:3.8934 train_time:1257941ms step_avg:169.90ms\nstep:7415/10000 train_loss:3.7505 train_time:1258110ms step_avg:169.90ms\nstep:7416/10000 train_loss:3.8089 train_time:1258278ms step_avg:169.90ms\nstep:7417/10000 train_loss:3.7658 train_time:1258447ms step_avg:169.90ms\nstep:7418/10000 train_loss:3.8318 train_time:1258618ms step_avg:169.90ms\nstep:7419/10000 train_loss:3.7282 train_time:1258787ms step_avg:169.90ms\nstep:7420/10000 train_loss:3.7774 train_time:1258956ms step_avg:169.90ms\nstep:7421/10000 train_loss:3.8446 train_time:1259125ms step_avg:169.90ms\nstep:7422/10000 train_loss:3.8720 train_time:1259294ms step_avg:169.90ms\nstep:7423/10000 train_loss:3.7624 train_time:1259462ms step_avg:169.90ms\nstep:7424/10000 train_loss:3.9631 train_time:1259633ms step_avg:169.90ms\nstep:7425/10000 train_loss:3.5479 train_time:1259806ms step_avg:169.90ms\nstep:7426/10000 train_loss:3.8523 train_time:1259972ms step_avg:169.90ms\nstep:7427/10000 train_loss:3.8355 train_time:1260141ms step_avg:169.90ms\nstep:7428/10000 train_loss:3.9179 train_time:1260312ms step_avg:169.90ms\nstep:7429/10000 train_loss:3.9122 train_time:1260480ms step_avg:169.90ms\nstep:7430/10000 train_loss:3.8607 train_time:1260649ms step_avg:169.90ms\nstep:7431/10000 train_loss:3.8405 train_time:1260820ms step_avg:169.90ms\nstep:7432/10000 train_loss:3.8258 train_time:1260988ms step_avg:169.90ms\nstep:7433/10000 train_loss:3.8136 train_time:1261160ms step_avg:169.90ms\nstep:7434/10000 train_loss:3.8078 train_time:1261328ms step_avg:169.90ms\nstep:7435/10000 train_loss:3.9046 train_time:1261499ms step_avg:169.90ms\nstep:7436/10000 train_loss:3.7736 train_time:1261668ms step_avg:169.90ms\nstep:7437/10000 train_loss:3.8953 train_time:1261837ms step_avg:169.90ms\nstep:7438/10000 train_loss:3.7869 train_time:1262008ms step_avg:169.90ms\nstep:7439/10000 train_loss:3.7388 train_time:1262179ms step_avg:169.90ms\nstep:7440/10000 train_loss:3.8248 train_time:1262347ms step_avg:169.90ms\nstep:7441/10000 train_loss:3.9746 train_time:1262517ms step_avg:169.90ms\nstep:7442/10000 train_loss:3.7657 train_time:1262686ms step_avg:169.90ms\nstep:7443/10000 train_loss:3.8131 train_time:1262862ms step_avg:169.90ms\nstep:7444/10000 train_loss:4.0303 train_time:1263029ms step_avg:169.90ms\nstep:7445/10000 train_loss:3.7789 train_time:1263203ms step_avg:169.90ms\nstep:7446/10000 train_loss:3.8318 train_time:1263370ms step_avg:169.90ms\nstep:7447/10000 train_loss:3.8318 train_time:1263541ms step_avg:169.90ms\nstep:7448/10000 train_loss:3.9029 train_time:1263707ms step_avg:169.90ms\nstep:7449/10000 train_loss:3.8381 train_time:1263870ms step_avg:169.90ms\nstep:7450/10000 train_loss:4.0663 train_time:1264041ms step_avg:169.90ms\nstep:7451/10000 train_loss:3.8799 train_time:1264211ms step_avg:169.90ms\nstep:7452/10000 train_loss:3.8308 train_time:1264379ms step_avg:169.90ms\nstep:7453/10000 train_loss:3.7733 train_time:1264549ms step_avg:169.90ms\nstep:7454/10000 train_loss:3.7764 train_time:1264718ms step_avg:169.90ms\nstep:7455/10000 train_loss:3.8721 train_time:1264887ms step_avg:169.90ms\nstep:7456/10000 train_loss:3.8679 train_time:1265059ms step_avg:169.90ms\nstep:7457/10000 train_loss:3.8151 train_time:1265226ms step_avg:169.90ms\nstep:7458/10000 train_loss:3.8740 train_time:1265393ms step_avg:169.90ms\nstep:7459/10000 train_loss:3.6979 train_time:1265561ms step_avg:169.90ms\nstep:7460/10000 train_loss:3.7969 train_time:1265730ms step_avg:169.90ms\nstep:7461/10000 train_loss:3.7888 train_time:1265899ms step_avg:169.90ms\nstep:7462/10000 train_loss:3.7696 train_time:1266069ms step_avg:169.90ms\nstep:7463/10000 train_loss:3.7641 train_time:1266241ms step_avg:169.90ms\nstep:7464/10000 train_loss:3.7653 train_time:1266410ms step_avg:169.90ms\nstep:7465/10000 train_loss:4.0111 train_time:1266580ms step_avg:169.90ms\nstep:7466/10000 train_loss:3.8245 train_time:1266749ms step_avg:169.90ms\nstep:7467/10000 train_loss:3.8179 train_time:1266918ms step_avg:169.90ms\nstep:7468/10000 train_loss:3.8374 train_time:1267089ms step_avg:169.90ms\nstep:7469/10000 train_loss:4.0133 train_time:1267259ms step_avg:169.90ms\nstep:7470/10000 train_loss:3.8896 train_time:1267428ms step_avg:169.90ms\nstep:7471/10000 train_loss:3.8563 train_time:1267600ms step_avg:169.90ms\nstep:7472/10000 train_loss:3.8635 train_time:1267769ms step_avg:169.90ms\nstep:7473/10000 train_loss:3.7803 train_time:1267938ms step_avg:169.90ms\nstep:7474/10000 train_loss:3.7723 train_time:1268106ms step_avg:169.90ms\nstep:7475/10000 train_loss:4.1106 train_time:1268277ms step_avg:169.90ms\nstep:7476/10000 train_loss:3.6417 train_time:1268446ms step_avg:169.90ms\nstep:7477/10000 train_loss:3.7777 train_time:1268616ms step_avg:169.90ms\nstep:7478/10000 train_loss:3.9873 train_time:1268785ms step_avg:169.90ms\nstep:7479/10000 train_loss:3.8337 train_time:1268955ms step_avg:169.90ms\nstep:7480/10000 train_loss:3.7559 train_time:1269123ms step_avg:169.90ms\nstep:7481/10000 train_loss:3.7547 train_time:1269292ms step_avg:169.90ms\nstep:7482/10000 train_loss:3.8110 train_time:1269461ms step_avg:169.90ms\nstep:7483/10000 train_loss:3.8039 train_time:1269631ms step_avg:169.90ms\nstep:7484/10000 train_loss:3.7054 train_time:1269798ms step_avg:169.90ms\nstep:7485/10000 train_loss:4.0424 train_time:1269970ms step_avg:169.90ms\nstep:7486/10000 train_loss:3.8801 train_time:1270142ms step_avg:169.90ms\nstep:7487/10000 train_loss:3.8099 train_time:1270312ms step_avg:169.90ms\nstep:7488/10000 train_loss:3.7217 train_time:1270479ms step_avg:169.90ms\nstep:7489/10000 train_loss:3.9810 train_time:1270648ms step_avg:169.90ms\nstep:7490/10000 train_loss:3.9048 train_time:1270819ms step_avg:169.90ms\nstep:7491/10000 train_loss:3.8506 train_time:1270987ms step_avg:169.90ms\nstep:7492/10000 train_loss:3.8765 train_time:1271157ms step_avg:169.90ms\nstep:7493/10000 train_loss:3.6483 train_time:1271326ms step_avg:169.90ms\nstep:7494/10000 train_loss:3.7566 train_time:1271496ms step_avg:169.90ms\nstep:7495/10000 train_loss:3.8501 train_time:1271665ms step_avg:169.90ms\nstep:7496/10000 train_loss:3.8562 train_time:1271834ms step_avg:169.89ms\nstep:7497/10000 train_loss:3.9066 train_time:1272003ms step_avg:169.89ms\nstep:7498/10000 train_loss:3.8686 train_time:1272174ms step_avg:169.90ms\nstep:7499/10000 train_loss:3.7930 train_time:1272343ms step_avg:169.89ms\nstep:7500/10000 train_loss:3.8117 train_time:1272513ms step_avg:169.89ms\nstep:7501/10000 train_loss:3.8456 train_time:1272682ms step_avg:169.89ms\nstep:7502/10000 train_loss:3.8060 train_time:1272851ms step_avg:169.89ms\nstep:7503/10000 train_loss:3.9084 train_time:1273020ms step_avg:169.89ms\nstep:7504/10000 train_loss:3.8097 train_time:1273189ms step_avg:169.89ms\nstep:7505/10000 train_loss:3.9203 train_time:1273358ms step_avg:169.89ms\nstep:7506/10000 train_loss:3.7640 train_time:1273524ms step_avg:169.89ms\nstep:7507/10000 train_loss:3.6116 train_time:1273695ms step_avg:169.89ms\nstep:7508/10000 train_loss:3.7492 train_time:1273864ms step_avg:169.89ms\nstep:7509/10000 train_loss:3.9012 train_time:1274035ms step_avg:169.89ms\nstep:7510/10000 train_loss:3.7769 train_time:1274204ms step_avg:169.89ms\nstep:7511/10000 train_loss:3.8297 train_time:1274374ms step_avg:169.89ms\nstep:7512/10000 train_loss:3.8587 train_time:1274542ms step_avg:169.89ms\nstep:7513/10000 train_loss:3.7217 train_time:1274715ms step_avg:169.89ms\nstep:7514/10000 train_loss:3.8144 train_time:1274886ms step_avg:169.89ms\nstep:7515/10000 train_loss:3.8639 train_time:1275056ms step_avg:169.89ms\nstep:7516/10000 train_loss:3.8578 train_time:1275224ms step_avg:169.89ms\nstep:7517/10000 train_loss:3.9655 train_time:1275395ms step_avg:169.89ms\nstep:7518/10000 train_loss:4.1768 train_time:1275564ms step_avg:169.89ms\nstep:7519/10000 train_loss:3.7819 train_time:1275736ms step_avg:169.89ms\nstep:7520/10000 train_loss:3.7870 train_time:1275905ms step_avg:169.89ms\nstep:7521/10000 train_loss:3.7204 train_time:1276076ms step_avg:169.89ms\nstep:7522/10000 train_loss:3.9065 train_time:1276244ms step_avg:169.89ms\nstep:7523/10000 train_loss:3.7199 train_time:1276413ms step_avg:169.89ms\nstep:7524/10000 train_loss:3.8235 train_time:1276579ms step_avg:169.89ms\nstep:7525/10000 train_loss:3.9616 train_time:1276746ms step_avg:169.89ms\nstep:7526/10000 train_loss:3.8477 train_time:1276918ms step_avg:169.89ms\nstep:7527/10000 train_loss:3.8784 train_time:1277090ms step_avg:169.89ms\nstep:7528/10000 train_loss:3.9310 train_time:1277262ms step_avg:169.89ms\nstep:7529/10000 train_loss:3.8410 train_time:1277433ms step_avg:169.89ms\nstep:7530/10000 train_loss:3.9464 train_time:1277601ms step_avg:169.89ms\nstep:7531/10000 train_loss:3.9598 train_time:1277771ms step_avg:169.89ms\nstep:7532/10000 train_loss:3.8031 train_time:1277940ms step_avg:169.89ms\nstep:7533/10000 train_loss:4.2051 train_time:1278109ms step_avg:169.89ms\nstep:7534/10000 train_loss:3.8891 train_time:1278280ms step_avg:169.89ms\nstep:7535/10000 train_loss:3.8649 train_time:1278452ms step_avg:169.89ms\nstep:7536/10000 train_loss:3.8718 train_time:1278623ms step_avg:169.89ms\nstep:7537/10000 train_loss:3.8566 train_time:1278791ms step_avg:169.89ms\nstep:7538/10000 train_loss:3.7559 train_time:1278958ms step_avg:169.89ms\nstep:7539/10000 train_loss:3.8252 train_time:1279127ms step_avg:169.89ms\nstep:7540/10000 train_loss:3.7720 train_time:1279297ms step_avg:169.89ms\nstep:7541/10000 train_loss:3.8214 train_time:1279467ms step_avg:169.89ms\nstep:7542/10000 train_loss:3.7419 train_time:1279637ms step_avg:169.89ms\nstep:7543/10000 train_loss:3.7605 train_time:1279805ms step_avg:169.89ms\nstep:7544/10000 train_loss:3.8531 train_time:1279975ms step_avg:169.89ms\nstep:7545/10000 train_loss:3.8853 train_time:1280143ms step_avg:169.89ms\nstep:7546/10000 train_loss:3.7842 train_time:1280314ms step_avg:169.89ms\nstep:7547/10000 train_loss:3.8235 train_time:1280482ms step_avg:169.89ms\nstep:7548/10000 train_loss:3.8516 train_time:1280653ms step_avg:169.89ms\nstep:7549/10000 train_loss:3.8576 train_time:1280822ms step_avg:169.89ms\nstep:7550/10000 train_loss:3.8125 train_time:1280992ms step_avg:169.89ms\nstep:7551/10000 train_loss:3.8073 train_time:1281160ms step_avg:169.89ms\nstep:7552/10000 train_loss:3.5900 train_time:1281331ms step_avg:169.89ms\nstep:7553/10000 train_loss:3.9414 train_time:1281499ms step_avg:169.89ms\nstep:7554/10000 train_loss:4.0666 train_time:1281666ms step_avg:169.89ms\nstep:7555/10000 train_loss:3.6866 train_time:1281836ms step_avg:169.89ms\nstep:7556/10000 train_loss:4.0256 train_time:1282004ms step_avg:169.89ms\nstep:7557/10000 train_loss:3.8347 train_time:1282174ms step_avg:169.89ms\nstep:7558/10000 train_loss:3.9349 train_time:1282343ms step_avg:169.89ms\nstep:7559/10000 train_loss:3.8677 train_time:1282515ms step_avg:169.89ms\nstep:7560/10000 train_loss:3.7290 train_time:1282683ms step_avg:169.89ms\nstep:7561/10000 train_loss:3.8545 train_time:1282853ms step_avg:169.89ms\nstep:7562/10000 train_loss:3.7540 train_time:1283023ms step_avg:169.89ms\nstep:7563/10000 train_loss:3.9070 train_time:1283196ms step_avg:169.89ms\nstep:7564/10000 train_loss:3.7724 train_time:1283364ms step_avg:169.89ms\nstep:7565/10000 train_loss:3.8427 train_time:1283535ms step_avg:169.89ms\nstep:7566/10000 train_loss:3.7810 train_time:1283704ms step_avg:169.89ms\nstep:7567/10000 train_loss:3.7940 train_time:1283875ms step_avg:169.89ms\nstep:7568/10000 train_loss:3.8019 train_time:1284048ms step_avg:169.89ms\nstep:7569/10000 train_loss:3.8039 train_time:1284211ms step_avg:169.89ms\nstep:7570/10000 train_loss:3.7377 train_time:1284382ms step_avg:169.89ms\nstep:7571/10000 train_loss:3.7775 train_time:1284554ms step_avg:169.89ms\nstep:7572/10000 train_loss:3.8500 train_time:1284721ms step_avg:169.89ms\nstep:7573/10000 train_loss:3.8589 train_time:1284890ms step_avg:169.89ms\nstep:7574/10000 train_loss:3.7422 train_time:1285059ms step_avg:169.89ms\nstep:7575/10000 train_loss:3.8160 train_time:1285229ms step_avg:169.89ms\nstep:7576/10000 train_loss:3.7134 train_time:1285399ms step_avg:169.89ms\nstep:7577/10000 train_loss:3.9022 train_time:1285569ms step_avg:169.89ms\nstep:7578/10000 train_loss:3.7999 train_time:1285739ms step_avg:169.89ms\nstep:7579/10000 train_loss:3.8294 train_time:1285907ms step_avg:169.89ms\nstep:7580/10000 train_loss:3.9718 train_time:1286078ms step_avg:169.89ms\nstep:7581/10000 train_loss:3.7621 train_time:1286247ms step_avg:169.89ms\nstep:7582/10000 train_loss:3.9309 train_time:1286417ms step_avg:169.89ms\nstep:7583/10000 train_loss:3.8512 train_time:1286585ms step_avg:169.89ms\nstep:7584/10000 train_loss:3.7477 train_time:1286754ms step_avg:169.89ms\nstep:7585/10000 train_loss:3.6530 train_time:1286923ms step_avg:169.89ms\nstep:7586/10000 train_loss:3.8418 train_time:1287093ms step_avg:169.89ms\nstep:7587/10000 train_loss:3.7822 train_time:1287262ms step_avg:169.89ms\nstep:7588/10000 train_loss:3.8354 train_time:1287433ms step_avg:169.89ms\nstep:7589/10000 train_loss:3.8318 train_time:1287602ms step_avg:169.89ms\nstep:7590/10000 train_loss:3.9519 train_time:1287772ms step_avg:169.89ms\nstep:7591/10000 train_loss:3.8289 train_time:1287939ms step_avg:169.89ms\nstep:7592/10000 train_loss:3.9044 train_time:1288109ms step_avg:169.89ms\nstep:7593/10000 train_loss:3.8221 train_time:1288277ms step_avg:169.89ms\nstep:7594/10000 train_loss:3.8464 train_time:1288446ms step_avg:169.89ms\nstep:7595/10000 train_loss:3.8392 train_time:1288616ms step_avg:169.89ms\nstep:7596/10000 train_loss:3.6975 train_time:1288784ms step_avg:169.89ms\nstep:7597/10000 train_loss:3.7740 train_time:1288954ms step_avg:169.89ms\nstep:7598/10000 train_loss:3.8426 train_time:1289123ms step_avg:169.89ms\nstep:7599/10000 train_loss:3.7167 train_time:1289293ms step_avg:169.89ms\nstep:7600/10000 train_loss:3.8413 train_time:1289461ms step_avg:169.89ms\nstep:7601/10000 train_loss:3.7980 train_time:1289630ms step_avg:169.89ms\nstep:7602/10000 train_loss:4.0194 train_time:1289796ms step_avg:169.89ms\nstep:7603/10000 train_loss:3.7314 train_time:1289966ms step_avg:169.89ms\nstep:7604/10000 train_loss:3.8248 train_time:1290136ms step_avg:169.89ms\nstep:7605/10000 train_loss:3.9720 train_time:1290308ms step_avg:169.89ms\nstep:7606/10000 train_loss:3.8631 train_time:1290478ms step_avg:169.89ms\nstep:7607/10000 train_loss:3.9438 train_time:1290647ms step_avg:169.89ms\nstep:7608/10000 train_loss:3.8978 train_time:1290817ms step_avg:169.89ms\nstep:7609/10000 train_loss:3.7983 train_time:1290986ms step_avg:169.89ms\nstep:7610/10000 train_loss:3.8412 train_time:1291155ms step_avg:169.89ms\nstep:7611/10000 train_loss:3.7212 train_time:1291324ms step_avg:169.89ms\nstep:7612/10000 train_loss:3.6854 train_time:1291495ms step_avg:169.89ms\nstep:7613/10000 train_loss:3.6647 train_time:1291673ms step_avg:169.89ms\nstep:7614/10000 train_loss:3.9551 train_time:1291841ms step_avg:169.89ms\nstep:7615/10000 train_loss:3.7763 train_time:1292006ms step_avg:169.89ms\nstep:7616/10000 train_loss:3.9133 train_time:1292171ms step_avg:169.89ms\nstep:7617/10000 train_loss:3.8479 train_time:1292340ms step_avg:169.89ms\nstep:7618/10000 train_loss:3.7850 train_time:1292511ms step_avg:169.89ms\nstep:7619/10000 train_loss:3.7902 train_time:1292680ms step_avg:169.89ms\nstep:7620/10000 train_loss:4.0970 train_time:1293078ms step_avg:169.92ms\nstep:7621/10000 train_loss:3.8796 train_time:1293243ms step_avg:169.92ms\nstep:7622/10000 train_loss:3.7979 train_time:1293413ms step_avg:169.92ms\nstep:7623/10000 train_loss:3.8402 train_time:1293581ms step_avg:169.92ms\nstep:7624/10000 train_loss:3.7960 train_time:1293748ms step_avg:169.92ms\nstep:7625/10000 train_loss:3.6713 train_time:1293917ms step_avg:169.92ms\nstep:7626/10000 train_loss:3.9465 train_time:1294091ms step_avg:169.92ms\nstep:7627/10000 train_loss:3.7981 train_time:1294264ms step_avg:169.92ms\nstep:7628/10000 train_loss:3.7819 train_time:1294433ms step_avg:169.92ms\nstep:7629/10000 train_loss:3.8876 train_time:1294602ms step_avg:169.92ms\nstep:7630/10000 train_loss:3.7786 train_time:1294770ms step_avg:169.92ms\nstep:7631/10000 train_loss:3.9301 train_time:1294941ms step_avg:169.92ms\nstep:7632/10000 train_loss:3.7903 train_time:1295114ms step_avg:169.92ms\nstep:7633/10000 train_loss:3.9497 train_time:1295283ms step_avg:169.92ms\nstep:7634/10000 train_loss:4.0463 train_time:1295454ms step_avg:169.92ms\nstep:7635/10000 train_loss:3.7662 train_time:1295621ms step_avg:169.92ms\nstep:7636/10000 train_loss:3.8063 train_time:1295789ms step_avg:169.92ms\nstep:7637/10000 train_loss:3.8850 train_time:1295955ms step_avg:169.92ms\nstep:7638/10000 train_loss:3.7900 train_time:1296124ms step_avg:169.92ms\nstep:7639/10000 train_loss:3.7874 train_time:1296292ms step_avg:169.92ms\nstep:7640/10000 train_loss:3.8152 train_time:1296461ms step_avg:169.92ms\nstep:7641/10000 train_loss:3.8245 train_time:1296632ms step_avg:169.92ms\nstep:7642/10000 train_loss:3.7719 train_time:1296801ms step_avg:169.92ms\nstep:7643/10000 train_loss:3.7693 train_time:1296972ms step_avg:169.92ms\nstep:7644/10000 train_loss:3.9172 train_time:1297143ms step_avg:169.92ms\nstep:7645/10000 train_loss:3.8354 train_time:1297314ms step_avg:169.92ms\nstep:7646/10000 train_loss:3.8665 train_time:1297483ms step_avg:169.92ms\nstep:7647/10000 train_loss:3.6965 train_time:1297652ms step_avg:169.92ms\nstep:7648/10000 train_loss:3.8316 train_time:1297821ms step_avg:169.92ms\nstep:7649/10000 train_loss:3.8305 train_time:1297992ms step_avg:169.92ms\nstep:7650/10000 train_loss:3.7574 train_time:1298161ms step_avg:169.92ms\nstep:7651/10000 train_loss:3.8505 train_time:1298332ms step_avg:169.92ms\nstep:7652/10000 train_loss:3.8528 train_time:1298502ms step_avg:169.92ms\nstep:7653/10000 train_loss:3.8189 train_time:1298673ms step_avg:169.92ms\nstep:7654/10000 train_loss:3.8084 train_time:1298842ms step_avg:169.92ms\nstep:7655/10000 train_loss:3.8068 train_time:1299013ms step_avg:169.92ms\nstep:7656/10000 train_loss:3.8832 train_time:1299183ms step_avg:169.92ms\nstep:7657/10000 train_loss:3.7856 train_time:1299352ms step_avg:169.92ms\nstep:7658/10000 train_loss:3.8934 train_time:1299523ms step_avg:169.92ms\nstep:7659/10000 train_loss:3.7699 train_time:1299694ms step_avg:169.92ms\nstep:7660/10000 train_loss:3.8017 train_time:1299861ms step_avg:169.92ms\nstep:7661/10000 train_loss:3.7334 train_time:1300031ms step_avg:169.92ms\nstep:7662/10000 train_loss:3.8287 train_time:1300201ms step_avg:169.92ms\nstep:7663/10000 train_loss:3.6876 train_time:1300373ms step_avg:169.92ms\nstep:7664/10000 train_loss:3.9332 train_time:1300544ms step_avg:169.92ms\nstep:7665/10000 train_loss:3.8570 train_time:1300715ms step_avg:169.92ms\nstep:7666/10000 train_loss:3.6923 train_time:1300881ms step_avg:169.92ms\nstep:7667/10000 train_loss:3.7178 train_time:1301047ms step_avg:169.92ms\nstep:7668/10000 train_loss:3.7103 train_time:1301219ms step_avg:169.92ms\nstep:7669/10000 train_loss:3.7886 train_time:1301388ms step_avg:169.92ms\nstep:7670/10000 train_loss:3.6197 train_time:1301557ms step_avg:169.92ms\nstep:7671/10000 train_loss:3.9315 train_time:1301727ms step_avg:169.92ms\nstep:7672/10000 train_loss:4.2796 train_time:1301896ms step_avg:169.92ms\nstep:7673/10000 train_loss:3.7945 train_time:1302064ms step_avg:169.92ms\nstep:7674/10000 train_loss:3.7869 train_time:1302234ms step_avg:169.92ms\nstep:7675/10000 train_loss:3.8111 train_time:1302402ms step_avg:169.92ms\nstep:7676/10000 train_loss:5.3140 train_time:1302573ms step_avg:169.92ms\nstep:7677/10000 train_loss:3.8332 train_time:1302742ms step_avg:169.92ms\nstep:7678/10000 train_loss:4.0254 train_time:1302913ms step_avg:169.92ms\nstep:7679/10000 train_loss:3.7568 train_time:1303080ms step_avg:169.92ms\nstep:7680/10000 train_loss:3.7469 train_time:1303248ms step_avg:169.92ms\nstep:7681/10000 train_loss:3.6826 train_time:1303416ms step_avg:169.91ms\nstep:7682/10000 train_loss:3.9705 train_time:1303584ms step_avg:169.91ms\nstep:7683/10000 train_loss:3.8308 train_time:1303754ms step_avg:169.91ms\nstep:7684/10000 train_loss:3.6123 train_time:1303924ms step_avg:169.91ms\nstep:7685/10000 train_loss:3.8899 train_time:1304093ms step_avg:169.91ms\nstep:7686/10000 train_loss:3.9216 train_time:1304261ms step_avg:169.91ms\nstep:7687/10000 train_loss:3.7827 train_time:1304429ms step_avg:169.91ms\nstep:7688/10000 train_loss:3.8508 train_time:1304597ms step_avg:169.91ms\nstep:7689/10000 train_loss:3.7835 train_time:1304768ms step_avg:169.91ms\nstep:7690/10000 train_loss:3.8046 train_time:1304940ms step_avg:169.91ms\nstep:7691/10000 train_loss:3.7993 train_time:1305112ms step_avg:169.91ms\nstep:7692/10000 train_loss:3.9026 train_time:1305282ms step_avg:169.91ms\nstep:7693/10000 train_loss:3.8392 train_time:1305451ms step_avg:169.91ms\nstep:7694/10000 train_loss:3.9953 train_time:1305619ms step_avg:169.91ms\nstep:7695/10000 train_loss:3.8662 train_time:1305788ms step_avg:169.91ms\nstep:7696/10000 train_loss:3.8594 train_time:1305956ms step_avg:169.91ms\nstep:7697/10000 train_loss:3.8055 train_time:1306124ms step_avg:169.91ms\nstep:7698/10000 train_loss:3.8335 train_time:1306292ms step_avg:169.91ms\nstep:7699/10000 train_loss:3.8224 train_time:1306461ms step_avg:169.91ms\nstep:7700/10000 train_loss:3.7489 train_time:1306631ms step_avg:169.91ms\nstep:7701/10000 train_loss:3.7761 train_time:1306799ms step_avg:169.91ms\nstep:7702/10000 train_loss:3.7510 train_time:1306968ms step_avg:169.91ms\nstep:7703/10000 train_loss:3.9203 train_time:1307136ms step_avg:169.91ms\nstep:7704/10000 train_loss:3.7393 train_time:1307304ms step_avg:169.91ms\nstep:7705/10000 train_loss:3.8669 train_time:1307473ms step_avg:169.91ms\nstep:7706/10000 train_loss:3.9334 train_time:1307645ms step_avg:169.91ms\nstep:7707/10000 train_loss:3.7376 train_time:1307810ms step_avg:169.91ms\nstep:7708/10000 train_loss:3.8522 train_time:1307980ms step_avg:169.91ms\nstep:7709/10000 train_loss:3.7275 train_time:1308149ms step_avg:169.91ms\nstep:7710/10000 train_loss:3.8994 train_time:1308317ms step_avg:169.91ms\nstep:7711/10000 train_loss:3.8548 train_time:1308486ms step_avg:169.91ms\nstep:7712/10000 train_loss:3.7659 train_time:1308657ms step_avg:169.91ms\nstep:7713/10000 train_loss:3.8912 train_time:1308826ms step_avg:169.91ms\nstep:7714/10000 train_loss:3.7287 train_time:1308997ms step_avg:169.91ms\nstep:7715/10000 train_loss:3.8057 train_time:1309164ms step_avg:169.91ms\nstep:7716/10000 train_loss:3.8126 train_time:1309333ms step_avg:169.91ms\nstep:7717/10000 train_loss:3.8445 train_time:1309503ms step_avg:169.91ms\nstep:7718/10000 train_loss:3.7772 train_time:1309673ms step_avg:169.91ms\nstep:7719/10000 train_loss:3.8457 train_time:1309844ms step_avg:169.91ms\nstep:7720/10000 train_loss:3.8711 train_time:1310012ms step_avg:169.91ms\nstep:7721/10000 train_loss:3.7907 train_time:1310182ms step_avg:169.91ms\nstep:7722/10000 train_loss:3.6859 train_time:1310352ms step_avg:169.91ms\nstep:7723/10000 train_loss:3.7730 train_time:1310520ms step_avg:169.91ms\nstep:7724/10000 train_loss:3.9315 train_time:1310690ms step_avg:169.91ms\nstep:7725/10000 train_loss:3.8403 train_time:1310858ms step_avg:169.91ms\nstep:7726/10000 train_loss:3.7634 train_time:1311027ms step_avg:169.91ms\nstep:7727/10000 train_loss:3.8703 train_time:1311199ms step_avg:169.91ms\nstep:7728/10000 train_loss:3.7152 train_time:1311369ms step_avg:169.91ms\nstep:7729/10000 train_loss:4.1361 train_time:1311539ms step_avg:169.91ms\nstep:7730/10000 train_loss:3.6953 train_time:1311707ms step_avg:169.91ms\nstep:7731/10000 train_loss:4.0310 train_time:1311877ms step_avg:169.91ms\nstep:7732/10000 train_loss:3.8187 train_time:1312046ms step_avg:169.91ms\nstep:7733/10000 train_loss:3.8969 train_time:1312216ms step_avg:169.91ms\nstep:7734/10000 train_loss:3.7566 train_time:1312385ms step_avg:169.91ms\nstep:7735/10000 train_loss:3.8414 train_time:1312555ms step_avg:169.91ms\nstep:7736/10000 train_loss:3.9767 train_time:1312723ms step_avg:169.91ms\nstep:7737/10000 train_loss:3.8904 train_time:1312895ms step_avg:169.91ms\nstep:7738/10000 train_loss:3.8851 train_time:1313062ms step_avg:169.91ms\nstep:7739/10000 train_loss:4.0199 train_time:1313230ms step_avg:169.91ms\nstep:7740/10000 train_loss:3.8126 train_time:1313400ms step_avg:169.91ms\nstep:7741/10000 train_loss:3.8564 train_time:1313569ms step_avg:169.91ms\nstep:7742/10000 train_loss:3.7678 train_time:1313742ms step_avg:169.91ms\nstep:7743/10000 train_loss:3.7416 train_time:1313912ms step_avg:169.91ms\nstep:7744/10000 train_loss:3.6614 train_time:1314081ms step_avg:169.91ms\nstep:7745/10000 train_loss:3.7161 train_time:1314252ms step_avg:169.91ms\nstep:7746/10000 train_loss:3.8912 train_time:1314421ms step_avg:169.91ms\nstep:7747/10000 train_loss:3.6749 train_time:1314590ms step_avg:169.91ms\nstep:7748/10000 train_loss:5.6038 train_time:1314760ms step_avg:169.91ms\nstep:7749/10000 train_loss:3.9305 train_time:1314931ms step_avg:169.91ms\nstep:7750/10000 train_loss:3.8176 train_time:1315099ms step_avg:169.91ms\nstep:7751/10000 train_loss:3.7476 train_time:1315269ms step_avg:169.91ms\nstep:7752/10000 train_loss:3.8935 train_time:1315439ms step_avg:169.91ms\nstep:7753/10000 train_loss:3.7017 train_time:1315609ms step_avg:169.91ms\nstep:7754/10000 train_loss:3.8324 train_time:1315780ms step_avg:169.91ms\nstep:7755/10000 train_loss:3.8156 train_time:1315949ms step_avg:169.91ms\nstep:7756/10000 train_loss:3.8235 train_time:1316118ms step_avg:169.91ms\nstep:7757/10000 train_loss:3.8514 train_time:1316288ms step_avg:169.91ms\nstep:7758/10000 train_loss:3.8847 train_time:1316461ms step_avg:169.91ms\nstep:7759/10000 train_loss:3.8284 train_time:1316632ms step_avg:169.91ms\nstep:7760/10000 train_loss:4.0987 train_time:1316802ms step_avg:169.91ms\nstep:7761/10000 train_loss:3.7936 train_time:1316975ms step_avg:169.91ms\nstep:7762/10000 train_loss:3.8204 train_time:1317141ms step_avg:169.91ms\nstep:7763/10000 train_loss:3.8790 train_time:1317309ms step_avg:169.91ms\nstep:7764/10000 train_loss:3.8492 train_time:1317479ms step_avg:169.91ms\nstep:7765/10000 train_loss:3.8039 train_time:1317647ms step_avg:169.91ms\nstep:7766/10000 train_loss:3.8496 train_time:1317818ms step_avg:169.91ms\nstep:7767/10000 train_loss:3.7818 train_time:1317988ms step_avg:169.91ms\nstep:7768/10000 train_loss:3.8289 train_time:1318158ms step_avg:169.91ms\nstep:7769/10000 train_loss:3.7969 train_time:1318326ms step_avg:169.91ms\nstep:7770/10000 train_loss:3.7893 train_time:1318497ms step_avg:169.91ms\nstep:7771/10000 train_loss:3.8511 train_time:1318664ms step_avg:169.91ms\nstep:7772/10000 train_loss:3.8296 train_time:1318837ms step_avg:169.91ms\nstep:7773/10000 train_loss:3.7573 train_time:1319008ms step_avg:169.91ms\nstep:7774/10000 train_loss:3.8724 train_time:1319179ms step_avg:169.91ms\nstep:7775/10000 train_loss:3.8177 train_time:1319348ms step_avg:169.91ms\nstep:7776/10000 train_loss:3.8595 train_time:1319517ms step_avg:169.91ms\nstep:7777/10000 train_loss:3.7527 train_time:1319685ms step_avg:169.91ms\nstep:7778/10000 train_loss:3.9037 train_time:1319854ms step_avg:169.91ms\nstep:7779/10000 train_loss:3.8783 train_time:1320022ms step_avg:169.91ms\nstep:7780/10000 train_loss:3.7240 train_time:1320194ms step_avg:169.91ms\nstep:7781/10000 train_loss:3.9421 train_time:1320362ms step_avg:169.91ms\nstep:7782/10000 train_loss:3.7286 train_time:1320533ms step_avg:169.91ms\nstep:7783/10000 train_loss:3.8051 train_time:1320699ms step_avg:169.91ms\nstep:7784/10000 train_loss:3.8681 train_time:1320868ms step_avg:169.91ms\nstep:7785/10000 train_loss:3.8704 train_time:1321036ms step_avg:169.91ms\nstep:7786/10000 train_loss:3.7507 train_time:1321206ms step_avg:169.91ms\nstep:7787/10000 train_loss:3.7689 train_time:1321379ms step_avg:169.91ms\nstep:7788/10000 train_loss:3.6484 train_time:1321548ms step_avg:169.91ms\nstep:7789/10000 train_loss:3.7936 train_time:1321719ms step_avg:169.91ms\nstep:7790/10000 train_loss:3.8412 train_time:1321886ms step_avg:169.91ms\nstep:7791/10000 train_loss:3.8431 train_time:1322056ms step_avg:169.91ms\nstep:7792/10000 train_loss:4.0008 train_time:1322224ms step_avg:169.91ms\nstep:7793/10000 train_loss:3.9434 train_time:1322394ms step_avg:169.91ms\nstep:7794/10000 train_loss:3.6894 train_time:1322563ms step_avg:169.91ms\nstep:7795/10000 train_loss:3.7504 train_time:1322735ms step_avg:169.91ms\nstep:7796/10000 train_loss:3.6716 train_time:1322904ms step_avg:169.91ms\nstep:7797/10000 train_loss:3.8635 train_time:1323074ms step_avg:169.91ms\nstep:7798/10000 train_loss:3.9372 train_time:1323241ms step_avg:169.91ms\nstep:7799/10000 train_loss:3.6901 train_time:1323410ms step_avg:169.91ms\nstep:7800/10000 train_loss:3.8823 train_time:1323580ms step_avg:169.91ms\nstep:7801/10000 train_loss:3.8177 train_time:1323748ms step_avg:169.91ms\nstep:7802/10000 train_loss:3.8182 train_time:1323920ms step_avg:169.91ms\nstep:7803/10000 train_loss:3.8268 train_time:1324090ms step_avg:169.91ms\nstep:7804/10000 train_loss:3.8406 train_time:1324262ms step_avg:169.91ms\nstep:7805/10000 train_loss:3.9410 train_time:1324433ms step_avg:169.91ms\nstep:7806/10000 train_loss:3.8255 train_time:1324603ms step_avg:169.91ms\nstep:7807/10000 train_loss:3.7377 train_time:1324773ms step_avg:169.91ms\nstep:7808/10000 train_loss:3.7257 train_time:1324940ms step_avg:169.91ms\nstep:7809/10000 train_loss:3.7260 train_time:1325109ms step_avg:169.91ms\nstep:7810/10000 train_loss:3.8137 train_time:1325281ms step_avg:169.91ms\nstep:7811/10000 train_loss:3.8256 train_time:1325452ms step_avg:169.91ms\nstep:7812/10000 train_loss:3.9275 train_time:1325620ms step_avg:169.91ms\nstep:7813/10000 train_loss:3.9414 train_time:1325789ms step_avg:169.91ms\nstep:7814/10000 train_loss:3.7027 train_time:1325959ms step_avg:169.91ms\nstep:7815/10000 train_loss:3.6916 train_time:1326127ms step_avg:169.91ms\nstep:7816/10000 train_loss:3.8614 train_time:1326300ms step_avg:169.91ms\nstep:7817/10000 train_loss:3.8526 train_time:1326469ms step_avg:169.91ms\nstep:7818/10000 train_loss:3.8219 train_time:1326638ms step_avg:169.91ms\nstep:7819/10000 train_loss:3.8292 train_time:1326807ms step_avg:169.91ms\nstep:7820/10000 train_loss:3.7829 train_time:1326980ms step_avg:169.91ms\nstep:7821/10000 train_loss:3.8732 train_time:1327147ms step_avg:169.91ms\nstep:7822/10000 train_loss:3.6618 train_time:1327318ms step_avg:169.91ms\nstep:7823/10000 train_loss:3.8441 train_time:1327486ms step_avg:169.91ms\nstep:7824/10000 train_loss:3.8403 train_time:1327658ms step_avg:169.91ms\nstep:7825/10000 train_loss:3.9394 train_time:1327825ms step_avg:169.91ms\nstep:7826/10000 train_loss:3.8616 train_time:1327995ms step_avg:169.91ms\nstep:7827/10000 train_loss:3.7709 train_time:1328164ms step_avg:169.91ms\nstep:7828/10000 train_loss:3.7523 train_time:1328333ms step_avg:169.91ms\nstep:7829/10000 train_loss:3.8269 train_time:1328503ms step_avg:169.91ms\nstep:7830/10000 train_loss:3.8731 train_time:1328673ms step_avg:169.91ms\nstep:7831/10000 train_loss:3.9081 train_time:1328842ms step_avg:169.91ms\nstep:7832/10000 train_loss:3.7372 train_time:1329014ms step_avg:169.91ms\nstep:7833/10000 train_loss:3.8613 train_time:1329182ms step_avg:169.91ms\nstep:7834/10000 train_loss:3.8110 train_time:1329351ms step_avg:169.91ms\nstep:7835/10000 train_loss:3.7281 train_time:1329523ms step_avg:169.91ms\nstep:7836/10000 train_loss:3.7254 train_time:1329694ms step_avg:169.91ms\nstep:7837/10000 train_loss:3.7395 train_time:1329862ms step_avg:169.91ms\nstep:7838/10000 train_loss:3.7070 train_time:1330033ms step_avg:169.91ms\nstep:7839/10000 train_loss:3.9178 train_time:1330201ms step_avg:169.91ms\nstep:7840/10000 train_loss:3.8617 train_time:1330369ms step_avg:169.91ms\nstep:7841/10000 train_loss:3.9652 train_time:1330541ms step_avg:169.91ms\nstep:7842/10000 train_loss:3.7804 train_time:1330712ms step_avg:169.91ms\nstep:7843/10000 train_loss:3.7252 train_time:1330882ms step_avg:169.91ms\nstep:7844/10000 train_loss:3.8826 train_time:1331052ms step_avg:169.91ms\nstep:7845/10000 train_loss:3.9546 train_time:1331221ms step_avg:169.91ms\nstep:7846/10000 train_loss:3.9693 train_time:1331391ms step_avg:169.91ms\nstep:7847/10000 train_loss:3.8646 train_time:1331563ms step_avg:169.91ms\nstep:7848/10000 train_loss:3.6697 train_time:1331733ms step_avg:169.91ms\nstep:7849/10000 train_loss:3.8063 train_time:1331901ms step_avg:169.91ms\nstep:7850/10000 train_loss:3.8953 train_time:1332072ms step_avg:169.91ms\nstep:7851/10000 train_loss:3.8215 train_time:1332239ms step_avg:169.91ms\nstep:7852/10000 train_loss:3.8761 train_time:1332408ms step_avg:169.91ms\nstep:7853/10000 train_loss:3.7840 train_time:1332579ms step_avg:169.91ms\nstep:7854/10000 train_loss:3.9021 train_time:1332749ms step_avg:169.91ms\nstep:7855/10000 train_loss:3.8410 train_time:1332919ms step_avg:169.91ms\nstep:7856/10000 train_loss:3.8429 train_time:1333088ms step_avg:169.91ms\nstep:7857/10000 train_loss:4.0236 train_time:1333258ms step_avg:169.91ms\nstep:7858/10000 train_loss:3.7286 train_time:1333427ms step_avg:169.91ms\nstep:7859/10000 train_loss:3.9842 train_time:1333600ms step_avg:169.91ms\nstep:7860/10000 train_loss:3.6809 train_time:1333765ms step_avg:169.91ms\nstep:7861/10000 train_loss:3.8391 train_time:1333933ms step_avg:169.91ms\nstep:7862/10000 train_loss:3.8385 train_time:1334102ms step_avg:169.91ms\nstep:7863/10000 train_loss:3.8715 train_time:1334271ms step_avg:169.91ms\nstep:7864/10000 train_loss:3.9928 train_time:1334442ms step_avg:169.91ms\nstep:7865/10000 train_loss:3.8554 train_time:1334611ms step_avg:169.91ms\nstep:7866/10000 train_loss:3.8696 train_time:1334780ms step_avg:169.91ms\nstep:7867/10000 train_loss:3.7965 train_time:1334949ms step_avg:169.91ms\nstep:7868/10000 train_loss:3.8404 train_time:1335119ms step_avg:169.91ms\nstep:7869/10000 train_loss:3.7555 train_time:1335287ms step_avg:169.91ms\nstep:7870/10000 train_loss:3.8350 train_time:1335459ms step_avg:169.91ms\nstep:7871/10000 train_loss:3.8041 train_time:1335630ms step_avg:169.91ms\nstep:7872/10000 train_loss:3.7366 train_time:1335800ms step_avg:169.91ms\nstep:7873/10000 train_loss:3.8996 train_time:1335968ms step_avg:169.91ms\nstep:7874/10000 train_loss:3.7194 train_time:1336139ms step_avg:169.91ms\nstep:7875/10000 train_loss:3.8159 train_time:1336309ms step_avg:169.91ms\nstep:7876/10000 train_loss:3.7703 train_time:1336480ms step_avg:169.91ms\nstep:7877/10000 train_loss:3.8635 train_time:1336650ms step_avg:169.91ms\nstep:7878/10000 train_loss:4.1195 train_time:1336819ms step_avg:169.91ms\nstep:7879/10000 train_loss:3.7896 train_time:1336987ms step_avg:169.91ms\nstep:7880/10000 train_loss:3.8835 train_time:1337157ms step_avg:169.91ms\nstep:7881/10000 train_loss:3.7978 train_time:1337324ms step_avg:169.91ms\nstep:7882/10000 train_loss:3.7837 train_time:1337495ms step_avg:169.91ms\nstep:7883/10000 train_loss:3.8197 train_time:1337665ms step_avg:169.91ms\nstep:7884/10000 train_loss:3.9953 train_time:1337832ms step_avg:169.91ms\nstep:7885/10000 train_loss:3.9496 train_time:1338001ms step_avg:169.90ms\nstep:7886/10000 train_loss:3.7775 train_time:1338173ms step_avg:169.91ms\nstep:7887/10000 train_loss:3.8425 train_time:1338342ms step_avg:169.91ms\nstep:7888/10000 train_loss:3.6344 train_time:1338512ms step_avg:169.91ms\nstep:7889/10000 train_loss:3.8446 train_time:1338682ms step_avg:169.91ms\nstep:7890/10000 train_loss:3.8469 train_time:1338851ms step_avg:169.90ms\nstep:7891/10000 train_loss:3.8126 train_time:1339019ms step_avg:169.90ms\nstep:7892/10000 train_loss:3.8182 train_time:1339188ms step_avg:169.90ms\nstep:7893/10000 train_loss:3.8592 train_time:1339358ms step_avg:169.90ms\nstep:7894/10000 train_loss:3.8012 train_time:1339527ms step_avg:169.90ms\nstep:7895/10000 train_loss:3.8884 train_time:1339695ms step_avg:169.90ms\nstep:7896/10000 train_loss:3.8934 train_time:1339865ms step_avg:169.90ms\nstep:7897/10000 train_loss:3.6368 train_time:1340031ms step_avg:169.90ms\nstep:7898/10000 train_loss:3.7165 train_time:1340202ms step_avg:169.90ms\nstep:7899/10000 train_loss:3.9084 train_time:1340373ms step_avg:169.90ms\nstep:7900/10000 train_loss:3.7969 train_time:1340541ms step_avg:169.90ms\nstep:7901/10000 train_loss:3.7429 train_time:1340713ms step_avg:169.90ms\nstep:7902/10000 train_loss:3.6870 train_time:1340883ms step_avg:169.90ms\nstep:7903/10000 train_loss:3.8525 train_time:1341053ms step_avg:169.90ms\nstep:7904/10000 train_loss:3.9908 train_time:1341222ms step_avg:169.90ms\nstep:7905/10000 train_loss:3.9225 train_time:1341392ms step_avg:169.90ms\nstep:7906/10000 train_loss:3.7926 train_time:1341561ms step_avg:169.90ms\nstep:7907/10000 train_loss:3.7048 train_time:1341731ms step_avg:169.90ms\nstep:7908/10000 train_loss:3.8285 train_time:1341899ms step_avg:169.90ms\nstep:7909/10000 train_loss:3.8549 train_time:1342068ms step_avg:169.90ms\nstep:7910/10000 train_loss:3.7911 train_time:1342239ms step_avg:169.90ms\nstep:7911/10000 train_loss:3.6053 train_time:1342408ms step_avg:169.90ms\nstep:7912/10000 train_loss:3.8802 train_time:1342580ms step_avg:169.90ms\nstep:7913/10000 train_loss:3.6582 train_time:1342750ms step_avg:169.90ms\nstep:7914/10000 train_loss:3.7086 train_time:1342921ms step_avg:169.90ms\nstep:7915/10000 train_loss:3.8006 train_time:1343090ms step_avg:169.90ms\nstep:7916/10000 train_loss:3.8187 train_time:1343261ms step_avg:169.90ms\nstep:7917/10000 train_loss:3.8046 train_time:1343434ms step_avg:169.90ms\nstep:7918/10000 train_loss:3.8796 train_time:1343602ms step_avg:169.90ms\nstep:7919/10000 train_loss:3.8644 train_time:1343771ms step_avg:169.90ms\nstep:7920/10000 train_loss:3.7863 train_time:1343942ms step_avg:169.90ms\nstep:7921/10000 train_loss:3.8501 train_time:1344111ms step_avg:169.90ms\nstep:7922/10000 train_loss:3.7519 train_time:1344281ms step_avg:169.90ms\nstep:7923/10000 train_loss:3.6786 train_time:1344451ms step_avg:169.90ms\nstep:7924/10000 train_loss:3.8772 train_time:1344618ms step_avg:169.90ms\nstep:7925/10000 train_loss:3.7943 train_time:1344787ms step_avg:169.90ms\nstep:7926/10000 train_loss:4.1253 train_time:1344956ms step_avg:169.90ms\nstep:7927/10000 train_loss:3.9038 train_time:1345126ms step_avg:169.90ms\nstep:7928/10000 train_loss:3.8718 train_time:1345298ms step_avg:169.90ms\nstep:7929/10000 train_loss:3.7922 train_time:1345468ms step_avg:169.90ms\nstep:7930/10000 train_loss:3.8346 train_time:1345637ms step_avg:169.90ms\nstep:7931/10000 train_loss:3.8105 train_time:1345805ms step_avg:169.90ms\nstep:7932/10000 train_loss:3.8959 train_time:1345975ms step_avg:169.90ms\nstep:7933/10000 train_loss:3.7686 train_time:1346142ms step_avg:169.90ms\nstep:7934/10000 train_loss:3.8087 train_time:1346313ms step_avg:169.90ms\nstep:7935/10000 train_loss:3.8178 train_time:1346482ms step_avg:169.90ms\nstep:7936/10000 train_loss:3.7803 train_time:1346652ms step_avg:169.90ms\nstep:7937/10000 train_loss:3.8288 train_time:1346820ms step_avg:169.90ms\nstep:7938/10000 train_loss:3.7984 train_time:1346988ms step_avg:169.90ms\nstep:7939/10000 train_loss:3.8589 train_time:1347158ms step_avg:169.90ms\nstep:7940/10000 train_loss:3.7472 train_time:1347327ms step_avg:169.90ms\nstep:7941/10000 train_loss:3.8572 train_time:1347498ms step_avg:169.90ms\nstep:7942/10000 train_loss:3.8401 train_time:1347667ms step_avg:169.90ms\nstep:7943/10000 train_loss:3.7851 train_time:1347836ms step_avg:169.90ms\nstep:7944/10000 train_loss:3.9290 train_time:1348004ms step_avg:169.90ms\nstep:7945/10000 train_loss:3.8038 train_time:1348174ms step_avg:169.90ms\nstep:7946/10000 train_loss:3.7818 train_time:1348341ms step_avg:169.90ms\nstep:7947/10000 train_loss:3.8460 train_time:1348513ms step_avg:169.90ms\nstep:7948/10000 train_loss:3.9088 train_time:1348683ms step_avg:169.90ms\nstep:7949/10000 train_loss:3.9361 train_time:1348852ms step_avg:169.90ms\nstep:7950/10000 train_loss:3.7613 train_time:1349021ms step_avg:169.90ms\nstep:7951/10000 train_loss:3.8591 train_time:1349190ms step_avg:169.90ms\nstep:7952/10000 train_loss:4.0814 train_time:1349358ms step_avg:169.90ms\nstep:7953/10000 train_loss:3.7841 train_time:1349526ms step_avg:169.90ms\nstep:7954/10000 train_loss:3.7901 train_time:1349701ms step_avg:169.90ms\nstep:7955/10000 train_loss:3.5684 train_time:1349870ms step_avg:169.90ms\nstep:7956/10000 train_loss:3.8882 train_time:1350040ms step_avg:169.90ms\nstep:7957/10000 train_loss:3.7849 train_time:1350210ms step_avg:169.90ms\nstep:7958/10000 train_loss:3.8448 train_time:1350378ms step_avg:169.90ms\nstep:7959/10000 train_loss:3.9173 train_time:1350546ms step_avg:169.90ms\nstep:7960/10000 train_loss:3.7759 train_time:1350716ms step_avg:169.90ms\nstep:7961/10000 train_loss:3.8154 train_time:1350883ms step_avg:169.90ms\nstep:7962/10000 train_loss:3.8496 train_time:1351052ms step_avg:169.90ms\nstep:7963/10000 train_loss:3.7620 train_time:1351223ms step_avg:169.90ms\nstep:7964/10000 train_loss:3.8859 train_time:1351393ms step_avg:169.90ms\nstep:7965/10000 train_loss:3.8651 train_time:1351561ms step_avg:169.90ms\nstep:7966/10000 train_loss:3.7922 train_time:1351730ms step_avg:169.90ms\nstep:7967/10000 train_loss:3.7320 train_time:1351902ms step_avg:169.90ms\nstep:7968/10000 train_loss:3.7840 train_time:1352073ms step_avg:169.90ms\nstep:7969/10000 train_loss:3.8695 train_time:1352241ms step_avg:169.90ms\nstep:7970/10000 train_loss:3.8856 train_time:1352412ms step_avg:169.90ms\nstep:7971/10000 train_loss:3.7931 train_time:1352581ms step_avg:169.90ms\nstep:7972/10000 train_loss:3.9152 train_time:1352749ms step_avg:169.90ms\nstep:7973/10000 train_loss:3.8936 train_time:1352920ms step_avg:169.90ms\nstep:7974/10000 train_loss:3.9102 train_time:1353089ms step_avg:169.90ms\nstep:7975/10000 train_loss:3.7105 train_time:1353261ms step_avg:169.90ms\nstep:7976/10000 train_loss:3.8451 train_time:1353431ms step_avg:169.90ms\nstep:7977/10000 train_loss:3.6461 train_time:1353603ms step_avg:169.90ms\nstep:7978/10000 train_loss:3.6680 train_time:1353770ms step_avg:169.90ms\nstep:7979/10000 train_loss:3.8409 train_time:1353939ms step_avg:169.90ms\nstep:7980/10000 train_loss:3.6807 train_time:1354108ms step_avg:169.90ms\nstep:7981/10000 train_loss:4.0377 train_time:1354276ms step_avg:169.90ms\nstep:7982/10000 train_loss:3.7069 train_time:1354445ms step_avg:169.90ms\nstep:7983/10000 train_loss:3.8910 train_time:1354614ms step_avg:169.90ms\nstep:7984/10000 train_loss:3.8677 train_time:1354783ms step_avg:169.90ms\nstep:7985/10000 train_loss:3.8498 train_time:1354953ms step_avg:169.90ms\nstep:7986/10000 train_loss:3.8859 train_time:1355120ms step_avg:169.90ms\nstep:7987/10000 train_loss:3.8490 train_time:1355291ms step_avg:169.90ms\nstep:7988/10000 train_loss:3.8434 train_time:1355463ms step_avg:169.90ms\nstep:7989/10000 train_loss:3.8653 train_time:1355631ms step_avg:169.90ms\nstep:7990/10000 train_loss:3.6849 train_time:1355799ms step_avg:169.90ms\nstep:7991/10000 train_loss:3.9011 train_time:1355968ms step_avg:169.90ms\nstep:7992/10000 train_loss:3.8513 train_time:1356140ms step_avg:169.90ms\nstep:7993/10000 train_loss:3.9140 train_time:1356308ms step_avg:169.90ms\nstep:7994/10000 train_loss:3.8087 train_time:1356478ms step_avg:169.90ms\nstep:7995/10000 train_loss:3.7481 train_time:1356647ms step_avg:169.90ms\nstep:7996/10000 train_loss:4.4600 train_time:1356816ms step_avg:169.90ms\nstep:7997/10000 train_loss:3.7828 train_time:1356983ms step_avg:169.90ms\nstep:7998/10000 train_loss:3.8709 train_time:1357151ms step_avg:169.90ms\nstep:7999/10000 train_loss:3.9427 train_time:1357321ms step_avg:169.90ms\nstep:8000/10000 train_loss:3.8032 train_time:1357491ms step_avg:169.90ms\nstep:8001/10000 train_loss:3.7100 train_time:1357659ms step_avg:169.90ms\nstep:8002/10000 train_loss:3.7181 train_time:1357829ms step_avg:169.90ms\nstep:8003/10000 train_loss:3.9722 train_time:1357999ms step_avg:169.90ms\nstep:8004/10000 train_loss:3.8559 train_time:1358167ms step_avg:169.90ms\nstep:8005/10000 train_loss:4.0877 train_time:1358334ms step_avg:169.90ms\nstep:8006/10000 train_loss:3.8290 train_time:1358503ms step_avg:169.90ms\nstep:8007/10000 train_loss:3.7946 train_time:1358672ms step_avg:169.90ms\nstep:8008/10000 train_loss:3.7862 train_time:1358839ms step_avg:169.90ms\nstep:8009/10000 train_loss:4.1201 train_time:1359008ms step_avg:169.90ms\nstep:8010/10000 train_loss:3.8624 train_time:1359177ms step_avg:169.90ms\nstep:8011/10000 train_loss:3.7940 train_time:1359345ms step_avg:169.90ms\nstep:8012/10000 train_loss:3.8964 train_time:1359515ms step_avg:169.90ms\nstep:8013/10000 train_loss:3.8074 train_time:1359685ms step_avg:169.90ms\nstep:8014/10000 train_loss:3.7655 train_time:1359856ms step_avg:169.90ms\nstep:8015/10000 train_loss:4.3371 train_time:1360024ms step_avg:169.90ms\nstep:8016/10000 train_loss:3.8374 train_time:1360193ms step_avg:169.90ms\nstep:8017/10000 train_loss:3.8221 train_time:1360362ms step_avg:169.90ms\nstep:8018/10000 train_loss:3.8902 train_time:1360533ms step_avg:169.90ms\nstep:8019/10000 train_loss:3.6035 train_time:1360701ms step_avg:169.90ms\nstep:8020/10000 train_loss:3.7765 train_time:1360871ms step_avg:169.90ms\nstep:8021/10000 train_loss:3.6453 train_time:1361039ms step_avg:169.90ms\nstep:8022/10000 train_loss:3.8594 train_time:1361207ms step_avg:169.90ms\nstep:8023/10000 train_loss:3.8502 train_time:1361376ms step_avg:169.90ms\nstep:8024/10000 train_loss:3.8184 train_time:1361547ms step_avg:169.90ms\nstep:8025/10000 train_loss:3.8922 train_time:1361719ms step_avg:169.90ms\nstep:8026/10000 train_loss:3.9341 train_time:1361888ms step_avg:169.90ms\nstep:8027/10000 train_loss:3.8328 train_time:1362059ms step_avg:169.90ms\nstep:8028/10000 train_loss:3.6643 train_time:1362228ms step_avg:169.90ms\nstep:8029/10000 train_loss:3.9409 train_time:1362399ms step_avg:169.90ms\nstep:8030/10000 train_loss:3.8457 train_time:1362569ms step_avg:169.90ms\nstep:8031/10000 train_loss:3.7863 train_time:1362739ms step_avg:169.90ms\nstep:8032/10000 train_loss:3.8239 train_time:1362908ms step_avg:169.90ms\nstep:8033/10000 train_loss:3.8415 train_time:1363076ms step_avg:169.90ms\nstep:8034/10000 train_loss:3.8912 train_time:1363244ms step_avg:169.90ms\nstep:8035/10000 train_loss:4.8211 train_time:1363417ms step_avg:169.90ms\nstep:8036/10000 train_loss:3.8503 train_time:1363585ms step_avg:169.90ms\nstep:8037/10000 train_loss:3.8774 train_time:1363754ms step_avg:169.90ms\nstep:8038/10000 train_loss:3.7493 train_time:1363922ms step_avg:169.90ms\nstep:8039/10000 train_loss:3.7276 train_time:1364092ms step_avg:169.90ms\nstep:8040/10000 train_loss:3.7999 train_time:1364260ms step_avg:169.90ms\nstep:8041/10000 train_loss:3.8386 train_time:1364432ms step_avg:169.90ms\nstep:8042/10000 train_loss:3.7897 train_time:1364604ms step_avg:169.90ms\nstep:8043/10000 train_loss:3.8456 train_time:1364773ms step_avg:169.90ms\nstep:8044/10000 train_loss:3.7446 train_time:1364943ms step_avg:169.90ms\nstep:8045/10000 train_loss:3.8015 train_time:1365112ms step_avg:169.90ms\nstep:8046/10000 train_loss:3.7870 train_time:1365280ms step_avg:169.90ms\nstep:8047/10000 train_loss:3.8656 train_time:1365452ms step_avg:169.90ms\nstep:8048/10000 train_loss:3.7782 train_time:1365623ms step_avg:169.90ms\nstep:8049/10000 train_loss:3.8565 train_time:1365795ms step_avg:169.90ms\nstep:8050/10000 train_loss:3.8501 train_time:1365963ms step_avg:169.90ms\nstep:8051/10000 train_loss:3.7030 train_time:1366133ms step_avg:169.90ms\nstep:8052/10000 train_loss:3.6282 train_time:1366301ms step_avg:169.90ms\nstep:8053/10000 train_loss:3.7699 train_time:1366472ms step_avg:169.90ms\nstep:8054/10000 train_loss:3.7559 train_time:1366641ms step_avg:169.90ms\nstep:8055/10000 train_loss:3.8442 train_time:1366812ms step_avg:169.90ms\nstep:8056/10000 train_loss:3.7342 train_time:1366979ms step_avg:169.90ms\nstep:8057/10000 train_loss:4.0094 train_time:1367146ms step_avg:169.90ms\nstep:8058/10000 train_loss:3.8455 train_time:1367313ms step_avg:169.89ms\nstep:8059/10000 train_loss:3.9438 train_time:1367482ms step_avg:169.89ms\nstep:8060/10000 train_loss:3.7432 train_time:1367653ms step_avg:169.89ms\nstep:8061/10000 train_loss:3.7763 train_time:1367822ms step_avg:169.89ms\nstep:8062/10000 train_loss:3.8006 train_time:1367991ms step_avg:169.89ms\nstep:8063/10000 train_loss:3.9747 train_time:1368161ms step_avg:169.89ms\nstep:8064/10000 train_loss:3.7869 train_time:1368331ms step_avg:169.89ms\nstep:8065/10000 train_loss:3.8796 train_time:1368500ms step_avg:169.89ms\nstep:8066/10000 train_loss:3.8284 train_time:1368670ms step_avg:169.89ms\nstep:8067/10000 train_loss:3.7695 train_time:1368842ms step_avg:169.89ms\nstep:8068/10000 train_loss:3.7901 train_time:1369011ms step_avg:169.89ms\nstep:8069/10000 train_loss:3.9918 train_time:1369182ms step_avg:169.89ms\nstep:8070/10000 train_loss:3.7612 train_time:1369352ms step_avg:169.89ms\nstep:8071/10000 train_loss:3.9168 train_time:1369522ms step_avg:169.89ms\nstep:8072/10000 train_loss:3.7574 train_time:1369695ms step_avg:169.90ms\nstep:8073/10000 train_loss:3.8812 train_time:1369865ms step_avg:169.90ms\nstep:8074/10000 train_loss:3.7989 train_time:1370034ms step_avg:169.90ms\nstep:8075/10000 train_loss:3.7179 train_time:1370202ms step_avg:169.89ms\nstep:8076/10000 train_loss:3.7320 train_time:1370372ms step_avg:169.89ms\nstep:8077/10000 train_loss:3.9680 train_time:1370543ms step_avg:169.90ms\nstep:8078/10000 train_loss:3.7883 train_time:1370714ms step_avg:169.90ms\nstep:8079/10000 train_loss:3.7884 train_time:1370881ms step_avg:169.89ms\nstep:8080/10000 train_loss:3.6651 train_time:1371051ms step_avg:169.89ms\nstep:8081/10000 train_loss:3.7992 train_time:1371221ms step_avg:169.89ms\nstep:8082/10000 train_loss:3.9446 train_time:1371391ms step_avg:169.89ms\nstep:8083/10000 train_loss:3.8535 train_time:1371561ms step_avg:169.89ms\nstep:8084/10000 train_loss:3.9607 train_time:1371733ms step_avg:169.90ms\nstep:8085/10000 train_loss:3.8487 train_time:1371902ms step_avg:169.90ms\nstep:8086/10000 train_loss:3.7551 train_time:1372072ms step_avg:169.90ms\nstep:8087/10000 train_loss:3.8402 train_time:1372244ms step_avg:169.90ms\nstep:8088/10000 train_loss:3.8547 train_time:1372414ms step_avg:169.90ms\nstep:8089/10000 train_loss:3.7171 train_time:1372584ms step_avg:169.90ms\nstep:8090/10000 train_loss:3.9347 train_time:1372753ms step_avg:169.90ms\nstep:8091/10000 train_loss:3.8583 train_time:1372922ms step_avg:169.90ms\nstep:8092/10000 train_loss:3.8051 train_time:1373092ms step_avg:169.90ms\nstep:8093/10000 train_loss:3.8255 train_time:1373261ms step_avg:169.89ms\nstep:8094/10000 train_loss:3.9227 train_time:1373434ms step_avg:169.90ms\nstep:8095/10000 train_loss:3.7859 train_time:1373603ms step_avg:169.90ms\nstep:8096/10000 train_loss:3.8148 train_time:1373773ms step_avg:169.90ms\nstep:8097/10000 train_loss:3.9367 train_time:1373941ms step_avg:169.90ms\nstep:8098/10000 train_loss:3.7088 train_time:1374112ms step_avg:169.90ms\nstep:8099/10000 train_loss:3.8042 train_time:1374281ms step_avg:169.89ms\nstep:8100/10000 train_loss:3.7796 train_time:1374451ms step_avg:169.90ms\nstep:8101/10000 train_loss:3.8045 train_time:1374618ms step_avg:169.89ms\nstep:8102/10000 train_loss:3.8966 train_time:1374787ms step_avg:169.89ms\nstep:8103/10000 train_loss:3.8289 train_time:1374956ms step_avg:169.89ms\nstep:8104/10000 train_loss:3.7446 train_time:1375124ms step_avg:169.89ms\nstep:8105/10000 train_loss:3.7706 train_time:1375293ms step_avg:169.89ms\nstep:8106/10000 train_loss:3.8100 train_time:1375464ms step_avg:169.89ms\nstep:8107/10000 train_loss:3.8768 train_time:1375635ms step_avg:169.89ms\nstep:8108/10000 train_loss:3.8110 train_time:1375803ms step_avg:169.89ms\nstep:8109/10000 train_loss:3.7060 train_time:1375974ms step_avg:169.89ms\nstep:8110/10000 train_loss:3.8983 train_time:1376143ms step_avg:169.89ms\nstep:8111/10000 train_loss:3.8205 train_time:1376312ms step_avg:169.89ms\nstep:8112/10000 train_loss:3.8896 train_time:1376481ms step_avg:169.89ms\nstep:8113/10000 train_loss:3.6612 train_time:1376653ms step_avg:169.89ms\nstep:8114/10000 train_loss:3.9653 train_time:1376822ms step_avg:169.89ms\nstep:8115/10000 train_loss:3.8309 train_time:1376994ms step_avg:169.89ms\nstep:8116/10000 train_loss:3.6990 train_time:1377161ms step_avg:169.89ms\nstep:8117/10000 train_loss:3.9322 train_time:1377332ms step_avg:169.89ms\nstep:8118/10000 train_loss:3.7372 train_time:1377501ms step_avg:169.89ms\nstep:8119/10000 train_loss:3.7463 train_time:1377672ms step_avg:169.89ms\nstep:8120/10000 train_loss:3.7674 train_time:1377839ms step_avg:169.89ms\nstep:8121/10000 train_loss:3.9132 train_time:1378007ms step_avg:169.89ms\nstep:8122/10000 train_loss:3.8089 train_time:1378179ms step_avg:169.89ms\nstep:8123/10000 train_loss:3.8784 train_time:1378349ms step_avg:169.89ms\nstep:8124/10000 train_loss:3.8490 train_time:1378519ms step_avg:169.89ms\nstep:8125/10000 train_loss:3.8301 train_time:1378688ms step_avg:169.89ms\nstep:8126/10000 train_loss:3.6700 train_time:1378858ms step_avg:169.89ms\nstep:8127/10000 train_loss:3.8401 train_time:1379027ms step_avg:169.89ms\nstep:8128/10000 train_loss:3.8889 train_time:1379197ms step_avg:169.89ms\nstep:8129/10000 train_loss:3.7840 train_time:1379364ms step_avg:169.89ms\nstep:8130/10000 train_loss:3.7968 train_time:1379534ms step_avg:169.89ms\nstep:8131/10000 train_loss:3.7691 train_time:1379702ms step_avg:169.89ms\nstep:8132/10000 train_loss:3.5758 train_time:1379874ms step_avg:169.89ms\nstep:8133/10000 train_loss:3.7943 train_time:1380042ms step_avg:169.89ms\nstep:8134/10000 train_loss:3.7158 train_time:1380212ms step_avg:169.89ms\nstep:8135/10000 train_loss:3.8652 train_time:1380380ms step_avg:169.89ms\nstep:8136/10000 train_loss:3.8114 train_time:1380548ms step_avg:169.89ms\nstep:8137/10000 train_loss:3.8100 train_time:1380720ms step_avg:169.89ms\nstep:8138/10000 train_loss:3.7871 train_time:1380892ms step_avg:169.89ms\nstep:8139/10000 train_loss:3.6634 train_time:1381062ms step_avg:169.89ms\nstep:8140/10000 train_loss:3.7422 train_time:1381234ms step_avg:169.89ms\nstep:8141/10000 train_loss:3.8687 train_time:1381403ms step_avg:169.89ms\nstep:8142/10000 train_loss:3.8050 train_time:1381573ms step_avg:169.89ms\nstep:8143/10000 train_loss:3.8118 train_time:1381744ms step_avg:169.89ms\nstep:8144/10000 train_loss:3.8477 train_time:1381914ms step_avg:169.89ms\nstep:8145/10000 train_loss:3.7730 train_time:1382081ms step_avg:169.89ms\nstep:8146/10000 train_loss:3.8184 train_time:1382252ms step_avg:169.89ms\nstep:8147/10000 train_loss:3.8535 train_time:1382422ms step_avg:169.89ms\nstep:8148/10000 train_loss:3.7791 train_time:1382591ms step_avg:169.89ms\nstep:8149/10000 train_loss:3.8287 train_time:1382761ms step_avg:169.89ms\nstep:8150/10000 train_loss:3.7139 train_time:1382932ms step_avg:169.89ms\nstep:8151/10000 train_loss:3.7928 train_time:1383101ms step_avg:169.89ms\nstep:8152/10000 train_loss:3.8242 train_time:1383274ms step_avg:169.89ms\nstep:8153/10000 train_loss:3.8704 train_time:1383442ms step_avg:169.89ms\nstep:8154/10000 train_loss:3.8519 train_time:1383614ms step_avg:169.89ms\nstep:8155/10000 train_loss:3.8720 train_time:1383783ms step_avg:169.89ms\nstep:8156/10000 train_loss:3.8355 train_time:1383955ms step_avg:169.89ms\nstep:8157/10000 train_loss:3.8362 train_time:1384121ms step_avg:169.89ms\nstep:8158/10000 train_loss:3.8507 train_time:1384294ms step_avg:169.89ms\nstep:8159/10000 train_loss:3.5369 train_time:1384462ms step_avg:169.89ms\nstep:8160/10000 train_loss:3.8585 train_time:1384632ms step_avg:169.89ms\nstep:8161/10000 train_loss:3.9808 train_time:1384801ms step_avg:169.89ms\nstep:8162/10000 train_loss:3.8537 train_time:1384973ms step_avg:169.89ms\nstep:8163/10000 train_loss:3.7164 train_time:1385141ms step_avg:169.89ms\nstep:8164/10000 train_loss:4.0822 train_time:1385312ms step_avg:169.89ms\nstep:8165/10000 train_loss:3.7048 train_time:1385481ms step_avg:169.89ms\nstep:8166/10000 train_loss:3.7062 train_time:1385650ms step_avg:169.89ms\nstep:8167/10000 train_loss:3.7261 train_time:1385820ms step_avg:169.89ms\nstep:8168/10000 train_loss:3.8229 train_time:1385991ms step_avg:169.89ms\nstep:8169/10000 train_loss:4.0572 train_time:1386159ms step_avg:169.89ms\nstep:8170/10000 train_loss:3.9793 train_time:1386329ms step_avg:169.89ms\nstep:8171/10000 train_loss:3.7411 train_time:1386499ms step_avg:169.89ms\nstep:8172/10000 train_loss:3.7782 train_time:1386669ms step_avg:169.89ms\nstep:8173/10000 train_loss:3.7420 train_time:1386841ms step_avg:169.89ms\nstep:8174/10000 train_loss:3.8056 train_time:1387013ms step_avg:169.89ms\nstep:8175/10000 train_loss:3.7717 train_time:1387180ms step_avg:169.89ms\nstep:8176/10000 train_loss:3.7302 train_time:1387349ms step_avg:169.89ms\nstep:8177/10000 train_loss:3.8032 train_time:1387517ms step_avg:169.89ms\nstep:8178/10000 train_loss:3.8366 train_time:1387686ms step_avg:169.89ms\nstep:8179/10000 train_loss:3.8219 train_time:1387857ms step_avg:169.89ms\nstep:8180/10000 train_loss:3.8043 train_time:1388027ms step_avg:169.89ms\nstep:8181/10000 train_loss:3.8548 train_time:1388197ms step_avg:169.89ms\nstep:8182/10000 train_loss:3.7775 train_time:1388365ms step_avg:169.89ms\nstep:8183/10000 train_loss:3.9303 train_time:1388534ms step_avg:169.89ms\nstep:8184/10000 train_loss:3.7271 train_time:1388700ms step_avg:169.89ms\nstep:8185/10000 train_loss:3.5830 train_time:1388872ms step_avg:169.89ms\nstep:8186/10000 train_loss:3.6608 train_time:1389042ms step_avg:169.89ms\nstep:8187/10000 train_loss:3.8500 train_time:1389211ms step_avg:169.89ms\nstep:8188/10000 train_loss:3.7041 train_time:1389379ms step_avg:169.89ms\nstep:8189/10000 train_loss:3.9674 train_time:1389546ms step_avg:169.89ms\nstep:8190/10000 train_loss:3.8176 train_time:1389715ms step_avg:169.89ms\nstep:8191/10000 train_loss:3.8193 train_time:1389883ms step_avg:169.89ms\nstep:8192/10000 train_loss:3.7365 train_time:1390054ms step_avg:169.89ms\nstep:8193/10000 train_loss:3.8058 train_time:1390224ms step_avg:169.89ms\nstep:8194/10000 train_loss:3.8374 train_time:1390396ms step_avg:169.89ms\nstep:8195/10000 train_loss:3.8454 train_time:1390563ms step_avg:169.89ms\nstep:8196/10000 train_loss:3.7646 train_time:1390734ms step_avg:169.89ms\nstep:8197/10000 train_loss:3.8717 train_time:1390903ms step_avg:169.89ms\nstep:8198/10000 train_loss:3.6723 train_time:1391076ms step_avg:169.89ms\nstep:8199/10000 train_loss:3.7752 train_time:1391245ms step_avg:169.89ms\nstep:8200/10000 train_loss:3.8324 train_time:1391415ms step_avg:169.89ms\nstep:8201/10000 train_loss:3.7318 train_time:1391584ms step_avg:169.89ms\nstep:8202/10000 train_loss:3.7567 train_time:1391756ms step_avg:169.89ms\nstep:8203/10000 train_loss:3.9716 train_time:1391925ms step_avg:169.89ms\nstep:8204/10000 train_loss:3.7974 train_time:1392094ms step_avg:169.89ms\nstep:8205/10000 train_loss:3.8521 train_time:1392265ms step_avg:169.89ms\nstep:8206/10000 train_loss:3.8082 train_time:1392435ms step_avg:169.89ms\nstep:8207/10000 train_loss:3.8443 train_time:1392603ms step_avg:169.89ms\nstep:8208/10000 train_loss:3.8356 train_time:1392775ms step_avg:169.89ms\nstep:8209/10000 train_loss:3.7780 train_time:1392942ms step_avg:169.89ms\nstep:8210/10000 train_loss:3.7668 train_time:1393111ms step_avg:169.89ms\nstep:8211/10000 train_loss:3.7969 train_time:1393280ms step_avg:169.89ms\nstep:8212/10000 train_loss:3.7170 train_time:1393451ms step_avg:169.89ms\nstep:8213/10000 train_loss:3.7651 train_time:1393618ms step_avg:169.89ms\nstep:8214/10000 train_loss:3.7901 train_time:1393788ms step_avg:169.89ms\nstep:8215/10000 train_loss:3.7429 train_time:1393956ms step_avg:169.89ms\nstep:8216/10000 train_loss:3.9234 train_time:1394123ms step_avg:169.89ms\nstep:8217/10000 train_loss:3.8476 train_time:1394295ms step_avg:169.89ms\nstep:8218/10000 train_loss:3.9581 train_time:1394464ms step_avg:169.89ms\nstep:8219/10000 train_loss:3.7917 train_time:1394633ms step_avg:169.89ms\nstep:8220/10000 train_loss:3.7985 train_time:1394801ms step_avg:169.89ms\nstep:8221/10000 train_loss:3.8322 train_time:1394971ms step_avg:169.89ms\nstep:8222/10000 train_loss:3.8320 train_time:1395143ms step_avg:169.89ms\nstep:8223/10000 train_loss:3.7711 train_time:1395312ms step_avg:169.89ms\nstep:8224/10000 train_loss:3.9019 train_time:1395482ms step_avg:169.89ms\nstep:8225/10000 train_loss:3.8041 train_time:1395657ms step_avg:169.89ms\nstep:8226/10000 train_loss:3.7679 train_time:1395823ms step_avg:169.89ms\nstep:8227/10000 train_loss:3.8775 train_time:1395992ms step_avg:169.89ms\nstep:8228/10000 train_loss:3.8277 train_time:1396160ms step_avg:169.89ms\nstep:8229/10000 train_loss:3.8423 train_time:1396330ms step_avg:169.89ms\nstep:8230/10000 train_loss:3.7628 train_time:1396498ms step_avg:169.89ms\nstep:8231/10000 train_loss:4.0065 train_time:1396666ms step_avg:169.89ms\nstep:8232/10000 train_loss:3.8148 train_time:1396835ms step_avg:169.89ms\nstep:8233/10000 train_loss:3.8371 train_time:1397004ms step_avg:169.89ms\nstep:8234/10000 train_loss:3.6285 train_time:1397173ms step_avg:169.89ms\nstep:8235/10000 train_loss:3.9328 train_time:1397342ms step_avg:169.89ms\nstep:8236/10000 train_loss:3.8752 train_time:1397513ms step_avg:169.89ms\nstep:8237/10000 train_loss:3.7708 train_time:1397683ms step_avg:169.89ms\nstep:8238/10000 train_loss:3.7329 train_time:1397854ms step_avg:169.89ms\nstep:8239/10000 train_loss:3.8380 train_time:1398024ms step_avg:169.89ms\nstep:8240/10000 train_loss:3.8008 train_time:1398192ms step_avg:169.89ms\nstep:8241/10000 train_loss:3.7765 train_time:1398363ms step_avg:169.89ms\nstep:8242/10000 train_loss:3.9205 train_time:1398534ms step_avg:169.89ms\nstep:8243/10000 train_loss:3.8032 train_time:1398703ms step_avg:169.89ms\nstep:8244/10000 train_loss:3.7338 train_time:1398873ms step_avg:169.89ms\nstep:8245/10000 train_loss:3.6806 train_time:1399042ms step_avg:169.89ms\nstep:8246/10000 train_loss:3.9114 train_time:1399211ms step_avg:169.89ms\nstep:8247/10000 train_loss:3.5168 train_time:1399381ms step_avg:169.89ms\nstep:8248/10000 train_loss:3.6966 train_time:1399552ms step_avg:169.89ms\nstep:8249/10000 train_loss:3.8670 train_time:1399719ms step_avg:169.89ms\nstep:8250/10000 train_loss:3.7898 train_time:1399888ms step_avg:169.89ms\nstep:8251/10000 train_loss:4.0358 train_time:1400055ms step_avg:169.89ms\nstep:8252/10000 train_loss:3.8839 train_time:1400224ms step_avg:169.89ms\nstep:8253/10000 train_loss:3.6930 train_time:1400394ms step_avg:169.89ms\nstep:8254/10000 train_loss:3.8422 train_time:1400564ms step_avg:169.89ms\nstep:8255/10000 train_loss:3.7891 train_time:1400733ms step_avg:169.89ms\nstep:8256/10000 train_loss:3.9792 train_time:1400904ms step_avg:169.89ms\nstep:8257/10000 train_loss:3.8199 train_time:1401075ms step_avg:169.89ms\nstep:8258/10000 train_loss:3.8309 train_time:1401243ms step_avg:169.89ms\nstep:8259/10000 train_loss:3.7325 train_time:1401413ms step_avg:169.89ms\nstep:8260/10000 train_loss:3.9229 train_time:1401581ms step_avg:169.89ms\nstep:8261/10000 train_loss:3.5778 train_time:1401751ms step_avg:169.89ms\nstep:8262/10000 train_loss:3.7928 train_time:1401920ms step_avg:169.89ms\nstep:8263/10000 train_loss:3.6534 train_time:1402090ms step_avg:169.89ms\nstep:8264/10000 train_loss:3.8502 train_time:1402259ms step_avg:169.89ms\nstep:8265/10000 train_loss:3.6624 train_time:1402427ms step_avg:169.89ms\nstep:8266/10000 train_loss:3.7128 train_time:1402595ms step_avg:169.89ms\nstep:8267/10000 train_loss:3.7810 train_time:1402762ms step_avg:169.89ms\nstep:8268/10000 train_loss:3.9640 train_time:1402934ms step_avg:169.89ms\nstep:8269/10000 train_loss:3.8130 train_time:1403104ms step_avg:169.89ms\nstep:8270/10000 train_loss:4.0072 train_time:1403273ms step_avg:169.89ms\nstep:8271/10000 train_loss:3.8766 train_time:1403440ms step_avg:169.89ms\nstep:8272/10000 train_loss:3.7216 train_time:1403609ms step_avg:169.89ms\nstep:8273/10000 train_loss:3.8506 train_time:1403778ms step_avg:169.89ms\nstep:8274/10000 train_loss:3.8629 train_time:1403947ms step_avg:169.89ms\nstep:8275/10000 train_loss:3.7457 train_time:1404118ms step_avg:169.89ms\nstep:8276/10000 train_loss:3.8312 train_time:1404285ms step_avg:169.89ms\nstep:8277/10000 train_loss:3.7618 train_time:1404456ms step_avg:169.89ms\nstep:8278/10000 train_loss:3.7981 train_time:1404626ms step_avg:169.89ms\nstep:8279/10000 train_loss:3.7831 train_time:1404793ms step_avg:169.89ms\nstep:8280/10000 train_loss:3.6398 train_time:1404963ms step_avg:169.89ms\nstep:8281/10000 train_loss:3.8682 train_time:1405136ms step_avg:169.89ms\nstep:8282/10000 train_loss:3.8539 train_time:1405304ms step_avg:169.89ms\nstep:8283/10000 train_loss:3.8868 train_time:1405475ms step_avg:169.89ms\nstep:8284/10000 train_loss:3.8366 train_time:1405643ms step_avg:169.89ms\nstep:8285/10000 train_loss:3.7643 train_time:1405815ms step_avg:169.89ms\nstep:8286/10000 train_loss:3.7626 train_time:1405983ms step_avg:169.89ms\nstep:8287/10000 train_loss:3.8041 train_time:1406154ms step_avg:169.89ms\nstep:8288/10000 train_loss:3.8968 train_time:1406324ms step_avg:169.89ms\nstep:8289/10000 train_loss:3.7752 train_time:1406497ms step_avg:169.89ms\nstep:8290/10000 train_loss:3.8160 train_time:1406666ms step_avg:169.89ms\nstep:8291/10000 train_loss:3.5520 train_time:1406838ms step_avg:169.89ms\nstep:8292/10000 train_loss:4.0184 train_time:1407006ms step_avg:169.89ms\nstep:8293/10000 train_loss:3.8422 train_time:1407175ms step_avg:169.89ms\nstep:8294/10000 train_loss:3.9435 train_time:1407344ms step_avg:169.89ms\nstep:8295/10000 train_loss:3.8378 train_time:1407514ms step_avg:169.89ms\nstep:8296/10000 train_loss:3.9865 train_time:1407682ms step_avg:169.89ms\nstep:8297/10000 train_loss:3.8449 train_time:1407853ms step_avg:169.89ms\nstep:8298/10000 train_loss:3.9499 train_time:1408022ms step_avg:169.89ms\nstep:8299/10000 train_loss:3.7385 train_time:1408192ms step_avg:169.89ms\nstep:8300/10000 train_loss:3.8266 train_time:1408361ms step_avg:169.89ms\nstep:8301/10000 train_loss:3.7904 train_time:1408536ms step_avg:169.89ms\nstep:8302/10000 train_loss:3.8004 train_time:1408698ms step_avg:169.89ms\nstep:8303/10000 train_loss:4.1145 train_time:1408866ms step_avg:169.89ms\nstep:8304/10000 train_loss:3.9547 train_time:1409037ms step_avg:169.89ms\nstep:8305/10000 train_loss:3.7756 train_time:1409205ms step_avg:169.89ms\nstep:8306/10000 train_loss:3.8292 train_time:1409377ms step_avg:169.89ms\nstep:8307/10000 train_loss:3.7988 train_time:1409546ms step_avg:169.89ms\nstep:8308/10000 train_loss:3.9665 train_time:1409715ms step_avg:169.89ms\nstep:8309/10000 train_loss:3.7684 train_time:1409883ms step_avg:169.89ms\nstep:8310/10000 train_loss:3.7236 train_time:1410052ms step_avg:169.89ms\nstep:8311/10000 train_loss:3.7463 train_time:1410218ms step_avg:169.89ms\nstep:8312/10000 train_loss:4.6400 train_time:1410387ms step_avg:169.89ms\nstep:8313/10000 train_loss:3.7784 train_time:1410558ms step_avg:169.89ms\nstep:8314/10000 train_loss:4.2412 train_time:1410727ms step_avg:169.89ms\nstep:8315/10000 train_loss:3.7658 train_time:1410897ms step_avg:169.89ms\nstep:8316/10000 train_loss:3.7961 train_time:1411063ms step_avg:169.88ms\nstep:8317/10000 train_loss:3.6983 train_time:1411232ms step_avg:169.88ms\nstep:8318/10000 train_loss:3.6644 train_time:1411400ms step_avg:169.88ms\nstep:8319/10000 train_loss:3.8106 train_time:1411570ms step_avg:169.88ms\nstep:8320/10000 train_loss:3.8299 train_time:1411739ms step_avg:169.88ms\nstep:8321/10000 train_loss:3.8540 train_time:1411909ms step_avg:169.88ms\nstep:8322/10000 train_loss:3.8434 train_time:1412078ms step_avg:169.88ms\nstep:8323/10000 train_loss:3.8820 train_time:1412247ms step_avg:169.88ms\nstep:8324/10000 train_loss:3.8041 train_time:1412417ms step_avg:169.88ms\nstep:8325/10000 train_loss:3.8643 train_time:1412587ms step_avg:169.88ms\nstep:8326/10000 train_loss:3.7332 train_time:1412756ms step_avg:169.88ms\nstep:8327/10000 train_loss:3.8079 train_time:1412925ms step_avg:169.88ms\nstep:8328/10000 train_loss:3.8126 train_time:1413097ms step_avg:169.88ms\nstep:8329/10000 train_loss:3.6979 train_time:1413263ms step_avg:169.88ms\nstep:8330/10000 train_loss:3.7010 train_time:1413433ms step_avg:169.88ms\nstep:8331/10000 train_loss:3.7357 train_time:1413601ms step_avg:169.88ms\nstep:8332/10000 train_loss:3.7320 train_time:1413768ms step_avg:169.88ms\nstep:8333/10000 train_loss:3.7260 train_time:1413935ms step_avg:169.88ms\nstep:8334/10000 train_loss:3.7895 train_time:1414105ms step_avg:169.88ms\nstep:8335/10000 train_loss:3.8119 train_time:1414275ms step_avg:169.88ms\nstep:8336/10000 train_loss:3.8312 train_time:1414443ms step_avg:169.88ms\nstep:8337/10000 train_loss:3.7070 train_time:1414614ms step_avg:169.88ms\nstep:8338/10000 train_loss:4.1121 train_time:1414783ms step_avg:169.88ms\nstep:8339/10000 train_loss:3.8445 train_time:1414953ms step_avg:169.88ms\nstep:8340/10000 train_loss:3.7450 train_time:1415122ms step_avg:169.88ms\nstep:8341/10000 train_loss:3.9013 train_time:1415294ms step_avg:169.88ms\nstep:8342/10000 train_loss:3.6931 train_time:1415461ms step_avg:169.88ms\nstep:8343/10000 train_loss:3.8815 train_time:1415631ms step_avg:169.88ms\nstep:8344/10000 train_loss:3.8475 train_time:1415802ms step_avg:169.88ms\nstep:8345/10000 train_loss:3.9371 train_time:1415973ms step_avg:169.88ms\nstep:8346/10000 train_loss:4.0180 train_time:1416140ms step_avg:169.88ms\nstep:8347/10000 train_loss:3.8383 train_time:1416311ms step_avg:169.88ms\nstep:8348/10000 train_loss:3.7990 train_time:1416478ms step_avg:169.88ms\nstep:8349/10000 train_loss:3.7564 train_time:1416647ms step_avg:169.88ms\nstep:8350/10000 train_loss:3.7437 train_time:1416817ms step_avg:169.88ms\nstep:8351/10000 train_loss:3.7857 train_time:1416986ms step_avg:169.88ms\nstep:8352/10000 train_loss:3.7500 train_time:1417155ms step_avg:169.88ms\nstep:8353/10000 train_loss:3.8006 train_time:1417325ms step_avg:169.88ms\nstep:8354/10000 train_loss:3.6924 train_time:1417495ms step_avg:169.88ms\nstep:8355/10000 train_loss:3.7901 train_time:1417663ms step_avg:169.88ms\nstep:8356/10000 train_loss:3.7912 train_time:1417831ms step_avg:169.88ms\nstep:8357/10000 train_loss:3.7893 train_time:1417999ms step_avg:169.88ms\nstep:8358/10000 train_loss:3.9324 train_time:1418167ms step_avg:169.88ms\nstep:8359/10000 train_loss:4.0963 train_time:1418335ms step_avg:169.88ms\nstep:8360/10000 train_loss:3.7585 train_time:1418505ms step_avg:169.88ms\nstep:8361/10000 train_loss:3.8237 train_time:1418676ms step_avg:169.88ms\nstep:8362/10000 train_loss:3.8961 train_time:1418845ms step_avg:169.88ms\nstep:8363/10000 train_loss:3.7411 train_time:1419014ms step_avg:169.88ms\nstep:8364/10000 train_loss:3.7302 train_time:1419185ms step_avg:169.88ms\nstep:8365/10000 train_loss:3.8199 train_time:1419353ms step_avg:169.88ms\nstep:8366/10000 train_loss:3.8576 train_time:1419522ms step_avg:169.88ms\nstep:8367/10000 train_loss:3.6302 train_time:1419692ms step_avg:169.88ms\nstep:8368/10000 train_loss:3.7021 train_time:1419861ms step_avg:169.88ms\nstep:8369/10000 train_loss:3.8088 train_time:1420034ms step_avg:169.88ms\nstep:8370/10000 train_loss:3.9127 train_time:1420202ms step_avg:169.88ms\nstep:8371/10000 train_loss:3.8984 train_time:1420372ms step_avg:169.88ms\nstep:8372/10000 train_loss:3.7365 train_time:1420540ms step_avg:169.88ms\nstep:8373/10000 train_loss:3.7151 train_time:1420709ms step_avg:169.88ms\nstep:8374/10000 train_loss:3.8358 train_time:1420876ms step_avg:169.88ms\nstep:8375/10000 train_loss:3.6007 train_time:1421044ms step_avg:169.88ms\nstep:8376/10000 train_loss:3.7264 train_time:1421212ms step_avg:169.88ms\nstep:8377/10000 train_loss:3.8367 train_time:1421382ms step_avg:169.88ms\nstep:8378/10000 train_loss:3.8204 train_time:1421558ms step_avg:169.88ms\nstep:8379/10000 train_loss:3.7967 train_time:1421721ms step_avg:169.88ms\nstep:8380/10000 train_loss:3.8040 train_time:1421892ms step_avg:169.88ms\nstep:8381/10000 train_loss:3.6386 train_time:1422059ms step_avg:169.88ms\nstep:8382/10000 train_loss:3.8081 train_time:1422480ms step_avg:169.91ms\nstep:8383/10000 train_loss:3.7766 train_time:1422646ms step_avg:169.91ms\nstep:8384/10000 train_loss:3.7893 train_time:1422815ms step_avg:169.91ms\nstep:8385/10000 train_loss:3.7478 train_time:1422982ms step_avg:169.91ms\nstep:8386/10000 train_loss:3.7200 train_time:1423151ms step_avg:169.91ms\nstep:8387/10000 train_loss:3.7554 train_time:1423318ms step_avg:169.91ms\nstep:8388/10000 train_loss:3.8553 train_time:1423494ms step_avg:169.91ms\nstep:8389/10000 train_loss:3.7479 train_time:1423666ms step_avg:169.91ms\nstep:8390/10000 train_loss:3.8285 train_time:1423835ms step_avg:169.91ms\nstep:8391/10000 train_loss:3.7632 train_time:1424003ms step_avg:169.91ms\nstep:8392/10000 train_loss:3.7002 train_time:1424171ms step_avg:169.91ms\nstep:8393/10000 train_loss:3.6925 train_time:1424342ms step_avg:169.91ms\nstep:8394/10000 train_loss:3.8677 train_time:1424514ms step_avg:169.91ms\nstep:8395/10000 train_loss:3.7680 train_time:1424682ms step_avg:169.91ms\nstep:8396/10000 train_loss:3.5419 train_time:1424851ms step_avg:169.91ms\nstep:8397/10000 train_loss:4.1180 train_time:1425019ms step_avg:169.91ms\nstep:8398/10000 train_loss:3.7137 train_time:1425187ms step_avg:169.91ms\nstep:8399/10000 train_loss:4.0145 train_time:1425360ms step_avg:169.91ms\nstep:8400/10000 train_loss:3.7778 train_time:1425530ms step_avg:169.91ms\nstep:8401/10000 train_loss:3.9064 train_time:1425701ms step_avg:169.91ms\nstep:8402/10000 train_loss:3.6253 train_time:1425870ms step_avg:169.91ms\nstep:8403/10000 train_loss:3.8891 train_time:1426038ms step_avg:169.91ms\nstep:8404/10000 train_loss:3.9674 train_time:1426208ms step_avg:169.91ms\nstep:8405/10000 train_loss:3.6883 train_time:1426379ms step_avg:169.91ms\nstep:8406/10000 train_loss:3.8526 train_time:1426552ms step_avg:169.91ms\nstep:8407/10000 train_loss:3.8527 train_time:1426723ms step_avg:169.91ms\nstep:8408/10000 train_loss:3.8225 train_time:1426893ms step_avg:169.91ms\nstep:8409/10000 train_loss:3.8606 train_time:1427062ms step_avg:169.91ms\nstep:8410/10000 train_loss:3.8029 train_time:1427232ms step_avg:169.91ms\nstep:8411/10000 train_loss:3.7635 train_time:1427404ms step_avg:169.91ms\nstep:8412/10000 train_loss:3.6627 train_time:1427576ms step_avg:169.91ms\nstep:8413/10000 train_loss:3.7824 train_time:1427745ms step_avg:169.91ms\nstep:8414/10000 train_loss:3.8241 train_time:1427913ms step_avg:169.91ms\nstep:8415/10000 train_loss:3.7276 train_time:1428081ms step_avg:169.91ms\nstep:8416/10000 train_loss:3.7714 train_time:1428251ms step_avg:169.91ms\nstep:8417/10000 train_loss:3.9313 train_time:1428421ms step_avg:169.91ms\nstep:8418/10000 train_loss:3.7190 train_time:1428593ms step_avg:169.91ms\nstep:8419/10000 train_loss:3.8124 train_time:1428763ms step_avg:169.91ms\nstep:8420/10000 train_loss:3.7516 train_time:1428934ms step_avg:169.91ms\nstep:8421/10000 train_loss:3.7964 train_time:1429103ms step_avg:169.91ms\nstep:8422/10000 train_loss:3.8081 train_time:1429273ms step_avg:169.91ms\nstep:8423/10000 train_loss:3.9819 train_time:1429443ms step_avg:169.91ms\nstep:8424/10000 train_loss:3.8664 train_time:1429615ms step_avg:169.91ms\nstep:8425/10000 train_loss:3.7593 train_time:1429784ms step_avg:169.91ms\nstep:8426/10000 train_loss:3.8357 train_time:1429954ms step_avg:169.91ms\nstep:8427/10000 train_loss:3.8677 train_time:1430122ms step_avg:169.91ms\nstep:8428/10000 train_loss:3.7138 train_time:1430293ms step_avg:169.91ms\nstep:8429/10000 train_loss:3.8085 train_time:1430461ms step_avg:169.91ms\nstep:8430/10000 train_loss:3.9463 train_time:1430632ms step_avg:169.91ms\nstep:8431/10000 train_loss:3.9269 train_time:1430802ms step_avg:169.91ms\nstep:8432/10000 train_loss:3.6627 train_time:1430970ms step_avg:169.91ms\nstep:8433/10000 train_loss:3.6455 train_time:1431141ms step_avg:169.91ms\nstep:8434/10000 train_loss:3.8946 train_time:1431312ms step_avg:169.91ms\nstep:8435/10000 train_loss:3.7632 train_time:1431481ms step_avg:169.91ms\nstep:8436/10000 train_loss:3.9358 train_time:1431652ms step_avg:169.91ms\nstep:8437/10000 train_loss:3.7555 train_time:1431821ms step_avg:169.91ms\nstep:8438/10000 train_loss:3.8115 train_time:1431991ms step_avg:169.91ms\nstep:8439/10000 train_loss:3.6740 train_time:1432158ms step_avg:169.91ms\nstep:8440/10000 train_loss:3.6944 train_time:1432328ms step_avg:169.91ms\nstep:8441/10000 train_loss:3.8221 train_time:1432500ms step_avg:169.91ms\nstep:8442/10000 train_loss:3.9682 train_time:1432670ms step_avg:169.91ms\nstep:8443/10000 train_loss:3.8382 train_time:1432839ms step_avg:169.91ms\nstep:8444/10000 train_loss:4.0295 train_time:1433008ms step_avg:169.91ms\nstep:8445/10000 train_loss:3.8785 train_time:1433178ms step_avg:169.91ms\nstep:8446/10000 train_loss:3.7717 train_time:1433345ms step_avg:169.91ms\nstep:8447/10000 train_loss:3.6801 train_time:1433515ms step_avg:169.91ms\nstep:8448/10000 train_loss:3.8928 train_time:1433686ms step_avg:169.91ms\nstep:8449/10000 train_loss:3.8540 train_time:1433855ms step_avg:169.91ms\nstep:8450/10000 train_loss:3.9664 train_time:1434024ms step_avg:169.91ms\nstep:8451/10000 train_loss:3.7152 train_time:1434195ms step_avg:169.91ms\nstep:8452/10000 train_loss:3.8181 train_time:1434365ms step_avg:169.91ms\nstep:8453/10000 train_loss:3.7767 train_time:1434536ms step_avg:169.91ms\nstep:8454/10000 train_loss:4.0335 train_time:1434705ms step_avg:169.91ms\nstep:8455/10000 train_loss:3.7223 train_time:1434873ms step_avg:169.91ms\nstep:8456/10000 train_loss:3.8287 train_time:1435041ms step_avg:169.91ms\nstep:8457/10000 train_loss:3.8021 train_time:1435210ms step_avg:169.91ms\nstep:8458/10000 train_loss:3.6676 train_time:1435381ms step_avg:169.91ms\nstep:8459/10000 train_loss:4.1060 train_time:1435550ms step_avg:169.91ms\nstep:8460/10000 train_loss:3.6603 train_time:1435719ms step_avg:169.91ms\nstep:8461/10000 train_loss:3.8334 train_time:1435887ms step_avg:169.91ms\nstep:8462/10000 train_loss:3.7580 train_time:1436055ms step_avg:169.91ms\nstep:8463/10000 train_loss:3.6649 train_time:1436224ms step_avg:169.91ms\nstep:8464/10000 train_loss:4.1115 train_time:1436395ms step_avg:169.91ms\nstep:8465/10000 train_loss:3.7282 train_time:1436564ms step_avg:169.91ms\nstep:8466/10000 train_loss:3.6380 train_time:1436734ms step_avg:169.91ms\nstep:8467/10000 train_loss:3.8375 train_time:1436904ms step_avg:169.91ms\nstep:8468/10000 train_loss:3.8475 train_time:1437074ms step_avg:169.91ms\nstep:8469/10000 train_loss:3.8340 train_time:1437243ms step_avg:169.91ms\nstep:8470/10000 train_loss:3.8024 train_time:1437413ms step_avg:169.91ms\nstep:8471/10000 train_loss:3.7525 train_time:1437584ms step_avg:169.91ms\nstep:8472/10000 train_loss:3.7343 train_time:1437753ms step_avg:169.91ms\nstep:8473/10000 train_loss:3.6947 train_time:1437921ms step_avg:169.91ms\nstep:8474/10000 train_loss:3.8608 train_time:1438091ms step_avg:169.91ms\nstep:8475/10000 train_loss:3.7036 train_time:1438258ms step_avg:169.91ms\nstep:8476/10000 train_loss:3.8730 train_time:1438429ms step_avg:169.91ms\nstep:8477/10000 train_loss:3.8748 train_time:1438600ms step_avg:169.91ms\nstep:8478/10000 train_loss:3.7653 train_time:1438768ms step_avg:169.91ms\nstep:8479/10000 train_loss:3.7890 train_time:1438939ms step_avg:169.91ms\nstep:8480/10000 train_loss:3.8270 train_time:1439110ms step_avg:169.91ms\nstep:8481/10000 train_loss:3.7307 train_time:1439281ms step_avg:169.91ms\nstep:8482/10000 train_loss:3.9497 train_time:1439450ms step_avg:169.91ms\nstep:8483/10000 train_loss:3.7061 train_time:1439621ms step_avg:169.91ms\nstep:8484/10000 train_loss:3.7825 train_time:1439789ms step_avg:169.91ms\nstep:8485/10000 train_loss:3.7680 train_time:1439957ms step_avg:169.91ms\nstep:8486/10000 train_loss:3.9033 train_time:1440126ms step_avg:169.91ms\nstep:8487/10000 train_loss:4.0947 train_time:1440296ms step_avg:169.91ms\nstep:8488/10000 train_loss:3.8534 train_time:1440463ms step_avg:169.91ms\nstep:8489/10000 train_loss:3.6950 train_time:1440634ms step_avg:169.91ms\nstep:8490/10000 train_loss:3.9033 train_time:1440804ms step_avg:169.91ms\nstep:8491/10000 train_loss:3.7178 train_time:1440973ms step_avg:169.91ms\nstep:8492/10000 train_loss:3.7090 train_time:1441144ms step_avg:169.91ms\nstep:8493/10000 train_loss:4.4228 train_time:1441314ms step_avg:169.91ms\nstep:8494/10000 train_loss:3.8356 train_time:1441482ms step_avg:169.91ms\nstep:8495/10000 train_loss:3.7456 train_time:1441651ms step_avg:169.91ms\nstep:8496/10000 train_loss:3.7602 train_time:1441819ms step_avg:169.91ms\nstep:8497/10000 train_loss:3.6780 train_time:1441987ms step_avg:169.91ms\nstep:8498/10000 train_loss:3.7058 train_time:1442157ms step_avg:169.91ms\nstep:8499/10000 train_loss:3.8439 train_time:1442326ms step_avg:169.91ms\nstep:8500/10000 train_loss:3.4932 train_time:1442497ms step_avg:169.91ms\nstep:8501/10000 train_loss:3.8475 train_time:1442665ms step_avg:169.91ms\nstep:8502/10000 train_loss:3.7208 train_time:1442836ms step_avg:169.91ms\nstep:8503/10000 train_loss:3.7228 train_time:1443002ms step_avg:169.90ms\nstep:8504/10000 train_loss:3.6853 train_time:1443173ms step_avg:169.90ms\nstep:8505/10000 train_loss:3.8172 train_time:1443340ms step_avg:169.90ms\nstep:8506/10000 train_loss:3.8408 train_time:1443510ms step_avg:169.90ms\nstep:8507/10000 train_loss:3.8433 train_time:1443680ms step_avg:169.90ms\nstep:8508/10000 train_loss:3.8230 train_time:1443848ms step_avg:169.90ms\nstep:8509/10000 train_loss:3.7436 train_time:1444017ms step_avg:169.90ms\nstep:8510/10000 train_loss:3.7861 train_time:1444188ms step_avg:169.90ms\nstep:8511/10000 train_loss:3.7928 train_time:1444358ms step_avg:169.90ms\nstep:8512/10000 train_loss:3.7907 train_time:1444529ms step_avg:169.90ms\nstep:8513/10000 train_loss:3.7758 train_time:1444698ms step_avg:169.90ms\nstep:8514/10000 train_loss:3.8104 train_time:1444862ms step_avg:169.90ms\nstep:8515/10000 train_loss:3.6916 train_time:1445033ms step_avg:169.90ms\nstep:8516/10000 train_loss:3.8645 train_time:1445201ms step_avg:169.90ms\nstep:8517/10000 train_loss:3.6501 train_time:1445374ms step_avg:169.90ms\nstep:8518/10000 train_loss:3.7846 train_time:1445543ms step_avg:169.90ms\nstep:8519/10000 train_loss:3.8907 train_time:1445713ms step_avg:169.90ms\nstep:8520/10000 train_loss:3.7621 train_time:1445883ms step_avg:169.90ms\nstep:8521/10000 train_loss:3.6698 train_time:1446053ms step_avg:169.90ms\nstep:8522/10000 train_loss:3.7939 train_time:1446221ms step_avg:169.90ms\nstep:8523/10000 train_loss:3.6907 train_time:1446389ms step_avg:169.90ms\nstep:8524/10000 train_loss:3.9066 train_time:1446558ms step_avg:169.90ms\nstep:8525/10000 train_loss:3.7850 train_time:1446733ms step_avg:169.90ms\nstep:8526/10000 train_loss:3.8354 train_time:1446904ms step_avg:169.90ms\nstep:8527/10000 train_loss:4.0503 train_time:1447066ms step_avg:169.90ms\nstep:8528/10000 train_loss:3.8709 train_time:1447237ms step_avg:169.90ms\nstep:8529/10000 train_loss:3.8821 train_time:1447405ms step_avg:169.90ms\nstep:8530/10000 train_loss:3.6557 train_time:1447574ms step_avg:169.90ms\nstep:8531/10000 train_loss:3.9415 train_time:1447741ms step_avg:169.90ms\nstep:8532/10000 train_loss:3.7475 train_time:1447911ms step_avg:169.90ms\nstep:8533/10000 train_loss:3.7160 train_time:1448079ms step_avg:169.90ms\nstep:8534/10000 train_loss:3.7199 train_time:1448249ms step_avg:169.90ms\nstep:8535/10000 train_loss:3.7782 train_time:1448419ms step_avg:169.90ms\nstep:8536/10000 train_loss:3.6953 train_time:1448587ms step_avg:169.90ms\nstep:8537/10000 train_loss:3.8124 train_time:1448757ms step_avg:169.90ms\nstep:8538/10000 train_loss:3.8583 train_time:1448926ms step_avg:169.90ms\nstep:8539/10000 train_loss:3.8725 train_time:1449097ms step_avg:169.90ms\nstep:8540/10000 train_loss:3.7384 train_time:1449266ms step_avg:169.90ms\nstep:8541/10000 train_loss:3.8803 train_time:1449437ms step_avg:169.90ms\nstep:8542/10000 train_loss:3.7108 train_time:1449607ms step_avg:169.90ms\nstep:8543/10000 train_loss:3.7582 train_time:1449776ms step_avg:169.90ms\nstep:8544/10000 train_loss:3.8880 train_time:1449946ms step_avg:169.90ms\nstep:8545/10000 train_loss:4.0179 train_time:1450116ms step_avg:169.90ms\nstep:8546/10000 train_loss:3.6570 train_time:1450285ms step_avg:169.90ms\nstep:8547/10000 train_loss:3.8109 train_time:1450456ms step_avg:169.90ms\nstep:8548/10000 train_loss:3.7676 train_time:1450625ms step_avg:169.90ms\nstep:8549/10000 train_loss:3.7424 train_time:1450793ms step_avg:169.90ms\nstep:8550/10000 train_loss:3.7236 train_time:1450961ms step_avg:169.90ms\nstep:8551/10000 train_loss:3.8791 train_time:1451131ms step_avg:169.90ms\nstep:8552/10000 train_loss:3.6922 train_time:1451302ms step_avg:169.90ms\nstep:8553/10000 train_loss:3.9196 train_time:1451474ms step_avg:169.90ms\nstep:8554/10000 train_loss:3.7498 train_time:1451643ms step_avg:169.90ms\nstep:8555/10000 train_loss:3.6858 train_time:1451814ms step_avg:169.90ms\nstep:8556/10000 train_loss:3.7901 train_time:1451982ms step_avg:169.90ms\nstep:8557/10000 train_loss:3.7675 train_time:1452152ms step_avg:169.90ms\nstep:8558/10000 train_loss:3.8716 train_time:1452324ms step_avg:169.90ms\nstep:8559/10000 train_loss:3.7314 train_time:1452494ms step_avg:169.90ms\nstep:8560/10000 train_loss:3.8728 train_time:1452663ms step_avg:169.90ms\nstep:8561/10000 train_loss:3.6928 train_time:1452834ms step_avg:169.90ms\nstep:8562/10000 train_loss:3.9349 train_time:1453002ms step_avg:169.90ms\nstep:8563/10000 train_loss:3.7519 train_time:1453172ms step_avg:169.90ms\nstep:8564/10000 train_loss:3.8735 train_time:1453342ms step_avg:169.90ms\nstep:8565/10000 train_loss:3.7734 train_time:1453513ms step_avg:169.90ms\nstep:8566/10000 train_loss:3.8405 train_time:1453681ms step_avg:169.90ms\nstep:8567/10000 train_loss:3.7379 train_time:1453854ms step_avg:169.90ms\nstep:8568/10000 train_loss:3.8358 train_time:1454021ms step_avg:169.90ms\nstep:8569/10000 train_loss:3.5378 train_time:1454190ms step_avg:169.90ms\nstep:8570/10000 train_loss:3.9324 train_time:1454360ms step_avg:169.90ms\nstep:8571/10000 train_loss:3.8386 train_time:1454529ms step_avg:169.90ms\nstep:8572/10000 train_loss:3.7960 train_time:1454700ms step_avg:169.90ms\nstep:8573/10000 train_loss:3.7581 train_time:1454870ms step_avg:169.90ms\nstep:8574/10000 train_loss:3.8058 train_time:1455041ms step_avg:169.90ms\nstep:8575/10000 train_loss:3.7758 train_time:1455209ms step_avg:169.90ms\nstep:8576/10000 train_loss:3.7968 train_time:1455378ms step_avg:169.90ms\nstep:8577/10000 train_loss:3.8388 train_time:1455547ms step_avg:169.90ms\nstep:8578/10000 train_loss:3.6614 train_time:1455720ms step_avg:169.90ms\nstep:8579/10000 train_loss:3.8560 train_time:1455889ms step_avg:169.90ms\nstep:8580/10000 train_loss:3.8818 train_time:1456061ms step_avg:169.90ms\nstep:8581/10000 train_loss:4.0320 train_time:1456231ms step_avg:169.90ms\nstep:8582/10000 train_loss:3.9509 train_time:1456401ms step_avg:169.90ms\nstep:8583/10000 train_loss:3.9017 train_time:1456570ms step_avg:169.90ms\nstep:8584/10000 train_loss:4.1645 train_time:1456738ms step_avg:169.90ms\nstep:8585/10000 train_loss:3.8610 train_time:1456906ms step_avg:169.90ms\nstep:8586/10000 train_loss:3.7250 train_time:1457078ms step_avg:169.90ms\nstep:8587/10000 train_loss:3.7538 train_time:1457247ms step_avg:169.90ms\nstep:8588/10000 train_loss:3.7019 train_time:1457418ms step_avg:169.90ms\nstep:8589/10000 train_loss:3.8910 train_time:1457584ms step_avg:169.90ms\nstep:8590/10000 train_loss:3.7628 train_time:1457754ms step_avg:169.90ms\nstep:8591/10000 train_loss:3.7903 train_time:1457923ms step_avg:169.90ms\nstep:8592/10000 train_loss:3.7491 train_time:1458093ms step_avg:169.90ms\nstep:8593/10000 train_loss:3.7700 train_time:1458261ms step_avg:169.90ms\nstep:8594/10000 train_loss:3.8162 train_time:1458434ms step_avg:169.90ms\nstep:8595/10000 train_loss:3.6693 train_time:1458607ms step_avg:169.90ms\nstep:8596/10000 train_loss:3.7742 train_time:1458776ms step_avg:169.90ms\nstep:8597/10000 train_loss:3.7660 train_time:1458944ms step_avg:169.90ms\nstep:8598/10000 train_loss:3.7872 train_time:1459114ms step_avg:169.90ms\nstep:8599/10000 train_loss:3.6684 train_time:1459283ms step_avg:169.90ms\nstep:8600/10000 train_loss:3.8914 train_time:1459452ms step_avg:169.90ms\nstep:8601/10000 train_loss:4.0158 train_time:1459619ms step_avg:169.90ms\nstep:8602/10000 train_loss:3.7829 train_time:1459788ms step_avg:169.90ms\nstep:8603/10000 train_loss:3.8051 train_time:1459959ms step_avg:169.90ms\nstep:8604/10000 train_loss:3.8359 train_time:1460127ms step_avg:169.90ms\nstep:8605/10000 train_loss:3.8576 train_time:1460296ms step_avg:169.90ms\nstep:8606/10000 train_loss:3.7281 train_time:1460466ms step_avg:169.90ms\nstep:8607/10000 train_loss:3.7952 train_time:1460637ms step_avg:169.90ms\nstep:8608/10000 train_loss:4.2569 train_time:1460808ms step_avg:169.90ms\nstep:8609/10000 train_loss:3.8062 train_time:1460979ms step_avg:169.90ms\nstep:8610/10000 train_loss:4.0512 train_time:1461147ms step_avg:169.90ms\nstep:8611/10000 train_loss:3.7000 train_time:1461318ms step_avg:169.90ms\nstep:8612/10000 train_loss:3.7456 train_time:1461486ms step_avg:169.90ms\nstep:8613/10000 train_loss:3.7811 train_time:1461660ms step_avg:169.90ms\nstep:8614/10000 train_loss:3.9654 train_time:1461829ms step_avg:169.90ms\nstep:8615/10000 train_loss:3.7594 train_time:1462000ms step_avg:169.90ms\nstep:8616/10000 train_loss:3.7596 train_time:1462169ms step_avg:169.90ms\nstep:8617/10000 train_loss:3.7509 train_time:1462338ms step_avg:169.90ms\nstep:8618/10000 train_loss:3.7285 train_time:1462507ms step_avg:169.90ms\nstep:8619/10000 train_loss:3.8052 train_time:1462676ms step_avg:169.90ms\nstep:8620/10000 train_loss:3.7806 train_time:1462846ms step_avg:169.90ms\nstep:8621/10000 train_loss:3.8058 train_time:1463015ms step_avg:169.90ms\nstep:8622/10000 train_loss:3.8739 train_time:1463183ms step_avg:169.90ms\nstep:8623/10000 train_loss:4.0662 train_time:1463354ms step_avg:169.90ms\nstep:8624/10000 train_loss:3.8381 train_time:1463525ms step_avg:169.90ms\nstep:8625/10000 train_loss:3.7637 train_time:1463694ms step_avg:169.90ms\nstep:8626/10000 train_loss:3.8530 train_time:1463863ms step_avg:169.90ms\nstep:8627/10000 train_loss:3.7825 train_time:1464031ms step_avg:169.90ms\nstep:8628/10000 train_loss:3.8160 train_time:1464200ms step_avg:169.90ms\nstep:8629/10000 train_loss:4.7839 train_time:1464369ms step_avg:169.90ms\nstep:8630/10000 train_loss:3.8320 train_time:1464538ms step_avg:169.90ms\nstep:8631/10000 train_loss:3.6845 train_time:1464707ms step_avg:169.90ms\nstep:8632/10000 train_loss:3.7263 train_time:1464875ms step_avg:169.90ms\nstep:8633/10000 train_loss:3.7500 train_time:1465043ms step_avg:169.90ms\nstep:8634/10000 train_loss:3.8182 train_time:1465213ms step_avg:169.90ms\nstep:8635/10000 train_loss:3.9071 train_time:1465381ms step_avg:169.90ms\nstep:8636/10000 train_loss:3.9434 train_time:1465552ms step_avg:169.90ms\nstep:8637/10000 train_loss:4.0005 train_time:1465724ms step_avg:169.90ms\nstep:8638/10000 train_loss:3.6116 train_time:1465894ms step_avg:169.90ms\nstep:8639/10000 train_loss:4.0168 train_time:1466062ms step_avg:169.90ms\nstep:8640/10000 train_loss:3.5807 train_time:1466232ms step_avg:169.90ms\nstep:8641/10000 train_loss:3.8452 train_time:1466403ms step_avg:169.90ms\nstep:8642/10000 train_loss:3.7724 train_time:1466573ms step_avg:169.90ms\nstep:8643/10000 train_loss:3.7706 train_time:1466746ms step_avg:169.90ms\nstep:8644/10000 train_loss:3.7305 train_time:1466914ms step_avg:169.90ms\nstep:8645/10000 train_loss:3.8038 train_time:1467082ms step_avg:169.90ms\nstep:8646/10000 train_loss:3.6942 train_time:1467251ms step_avg:169.90ms\nstep:8647/10000 train_loss:3.6571 train_time:1467422ms step_avg:169.90ms\nstep:8648/10000 train_loss:3.8646 train_time:1467592ms step_avg:169.90ms\nstep:8649/10000 train_loss:3.7912 train_time:1467761ms step_avg:169.90ms\nstep:8650/10000 train_loss:3.8126 train_time:1467931ms step_avg:169.90ms\nstep:8651/10000 train_loss:3.8534 train_time:1468101ms step_avg:169.90ms\nstep:8652/10000 train_loss:3.7910 train_time:1468271ms step_avg:169.90ms\nstep:8653/10000 train_loss:3.7070 train_time:1468438ms step_avg:169.90ms\nstep:8654/10000 train_loss:3.6533 train_time:1468607ms step_avg:169.90ms\nstep:8655/10000 train_loss:3.6553 train_time:1468777ms step_avg:169.90ms\nstep:8656/10000 train_loss:3.9331 train_time:1468953ms step_avg:169.90ms\nstep:8657/10000 train_loss:3.6807 train_time:1469123ms step_avg:169.90ms\nstep:8658/10000 train_loss:3.7808 train_time:1469290ms step_avg:169.90ms\nstep:8659/10000 train_loss:4.0545 train_time:1469454ms step_avg:169.90ms\nstep:8660/10000 train_loss:3.8839 train_time:1469623ms step_avg:169.90ms\nstep:8661/10000 train_loss:3.7869 train_time:1469794ms step_avg:169.90ms\nstep:8662/10000 train_loss:3.8827 train_time:1469962ms step_avg:169.90ms\nstep:8663/10000 train_loss:3.8246 train_time:1470132ms step_avg:169.90ms\nstep:8664/10000 train_loss:3.7918 train_time:1470302ms step_avg:169.90ms\nstep:8665/10000 train_loss:3.6554 train_time:1470474ms step_avg:169.90ms\nstep:8666/10000 train_loss:3.7626 train_time:1470642ms step_avg:169.90ms\nstep:8667/10000 train_loss:3.8824 train_time:1470813ms step_avg:169.90ms\nstep:8668/10000 train_loss:3.7519 train_time:1470982ms step_avg:169.90ms\nstep:8669/10000 train_loss:3.6891 train_time:1471154ms step_avg:169.90ms\nstep:8670/10000 train_loss:3.7696 train_time:1471323ms step_avg:169.90ms\nstep:8671/10000 train_loss:3.9233 train_time:1471494ms step_avg:169.90ms\nstep:8672/10000 train_loss:3.7777 train_time:1471663ms step_avg:169.90ms\nstep:8673/10000 train_loss:3.7868 train_time:1471833ms step_avg:169.90ms\nstep:8674/10000 train_loss:3.9463 train_time:1472002ms step_avg:169.90ms\nstep:8675/10000 train_loss:3.9893 train_time:1472173ms step_avg:169.90ms\nstep:8676/10000 train_loss:3.8490 train_time:1472342ms step_avg:169.90ms\nstep:8677/10000 train_loss:3.8056 train_time:1472512ms step_avg:169.90ms\nstep:8678/10000 train_loss:3.7662 train_time:1472682ms step_avg:169.90ms\nstep:8679/10000 train_loss:3.7812 train_time:1472853ms step_avg:169.90ms\nstep:8680/10000 train_loss:3.8630 train_time:1473020ms step_avg:169.90ms\nstep:8681/10000 train_loss:4.1389 train_time:1473190ms step_avg:169.90ms\nstep:8682/10000 train_loss:3.6961 train_time:1473359ms step_avg:169.90ms\nstep:8683/10000 train_loss:3.8061 train_time:1473528ms step_avg:169.90ms\nstep:8684/10000 train_loss:3.8364 train_time:1473699ms step_avg:169.90ms\nstep:8685/10000 train_loss:3.9105 train_time:1473868ms step_avg:169.90ms\nstep:8686/10000 train_loss:3.9033 train_time:1474039ms step_avg:169.90ms\nstep:8687/10000 train_loss:3.8172 train_time:1474207ms step_avg:169.90ms\nstep:8688/10000 train_loss:3.6852 train_time:1474376ms step_avg:169.90ms\nstep:8689/10000 train_loss:3.7206 train_time:1474544ms step_avg:169.90ms\nstep:8690/10000 train_loss:3.8414 train_time:1474714ms step_avg:169.90ms\nstep:8691/10000 train_loss:3.9311 train_time:1474883ms step_avg:169.90ms\nstep:8692/10000 train_loss:3.7542 train_time:1475052ms step_avg:169.90ms\nstep:8693/10000 train_loss:3.9273 train_time:1475221ms step_avg:169.90ms\nstep:8694/10000 train_loss:3.6636 train_time:1475392ms step_avg:169.90ms\nstep:8695/10000 train_loss:3.9348 train_time:1475562ms step_avg:169.90ms\nstep:8696/10000 train_loss:3.8367 train_time:1475731ms step_avg:169.90ms\nstep:8697/10000 train_loss:3.7356 train_time:1475900ms step_avg:169.90ms\nstep:8698/10000 train_loss:4.1072 train_time:1476070ms step_avg:169.90ms\nstep:8699/10000 train_loss:3.8600 train_time:1476238ms step_avg:169.90ms\nstep:8700/10000 train_loss:3.9234 train_time:1476409ms step_avg:169.90ms\nstep:8701/10000 train_loss:3.7086 train_time:1476575ms step_avg:169.90ms\nstep:8702/10000 train_loss:3.8165 train_time:1476744ms step_avg:169.90ms\nstep:8703/10000 train_loss:3.7891 train_time:1476914ms step_avg:169.90ms\nstep:8704/10000 train_loss:3.7547 train_time:1477084ms step_avg:169.90ms\nstep:8705/10000 train_loss:3.8784 train_time:1477255ms step_avg:169.90ms\nstep:8706/10000 train_loss:3.7852 train_time:1477424ms step_avg:169.90ms\nstep:8707/10000 train_loss:3.6575 train_time:1477593ms step_avg:169.90ms\nstep:8708/10000 train_loss:3.7199 train_time:1477761ms step_avg:169.90ms\nstep:8709/10000 train_loss:3.7151 train_time:1477931ms step_avg:169.90ms\nstep:8710/10000 train_loss:3.8920 train_time:1478099ms step_avg:169.90ms\nstep:8711/10000 train_loss:3.8592 train_time:1478269ms step_avg:169.90ms\nstep:8712/10000 train_loss:3.7093 train_time:1478440ms step_avg:169.90ms\nstep:8713/10000 train_loss:3.8861 train_time:1478608ms step_avg:169.90ms\nstep:8714/10000 train_loss:4.1920 train_time:1478776ms step_avg:169.90ms\nstep:8715/10000 train_loss:3.7375 train_time:1478945ms step_avg:169.90ms\nstep:8716/10000 train_loss:3.7923 train_time:1479115ms step_avg:169.90ms\nstep:8717/10000 train_loss:3.7605 train_time:1479283ms step_avg:169.90ms\nstep:8718/10000 train_loss:3.8379 train_time:1479452ms step_avg:169.90ms\nstep:8719/10000 train_loss:3.7140 train_time:1479619ms step_avg:169.90ms\nstep:8720/10000 train_loss:3.7695 train_time:1479788ms step_avg:169.90ms\nstep:8721/10000 train_loss:3.8394 train_time:1479958ms step_avg:169.90ms\nstep:8722/10000 train_loss:3.9536 train_time:1480128ms step_avg:169.90ms\nstep:8723/10000 train_loss:3.8084 train_time:1480297ms step_avg:169.90ms\nstep:8724/10000 train_loss:3.7287 train_time:1480464ms step_avg:169.89ms\nstep:8725/10000 train_loss:3.8685 train_time:1480634ms step_avg:169.89ms\nstep:8726/10000 train_loss:3.8146 train_time:1480802ms step_avg:169.89ms\nstep:8727/10000 train_loss:3.7725 train_time:1480970ms step_avg:169.89ms\nstep:8728/10000 train_loss:3.9412 train_time:1481142ms step_avg:169.89ms\nstep:8729/10000 train_loss:3.7447 train_time:1481313ms step_avg:169.89ms\nstep:8730/10000 train_loss:4.0719 train_time:1481482ms step_avg:169.89ms\nstep:8731/10000 train_loss:3.7293 train_time:1481653ms step_avg:169.89ms\nstep:8732/10000 train_loss:3.7524 train_time:1481821ms step_avg:169.89ms\nstep:8733/10000 train_loss:3.7204 train_time:1481990ms step_avg:169.89ms\nstep:8734/10000 train_loss:3.7735 train_time:1482161ms step_avg:169.89ms\nstep:8735/10000 train_loss:3.8647 train_time:1482334ms step_avg:169.89ms\nstep:8736/10000 train_loss:3.7411 train_time:1482503ms step_avg:169.89ms\nstep:8737/10000 train_loss:3.7297 train_time:1482675ms step_avg:169.90ms\nstep:8738/10000 train_loss:3.6420 train_time:1482843ms step_avg:169.89ms\nstep:8739/10000 train_loss:3.8900 train_time:1483013ms step_avg:169.89ms\nstep:8740/10000 train_loss:3.8636 train_time:1483183ms step_avg:169.89ms\nstep:8741/10000 train_loss:3.9155 train_time:1483354ms step_avg:169.90ms\nstep:8742/10000 train_loss:4.2396 train_time:1483523ms step_avg:169.89ms\nstep:8743/10000 train_loss:3.8354 train_time:1483693ms step_avg:169.90ms\nstep:8744/10000 train_loss:3.7893 train_time:1483861ms step_avg:169.89ms\nstep:8745/10000 train_loss:3.7060 train_time:1484028ms step_avg:169.89ms\nstep:8746/10000 train_loss:3.9106 train_time:1484197ms step_avg:169.89ms\nstep:8747/10000 train_loss:3.8326 train_time:1484368ms step_avg:169.89ms\nstep:8748/10000 train_loss:3.8158 train_time:1484541ms step_avg:169.89ms\nstep:8749/10000 train_loss:3.9263 train_time:1484710ms step_avg:169.89ms\nstep:8750/10000 train_loss:3.7715 train_time:1484879ms step_avg:169.89ms\nstep:8751/10000 train_loss:3.6601 train_time:1485047ms step_avg:169.89ms\nstep:8752/10000 train_loss:3.6948 train_time:1485216ms step_avg:169.89ms\nstep:8753/10000 train_loss:3.7587 train_time:1485392ms step_avg:169.90ms\nstep:8754/10000 train_loss:3.7393 train_time:1485556ms step_avg:169.89ms\nstep:8755/10000 train_loss:3.7163 train_time:1485725ms step_avg:169.89ms\nstep:8756/10000 train_loss:3.8317 train_time:1485893ms step_avg:169.89ms\nstep:8757/10000 train_loss:3.6234 train_time:1486064ms step_avg:169.89ms\nstep:8758/10000 train_loss:3.8056 train_time:1486234ms step_avg:169.89ms\nstep:8759/10000 train_loss:3.7969 train_time:1486404ms step_avg:169.89ms\nstep:8760/10000 train_loss:4.0234 train_time:1486573ms step_avg:169.89ms\nstep:8761/10000 train_loss:3.8893 train_time:1486742ms step_avg:169.89ms\nstep:8762/10000 train_loss:3.8661 train_time:1486912ms step_avg:169.89ms\nstep:8763/10000 train_loss:3.8527 train_time:1487082ms step_avg:169.89ms\nstep:8764/10000 train_loss:3.6506 train_time:1487252ms step_avg:169.89ms\nstep:8765/10000 train_loss:3.7620 train_time:1487422ms step_avg:169.89ms\nstep:8766/10000 train_loss:3.5474 train_time:1487593ms step_avg:169.89ms\nstep:8767/10000 train_loss:3.8397 train_time:1487761ms step_avg:169.89ms\nstep:8768/10000 train_loss:3.7142 train_time:1487933ms step_avg:169.89ms\nstep:8769/10000 train_loss:3.7249 train_time:1488102ms step_avg:169.89ms\nstep:8770/10000 train_loss:3.7190 train_time:1488273ms step_avg:169.89ms\nstep:8771/10000 train_loss:3.7020 train_time:1488441ms step_avg:169.89ms\nstep:8772/10000 train_loss:3.6109 train_time:1488610ms step_avg:169.89ms\nstep:8773/10000 train_loss:3.7857 train_time:1488779ms step_avg:169.89ms\nstep:8774/10000 train_loss:3.7920 train_time:1488949ms step_avg:169.89ms\nstep:8775/10000 train_loss:3.7316 train_time:1489119ms step_avg:169.89ms\nstep:8776/10000 train_loss:3.6995 train_time:1489288ms step_avg:169.89ms\nstep:8777/10000 train_loss:3.7208 train_time:1489458ms step_avg:169.89ms\nstep:8778/10000 train_loss:3.6783 train_time:1489628ms step_avg:169.89ms\nstep:8779/10000 train_loss:3.6784 train_time:1489794ms step_avg:169.89ms\nstep:8780/10000 train_loss:3.8380 train_time:1489962ms step_avg:169.89ms\nstep:8781/10000 train_loss:3.7751 train_time:1490132ms step_avg:169.89ms\nstep:8782/10000 train_loss:3.8658 train_time:1490304ms step_avg:169.89ms\nstep:8783/10000 train_loss:3.7675 train_time:1490476ms step_avg:169.89ms\nstep:8784/10000 train_loss:3.7985 train_time:1490646ms step_avg:169.89ms\nstep:8785/10000 train_loss:3.6106 train_time:1490814ms step_avg:169.89ms\nstep:8786/10000 train_loss:3.8806 train_time:1490982ms step_avg:169.89ms\nstep:8787/10000 train_loss:3.7322 train_time:1491154ms step_avg:169.89ms\nstep:8788/10000 train_loss:4.0022 train_time:1491322ms step_avg:169.89ms\nstep:8789/10000 train_loss:3.8074 train_time:1491494ms step_avg:169.89ms\nstep:8790/10000 train_loss:3.8899 train_time:1491664ms step_avg:169.89ms\nstep:8791/10000 train_loss:3.7673 train_time:1491836ms step_avg:169.89ms\nstep:8792/10000 train_loss:3.7840 train_time:1492002ms step_avg:169.89ms\nstep:8793/10000 train_loss:3.9079 train_time:1492172ms step_avg:169.89ms\nstep:8794/10000 train_loss:3.8490 train_time:1492341ms step_avg:169.89ms\nstep:8795/10000 train_loss:3.8417 train_time:1492513ms step_avg:169.89ms\nstep:8796/10000 train_loss:3.8628 train_time:1492680ms step_avg:169.89ms\nstep:8797/10000 train_loss:3.8333 train_time:1492850ms step_avg:169.89ms\nstep:8798/10000 train_loss:3.8578 train_time:1493020ms step_avg:169.89ms\nstep:8799/10000 train_loss:3.9488 train_time:1493189ms step_avg:169.89ms\nstep:8800/10000 train_loss:3.7798 train_time:1493358ms step_avg:169.89ms\nstep:8801/10000 train_loss:3.7401 train_time:1493527ms step_avg:169.89ms\nstep:8802/10000 train_loss:3.8521 train_time:1493699ms step_avg:169.89ms\nstep:8803/10000 train_loss:3.7501 train_time:1493869ms step_avg:169.89ms\nstep:8804/10000 train_loss:3.6888 train_time:1494037ms step_avg:169.89ms\nstep:8805/10000 train_loss:3.8923 train_time:1494206ms step_avg:169.89ms\nstep:8806/10000 train_loss:3.8169 train_time:1494377ms step_avg:169.89ms\nstep:8807/10000 train_loss:3.8013 train_time:1494544ms step_avg:169.89ms\nstep:8808/10000 train_loss:3.7905 train_time:1494714ms step_avg:169.89ms\nstep:8809/10000 train_loss:3.7966 train_time:1494882ms step_avg:169.89ms\nstep:8810/10000 train_loss:3.9566 train_time:1495053ms step_avg:169.89ms\nstep:8811/10000 train_loss:3.7421 train_time:1495220ms step_avg:169.89ms\nstep:8812/10000 train_loss:3.7485 train_time:1495388ms step_avg:169.89ms\nstep:8813/10000 train_loss:3.8416 train_time:1495556ms step_avg:169.89ms\nstep:8814/10000 train_loss:3.7746 train_time:1495726ms step_avg:169.89ms\nstep:8815/10000 train_loss:3.7513 train_time:1495897ms step_avg:169.89ms\nstep:8816/10000 train_loss:3.7419 train_time:1496068ms step_avg:169.89ms\nstep:8817/10000 train_loss:3.7292 train_time:1496239ms step_avg:169.89ms\nstep:8818/10000 train_loss:3.8468 train_time:1496409ms step_avg:169.89ms\nstep:8819/10000 train_loss:3.6868 train_time:1496578ms step_avg:169.89ms\nstep:8820/10000 train_loss:3.8041 train_time:1496748ms step_avg:169.89ms\nstep:8821/10000 train_loss:3.8778 train_time:1496919ms step_avg:169.89ms\nstep:8822/10000 train_loss:3.8068 train_time:1497089ms step_avg:169.89ms\nstep:8823/10000 train_loss:3.7597 train_time:1497261ms step_avg:169.89ms\nstep:8824/10000 train_loss:3.7561 train_time:1497431ms step_avg:169.89ms\nstep:8825/10000 train_loss:3.7849 train_time:1497601ms step_avg:169.89ms\nstep:8826/10000 train_loss:3.6863 train_time:1497772ms step_avg:169.89ms\nstep:8827/10000 train_loss:3.7320 train_time:1497943ms step_avg:169.89ms\nstep:8828/10000 train_loss:3.7386 train_time:1498114ms step_avg:169.89ms\nstep:8829/10000 train_loss:3.9080 train_time:1498282ms step_avg:169.89ms\nstep:8830/10000 train_loss:3.9440 train_time:1498450ms step_avg:169.89ms\nstep:8831/10000 train_loss:5.0594 train_time:1498620ms step_avg:169.89ms\nstep:8832/10000 train_loss:4.2489 train_time:1498788ms step_avg:169.89ms\nstep:8833/10000 train_loss:3.8105 train_time:1498959ms step_avg:169.89ms\nstep:8834/10000 train_loss:3.9138 train_time:1499130ms step_avg:169.89ms\nstep:8835/10000 train_loss:3.8579 train_time:1499302ms step_avg:169.89ms\nstep:8836/10000 train_loss:3.6684 train_time:1499471ms step_avg:169.89ms\nstep:8837/10000 train_loss:3.7963 train_time:1499641ms step_avg:169.89ms\nstep:8838/10000 train_loss:3.6773 train_time:1499811ms step_avg:169.89ms\nstep:8839/10000 train_loss:3.8022 train_time:1499980ms step_avg:169.89ms\nstep:8840/10000 train_loss:3.7945 train_time:1500150ms step_avg:169.89ms\nstep:8841/10000 train_loss:3.8589 train_time:1500322ms step_avg:169.89ms\nstep:8842/10000 train_loss:3.8423 train_time:1500494ms step_avg:169.89ms\nstep:8843/10000 train_loss:3.7917 train_time:1500661ms step_avg:169.89ms\nstep:8844/10000 train_loss:3.7817 train_time:1500829ms step_avg:169.89ms\nstep:8845/10000 train_loss:3.9039 train_time:1500998ms step_avg:169.89ms\nstep:8846/10000 train_loss:3.7709 train_time:1501166ms step_avg:169.89ms\nstep:8847/10000 train_loss:3.8067 train_time:1501337ms step_avg:169.89ms\nstep:8848/10000 train_loss:3.8434 train_time:1501507ms step_avg:169.89ms\nstep:8849/10000 train_loss:3.8383 train_time:1501676ms step_avg:169.89ms\nstep:8850/10000 train_loss:3.7610 train_time:1501845ms step_avg:169.89ms\nstep:8851/10000 train_loss:3.8687 train_time:1502017ms step_avg:169.89ms\nstep:8852/10000 train_loss:3.6986 train_time:1502182ms step_avg:169.89ms\nstep:8853/10000 train_loss:3.9332 train_time:1502354ms step_avg:169.89ms\nstep:8854/10000 train_loss:3.7361 train_time:1502525ms step_avg:169.89ms\nstep:8855/10000 train_loss:3.8878 train_time:1502692ms step_avg:169.89ms\nstep:8856/10000 train_loss:3.7875 train_time:1502863ms step_avg:169.89ms\nstep:8857/10000 train_loss:3.8296 train_time:1503032ms step_avg:169.89ms\nstep:8858/10000 train_loss:3.8102 train_time:1503202ms step_avg:169.89ms\nstep:8859/10000 train_loss:3.7427 train_time:1503373ms step_avg:169.89ms\nstep:8860/10000 train_loss:3.7978 train_time:1503544ms step_avg:169.89ms\nstep:8861/10000 train_loss:3.7454 train_time:1503715ms step_avg:169.89ms\nstep:8862/10000 train_loss:3.6128 train_time:1503882ms step_avg:169.89ms\nstep:8863/10000 train_loss:3.8314 train_time:1504052ms step_avg:169.89ms\nstep:8864/10000 train_loss:3.7919 train_time:1504220ms step_avg:169.89ms\nstep:8865/10000 train_loss:3.7423 train_time:1504391ms step_avg:169.89ms\nstep:8866/10000 train_loss:3.6160 train_time:1504561ms step_avg:169.89ms\nstep:8867/10000 train_loss:3.8276 train_time:1504731ms step_avg:169.89ms\nstep:8868/10000 train_loss:3.8375 train_time:1504901ms step_avg:169.89ms\nstep:8869/10000 train_loss:3.7880 train_time:1505070ms step_avg:169.89ms\nstep:8870/10000 train_loss:3.9406 train_time:1505241ms step_avg:169.89ms\nstep:8871/10000 train_loss:3.9325 train_time:1505413ms step_avg:169.89ms\nstep:8872/10000 train_loss:3.6858 train_time:1505581ms step_avg:169.89ms\nstep:8873/10000 train_loss:3.8851 train_time:1505751ms step_avg:169.89ms\nstep:8874/10000 train_loss:3.8086 train_time:1505921ms step_avg:169.89ms\nstep:8875/10000 train_loss:3.6669 train_time:1506092ms step_avg:169.89ms\nstep:8876/10000 train_loss:3.7385 train_time:1506261ms step_avg:169.89ms\nstep:8877/10000 train_loss:3.8074 train_time:1506432ms step_avg:169.89ms\nstep:8878/10000 train_loss:3.9004 train_time:1506601ms step_avg:169.89ms\nstep:8879/10000 train_loss:3.7535 train_time:1506779ms step_avg:169.89ms\nstep:8880/10000 train_loss:3.8860 train_time:1506949ms step_avg:169.89ms\nstep:8881/10000 train_loss:3.6680 train_time:1507118ms step_avg:169.89ms\nstep:8882/10000 train_loss:3.9145 train_time:1507288ms step_avg:169.89ms\nstep:8883/10000 train_loss:3.8289 train_time:1507460ms step_avg:169.89ms\nstep:8884/10000 train_loss:3.8495 train_time:1507630ms step_avg:169.89ms\nstep:8885/10000 train_loss:3.4859 train_time:1507798ms step_avg:169.89ms\nstep:8886/10000 train_loss:4.1614 train_time:1507969ms step_avg:169.89ms\nstep:8887/10000 train_loss:3.8749 train_time:1508140ms step_avg:169.89ms\nstep:8888/10000 train_loss:3.7892 train_time:1508307ms step_avg:169.89ms\nstep:8889/10000 train_loss:3.7057 train_time:1508473ms step_avg:169.89ms\nstep:8890/10000 train_loss:3.8703 train_time:1508644ms step_avg:169.89ms\nstep:8891/10000 train_loss:3.7986 train_time:1508817ms step_avg:169.89ms\nstep:8892/10000 train_loss:3.7953 train_time:1508984ms step_avg:169.89ms\nstep:8893/10000 train_loss:3.6930 train_time:1509155ms step_avg:169.89ms\nstep:8894/10000 train_loss:3.8219 train_time:1509323ms step_avg:169.89ms\nstep:8895/10000 train_loss:3.5975 train_time:1509494ms step_avg:169.89ms\nstep:8896/10000 train_loss:3.7084 train_time:1509662ms step_avg:169.89ms\nstep:8897/10000 train_loss:3.8096 train_time:1509832ms step_avg:169.89ms\nstep:8898/10000 train_loss:3.8199 train_time:1510001ms step_avg:169.89ms\nstep:8899/10000 train_loss:3.9835 train_time:1510173ms step_avg:169.89ms\nstep:8900/10000 train_loss:3.9778 train_time:1510342ms step_avg:169.89ms\nstep:8901/10000 train_loss:3.7094 train_time:1510514ms step_avg:169.89ms\nstep:8902/10000 train_loss:3.6977 train_time:1510690ms step_avg:169.89ms\nstep:8903/10000 train_loss:3.8218 train_time:1510859ms step_avg:169.89ms\nstep:8904/10000 train_loss:3.8733 train_time:1511026ms step_avg:169.89ms\nstep:8905/10000 train_loss:3.8029 train_time:1511190ms step_avg:169.89ms\nstep:8906/10000 train_loss:3.7602 train_time:1511356ms step_avg:169.89ms\nstep:8907/10000 train_loss:3.7544 train_time:1511525ms step_avg:169.89ms\nstep:8908/10000 train_loss:3.9118 train_time:1511697ms step_avg:169.89ms\nstep:8909/10000 train_loss:3.7138 train_time:1511865ms step_avg:169.89ms\nstep:8910/10000 train_loss:3.7515 train_time:1512038ms step_avg:169.89ms\nstep:8911/10000 train_loss:3.8777 train_time:1512206ms step_avg:169.89ms\nstep:8912/10000 train_loss:3.7773 train_time:1512373ms step_avg:169.89ms\nstep:8913/10000 train_loss:3.7361 train_time:1512541ms step_avg:169.89ms\nstep:8914/10000 train_loss:4.0871 train_time:1512713ms step_avg:169.89ms\nstep:8915/10000 train_loss:3.7177 train_time:1512883ms step_avg:169.89ms\nstep:8916/10000 train_loss:3.8917 train_time:1513053ms step_avg:169.89ms\nstep:8917/10000 train_loss:3.7073 train_time:1513221ms step_avg:169.89ms\nstep:8918/10000 train_loss:3.8464 train_time:1513392ms step_avg:169.89ms\nstep:8919/10000 train_loss:3.7772 train_time:1513561ms step_avg:169.89ms\nstep:8920/10000 train_loss:3.6495 train_time:1513733ms step_avg:169.89ms\nstep:8921/10000 train_loss:3.7587 train_time:1513901ms step_avg:169.89ms\nstep:8922/10000 train_loss:3.7629 train_time:1514072ms step_avg:169.89ms\nstep:8923/10000 train_loss:3.7758 train_time:1514242ms step_avg:169.89ms\nstep:8924/10000 train_loss:3.8900 train_time:1514412ms step_avg:169.89ms\nstep:8925/10000 train_loss:3.6805 train_time:1514578ms step_avg:169.89ms\nstep:8926/10000 train_loss:3.8078 train_time:1514747ms step_avg:169.89ms\nstep:8927/10000 train_loss:3.7654 train_time:1514918ms step_avg:169.89ms\nstep:8928/10000 train_loss:3.8479 train_time:1515088ms step_avg:169.89ms\nstep:8929/10000 train_loss:4.1711 train_time:1515259ms step_avg:169.89ms\nstep:8930/10000 train_loss:4.0213 train_time:1515429ms step_avg:169.89ms\nstep:8931/10000 train_loss:3.6374 train_time:1515597ms step_avg:169.89ms\nstep:8932/10000 train_loss:3.6547 train_time:1515765ms step_avg:169.89ms\nstep:8933/10000 train_loss:3.6895 train_time:1515934ms step_avg:169.89ms\nstep:8934/10000 train_loss:3.7145 train_time:1516102ms step_avg:169.89ms\nstep:8935/10000 train_loss:3.7715 train_time:1516272ms step_avg:169.89ms\nstep:8936/10000 train_loss:3.9975 train_time:1516440ms step_avg:169.89ms\nstep:8937/10000 train_loss:3.8472 train_time:1516611ms step_avg:169.89ms\nstep:8938/10000 train_loss:3.8028 train_time:1516779ms step_avg:169.89ms\nstep:8939/10000 train_loss:3.7485 train_time:1516948ms step_avg:169.89ms\nstep:8940/10000 train_loss:3.7005 train_time:1517117ms step_avg:169.89ms\nstep:8941/10000 train_loss:3.8980 train_time:1517286ms step_avg:169.89ms\nstep:8942/10000 train_loss:3.6750 train_time:1517458ms step_avg:169.89ms\nstep:8943/10000 train_loss:3.8113 train_time:1517626ms step_avg:169.89ms\nstep:8944/10000 train_loss:4.1556 train_time:1517793ms step_avg:169.89ms\nstep:8945/10000 train_loss:3.9496 train_time:1517961ms step_avg:169.89ms\nstep:8946/10000 train_loss:3.7134 train_time:1518132ms step_avg:169.89ms\nstep:8947/10000 train_loss:3.6564 train_time:1518301ms step_avg:169.89ms\nstep:8948/10000 train_loss:3.8524 train_time:1518472ms step_avg:169.89ms\nstep:8949/10000 train_loss:3.6746 train_time:1518644ms step_avg:169.89ms\nstep:8950/10000 train_loss:3.7474 train_time:1518813ms step_avg:169.89ms\nstep:8951/10000 train_loss:3.7734 train_time:1518982ms step_avg:169.89ms\nstep:8952/10000 train_loss:3.7684 train_time:1519153ms step_avg:169.89ms\nstep:8953/10000 train_loss:3.7935 train_time:1519321ms step_avg:169.89ms\nstep:8954/10000 train_loss:3.8408 train_time:1519491ms step_avg:169.89ms\nstep:8955/10000 train_loss:3.8175 train_time:1519661ms step_avg:169.89ms\nstep:8956/10000 train_loss:3.8297 train_time:1519833ms step_avg:169.89ms\nstep:8957/10000 train_loss:3.9372 train_time:1520002ms step_avg:169.89ms\nstep:8958/10000 train_loss:3.7815 train_time:1520171ms step_avg:169.89ms\nstep:8959/10000 train_loss:3.7825 train_time:1520342ms step_avg:169.89ms\nstep:8960/10000 train_loss:3.6389 train_time:1520514ms step_avg:169.89ms\nstep:8961/10000 train_loss:3.5838 train_time:1520682ms step_avg:169.89ms\nstep:8962/10000 train_loss:3.8202 train_time:1520851ms step_avg:169.89ms\nstep:8963/10000 train_loss:3.7834 train_time:1521019ms step_avg:169.89ms\nstep:8964/10000 train_loss:3.5983 train_time:1521187ms step_avg:169.89ms\nstep:8965/10000 train_loss:3.9653 train_time:1521357ms step_avg:169.89ms\nstep:8966/10000 train_loss:3.9505 train_time:1521526ms step_avg:169.89ms\nstep:8967/10000 train_loss:3.7374 train_time:1521698ms step_avg:169.89ms\nstep:8968/10000 train_loss:3.8381 train_time:1521866ms step_avg:169.89ms\nstep:8969/10000 train_loss:3.8362 train_time:1522033ms step_avg:169.89ms\nstep:8970/10000 train_loss:3.7613 train_time:1522201ms step_avg:169.89ms\nstep:8971/10000 train_loss:3.6519 train_time:1522372ms step_avg:169.89ms\nstep:8972/10000 train_loss:3.9200 train_time:1522542ms step_avg:169.89ms\nstep:8973/10000 train_loss:3.6477 train_time:1522711ms step_avg:169.89ms\nstep:8974/10000 train_loss:3.8190 train_time:1522881ms step_avg:169.89ms\nstep:8975/10000 train_loss:3.8037 train_time:1523052ms step_avg:169.89ms\nstep:8976/10000 train_loss:3.6427 train_time:1523220ms step_avg:169.89ms\nstep:8977/10000 train_loss:3.7032 train_time:1523390ms step_avg:169.89ms\nstep:8978/10000 train_loss:3.8142 train_time:1523564ms step_avg:169.89ms\nstep:8979/10000 train_loss:3.6451 train_time:1523733ms step_avg:169.89ms\nstep:8980/10000 train_loss:3.7436 train_time:1523904ms step_avg:169.89ms\nstep:8981/10000 train_loss:3.7751 train_time:1524069ms step_avg:169.89ms\nstep:8982/10000 train_loss:3.6409 train_time:1524234ms step_avg:169.89ms\nstep:8983/10000 train_loss:3.7564 train_time:1524402ms step_avg:169.89ms\nstep:8984/10000 train_loss:3.8804 train_time:1524572ms step_avg:169.89ms\nstep:8985/10000 train_loss:3.6938 train_time:1524739ms step_avg:169.89ms\nstep:8986/10000 train_loss:3.7711 train_time:1524910ms step_avg:169.89ms\nstep:8987/10000 train_loss:3.8138 train_time:1525078ms step_avg:169.89ms\nstep:8988/10000 train_loss:3.8658 train_time:1525249ms step_avg:169.89ms\nstep:8989/10000 train_loss:3.6331 train_time:1525419ms step_avg:169.89ms\nstep:8990/10000 train_loss:3.8019 train_time:1525588ms step_avg:169.89ms\nstep:8991/10000 train_loss:3.8768 train_time:1525756ms step_avg:169.89ms\nstep:8992/10000 train_loss:3.7395 train_time:1525926ms step_avg:169.89ms\nstep:8993/10000 train_loss:3.7793 train_time:1526096ms step_avg:169.89ms\nstep:8994/10000 train_loss:3.7128 train_time:1526266ms step_avg:169.89ms\nstep:8995/10000 train_loss:4.0300 train_time:1526435ms step_avg:169.89ms\nstep:8996/10000 train_loss:3.7757 train_time:1526604ms step_avg:169.89ms\nstep:8997/10000 train_loss:3.7251 train_time:1526776ms step_avg:169.89ms\nstep:8998/10000 train_loss:3.7269 train_time:1526945ms step_avg:169.89ms\nstep:8999/10000 train_loss:3.7400 train_time:1527114ms step_avg:169.89ms\nstep:9000/10000 train_loss:3.8133 train_time:1527283ms step_avg:169.89ms\nstep:9001/10000 train_loss:3.7317 train_time:1527455ms step_avg:169.89ms\nstep:9002/10000 train_loss:3.6676 train_time:1527623ms step_avg:169.89ms\nstep:9003/10000 train_loss:3.8334 train_time:1527794ms step_avg:169.89ms\nstep:9004/10000 train_loss:3.6883 train_time:1527963ms step_avg:169.89ms\nstep:9005/10000 train_loss:3.8554 train_time:1528132ms step_avg:169.89ms\nstep:9006/10000 train_loss:3.7542 train_time:1528302ms step_avg:169.89ms\nstep:9007/10000 train_loss:3.8296 train_time:1528474ms step_avg:169.89ms\nstep:9008/10000 train_loss:3.6619 train_time:1528646ms step_avg:169.89ms\nstep:9009/10000 train_loss:3.7910 train_time:1528811ms step_avg:169.89ms\nstep:9010/10000 train_loss:3.7837 train_time:1528977ms step_avg:169.89ms\nstep:9011/10000 train_loss:3.7383 train_time:1529146ms step_avg:169.89ms\nstep:9012/10000 train_loss:3.8931 train_time:1529317ms step_avg:169.89ms\nstep:9013/10000 train_loss:3.7678 train_time:1529486ms step_avg:169.89ms\nstep:9014/10000 train_loss:3.7452 train_time:1529655ms step_avg:169.89ms\nstep:9015/10000 train_loss:3.8176 train_time:1529823ms step_avg:169.89ms\nstep:9016/10000 train_loss:3.6570 train_time:1529992ms step_avg:169.89ms\nstep:9017/10000 train_loss:3.8043 train_time:1530160ms step_avg:169.89ms\nstep:9018/10000 train_loss:3.7548 train_time:1530330ms step_avg:169.89ms\nstep:9019/10000 train_loss:3.8726 train_time:1530501ms step_avg:169.89ms\nstep:9020/10000 train_loss:3.6427 train_time:1530673ms step_avg:169.89ms\nstep:9021/10000 train_loss:3.6506 train_time:1530841ms step_avg:169.89ms\nstep:9022/10000 train_loss:3.8693 train_time:1531008ms step_avg:169.89ms\nstep:9023/10000 train_loss:3.6525 train_time:1531179ms step_avg:169.89ms\nstep:9024/10000 train_loss:3.7022 train_time:1531350ms step_avg:169.89ms\nstep:9025/10000 train_loss:3.8381 train_time:1531520ms step_avg:169.89ms\nstep:9026/10000 train_loss:3.7786 train_time:1531690ms step_avg:169.89ms\nstep:9027/10000 train_loss:3.6802 train_time:1531859ms step_avg:169.89ms\nstep:9028/10000 train_loss:3.7094 train_time:1532027ms step_avg:169.89ms\nstep:9029/10000 train_loss:3.9680 train_time:1532196ms step_avg:169.89ms\nstep:9030/10000 train_loss:3.8806 train_time:1532364ms step_avg:169.89ms\nstep:9031/10000 train_loss:3.9139 train_time:1532533ms step_avg:169.89ms\nstep:9032/10000 train_loss:3.7265 train_time:1532703ms step_avg:169.89ms\nstep:9033/10000 train_loss:3.7131 train_time:1532872ms step_avg:169.88ms\nstep:9034/10000 train_loss:3.7326 train_time:1533042ms step_avg:169.88ms\nstep:9035/10000 train_loss:3.6574 train_time:1533212ms step_avg:169.88ms\nstep:9036/10000 train_loss:3.7849 train_time:1533382ms step_avg:169.89ms\nstep:9037/10000 train_loss:3.7485 train_time:1533554ms step_avg:169.89ms\nstep:9038/10000 train_loss:3.6324 train_time:1533723ms step_avg:169.89ms\nstep:9039/10000 train_loss:3.8240 train_time:1533893ms step_avg:169.89ms\nstep:9040/10000 train_loss:3.6987 train_time:1534063ms step_avg:169.89ms\nstep:9041/10000 train_loss:3.8215 train_time:1534233ms step_avg:169.89ms\nstep:9042/10000 train_loss:3.9243 train_time:1534401ms step_avg:169.88ms\nstep:9043/10000 train_loss:3.8457 train_time:1534572ms step_avg:169.89ms\nstep:9044/10000 train_loss:3.9461 train_time:1534742ms step_avg:169.89ms\nstep:9045/10000 train_loss:3.6486 train_time:1534914ms step_avg:169.89ms\nstep:9046/10000 train_loss:3.7135 train_time:1535082ms step_avg:169.89ms\nstep:9047/10000 train_loss:3.8229 train_time:1535252ms step_avg:169.89ms\nstep:9048/10000 train_loss:3.8566 train_time:1535421ms step_avg:169.89ms\nstep:9049/10000 train_loss:3.8358 train_time:1535591ms step_avg:169.89ms\nstep:9050/10000 train_loss:4.9390 train_time:1535762ms step_avg:169.89ms\nstep:9051/10000 train_loss:3.6454 train_time:1535933ms step_avg:169.89ms\nstep:9052/10000 train_loss:4.1529 train_time:1536101ms step_avg:169.89ms\nstep:9053/10000 train_loss:3.6999 train_time:1536271ms step_avg:169.89ms\nstep:9054/10000 train_loss:3.9034 train_time:1536442ms step_avg:169.89ms\nstep:9055/10000 train_loss:3.7844 train_time:1536613ms step_avg:169.89ms\nstep:9056/10000 train_loss:3.8303 train_time:1536780ms step_avg:169.89ms\nstep:9057/10000 train_loss:3.7351 train_time:1536951ms step_avg:169.89ms\nstep:9058/10000 train_loss:3.7995 train_time:1537119ms step_avg:169.88ms\nstep:9059/10000 train_loss:3.8074 train_time:1537289ms step_avg:169.88ms\nstep:9060/10000 train_loss:3.8944 train_time:1537458ms step_avg:169.88ms\nstep:9061/10000 train_loss:3.8098 train_time:1537629ms step_avg:169.88ms\nstep:9062/10000 train_loss:3.7434 train_time:1537800ms step_avg:169.89ms\nstep:9063/10000 train_loss:3.6171 train_time:1537970ms step_avg:169.89ms\nstep:9064/10000 train_loss:3.8101 train_time:1538139ms step_avg:169.89ms\nstep:9065/10000 train_loss:3.8666 train_time:1538308ms step_avg:169.88ms\nstep:9066/10000 train_loss:3.7422 train_time:1538480ms step_avg:169.89ms\nstep:9067/10000 train_loss:3.7687 train_time:1538651ms step_avg:169.89ms\nstep:9068/10000 train_loss:3.8078 train_time:1538823ms step_avg:169.89ms\nstep:9069/10000 train_loss:3.7057 train_time:1538993ms step_avg:169.89ms\nstep:9070/10000 train_loss:3.7622 train_time:1539161ms step_avg:169.89ms\nstep:9071/10000 train_loss:3.7578 train_time:1539330ms step_avg:169.89ms\nstep:9072/10000 train_loss:3.5289 train_time:1539499ms step_avg:169.89ms\nstep:9073/10000 train_loss:3.8422 train_time:1539668ms step_avg:169.89ms\nstep:9074/10000 train_loss:3.7822 train_time:1539838ms step_avg:169.89ms\nstep:9075/10000 train_loss:3.8037 train_time:1540010ms step_avg:169.89ms\nstep:9076/10000 train_loss:3.6956 train_time:1540177ms step_avg:169.88ms\nstep:9077/10000 train_loss:3.7292 train_time:1540346ms step_avg:169.88ms\nstep:9078/10000 train_loss:3.8306 train_time:1540515ms step_avg:169.88ms\nstep:9079/10000 train_loss:3.8277 train_time:1540684ms step_avg:169.88ms\nstep:9080/10000 train_loss:3.8929 train_time:1540852ms step_avg:169.88ms\nstep:9081/10000 train_loss:3.7324 train_time:1541020ms step_avg:169.88ms\nstep:9082/10000 train_loss:3.7163 train_time:1541190ms step_avg:169.88ms\nstep:9083/10000 train_loss:3.7909 train_time:1541359ms step_avg:169.88ms\nstep:9084/10000 train_loss:3.6195 train_time:1541530ms step_avg:169.88ms\nstep:9085/10000 train_loss:3.8278 train_time:1541700ms step_avg:169.88ms\nstep:9086/10000 train_loss:3.6907 train_time:1541871ms step_avg:169.88ms\nstep:9087/10000 train_loss:3.8026 train_time:1542040ms step_avg:169.88ms\nstep:9088/10000 train_loss:3.6903 train_time:1542211ms step_avg:169.88ms\nstep:9089/10000 train_loss:3.8336 train_time:1542378ms step_avg:169.88ms\nstep:9090/10000 train_loss:3.9090 train_time:1542546ms step_avg:169.88ms\nstep:9091/10000 train_loss:3.7391 train_time:1542716ms step_avg:169.88ms\nstep:9092/10000 train_loss:3.6615 train_time:1542885ms step_avg:169.88ms\nstep:9093/10000 train_loss:3.6905 train_time:1543054ms step_avg:169.88ms\nstep:9094/10000 train_loss:3.7562 train_time:1543223ms step_avg:169.88ms\nstep:9095/10000 train_loss:3.7384 train_time:1543394ms step_avg:169.88ms\nstep:9096/10000 train_loss:3.6401 train_time:1543562ms step_avg:169.88ms\nstep:9097/10000 train_loss:3.6721 train_time:1543730ms step_avg:169.88ms\nstep:9098/10000 train_loss:3.8255 train_time:1543897ms step_avg:169.88ms\nstep:9099/10000 train_loss:3.7651 train_time:1544066ms step_avg:169.88ms\nstep:9100/10000 train_loss:4.0390 train_time:1544236ms step_avg:169.88ms\nstep:9101/10000 train_loss:3.6439 train_time:1544403ms step_avg:169.88ms\nstep:9102/10000 train_loss:3.7423 train_time:1544573ms step_avg:169.88ms\nstep:9103/10000 train_loss:3.7672 train_time:1544744ms step_avg:169.88ms\nstep:9104/10000 train_loss:3.6871 train_time:1544914ms step_avg:169.88ms\nstep:9105/10000 train_loss:3.9022 train_time:1545083ms step_avg:169.88ms\nstep:9106/10000 train_loss:3.8545 train_time:1545255ms step_avg:169.88ms\nstep:9107/10000 train_loss:3.7918 train_time:1545425ms step_avg:169.88ms\nstep:9108/10000 train_loss:3.7070 train_time:1545593ms step_avg:169.88ms\nstep:9109/10000 train_loss:3.7681 train_time:1545762ms step_avg:169.88ms\nstep:9110/10000 train_loss:3.8359 train_time:1545934ms step_avg:169.88ms\nstep:9111/10000 train_loss:3.6884 train_time:1546101ms step_avg:169.88ms\nstep:9112/10000 train_loss:3.7835 train_time:1546271ms step_avg:169.88ms\nstep:9113/10000 train_loss:3.7614 train_time:1546441ms step_avg:169.88ms\nstep:9114/10000 train_loss:3.7586 train_time:1546611ms step_avg:169.88ms\nstep:9115/10000 train_loss:3.8067 train_time:1546780ms step_avg:169.88ms\nstep:9116/10000 train_loss:3.7363 train_time:1546952ms step_avg:169.88ms\nstep:9117/10000 train_loss:3.7457 train_time:1547121ms step_avg:169.88ms\nstep:9118/10000 train_loss:3.8636 train_time:1547291ms step_avg:169.88ms\nstep:9119/10000 train_loss:3.9640 train_time:1547462ms step_avg:169.88ms\nstep:9120/10000 train_loss:3.7484 train_time:1547631ms step_avg:169.88ms\nstep:9121/10000 train_loss:3.7464 train_time:1547801ms step_avg:169.88ms\nstep:9122/10000 train_loss:3.9239 train_time:1547969ms step_avg:169.88ms\nstep:9123/10000 train_loss:3.7394 train_time:1548139ms step_avg:169.88ms\nstep:9124/10000 train_loss:3.6207 train_time:1548309ms step_avg:169.88ms\nstep:9125/10000 train_loss:3.7063 train_time:1548480ms step_avg:169.88ms\nstep:9126/10000 train_loss:3.7135 train_time:1548648ms step_avg:169.88ms\nstep:9127/10000 train_loss:3.7711 train_time:1548816ms step_avg:169.88ms\nstep:9128/10000 train_loss:3.7324 train_time:1548983ms step_avg:169.88ms\nstep:9129/10000 train_loss:3.6943 train_time:1549153ms step_avg:169.88ms\nstep:9130/10000 train_loss:3.6538 train_time:1549324ms step_avg:169.88ms\nstep:9131/10000 train_loss:3.7867 train_time:1549496ms step_avg:169.88ms\nstep:9132/10000 train_loss:3.8707 train_time:1549666ms step_avg:169.88ms\nstep:9133/10000 train_loss:3.8075 train_time:1549836ms step_avg:169.88ms\nstep:9134/10000 train_loss:3.8562 train_time:1550005ms step_avg:169.88ms\nstep:9135/10000 train_loss:3.7376 train_time:1550175ms step_avg:169.88ms\nstep:9136/10000 train_loss:3.6545 train_time:1550345ms step_avg:169.88ms\nstep:9137/10000 train_loss:3.8784 train_time:1550516ms step_avg:169.88ms\nstep:9138/10000 train_loss:3.6951 train_time:1550684ms step_avg:169.88ms\nstep:9139/10000 train_loss:3.8210 train_time:1550853ms step_avg:169.88ms\nstep:9140/10000 train_loss:3.7092 train_time:1551021ms step_avg:169.88ms\nstep:9141/10000 train_loss:3.6732 train_time:1551191ms step_avg:169.88ms\nstep:9142/10000 train_loss:3.6215 train_time:1551360ms step_avg:169.88ms\nstep:9143/10000 train_loss:3.7586 train_time:1551528ms step_avg:169.88ms\nstep:9144/10000 train_loss:3.8618 train_time:1551932ms step_avg:169.91ms\nstep:9145/10000 train_loss:3.8730 train_time:1552096ms step_avg:169.91ms\nstep:9146/10000 train_loss:3.7315 train_time:1552265ms step_avg:169.91ms\nstep:9147/10000 train_loss:3.7721 train_time:1552431ms step_avg:169.91ms\nstep:9148/10000 train_loss:3.8471 train_time:1552598ms step_avg:169.91ms\nstep:9149/10000 train_loss:3.8236 train_time:1552767ms step_avg:169.91ms\nstep:9150/10000 train_loss:3.8740 train_time:1552946ms step_avg:169.91ms\nstep:9151/10000 train_loss:3.6934 train_time:1553118ms step_avg:169.91ms\nstep:9152/10000 train_loss:3.6709 train_time:1553286ms step_avg:169.91ms\nstep:9153/10000 train_loss:3.7674 train_time:1553456ms step_avg:169.91ms\nstep:9154/10000 train_loss:3.7478 train_time:1553624ms step_avg:169.91ms\nstep:9155/10000 train_loss:3.7776 train_time:1553792ms step_avg:169.91ms\nstep:9156/10000 train_loss:3.8050 train_time:1553961ms step_avg:169.91ms\nstep:9157/10000 train_loss:3.7753 train_time:1554133ms step_avg:169.91ms\nstep:9158/10000 train_loss:3.7554 train_time:1554301ms step_avg:169.91ms\nstep:9159/10000 train_loss:3.7759 train_time:1554471ms step_avg:169.91ms\nstep:9160/10000 train_loss:3.7382 train_time:1554641ms step_avg:169.91ms\nstep:9161/10000 train_loss:3.8727 train_time:1554809ms step_avg:169.91ms\nstep:9162/10000 train_loss:3.7716 train_time:1554977ms step_avg:169.91ms\nstep:9163/10000 train_loss:3.7507 train_time:1555145ms step_avg:169.91ms\nstep:9164/10000 train_loss:3.7429 train_time:1555315ms step_avg:169.91ms\nstep:9165/10000 train_loss:3.7897 train_time:1555484ms step_avg:169.91ms\nstep:9166/10000 train_loss:3.7581 train_time:1555654ms step_avg:169.91ms\nstep:9167/10000 train_loss:3.8363 train_time:1555824ms step_avg:169.91ms\nstep:9168/10000 train_loss:3.9362 train_time:1555995ms step_avg:169.91ms\nstep:9169/10000 train_loss:3.7689 train_time:1556162ms step_avg:169.91ms\nstep:9170/10000 train_loss:3.7084 train_time:1556332ms step_avg:169.91ms\nstep:9171/10000 train_loss:3.8159 train_time:1556502ms step_avg:169.91ms\nstep:9172/10000 train_loss:3.7090 train_time:1556671ms step_avg:169.91ms\nstep:9173/10000 train_loss:3.7958 train_time:1556843ms step_avg:169.91ms\nstep:9174/10000 train_loss:3.8536 train_time:1557013ms step_avg:169.91ms\nstep:9175/10000 train_loss:3.8187 train_time:1557182ms step_avg:169.91ms\nstep:9176/10000 train_loss:3.7597 train_time:1557354ms step_avg:169.91ms\nstep:9177/10000 train_loss:3.8004 train_time:1557523ms step_avg:169.91ms\nstep:9178/10000 train_loss:3.8376 train_time:1557693ms step_avg:169.91ms\nstep:9179/10000 train_loss:3.7848 train_time:1557859ms step_avg:169.91ms\nstep:9180/10000 train_loss:3.7057 train_time:1558031ms step_avg:169.91ms\nstep:9181/10000 train_loss:3.7267 train_time:1558199ms step_avg:169.91ms\nstep:9182/10000 train_loss:3.7075 train_time:1558368ms step_avg:169.90ms\nstep:9183/10000 train_loss:3.7047 train_time:1558535ms step_avg:169.90ms\nstep:9184/10000 train_loss:3.7435 train_time:1558703ms step_avg:169.90ms\nstep:9185/10000 train_loss:3.7346 train_time:1558873ms step_avg:169.90ms\nstep:9186/10000 train_loss:4.0323 train_time:1559041ms step_avg:169.90ms\nstep:9187/10000 train_loss:3.8108 train_time:1559211ms step_avg:169.90ms\nstep:9188/10000 train_loss:3.8014 train_time:1559378ms step_avg:169.90ms\nstep:9189/10000 train_loss:3.7347 train_time:1559546ms step_avg:169.90ms\nstep:9190/10000 train_loss:3.7080 train_time:1559715ms step_avg:169.90ms\nstep:9191/10000 train_loss:3.7321 train_time:1559883ms step_avg:169.90ms\nstep:9192/10000 train_loss:3.8650 train_time:1560055ms step_avg:169.90ms\nstep:9193/10000 train_loss:3.6797 train_time:1560224ms step_avg:169.90ms\nstep:9194/10000 train_loss:3.6762 train_time:1560392ms step_avg:169.90ms\nstep:9195/10000 train_loss:3.8327 train_time:1560563ms step_avg:169.90ms\nstep:9196/10000 train_loss:3.7858 train_time:1560733ms step_avg:169.90ms\nstep:9197/10000 train_loss:3.8709 train_time:1560901ms step_avg:169.90ms\nstep:9198/10000 train_loss:3.8375 train_time:1561072ms step_avg:169.90ms\nstep:9199/10000 train_loss:3.8163 train_time:1561240ms step_avg:169.90ms\nstep:9200/10000 train_loss:3.7381 train_time:1561410ms step_avg:169.90ms\nstep:9201/10000 train_loss:3.8246 train_time:1561581ms step_avg:169.90ms\nstep:9202/10000 train_loss:3.7992 train_time:1561754ms step_avg:169.90ms\nstep:9203/10000 train_loss:3.7255 train_time:1561923ms step_avg:169.90ms\nstep:9204/10000 train_loss:3.8755 train_time:1562096ms step_avg:169.90ms\nstep:9205/10000 train_loss:3.7821 train_time:1562263ms step_avg:169.90ms\nstep:9206/10000 train_loss:3.7278 train_time:1562433ms step_avg:169.90ms\nstep:9207/10000 train_loss:3.7625 train_time:1562602ms step_avg:169.90ms\nstep:9208/10000 train_loss:3.8566 train_time:1562773ms step_avg:169.90ms\nstep:9209/10000 train_loss:3.7822 train_time:1562942ms step_avg:169.90ms\nstep:9210/10000 train_loss:4.1327 train_time:1563114ms step_avg:169.90ms\nstep:9211/10000 train_loss:3.6080 train_time:1563281ms step_avg:169.90ms\nstep:9212/10000 train_loss:3.6210 train_time:1563452ms step_avg:169.90ms\nstep:9213/10000 train_loss:3.7352 train_time:1563624ms step_avg:169.90ms\nstep:9214/10000 train_loss:3.6732 train_time:1563794ms step_avg:169.90ms\nstep:9215/10000 train_loss:3.9143 train_time:1563964ms step_avg:169.90ms\nstep:9216/10000 train_loss:3.7181 train_time:1564133ms step_avg:169.90ms\nstep:9217/10000 train_loss:3.6671 train_time:1564301ms step_avg:169.90ms\nstep:9218/10000 train_loss:3.6967 train_time:1564471ms step_avg:169.90ms\nstep:9219/10000 train_loss:3.9080 train_time:1564641ms step_avg:169.90ms\nstep:9220/10000 train_loss:3.7848 train_time:1564813ms step_avg:169.90ms\nstep:9221/10000 train_loss:3.8051 train_time:1564984ms step_avg:169.90ms\nstep:9222/10000 train_loss:4.0173 train_time:1565155ms step_avg:169.90ms\nstep:9223/10000 train_loss:3.6786 train_time:1565323ms step_avg:169.90ms\nstep:9224/10000 train_loss:3.5895 train_time:1565492ms step_avg:169.90ms\nstep:9225/10000 train_loss:3.7792 train_time:1565660ms step_avg:169.90ms\nstep:9226/10000 train_loss:3.7351 train_time:1565831ms step_avg:169.90ms\nstep:9227/10000 train_loss:3.7674 train_time:1566000ms step_avg:169.90ms\nstep:9228/10000 train_loss:3.7279 train_time:1566172ms step_avg:169.90ms\nstep:9229/10000 train_loss:3.7574 train_time:1566341ms step_avg:169.90ms\nstep:9230/10000 train_loss:3.7311 train_time:1566511ms step_avg:169.90ms\nstep:9231/10000 train_loss:3.5506 train_time:1566680ms step_avg:169.90ms\nstep:9232/10000 train_loss:3.8898 train_time:1566849ms step_avg:169.90ms\nstep:9233/10000 train_loss:3.6733 train_time:1567019ms step_avg:169.90ms\nstep:9234/10000 train_loss:3.7406 train_time:1567188ms step_avg:169.90ms\nstep:9235/10000 train_loss:3.6644 train_time:1567355ms step_avg:169.90ms\nstep:9236/10000 train_loss:3.7339 train_time:1567524ms step_avg:169.90ms\nstep:9237/10000 train_loss:3.7609 train_time:1567693ms step_avg:169.90ms\nstep:9238/10000 train_loss:3.6737 train_time:1567862ms step_avg:169.90ms\nstep:9239/10000 train_loss:3.6852 train_time:1568033ms step_avg:169.90ms\nstep:9240/10000 train_loss:3.6809 train_time:1568201ms step_avg:169.90ms\nstep:9241/10000 train_loss:3.8563 train_time:1568370ms step_avg:169.90ms\nstep:9242/10000 train_loss:3.8216 train_time:1568540ms step_avg:169.90ms\nstep:9243/10000 train_loss:3.6835 train_time:1568711ms step_avg:169.90ms\nstep:9244/10000 train_loss:3.7420 train_time:1568878ms step_avg:169.90ms\nstep:9245/10000 train_loss:3.7378 train_time:1569047ms step_avg:169.90ms\nstep:9246/10000 train_loss:3.6811 train_time:1569218ms step_avg:169.90ms\nstep:9247/10000 train_loss:3.7265 train_time:1569387ms step_avg:169.90ms\nstep:9248/10000 train_loss:3.5855 train_time:1569556ms step_avg:169.90ms\nstep:9249/10000 train_loss:3.7412 train_time:1569724ms step_avg:169.90ms\nstep:9250/10000 train_loss:3.7073 train_time:1569892ms step_avg:169.90ms\nstep:9251/10000 train_loss:3.7332 train_time:1570061ms step_avg:169.90ms\nstep:9252/10000 train_loss:3.7890 train_time:1570232ms step_avg:169.90ms\nstep:9253/10000 train_loss:3.7405 train_time:1570400ms step_avg:169.90ms\nstep:9254/10000 train_loss:3.8576 train_time:1570570ms step_avg:169.90ms\nstep:9255/10000 train_loss:3.7685 train_time:1570739ms step_avg:169.90ms\nstep:9256/10000 train_loss:3.6403 train_time:1570908ms step_avg:169.90ms\nstep:9257/10000 train_loss:3.8864 train_time:1571076ms step_avg:169.90ms\nstep:9258/10000 train_loss:3.7768 train_time:1571245ms step_avg:169.90ms\nstep:9259/10000 train_loss:3.9276 train_time:1571423ms step_avg:169.90ms\nstep:9260/10000 train_loss:3.8245 train_time:1571588ms step_avg:169.90ms\nstep:9261/10000 train_loss:3.6795 train_time:1571760ms step_avg:169.90ms\nstep:9262/10000 train_loss:3.6583 train_time:1571931ms step_avg:169.90ms\nstep:9263/10000 train_loss:3.6418 train_time:1572098ms step_avg:169.90ms\nstep:9264/10000 train_loss:3.7937 train_time:1572268ms step_avg:169.90ms\nstep:9265/10000 train_loss:3.7203 train_time:1572440ms step_avg:169.90ms\nstep:9266/10000 train_loss:3.7691 train_time:1572610ms step_avg:169.90ms\nstep:9267/10000 train_loss:3.8322 train_time:1572781ms step_avg:169.90ms\nstep:9268/10000 train_loss:3.7153 train_time:1572953ms step_avg:169.90ms\nstep:9269/10000 train_loss:3.6655 train_time:1573120ms step_avg:169.90ms\nstep:9270/10000 train_loss:3.8840 train_time:1573288ms step_avg:169.90ms\nstep:9271/10000 train_loss:3.8377 train_time:1573458ms step_avg:169.90ms\nstep:9272/10000 train_loss:3.7384 train_time:1573629ms step_avg:169.90ms\nstep:9273/10000 train_loss:3.7403 train_time:1573798ms step_avg:169.90ms\nstep:9274/10000 train_loss:3.7272 train_time:1573971ms step_avg:169.90ms\nstep:9275/10000 train_loss:3.8237 train_time:1574138ms step_avg:169.90ms\nstep:9276/10000 train_loss:3.6340 train_time:1574307ms step_avg:169.90ms\nstep:9277/10000 train_loss:3.7407 train_time:1574475ms step_avg:169.90ms\nstep:9278/10000 train_loss:3.6264 train_time:1574644ms step_avg:169.90ms\nstep:9279/10000 train_loss:3.6579 train_time:1574815ms step_avg:169.90ms\nstep:9280/10000 train_loss:3.9542 train_time:1574984ms step_avg:169.90ms\nstep:9281/10000 train_loss:3.7930 train_time:1575154ms step_avg:169.90ms\nstep:9282/10000 train_loss:3.7310 train_time:1575321ms step_avg:169.90ms\nstep:9283/10000 train_loss:3.7838 train_time:1575489ms step_avg:169.90ms\nstep:9284/10000 train_loss:3.8195 train_time:1575658ms step_avg:169.90ms\nstep:9285/10000 train_loss:3.8661 train_time:1575828ms step_avg:169.90ms\nstep:9286/10000 train_loss:3.6120 train_time:1575996ms step_avg:169.90ms\nstep:9287/10000 train_loss:3.7715 train_time:1576168ms step_avg:169.90ms\nstep:9288/10000 train_loss:3.6696 train_time:1576338ms step_avg:169.90ms\nstep:9289/10000 train_loss:3.8238 train_time:1576507ms step_avg:169.90ms\nstep:9290/10000 train_loss:3.7091 train_time:1576676ms step_avg:169.90ms\nstep:9291/10000 train_loss:3.8192 train_time:1576846ms step_avg:169.90ms\nstep:9292/10000 train_loss:3.6328 train_time:1577017ms step_avg:169.90ms\nstep:9293/10000 train_loss:3.7145 train_time:1577185ms step_avg:169.90ms\nstep:9294/10000 train_loss:3.7554 train_time:1577353ms step_avg:169.90ms\nstep:9295/10000 train_loss:3.7174 train_time:1577521ms step_avg:169.90ms\nstep:9296/10000 train_loss:3.6875 train_time:1577688ms step_avg:169.90ms\nstep:9297/10000 train_loss:3.7617 train_time:1577855ms step_avg:169.90ms\nstep:9298/10000 train_loss:3.5480 train_time:1578025ms step_avg:169.90ms\nstep:9299/10000 train_loss:3.7413 train_time:1578195ms step_avg:169.90ms\nstep:9300/10000 train_loss:3.7344 train_time:1578363ms step_avg:169.90ms\nstep:9301/10000 train_loss:4.1584 train_time:1578532ms step_avg:169.90ms\nstep:9302/10000 train_loss:3.7206 train_time:1578702ms step_avg:169.90ms\nstep:9303/10000 train_loss:3.8085 train_time:1578871ms step_avg:169.90ms\nstep:9304/10000 train_loss:3.6934 train_time:1579039ms step_avg:169.90ms\nstep:9305/10000 train_loss:3.7854 train_time:1579210ms step_avg:169.90ms\nstep:9306/10000 train_loss:3.8002 train_time:1579381ms step_avg:169.90ms\nstep:9307/10000 train_loss:3.7381 train_time:1579551ms step_avg:169.90ms\nstep:9308/10000 train_loss:3.9605 train_time:1579719ms step_avg:169.90ms\nstep:9309/10000 train_loss:3.6738 train_time:1579887ms step_avg:169.90ms\nstep:9310/10000 train_loss:3.6775 train_time:1580055ms step_avg:169.90ms\nstep:9311/10000 train_loss:3.6694 train_time:1580226ms step_avg:169.90ms\nstep:9312/10000 train_loss:3.9227 train_time:1580395ms step_avg:169.90ms\nstep:9313/10000 train_loss:3.7865 train_time:1580564ms step_avg:169.90ms\nstep:9314/10000 train_loss:3.7174 train_time:1580734ms step_avg:169.90ms\nstep:9315/10000 train_loss:3.6905 train_time:1580902ms step_avg:169.90ms\nstep:9316/10000 train_loss:3.5494 train_time:1581073ms step_avg:169.90ms\nstep:9317/10000 train_loss:3.6287 train_time:1581242ms step_avg:169.90ms\nstep:9318/10000 train_loss:3.6513 train_time:1581411ms step_avg:169.90ms\nstep:9319/10000 train_loss:3.8950 train_time:1581583ms step_avg:169.90ms\nstep:9320/10000 train_loss:3.7599 train_time:1581753ms step_avg:169.90ms\nstep:9321/10000 train_loss:3.6517 train_time:1581921ms step_avg:169.90ms\nstep:9322/10000 train_loss:3.7711 train_time:1582091ms step_avg:169.90ms\nstep:9323/10000 train_loss:3.7231 train_time:1582261ms step_avg:169.90ms\nstep:9324/10000 train_loss:3.6241 train_time:1582433ms step_avg:169.90ms\nstep:9325/10000 train_loss:3.7679 train_time:1582601ms step_avg:169.90ms\nstep:9326/10000 train_loss:3.7907 train_time:1582770ms step_avg:169.90ms\nstep:9327/10000 train_loss:3.9233 train_time:1582940ms step_avg:169.90ms\nstep:9328/10000 train_loss:3.7716 train_time:1583109ms step_avg:169.90ms\nstep:9329/10000 train_loss:3.7037 train_time:1583278ms step_avg:169.90ms\nstep:9330/10000 train_loss:3.5709 train_time:1583447ms step_avg:169.90ms\nstep:9331/10000 train_loss:3.7113 train_time:1583619ms step_avg:169.90ms\nstep:9332/10000 train_loss:3.7535 train_time:1583787ms step_avg:169.90ms\nstep:9333/10000 train_loss:3.8025 train_time:1583955ms step_avg:169.90ms\nstep:9334/10000 train_loss:3.6930 train_time:1584123ms step_avg:169.90ms\nstep:9335/10000 train_loss:3.6703 train_time:1584295ms step_avg:169.90ms\nstep:9336/10000 train_loss:3.6732 train_time:1584464ms step_avg:169.90ms\nstep:9337/10000 train_loss:3.6918 train_time:1584635ms step_avg:169.90ms\nstep:9338/10000 train_loss:3.7260 train_time:1584802ms step_avg:169.90ms\nstep:9339/10000 train_loss:4.3284 train_time:1584972ms step_avg:169.90ms\nstep:9340/10000 train_loss:3.8766 train_time:1585140ms step_avg:169.90ms\nstep:9341/10000 train_loss:3.7821 train_time:1585312ms step_avg:169.90ms\nstep:9342/10000 train_loss:3.6622 train_time:1585482ms step_avg:169.90ms\nstep:9343/10000 train_loss:3.7754 train_time:1585651ms step_avg:169.90ms\nstep:9344/10000 train_loss:3.7640 train_time:1585822ms step_avg:169.90ms\nstep:9345/10000 train_loss:3.7717 train_time:1585990ms step_avg:169.90ms\nstep:9346/10000 train_loss:3.8276 train_time:1586160ms step_avg:169.90ms\nstep:9347/10000 train_loss:3.7971 train_time:1586328ms step_avg:169.90ms\nstep:9348/10000 train_loss:3.6496 train_time:1586498ms step_avg:169.90ms\nstep:9349/10000 train_loss:3.7368 train_time:1586666ms step_avg:169.90ms\nstep:9350/10000 train_loss:3.7653 train_time:1586835ms step_avg:169.90ms\nstep:9351/10000 train_loss:3.7342 train_time:1587002ms step_avg:169.90ms\nstep:9352/10000 train_loss:3.8332 train_time:1587172ms step_avg:169.90ms\nstep:9353/10000 train_loss:3.8230 train_time:1587342ms step_avg:169.90ms\nstep:9354/10000 train_loss:3.7144 train_time:1587512ms step_avg:169.90ms\nstep:9355/10000 train_loss:3.8287 train_time:1587680ms step_avg:169.90ms\nstep:9356/10000 train_loss:4.0774 train_time:1587849ms step_avg:169.90ms\nstep:9357/10000 train_loss:3.7728 train_time:1588018ms step_avg:169.90ms\nstep:9358/10000 train_loss:3.8002 train_time:1588187ms step_avg:169.90ms\nstep:9359/10000 train_loss:3.7003 train_time:1588361ms step_avg:169.90ms\nstep:9360/10000 train_loss:3.7026 train_time:1588529ms step_avg:169.90ms\nstep:9361/10000 train_loss:3.7240 train_time:1588704ms step_avg:169.90ms\nstep:9362/10000 train_loss:3.6422 train_time:1588870ms step_avg:169.90ms\nstep:9363/10000 train_loss:3.6602 train_time:1589038ms step_avg:169.90ms\nstep:9364/10000 train_loss:3.7332 train_time:1589208ms step_avg:169.90ms\nstep:9365/10000 train_loss:3.5841 train_time:1589380ms step_avg:169.90ms\nstep:9366/10000 train_loss:3.7703 train_time:1589548ms step_avg:169.90ms\nstep:9367/10000 train_loss:3.7122 train_time:1589719ms step_avg:169.90ms\nstep:9368/10000 train_loss:3.7256 train_time:1589887ms step_avg:169.90ms\nstep:9369/10000 train_loss:3.9922 train_time:1590059ms step_avg:169.90ms\nstep:9370/10000 train_loss:3.7311 train_time:1590228ms step_avg:169.90ms\nstep:9371/10000 train_loss:3.6847 train_time:1590399ms step_avg:169.90ms\nstep:9372/10000 train_loss:3.8203 train_time:1590568ms step_avg:169.90ms\nstep:9373/10000 train_loss:3.7354 train_time:1590738ms step_avg:169.90ms\nstep:9374/10000 train_loss:3.7069 train_time:1590907ms step_avg:169.90ms\nstep:9375/10000 train_loss:3.7471 train_time:1591074ms step_avg:169.90ms\nstep:9376/10000 train_loss:3.8475 train_time:1591244ms step_avg:169.90ms\nstep:9377/10000 train_loss:3.6355 train_time:1591415ms step_avg:169.90ms\nstep:9378/10000 train_loss:3.7767 train_time:1591583ms step_avg:169.90ms\nstep:9379/10000 train_loss:3.6260 train_time:1591751ms step_avg:169.90ms\nstep:9380/10000 train_loss:3.9265 train_time:1591919ms step_avg:169.90ms\nstep:9381/10000 train_loss:3.5771 train_time:1592087ms step_avg:169.90ms\nstep:9382/10000 train_loss:3.8108 train_time:1592257ms step_avg:169.90ms\nstep:9383/10000 train_loss:3.6279 train_time:1592428ms step_avg:169.90ms\nstep:9384/10000 train_loss:3.6082 train_time:1592597ms step_avg:169.90ms\nstep:9385/10000 train_loss:3.6985 train_time:1592764ms step_avg:169.89ms\nstep:9386/10000 train_loss:3.7069 train_time:1592933ms step_avg:169.89ms\nstep:9387/10000 train_loss:3.7436 train_time:1593101ms step_avg:169.89ms\nstep:9388/10000 train_loss:3.6595 train_time:1593271ms step_avg:169.89ms\nstep:9389/10000 train_loss:3.6685 train_time:1593441ms step_avg:169.89ms\nstep:9390/10000 train_loss:3.6875 train_time:1593611ms step_avg:169.89ms\nstep:9391/10000 train_loss:3.7301 train_time:1593781ms step_avg:169.89ms\nstep:9392/10000 train_loss:3.5577 train_time:1593952ms step_avg:169.89ms\nstep:9393/10000 train_loss:4.2319 train_time:1594121ms step_avg:169.89ms\nstep:9394/10000 train_loss:3.7491 train_time:1594291ms step_avg:169.89ms\nstep:9395/10000 train_loss:3.6783 train_time:1594463ms step_avg:169.89ms\nstep:9396/10000 train_loss:3.8394 train_time:1594631ms step_avg:169.89ms\nstep:9397/10000 train_loss:3.7505 train_time:1594799ms step_avg:169.89ms\nstep:9398/10000 train_loss:3.7087 train_time:1594971ms step_avg:169.89ms\nstep:9399/10000 train_loss:3.7508 train_time:1595140ms step_avg:169.89ms\nstep:9400/10000 train_loss:3.6841 train_time:1595309ms step_avg:169.89ms\nstep:9401/10000 train_loss:3.8642 train_time:1595477ms step_avg:169.89ms\nstep:9402/10000 train_loss:3.8374 train_time:1595644ms step_avg:169.89ms\nstep:9403/10000 train_loss:3.8012 train_time:1595814ms step_avg:169.89ms\nstep:9404/10000 train_loss:3.7775 train_time:1595982ms step_avg:169.89ms\nstep:9405/10000 train_loss:3.7597 train_time:1596153ms step_avg:169.89ms\nstep:9406/10000 train_loss:3.7510 train_time:1596323ms step_avg:169.89ms\nstep:9407/10000 train_loss:3.4638 train_time:1596494ms step_avg:169.89ms\nstep:9408/10000 train_loss:3.9334 train_time:1596662ms step_avg:169.89ms\nstep:9409/10000 train_loss:3.6495 train_time:1596832ms step_avg:169.89ms\nstep:9410/10000 train_loss:3.8307 train_time:1597000ms step_avg:169.89ms\nstep:9411/10000 train_loss:3.8354 train_time:1597169ms step_avg:169.89ms\nstep:9412/10000 train_loss:3.9171 train_time:1597337ms step_avg:169.89ms\nstep:9413/10000 train_loss:3.7832 train_time:1597507ms step_avg:169.89ms\nstep:9414/10000 train_loss:3.7885 train_time:1597674ms step_avg:169.89ms\nstep:9415/10000 train_loss:3.9235 train_time:1597844ms step_avg:169.89ms\nstep:9416/10000 train_loss:3.6058 train_time:1598016ms step_avg:169.89ms\nstep:9417/10000 train_loss:3.9934 train_time:1598183ms step_avg:169.89ms\nstep:9418/10000 train_loss:3.8238 train_time:1598356ms step_avg:169.89ms\nstep:9419/10000 train_loss:3.7337 train_time:1598524ms step_avg:169.89ms\nstep:9420/10000 train_loss:3.6798 train_time:1598695ms step_avg:169.89ms\nstep:9421/10000 train_loss:3.7256 train_time:1598864ms step_avg:169.89ms\nstep:9422/10000 train_loss:3.5232 train_time:1599032ms step_avg:169.89ms\nstep:9423/10000 train_loss:3.7795 train_time:1599200ms step_avg:169.89ms\nstep:9424/10000 train_loss:3.7187 train_time:1599369ms step_avg:169.89ms\nstep:9425/10000 train_loss:3.7900 train_time:1599539ms step_avg:169.89ms\nstep:9426/10000 train_loss:3.6608 train_time:1599708ms step_avg:169.89ms\nstep:9427/10000 train_loss:3.7884 train_time:1599876ms step_avg:169.89ms\nstep:9428/10000 train_loss:3.7023 train_time:1600045ms step_avg:169.89ms\nstep:9429/10000 train_loss:3.7080 train_time:1600213ms step_avg:169.89ms\nstep:9430/10000 train_loss:3.6402 train_time:1600382ms step_avg:169.89ms\nstep:9431/10000 train_loss:3.8213 train_time:1600552ms step_avg:169.89ms\nstep:9432/10000 train_loss:3.6693 train_time:1600722ms step_avg:169.89ms\nstep:9433/10000 train_loss:3.6017 train_time:1600893ms step_avg:169.89ms\nstep:9434/10000 train_loss:3.8486 train_time:1601063ms step_avg:169.89ms\nstep:9435/10000 train_loss:3.8454 train_time:1601232ms step_avg:169.89ms\nstep:9436/10000 train_loss:3.8831 train_time:1601402ms step_avg:169.89ms\nstep:9437/10000 train_loss:3.8826 train_time:1601573ms step_avg:169.89ms\nstep:9438/10000 train_loss:3.6604 train_time:1601741ms step_avg:169.89ms\nstep:9439/10000 train_loss:3.5945 train_time:1601911ms step_avg:169.89ms\nstep:9440/10000 train_loss:3.9791 train_time:1602080ms step_avg:169.89ms\nstep:9441/10000 train_loss:3.6761 train_time:1602249ms step_avg:169.89ms\nstep:9442/10000 train_loss:3.6505 train_time:1602415ms step_avg:169.89ms\nstep:9443/10000 train_loss:3.6833 train_time:1602584ms step_avg:169.89ms\nstep:9444/10000 train_loss:3.2232 train_time:1602755ms step_avg:169.89ms\nstep:9445/10000 train_loss:3.8236 train_time:1602924ms step_avg:169.89ms\nstep:9446/10000 train_loss:3.7999 train_time:1603093ms step_avg:169.89ms\nstep:9447/10000 train_loss:3.7016 train_time:1603262ms step_avg:169.89ms\nstep:9448/10000 train_loss:4.1264 train_time:1603431ms step_avg:169.89ms\nstep:9449/10000 train_loss:3.8790 train_time:1603601ms step_avg:169.89ms\nstep:9450/10000 train_loss:3.6648 train_time:1603772ms step_avg:169.89ms\nstep:9451/10000 train_loss:3.7606 train_time:1603937ms step_avg:169.89ms\nstep:9452/10000 train_loss:4.0361 train_time:1604108ms step_avg:169.89ms\nstep:9453/10000 train_loss:3.7384 train_time:1604279ms step_avg:169.89ms\nstep:9454/10000 train_loss:3.7838 train_time:1604447ms step_avg:169.89ms\nstep:9455/10000 train_loss:3.6647 train_time:1604618ms step_avg:169.89ms\nstep:9456/10000 train_loss:3.8348 train_time:1604789ms step_avg:169.89ms\nstep:9457/10000 train_loss:3.7063 train_time:1604961ms step_avg:169.89ms\nstep:9458/10000 train_loss:3.6462 train_time:1605132ms step_avg:169.89ms\nstep:9459/10000 train_loss:3.8500 train_time:1605302ms step_avg:169.89ms\nstep:9460/10000 train_loss:3.7525 train_time:1605473ms step_avg:169.89ms\nstep:9461/10000 train_loss:3.7485 train_time:1605640ms step_avg:169.89ms\nstep:9462/10000 train_loss:3.6643 train_time:1605811ms step_avg:169.89ms\nstep:9463/10000 train_loss:3.8110 train_time:1605982ms step_avg:169.89ms\nstep:9464/10000 train_loss:3.7576 train_time:1606151ms step_avg:169.89ms\nstep:9465/10000 train_loss:3.6725 train_time:1606321ms step_avg:169.89ms\nstep:9466/10000 train_loss:3.7288 train_time:1606492ms step_avg:169.89ms\nstep:9467/10000 train_loss:3.7575 train_time:1606660ms step_avg:169.89ms\nstep:9468/10000 train_loss:3.7441 train_time:1606830ms step_avg:169.89ms\nstep:9469/10000 train_loss:3.7884 train_time:1607002ms step_avg:169.89ms\nstep:9470/10000 train_loss:3.7282 train_time:1607172ms step_avg:169.89ms\nstep:9471/10000 train_loss:3.6593 train_time:1607341ms step_avg:169.89ms\nstep:9472/10000 train_loss:3.7265 train_time:1607511ms step_avg:169.89ms\nstep:9473/10000 train_loss:3.7010 train_time:1607680ms step_avg:169.89ms\nstep:9474/10000 train_loss:3.7063 train_time:1607849ms step_avg:169.89ms\nstep:9475/10000 train_loss:3.7638 train_time:1608017ms step_avg:169.89ms\nstep:9476/10000 train_loss:3.6265 train_time:1608185ms step_avg:169.89ms\nstep:9477/10000 train_loss:3.6810 train_time:1608356ms step_avg:169.89ms\nstep:9478/10000 train_loss:4.1392 train_time:1608525ms step_avg:169.89ms\nstep:9479/10000 train_loss:3.4923 train_time:1608694ms step_avg:169.89ms\nstep:9480/10000 train_loss:3.8416 train_time:1608864ms step_avg:169.89ms\nstep:9481/10000 train_loss:3.6002 train_time:1609034ms step_avg:169.89ms\nstep:9482/10000 train_loss:3.7625 train_time:1609202ms step_avg:169.89ms\nstep:9483/10000 train_loss:3.5099 train_time:1609374ms step_avg:169.89ms\nstep:9484/10000 train_loss:3.6929 train_time:1609543ms step_avg:169.89ms\nstep:9485/10000 train_loss:3.7787 train_time:1609713ms step_avg:169.89ms\nstep:9486/10000 train_loss:3.9295 train_time:1609882ms step_avg:169.89ms\nstep:9487/10000 train_loss:3.8171 train_time:1610052ms step_avg:169.89ms\nstep:9488/10000 train_loss:3.7061 train_time:1610221ms step_avg:169.89ms\nstep:9489/10000 train_loss:3.5801 train_time:1610391ms step_avg:169.89ms\nstep:9490/10000 train_loss:3.7612 train_time:1610559ms step_avg:169.89ms\nstep:9491/10000 train_loss:3.9941 train_time:1610730ms step_avg:169.89ms\nstep:9492/10000 train_loss:4.0627 train_time:1610898ms step_avg:169.89ms\nstep:9493/10000 train_loss:3.8598 train_time:1611067ms step_avg:169.89ms\nstep:9494/10000 train_loss:3.7380 train_time:1611236ms step_avg:169.89ms\nstep:9495/10000 train_loss:3.7785 train_time:1611404ms step_avg:169.89ms\nstep:9496/10000 train_loss:3.7067 train_time:1611575ms step_avg:169.89ms\nstep:9497/10000 train_loss:3.8981 train_time:1611743ms step_avg:169.89ms\nstep:9498/10000 train_loss:3.7704 train_time:1611913ms step_avg:169.89ms\nstep:9499/10000 train_loss:3.8414 train_time:1612081ms step_avg:169.89ms\nstep:9500/10000 train_loss:3.6882 train_time:1612251ms step_avg:169.89ms\nstep:9501/10000 train_loss:3.6452 train_time:1612420ms step_avg:169.89ms\nstep:9502/10000 train_loss:3.6582 train_time:1612590ms step_avg:169.89ms\nstep:9503/10000 train_loss:3.6318 train_time:1612761ms step_avg:169.89ms\nstep:9504/10000 train_loss:3.6949 train_time:1612930ms step_avg:169.89ms\nstep:9505/10000 train_loss:3.7303 train_time:1613099ms step_avg:169.89ms\nstep:9506/10000 train_loss:3.8730 train_time:1613268ms step_avg:169.89ms\nstep:9507/10000 train_loss:3.7001 train_time:1613438ms step_avg:169.89ms\nstep:9508/10000 train_loss:3.7480 train_time:1613607ms step_avg:169.89ms\nstep:9509/10000 train_loss:3.7257 train_time:1613778ms step_avg:169.89ms\nstep:9510/10000 train_loss:3.7236 train_time:1613947ms step_avg:169.89ms\nstep:9511/10000 train_loss:3.7251 train_time:1614116ms step_avg:169.89ms\nstep:9512/10000 train_loss:3.6023 train_time:1614286ms step_avg:169.89ms\nstep:9513/10000 train_loss:3.6072 train_time:1614457ms step_avg:169.89ms\nstep:9514/10000 train_loss:4.2715 train_time:1614625ms step_avg:169.89ms\nstep:9515/10000 train_loss:3.6696 train_time:1614793ms step_avg:169.89ms\nstep:9516/10000 train_loss:3.7737 train_time:1614963ms step_avg:169.89ms\nstep:9517/10000 train_loss:3.7044 train_time:1615133ms step_avg:169.89ms\nstep:9518/10000 train_loss:3.6856 train_time:1615304ms step_avg:169.89ms\nstep:9519/10000 train_loss:3.7947 train_time:1615474ms step_avg:169.89ms\nstep:9520/10000 train_loss:3.6460 train_time:1615643ms step_avg:169.89ms\nstep:9521/10000 train_loss:3.6491 train_time:1615812ms step_avg:169.89ms\nstep:9522/10000 train_loss:3.6170 train_time:1615979ms step_avg:169.89ms\nstep:9523/10000 train_loss:3.7376 train_time:1616151ms step_avg:169.89ms\nstep:9524/10000 train_loss:3.7065 train_time:1616322ms step_avg:169.89ms\nstep:9525/10000 train_loss:3.6839 train_time:1616493ms step_avg:169.89ms\nstep:9526/10000 train_loss:3.7887 train_time:1616663ms step_avg:169.89ms\nstep:9527/10000 train_loss:3.8480 train_time:1616834ms step_avg:169.89ms\nstep:9528/10000 train_loss:3.7324 train_time:1617002ms step_avg:169.89ms\nstep:9529/10000 train_loss:3.7531 train_time:1617172ms step_avg:169.89ms\nstep:9530/10000 train_loss:3.7377 train_time:1617341ms step_avg:169.89ms\nstep:9531/10000 train_loss:3.7358 train_time:1617512ms step_avg:169.89ms\nstep:9532/10000 train_loss:4.0149 train_time:1617681ms step_avg:169.89ms\nstep:9533/10000 train_loss:3.6926 train_time:1617849ms step_avg:169.89ms\nstep:9534/10000 train_loss:3.7514 train_time:1618019ms step_avg:169.89ms\nstep:9535/10000 train_loss:3.6526 train_time:1618187ms step_avg:169.89ms\nstep:9536/10000 train_loss:3.7115 train_time:1618358ms step_avg:169.89ms\nstep:9537/10000 train_loss:3.7293 train_time:1618528ms step_avg:169.89ms\nstep:9538/10000 train_loss:3.7549 train_time:1618700ms step_avg:169.89ms\nstep:9539/10000 train_loss:3.6877 train_time:1618868ms step_avg:169.89ms\nstep:9540/10000 train_loss:3.7583 train_time:1619040ms step_avg:169.89ms\nstep:9541/10000 train_loss:3.6805 train_time:1619209ms step_avg:169.89ms\nstep:9542/10000 train_loss:3.6469 train_time:1619379ms step_avg:169.89ms\nstep:9543/10000 train_loss:3.6537 train_time:1619551ms step_avg:169.89ms\nstep:9544/10000 train_loss:3.7647 train_time:1619720ms step_avg:169.89ms\nstep:9545/10000 train_loss:3.6488 train_time:1619888ms step_avg:169.89ms\nstep:9546/10000 train_loss:3.7229 train_time:1620056ms step_avg:169.89ms\nstep:9547/10000 train_loss:3.7462 train_time:1620224ms step_avg:169.89ms\nstep:9548/10000 train_loss:3.6757 train_time:1620392ms step_avg:169.89ms\nstep:9549/10000 train_loss:3.7308 train_time:1620560ms step_avg:169.89ms\nstep:9550/10000 train_loss:3.7460 train_time:1620731ms step_avg:169.89ms\nstep:9551/10000 train_loss:3.7029 train_time:1620900ms step_avg:169.89ms\nstep:9552/10000 train_loss:3.7269 train_time:1621070ms step_avg:169.89ms\nstep:9553/10000 train_loss:3.7191 train_time:1621238ms step_avg:169.89ms\nstep:9554/10000 train_loss:3.7080 train_time:1621408ms step_avg:169.89ms\nstep:9555/10000 train_loss:3.7089 train_time:1621579ms step_avg:169.89ms\nstep:9556/10000 train_loss:3.5627 train_time:1621748ms step_avg:169.89ms\nstep:9557/10000 train_loss:3.8126 train_time:1621919ms step_avg:169.89ms\nstep:9558/10000 train_loss:3.6254 train_time:1622088ms step_avg:169.89ms\nstep:9559/10000 train_loss:3.6442 train_time:1622259ms step_avg:169.89ms\nstep:9560/10000 train_loss:3.7607 train_time:1622427ms step_avg:169.89ms\nstep:9561/10000 train_loss:3.7809 train_time:1622597ms step_avg:169.89ms\nstep:9562/10000 train_loss:3.7378 train_time:1622767ms step_avg:169.89ms\nstep:9563/10000 train_loss:3.6397 train_time:1622938ms step_avg:169.89ms\nstep:9564/10000 train_loss:3.7637 train_time:1623107ms step_avg:169.89ms\nstep:9565/10000 train_loss:3.6070 train_time:1623278ms step_avg:169.89ms\nstep:9566/10000 train_loss:3.7081 train_time:1623447ms step_avg:169.89ms\nstep:9567/10000 train_loss:3.7141 train_time:1623616ms step_avg:169.89ms\nstep:9568/10000 train_loss:3.7215 train_time:1623786ms step_avg:169.89ms\nstep:9569/10000 train_loss:3.6540 train_time:1623958ms step_avg:169.89ms\nstep:9570/10000 train_loss:3.6975 train_time:1624126ms step_avg:169.89ms\nstep:9571/10000 train_loss:3.6875 train_time:1624296ms step_avg:169.89ms\nstep:9572/10000 train_loss:3.7281 train_time:1624464ms step_avg:169.89ms\nstep:9573/10000 train_loss:3.7085 train_time:1624634ms step_avg:169.89ms\nstep:9574/10000 train_loss:3.7170 train_time:1624803ms step_avg:169.89ms\nstep:9575/10000 train_loss:3.6553 train_time:1624973ms step_avg:169.89ms\nstep:9576/10000 train_loss:3.7822 train_time:1625142ms step_avg:169.89ms\nstep:9577/10000 train_loss:3.6918 train_time:1625314ms step_avg:169.89ms\nstep:9578/10000 train_loss:3.7144 train_time:1625483ms step_avg:169.89ms\nstep:9579/10000 train_loss:3.7118 train_time:1625654ms step_avg:169.89ms\nstep:9580/10000 train_loss:3.7174 train_time:1625823ms step_avg:169.89ms\nstep:9581/10000 train_loss:3.7682 train_time:1625992ms step_avg:169.89ms\nstep:9582/10000 train_loss:3.7161 train_time:1626161ms step_avg:169.89ms\nstep:9583/10000 train_loss:3.6712 train_time:1626330ms step_avg:169.89ms\nstep:9584/10000 train_loss:3.6982 train_time:1626501ms step_avg:169.89ms\nstep:9585/10000 train_loss:3.8637 train_time:1626673ms step_avg:169.89ms\nstep:9586/10000 train_loss:3.5828 train_time:1626841ms step_avg:169.89ms\nstep:9587/10000 train_loss:3.7902 train_time:1627012ms step_avg:169.89ms\nstep:9588/10000 train_loss:3.8032 train_time:1627181ms step_avg:169.89ms\nstep:9589/10000 train_loss:3.7432 train_time:1627351ms step_avg:169.89ms\nstep:9590/10000 train_loss:3.7922 train_time:1627522ms step_avg:169.89ms\nstep:9591/10000 train_loss:3.6715 train_time:1627693ms step_avg:169.89ms\nstep:9592/10000 train_loss:3.7513 train_time:1627862ms step_avg:169.89ms\nstep:9593/10000 train_loss:3.5471 train_time:1628032ms step_avg:169.89ms\nstep:9594/10000 train_loss:3.6724 train_time:1628202ms step_avg:169.89ms\nstep:9595/10000 train_loss:3.6844 train_time:1628372ms step_avg:169.89ms\nstep:9596/10000 train_loss:3.7779 train_time:1628542ms step_avg:169.89ms\nstep:9597/10000 train_loss:3.7786 train_time:1628714ms step_avg:169.89ms\nstep:9598/10000 train_loss:3.7349 train_time:1628882ms step_avg:169.89ms\nstep:9599/10000 train_loss:3.7512 train_time:1629050ms step_avg:169.89ms\nstep:9600/10000 train_loss:3.8308 train_time:1629219ms step_avg:169.89ms\nstep:9601/10000 train_loss:3.8340 train_time:1629391ms step_avg:169.89ms\nstep:9602/10000 train_loss:3.8609 train_time:1629561ms step_avg:169.89ms\nstep:9603/10000 train_loss:3.7800 train_time:1629733ms step_avg:169.89ms\nstep:9604/10000 train_loss:3.4096 train_time:1629906ms step_avg:169.89ms\nstep:9605/10000 train_loss:3.6581 train_time:1630072ms step_avg:169.89ms\nstep:9606/10000 train_loss:3.8799 train_time:1630243ms step_avg:169.89ms\nstep:9607/10000 train_loss:3.7651 train_time:1630413ms step_avg:169.89ms\nstep:9608/10000 train_loss:3.7300 train_time:1630583ms step_avg:169.89ms\nstep:9609/10000 train_loss:3.6710 train_time:1630752ms step_avg:169.89ms\nstep:9610/10000 train_loss:3.8084 train_time:1630922ms step_avg:169.89ms\nstep:9611/10000 train_loss:3.7201 train_time:1631092ms step_avg:169.89ms\nstep:9612/10000 train_loss:3.6611 train_time:1631266ms step_avg:169.89ms\nstep:9613/10000 train_loss:3.8925 train_time:1631434ms step_avg:169.89ms\nstep:9614/10000 train_loss:3.7877 train_time:1631603ms step_avg:169.89ms\nstep:9615/10000 train_loss:3.6837 train_time:1631774ms step_avg:169.89ms\nstep:9616/10000 train_loss:3.5326 train_time:1631943ms step_avg:169.89ms\nstep:9617/10000 train_loss:3.7877 train_time:1632113ms step_avg:169.89ms\nstep:9618/10000 train_loss:3.6723 train_time:1632282ms step_avg:169.89ms\nstep:9619/10000 train_loss:3.7048 train_time:1632453ms step_avg:169.89ms\nstep:9620/10000 train_loss:3.5123 train_time:1632621ms step_avg:169.89ms\nstep:9621/10000 train_loss:3.6091 train_time:1632790ms step_avg:169.89ms\nstep:9622/10000 train_loss:3.6495 train_time:1632962ms step_avg:169.89ms\nstep:9623/10000 train_loss:3.7542 train_time:1633131ms step_avg:169.89ms\nstep:9624/10000 train_loss:3.6413 train_time:1633301ms step_avg:169.89ms\nstep:9625/10000 train_loss:3.7232 train_time:1633473ms step_avg:169.89ms\nstep:9626/10000 train_loss:3.7726 train_time:1633643ms step_avg:169.89ms\nstep:9627/10000 train_loss:3.6887 train_time:1633814ms step_avg:169.89ms\nstep:9628/10000 train_loss:3.6940 train_time:1633983ms step_avg:169.89ms\nstep:9629/10000 train_loss:3.6313 train_time:1634152ms step_avg:169.89ms\nstep:9630/10000 train_loss:3.6099 train_time:1634319ms step_avg:169.89ms\nstep:9631/10000 train_loss:3.6177 train_time:1634488ms step_avg:169.89ms\nstep:9632/10000 train_loss:3.7196 train_time:1634658ms step_avg:169.89ms\nstep:9633/10000 train_loss:3.6744 train_time:1634830ms step_avg:169.89ms\nstep:9634/10000 train_loss:3.7463 train_time:1635001ms step_avg:169.89ms\nstep:9635/10000 train_loss:3.6541 train_time:1635171ms step_avg:169.89ms\nstep:9636/10000 train_loss:3.6946 train_time:1635338ms step_avg:169.89ms\nstep:9637/10000 train_loss:3.6827 train_time:1635507ms step_avg:169.89ms\nstep:9638/10000 train_loss:3.9453 train_time:1635674ms step_avg:169.89ms\nstep:9639/10000 train_loss:3.7777 train_time:1635843ms step_avg:169.89ms\nstep:9640/10000 train_loss:3.6783 train_time:1636015ms step_avg:169.89ms\nstep:9641/10000 train_loss:3.9037 train_time:1636182ms step_avg:169.89ms\nstep:9642/10000 train_loss:3.7060 train_time:1636354ms step_avg:169.89ms\nstep:9643/10000 train_loss:3.7419 train_time:1636523ms step_avg:169.89ms\nstep:9644/10000 train_loss:3.6980 train_time:1636694ms step_avg:169.89ms\nstep:9645/10000 train_loss:3.6818 train_time:1636863ms step_avg:169.89ms\nstep:9646/10000 train_loss:3.8254 train_time:1637036ms step_avg:169.89ms\nstep:9647/10000 train_loss:3.6552 train_time:1637204ms step_avg:169.89ms\nstep:9648/10000 train_loss:3.8565 train_time:1637373ms step_avg:169.89ms\nstep:9649/10000 train_loss:3.7348 train_time:1637542ms step_avg:169.89ms\nstep:9650/10000 train_loss:3.6915 train_time:1637714ms step_avg:169.89ms\nstep:9651/10000 train_loss:3.5799 train_time:1637883ms step_avg:169.89ms\nstep:9652/10000 train_loss:3.6876 train_time:1638055ms step_avg:169.89ms\nstep:9653/10000 train_loss:3.5454 train_time:1638223ms step_avg:169.89ms\nstep:9654/10000 train_loss:3.6421 train_time:1638394ms step_avg:169.89ms\nstep:9655/10000 train_loss:3.6877 train_time:1638561ms step_avg:169.89ms\nstep:9656/10000 train_loss:3.7798 train_time:1638730ms step_avg:169.89ms\nstep:9657/10000 train_loss:3.6647 train_time:1638899ms step_avg:169.89ms\nstep:9658/10000 train_loss:3.7482 train_time:1639067ms step_avg:169.89ms\nstep:9659/10000 train_loss:3.6780 train_time:1639237ms step_avg:169.89ms\nstep:9660/10000 train_loss:3.6039 train_time:1639406ms step_avg:169.89ms\nstep:9661/10000 train_loss:3.6034 train_time:1639576ms step_avg:169.89ms\nstep:9662/10000 train_loss:3.5728 train_time:1639741ms step_avg:169.89ms\nstep:9663/10000 train_loss:3.7475 train_time:1639912ms step_avg:169.89ms\nstep:9664/10000 train_loss:3.9089 train_time:1640079ms step_avg:169.89ms\nstep:9665/10000 train_loss:3.6947 train_time:1640248ms step_avg:169.89ms\nstep:9666/10000 train_loss:3.9743 train_time:1640418ms step_avg:169.89ms\nstep:9667/10000 train_loss:3.6946 train_time:1640587ms step_avg:169.89ms\nstep:9668/10000 train_loss:3.7300 train_time:1640757ms step_avg:169.89ms\nstep:9669/10000 train_loss:3.7450 train_time:1640928ms step_avg:169.89ms\nstep:9670/10000 train_loss:3.6564 train_time:1641099ms step_avg:169.89ms\nstep:9671/10000 train_loss:3.5161 train_time:1641269ms step_avg:169.89ms\nstep:9672/10000 train_loss:3.7474 train_time:1641437ms step_avg:169.89ms\nstep:9673/10000 train_loss:3.5903 train_time:1641606ms step_avg:169.89ms\nstep:9674/10000 train_loss:3.7234 train_time:1641777ms step_avg:169.89ms\nstep:9675/10000 train_loss:3.6532 train_time:1641945ms step_avg:169.89ms\nstep:9676/10000 train_loss:3.6543 train_time:1642115ms step_avg:169.89ms\nstep:9677/10000 train_loss:3.7781 train_time:1642283ms step_avg:169.89ms\nstep:9678/10000 train_loss:3.6639 train_time:1642454ms step_avg:169.89ms\nstep:9679/10000 train_loss:3.7487 train_time:1642622ms step_avg:169.89ms\nstep:9680/10000 train_loss:3.8565 train_time:1642793ms step_avg:169.89ms\nstep:9681/10000 train_loss:4.0989 train_time:1642961ms step_avg:169.89ms\nstep:9682/10000 train_loss:3.7295 train_time:1643132ms step_avg:169.89ms\nstep:9683/10000 train_loss:3.7525 train_time:1643300ms step_avg:169.89ms\nstep:9684/10000 train_loss:3.7112 train_time:1643468ms step_avg:169.89ms\nstep:9685/10000 train_loss:3.7580 train_time:1643637ms step_avg:169.88ms\nstep:9686/10000 train_loss:3.5807 train_time:1643805ms step_avg:169.88ms\nstep:9687/10000 train_loss:3.7857 train_time:1643975ms step_avg:169.88ms\nstep:9688/10000 train_loss:3.7801 train_time:1644145ms step_avg:169.88ms\nstep:9689/10000 train_loss:3.6465 train_time:1644316ms step_avg:169.88ms\nstep:9690/10000 train_loss:3.5104 train_time:1644484ms step_avg:169.88ms\nstep:9691/10000 train_loss:3.6341 train_time:1644653ms step_avg:169.88ms\nstep:9692/10000 train_loss:3.7271 train_time:1644822ms step_avg:169.88ms\nstep:9693/10000 train_loss:3.7786 train_time:1644993ms step_avg:169.88ms\nstep:9694/10000 train_loss:3.7675 train_time:1645161ms step_avg:169.88ms\nstep:9695/10000 train_loss:3.5999 train_time:1645331ms step_avg:169.88ms\nstep:9696/10000 train_loss:3.6153 train_time:1645500ms step_avg:169.88ms\nstep:9697/10000 train_loss:3.7500 train_time:1645669ms step_avg:169.88ms\nstep:9698/10000 train_loss:3.6493 train_time:1645837ms step_avg:169.88ms\nstep:9699/10000 train_loss:3.8149 train_time:1646006ms step_avg:169.88ms\nstep:9700/10000 train_loss:3.7347 train_time:1646178ms step_avg:169.88ms\nstep:9701/10000 train_loss:3.7282 train_time:1646346ms step_avg:169.88ms\nstep:9702/10000 train_loss:3.7012 train_time:1646517ms step_avg:169.88ms\nstep:9703/10000 train_loss:3.6262 train_time:1646687ms step_avg:169.88ms\nstep:9704/10000 train_loss:3.6529 train_time:1646857ms step_avg:169.88ms\nstep:9705/10000 train_loss:3.6747 train_time:1647026ms step_avg:169.88ms\nstep:9706/10000 train_loss:3.7790 train_time:1647197ms step_avg:169.88ms\nstep:9707/10000 train_loss:3.7799 train_time:1647369ms step_avg:169.88ms\nstep:9708/10000 train_loss:4.1841 train_time:1647540ms step_avg:169.88ms\nstep:9709/10000 train_loss:3.8493 train_time:1647708ms step_avg:169.88ms\nstep:9710/10000 train_loss:3.8370 train_time:1647880ms step_avg:169.88ms\nstep:9711/10000 train_loss:3.6808 train_time:1648047ms step_avg:169.88ms\nstep:9712/10000 train_loss:3.5635 train_time:1648216ms step_avg:169.88ms\nstep:9713/10000 train_loss:4.3573 train_time:1648386ms step_avg:169.88ms\nstep:9714/10000 train_loss:3.6512 train_time:1648555ms step_avg:169.88ms\nstep:9715/10000 train_loss:3.7278 train_time:1648723ms step_avg:169.88ms\nstep:9716/10000 train_loss:3.7812 train_time:1648893ms step_avg:169.88ms\nstep:9717/10000 train_loss:3.8374 train_time:1649063ms step_avg:169.88ms\nstep:9718/10000 train_loss:3.7393 train_time:1649235ms step_avg:169.88ms\nstep:9719/10000 train_loss:3.7117 train_time:1649403ms step_avg:169.88ms\nstep:9720/10000 train_loss:3.8153 train_time:1649571ms step_avg:169.88ms\nstep:9721/10000 train_loss:3.7757 train_time:1649741ms step_avg:169.88ms\nstep:9722/10000 train_loss:3.7423 train_time:1649909ms step_avg:169.88ms\nstep:9723/10000 train_loss:3.6653 train_time:1650075ms step_avg:169.88ms\nstep:9724/10000 train_loss:3.5584 train_time:1650242ms step_avg:169.88ms\nstep:9725/10000 train_loss:3.7619 train_time:1650414ms step_avg:169.88ms\nstep:9726/10000 train_loss:3.6727 train_time:1650582ms step_avg:169.88ms\nstep:9727/10000 train_loss:3.6064 train_time:1650750ms step_avg:169.88ms\nstep:9728/10000 train_loss:3.7518 train_time:1650921ms step_avg:169.88ms\nstep:9729/10000 train_loss:3.6140 train_time:1651091ms step_avg:169.88ms\nstep:9730/10000 train_loss:3.6244 train_time:1651260ms step_avg:169.88ms\nstep:9731/10000 train_loss:3.6647 train_time:1651430ms step_avg:169.88ms\nstep:9732/10000 train_loss:4.1568 train_time:1651600ms step_avg:169.88ms\nstep:9733/10000 train_loss:3.8125 train_time:1651771ms step_avg:169.88ms\nstep:9734/10000 train_loss:3.7326 train_time:1651943ms step_avg:169.88ms\nstep:9735/10000 train_loss:3.6133 train_time:1652114ms step_avg:169.88ms\nstep:9736/10000 train_loss:3.6461 train_time:1652281ms step_avg:169.88ms\nstep:9737/10000 train_loss:3.6572 train_time:1652451ms step_avg:169.88ms\nstep:9738/10000 train_loss:3.6664 train_time:1652620ms step_avg:169.88ms\nstep:9739/10000 train_loss:3.7141 train_time:1652791ms step_avg:169.88ms\nstep:9740/10000 train_loss:3.6660 train_time:1652961ms step_avg:169.88ms\nstep:9741/10000 train_loss:3.5893 train_time:1653132ms step_avg:169.88ms\nstep:9742/10000 train_loss:3.6921 train_time:1653300ms step_avg:169.88ms\nstep:9743/10000 train_loss:3.6783 train_time:1653469ms step_avg:169.88ms\nstep:9744/10000 train_loss:3.7059 train_time:1653636ms step_avg:169.88ms\nstep:9745/10000 train_loss:3.7678 train_time:1653806ms step_avg:169.88ms\nstep:9746/10000 train_loss:3.7477 train_time:1653979ms step_avg:169.88ms\nstep:9747/10000 train_loss:3.6560 train_time:1654150ms step_avg:169.88ms\nstep:9748/10000 train_loss:4.2435 train_time:1654320ms step_avg:169.88ms\nstep:9749/10000 train_loss:3.8723 train_time:1654489ms step_avg:169.88ms\nstep:9750/10000 train_loss:3.7117 train_time:1654658ms step_avg:169.88ms\nstep:9751/10000 train_loss:3.6655 train_time:1654826ms step_avg:169.88ms\nstep:9752/10000 train_loss:3.5773 train_time:1654998ms step_avg:169.88ms\nstep:9753/10000 train_loss:3.9171 train_time:1655169ms step_avg:169.88ms\nstep:9754/10000 train_loss:3.6011 train_time:1655342ms step_avg:169.88ms\nstep:9755/10000 train_loss:3.5535 train_time:1655510ms step_avg:169.88ms\nstep:9756/10000 train_loss:3.7163 train_time:1655676ms step_avg:169.88ms\nstep:9757/10000 train_loss:3.8084 train_time:1655843ms step_avg:169.88ms\nstep:9758/10000 train_loss:3.5605 train_time:1656012ms step_avg:169.88ms\nstep:9759/10000 train_loss:3.7171 train_time:1656184ms step_avg:169.88ms\nstep:9760/10000 train_loss:3.7385 train_time:1656356ms step_avg:169.88ms\nstep:9761/10000 train_loss:3.7107 train_time:1656526ms step_avg:169.88ms\nstep:9762/10000 train_loss:3.7096 train_time:1656698ms step_avg:169.88ms\nstep:9763/10000 train_loss:3.7707 train_time:1656867ms step_avg:169.88ms\nstep:9764/10000 train_loss:3.7118 train_time:1657035ms step_avg:169.88ms\nstep:9765/10000 train_loss:3.6043 train_time:1657203ms step_avg:169.88ms\nstep:9766/10000 train_loss:3.6081 train_time:1657373ms step_avg:169.88ms\nstep:9767/10000 train_loss:3.6901 train_time:1657543ms step_avg:169.88ms\nstep:9768/10000 train_loss:3.6788 train_time:1657717ms step_avg:169.88ms\nstep:9769/10000 train_loss:3.8302 train_time:1657885ms step_avg:169.88ms\nstep:9770/10000 train_loss:3.6526 train_time:1658055ms step_avg:169.88ms\nstep:9771/10000 train_loss:3.6793 train_time:1658223ms step_avg:169.88ms\nstep:9772/10000 train_loss:3.6717 train_time:1658393ms step_avg:169.88ms\nstep:9773/10000 train_loss:3.7395 train_time:1658562ms step_avg:169.88ms\nstep:9774/10000 train_loss:3.7282 train_time:1658732ms step_avg:169.88ms\nstep:9775/10000 train_loss:3.6350 train_time:1658901ms step_avg:169.88ms\nstep:9776/10000 train_loss:3.6388 train_time:1659073ms step_avg:169.88ms\nstep:9777/10000 train_loss:3.5928 train_time:1659243ms step_avg:169.88ms\nstep:9778/10000 train_loss:3.6470 train_time:1659415ms step_avg:169.88ms\nstep:9779/10000 train_loss:3.8263 train_time:1659584ms step_avg:169.88ms\nstep:9780/10000 train_loss:3.7345 train_time:1659755ms step_avg:169.88ms\nstep:9781/10000 train_loss:3.5615 train_time:1659924ms step_avg:169.88ms\nstep:9782/10000 train_loss:3.7531 train_time:1660094ms step_avg:169.88ms\nstep:9783/10000 train_loss:3.6623 train_time:1660262ms step_avg:169.88ms\nstep:9784/10000 train_loss:3.6144 train_time:1660433ms step_avg:169.88ms\nstep:9785/10000 train_loss:3.8423 train_time:1660601ms step_avg:169.88ms\nstep:9786/10000 train_loss:3.6329 train_time:1660771ms step_avg:169.88ms\nstep:9787/10000 train_loss:3.6189 train_time:1660939ms step_avg:169.88ms\nstep:9788/10000 train_loss:3.6864 train_time:1661108ms step_avg:169.88ms\nstep:9789/10000 train_loss:3.6691 train_time:1661278ms step_avg:169.88ms\nstep:9790/10000 train_loss:3.7028 train_time:1661445ms step_avg:169.88ms\nstep:9791/10000 train_loss:3.7254 train_time:1661615ms step_avg:169.88ms\nstep:9792/10000 train_loss:3.7972 train_time:1661783ms step_avg:169.88ms\nstep:9793/10000 train_loss:3.6245 train_time:1661952ms step_avg:169.88ms\nstep:9794/10000 train_loss:3.5705 train_time:1662121ms step_avg:169.88ms\nstep:9795/10000 train_loss:3.7148 train_time:1662292ms step_avg:169.88ms\nstep:9796/10000 train_loss:3.7716 train_time:1662461ms step_avg:169.88ms\nstep:9797/10000 train_loss:3.6075 train_time:1662631ms step_avg:169.88ms\nstep:9798/10000 train_loss:3.5721 train_time:1662801ms step_avg:169.88ms\nstep:9799/10000 train_loss:3.5673 train_time:1662970ms step_avg:169.88ms\nstep:9800/10000 train_loss:3.6733 train_time:1663140ms step_avg:169.88ms\nstep:9801/10000 train_loss:3.5878 train_time:1663310ms step_avg:169.88ms\nstep:9802/10000 train_loss:3.7664 train_time:1663480ms step_avg:169.88ms\nstep:9803/10000 train_loss:3.7205 train_time:1663649ms step_avg:169.88ms\nstep:9804/10000 train_loss:3.8100 train_time:1663821ms step_avg:169.88ms\nstep:9805/10000 train_loss:3.7848 train_time:1663991ms step_avg:169.88ms\nstep:9806/10000 train_loss:3.5884 train_time:1664160ms step_avg:169.88ms\nstep:9807/10000 train_loss:3.6765 train_time:1664330ms step_avg:169.88ms\nstep:9808/10000 train_loss:3.7384 train_time:1664499ms step_avg:169.88ms\nstep:9809/10000 train_loss:3.7327 train_time:1664667ms step_avg:169.88ms\nstep:9810/10000 train_loss:3.6760 train_time:1664837ms step_avg:169.88ms\nstep:9811/10000 train_loss:3.6650 train_time:1665005ms step_avg:169.88ms\nstep:9812/10000 train_loss:3.7244 train_time:1665176ms step_avg:169.88ms\nstep:9813/10000 train_loss:3.6743 train_time:1665344ms step_avg:169.88ms\nstep:9814/10000 train_loss:3.8387 train_time:1665513ms step_avg:169.88ms\nstep:9815/10000 train_loss:3.6913 train_time:1665682ms step_avg:169.88ms\nstep:9816/10000 train_loss:3.7601 train_time:1665850ms step_avg:169.88ms\nstep:9817/10000 train_loss:3.8147 train_time:1666021ms step_avg:169.88ms\nstep:9818/10000 train_loss:3.6608 train_time:1666191ms step_avg:169.88ms\nstep:9819/10000 train_loss:3.7743 train_time:1666358ms step_avg:169.88ms\nstep:9820/10000 train_loss:3.6403 train_time:1666527ms step_avg:169.88ms\nstep:9821/10000 train_loss:3.7806 train_time:1666701ms step_avg:169.88ms\nstep:9822/10000 train_loss:3.7473 train_time:1666867ms step_avg:169.88ms\nstep:9823/10000 train_loss:3.6914 train_time:1667035ms step_avg:169.88ms\nstep:9824/10000 train_loss:3.6491 train_time:1667204ms step_avg:169.88ms\nstep:9825/10000 train_loss:3.6762 train_time:1667372ms step_avg:169.88ms\nstep:9826/10000 train_loss:3.6463 train_time:1667541ms step_avg:169.88ms\nstep:9827/10000 train_loss:3.7040 train_time:1667710ms step_avg:169.88ms\nstep:9828/10000 train_loss:3.8251 train_time:1667881ms step_avg:169.88ms\nstep:9829/10000 train_loss:3.6149 train_time:1668052ms step_avg:169.88ms\nstep:9830/10000 train_loss:3.7441 train_time:1668222ms step_avg:169.88ms\nstep:9831/10000 train_loss:3.7843 train_time:1668391ms step_avg:169.88ms\nstep:9832/10000 train_loss:3.6422 train_time:1668561ms step_avg:169.88ms\nstep:9833/10000 train_loss:3.9195 train_time:1668731ms step_avg:169.88ms\nstep:9834/10000 train_loss:3.6410 train_time:1668901ms step_avg:169.88ms\nstep:9835/10000 train_loss:3.7296 train_time:1669070ms step_avg:169.88ms\nstep:9836/10000 train_loss:3.4865 train_time:1669240ms step_avg:169.88ms\nstep:9837/10000 train_loss:3.6698 train_time:1669409ms step_avg:169.88ms\nstep:9838/10000 train_loss:3.7723 train_time:1669578ms step_avg:169.88ms\nstep:9839/10000 train_loss:3.8030 train_time:1669745ms step_avg:169.88ms\nstep:9840/10000 train_loss:3.6923 train_time:1669914ms step_avg:169.88ms\nstep:9841/10000 train_loss:3.6907 train_time:1670083ms step_avg:169.88ms\nstep:9842/10000 train_loss:3.6946 train_time:1670252ms step_avg:169.88ms\nstep:9843/10000 train_loss:3.7918 train_time:1670420ms step_avg:169.88ms\nstep:9844/10000 train_loss:3.6612 train_time:1670590ms step_avg:169.88ms\nstep:9845/10000 train_loss:4.1774 train_time:1670761ms step_avg:169.88ms\nstep:9846/10000 train_loss:3.6204 train_time:1670932ms step_avg:169.88ms\nstep:9847/10000 train_loss:3.6148 train_time:1671100ms step_avg:169.88ms\nstep:9848/10000 train_loss:3.6487 train_time:1671270ms step_avg:169.88ms\nstep:9849/10000 train_loss:3.7314 train_time:1671440ms step_avg:169.88ms\nstep:9850/10000 train_loss:3.5895 train_time:1671611ms step_avg:169.88ms\nstep:9851/10000 train_loss:3.6636 train_time:1671782ms step_avg:169.88ms\nstep:9852/10000 train_loss:3.8189 train_time:1671953ms step_avg:169.88ms\nstep:9853/10000 train_loss:3.5961 train_time:1672122ms step_avg:169.88ms\nstep:9854/10000 train_loss:3.6737 train_time:1672291ms step_avg:169.88ms\nstep:9855/10000 train_loss:3.6323 train_time:1672460ms step_avg:169.88ms\nstep:9856/10000 train_loss:4.5952 train_time:1672631ms step_avg:169.88ms\nstep:9857/10000 train_loss:3.8450 train_time:1672800ms step_avg:169.88ms\nstep:9858/10000 train_loss:4.2515 train_time:1672970ms step_avg:169.88ms\nstep:9859/10000 train_loss:3.7444 train_time:1673139ms step_avg:169.88ms\nstep:9860/10000 train_loss:3.7886 train_time:1673308ms step_avg:169.88ms\nstep:9861/10000 train_loss:3.8890 train_time:1673476ms step_avg:169.88ms\nstep:9862/10000 train_loss:3.7471 train_time:1673644ms step_avg:169.88ms\nstep:9863/10000 train_loss:3.7045 train_time:1673813ms step_avg:169.88ms\nstep:9864/10000 train_loss:3.6894 train_time:1673981ms step_avg:169.88ms\nstep:9865/10000 train_loss:3.7090 train_time:1674151ms step_avg:169.88ms\nstep:9866/10000 train_loss:3.5687 train_time:1674321ms step_avg:169.88ms\nstep:9867/10000 train_loss:3.6547 train_time:1674494ms step_avg:169.88ms\nstep:9868/10000 train_loss:3.6377 train_time:1674661ms step_avg:169.88ms\nstep:9869/10000 train_loss:3.6003 train_time:1674830ms step_avg:169.88ms\nstep:9870/10000 train_loss:3.7394 train_time:1674999ms step_avg:169.88ms\nstep:9871/10000 train_loss:3.6774 train_time:1675167ms step_avg:169.88ms\nstep:9872/10000 train_loss:3.7202 train_time:1675337ms step_avg:169.88ms\nstep:9873/10000 train_loss:3.9393 train_time:1675505ms step_avg:169.88ms\nstep:9874/10000 train_loss:3.6801 train_time:1675673ms step_avg:169.88ms\nstep:9875/10000 train_loss:3.7615 train_time:1675841ms step_avg:169.88ms\nstep:9876/10000 train_loss:3.6678 train_time:1676011ms step_avg:169.88ms\nstep:9877/10000 train_loss:3.6619 train_time:1676180ms step_avg:169.88ms\nstep:9878/10000 train_loss:3.8594 train_time:1676352ms step_avg:169.88ms\nstep:9879/10000 train_loss:3.7600 train_time:1676526ms step_avg:169.88ms\nstep:9880/10000 train_loss:3.9930 train_time:1676691ms step_avg:169.88ms\nstep:9881/10000 train_loss:3.6825 train_time:1676859ms step_avg:169.88ms\nstep:9882/10000 train_loss:4.1160 train_time:1677027ms step_avg:169.88ms\nstep:9883/10000 train_loss:3.7373 train_time:1677195ms step_avg:169.88ms\nstep:9884/10000 train_loss:3.5849 train_time:1677364ms step_avg:169.88ms\nstep:9885/10000 train_loss:3.5390 train_time:1677533ms step_avg:169.88ms\nstep:9886/10000 train_loss:3.7886 train_time:1677704ms step_avg:169.88ms\nstep:9887/10000 train_loss:3.6724 train_time:1677876ms step_avg:169.88ms\nstep:9888/10000 train_loss:3.7352 train_time:1678044ms step_avg:169.88ms\nstep:9889/10000 train_loss:3.7349 train_time:1678215ms step_avg:169.88ms\nstep:9890/10000 train_loss:3.6443 train_time:1678384ms step_avg:169.88ms\nstep:9891/10000 train_loss:3.6958 train_time:1678552ms step_avg:169.88ms\nstep:9892/10000 train_loss:3.6253 train_time:1678723ms step_avg:169.88ms\nstep:9893/10000 train_loss:3.5338 train_time:1678893ms step_avg:169.88ms\nstep:9894/10000 train_loss:3.7536 train_time:1679069ms step_avg:169.88ms\nstep:9895/10000 train_loss:3.6555 train_time:1679240ms step_avg:169.88ms\nstep:9896/10000 train_loss:3.7558 train_time:1679407ms step_avg:169.88ms\nstep:9897/10000 train_loss:3.6357 train_time:1679578ms step_avg:169.88ms\nstep:9898/10000 train_loss:3.7918 train_time:1679747ms step_avg:169.88ms\nstep:9899/10000 train_loss:3.7244 train_time:1679917ms step_avg:169.88ms\nstep:9900/10000 train_loss:3.7873 train_time:1680086ms step_avg:169.88ms\nstep:9901/10000 train_loss:3.7246 train_time:1680255ms step_avg:169.88ms\nstep:9902/10000 train_loss:3.5877 train_time:1680424ms step_avg:169.88ms\nstep:9903/10000 train_loss:3.7432 train_time:1680596ms step_avg:169.88ms\nstep:9904/10000 train_loss:3.7583 train_time:1680766ms step_avg:169.88ms\nstep:9905/10000 train_loss:3.6490 train_time:1680938ms step_avg:169.88ms\nstep:9906/10000 train_loss:3.7251 train_time:1681345ms step_avg:169.90ms\nstep:9907/10000 train_loss:3.7084 train_time:1681510ms step_avg:169.90ms\nstep:9908/10000 train_loss:3.6798 train_time:1681676ms step_avg:169.90ms\nstep:9909/10000 train_loss:3.6742 train_time:1681845ms step_avg:169.90ms\nstep:9910/10000 train_loss:3.7021 train_time:1682022ms step_avg:169.90ms\nstep:9911/10000 train_loss:3.7770 train_time:1682189ms step_avg:169.90ms\nstep:9912/10000 train_loss:3.6671 train_time:1682357ms step_avg:169.90ms\nstep:9913/10000 train_loss:3.6411 train_time:1682526ms step_avg:169.90ms\nstep:9914/10000 train_loss:3.6393 train_time:1682695ms step_avg:169.90ms\nstep:9915/10000 train_loss:3.7383 train_time:1682861ms step_avg:169.90ms\nstep:9916/10000 train_loss:3.8174 train_time:1683030ms step_avg:169.90ms\nstep:9917/10000 train_loss:3.7670 train_time:1683200ms step_avg:169.90ms\nstep:9918/10000 train_loss:3.5910 train_time:1683374ms step_avg:169.90ms\nstep:9919/10000 train_loss:3.6494 train_time:1683544ms step_avg:169.90ms\nstep:9920/10000 train_loss:3.9239 train_time:1683714ms step_avg:169.90ms\nstep:9921/10000 train_loss:3.6629 train_time:1683881ms step_avg:169.90ms\nstep:9922/10000 train_loss:3.6808 train_time:1684049ms step_avg:169.90ms\nstep:9923/10000 train_loss:3.7067 train_time:1684216ms step_avg:169.90ms\nstep:9924/10000 train_loss:3.5750 train_time:1684386ms step_avg:169.90ms\nstep:9925/10000 train_loss:3.6201 train_time:1684558ms step_avg:169.90ms\nstep:9926/10000 train_loss:3.6553 train_time:1684728ms step_avg:169.90ms\nstep:9927/10000 train_loss:3.6861 train_time:1684904ms step_avg:169.90ms\nstep:9928/10000 train_loss:3.7347 train_time:1685070ms step_avg:169.90ms\nstep:9929/10000 train_loss:3.6351 train_time:1685241ms step_avg:169.90ms\nstep:9930/10000 train_loss:3.7243 train_time:1685411ms step_avg:169.90ms\nstep:9931/10000 train_loss:3.6299 train_time:1685581ms step_avg:169.90ms\nstep:9932/10000 train_loss:3.7094 train_time:1685752ms step_avg:169.90ms\nstep:9933/10000 train_loss:3.8358 train_time:1685920ms step_avg:169.90ms\nstep:9934/10000 train_loss:3.6675 train_time:1686088ms step_avg:169.90ms\nstep:9935/10000 train_loss:3.6477 train_time:1686257ms step_avg:169.90ms\nstep:9936/10000 train_loss:3.5616 train_time:1686425ms step_avg:169.90ms\nstep:9937/10000 train_loss:3.6963 train_time:1686594ms step_avg:169.90ms\nstep:9938/10000 train_loss:3.7942 train_time:1686763ms step_avg:169.90ms\nstep:9939/10000 train_loss:3.7356 train_time:1686932ms step_avg:169.90ms\nstep:9940/10000 train_loss:3.5269 train_time:1687102ms step_avg:169.90ms\nstep:9941/10000 train_loss:3.9200 train_time:1687274ms step_avg:169.90ms\nstep:9942/10000 train_loss:3.7135 train_time:1687443ms step_avg:169.90ms\nstep:9943/10000 train_loss:3.7271 train_time:1687617ms step_avg:169.90ms\nstep:9944/10000 train_loss:3.6932 train_time:1687787ms step_avg:169.90ms\nstep:9945/10000 train_loss:3.7790 train_time:1687957ms step_avg:169.90ms\nstep:9946/10000 train_loss:3.7116 train_time:1688126ms step_avg:169.90ms\nstep:9947/10000 train_loss:3.6949 train_time:1688299ms step_avg:169.90ms\nstep:9948/10000 train_loss:3.7155 train_time:1688467ms step_avg:169.90ms\nstep:9949/10000 train_loss:3.6232 train_time:1688635ms step_avg:169.90ms\nstep:9950/10000 train_loss:3.8278 train_time:1688803ms step_avg:169.90ms\nstep:9951/10000 train_loss:3.8860 train_time:1688973ms step_avg:169.90ms\nstep:9952/10000 train_loss:3.7120 train_time:1689141ms step_avg:169.90ms\nstep:9953/10000 train_loss:3.6318 train_time:1689313ms step_avg:169.90ms\nstep:9954/10000 train_loss:3.3604 train_time:1689482ms step_avg:169.90ms\nstep:9955/10000 train_loss:3.7658 train_time:1689656ms step_avg:169.90ms\nstep:9956/10000 train_loss:3.6560 train_time:1689823ms step_avg:169.90ms\nstep:9957/10000 train_loss:3.7500 train_time:1689992ms step_avg:169.90ms\nstep:9958/10000 train_loss:3.6611 train_time:1690161ms step_avg:169.90ms\nstep:9959/10000 train_loss:3.6598 train_time:1690332ms step_avg:169.90ms\nstep:9960/10000 train_loss:3.6839 train_time:1690503ms step_avg:169.90ms\nstep:9961/10000 train_loss:3.7708 train_time:1690672ms step_avg:169.90ms\nstep:9962/10000 train_loss:3.7567 train_time:1690844ms step_avg:169.90ms\nstep:9963/10000 train_loss:3.7998 train_time:1691015ms step_avg:169.90ms\nstep:9964/10000 train_loss:3.5769 train_time:1691183ms step_avg:169.90ms\nstep:9965/10000 train_loss:4.1731 train_time:1691354ms step_avg:169.90ms\nstep:9966/10000 train_loss:3.6969 train_time:1691521ms step_avg:169.90ms\nstep:9967/10000 train_loss:3.7343 train_time:1691691ms step_avg:169.90ms\nstep:9968/10000 train_loss:3.7480 train_time:1691859ms step_avg:169.90ms\nstep:9969/10000 train_loss:3.6995 train_time:1692028ms step_avg:169.90ms\nstep:9970/10000 train_loss:3.6961 train_time:1692197ms step_avg:169.90ms\nstep:9971/10000 train_loss:3.6591 train_time:1692364ms step_avg:169.90ms\nstep:9972/10000 train_loss:3.7321 train_time:1692534ms step_avg:169.90ms\nstep:9973/10000 train_loss:3.7869 train_time:1692703ms step_avg:169.90ms\nstep:9974/10000 train_loss:3.7461 train_time:1692874ms step_avg:169.90ms\nstep:9975/10000 train_loss:3.6106 train_time:1693041ms step_avg:169.90ms\nstep:9976/10000 train_loss:3.6410 train_time:1693213ms step_avg:169.90ms\nstep:9977/10000 train_loss:3.7729 train_time:1693380ms step_avg:169.90ms\nstep:9978/10000 train_loss:3.7745 train_time:1693550ms step_avg:169.90ms\nstep:9979/10000 train_loss:3.5776 train_time:1693722ms step_avg:169.90ms\nstep:9980/10000 train_loss:3.7245 train_time:1693891ms step_avg:169.90ms\nstep:9981/10000 train_loss:3.7756 train_time:1694060ms step_avg:169.90ms\nstep:9982/10000 train_loss:3.7703 train_time:1694230ms step_avg:169.90ms\nstep:9983/10000 train_loss:3.6259 train_time:1694400ms step_avg:169.90ms\nstep:9984/10000 train_loss:3.7806 train_time:1694569ms step_avg:169.90ms\nstep:9985/10000 train_loss:3.7367 train_time:1694742ms step_avg:169.90ms\nstep:9986/10000 train_loss:3.7010 train_time:1694911ms step_avg:169.90ms\nstep:9987/10000 train_loss:3.7424 train_time:1695081ms step_avg:169.90ms\nstep:9988/10000 train_loss:3.8657 train_time:1695249ms step_avg:169.90ms\nstep:9989/10000 train_loss:3.7247 train_time:1695419ms step_avg:169.90ms\nstep:9990/10000 train_loss:3.7629 train_time:1695589ms step_avg:169.90ms\nstep:9991/10000 train_loss:3.7939 train_time:1695760ms step_avg:169.90ms\nstep:9992/10000 train_loss:3.7157 train_time:1695929ms step_avg:169.90ms\nstep:9993/10000 train_loss:3.8757 train_time:1696100ms step_avg:169.90ms\nstep:9994/10000 train_loss:3.4832 train_time:1696269ms step_avg:169.90ms\nstep:9995/10000 train_loss:3.5568 train_time:1696440ms step_avg:169.90ms\nstep:9996/10000 train_loss:3.6427 train_time:1696609ms step_avg:169.90ms\nstep:9997/10000 train_loss:3.7315 train_time:1696779ms step_avg:169.90ms\nstep:9998/10000 train_loss:3.5819 train_time:1696950ms step_avg:169.90ms\nstep:9999/10000 train_loss:3.6803 train_time:1697118ms step_avg:169.90ms\nstep:10000/10000 train_loss:3.7769 train_time:1697290ms step_avg:169.90ms\npeak memory consumption: 37806 MiB\n(Current Step: 19, Remaining Steps: 31)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe training completed successfully this time! The final loss was around 3.78, which seems reasonable for this model size after ~2 epochs. The training took about 1697 seconds (~28 minutes).\n\nThe script saved `model.pt` and `model_config.pt`. Let's check if they exist.\n```bash\nls -l\n```",
      "thought": "DISCUSSION\nThe training completed successfully this time! The final loss was around 3.78, which seems reasonable for this model size after ~2 epochs. The training took about 1697 seconds (~28 minutes).\n\nThe script saved `model.pt` and `model_config.pt`. Let's check if they exist.\n",
      "action": "ls -l\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "total 633664\n-rw-r--r-- 1 agent mlgym     17770 Apr  5 11:21 baseline.py\ndrwxr-xr-x 1 agent mlgym        20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym      6550 Mar 21 18:52 evaluate.py*\ndrwxr-xr-x 1 agent mlgym       152 Apr  5 11:22 logs/\n-rw-r--r-- 1 agent mlgym 648835100 Apr  5 11:51 model.pt\n-rw-r--r-- 1 agent mlgym        98 Apr  5 11:51 model_config.pt\n(Current Step: 20, Remaining Steps: 30)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "Exit due to cost limit",
      "thought": "Exit due to cost limit",
      "action": "exit_cost",
      "agent": "primary_1"
    }
  ],
  "info": {
    "model_stats": {
      "total_cost": 4.531479,
      "task_cost": 4.531479,
      "tokens_sent": 1476033,
      "tokens_received": 8615,
      "api_calls": 21
    },
    "exit_status": "evaluation_format_error (exit_cost)",
    "submission": null,
    "score": [],
    "summarizer": {}
  }
}