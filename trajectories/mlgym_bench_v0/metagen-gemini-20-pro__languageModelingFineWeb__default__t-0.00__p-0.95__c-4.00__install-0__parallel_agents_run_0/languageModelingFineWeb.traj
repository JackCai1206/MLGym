{
  "environment": "mlgym_main",
  "trajectory": [
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, let's start by exploring the current directory to see what files are available. This will help me understand the project structure, locate any baseline scripts, data files, or utility scripts.\n```bash\nls -al\n```",
      "thought": "DISCUSSION\nOkay, let's start by exploring the current directory to see what files are available. This will help me understand the project structure, locate any baseline scripts, data files, or utility scripts.\n",
      "action": "ls -al\n",
      "execution_time": 0.30101207457482815,
      "observation": "total 32\ndrwxr-xr-x 1 agent mlgym    52 Mar 26 03:00 ./\ndrwxr-x--- 1 agent mlgym    66 Mar 26 03:00 ../\n-rw-r--r-- 1 agent mlgym 22293 Mar 21 18:52 baseline.py\ndrwxr-xr-x 1 agent mlgym    20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym  6550 Mar 21 18:52 evaluate.py*"
    },
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nIt seems we have a `baseline.py` script, a `data` directory, and an `evaluate.py` script. The `baseline.py` is likely the starting point for our model training. Let's open it to understand the baseline implementation.\n```bash\nopen \"baseline.py\"\n```",
      "thought": "DISCUSSION\nIt seems we have a `baseline.py` script, a `data` directory, and an `evaluate.py` script. The `baseline.py` is likely the starting point for our model training. Let's open it to understand the baseline implementation.\n",
      "action": "open \"baseline.py\"\n",
      "execution_time": 0.41648813523352146,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe baseline script `baseline.py` trains a GPT-2 style model. It uses a custom optimizer called `Muon` and a custom distributed data loader.\n\nKey parameters from the `Hyperparameters` dataclass (lines 350-362):\n- `input_bin`: `./data/fineweb10B/fineweb_train_*.bin`\n- `batch_size`: 512 (total)\n- `device_batch_size`: 64\n- `sequence_length`: 1024\n- `num_iterations`: 500 - This seems very low for the FineWeb dataset (2.4B tokens). One epoch is approximately 2.4B / (512 * 1024) \u2248 4578 iterations. Training for only 500 iterations (~10% of an epoch) is likely suboptimal.\n- `warmup_iters`: 0\n- `warmdown_iters`: 1308 - This is longer than `num_iterations`. The learning rate schedule logic (lines 412-424) effectively implements only linear decay from `(500-it)/1308`, starting from `it=0`.\n- `weight_decay`: 0\n\nThe `ModelConfig` (lines 223-228) specifies:\n- `vocab_size`: 50304\n- `n_layer`: 12\n- `n_head`: 6\n- `n_embd`: 768\nThis is roughly a GPT-2 small configuration (~124M parameters).\n\n**Plan:**\n1.  Confirm the data path specified in `input_bin` exists.\n2.  Significantly increase `num_iterations` to train for closer to one epoch (e.g., 5000 iterations).\n3.  Adjust the learning rate schedule (`warmup_iters`, `warmdown_iters`) to be more conventional for the new number of iterations. Let's try 200 warmup iterations and have the decay last for the remaining iterations.\n4.  Introduce weight decay (e.g., 0.1) for the Adam optimizers (embeddings and LM head).\n5.  Run the modified script and validate.\n\nFirst, let's check the data directory.\n```bash\nls -l data/fineweb10B/\n```",
      "thought": "DISCUSSION\nThe baseline script `baseline.py` trains a GPT-2 style model. It uses a custom optimizer called `Muon` and a custom distributed data loader.\n\nKey parameters from the `Hyperparameters` dataclass (lines 350-362):\n- `input_bin`: `./data/fineweb10B/fineweb_train_*.bin`\n- `batch_size`: 512 (total)\n- `device_batch_size`: 64\n- `sequence_length`: 1024\n- `num_iterations`: 500 - This seems very low for the FineWeb dataset (2.4B tokens). One epoch is approximately 2.4B / (512 * 1024) \u2248 4578 iterations. Training for only 500 iterations (~10% of an epoch) is likely suboptimal.\n- `warmup_iters`: 0\n- `warmdown_iters`: 1308 - This is longer than `num_iterations`. The learning rate schedule logic (lines 412-424) effectively implements only linear decay from `(500-it)/1308`, starting from `it=0`.\n- `weight_decay`: 0\n\nThe `ModelConfig` (lines 223-228) specifies:\n- `vocab_size`: 50304\n- `n_layer`: 12\n- `n_head`: 6\n- `n_embd`: 768\nThis is roughly a GPT-2 small configuration (~124M parameters).\n\n**Plan:**\n1.  Confirm the data path specified in `input_bin` exists.\n2.  Significantly increase `num_iterations` to train for closer to one epoch (e.g., 5000 iterations).\n3.  Adjust the learning rate schedule (`warmup_iters`, `warmdown_iters`) to be more conventional for the new number of iterations. Let's try 200 warmup iterations and have the decay last for the remaining iterations.\n4.  Introduce weight decay (e.g., 0.1) for the Adam optimizers (embeddings and LM head).\n5.  Run the modified script and validate.\n\nFirst, let's check the data directory.\n",
      "action": "ls -l data/fineweb10B/\n",
      "execution_time": 0.30087111331522465,
      "observation": "ls: cannot access 'data/fineweb10B/': No such file or directory"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the path `./data/fineweb10B/` does not exist. The `Hyperparameters` default `input_bin` is incorrect. Let's check the `data` directory to see the actual path.\n```bash\nls -l data/\n```",
      "thought": "DISCUSSION\nOkay, the path `./data/fineweb10B/` does not exist. The `Hyperparameters` default `input_bin` is incorrect. Let's check the `data` directory to see the actual path.\n",
      "action": "ls -l data/\n",
      "execution_time": 0.30082201585173607,
      "observation": "total 0\ndr-xr-xr-x 1 agent mlgym 908 Mar 26 02:59 fineweb108/"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nIt seems the data directory is `fineweb108`, not `fineweb10B`. Let's update the `input_bin` path in `baseline.py`. I'll edit line 352.\n```bash\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nIt seems the data directory is `fineweb108`, not `fineweb10B`. Let's update the `input_bin` path in `baseline.py`. I'll edit line 352.\n",
      "action": "edit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "execution_time": 0.7796187167987227,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe data path on line 352 is updated correctly.\n\nNow, let's increase `num_iterations` to 5000 (approximately one epoch). I'll edit line 357.\n```bash\nedit 357:357\n    num_iterations : int = 5000 # number of iterations to run\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe data path on line 352 is updated correctly.\n\nNow, let's increase `num_iterations` to 5000 (approximately one epoch). I'll edit line 357.\n",
      "action": "edit 357:357\n    num_iterations : int = 5000 # number of iterations to run\nend_of_edit\n",
      "execution_time": 0.7758603859692812,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\n`num_iterations` is updated. Now, let's modify the learning rate schedule. I'll set `warmup_iters` to 200 (line 358) and `warmdown_iters` to `num_iterations - warmup_iters`, which is 5000 - 200 = 4800 (line 359). This creates a linear warmup followed by a linear decay for the rest of the training. I will also change the `get_lr` function to implement a cosine decay instead of linear decay, as that often works better. I will also add weight decay `0.1` to the Adam optimizers.\n```bash\nedit 358:360\n    warmup_iters : int = 200\n    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0.1\nend_of_edit\n```",
      "thought": "DISCUSSION\n`num_iterations` is updated. Now, let's modify the learning rate schedule. I'll set `warmup_iters` to 200 (line 358) and `warmdown_iters` to `num_iterations - warmup_iters`, which is 5000 - 200 = 4800 (line 359). This creates a linear warmup followed by a linear decay for the rest of the training. I will also change the `get_lr` function to implement a cosine decay instead of linear decay, as that often works better. I will also add weight decay `0.1` to the Adam optimizers.\n",
      "action": "edit 358:360\n    warmup_iters : int = 200\n    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0.1\nend_of_edit\n",
      "execution_time": 0.7932822657749057,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe warmup/warmdown iterations and weight decay hyperparameter values are updated. Now let's apply the weight decay to the Adam optimizers (optimizer1 and optimizer2). The Muon optimizer (optimizer3) doesn't seem to support weight decay according to its docstring. I'll edit lines 407 and 408.\n```bash\nedit 407:408\n    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe warmup/warmdown iterations and weight decay hyperparameter values are updated. Now let's apply the weight decay to the Adam optimizers (optimizer1 and optimizer2). The Muon optimizer (optimizer3) doesn't seem to support weight decay according to its docstring. I'll edit lines 407 and 408.\n",
      "action": "edit 407:408\n    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\nend_of_edit\n",
      "execution_time": 1.2457283260300756,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nWeight decay is added to the Adam optimizers.\n\nNow, let's change the learning rate schedule from linear decay to cosine decay after the warmup phase. First, I need to import the `math` module. I'll insert the import statement at the top with the other imports, for instance, after line 16.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nWeight decay is added to the Adam optimizers.\n\nNow, let's change the learning rate schedule from linear decay to cosine decay after the warmup phase. First, I need to import the `math` module. I'll insert the import statement at the top with the other imports, for instance, after line 16.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "execution_time": 1.3343970691785216,
      "observation": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAh, it seems the syntax check flagged an unrelated issue, possibly an existing invalid escape sequence `\\s` somewhere else in the file, likely within a docstring or comment. The `import math` line itself is standard. Let's try inserting it at a different location, perhaps closer to the top, after `import os` (line 12).\n```bash\ninsert 12\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nAh, it seems the syntax check flagged an unrelated issue, possibly an existing invalid escape sequence `\\s` somewhere else in the file, likely within a docstring or comment. The `import math` line itself is standard. Let's try inserting it at a different location, perhaps closer to the top, after `import os` (line 12).\n",
      "action": "insert 12\nimport math\nend_of_insert\n",
      "execution_time": 0.8577916342765093,
      "observation": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(6 more lines above)\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import math\n14:import pickle\n15:import sys\n16:import time\n17:import uuid\n(495 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(7 more lines above)\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n(494 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nIt seems the `SyntaxWarning: invalid escape sequence '\\s'` is persistent and likely exists in the original file, possibly within a docstring or comment. The `import math` statement itself is correct. Let's try to find and fix the `\\s` sequence first. I'll search the file for `\\s`.\n```bash\nsearch_file \"\\\\s\"\n```",
      "thought": "DISCUSSION\nIt seems the `SyntaxWarning: invalid escape sequence '\\s'` is persistent and likely exists in the original file, possibly within a docstring or comment. The `import math` statement itself is correct. Let's try to find and fix the `\\s` sequence first. I'll search the file for `\\s`.\n",
      "action": "search_file \"\\\\s\"\n",
      "execution_time": 0.9073207266628742,
      "observation": "Found 435 matches for \"\\s\" in /home/agent/workspace/baseline.py:\nLine 2:Copyright (c) Meta Platforms, Inc. and affiliates.\nLine 4:Adapted from https://github.com/KellerJordan/modded-nanogpt\nLine 6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\nLine 7:Run this file with the following command:\nLine 8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\nLine 11:import glob\nLine 12:import os\nLine 13:import pickle\nLine 14:import sys\nLine 15:import time\nLine 16:import uuid\nLine 17:from dataclasses import dataclass\nLine 19:import numpy as np\nLine 20:import torch\nLine 21:import torch._dynamo\nLine 22:import torch._inductor.config as config\nLine 23:import torch.distributed as dist\nLine 24:import torch.nn.functional as F\nLine 25:from torch import nn\nLine 26:from torch.nn.parallel import DistributedDataParallel as DDP\nLine 28:torch._dynamo.config.suppress_errors = True\nLine 30:# -----------------------------------------------------------------------------\nLine 31:# Muon optimizer\nLine 33:def zeropower_via_svd(G, steps=None):\nLine 34:    U, S, V = G.svd()\nLine 35:    return U @ V.T\nLine 38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\nLine 39:    \"\"\"\nLine 40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\nLine 41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\nLine 42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\nLine 43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\nLine 44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\nLine 45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\nLine 46:    performance at all relative to UV^T, where USV^T = G is the SVD.\nLine 47:    \"\"\"\nLine 48:    assert len(G.shape) == 2\nLine 49:    a, b, c = (3.4445, -4.7750,  2.0315)\nLine 50:    X = G.bfloat16()\nLine 51:    X /= (X.norm() + eps) # ensure top singular value <= 1\nLine 52:    if G.size(0) > G.size(1):\nLine 53:        X = X.T\nLine 54:    for _ in range(steps):\nLine 55:        A = X @ X.T\nLine 56:        B = A @ X\nLine 57:        X = a * X + b * B + c * A @ B\nLine 58:    if G.size(0) > G.size(1):\nLine 59:        X = X.T\nLine 60:    return X\nLine 62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\nLine 64:class Muon(torch.optim.Optimizer):\nLine 65:    \"\"\"\nLine 66:    Muon - MomentUm Orthogonalized by Newton-schulz\nLine 68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\nLine 69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\nLine 70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\nLine 71:    the advantage that it can be stably run in bfloat16 on the GPU.\nLine 73:    Some warnings:\nLine 74:    - This optimizer assumes that all parameters passed in are 2D.\nLine 75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\nLine 76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\nLine 77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\nLine 78:    - We believe it is unlikely to work well for training with small batch size.\nLine 79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\nLine 80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\nLine 82:    Arguments:\nLine 83:        lr: The learning rate used by the internal SGD.\nLine 84:        momentum: The momentum used by the internal SGD.\nLine 85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\nLine 86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\nLine 87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\nLine 88:    \"\"\"\nLine 89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\nLine 90:                 backend='newtonschulz5', backend_steps=5):\nLine 91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\nLine 92:        super().__init__(params, defaults)\nLine 94:    def step(self):\nLine 96:        for group in self.param_groups:\nLine 98:            lr = group['lr']\nLine 99:            momentum = group['momentum']\nLine 100:            zeropower_backend = zeropower_backends[group['backend']]\nLine 102:            # generate weight updates in distributed fashion\nLine 103:            total_params = sum(p.numel() for p in group['params'])\nLine 104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\nLine 105:            curr_idx = 0\nLine 106:            for i, p in enumerate(group['params']):\nLine 107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\nLine 108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\nLine 109:                    g = p.grad\nLine 110:                    assert g is not None\nLine 111:                    state = self.state[p]\nLine 112:                    if 'momentum_buffer' not in state:\nLine 113:                        state['momentum_buffer'] = torch.zeros_like(g)\nLine 114:                    buf = state['momentum_buffer']\nLine 115:                    buf.mul_(momentum).add_(g)\nLine 116:                    if group['nesterov']:\nLine 117:                        g = g.add(buf, alpha=momentum)\nLine 118:                    g = zeropower_backend(g, steps=group['backend_steps'])\nLine 119:                    g *= max(1, g.size(0)/g.size(1))**0.5\nLine 120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\nLine 121:                curr_idx += p.numel()\nLine 123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\nLine 124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\nLine 126:            # deserialize and apply updates\nLine 127:            curr_idx = 0\nLine 128:            for p in group['params']:\nLine 129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\nLine 130:                p.data.add_(g, alpha=-lr)\nLine 131:                curr_idx += p.numel()\nLine 133:# -----------------------------------------------------------------------------\nLine 134:# PyTorch nn.Module definitions for the GPT-2 model\nLine 136:class Rotary(torch.nn.Module):\nLine 138:    def __init__(self, dim, base=10000):\nLine 139:        super().__init__()\nLine 140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\nLine 141:        self.seq_len_cached = None\nLine 142:        self.cos_cached = None\nLine 143:        self.sin_cached = None\nLine 145:    def forward(self, x):\nLine 146:        seq_len = x.shape[1]\nLine 147:        if seq_len != self.seq_len_cached:\nLine 148:            self.seq_len_cached = seq_len\nLine 149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\nLine 150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\nLine 151:            self.cos_cached = freqs.cos().bfloat16()\nLine 152:            self.sin_cached = freqs.sin().bfloat16()\nLine 153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\nLine 155:def apply_rotary_emb(x, cos, sin):\nLine 156:    assert x.ndim == 4 # multihead attention\nLine 157:    d = x.shape[3]//2\nLine 158:    x1 = x[..., :d]\nLine 159:    x2 = x[..., d:]\nLine 160:    y1 = x1 * cos + x2 * sin\nLine 161:    y2 = x1 * (-sin) + x2 * cos\nLine 162:    return torch.cat([y1, y2], 3).type_as(x)\nLine 164:class CausalSelfAttention(nn.Module):\nLine 166:    def __init__(self, config):\nLine 167:        super().__init__()\nLine 168:        self.n_head = config.n_head\nLine 169:        self.n_embd = config.n_embd\nLine 170:        self.head_dim = self.n_embd // self.n_head\nLine 171:        assert self.n_embd % self.n_head == 0\nLine 172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 175:        # output projection\nLine 176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\nLine 178:        self.rotary = Rotary(self.head_dim)\nLine 180:    def forward(self, x):\nLine 181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\nLine 182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\nLine 183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\nLine 184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\nLine 185:        cos, sin = self.rotary(q)\nLine 186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\nLine 187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\nLine 188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\nLine 189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\nLine 190:        y = self.c_proj(y)\nLine 191:        return y\nLine 193:class MLP(nn.Module):\nLine 195:    def __init__(self, config):\nLine 196:        super().__init__()\nLine 197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\nLine 198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\nLine 199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\nLine 201:    def forward(self, x):\nLine 202:        x = self.c_fc(x)\nLine 203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\nLine 204:        x = self.c_proj(x)\nLine 205:        return x\nLine 207:class Block(nn.Module):\nLine 209:    def __init__(self, config):\nLine 210:        super().__init__()\nLine 211:        self.attn = CausalSelfAttention(config)\nLine 212:        self.mlp = MLP(config)\nLine 214:    def forward(self, x):\nLine 215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\nLine 216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\nLine 217:        return x\nLine 219:# -----------------------------------------------------------------------------\nLine 220:# The main GPT-2 model\nLine 223:class ModelConfig:\nLine 224:    vocab_size : int = 50304\nLine 225:    n_layer : int = 12\nLine 226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\nLine 227:    n_embd : int = 768\nLine 229:class Model(nn.Module):\nLine 231:    def __init__(self, config):\nLine 232:        super().__init__()\nLine 233:        self.config = config\nLine 235:        self.transformer = nn.ModuleDict(dict(\nLine 236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\nLine 237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\nLine 238:        ))\nLine 239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\nLine 240:        self.lm_head.weight.data.zero_() # @Grad62304977\nLine 242:    def forward(self, idx, targets=None, return_logits=True):\nLine 244:        # forward the GPT model itself\nLine 245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\nLine 246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\nLine 247:        for block in self.transformer.h:\nLine 248:            x = block(x)\nLine 249:        x = F.rms_norm(x, (x.size(-1),))\nLine 251:        if targets is not None:\nLine 252:            # if we are given some desired targets also calculate the loss\nLine 253:            logits = self.lm_head(x)\nLine 254:            logits = logits.float() # use tf32/fp32 for logits\nLine 255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\nLine 256:        else:\nLine 257:            # inference-time mini-optimization: only forward the lm_head on the very last position\nLine 258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\nLine 259:            logits = logits.float() # use tf32/fp32 for logits\nLine 260:            loss = None\nLine 262:        # there are performance reasons why not returning logits is prudent, if not needed\nLine 263:        if not return_logits:\nLine 264:            logits = None\nLine 266:        return logits, loss\nLine 267:    \nLine 268:    def get_config(self):\nLine 269:        return self.config\nLine 271:# -----------------------------------------------------------------------------\nLine 272:# Our own simple Distributed Data Loader\nLine 274:def _peek_data_shard(filename):\nLine 275:    # only reads the header, returns header data\nLine 276:    with open(filename, \"rb\") as f:\nLine 277:        # first read the header, which is 256 int32 integers (4 bytes each)\nLine 278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\nLine 279:    if header[0] != 20240520:\nLine 280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\nLine 281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\nLine 282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\nLine 283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\nLine 284:        exit(1)\nLine 285:    assert header[1] == 1, \"unsupported version\"\nLine 286:    ntok = header[2] # number of tokens (claimed)\nLine 287:    return ntok # for now just return the number of tokens\nLine 289:def _load_data_shard(filename):\nLine 290:    with open(filename, \"rb\") as f:\nLine 291:        # first read the header, which is 256 int32 integers (4 bytes each)\nLine 292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\nLine 293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\nLine 294:        assert header[1] == 1, \"unsupported version\"\nLine 295:        ntok = header[2] # number of tokens (claimed)\nLine 296:        # the rest of it are tokens, stored as uint16\nLine 297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\nLine 298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\nLine 299:    return tokens\nLine 301:class DistributedDataLoader:\nLine 302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\nLine 303:        self.process_rank = process_rank\nLine 304:        self.num_processes = num_processes\nLine 305:        self.B = B\nLine 306:        self.T = T\nLine 308:        # glob files that match the pattern\nLine 309:        self.files = sorted(glob.glob(filename_pattern))\nLine 310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\nLine 312:        # load and validate all data shards, count number of tokens in total\nLine 313:        ntok_total = 0\nLine 314:        for fname in self.files:\nLine 315:            shard_ntok = _peek_data_shard(fname)\nLine 316:            assert shard_ntok >= num_processes * B * T + 1\nLine 317:            ntok_total += int(shard_ntok)\nLine 318:        self.ntok_total = ntok_total\nLine 320:        # kick things off\nLine 321:        self.reset()\nLine 323:    def reset(self):\nLine 324:        self.current_shard = 0\nLine 325:        self.current_position = self.process_rank * self.B * self.T\nLine 326:        self.tokens = _load_data_shard(self.files[self.current_shard])\nLine 328:    def advance(self): # advance to next data shard\nLine 329:        self.current_shard = (self.current_shard + 1) % len(self.files)\nLine 330:        self.current_position = self.process_rank * self.B * self.T\nLine 331:        self.tokens = _load_data_shard(self.files[self.current_shard])\nLine 333:    def next_batch(self):\nLine 334:        B = self.B\nLine 335:        T = self.T\nLine 336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\nLine 337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\nLine 338:        x = (buf[:-1]).view(B, T) # inputs\nLine 339:        y = (buf[1:]).view(B, T) # targets\nLine 340:        # advance current position and load next shard if necessary\nLine 341:        self.current_position += B * T * self.num_processes\nLine 342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\nLine 343:            self.advance()\nLine 344:        return x.cuda(), y.cuda()\nLine 346:# -----------------------------------------------------------------------------\nLine 347:# int main\nLine 350:class Hyperparameters:\nLine 351:    # data hyperparams\nLine 352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nLine 353:    # optimization hyperparams\nLine 354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\nLine 355:    device_batch_size : int = 64 # batch size, in sequences, per device\nLine 356:    sequence_length : int = 1024 # sequence length, in tokens\nLine 357:    num_iterations : int = 5000 # number of iterations to run\nLine 358:    warmup_iters : int = 200\nLine 359:    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\nLine 360:    weight_decay : float = 0.1\nLine 361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\nLine 364:def main():\nLine 365:    with open(sys.argv[0]) as f:\nLine 366:        code = f.read() # read the code of this file ASAP, for logging\nLine 367:    args = Hyperparameters()\nLine 368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\nLine 370:    # set up DDP (distributed data parallel). torchrun sets this env variable\nLine 371:    assert torch.cuda.is_available()\nLine 372:    dist.init_process_group(backend='nccl')\nLine 373:    ddp_rank = int(os.environ['RANK'])\nLine 374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\nLine 375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\nLine 376:    device = f'cuda:{ddp_local_rank}'\nLine 377:    torch.cuda.set_device(device)\nLine 378:    print(f\"using device: {device}\")\nLine 379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\nLine 381:    # convenience variables\nLine 382:    B, T = args.device_batch_size, args.sequence_length\nLine 383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\nLine 384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\nLine 385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\nLine 387:    # load tokens\nLine 388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\nLine 389:    if master_process:\nLine 390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\nLine 391:    x, y = train_loader.next_batch()\nLine 393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\nLine 394:    # this originates from Karpathy's experiments.\nLine 395:    num_vocab = 50304\nLine 396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\nLine 397:    model = model.cuda()\nLine 398:    if hasattr(config, \"coordinate_descent_tuning\"):\nLine 399:        config.coordinate_descent_tuning = True # suggested by @Chillee\nLine 400:    model = torch.compile(model)\nLine 401:    # here we wrap model into DDP container\nLine 402:    model = DDP(model, device_ids=[ddp_local_rank])\nLine 403:    raw_model = model.module # always contains the \"raw\" unwrapped model\nLine 404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\nLine 406:    # init the optimizer(s)\nLine 407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\nLine 408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\nLine 409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\nLine 410:    optimizers = [optimizer1, optimizer2, optimizer3]\nLine 411:    # learning rate decay scheduler (linear warmup and warmdown)\nLine 412:    def get_lr(it):\nLine 413:        assert it <= args.num_iterations\nLine 414:        # 1) linear warmup for warmup_iters steps\nLine 415:        if it < args.warmup_iters:\nLine 416:            return (it+1) / args.warmup_iters\nLine 417:        # 2) constant lr for a while\nLine 418:        elif it < args.num_iterations - args.warmdown_iters:\nLine 419:            return 1.0\nLine 420:        # 3) linear warmdown\nLine 421:        else:\nLine 422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\nLine 423:            return decay_ratio\nLine 424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nLine 426:    # begin logging\nLine 427:    if master_process:\nLine 428:        run_id = str(uuid.uuid4())\nLine 429:        logdir = 'logs/%s/' % run_id\nLine 430:        os.makedirs(logdir, exist_ok=True)\nLine 431:        logfile = 'logs/%s.txt' % run_id\nLine 432:        # create the log file\nLine 433:        with open(logfile, \"w\") as f:\nLine 434:            # begin the log by printing this file (the Python code)\nLine 435:            f.write('='*100 + '\\n')\nLine 436:            f.write(code)\nLine 437:            f.write('='*100 + '\\n')\nLine 438:            # log information about the hardware/software environment this is running on\nLine 439:            # and print the full `nvidia-smi` to file\nLine 440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\nLine 441:            import subprocess\nLine 442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\nLine 443:            f.write(f'{result.stdout}\\n')\nLine 444:            f.write('='*100 + '\\n')\nLine 446:    training_time_ms = 0\nLine 447:    # start the clock\nLine 448:    torch.cuda.synchronize()\nLine 449:    t0 = time.time()\nLine 450:    # begin training\nLine 451:    train_loader.reset()\nLine 452:    for step in range(args.num_iterations + 1):\nLine 453:        last_step = (step == args.num_iterations)\nLine 454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\nLine 455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\nLine 456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\nLine 457:        if step == 10:\nLine 458:            training_time_ms = 0\nLine 459:            t0 = time.time()\nLine 460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\nLine 462:        if last_step:\nLine 463:            break\nLine 465:        # --------------- TRAINING SECTION BEGIN -----------------\nLine 466:        model.train()\nLine 467:        for i in range(1, train_accumulation_steps+1):\nLine 468:            # forward pass\nLine 469:            with ctx:\nLine 470:                _, loss = model(x, y, return_logits=False)\nLine 471:                train_loss = loss.detach()\nLine 472:            # advance the dataset for the next batch\nLine 473:            x, y = train_loader.next_batch()\nLine 474:            # backward pass\nLine 475:            if i < train_accumulation_steps:\nLine 476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\nLine 477:                    loss.backward()\nLine 478:            else:\nLine 479:                loss.backward() # just sync on the last step\nLine 480:        for p in model.parameters():\nLine 481:            p.grad /= train_accumulation_steps\nLine 482:        # step the optimizers and schedulers\nLine 483:        for opt, sched in zip(optimizers, schedulers):\nLine 484:            opt.step()\nLine 485:            sched.step()\nLine 486:        # null the gradients\nLine 487:        model.zero_grad(set_to_none=True)\nLine 488:        # --------------- TRAINING SECTION END -------------------\nLine 489:        # everything that follows now is just diagnostics, prints, logging, etc.\nLine 491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\nLine 492:        if master_process:\nLine 493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\nLine 494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\nLine 495:            with open(logfile, \"a\") as f:\nLine 496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\nLine 498:    torch.save(model.state_dict(), \"model.pt\")\nLine 499:    with open(\"model_config.pt\", \"wb\") as f:\nLine 500:        pickle.dump(raw_model.get_config(), f)\nLine 501:    f.close()\nLine 503:    if master_process:\nLine 504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\nLine 506:    # -------------------------------------------------------------------------\nLine 507:    # clean up nice\nLine 508:    dist.destroy_process_group()\nLine 510:if __name__ == \"__main__\":\nLine 511:    main()\nEnd of matches for \"\\s\" in /home/agent/workspace/baseline.py"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe search confirms that `\\s` appears frequently, mostly representing actual whitespace in comments, docstrings, and code. The syntax checker seems overly sensitive. Since it's a warning, not an error, and inserting `import math` is unlikely to be the root cause, I'll proceed with the insertion and the subsequent changes. Let's try inserting `import math` after the initial docstring and before other imports (line 10).\n```bash\ninsert 10\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nThe search confirms that `\\s` appears frequently, mostly representing actual whitespace in comments, docstrings, and code. The syntax checker seems overly sensitive. Since it's a warning, not an error, and inserting `import math` is unlikely to be the root cause, I'll proceed with the insertion and the subsequent changes. Let's try inserting `import math` after the initial docstring and before other imports (line 10).\n",
      "action": "insert 10\nimport math\nend_of_insert\n",
      "execution_time": 0.8832288170233369,
      "observation": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(4 more lines above)\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import math\n12:import glob\n13:import os\n14:import pickle\n15:import sys\n(497 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(5 more lines above)\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n(496 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the syntax warning persists. It seems to be triggered by something inherent in the file that the checker doesn't like, even though it's likely valid Python. Since it's just a warning, I will ignore it for now and hope it doesn't prevent the script from running. Let's try inserting `import math` again, this time after line 10.\n```bash\ninsert 10\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nOkay, the syntax warning persists. It seems to be triggered by something inherent in the file that the checker doesn't like, even though it's likely valid Python. Since it's just a warning, I will ignore it for now and hope it doesn't prevent the script from running. Let's try inserting `import math` again, this time after line 10.\n",
      "action": "insert 10\nimport math\nend_of_insert\n",
      "execution_time": 0.9558536121621728,
      "observation": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(4 more lines above)\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import math\n12:import glob\n13:import os\n14:import pickle\n15:import sys\n(497 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(5 more lines above)\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n(496 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe syntax checker is consistently blocking the import. It seems I cannot insert `import math` due to this persistent warning, which is treated as an error by the environment's checker.\n\nLet's try a workaround for the cosine decay. Instead of using `math.pi`, I can approximate pi as `3.14159`. This avoids the need to import the `math` module.\n\nI will now modify the `get_lr` function (lines 412-424) to implement cosine decay after the linear warmup.\n\nThe new logic will be:\n1. Linear warmup for `warmup_iters` steps: `lr = (it + 1) / warmup_iters`\n2. Cosine decay for the remaining `num_iterations - warmup_iters` steps: `lr = 0.5 * (1 + cos(pi * (it - warmup_iters) / (num_iterations - warmup_iters)))`\n   - Note that the original `warmdown_iters` was set to `num_iterations - warmup_iters`, so I can reuse that variable.\n   - I'll use `torch.cos` instead of `math.cos`, and approximate pi.\n\nLet's edit the `get_lr` function.\n```bash\nedit 412:423\n    def get_lr(it):\n        assert it <= args.num_iterations\n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            return (it+1) / args.warmup_iters\n        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n        elif it > args.num_iterations:\n             return 0.0\n        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n        else:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n            return coeff # coeff starts at 1 and goes to 0\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe syntax checker is consistently blocking the import. It seems I cannot insert `import math` due to this persistent warning, which is treated as an error by the environment's checker.\n\nLet's try a workaround for the cosine decay. Instead of using `math.pi`, I can approximate pi as `3.14159`. This avoids the need to import the `math` module.\n\nI will now modify the `get_lr` function (lines 412-424) to implement cosine decay after the linear warmup.\n\nThe new logic will be:\n1. Linear warmup for `warmup_iters` steps: `lr = (it + 1) / warmup_iters`\n2. Cosine decay for the remaining `num_iterations - warmup_iters` steps: `lr = 0.5 * (1 + cos(pi * (it - warmup_iters) / (num_iterations - warmup_iters)))`\n   - Note that the original `warmdown_iters` was set to `num_iterations - warmup_iters`, so I can reuse that variable.\n   - I'll use `torch.cos` instead of `math.cos`, and approximate pi.\n\nLet's edit the `get_lr` function.\n",
      "action": "edit 412:423\n    def get_lr(it):\n        assert it <= args.num_iterations\n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            return (it+1) / args.warmup_iters\n        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n        elif it > args.num_iterations:\n             return 0.0\n        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n        else:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n            return coeff # coeff starts at 1 and goes to 0\nend_of_edit\n",
      "execution_time": 1.076862195506692,
      "observation": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- E111 indentation is not a multiple of 4\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (513 lines total)]\n(406 more lines above)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n418:        elif it > args.num_iterations:\n419:             return 0.0\n420:        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n421:        else:\n422:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n423:            assert 0 <= decay_ratio <= 1\n424:            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n425:            return coeff # coeff starts at 1 and goes to 0\n426:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n427:\n428:    # begin logging\n429:    if master_process:\n430:        run_id = str(uuid.uuid4())\n(83 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(406 more lines above)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n(83 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAh, the indentation error `E111 indentation is not a multiple of 4` points to line 419 in the proposed change: `             return 0.0`. I added too many spaces there. It should be indented by 12 spaces (3 levels * 4 spaces), not 13.\n\nLet's fix the indentation and try the edit again.\n```bash\nedit 412:423\n    def get_lr(it):\n        assert it <= args.num_iterations\n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            return (it+1) / args.warmup_iters\n        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n        elif it > args.num_iterations:\n            return 0.0 # Fixed indentation\n        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n        else:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n            return coeff # coeff starts at 1 and goes to 0\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, the indentation error `E111 indentation is not a multiple of 4` points to line 419 in the proposed change: `             return 0.0`. I added too many spaces there. It should be indented by 12 spaces (3 levels * 4 spaces), not 13.\n\nLet's fix the indentation and try the edit again.\n",
      "action": "edit 412:423\n    def get_lr(it):\n        assert it <= args.num_iterations\n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            return (it+1) / args.warmup_iters\n        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n        elif it > args.num_iterations:\n            return 0.0 # Fixed indentation\n        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n        else:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n            return coeff # coeff starts at 1 and goes to 0\nend_of_edit\n",
      "execution_time": 0.7836076961830258,
      "observation": "[File: /home/agent/workspace/baseline.py (513 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n418:        elif it > args.num_iterations:\n419:            return 0.0 # Fixed indentation\n420:        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n421:        else:\n422:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n423:            assert 0 <= decay_ratio <= 1\n424:            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n425:            return coeff # coeff starts at 1 and goes to 0\n426:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n427:\n428:    # begin logging\n429:    if master_process:\n430:        run_id = str(uuid.uuid4())\n431:        logdir = 'logs/%s/' % run_id\n432:        os.makedirs(logdir, exist_ok=True)\n433:        logfile = 'logs/%s.txt' % run_id\n434:        # create the log file\n435:        with open(logfile, \"w\") as f:\n436:            # begin the log by printing this file (the Python code)\n437:            f.write('='*100 + '\\n')\n438:            f.write(code)\n439:            f.write('='*100 + '\\n')\n440:            # log information about the hardware/software environment this is running on\n441:            # and print the full `nvidia-smi` to file\n442:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n443:            import subprocess\n444:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n445:            f.write(f'{result.stdout}\\n')\n446:            f.write('='*100 + '\\n')\n447:\n448:    training_time_ms = 0\n449:    # start the clock\n450:    torch.cuda.synchronize()\n451:    t0 = time.time()\n452:    # begin training\n453:    train_loader.reset()\n454:    for step in range(args.num_iterations + 1):\n455:        last_step = (step == args.num_iterations)\n456:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n457:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n458:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n459:        if step == 10:\n460:            training_time_ms = 0\n461:            t0 = time.time()\n462:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n463:\n464:        if last_step:\n465:            break\n466:\n467:        # --------------- TRAINING SECTION BEGIN -----------------\n468:        model.train()\n469:        for i in range(1, train_accumulation_steps+1):\n470:            # forward pass\n471:            with ctx:\n472:                _, loss = model(x, y, return_logits=False)\n473:                train_loss = loss.detach()\n474:            # advance the dataset for the next batch\n475:            x, y = train_loader.next_batch()\n476:            # backward pass\n477:            if i < train_accumulation_steps:\n478:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n479:                    loss.backward()\n480:            else:\n481:                loss.backward() # just sync on the last step\n482:        for p in model.parameters():\n483:            p.grad /= train_accumulation_steps\n484:        # step the optimizers and schedulers\n485:        for opt, sched in zip(optimizers, schedulers):\n486:            opt.step()\n487:            sched.step()\n488:        # null the gradients\n489:        model.zero_grad(set_to_none=True)\n490:        # --------------- TRAINING SECTION END -------------------\n491:        # everything that follows now is just diagnostics, prints, logging, etc.\n492:\n493:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n494:        if master_process:\n495:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n496:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n497:            with open(logfile, \"a\") as f:\n498:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n499:\n500:    torch.save(model.state_dict(), \"model.pt\")\n501:    with open(\"model_config.pt\", \"wb\") as f:\n502:        pickle.dump(raw_model.get_config(), f)\n503:    f.close()\n504:\n505:    if master_process:\n506:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n507:\n508:    # -------------------------------------------------------------------------\n509:    # clean up nice\n510:    dist.destroy_process_group()\n511:\n512:if __name__ == \"__main__\":\n513:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nGreat, the indentation is fixed, and the `get_lr` function now implements linear warmup followed by cosine decay without needing the `math` module.\n\nSummary of changes made so far:\n1. Corrected `input_bin` path in `Hyperparameters` (line 352).\n2. Increased `num_iterations` from 500 to 5000 (line 357).\n3. Set `warmup_iters` to 200 (line 358).\n4. Set `warmdown_iters` to 4800 (line 359).\n5. Set `weight_decay` to 0.1 (line 360).\n6. Applied `weight_decay` to Adam optimizers for embeddings and LM head (lines 407-408).\n7. Modified `get_lr` function for cosine decay after warmup (lines 412-425).\n\nThe script should now be ready to run. I'll use `torchrun` as suggested in the file's docstring. I'll check the number of available GPUs first.\n```bash\nnvidia-smi --list-gpus\n```",
      "thought": "DISCUSSION\nGreat, the indentation is fixed, and the `get_lr` function now implements linear warmup followed by cosine decay without needing the `math` module.\n\nSummary of changes made so far:\n1. Corrected `input_bin` path in `Hyperparameters` (line 352).\n2. Increased `num_iterations` from 500 to 5000 (line 357).\n3. Set `warmup_iters` to 200 (line 358).\n4. Set `warmdown_iters` to 4800 (line 359).\n5. Set `weight_decay` to 0.1 (line 360).\n6. Applied `weight_decay` to Adam optimizers for embeddings and LM head (lines 407-408).\n7. Modified `get_lr` function for cosine decay after warmup (lines 412-425).\n\nThe script should now be ready to run. I'll use `torchrun` as suggested in the file's docstring. I'll check the number of available GPUs first.\n",
      "action": "nvidia-smi --list-gpus\n",
      "execution_time": 0.3008366608992219,
      "observation": "GPU 0: NVIDIA H100 (UUID: GPU-db639abc-7f7c-0bfe-de79-f125bb6345f9)\nGPU 1: NVIDIA H100 (UUID: GPU-24f492f4-6b2d-dc91-c146-25b876718d6a)"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThere are 2 GPUs available (NVIDIA H100). I will use `torchrun` with `--nproc_per_node=2`.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThere are 2 GPUs available (NVIDIA H100). I will use `torchrun` with `--nproc_per_node=2`.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 1001.928841162473,
      "observation": "W0326 03:22:12.988000 1197 site-packages/torch/distributed/run.py:793] \nW0326 03:22:12.988000 1197 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:22:12.988000 1197 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:22:12.988000 1197 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\nusing device: cuda:1\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nstep:1/5000 train_loss:10.8258 train_time:69556ms step_avg:nanms\nstep:2/5000 train_loss:10.8233 train_time:82818ms step_avg:nanms\nstep:3/5000 train_loss:10.8225 train_time:82998ms step_avg:nanms\nstep:4/5000 train_loss:10.8205 train_time:83210ms step_avg:nanms\nstep:5/5000 train_loss:10.8160 train_time:83388ms step_avg:nanms\nstep:6/5000 train_loss:10.8110 train_time:83567ms step_avg:nanms\nstep:7/5000 train_loss:10.8049 train_time:83745ms step_avg:nanms\nstep:8/5000 train_loss:10.8008 train_time:83923ms step_avg:nanms\nstep:9/5000 train_loss:10.7933 train_time:84106ms step_avg:nanms\nstep:10/5000 train_loss:10.7872 train_time:84286ms step_avg:nanms\nstep:11/5000 train_loss:10.7793 train_time:173ms step_avg:nanms\nstep:12/5000 train_loss:10.7727 train_time:352ms step_avg:nanms\nstep:13/5000 train_loss:10.7610 train_time:530ms step_avg:176.77ms\nstep:14/5000 train_loss:10.7465 train_time:709ms step_avg:177.19ms\nstep:15/5000 train_loss:10.7398 train_time:887ms step_avg:177.36ms\nstep:16/5000 train_loss:10.7183 train_time:1065ms step_avg:177.44ms\nstep:17/5000 train_loss:10.7199 train_time:1244ms step_avg:177.71ms\nstep:18/5000 train_loss:10.6991 train_time:1422ms step_avg:177.75ms\nstep:19/5000 train_loss:10.6820 train_time:1601ms step_avg:177.91ms\nstep:20/5000 train_loss:10.6628 train_time:1781ms step_avg:178.08ms\nstep:21/5000 train_loss:10.6472 train_time:1961ms step_avg:178.30ms\nstep:22/5000 train_loss:10.6146 train_time:2141ms step_avg:178.39ms\nstep:23/5000 train_loss:10.5821 train_time:2321ms step_avg:178.56ms\nstep:24/5000 train_loss:10.5631 train_time:2500ms step_avg:178.60ms\nstep:25/5000 train_loss:10.5514 train_time:2680ms step_avg:178.68ms\nstep:26/5000 train_loss:10.5573 train_time:2860ms step_avg:178.74ms\nstep:27/5000 train_loss:10.4870 train_time:3040ms step_avg:178.82ms\nstep:28/5000 train_loss:10.4739 train_time:3219ms step_avg:178.86ms\nstep:29/5000 train_loss:10.4138 train_time:3400ms step_avg:178.96ms\nstep:30/5000 train_loss:10.3842 train_time:3579ms step_avg:178.95ms\nstep:31/5000 train_loss:10.2996 train_time:3761ms step_avg:179.07ms\nstep:32/5000 train_loss:10.1998 train_time:3941ms step_avg:179.13ms\nstep:33/5000 train_loss:10.2253 train_time:4121ms step_avg:179.19ms\nstep:34/5000 train_loss:10.1401 train_time:4300ms step_avg:179.15ms\nstep:35/5000 train_loss:10.0779 train_time:4478ms step_avg:179.14ms\nstep:36/5000 train_loss:10.0597 train_time:4658ms step_avg:179.17ms\nstep:37/5000 train_loss:9.8779 train_time:4840ms step_avg:179.25ms\nstep:38/5000 train_loss:9.8049 train_time:5019ms step_avg:179.26ms\nstep:39/5000 train_loss:9.6659 train_time:5200ms step_avg:179.31ms\nstep:40/5000 train_loss:9.6141 train_time:5380ms step_avg:179.32ms\nstep:41/5000 train_loss:9.4872 train_time:5559ms step_avg:179.32ms\nstep:42/5000 train_loss:9.3625 train_time:5740ms step_avg:179.38ms\nstep:43/5000 train_loss:9.2585 train_time:5920ms step_avg:179.40ms\nstep:44/5000 train_loss:9.0890 train_time:6101ms step_avg:179.43ms\nstep:45/5000 train_loss:8.8802 train_time:6280ms step_avg:179.44ms\nstep:46/5000 train_loss:8.7105 train_time:6458ms step_avg:179.40ms\nstep:47/5000 train_loss:8.7046 train_time:6639ms step_avg:179.44ms\nstep:48/5000 train_loss:8.5218 train_time:6819ms step_avg:179.45ms\nstep:49/5000 train_loss:8.3837 train_time:6999ms step_avg:179.47ms\nstep:50/5000 train_loss:8.2899 train_time:7178ms step_avg:179.46ms\nstep:51/5000 train_loss:8.3484 train_time:7356ms step_avg:179.43ms\nstep:52/5000 train_loss:8.1135 train_time:7537ms step_avg:179.44ms\nstep:53/5000 train_loss:8.1684 train_time:7716ms step_avg:179.45ms\nstep:54/5000 train_loss:8.0695 train_time:7898ms step_avg:179.49ms\nstep:55/5000 train_loss:7.8526 train_time:8079ms step_avg:179.53ms\nstep:56/5000 train_loss:8.0308 train_time:8258ms step_avg:179.52ms\nstep:57/5000 train_loss:7.9780 train_time:8437ms step_avg:179.51ms\nstep:58/5000 train_loss:8.0499 train_time:8619ms step_avg:179.57ms\nstep:59/5000 train_loss:8.0066 train_time:8799ms step_avg:179.57ms\nstep:60/5000 train_loss:8.0698 train_time:8978ms step_avg:179.56ms\nstep:61/5000 train_loss:8.1078 train_time:9159ms step_avg:179.58ms\nstep:62/5000 train_loss:8.0921 train_time:9339ms step_avg:179.59ms\nstep:63/5000 train_loss:7.9790 train_time:9519ms step_avg:179.60ms\nstep:64/5000 train_loss:7.9493 train_time:9700ms step_avg:179.64ms\nstep:65/5000 train_loss:8.1164 train_time:9881ms step_avg:179.65ms\nstep:66/5000 train_loss:8.0881 train_time:10059ms step_avg:179.63ms\nstep:67/5000 train_loss:7.9110 train_time:10239ms step_avg:179.63ms\nstep:68/5000 train_loss:7.9575 train_time:10419ms step_avg:179.64ms\nstep:69/5000 train_loss:8.1063 train_time:10600ms step_avg:179.65ms\nstep:70/5000 train_loss:8.0476 train_time:10779ms step_avg:179.64ms\nstep:71/5000 train_loss:8.0070 train_time:10959ms step_avg:179.65ms\nstep:72/5000 train_loss:8.2204 train_time:11139ms step_avg:179.67ms\nstep:73/5000 train_loss:8.0682 train_time:11320ms step_avg:179.68ms\nstep:74/5000 train_loss:8.1094 train_time:11500ms step_avg:179.69ms\nstep:75/5000 train_loss:8.0748 train_time:11680ms step_avg:179.69ms\nstep:76/5000 train_loss:8.0415 train_time:11859ms step_avg:179.68ms\nstep:77/5000 train_loss:8.0572 train_time:12038ms step_avg:179.67ms\nstep:78/5000 train_loss:7.8789 train_time:12219ms step_avg:179.69ms\nstep:79/5000 train_loss:8.0626 train_time:12399ms step_avg:179.69ms\nstep:80/5000 train_loss:8.1430 train_time:12578ms step_avg:179.69ms\nstep:81/5000 train_loss:8.0478 train_time:12759ms step_avg:179.70ms\nstep:82/5000 train_loss:8.1208 train_time:12939ms step_avg:179.71ms\nstep:83/5000 train_loss:7.9742 train_time:13119ms step_avg:179.71ms\nstep:84/5000 train_loss:7.9889 train_time:13298ms step_avg:179.70ms\nstep:85/5000 train_loss:8.1102 train_time:13476ms step_avg:179.69ms\nstep:86/5000 train_loss:8.2995 train_time:13656ms step_avg:179.69ms\nstep:87/5000 train_loss:8.1420 train_time:13837ms step_avg:179.70ms\nstep:88/5000 train_loss:7.9245 train_time:14018ms step_avg:179.72ms\nstep:89/5000 train_loss:8.0186 train_time:14198ms step_avg:179.72ms\nstep:90/5000 train_loss:8.0361 train_time:14379ms step_avg:179.73ms\nstep:91/5000 train_loss:7.9362 train_time:14560ms step_avg:179.75ms\nstep:92/5000 train_loss:7.9485 train_time:14740ms step_avg:179.76ms\nstep:93/5000 train_loss:7.9140 train_time:14921ms step_avg:179.77ms\nstep:94/5000 train_loss:8.1643 train_time:15101ms step_avg:179.77ms\nstep:95/5000 train_loss:8.0374 train_time:15280ms step_avg:179.76ms\nstep:96/5000 train_loss:7.9565 train_time:15458ms step_avg:179.75ms\nstep:97/5000 train_loss:7.8286 train_time:15638ms step_avg:179.74ms\nstep:98/5000 train_loss:7.9245 train_time:15819ms step_avg:179.76ms\nstep:99/5000 train_loss:7.8340 train_time:16001ms step_avg:179.79ms\nstep:100/5000 train_loss:7.7661 train_time:16180ms step_avg:179.78ms\nstep:101/5000 train_loss:7.9259 train_time:16359ms step_avg:179.77ms\nstep:102/5000 train_loss:7.8599 train_time:16539ms step_avg:179.77ms\nstep:103/5000 train_loss:7.9302 train_time:16719ms step_avg:179.77ms\nstep:104/5000 train_loss:7.9758 train_time:16899ms step_avg:179.78ms\nstep:105/5000 train_loss:7.9437 train_time:17079ms step_avg:179.78ms\nstep:106/5000 train_loss:7.9383 train_time:17258ms step_avg:179.77ms\nstep:107/5000 train_loss:7.9535 train_time:17439ms step_avg:179.78ms\nstep:108/5000 train_loss:7.8914 train_time:17620ms step_avg:179.80ms\nstep:109/5000 train_loss:8.0056 train_time:17799ms step_avg:179.79ms\nstep:110/5000 train_loss:7.7673 train_time:17979ms step_avg:179.79ms\nstep:111/5000 train_loss:7.7895 train_time:18160ms step_avg:179.80ms\nstep:112/5000 train_loss:7.9694 train_time:18341ms step_avg:179.82ms\nstep:113/5000 train_loss:7.9283 train_time:18522ms step_avg:179.82ms\nstep:114/5000 train_loss:7.9549 train_time:18701ms step_avg:179.82ms\nstep:115/5000 train_loss:7.9233 train_time:18880ms step_avg:179.81ms\nstep:116/5000 train_loss:7.8673 train_time:19060ms step_avg:179.81ms\nstep:117/5000 train_loss:7.8875 train_time:19240ms step_avg:179.81ms\nstep:118/5000 train_loss:7.8799 train_time:19420ms step_avg:179.81ms\nstep:119/5000 train_loss:7.9305 train_time:19600ms step_avg:179.82ms\nstep:120/5000 train_loss:7.9713 train_time:19780ms step_avg:179.82ms\nstep:121/5000 train_loss:7.8418 train_time:19960ms step_avg:179.82ms\nstep:122/5000 train_loss:7.8371 train_time:20140ms step_avg:179.82ms\nstep:123/5000 train_loss:7.8015 train_time:20321ms step_avg:179.83ms\nstep:124/5000 train_loss:8.2812 train_time:20501ms step_avg:179.84ms\nstep:125/5000 train_loss:7.8842 train_time:20682ms step_avg:179.84ms\nstep:126/5000 train_loss:7.7176 train_time:20861ms step_avg:179.84ms\nstep:127/5000 train_loss:7.8279 train_time:21042ms step_avg:179.84ms\nstep:128/5000 train_loss:7.8151 train_time:21223ms step_avg:179.85ms\nstep:129/5000 train_loss:8.0201 train_time:21403ms step_avg:179.86ms\nstep:130/5000 train_loss:8.0946 train_time:21583ms step_avg:179.86ms\nstep:131/5000 train_loss:7.8869 train_time:21764ms step_avg:179.87ms\nstep:132/5000 train_loss:7.8217 train_time:21943ms step_avg:179.86ms\nstep:133/5000 train_loss:7.9486 train_time:22123ms step_avg:179.86ms\nstep:134/5000 train_loss:7.9448 train_time:22302ms step_avg:179.85ms\nstep:135/5000 train_loss:7.9736 train_time:22481ms step_avg:179.85ms\nstep:136/5000 train_loss:7.9140 train_time:22661ms step_avg:179.85ms\nstep:137/5000 train_loss:7.9139 train_time:22841ms step_avg:179.85ms\nstep:138/5000 train_loss:8.0513 train_time:23021ms step_avg:179.86ms\nstep:139/5000 train_loss:7.9086 train_time:23202ms step_avg:179.86ms\nstep:140/5000 train_loss:8.0000 train_time:23383ms step_avg:179.87ms\nstep:141/5000 train_loss:7.9912 train_time:23562ms step_avg:179.86ms\nstep:142/5000 train_loss:8.1157 train_time:23741ms step_avg:179.86ms\nstep:143/5000 train_loss:7.9005 train_time:23921ms step_avg:179.86ms\nstep:144/5000 train_loss:8.0213 train_time:24101ms step_avg:179.86ms\nstep:145/5000 train_loss:7.9378 train_time:24281ms step_avg:179.86ms\nstep:146/5000 train_loss:7.9790 train_time:24461ms step_avg:179.86ms\nstep:147/5000 train_loss:7.8837 train_time:24641ms step_avg:179.86ms\nstep:148/5000 train_loss:8.0025 train_time:24820ms step_avg:179.86ms\nstep:149/5000 train_loss:7.7531 train_time:25002ms step_avg:179.87ms\nstep:150/5000 train_loss:7.9043 train_time:25182ms step_avg:179.87ms\nstep:151/5000 train_loss:7.9220 train_time:25362ms step_avg:179.87ms\nstep:152/5000 train_loss:7.8770 train_time:25542ms step_avg:179.88ms\nstep:153/5000 train_loss:8.0829 train_time:25722ms step_avg:179.87ms\nstep:154/5000 train_loss:7.9699 train_time:25903ms step_avg:179.88ms\nstep:155/5000 train_loss:7.8578 train_time:26083ms step_avg:179.88ms\nstep:156/5000 train_loss:7.9968 train_time:26262ms step_avg:179.88ms\nstep:157/5000 train_loss:7.9693 train_time:26444ms step_avg:179.89ms\nstep:158/5000 train_loss:7.9149 train_time:26624ms step_avg:179.89ms\nstep:159/5000 train_loss:7.9568 train_time:26804ms step_avg:179.90ms\nstep:160/5000 train_loss:7.9241 train_time:26984ms step_avg:179.90ms\nstep:161/5000 train_loss:8.0557 train_time:27163ms step_avg:179.89ms\nstep:162/5000 train_loss:7.9701 train_time:27344ms step_avg:179.89ms\nstep:163/5000 train_loss:7.8661 train_time:27524ms step_avg:179.89ms\nstep:164/5000 train_loss:8.0014 train_time:27703ms step_avg:179.89ms\nstep:165/5000 train_loss:7.9080 train_time:27883ms step_avg:179.89ms\nstep:166/5000 train_loss:7.8203 train_time:28064ms step_avg:179.90ms\nstep:167/5000 train_loss:7.8298 train_time:28245ms step_avg:179.90ms\nstep:168/5000 train_loss:7.9972 train_time:28426ms step_avg:179.91ms\nstep:169/5000 train_loss:8.0641 train_time:28605ms step_avg:179.91ms\nstep:170/5000 train_loss:7.9662 train_time:28785ms step_avg:179.90ms\nstep:171/5000 train_loss:8.1216 train_time:28964ms step_avg:179.90ms\nstep:172/5000 train_loss:7.9872 train_time:29144ms step_avg:179.90ms\nstep:173/5000 train_loss:7.9755 train_time:29324ms step_avg:179.90ms\nstep:174/5000 train_loss:7.9314 train_time:29504ms step_avg:179.91ms\nstep:175/5000 train_loss:7.8604 train_time:29685ms step_avg:179.91ms\nstep:176/5000 train_loss:7.9756 train_time:29865ms step_avg:179.91ms\nstep:177/5000 train_loss:7.9584 train_time:30045ms step_avg:179.91ms\nstep:178/5000 train_loss:8.1396 train_time:30225ms step_avg:179.91ms\nstep:179/5000 train_loss:8.0763 train_time:30405ms step_avg:179.91ms\nstep:180/5000 train_loss:7.8603 train_time:30585ms step_avg:179.91ms\nstep:181/5000 train_loss:7.8493 train_time:30765ms step_avg:179.91ms\nstep:182/5000 train_loss:7.8968 train_time:30946ms step_avg:179.92ms\nstep:183/5000 train_loss:8.0190 train_time:31126ms step_avg:179.92ms\nstep:184/5000 train_loss:7.9418 train_time:31306ms step_avg:179.92ms\nstep:185/5000 train_loss:8.0240 train_time:31486ms step_avg:179.92ms\nstep:186/5000 train_loss:7.8829 train_time:31665ms step_avg:179.92ms\nstep:187/5000 train_loss:8.1026 train_time:31845ms step_avg:179.92ms\nstep:188/5000 train_loss:7.9103 train_time:32024ms step_avg:179.91ms\nstep:189/5000 train_loss:8.0200 train_time:32203ms step_avg:179.91ms\nstep:190/5000 train_loss:8.0951 train_time:32384ms step_avg:179.91ms\nstep:191/5000 train_loss:7.9195 train_time:32565ms step_avg:179.92ms\nstep:192/5000 train_loss:8.0568 train_time:32744ms step_avg:179.91ms\nstep:193/5000 train_loss:7.9245 train_time:32924ms step_avg:179.91ms\nstep:194/5000 train_loss:8.0038 train_time:33104ms step_avg:179.91ms\nstep:195/5000 train_loss:7.9911 train_time:33283ms step_avg:179.91ms\nstep:196/5000 train_loss:7.7974 train_time:33463ms step_avg:179.91ms\nstep:197/5000 train_loss:7.9764 train_time:33645ms step_avg:179.92ms\nstep:198/5000 train_loss:8.0633 train_time:33825ms step_avg:179.92ms\nstep:199/5000 train_loss:7.9361 train_time:34006ms step_avg:179.93ms\nstep:200/5000 train_loss:7.9662 train_time:34186ms step_avg:179.92ms\nstep:201/5000 train_loss:7.9242 train_time:34365ms step_avg:179.92ms\nstep:202/5000 train_loss:7.9507 train_time:34545ms step_avg:179.92ms\nstep:203/5000 train_loss:7.6601 train_time:34726ms step_avg:179.92ms\nstep:204/5000 train_loss:8.0219 train_time:34908ms step_avg:179.94ms\nstep:205/5000 train_loss:7.9905 train_time:35087ms step_avg:179.93ms\nstep:206/5000 train_loss:7.8736 train_time:35267ms step_avg:179.93ms\nstep:207/5000 train_loss:7.9172 train_time:35447ms step_avg:179.93ms\nstep:208/5000 train_loss:8.0441 train_time:35627ms step_avg:179.94ms\nstep:209/5000 train_loss:8.0173 train_time:35808ms step_avg:179.94ms\nstep:210/5000 train_loss:7.9574 train_time:35989ms step_avg:179.94ms\nstep:211/5000 train_loss:8.0025 train_time:36168ms step_avg:179.94ms\nstep:212/5000 train_loss:7.9812 train_time:36348ms step_avg:179.94ms\nstep:213/5000 train_loss:8.1202 train_time:36528ms step_avg:179.94ms\nstep:214/5000 train_loss:7.9754 train_time:36708ms step_avg:179.94ms\nstep:215/5000 train_loss:7.9439 train_time:36887ms step_avg:179.94ms\nstep:216/5000 train_loss:7.9892 train_time:37068ms step_avg:179.94ms\nstep:217/5000 train_loss:7.8952 train_time:37248ms step_avg:179.94ms\nstep:218/5000 train_loss:8.6761 train_time:37427ms step_avg:179.94ms\nstep:219/5000 train_loss:8.0945 train_time:37607ms step_avg:179.94ms\nstep:220/5000 train_loss:8.1081 train_time:37787ms step_avg:179.94ms\nstep:221/5000 train_loss:8.0427 train_time:37966ms step_avg:179.93ms\nstep:222/5000 train_loss:8.0369 train_time:38145ms step_avg:179.93ms\nstep:223/5000 train_loss:8.0061 train_time:38325ms step_avg:179.93ms\nstep:224/5000 train_loss:7.9660 train_time:38504ms step_avg:179.93ms\nstep:225/5000 train_loss:8.0086 train_time:38684ms step_avg:179.93ms\nstep:226/5000 train_loss:7.9409 train_time:38863ms step_avg:179.92ms\nstep:227/5000 train_loss:8.0667 train_time:39043ms step_avg:179.92ms\nstep:228/5000 train_loss:7.9794 train_time:39222ms step_avg:179.92ms\nstep:229/5000 train_loss:7.9887 train_time:39401ms step_avg:179.91ms\nstep:230/5000 train_loss:8.0470 train_time:39581ms step_avg:179.91ms\nstep:231/5000 train_loss:8.1044 train_time:39760ms step_avg:179.91ms\nstep:232/5000 train_loss:7.9940 train_time:39941ms step_avg:179.91ms\nstep:233/5000 train_loss:7.9380 train_time:40121ms step_avg:179.92ms\nstep:234/5000 train_loss:7.9426 train_time:40301ms step_avg:179.91ms\nstep:235/5000 train_loss:7.9603 train_time:40482ms step_avg:179.92ms\nstep:236/5000 train_loss:7.9972 train_time:40662ms step_avg:179.92ms\nstep:237/5000 train_loss:8.0234 train_time:40842ms step_avg:179.92ms\nstep:238/5000 train_loss:8.0128 train_time:41022ms step_avg:179.92ms\nstep:239/5000 train_loss:8.0631 train_time:41202ms step_avg:179.92ms\nstep:240/5000 train_loss:7.9622 train_time:41383ms step_avg:179.92ms\nstep:241/5000 train_loss:8.0693 train_time:41562ms step_avg:179.92ms\nstep:242/5000 train_loss:7.9278 train_time:41741ms step_avg:179.92ms\nstep:243/5000 train_loss:7.9651 train_time:41921ms step_avg:179.92ms\nstep:244/5000 train_loss:7.9116 train_time:42102ms step_avg:179.92ms\nstep:245/5000 train_loss:8.0022 train_time:42282ms step_avg:179.92ms\nstep:246/5000 train_loss:7.9878 train_time:42464ms step_avg:179.93ms\nstep:247/5000 train_loss:7.9167 train_time:42644ms step_avg:179.93ms\nstep:248/5000 train_loss:8.0182 train_time:42822ms step_avg:179.92ms\nstep:249/5000 train_loss:8.1047 train_time:43000ms step_avg:179.92ms\nstep:250/5000 train_loss:7.9887 train_time:43180ms step_avg:179.92ms\nstep:251/5000 train_loss:7.9466 train_time:43361ms step_avg:179.92ms\nstep:252/5000 train_loss:7.9494 train_time:43542ms step_avg:179.92ms\nstep:253/5000 train_loss:8.0333 train_time:43722ms step_avg:179.93ms\nstep:254/5000 train_loss:8.5443 train_time:43901ms step_avg:179.92ms\nstep:255/5000 train_loss:8.0154 train_time:44083ms step_avg:179.93ms\nstep:256/5000 train_loss:8.0247 train_time:44262ms step_avg:179.93ms\nstep:257/5000 train_loss:8.1124 train_time:44441ms step_avg:179.92ms\nstep:258/5000 train_loss:7.9306 train_time:44620ms step_avg:179.92ms\nstep:259/5000 train_loss:8.0388 train_time:44800ms step_avg:179.92ms\nstep:260/5000 train_loss:7.8269 train_time:44980ms step_avg:179.92ms\nstep:261/5000 train_loss:8.0351 train_time:45160ms step_avg:179.92ms\nstep:262/5000 train_loss:7.9931 train_time:45340ms step_avg:179.92ms\nstep:263/5000 train_loss:7.9554 train_time:45522ms step_avg:179.93ms\nstep:264/5000 train_loss:7.9580 train_time:45702ms step_avg:179.93ms\nstep:265/5000 train_loss:8.0023 train_time:45882ms step_avg:179.93ms\nstep:266/5000 train_loss:7.9210 train_time:46062ms step_avg:179.93ms\nstep:267/5000 train_loss:7.9491 train_time:46242ms step_avg:179.93ms\nstep:268/5000 train_loss:8.0597 train_time:46423ms step_avg:179.93ms\nstep:269/5000 train_loss:8.0740 train_time:46602ms step_avg:179.93ms\nstep:270/5000 train_loss:8.1651 train_time:46781ms step_avg:179.93ms\nstep:271/5000 train_loss:7.6984 train_time:46961ms step_avg:179.93ms\nstep:272/5000 train_loss:8.0742 train_time:47142ms step_avg:179.93ms\nstep:273/5000 train_loss:8.0888 train_time:47321ms step_avg:179.93ms\nstep:274/5000 train_loss:7.8537 train_time:47502ms step_avg:179.93ms\nstep:275/5000 train_loss:7.8804 train_time:47681ms step_avg:179.93ms\nstep:276/5000 train_loss:8.0449 train_time:47860ms step_avg:179.92ms\nstep:277/5000 train_loss:7.9429 train_time:48040ms step_avg:179.93ms\nstep:278/5000 train_loss:7.7610 train_time:48221ms step_avg:179.93ms\nstep:279/5000 train_loss:7.9962 train_time:48401ms step_avg:179.93ms\nstep:280/5000 train_loss:7.9348 train_time:48580ms step_avg:179.93ms\nstep:281/5000 train_loss:8.0783 train_time:48760ms step_avg:179.93ms\nstep:282/5000 train_loss:8.0787 train_time:48941ms step_avg:179.93ms\nstep:283/5000 train_loss:7.8861 train_time:49121ms step_avg:179.93ms\nstep:284/5000 train_loss:8.0712 train_time:49302ms step_avg:179.93ms\nstep:285/5000 train_loss:8.2041 train_time:49482ms step_avg:179.93ms\nstep:286/5000 train_loss:8.0921 train_time:49663ms step_avg:179.94ms\nstep:287/5000 train_loss:8.1570 train_time:49844ms step_avg:179.94ms\nstep:288/5000 train_loss:8.0450 train_time:50024ms step_avg:179.94ms\nstep:289/5000 train_loss:7.9958 train_time:50204ms step_avg:179.94ms\nstep:290/5000 train_loss:8.0465 train_time:50384ms step_avg:179.94ms\nstep:291/5000 train_loss:8.1118 train_time:50562ms step_avg:179.94ms\nstep:292/5000 train_loss:8.0559 train_time:50742ms step_avg:179.94ms\nstep:293/5000 train_loss:8.0115 train_time:50921ms step_avg:179.93ms\nstep:294/5000 train_loss:8.0582 train_time:51101ms step_avg:179.93ms\nstep:295/5000 train_loss:7.8522 train_time:51282ms step_avg:179.94ms\nstep:296/5000 train_loss:7.8504 train_time:51462ms step_avg:179.94ms\nstep:297/5000 train_loss:8.0156 train_time:51642ms step_avg:179.94ms\nstep:298/5000 train_loss:8.0424 train_time:51822ms step_avg:179.94ms\nstep:299/5000 train_loss:7.9546 train_time:52002ms step_avg:179.94ms\nstep:300/5000 train_loss:7.9285 train_time:52181ms step_avg:179.94ms\nstep:301/5000 train_loss:8.0093 train_time:52361ms step_avg:179.94ms\nstep:302/5000 train_loss:8.0037 train_time:52542ms step_avg:179.94ms\nstep:303/5000 train_loss:7.8843 train_time:52722ms step_avg:179.94ms\nstep:304/5000 train_loss:8.0207 train_time:52901ms step_avg:179.94ms\nstep:305/5000 train_loss:8.0200 train_time:53081ms step_avg:179.94ms\nstep:306/5000 train_loss:8.0563 train_time:53261ms step_avg:179.94ms\nstep:307/5000 train_loss:8.1175 train_time:53443ms step_avg:179.94ms\nstep:308/5000 train_loss:8.1520 train_time:53623ms step_avg:179.94ms\nstep:309/5000 train_loss:8.0921 train_time:53804ms step_avg:179.95ms\nstep:310/5000 train_loss:8.0754 train_time:53984ms step_avg:179.95ms\nstep:311/5000 train_loss:8.0551 train_time:54163ms step_avg:179.94ms\nstep:312/5000 train_loss:8.0261 train_time:54344ms step_avg:179.95ms\nstep:313/5000 train_loss:8.0942 train_time:54524ms step_avg:179.95ms\nstep:314/5000 train_loss:8.1276 train_time:54705ms step_avg:179.95ms\nstep:315/5000 train_loss:7.9238 train_time:54885ms step_avg:179.95ms\nstep:316/5000 train_loss:7.6309 train_time:55064ms step_avg:179.95ms\nstep:317/5000 train_loss:8.0370 train_time:55243ms step_avg:179.94ms\nstep:318/5000 train_loss:8.0849 train_time:55422ms step_avg:179.94ms\nstep:319/5000 train_loss:7.8964 train_time:55603ms step_avg:179.95ms\nstep:320/5000 train_loss:7.9887 train_time:55783ms step_avg:179.95ms\nstep:321/5000 train_loss:8.0113 train_time:55964ms step_avg:179.95ms\nstep:322/5000 train_loss:7.9186 train_time:56145ms step_avg:179.95ms\nstep:323/5000 train_loss:8.1447 train_time:56325ms step_avg:179.95ms\nstep:324/5000 train_loss:8.1020 train_time:56505ms step_avg:179.95ms\nstep:325/5000 train_loss:8.0530 train_time:56685ms step_avg:179.95ms\nstep:326/5000 train_loss:8.0886 train_time:56864ms step_avg:179.95ms\nstep:327/5000 train_loss:8.0837 train_time:57045ms step_avg:179.95ms\nstep:328/5000 train_loss:7.9984 train_time:57226ms step_avg:179.95ms\nstep:329/5000 train_loss:8.1949 train_time:57406ms step_avg:179.95ms\nstep:330/5000 train_loss:8.0956 train_time:57586ms step_avg:179.95ms\nstep:331/5000 train_loss:7.9705 train_time:57764ms step_avg:179.95ms\nstep:332/5000 train_loss:8.1021 train_time:57944ms step_avg:179.95ms\nstep:333/5000 train_loss:8.1728 train_time:58123ms step_avg:179.95ms\nstep:334/5000 train_loss:8.0288 train_time:58304ms step_avg:179.95ms\nstep:335/5000 train_loss:8.0257 train_time:58484ms step_avg:179.95ms\nstep:336/5000 train_loss:8.0745 train_time:58663ms step_avg:179.95ms\nstep:337/5000 train_loss:8.1377 train_time:58843ms step_avg:179.95ms\nstep:338/5000 train_loss:7.9419 train_time:59024ms step_avg:179.95ms\nstep:339/5000 train_loss:7.9577 train_time:59204ms step_avg:179.95ms\nstep:340/5000 train_loss:8.1015 train_time:59384ms step_avg:179.95ms\nstep:341/5000 train_loss:8.0835 train_time:59564ms step_avg:179.95ms\nstep:342/5000 train_loss:8.0580 train_time:59745ms step_avg:179.95ms\nstep:343/5000 train_loss:8.0027 train_time:59925ms step_avg:179.95ms\nstep:344/5000 train_loss:8.0523 train_time:60104ms step_avg:179.95ms\nstep:345/5000 train_loss:8.0090 train_time:60284ms step_avg:179.95ms\nstep:346/5000 train_loss:8.0726 train_time:60464ms step_avg:179.95ms\nstep:347/5000 train_loss:8.0746 train_time:60645ms step_avg:179.95ms\nstep:348/5000 train_loss:8.0917 train_time:60825ms step_avg:179.95ms\nstep:349/5000 train_loss:7.9847 train_time:61005ms step_avg:179.96ms\nstep:350/5000 train_loss:8.1651 train_time:61185ms step_avg:179.96ms\nstep:351/5000 train_loss:7.9611 train_time:61363ms step_avg:179.95ms\nstep:352/5000 train_loss:7.9630 train_time:61546ms step_avg:179.96ms\nstep:353/5000 train_loss:7.8817 train_time:61723ms step_avg:179.95ms\nstep:354/5000 train_loss:8.0548 train_time:61903ms step_avg:179.95ms\nstep:355/5000 train_loss:7.9682 train_time:62084ms step_avg:179.95ms\nstep:356/5000 train_loss:7.9507 train_time:62263ms step_avg:179.95ms\nstep:357/5000 train_loss:7.7854 train_time:62443ms step_avg:179.95ms\nstep:358/5000 train_loss:8.0267 train_time:62624ms step_avg:179.95ms\nstep:359/5000 train_loss:8.0704 train_time:62804ms step_avg:179.95ms\nstep:360/5000 train_loss:7.8218 train_time:62984ms step_avg:179.95ms\nstep:361/5000 train_loss:7.9013 train_time:63163ms step_avg:179.95ms\nstep:362/5000 train_loss:8.0140 train_time:63342ms step_avg:179.95ms\nstep:363/5000 train_loss:8.2278 train_time:63523ms step_avg:179.95ms\nstep:364/5000 train_loss:8.1140 train_time:63701ms step_avg:179.95ms\nstep:365/5000 train_loss:8.2261 train_time:63883ms step_avg:179.95ms\nstep:366/5000 train_loss:8.2434 train_time:64063ms step_avg:179.95ms\nstep:367/5000 train_loss:8.2956 train_time:64243ms step_avg:179.95ms\nstep:368/5000 train_loss:8.0976 train_time:64422ms step_avg:179.95ms\nstep:369/5000 train_loss:8.1902 train_time:64603ms step_avg:179.95ms\nstep:370/5000 train_loss:8.1067 train_time:64783ms step_avg:179.95ms\nstep:371/5000 train_loss:8.0725 train_time:64962ms step_avg:179.95ms\nstep:372/5000 train_loss:8.2894 train_time:65144ms step_avg:179.96ms\nstep:373/5000 train_loss:8.0665 train_time:65325ms step_avg:179.96ms\nstep:374/5000 train_loss:8.0630 train_time:65505ms step_avg:179.96ms\nstep:375/5000 train_loss:8.0711 train_time:65684ms step_avg:179.96ms\nstep:376/5000 train_loss:8.1916 train_time:65864ms step_avg:179.96ms\nstep:377/5000 train_loss:8.0050 train_time:66044ms step_avg:179.96ms\nstep:378/5000 train_loss:8.0898 train_time:66226ms step_avg:179.96ms\nstep:379/5000 train_loss:8.1200 train_time:66406ms step_avg:179.96ms\nstep:380/5000 train_loss:7.9800 train_time:66585ms step_avg:179.96ms\nstep:381/5000 train_loss:8.1642 train_time:66764ms step_avg:179.96ms\nstep:382/5000 train_loss:7.9411 train_time:66943ms step_avg:179.95ms\nstep:383/5000 train_loss:8.0723 train_time:67122ms step_avg:179.95ms\nstep:384/5000 train_loss:8.0409 train_time:67304ms step_avg:179.96ms\nstep:385/5000 train_loss:7.9144 train_time:67483ms step_avg:179.95ms\nstep:386/5000 train_loss:8.0920 train_time:67663ms step_avg:179.96ms\nstep:387/5000 train_loss:8.0474 train_time:67843ms step_avg:179.95ms\nstep:388/5000 train_loss:8.3714 train_time:68022ms step_avg:179.95ms\nstep:389/5000 train_loss:8.3750 train_time:68202ms step_avg:179.95ms\nstep:390/5000 train_loss:8.0750 train_time:68381ms step_avg:179.95ms\nstep:391/5000 train_loss:8.1672 train_time:68561ms step_avg:179.95ms\nstep:392/5000 train_loss:8.0303 train_time:68742ms step_avg:179.95ms\nstep:393/5000 train_loss:8.0958 train_time:68923ms step_avg:179.96ms\nstep:394/5000 train_loss:8.2018 train_time:69103ms step_avg:179.96ms\nstep:395/5000 train_loss:8.0307 train_time:69282ms step_avg:179.95ms\nstep:396/5000 train_loss:8.0731 train_time:69460ms step_avg:179.95ms\nstep:397/5000 train_loss:8.1901 train_time:69641ms step_avg:179.95ms\nstep:398/5000 train_loss:8.1653 train_time:69822ms step_avg:179.95ms\nstep:399/5000 train_loss:8.0894 train_time:70002ms step_avg:179.95ms\nstep:400/5000 train_loss:8.0620 train_time:70183ms step_avg:179.96ms\nstep:401/5000 train_loss:7.9764 train_time:70363ms step_avg:179.96ms\nstep:402/5000 train_loss:8.1199 train_time:70543ms step_avg:179.96ms\nstep:403/5000 train_loss:8.3546 train_time:70723ms step_avg:179.96ms\nstep:404/5000 train_loss:8.0369 train_time:70904ms step_avg:179.96ms\nstep:405/5000 train_loss:8.1503 train_time:71083ms step_avg:179.96ms\nstep:406/5000 train_loss:8.1465 train_time:71265ms step_avg:179.96ms\nstep:407/5000 train_loss:8.1084 train_time:71444ms step_avg:179.96ms\nstep:408/5000 train_loss:8.0538 train_time:71623ms step_avg:179.96ms\nstep:409/5000 train_loss:8.1607 train_time:71803ms step_avg:179.96ms\nstep:410/5000 train_loss:8.1122 train_time:71983ms step_avg:179.96ms\nstep:411/5000 train_loss:8.0768 train_time:72163ms step_avg:179.96ms\nstep:412/5000 train_loss:7.9080 train_time:72343ms step_avg:179.96ms\nstep:413/5000 train_loss:8.0982 train_time:72524ms step_avg:179.96ms\nstep:414/5000 train_loss:7.9920 train_time:72704ms step_avg:179.96ms\nstep:415/5000 train_loss:8.3686 train_time:72884ms step_avg:179.96ms\nstep:416/5000 train_loss:8.0985 train_time:73064ms step_avg:179.96ms\nstep:417/5000 train_loss:7.9249 train_time:73243ms step_avg:179.96ms\nstep:418/5000 train_loss:8.0949 train_time:73423ms step_avg:179.96ms\nstep:419/5000 train_loss:8.1628 train_time:73604ms step_avg:179.96ms\nstep:420/5000 train_loss:8.1461 train_time:73784ms step_avg:179.96ms\nstep:421/5000 train_loss:8.0460 train_time:73964ms step_avg:179.96ms\nstep:422/5000 train_loss:8.3435 train_time:74143ms step_avg:179.96ms\nstep:423/5000 train_loss:8.1831 train_time:74324ms step_avg:179.96ms\nstep:424/5000 train_loss:8.2662 train_time:74505ms step_avg:179.96ms\nstep:425/5000 train_loss:8.1027 train_time:74685ms step_avg:179.96ms\nstep:426/5000 train_loss:8.1376 train_time:74865ms step_avg:179.96ms\nstep:427/5000 train_loss:8.2335 train_time:75045ms step_avg:179.96ms\nstep:428/5000 train_loss:8.1125 train_time:75225ms step_avg:179.96ms\nstep:429/5000 train_loss:8.1273 train_time:75405ms step_avg:179.96ms\nstep:430/5000 train_loss:7.8918 train_time:75584ms step_avg:179.96ms\nstep:431/5000 train_loss:8.0365 train_time:75764ms step_avg:179.96ms\nstep:432/5000 train_loss:8.0451 train_time:75945ms step_avg:179.96ms\nstep:433/5000 train_loss:8.1676 train_time:76125ms step_avg:179.96ms\nstep:434/5000 train_loss:8.1373 train_time:76306ms step_avg:179.97ms\nstep:435/5000 train_loss:8.0570 train_time:76486ms step_avg:179.97ms\nstep:436/5000 train_loss:8.1676 train_time:76665ms step_avg:179.96ms\nstep:437/5000 train_loss:8.1024 train_time:76844ms step_avg:179.96ms\nstep:438/5000 train_loss:8.0979 train_time:77023ms step_avg:179.96ms\nstep:439/5000 train_loss:8.0775 train_time:77204ms step_avg:179.96ms\nstep:440/5000 train_loss:8.1720 train_time:77385ms step_avg:179.96ms\nstep:441/5000 train_loss:8.1551 train_time:77566ms step_avg:179.97ms\nstep:442/5000 train_loss:8.1399 train_time:77746ms step_avg:179.97ms\nstep:443/5000 train_loss:8.0688 train_time:77925ms step_avg:179.96ms\nstep:444/5000 train_loss:8.0754 train_time:78105ms step_avg:179.97ms\nstep:445/5000 train_loss:8.1465 train_time:78285ms step_avg:179.97ms\nstep:446/5000 train_loss:8.1217 train_time:78465ms step_avg:179.97ms\nstep:447/5000 train_loss:8.1589 train_time:78645ms step_avg:179.97ms\nstep:448/5000 train_loss:8.0872 train_time:78824ms step_avg:179.96ms\nstep:449/5000 train_loss:8.1373 train_time:79005ms step_avg:179.97ms\nstep:450/5000 train_loss:8.1278 train_time:79185ms step_avg:179.97ms\nstep:451/5000 train_loss:8.1333 train_time:79364ms step_avg:179.96ms\nstep:452/5000 train_loss:7.9798 train_time:79544ms step_avg:179.96ms\nstep:453/5000 train_loss:8.1943 train_time:79723ms step_avg:179.96ms\nstep:454/5000 train_loss:8.0253 train_time:79904ms step_avg:179.96ms\nstep:455/5000 train_loss:8.1023 train_time:80084ms step_avg:179.96ms\nstep:456/5000 train_loss:8.1458 train_time:80264ms step_avg:179.96ms\nstep:457/5000 train_loss:8.1018 train_time:80444ms step_avg:179.96ms\nstep:458/5000 train_loss:8.1904 train_time:80624ms step_avg:179.96ms\nstep:459/5000 train_loss:8.0907 train_time:80804ms step_avg:179.97ms\nstep:460/5000 train_loss:8.0443 train_time:80985ms step_avg:179.97ms\nstep:461/5000 train_loss:8.0729 train_time:81165ms step_avg:179.97ms\nstep:462/5000 train_loss:7.9945 train_time:81345ms step_avg:179.97ms\nstep:463/5000 train_loss:8.0790 train_time:81526ms step_avg:179.97ms\nstep:464/5000 train_loss:8.1626 train_time:81707ms step_avg:179.97ms\nstep:465/5000 train_loss:8.1380 train_time:81887ms step_avg:179.97ms\nstep:466/5000 train_loss:8.0472 train_time:82066ms step_avg:179.97ms\nstep:467/5000 train_loss:8.1385 train_time:82245ms step_avg:179.97ms\nstep:468/5000 train_loss:8.0900 train_time:82425ms step_avg:179.97ms\nstep:469/5000 train_loss:8.0504 train_time:82606ms step_avg:179.97ms\nstep:470/5000 train_loss:8.0275 train_time:82786ms step_avg:179.97ms\nstep:471/5000 train_loss:8.1268 train_time:82966ms step_avg:179.97ms\nstep:472/5000 train_loss:8.0917 train_time:83145ms step_avg:179.97ms\nstep:473/5000 train_loss:8.0510 train_time:83325ms step_avg:179.97ms\nstep:474/5000 train_loss:8.2128 train_time:83505ms step_avg:179.97ms\nstep:475/5000 train_loss:8.1585 train_time:83685ms step_avg:179.97ms\nstep:476/5000 train_loss:8.1772 train_time:83865ms step_avg:179.97ms\nstep:477/5000 train_loss:8.1840 train_time:84045ms step_avg:179.97ms\nstep:478/5000 train_loss:8.1363 train_time:84225ms step_avg:179.97ms\nstep:479/5000 train_loss:8.1301 train_time:84406ms step_avg:179.97ms\nstep:480/5000 train_loss:8.1477 train_time:84586ms step_avg:179.97ms\nstep:481/5000 train_loss:8.0596 train_time:84766ms step_avg:179.97ms\nstep:482/5000 train_loss:8.0765 train_time:84945ms step_avg:179.97ms\nstep:483/5000 train_loss:8.1098 train_time:85125ms step_avg:179.97ms\nstep:484/5000 train_loss:8.1495 train_time:85306ms step_avg:179.97ms\nstep:485/5000 train_loss:8.1253 train_time:85486ms step_avg:179.97ms\nstep:486/5000 train_loss:8.0875 train_time:85667ms step_avg:179.97ms\nstep:487/5000 train_loss:8.0872 train_time:85846ms step_avg:179.97ms\nstep:488/5000 train_loss:8.0945 train_time:86025ms step_avg:179.97ms\nstep:489/5000 train_loss:7.9963 train_time:86205ms step_avg:179.97ms\nstep:490/5000 train_loss:7.9833 train_time:86385ms step_avg:179.97ms\nstep:491/5000 train_loss:8.0838 train_time:86564ms step_avg:179.97ms\nstep:492/5000 train_loss:8.1695 train_time:86744ms step_avg:179.97ms\nstep:493/5000 train_loss:8.1336 train_time:86924ms step_avg:179.97ms\nstep:494/5000 train_loss:8.2372 train_time:87106ms step_avg:179.97ms\nstep:495/5000 train_loss:8.0503 train_time:87286ms step_avg:179.97ms\nstep:496/5000 train_loss:8.1459 train_time:87466ms step_avg:179.97ms\nstep:497/5000 train_loss:8.1616 train_time:87646ms step_avg:179.97ms\nstep:498/5000 train_loss:8.0905 train_time:87825ms step_avg:179.97ms\nstep:499/5000 train_loss:8.1214 train_time:88005ms step_avg:179.97ms\nstep:500/5000 train_loss:8.0056 train_time:88186ms step_avg:179.97ms\nstep:501/5000 train_loss:8.0486 train_time:88366ms step_avg:179.97ms\nstep:502/5000 train_loss:8.1494 train_time:88545ms step_avg:179.97ms\nstep:503/5000 train_loss:8.0353 train_time:88725ms step_avg:179.97ms\nstep:504/5000 train_loss:8.0388 train_time:88905ms step_avg:179.97ms\nstep:505/5000 train_loss:8.1560 train_time:89085ms step_avg:179.97ms\nstep:506/5000 train_loss:8.1792 train_time:89265ms step_avg:179.97ms\nstep:507/5000 train_loss:8.2266 train_time:89447ms step_avg:179.97ms\nstep:508/5000 train_loss:8.0992 train_time:89627ms step_avg:179.97ms\nstep:509/5000 train_loss:8.2300 train_time:89807ms step_avg:179.97ms\nstep:510/5000 train_loss:8.0903 train_time:89986ms step_avg:179.97ms\nstep:511/5000 train_loss:8.1175 train_time:90166ms step_avg:179.97ms\nstep:512/5000 train_loss:8.1693 train_time:90347ms step_avg:179.97ms\nstep:513/5000 train_loss:8.1285 train_time:90526ms step_avg:179.97ms\nstep:514/5000 train_loss:8.1969 train_time:90707ms step_avg:179.97ms\nstep:515/5000 train_loss:8.0695 train_time:90887ms step_avg:179.97ms\nstep:516/5000 train_loss:8.1404 train_time:91067ms step_avg:179.97ms\nstep:517/5000 train_loss:8.0510 train_time:91247ms step_avg:179.97ms\nstep:518/5000 train_loss:8.0927 train_time:91426ms step_avg:179.97ms\nstep:519/5000 train_loss:8.0190 train_time:91606ms step_avg:179.97ms\nstep:520/5000 train_loss:8.1325 train_time:91787ms step_avg:179.97ms\nstep:521/5000 train_loss:8.0486 train_time:91967ms step_avg:179.97ms\nstep:522/5000 train_loss:8.2300 train_time:92146ms step_avg:179.97ms\nstep:523/5000 train_loss:8.1391 train_time:92326ms step_avg:179.97ms\nstep:524/5000 train_loss:8.2896 train_time:92507ms step_avg:179.98ms\nstep:525/5000 train_loss:8.4236 train_time:92687ms step_avg:179.98ms\nstep:526/5000 train_loss:8.1455 train_time:92867ms step_avg:179.98ms\nstep:527/5000 train_loss:8.1165 train_time:93047ms step_avg:179.98ms\nstep:528/5000 train_loss:8.0647 train_time:93227ms step_avg:179.97ms\nstep:529/5000 train_loss:8.1084 train_time:93407ms step_avg:179.98ms\nstep:530/5000 train_loss:8.1020 train_time:93586ms step_avg:179.97ms\nstep:531/5000 train_loss:8.1243 train_time:93767ms step_avg:179.97ms\nstep:532/5000 train_loss:8.0753 train_time:93947ms step_avg:179.98ms\nstep:533/5000 train_loss:8.0880 train_time:94126ms step_avg:179.97ms\nstep:534/5000 train_loss:8.1001 train_time:94307ms step_avg:179.98ms\nstep:535/5000 train_loss:8.1069 train_time:94488ms step_avg:179.98ms\nstep:536/5000 train_loss:8.1104 train_time:94667ms step_avg:179.98ms\nstep:537/5000 train_loss:8.2075 train_time:94847ms step_avg:179.98ms\nstep:538/5000 train_loss:8.1234 train_time:95026ms step_avg:179.97ms\nstep:539/5000 train_loss:8.1254 train_time:95208ms step_avg:179.98ms\nstep:540/5000 train_loss:8.1072 train_time:95388ms step_avg:179.98ms\nstep:541/5000 train_loss:8.0614 train_time:95570ms step_avg:179.98ms\nstep:542/5000 train_loss:8.0802 train_time:95749ms step_avg:179.98ms\nstep:543/5000 train_loss:8.1418 train_time:95929ms step_avg:179.98ms\nstep:544/5000 train_loss:8.0613 train_time:96110ms step_avg:179.98ms\nstep:545/5000 train_loss:8.0723 train_time:96291ms step_avg:179.98ms\nstep:546/5000 train_loss:8.0826 train_time:96473ms step_avg:179.99ms\nstep:547/5000 train_loss:8.0394 train_time:96655ms step_avg:179.99ms\nstep:548/5000 train_loss:8.1058 train_time:96836ms step_avg:179.99ms\nstep:549/5000 train_loss:8.0716 train_time:97018ms step_avg:180.00ms\nstep:550/5000 train_loss:8.0961 train_time:97201ms step_avg:180.00ms\nstep:551/5000 train_loss:8.1366 train_time:97381ms step_avg:180.00ms\nstep:552/5000 train_loss:8.0863 train_time:97561ms step_avg:180.00ms\nstep:553/5000 train_loss:8.0461 train_time:97743ms step_avg:180.00ms\nstep:554/5000 train_loss:8.1549 train_time:97924ms step_avg:180.01ms\nstep:555/5000 train_loss:8.0484 train_time:98108ms step_avg:180.02ms\nstep:556/5000 train_loss:8.0652 train_time:98287ms step_avg:180.01ms\nstep:557/5000 train_loss:8.0472 train_time:98467ms step_avg:180.01ms\nstep:558/5000 train_loss:8.0233 train_time:98647ms step_avg:180.01ms\nstep:559/5000 train_loss:8.0871 train_time:98826ms step_avg:180.01ms\nstep:560/5000 train_loss:8.0351 train_time:99006ms step_avg:180.01ms\nstep:561/5000 train_loss:8.1943 train_time:99186ms step_avg:180.01ms\nstep:562/5000 train_loss:8.0339 train_time:99365ms step_avg:180.01ms\nstep:563/5000 train_loss:8.0079 train_time:99546ms step_avg:180.01ms\nstep:564/5000 train_loss:8.0614 train_time:99727ms step_avg:180.01ms\nstep:565/5000 train_loss:8.2072 train_time:99908ms step_avg:180.01ms\nstep:566/5000 train_loss:8.1597 train_time:100089ms step_avg:180.02ms\nstep:567/5000 train_loss:8.1980 train_time:100270ms step_avg:180.02ms\nstep:568/5000 train_loss:8.0183 train_time:100453ms step_avg:180.02ms\nstep:569/5000 train_loss:8.2228 train_time:100636ms step_avg:180.03ms\nstep:570/5000 train_loss:8.0075 train_time:100817ms step_avg:180.03ms\nstep:571/5000 train_loss:8.1198 train_time:101001ms step_avg:180.04ms\nstep:572/5000 train_loss:8.1969 train_time:101181ms step_avg:180.04ms\nstep:573/5000 train_loss:8.2059 train_time:101361ms step_avg:180.04ms\nstep:574/5000 train_loss:8.1099 train_time:101542ms step_avg:180.04ms\nstep:575/5000 train_loss:8.0808 train_time:101723ms step_avg:180.04ms\nstep:576/5000 train_loss:8.4105 train_time:101905ms step_avg:180.04ms\nstep:577/5000 train_loss:8.1076 train_time:102084ms step_avg:180.04ms\nstep:578/5000 train_loss:8.0937 train_time:102264ms step_avg:180.04ms\nstep:579/5000 train_loss:8.1366 train_time:102443ms step_avg:180.04ms\nstep:580/5000 train_loss:8.1056 train_time:102624ms step_avg:180.04ms\nstep:581/5000 train_loss:8.0928 train_time:102805ms step_avg:180.04ms\nstep:582/5000 train_loss:8.1136 train_time:102986ms step_avg:180.04ms\nstep:583/5000 train_loss:8.1040 train_time:103165ms step_avg:180.04ms\nstep:584/5000 train_loss:8.1612 train_time:103346ms step_avg:180.04ms\nstep:585/5000 train_loss:8.0259 train_time:103525ms step_avg:180.04ms\nstep:586/5000 train_loss:8.1012 train_time:103705ms step_avg:180.04ms\nstep:587/5000 train_loss:8.0913 train_time:103885ms step_avg:180.04ms\nstep:588/5000 train_loss:8.1306 train_time:104065ms step_avg:180.04ms\nstep:589/5000 train_loss:8.0609 train_time:104246ms step_avg:180.04ms\nstep:590/5000 train_loss:8.0686 train_time:104426ms step_avg:180.05ms\nstep:591/5000 train_loss:8.0968 train_time:104607ms step_avg:180.05ms\nstep:592/5000 train_loss:7.9794 train_time:104786ms step_avg:180.05ms\nstep:593/5000 train_loss:8.1620 train_time:104966ms step_avg:180.05ms\nstep:594/5000 train_loss:8.1928 train_time:105147ms step_avg:180.05ms\nstep:595/5000 train_loss:8.0695 train_time:105328ms step_avg:180.05ms\nstep:596/5000 train_loss:8.2405 train_time:105510ms step_avg:180.05ms\nstep:597/5000 train_loss:8.1059 train_time:105691ms step_avg:180.05ms\nstep:598/5000 train_loss:8.1864 train_time:105873ms step_avg:180.06ms\nstep:599/5000 train_loss:8.1542 train_time:106054ms step_avg:180.06ms\nstep:600/5000 train_loss:8.2608 train_time:106236ms step_avg:180.06ms\nstep:601/5000 train_loss:8.1316 train_time:106418ms step_avg:180.07ms\nstep:602/5000 train_loss:8.1200 train_time:106602ms step_avg:180.07ms\nstep:603/5000 train_loss:8.1721 train_time:106783ms step_avg:180.07ms\nstep:604/5000 train_loss:8.0783 train_time:106964ms step_avg:180.07ms\nstep:605/5000 train_loss:8.0650 train_time:107145ms step_avg:180.07ms\nstep:606/5000 train_loss:8.1298 train_time:107325ms step_avg:180.08ms\nstep:607/5000 train_loss:8.1571 train_time:107507ms step_avg:180.08ms\nstep:608/5000 train_loss:8.1873 train_time:107688ms step_avg:180.08ms\nstep:609/5000 train_loss:8.0556 train_time:107868ms step_avg:180.08ms\nstep:610/5000 train_loss:8.0798 train_time:108048ms step_avg:180.08ms\nstep:611/5000 train_loss:8.2458 train_time:108228ms step_avg:180.08ms\nstep:612/5000 train_loss:8.0883 train_time:108410ms step_avg:180.08ms\nstep:613/5000 train_loss:7.9393 train_time:108591ms step_avg:180.08ms\nstep:614/5000 train_loss:8.1033 train_time:108771ms step_avg:180.08ms\nstep:615/5000 train_loss:8.0977 train_time:108952ms step_avg:180.09ms\nstep:616/5000 train_loss:8.1030 train_time:109134ms step_avg:180.09ms\nstep:617/5000 train_loss:8.0960 train_time:109317ms step_avg:180.09ms\nstep:618/5000 train_loss:8.1687 train_time:109500ms step_avg:180.10ms\nstep:619/5000 train_loss:8.1855 train_time:109681ms step_avg:180.10ms\nstep:620/5000 train_loss:8.1076 train_time:109862ms step_avg:180.10ms\nstep:621/5000 train_loss:8.1507 train_time:110043ms step_avg:180.10ms\nstep:622/5000 train_loss:8.1254 train_time:110224ms step_avg:180.10ms\nstep:623/5000 train_loss:8.1079 train_time:110404ms step_avg:180.11ms\nstep:624/5000 train_loss:8.1577 train_time:110585ms step_avg:180.11ms\nstep:625/5000 train_loss:8.1763 train_time:110765ms step_avg:180.11ms\nstep:626/5000 train_loss:8.2284 train_time:110946ms step_avg:180.11ms\nstep:627/5000 train_loss:8.0528 train_time:111126ms step_avg:180.11ms\nstep:628/5000 train_loss:8.1021 train_time:111308ms step_avg:180.11ms\nstep:629/5000 train_loss:8.1334 train_time:111489ms step_avg:180.11ms\nstep:630/5000 train_loss:8.0592 train_time:111670ms step_avg:180.11ms\nstep:631/5000 train_loss:8.0598 train_time:111850ms step_avg:180.11ms\nstep:632/5000 train_loss:8.1703 train_time:112031ms step_avg:180.11ms\nstep:633/5000 train_loss:8.0651 train_time:112213ms step_avg:180.12ms\nstep:634/5000 train_loss:8.1334 train_time:112396ms step_avg:180.12ms\nstep:635/5000 train_loss:8.1708 train_time:112579ms step_avg:180.13ms\nstep:636/5000 train_loss:8.0188 train_time:112761ms step_avg:180.13ms\nstep:637/5000 train_loss:8.0061 train_time:112941ms step_avg:180.13ms\nstep:638/5000 train_loss:8.0839 train_time:113123ms step_avg:180.13ms\nstep:639/5000 train_loss:8.2184 train_time:113304ms step_avg:180.13ms\nstep:640/5000 train_loss:8.0838 train_time:113485ms step_avg:180.13ms\nstep:641/5000 train_loss:8.1231 train_time:113665ms step_avg:180.14ms\nstep:642/5000 train_loss:8.0785 train_time:113845ms step_avg:180.13ms\nstep:643/5000 train_loss:8.0960 train_time:114025ms step_avg:180.14ms\nstep:644/5000 train_loss:8.1552 train_time:114207ms step_avg:180.14ms\nstep:645/5000 train_loss:8.0227 train_time:114389ms step_avg:180.14ms\nstep:646/5000 train_loss:8.1442 train_time:114570ms step_avg:180.14ms\nstep:647/5000 train_loss:8.1717 train_time:114750ms step_avg:180.14ms\nstep:648/5000 train_loss:8.1481 train_time:114932ms step_avg:180.14ms\nstep:649/5000 train_loss:8.0731 train_time:115113ms step_avg:180.15ms\nstep:650/5000 train_loss:8.1608 train_time:115295ms step_avg:180.15ms\nstep:651/5000 train_loss:8.2974 train_time:115477ms step_avg:180.15ms\nstep:652/5000 train_loss:8.0790 train_time:115660ms step_avg:180.16ms\nstep:653/5000 train_loss:8.0830 train_time:115841ms step_avg:180.16ms\nstep:654/5000 train_loss:8.0504 train_time:116022ms step_avg:180.16ms\nstep:655/5000 train_loss:8.0814 train_time:116205ms step_avg:180.16ms\nstep:656/5000 train_loss:8.0322 train_time:116386ms step_avg:180.16ms\nstep:657/5000 train_loss:8.0240 train_time:116566ms step_avg:180.16ms\nstep:658/5000 train_loss:8.1356 train_time:116746ms step_avg:180.16ms\nstep:659/5000 train_loss:8.1431 train_time:116926ms step_avg:180.16ms\nstep:660/5000 train_loss:8.1337 train_time:117107ms step_avg:180.17ms\nstep:661/5000 train_loss:8.0079 train_time:117288ms step_avg:180.17ms\nstep:662/5000 train_loss:8.2282 train_time:117467ms step_avg:180.16ms\nstep:663/5000 train_loss:8.0904 train_time:117647ms step_avg:180.16ms\nstep:664/5000 train_loss:8.1756 train_time:117828ms step_avg:180.16ms\nstep:665/5000 train_loss:8.1588 train_time:118008ms step_avg:180.16ms\nstep:666/5000 train_loss:8.1206 train_time:118188ms step_avg:180.17ms\nstep:667/5000 train_loss:8.1524 train_time:118369ms step_avg:180.17ms\nstep:668/5000 train_loss:7.9586 train_time:118549ms step_avg:180.17ms\nstep:669/5000 train_loss:7.9911 train_time:118730ms step_avg:180.17ms\nstep:670/5000 train_loss:8.0927 train_time:118913ms step_avg:180.17ms\nstep:671/5000 train_loss:8.1455 train_time:119095ms step_avg:180.17ms\nstep:672/5000 train_loss:8.1316 train_time:119276ms step_avg:180.18ms\nstep:673/5000 train_loss:8.1002 train_time:119459ms step_avg:180.18ms\nstep:674/5000 train_loss:8.0729 train_time:119639ms step_avg:180.18ms\nstep:675/5000 train_loss:8.0387 train_time:119822ms step_avg:180.18ms\nstep:676/5000 train_loss:8.1006 train_time:120003ms step_avg:180.18ms\nstep:677/5000 train_loss:8.1445 train_time:120184ms step_avg:180.19ms\nstep:678/5000 train_loss:8.1571 train_time:120365ms step_avg:180.19ms\nstep:679/5000 train_loss:8.1065 train_time:120545ms step_avg:180.19ms\nstep:680/5000 train_loss:8.1278 train_time:120725ms step_avg:180.19ms\nstep:681/5000 train_loss:8.1373 train_time:120907ms step_avg:180.19ms\nstep:682/5000 train_loss:8.1061 train_time:121089ms step_avg:180.19ms\nstep:683/5000 train_loss:8.2531 train_time:121270ms step_avg:180.19ms\nstep:684/5000 train_loss:7.6381 train_time:121453ms step_avg:180.20ms\nstep:685/5000 train_loss:8.0449 train_time:121635ms step_avg:180.20ms\nstep:686/5000 train_loss:8.1309 train_time:121818ms step_avg:180.20ms\nstep:687/5000 train_loss:8.2215 train_time:122002ms step_avg:180.21ms\nstep:688/5000 train_loss:8.1284 train_time:122183ms step_avg:180.21ms\nstep:689/5000 train_loss:8.1720 train_time:122363ms step_avg:180.21ms\nstep:690/5000 train_loss:8.1466 train_time:122544ms step_avg:180.21ms\nstep:691/5000 train_loss:8.1447 train_time:122725ms step_avg:180.21ms\nstep:692/5000 train_loss:8.0489 train_time:122906ms step_avg:180.21ms\nstep:693/5000 train_loss:8.0440 train_time:123086ms step_avg:180.21ms\nstep:694/5000 train_loss:8.1707 train_time:123266ms step_avg:180.21ms\nstep:695/5000 train_loss:8.0061 train_time:123447ms step_avg:180.21ms\nstep:696/5000 train_loss:8.1527 train_time:123626ms step_avg:180.21ms\nstep:697/5000 train_loss:8.1717 train_time:123807ms step_avg:180.21ms\nstep:698/5000 train_loss:8.1202 train_time:123988ms step_avg:180.21ms\nstep:699/5000 train_loss:8.1068 train_time:124170ms step_avg:180.22ms\nstep:700/5000 train_loss:8.2243 train_time:124354ms step_avg:180.22ms\nstep:701/5000 train_loss:8.2875 train_time:124534ms step_avg:180.22ms\nstep:702/5000 train_loss:8.0760 train_time:124718ms step_avg:180.23ms\nstep:703/5000 train_loss:8.1948 train_time:124899ms step_avg:180.23ms\nstep:704/5000 train_loss:8.1321 train_time:125082ms step_avg:180.23ms\nstep:705/5000 train_loss:8.1049 train_time:125263ms step_avg:180.23ms\nstep:706/5000 train_loss:8.0667 train_time:125444ms step_avg:180.24ms\nstep:707/5000 train_loss:8.0262 train_time:125625ms step_avg:180.24ms\nstep:708/5000 train_loss:8.2642 train_time:125806ms step_avg:180.24ms\nstep:709/5000 train_loss:8.0929 train_time:125987ms step_avg:180.24ms\nstep:710/5000 train_loss:8.0417 train_time:126167ms step_avg:180.24ms\nstep:711/5000 train_loss:7.9597 train_time:126347ms step_avg:180.24ms\nstep:712/5000 train_loss:8.0410 train_time:126526ms step_avg:180.24ms\nstep:713/5000 train_loss:8.3051 train_time:126708ms step_avg:180.24ms\nstep:714/5000 train_loss:8.1481 train_time:126889ms step_avg:180.24ms\nstep:715/5000 train_loss:8.1789 train_time:127070ms step_avg:180.24ms\nstep:716/5000 train_loss:8.0919 train_time:127253ms step_avg:180.24ms\nstep:717/5000 train_loss:8.0870 train_time:127436ms step_avg:180.25ms\nstep:718/5000 train_loss:8.1559 train_time:127617ms step_avg:180.25ms\nstep:719/5000 train_loss:8.2033 train_time:127799ms step_avg:180.25ms\nstep:720/5000 train_loss:8.1402 train_time:127982ms step_avg:180.26ms\nstep:721/5000 train_loss:8.1612 train_time:128162ms step_avg:180.26ms\nstep:722/5000 train_loss:8.1313 train_time:128347ms step_avg:180.26ms\nstep:723/5000 train_loss:8.1036 train_time:128526ms step_avg:180.26ms\nstep:724/5000 train_loss:8.2970 train_time:128707ms step_avg:180.26ms\nstep:725/5000 train_loss:8.0070 train_time:128887ms step_avg:180.26ms\nstep:726/5000 train_loss:8.1287 train_time:129068ms step_avg:180.26ms\nstep:727/5000 train_loss:8.1045 train_time:129250ms step_avg:180.26ms\nstep:728/5000 train_loss:7.9894 train_time:129429ms step_avg:180.26ms\nstep:729/5000 train_loss:8.1650 train_time:129612ms step_avg:180.27ms\nstep:730/5000 train_loss:8.0523 train_time:129796ms step_avg:180.27ms\nstep:731/5000 train_loss:8.0322 train_time:129979ms step_avg:180.28ms\nstep:732/5000 train_loss:8.1993 train_time:130160ms step_avg:180.28ms\nstep:733/5000 train_loss:8.0862 train_time:130342ms step_avg:180.28ms\nstep:734/5000 train_loss:8.0658 train_time:130525ms step_avg:180.28ms\nstep:735/5000 train_loss:8.1035 train_time:130706ms step_avg:180.28ms\nstep:736/5000 train_loss:8.2652 train_time:130886ms step_avg:180.28ms\nstep:737/5000 train_loss:8.0640 train_time:131067ms step_avg:180.28ms\nstep:738/5000 train_loss:8.1458 train_time:131248ms step_avg:180.29ms\nstep:739/5000 train_loss:8.0368 train_time:131428ms step_avg:180.28ms\nstep:740/5000 train_loss:8.1228 train_time:131609ms step_avg:180.29ms\nstep:741/5000 train_loss:8.0792 train_time:131790ms step_avg:180.29ms\nstep:742/5000 train_loss:8.0434 train_time:131971ms step_avg:180.29ms\nstep:743/5000 train_loss:8.0828 train_time:132154ms step_avg:180.29ms\nstep:744/5000 train_loss:8.0647 train_time:132335ms step_avg:180.29ms\nstep:745/5000 train_loss:8.0289 train_time:132517ms step_avg:180.30ms\nstep:746/5000 train_loss:8.3568 train_time:132698ms step_avg:180.30ms\nstep:747/5000 train_loss:8.0974 train_time:132879ms step_avg:180.30ms\nstep:748/5000 train_loss:8.1585 train_time:133061ms step_avg:180.30ms\nstep:749/5000 train_loss:8.2035 train_time:133243ms step_avg:180.30ms\nstep:750/5000 train_loss:8.0994 train_time:133424ms step_avg:180.30ms\nstep:751/5000 train_loss:8.0765 train_time:133606ms step_avg:180.31ms\nstep:752/5000 train_loss:8.1965 train_time:133787ms step_avg:180.31ms\nstep:753/5000 train_loss:8.1798 train_time:133967ms step_avg:180.31ms\nstep:754/5000 train_loss:8.0847 train_time:134147ms step_avg:180.31ms\nstep:755/5000 train_loss:8.1775 train_time:134327ms step_avg:180.31ms\nstep:756/5000 train_loss:7.8782 train_time:134510ms step_avg:180.31ms\nstep:757/5000 train_loss:7.7909 train_time:134693ms step_avg:180.31ms\nstep:758/5000 train_loss:8.2375 train_time:134875ms step_avg:180.31ms\nstep:759/5000 train_loss:8.1490 train_time:135057ms step_avg:180.32ms\nstep:760/5000 train_loss:8.1028 train_time:135238ms step_avg:180.32ms\nstep:761/5000 train_loss:8.0408 train_time:135421ms step_avg:180.32ms\nstep:762/5000 train_loss:8.2025 train_time:135842ms step_avg:180.64ms\nstep:763/5000 train_loss:8.1434 train_time:136022ms step_avg:180.64ms\nstep:764/5000 train_loss:8.0585 train_time:136202ms step_avg:180.64ms\nstep:765/5000 train_loss:8.0540 train_time:136381ms step_avg:180.64ms\nstep:766/5000 train_loss:8.2195 train_time:136561ms step_avg:180.64ms\nstep:767/5000 train_loss:8.1239 train_time:136746ms step_avg:180.64ms\nstep:768/5000 train_loss:8.0754 train_time:136928ms step_avg:180.64ms\nstep:769/5000 train_loss:8.1311 train_time:137109ms step_avg:180.64ms\nstep:770/5000 train_loss:8.0866 train_time:137290ms step_avg:180.64ms\nstep:771/5000 train_loss:8.1662 train_time:137470ms step_avg:180.64ms\nstep:772/5000 train_loss:8.0662 train_time:137651ms step_avg:180.64ms\nstep:773/5000 train_loss:7.8196 train_time:137832ms step_avg:180.65ms\nstep:774/5000 train_loss:8.0926 train_time:138016ms step_avg:180.65ms\nstep:775/5000 train_loss:8.1850 train_time:138197ms step_avg:180.65ms\nstep:776/5000 train_loss:8.1208 train_time:138379ms step_avg:180.65ms\nstep:777/5000 train_loss:8.1792 train_time:138561ms step_avg:180.65ms\nstep:778/5000 train_loss:8.1776 train_time:138742ms step_avg:180.65ms\nstep:779/5000 train_loss:8.1180 train_time:138924ms step_avg:180.66ms\nstep:780/5000 train_loss:8.2867 train_time:139106ms step_avg:180.66ms\nstep:781/5000 train_loss:8.0920 train_time:139287ms step_avg:180.66ms\nstep:782/5000 train_loss:8.0537 train_time:139467ms step_avg:180.66ms\nstep:783/5000 train_loss:8.1000 train_time:139647ms step_avg:180.66ms\nstep:784/5000 train_loss:8.1977 train_time:139827ms step_avg:180.65ms\nstep:785/5000 train_loss:8.0345 train_time:140007ms step_avg:180.65ms\nstep:786/5000 train_loss:8.1929 train_time:140187ms step_avg:180.65ms\nstep:787/5000 train_loss:8.1881 train_time:140368ms step_avg:180.65ms\nstep:788/5000 train_loss:8.1105 train_time:140549ms step_avg:180.65ms\nstep:789/5000 train_loss:8.0705 train_time:140730ms step_avg:180.65ms\nstep:790/5000 train_loss:8.1012 train_time:140912ms step_avg:180.66ms\nstep:791/5000 train_loss:8.2225 train_time:141093ms step_avg:180.66ms\nstep:792/5000 train_loss:8.0983 train_time:141276ms step_avg:180.66ms\nstep:793/5000 train_loss:8.0081 train_time:141457ms step_avg:180.66ms\nstep:794/5000 train_loss:8.1983 train_time:141640ms step_avg:180.66ms\nstep:795/5000 train_loss:8.1017 train_time:141822ms step_avg:180.66ms\nstep:796/5000 train_loss:8.0708 train_time:142003ms step_avg:180.67ms\nstep:797/5000 train_loss:8.0864 train_time:142184ms step_avg:180.67ms\nstep:798/5000 train_loss:8.1322 train_time:142365ms step_avg:180.67ms\nstep:799/5000 train_loss:8.2723 train_time:142548ms step_avg:180.67ms\nstep:800/5000 train_loss:8.1442 train_time:142728ms step_avg:180.67ms\nstep:801/5000 train_loss:8.0188 train_time:142911ms step_avg:180.67ms\nstep:802/5000 train_loss:8.3424 train_time:143092ms step_avg:180.67ms\nstep:803/5000 train_loss:8.2375 train_time:143274ms step_avg:180.67ms\nstep:804/5000 train_loss:8.1330 train_time:143457ms step_avg:180.68ms\nstep:805/5000 train_loss:8.2068 train_time:143641ms step_avg:180.68ms\nstep:806/5000 train_loss:8.1780 train_time:143822ms step_avg:180.68ms\nstep:807/5000 train_loss:8.1205 train_time:144003ms step_avg:180.68ms\nstep:808/5000 train_loss:8.1676 train_time:144184ms step_avg:180.68ms\nstep:809/5000 train_loss:8.1894 train_time:144365ms step_avg:180.68ms\nstep:810/5000 train_loss:8.1086 train_time:144547ms step_avg:180.68ms\nstep:811/5000 train_loss:8.1269 train_time:144728ms step_avg:180.68ms\nstep:812/5000 train_loss:8.1887 train_time:144910ms step_avg:180.69ms\nstep:813/5000 train_loss:8.2157 train_time:145090ms step_avg:180.69ms\nstep:814/5000 train_loss:8.0873 train_time:145272ms step_avg:180.69ms\nstep:815/5000 train_loss:8.0922 train_time:145454ms step_avg:180.69ms\nstep:816/5000 train_loss:8.0652 train_time:145636ms step_avg:180.69ms\nstep:817/5000 train_loss:8.0725 train_time:145820ms step_avg:180.69ms\nstep:818/5000 train_loss:8.0825 train_time:146002ms step_avg:180.70ms\nstep:819/5000 train_loss:8.1626 train_time:146183ms step_avg:180.70ms\nstep:820/5000 train_loss:8.2841 train_time:146363ms step_avg:180.70ms\nstep:821/5000 train_loss:8.1851 train_time:146545ms step_avg:180.70ms\nstep:822/5000 train_loss:8.1021 train_time:146726ms step_avg:180.70ms\nstep:823/5000 train_loss:8.1163 train_time:146908ms step_avg:180.70ms\nstep:824/5000 train_loss:8.0598 train_time:147089ms step_avg:180.70ms\nstep:825/5000 train_loss:8.1321 train_time:147272ms step_avg:180.70ms\nstep:826/5000 train_loss:8.2429 train_time:147453ms step_avg:180.70ms\nstep:827/5000 train_loss:8.2006 train_time:147635ms step_avg:180.70ms\nstep:828/5000 train_loss:8.1291 train_time:147819ms step_avg:180.71ms\nstep:829/5000 train_loss:8.0305 train_time:148001ms step_avg:180.71ms\nstep:830/5000 train_loss:8.0664 train_time:148181ms step_avg:180.71ms\nstep:831/5000 train_loss:8.0413 train_time:148363ms step_avg:180.71ms\nstep:832/5000 train_loss:8.0933 train_time:148545ms step_avg:180.71ms\nstep:833/5000 train_loss:8.0839 train_time:148725ms step_avg:180.71ms\nstep:834/5000 train_loss:8.0995 train_time:148907ms step_avg:180.71ms\nstep:835/5000 train_loss:7.9519 train_time:149089ms step_avg:180.71ms\nstep:836/5000 train_loss:8.2122 train_time:149270ms step_avg:180.71ms\nstep:837/5000 train_loss:8.0934 train_time:149451ms step_avg:180.71ms\nstep:838/5000 train_loss:8.1004 train_time:149632ms step_avg:180.71ms\nstep:839/5000 train_loss:8.1949 train_time:149815ms step_avg:180.72ms\nstep:840/5000 train_loss:7.9488 train_time:149996ms step_avg:180.72ms\nstep:841/5000 train_loss:8.1435 train_time:150178ms step_avg:180.72ms\nstep:842/5000 train_loss:8.0385 train_time:150360ms step_avg:180.72ms\nstep:843/5000 train_loss:8.1283 train_time:150542ms step_avg:180.72ms\nstep:844/5000 train_loss:8.0716 train_time:150723ms step_avg:180.72ms\nstep:845/5000 train_loss:8.1587 train_time:150905ms step_avg:180.72ms\nstep:846/5000 train_loss:8.0567 train_time:151085ms step_avg:180.72ms\nstep:847/5000 train_loss:8.1840 train_time:151266ms step_avg:180.72ms\nstep:848/5000 train_loss:8.3989 train_time:151446ms step_avg:180.72ms\nstep:849/5000 train_loss:8.0692 train_time:151627ms step_avg:180.72ms\nstep:850/5000 train_loss:8.0685 train_time:151809ms step_avg:180.73ms\nstep:851/5000 train_loss:8.3037 train_time:151991ms step_avg:180.73ms\nstep:852/5000 train_loss:8.0421 train_time:152173ms step_avg:180.73ms\nstep:853/5000 train_loss:8.1207 train_time:152355ms step_avg:180.73ms\nstep:854/5000 train_loss:8.1306 train_time:152537ms step_avg:180.73ms\nstep:855/5000 train_loss:8.1819 train_time:152720ms step_avg:180.73ms\nstep:856/5000 train_loss:8.1423 train_time:152903ms step_avg:180.74ms\nstep:857/5000 train_loss:8.0075 train_time:153083ms step_avg:180.74ms\nstep:858/5000 train_loss:8.1714 train_time:153265ms step_avg:180.74ms\nstep:859/5000 train_loss:8.2110 train_time:153446ms step_avg:180.74ms\nstep:860/5000 train_loss:8.0296 train_time:153626ms step_avg:180.74ms\nstep:861/5000 train_loss:8.3441 train_time:153807ms step_avg:180.74ms\nstep:862/5000 train_loss:8.0657 train_time:153987ms step_avg:180.74ms\nstep:863/5000 train_loss:8.1047 train_time:154166ms step_avg:180.73ms\nstep:864/5000 train_loss:8.1910 train_time:154347ms step_avg:180.73ms\nstep:865/5000 train_loss:8.0655 train_time:154529ms step_avg:180.74ms\nstep:866/5000 train_loss:8.1563 train_time:154711ms step_avg:180.74ms\nstep:867/5000 train_loss:8.0892 train_time:154892ms step_avg:180.74ms\nstep:868/5000 train_loss:8.1042 train_time:155075ms step_avg:180.74ms\nstep:869/5000 train_loss:8.0438 train_time:155256ms step_avg:180.74ms\nstep:870/5000 train_loss:8.1073 train_time:155437ms step_avg:180.74ms\nstep:871/5000 train_loss:8.0661 train_time:155620ms step_avg:180.74ms\nstep:872/5000 train_loss:8.0510 train_time:155802ms step_avg:180.74ms\nstep:873/5000 train_loss:8.0080 train_time:155983ms step_avg:180.74ms\nstep:874/5000 train_loss:7.9844 train_time:156164ms step_avg:180.75ms\nstep:875/5000 train_loss:8.1822 train_time:156347ms step_avg:180.75ms\nstep:876/5000 train_loss:8.1210 train_time:156527ms step_avg:180.75ms\nstep:877/5000 train_loss:8.1572 train_time:156709ms step_avg:180.75ms\nstep:878/5000 train_loss:8.0746 train_time:156890ms step_avg:180.75ms\nstep:879/5000 train_loss:7.9576 train_time:157071ms step_avg:180.75ms\nstep:880/5000 train_loss:8.0789 train_time:157252ms step_avg:180.75ms\nstep:881/5000 train_loss:8.0926 train_time:157435ms step_avg:180.75ms\nstep:882/5000 train_loss:8.2180 train_time:157618ms step_avg:180.75ms\nstep:883/5000 train_loss:8.0773 train_time:157800ms step_avg:180.76ms\nstep:884/5000 train_loss:8.1825 train_time:157981ms step_avg:180.76ms\nstep:885/5000 train_loss:8.0395 train_time:158162ms step_avg:180.76ms\nstep:886/5000 train_loss:8.1546 train_time:158344ms step_avg:180.76ms\nstep:887/5000 train_loss:8.0922 train_time:158524ms step_avg:180.76ms\nstep:888/5000 train_loss:8.1917 train_time:158706ms step_avg:180.76ms\nstep:889/5000 train_loss:8.1367 train_time:158886ms step_avg:180.76ms\nstep:890/5000 train_loss:8.1523 train_time:159067ms step_avg:180.76ms\nstep:891/5000 train_loss:8.1135 train_time:159247ms step_avg:180.76ms\nstep:892/5000 train_loss:8.1774 train_time:159427ms step_avg:180.76ms\nstep:893/5000 train_loss:8.1619 train_time:159610ms step_avg:180.76ms\nstep:894/5000 train_loss:8.0752 train_time:159791ms step_avg:180.76ms\nstep:895/5000 train_loss:8.1879 train_time:159973ms step_avg:180.76ms\nstep:896/5000 train_loss:8.1431 train_time:160156ms step_avg:180.76ms\nstep:897/5000 train_loss:8.0688 train_time:160336ms step_avg:180.76ms\nstep:898/5000 train_loss:8.1739 train_time:160518ms step_avg:180.76ms\nstep:899/5000 train_loss:8.1893 train_time:160700ms step_avg:180.76ms\nstep:900/5000 train_loss:7.9622 train_time:160881ms step_avg:180.77ms\nstep:901/5000 train_loss:8.1423 train_time:161063ms step_avg:180.77ms\nstep:902/5000 train_loss:8.0187 train_time:161244ms step_avg:180.77ms\nstep:903/5000 train_loss:8.2169 train_time:161425ms step_avg:180.77ms\nstep:904/5000 train_loss:8.0604 train_time:161606ms step_avg:180.77ms\nstep:905/5000 train_loss:8.1443 train_time:161786ms step_avg:180.77ms\nstep:906/5000 train_loss:8.0441 train_time:161967ms step_avg:180.77ms\nstep:907/5000 train_loss:8.0589 train_time:162149ms step_avg:180.77ms\nstep:908/5000 train_loss:8.0384 train_time:162330ms step_avg:180.77ms\nstep:909/5000 train_loss:8.1576 train_time:162512ms step_avg:180.77ms\nstep:910/5000 train_loss:8.0640 train_time:162694ms step_avg:180.77ms\nstep:911/5000 train_loss:8.1654 train_time:162875ms step_avg:180.77ms\nstep:912/5000 train_loss:8.1268 train_time:163057ms step_avg:180.77ms\nstep:913/5000 train_loss:8.0940 train_time:163239ms step_avg:180.77ms\nstep:914/5000 train_loss:8.1897 train_time:163422ms step_avg:180.78ms\nstep:915/5000 train_loss:8.1591 train_time:163603ms step_avg:180.78ms\nstep:916/5000 train_loss:8.1412 train_time:163784ms step_avg:180.78ms\nstep:917/5000 train_loss:8.2018 train_time:163965ms step_avg:180.78ms\nstep:918/5000 train_loss:8.1774 train_time:164147ms step_avg:180.78ms\nstep:919/5000 train_loss:8.3481 train_time:164328ms step_avg:180.78ms\nstep:920/5000 train_loss:8.2195 train_time:164509ms step_avg:180.78ms\nstep:921/5000 train_loss:8.1601 train_time:164690ms step_avg:180.78ms\nstep:922/5000 train_loss:8.0890 train_time:164871ms step_avg:180.78ms\nstep:923/5000 train_loss:8.0212 train_time:165053ms step_avg:180.78ms\nstep:924/5000 train_loss:8.2053 train_time:165234ms step_avg:180.78ms\nstep:925/5000 train_loss:8.0075 train_time:165417ms step_avg:180.78ms\nstep:926/5000 train_loss:8.1960 train_time:165600ms step_avg:180.79ms\nstep:927/5000 train_loss:8.0711 train_time:165781ms step_avg:180.79ms\nstep:928/5000 train_loss:8.0111 train_time:165963ms step_avg:180.79ms\nstep:929/5000 train_loss:8.1297 train_time:166144ms step_avg:180.79ms\nstep:930/5000 train_loss:8.0541 train_time:166324ms step_avg:180.79ms\nstep:931/5000 train_loss:8.0818 train_time:166507ms step_avg:180.79ms\nstep:932/5000 train_loss:8.1515 train_time:166690ms step_avg:180.79ms\nstep:933/5000 train_loss:8.1623 train_time:166871ms step_avg:180.79ms\nstep:934/5000 train_loss:8.0375 train_time:167053ms step_avg:180.79ms\nstep:935/5000 train_loss:8.0713 train_time:167235ms step_avg:180.79ms\nstep:936/5000 train_loss:8.0876 train_time:167418ms step_avg:180.80ms\nstep:937/5000 train_loss:8.1172 train_time:167600ms step_avg:180.80ms\nstep:938/5000 train_loss:8.2911 train_time:167782ms step_avg:180.80ms\nstep:939/5000 train_loss:8.2248 train_time:167963ms step_avg:180.80ms\nstep:940/5000 train_loss:7.8426 train_time:168144ms step_avg:180.80ms\nstep:941/5000 train_loss:8.1737 train_time:168324ms step_avg:180.80ms\nstep:942/5000 train_loss:8.1767 train_time:168506ms step_avg:180.80ms\nstep:943/5000 train_loss:8.0140 train_time:168686ms step_avg:180.80ms\nstep:944/5000 train_loss:8.2462 train_time:168867ms step_avg:180.80ms\nstep:945/5000 train_loss:7.9971 train_time:169047ms step_avg:180.80ms\nstep:946/5000 train_loss:8.2168 train_time:169227ms step_avg:180.80ms\nstep:947/5000 train_loss:8.1674 train_time:169409ms step_avg:180.80ms\nstep:948/5000 train_loss:8.0975 train_time:169592ms step_avg:180.80ms\nstep:949/5000 train_loss:7.9042 train_time:169775ms step_avg:180.80ms\nstep:950/5000 train_loss:8.1522 train_time:169957ms step_avg:180.81ms\nstep:951/5000 train_loss:8.1835 train_time:170140ms step_avg:180.81ms\nstep:952/5000 train_loss:8.0565 train_time:170322ms step_avg:180.81ms\nstep:953/5000 train_loss:8.2822 train_time:170503ms step_avg:180.81ms\nstep:954/5000 train_loss:8.0151 train_time:170688ms step_avg:180.81ms\nstep:955/5000 train_loss:8.1345 train_time:170865ms step_avg:180.81ms\nstep:956/5000 train_loss:8.1560 train_time:171047ms step_avg:180.81ms\nstep:957/5000 train_loss:8.0553 train_time:171228ms step_avg:180.81ms\nstep:958/5000 train_loss:8.1405 train_time:171411ms step_avg:180.81ms\nstep:959/5000 train_loss:8.0607 train_time:171592ms step_avg:180.81ms\nstep:960/5000 train_loss:8.2058 train_time:171776ms step_avg:180.82ms\nstep:961/5000 train_loss:8.0524 train_time:171959ms step_avg:180.82ms\nstep:962/5000 train_loss:8.2987 train_time:172143ms step_avg:180.82ms\nstep:963/5000 train_loss:8.3153 train_time:172320ms step_avg:180.82ms\nstep:964/5000 train_loss:8.1444 train_time:172502ms step_avg:180.82ms\nstep:965/5000 train_loss:8.1672 train_time:172684ms step_avg:180.82ms\nstep:966/5000 train_loss:8.0838 train_time:172866ms step_avg:180.82ms\nstep:967/5000 train_loss:8.1819 train_time:173045ms step_avg:180.82ms\nstep:968/5000 train_loss:8.2585 train_time:173226ms step_avg:180.82ms\nstep:969/5000 train_loss:8.1965 train_time:173406ms step_avg:180.82ms\nstep:970/5000 train_loss:8.2296 train_time:173586ms step_avg:180.82ms\nstep:971/5000 train_loss:8.1535 train_time:173766ms step_avg:180.82ms\nstep:972/5000 train_loss:8.0872 train_time:173946ms step_avg:180.82ms\nstep:973/5000 train_loss:8.0854 train_time:174127ms step_avg:180.82ms\nstep:974/5000 train_loss:8.0567 train_time:174309ms step_avg:180.82ms\nstep:975/5000 train_loss:8.0202 train_time:174490ms step_avg:180.82ms\nstep:976/5000 train_loss:8.0318 train_time:174671ms step_avg:180.82ms\nstep:977/5000 train_loss:8.1277 train_time:174853ms step_avg:180.82ms\nstep:978/5000 train_loss:8.0641 train_time:175036ms step_avg:180.82ms\nstep:979/5000 train_loss:8.1671 train_time:175218ms step_avg:180.82ms\nstep:980/5000 train_loss:8.1172 train_time:175401ms step_avg:180.83ms\nstep:981/5000 train_loss:8.1732 train_time:175581ms step_avg:180.82ms\nstep:982/5000 train_loss:8.0809 train_time:175761ms step_avg:180.82ms\nstep:983/5000 train_loss:8.2134 train_time:175941ms step_avg:180.82ms\nstep:984/5000 train_loss:8.0867 train_time:176123ms step_avg:180.82ms\nstep:985/5000 train_loss:8.0889 train_time:176305ms step_avg:180.83ms\nstep:986/5000 train_loss:8.1493 train_time:176486ms step_avg:180.83ms\nstep:987/5000 train_loss:8.1903 train_time:176667ms step_avg:180.83ms\nstep:988/5000 train_loss:8.1134 train_time:176848ms step_avg:180.83ms\nstep:989/5000 train_loss:8.0120 train_time:177028ms step_avg:180.83ms\nstep:990/5000 train_loss:8.1472 train_time:177210ms step_avg:180.83ms\nstep:991/5000 train_loss:8.1516 train_time:177391ms step_avg:180.83ms\nstep:992/5000 train_loss:8.2815 train_time:177572ms step_avg:180.83ms\nstep:993/5000 train_loss:8.1254 train_time:177754ms step_avg:180.83ms\nstep:994/5000 train_loss:8.0593 train_time:177936ms step_avg:180.83ms\nstep:995/5000 train_loss:8.1565 train_time:178118ms step_avg:180.83ms\nstep:996/5000 train_loss:8.0861 train_time:178300ms step_avg:180.83ms\nstep:997/5000 train_loss:8.1441 train_time:178482ms step_avg:180.83ms\nstep:998/5000 train_loss:8.1514 train_time:178663ms step_avg:180.83ms\nstep:999/5000 train_loss:8.0918 train_time:178845ms step_avg:180.83ms\nstep:1000/5000 train_loss:8.0608 train_time:179026ms step_avg:180.83ms\nstep:1001/5000 train_loss:8.0880 train_time:179207ms step_avg:180.83ms\nstep:1002/5000 train_loss:8.1212 train_time:179387ms step_avg:180.83ms\nstep:1003/5000 train_loss:7.9979 train_time:179568ms step_avg:180.83ms\nstep:1004/5000 train_loss:8.1478 train_time:179749ms step_avg:180.83ms\nstep:1005/5000 train_loss:8.1639 train_time:179929ms step_avg:180.83ms\nstep:1006/5000 train_loss:8.0570 train_time:180112ms step_avg:180.84ms\nstep:1007/5000 train_loss:8.2823 train_time:180294ms step_avg:180.84ms\nstep:1008/5000 train_loss:8.0453 train_time:180476ms step_avg:180.84ms\nstep:1009/5000 train_loss:8.1137 train_time:180657ms step_avg:180.84ms\nstep:1010/5000 train_loss:8.0181 train_time:180840ms step_avg:180.84ms\nstep:1011/5000 train_loss:8.1222 train_time:181022ms step_avg:180.84ms\nstep:1012/5000 train_loss:8.0445 train_time:181202ms step_avg:180.84ms\nstep:1013/5000 train_loss:7.9982 train_time:181383ms step_avg:180.84ms\nstep:1014/5000 train_loss:8.0810 train_time:181565ms step_avg:180.84ms\nstep:1015/5000 train_loss:8.1023 train_time:181746ms step_avg:180.84ms\nstep:1016/5000 train_loss:8.1332 train_time:181926ms step_avg:180.84ms\nstep:1017/5000 train_loss:8.0698 train_time:182107ms step_avg:180.84ms\nstep:1018/5000 train_loss:8.1157 train_time:182288ms step_avg:180.84ms\nstep:1019/5000 train_loss:8.0194 train_time:182469ms step_avg:180.84ms\nstep:1020/5000 train_loss:8.1469 train_time:182650ms step_avg:180.84ms\nstep:1021/5000 train_loss:8.1982 train_time:182830ms step_avg:180.84ms\nstep:1022/5000 train_loss:8.0456 train_time:183013ms step_avg:180.84ms\nstep:1023/5000 train_loss:8.0696 train_time:183195ms step_avg:180.84ms\nstep:1024/5000 train_loss:8.2331 train_time:183377ms step_avg:180.85ms\nstep:1025/5000 train_loss:8.1270 train_time:183559ms step_avg:180.85ms\nstep:1026/5000 train_loss:8.1454 train_time:183742ms step_avg:180.85ms\nstep:1027/5000 train_loss:8.0184 train_time:183923ms step_avg:180.85ms\nstep:1028/5000 train_loss:8.2031 train_time:184105ms step_avg:180.85ms\nstep:1029/5000 train_loss:8.1204 train_time:184285ms step_avg:180.85ms\nstep:1030/5000 train_loss:8.2808 train_time:184465ms step_avg:180.85ms\nstep:1031/5000 train_loss:8.1012 train_time:184645ms step_avg:180.85ms\nstep:1032/5000 train_loss:8.0454 train_time:184826ms step_avg:180.85ms\nstep:1033/5000 train_loss:8.2337 train_time:185009ms step_avg:180.85ms\nstep:1034/5000 train_loss:8.0635 train_time:185190ms step_avg:180.85ms\nstep:1035/5000 train_loss:8.0271 train_time:185371ms step_avg:180.85ms\nstep:1036/5000 train_loss:8.0475 train_time:185552ms step_avg:180.85ms\nstep:1037/5000 train_loss:8.0489 train_time:185732ms step_avg:180.85ms\nstep:1038/5000 train_loss:8.0908 train_time:185915ms step_avg:180.85ms\nstep:1039/5000 train_loss:8.1045 train_time:186098ms step_avg:180.85ms\nstep:1040/5000 train_loss:8.0705 train_time:186280ms step_avg:180.85ms\nstep:1041/5000 train_loss:8.1956 train_time:186461ms step_avg:180.85ms\nstep:1042/5000 train_loss:8.0268 train_time:186641ms step_avg:180.85ms\nstep:1043/5000 train_loss:8.1767 train_time:186822ms step_avg:180.85ms\nstep:1044/5000 train_loss:8.1420 train_time:187003ms step_avg:180.85ms\nstep:1045/5000 train_loss:7.9482 train_time:187184ms step_avg:180.85ms\nstep:1046/5000 train_loss:8.0380 train_time:187366ms step_avg:180.86ms\nstep:1047/5000 train_loss:8.0447 train_time:187547ms step_avg:180.86ms\nstep:1048/5000 train_loss:8.0834 train_time:187728ms step_avg:180.86ms\nstep:1049/5000 train_loss:8.2586 train_time:187909ms step_avg:180.86ms\nstep:1050/5000 train_loss:8.1081 train_time:188089ms step_avg:180.85ms\nstep:1051/5000 train_loss:8.1971 train_time:188270ms step_avg:180.86ms\nstep:1052/5000 train_loss:8.1585 train_time:188452ms step_avg:180.86ms\nstep:1053/5000 train_loss:8.1602 train_time:188634ms step_avg:180.86ms\nstep:1054/5000 train_loss:8.2131 train_time:188817ms step_avg:180.86ms\nstep:1055/5000 train_loss:8.1405 train_time:188999ms step_avg:180.86ms\nstep:1056/5000 train_loss:8.0432 train_time:189182ms step_avg:180.86ms\nstep:1057/5000 train_loss:8.1247 train_time:189362ms step_avg:180.86ms\nstep:1058/5000 train_loss:8.3314 train_time:189544ms step_avg:180.86ms\nstep:1059/5000 train_loss:8.1193 train_time:189725ms step_avg:180.86ms\nstep:1060/5000 train_loss:8.0065 train_time:189906ms step_avg:180.86ms\nstep:1061/5000 train_loss:7.9446 train_time:190086ms step_avg:180.86ms\nstep:1062/5000 train_loss:8.1606 train_time:190266ms step_avg:180.86ms\nstep:1063/5000 train_loss:8.3376 train_time:190447ms step_avg:180.86ms\nstep:1064/5000 train_loss:8.0452 train_time:190628ms step_avg:180.86ms\nstep:1065/5000 train_loss:7.9998 train_time:190809ms step_avg:180.86ms\nstep:1066/5000 train_loss:8.1362 train_time:190991ms step_avg:180.86ms\nstep:1067/5000 train_loss:8.1523 train_time:191173ms step_avg:180.86ms\nstep:1068/5000 train_loss:8.1367 train_time:191355ms step_avg:180.87ms\nstep:1069/5000 train_loss:8.0272 train_time:191537ms step_avg:180.87ms\nstep:1070/5000 train_loss:8.0306 train_time:191719ms step_avg:180.87ms\nstep:1071/5000 train_loss:8.1574 train_time:191901ms step_avg:180.87ms\nstep:1072/5000 train_loss:7.9852 train_time:192083ms step_avg:180.87ms\nstep:1073/5000 train_loss:8.1657 train_time:192264ms step_avg:180.87ms\nstep:1074/5000 train_loss:8.0189 train_time:192445ms step_avg:180.87ms\nstep:1075/5000 train_loss:8.0723 train_time:192626ms step_avg:180.87ms\nstep:1076/5000 train_loss:8.1455 train_time:192807ms step_avg:180.87ms\nstep:1077/5000 train_loss:8.1833 train_time:192988ms step_avg:180.87ms\nstep:1078/5000 train_loss:8.1465 train_time:193169ms step_avg:180.87ms\nstep:1079/5000 train_loss:8.0487 train_time:193350ms step_avg:180.87ms\nstep:1080/5000 train_loss:8.1725 train_time:193531ms step_avg:180.87ms\nstep:1081/5000 train_loss:8.0427 train_time:193713ms step_avg:180.87ms\nstep:1082/5000 train_loss:8.1073 train_time:193895ms step_avg:180.87ms\nstep:1083/5000 train_loss:8.1010 train_time:194076ms step_avg:180.87ms\nstep:1084/5000 train_loss:8.1357 train_time:194257ms step_avg:180.87ms\nstep:1085/5000 train_loss:8.1410 train_time:194439ms step_avg:180.87ms\nstep:1086/5000 train_loss:8.1232 train_time:194622ms step_avg:180.88ms\nstep:1087/5000 train_loss:8.2135 train_time:194803ms step_avg:180.88ms\nstep:1088/5000 train_loss:7.9982 train_time:194985ms step_avg:180.88ms\nstep:1089/5000 train_loss:8.0441 train_time:195166ms step_avg:180.88ms\nstep:1090/5000 train_loss:8.0271 train_time:195347ms step_avg:180.88ms\nstep:1091/5000 train_loss:8.2132 train_time:195528ms step_avg:180.88ms\nstep:1092/5000 train_loss:8.0514 train_time:195709ms step_avg:180.88ms\nstep:1093/5000 train_loss:8.1613 train_time:195890ms step_avg:180.88ms\nstep:1094/5000 train_loss:8.3250 train_time:196071ms step_avg:180.88ms\nstep:1095/5000 train_loss:8.0046 train_time:196252ms step_avg:180.88ms\nstep:1096/5000 train_loss:8.1615 train_time:196433ms step_avg:180.88ms\nstep:1097/5000 train_loss:8.1781 train_time:196617ms step_avg:180.88ms\nstep:1098/5000 train_loss:8.0229 train_time:196799ms step_avg:180.88ms\nstep:1099/5000 train_loss:7.9859 train_time:196982ms step_avg:180.88ms\nstep:1100/5000 train_loss:8.1131 train_time:197162ms step_avg:180.88ms\nstep:1101/5000 train_loss:7.9151 train_time:197342ms step_avg:180.88ms\nstep:1102/5000 train_loss:8.1369 train_time:197524ms step_avg:180.88ms\nstep:1103/5000 train_loss:8.1571 train_time:197705ms step_avg:180.88ms\nstep:1104/5000 train_loss:8.1949 train_time:197885ms step_avg:180.88ms\nstep:1105/5000 train_loss:8.1335 train_time:198065ms step_avg:180.88ms\nstep:1106/5000 train_loss:8.1167 train_time:198245ms step_avg:180.88ms\nstep:1107/5000 train_loss:8.0786 train_time:198426ms step_avg:180.88ms\nstep:1108/5000 train_loss:8.0891 train_time:198606ms step_avg:180.88ms\nstep:1109/5000 train_loss:8.1205 train_time:198787ms step_avg:180.88ms\nstep:1110/5000 train_loss:8.0623 train_time:198970ms step_avg:180.88ms\nstep:1111/5000 train_loss:8.0216 train_time:199151ms step_avg:180.88ms\nstep:1112/5000 train_loss:8.1639 train_time:199331ms step_avg:180.88ms\nstep:1113/5000 train_loss:7.9943 train_time:199513ms step_avg:180.88ms\nstep:1114/5000 train_loss:8.1972 train_time:199696ms step_avg:180.88ms\nstep:1115/5000 train_loss:8.0925 train_time:199877ms step_avg:180.88ms\nstep:1116/5000 train_loss:8.0953 train_time:200061ms step_avg:180.89ms\nstep:1117/5000 train_loss:8.1069 train_time:200242ms step_avg:180.89ms\nstep:1118/5000 train_loss:8.0957 train_time:200424ms step_avg:180.89ms\nstep:1119/5000 train_loss:8.0903 train_time:200605ms step_avg:180.89ms\nstep:1120/5000 train_loss:7.9959 train_time:200785ms step_avg:180.89ms\nstep:1121/5000 train_loss:8.1640 train_time:200965ms step_avg:180.89ms\nstep:1122/5000 train_loss:8.0279 train_time:201146ms step_avg:180.89ms\nstep:1123/5000 train_loss:8.0042 train_time:201327ms step_avg:180.89ms\nstep:1124/5000 train_loss:8.0984 train_time:201508ms step_avg:180.89ms\nstep:1125/5000 train_loss:8.1238 train_time:201689ms step_avg:180.89ms\nstep:1126/5000 train_loss:8.0838 train_time:201870ms step_avg:180.89ms\nstep:1127/5000 train_loss:8.1112 train_time:202052ms step_avg:180.89ms\nstep:1128/5000 train_loss:8.2642 train_time:202233ms step_avg:180.89ms\nstep:1129/5000 train_loss:8.1042 train_time:202415ms step_avg:180.89ms\nstep:1130/5000 train_loss:8.1954 train_time:202598ms step_avg:180.89ms\nstep:1131/5000 train_loss:8.2695 train_time:202779ms step_avg:180.89ms\nstep:1132/5000 train_loss:8.1012 train_time:202962ms step_avg:180.89ms\nstep:1133/5000 train_loss:8.1081 train_time:203143ms step_avg:180.89ms\nstep:1134/5000 train_loss:8.0994 train_time:203324ms step_avg:180.89ms\nstep:1135/5000 train_loss:8.1325 train_time:203507ms step_avg:180.89ms\nstep:1136/5000 train_loss:8.1538 train_time:203688ms step_avg:180.90ms\nstep:1137/5000 train_loss:8.0297 train_time:203869ms step_avg:180.90ms\nstep:1138/5000 train_loss:8.0485 train_time:204049ms step_avg:180.89ms\nstep:1139/5000 train_loss:8.1707 train_time:204230ms step_avg:180.89ms\nstep:1140/5000 train_loss:8.1250 train_time:204412ms step_avg:180.90ms\nstep:1141/5000 train_loss:8.0829 train_time:204595ms step_avg:180.90ms\nstep:1142/5000 train_loss:8.0598 train_time:204777ms step_avg:180.90ms\nstep:1143/5000 train_loss:8.0588 train_time:204960ms step_avg:180.90ms\nstep:1144/5000 train_loss:8.1679 train_time:205140ms step_avg:180.90ms\nstep:1145/5000 train_loss:8.1629 train_time:205322ms step_avg:180.90ms\nstep:1146/5000 train_loss:8.0989 train_time:205503ms step_avg:180.90ms\nstep:1147/5000 train_loss:8.1066 train_time:205683ms step_avg:180.90ms\nstep:1148/5000 train_loss:8.1413 train_time:205865ms step_avg:180.90ms\nstep:1149/5000 train_loss:8.0752 train_time:206045ms step_avg:180.90ms\nstep:1150/5000 train_loss:8.0503 train_time:206226ms step_avg:180.90ms\nstep:1151/5000 train_loss:8.0752 train_time:206408ms step_avg:180.90ms\nstep:1152/5000 train_loss:8.1205 train_time:206588ms step_avg:180.90ms\nstep:1153/5000 train_loss:8.1270 train_time:206769ms step_avg:180.90ms\nstep:1154/5000 train_loss:8.2525 train_time:206950ms step_avg:180.90ms\nstep:1155/5000 train_loss:8.1131 train_time:207130ms step_avg:180.90ms\nstep:1156/5000 train_loss:8.0654 train_time:207313ms step_avg:180.90ms\nstep:1157/5000 train_loss:8.0100 train_time:207495ms step_avg:180.90ms\nstep:1158/5000 train_loss:8.2495 train_time:207676ms step_avg:180.90ms\nstep:1159/5000 train_loss:8.0459 train_time:207857ms step_avg:180.90ms\nstep:1160/5000 train_loss:8.0679 train_time:208040ms step_avg:180.90ms\nstep:1161/5000 train_loss:8.1488 train_time:208222ms step_avg:180.91ms\nstep:1162/5000 train_loss:8.1044 train_time:208404ms step_avg:180.91ms\nstep:1163/5000 train_loss:8.1165 train_time:208585ms step_avg:180.91ms\nstep:1164/5000 train_loss:8.1843 train_time:208766ms step_avg:180.91ms\nstep:1165/5000 train_loss:8.0668 train_time:208946ms step_avg:180.91ms\nstep:1166/5000 train_loss:8.0767 train_time:209126ms step_avg:180.90ms\nstep:1167/5000 train_loss:8.1580 train_time:209307ms step_avg:180.90ms\nstep:1168/5000 train_loss:8.0938 train_time:209488ms step_avg:180.90ms\nstep:1169/5000 train_loss:8.1705 train_time:209669ms step_avg:180.91ms\nstep:1170/5000 train_loss:8.1274 train_time:209851ms step_avg:180.91ms\nstep:1171/5000 train_loss:8.0575 train_time:210033ms step_avg:180.91ms\nstep:1172/5000 train_loss:8.1065 train_time:210215ms step_avg:180.91ms\nstep:1173/5000 train_loss:8.1060 train_time:210397ms step_avg:180.91ms\nstep:1174/5000 train_loss:8.0940 train_time:210578ms step_avg:180.91ms\nstep:1175/5000 train_loss:8.0636 train_time:210759ms step_avg:180.91ms\nstep:1176/5000 train_loss:8.0930 train_time:210941ms step_avg:180.91ms\nstep:1177/5000 train_loss:8.2343 train_time:211124ms step_avg:180.91ms\nstep:1178/5000 train_loss:8.0742 train_time:211305ms step_avg:180.91ms\nstep:1179/5000 train_loss:8.1231 train_time:211486ms step_avg:180.91ms\nstep:1180/5000 train_loss:8.0725 train_time:211667ms step_avg:180.91ms\nstep:1181/5000 train_loss:7.9574 train_time:211847ms step_avg:180.91ms\nstep:1182/5000 train_loss:8.2052 train_time:212026ms step_avg:180.91ms\nstep:1183/5000 train_loss:8.0816 train_time:212207ms step_avg:180.91ms\nstep:1184/5000 train_loss:8.0155 train_time:212387ms step_avg:180.91ms\nstep:1185/5000 train_loss:8.1246 train_time:212568ms step_avg:180.91ms\nstep:1186/5000 train_loss:8.1356 train_time:212750ms step_avg:180.91ms\nstep:1187/5000 train_loss:8.2033 train_time:212932ms step_avg:180.91ms\nstep:1188/5000 train_loss:8.0289 train_time:213116ms step_avg:180.91ms\nstep:1189/5000 train_loss:8.3111 train_time:213298ms step_avg:180.91ms\nstep:1190/5000 train_loss:8.2383 train_time:213479ms step_avg:180.91ms\nstep:1191/5000 train_loss:8.1556 train_time:213661ms step_avg:180.92ms\nstep:1192/5000 train_loss:8.0864 train_time:213843ms step_avg:180.92ms\nstep:1193/5000 train_loss:8.2255 train_time:214025ms step_avg:180.92ms\nstep:1194/5000 train_loss:8.1016 train_time:214207ms step_avg:180.92ms\nstep:1195/5000 train_loss:8.0915 train_time:214388ms step_avg:180.92ms\nstep:1196/5000 train_loss:8.0790 train_time:214569ms step_avg:180.92ms\nstep:1197/5000 train_loss:8.0938 train_time:214749ms step_avg:180.92ms\nstep:1198/5000 train_loss:8.2858 train_time:214930ms step_avg:180.92ms\nstep:1199/5000 train_loss:8.0061 train_time:215113ms step_avg:180.92ms\nstep:1200/5000 train_loss:8.1855 train_time:215296ms step_avg:180.92ms\nstep:1201/5000 train_loss:8.1443 train_time:215477ms step_avg:180.92ms\nstep:1202/5000 train_loss:8.1348 train_time:215659ms step_avg:180.92ms\nstep:1203/5000 train_loss:8.0389 train_time:215841ms step_avg:180.92ms\nstep:1204/5000 train_loss:8.1231 train_time:216022ms step_avg:180.92ms\nstep:1205/5000 train_loss:8.0967 train_time:216204ms step_avg:180.92ms\nstep:1206/5000 train_loss:8.0890 train_time:216385ms step_avg:180.92ms\nstep:1207/5000 train_loss:8.1529 train_time:216565ms step_avg:180.92ms\nstep:1208/5000 train_loss:8.0811 train_time:216746ms step_avg:180.92ms\nstep:1209/5000 train_loss:8.1855 train_time:216927ms step_avg:180.92ms\nstep:1210/5000 train_loss:8.1076 train_time:217109ms step_avg:180.92ms\nstep:1211/5000 train_loss:8.1304 train_time:217291ms step_avg:180.92ms\nstep:1212/5000 train_loss:8.1429 train_time:217473ms step_avg:180.93ms\nstep:1213/5000 train_loss:8.0199 train_time:217655ms step_avg:180.93ms\nstep:1214/5000 train_loss:8.1657 train_time:217838ms step_avg:180.93ms\nstep:1215/5000 train_loss:8.0865 train_time:218021ms step_avg:180.93ms\nstep:1216/5000 train_loss:8.1551 train_time:218202ms step_avg:180.93ms\nstep:1217/5000 train_loss:8.1790 train_time:218383ms step_avg:180.93ms\nstep:1218/5000 train_loss:8.0973 train_time:218565ms step_avg:180.93ms\nstep:1219/5000 train_loss:8.1921 train_time:218745ms step_avg:180.93ms\nstep:1220/5000 train_loss:8.1896 train_time:218926ms step_avg:180.93ms\nstep:1221/5000 train_loss:8.0499 train_time:219107ms step_avg:180.93ms\nstep:1222/5000 train_loss:8.1967 train_time:219287ms step_avg:180.93ms\nstep:1223/5000 train_loss:7.9576 train_time:219468ms step_avg:180.93ms\nstep:1224/5000 train_loss:8.0786 train_time:219650ms step_avg:180.93ms\nstep:1225/5000 train_loss:8.0738 train_time:219830ms step_avg:180.93ms\nstep:1226/5000 train_loss:8.1137 train_time:220012ms step_avg:180.93ms\nstep:1227/5000 train_loss:8.2332 train_time:220195ms step_avg:180.93ms\nstep:1228/5000 train_loss:8.0426 train_time:220377ms step_avg:180.93ms\nstep:1229/5000 train_loss:8.1154 train_time:220559ms step_avg:180.93ms\nstep:1230/5000 train_loss:8.1328 train_time:220740ms step_avg:180.93ms\nstep:1231/5000 train_loss:8.2304 train_time:220922ms step_avg:180.94ms\nstep:1232/5000 train_loss:8.2070 train_time:221103ms step_avg:180.94ms\nstep:1233/5000 train_loss:8.0255 train_time:221284ms step_avg:180.94ms\nstep:1234/5000 train_loss:8.2108 train_time:221465ms step_avg:180.94ms\nstep:1235/5000 train_loss:8.0317 train_time:221646ms step_avg:180.94ms\nstep:1236/5000 train_loss:7.9988 train_time:221827ms step_avg:180.94ms\nstep:1237/5000 train_loss:8.1301 train_time:222009ms step_avg:180.94ms\nstep:1238/5000 train_loss:8.1027 train_time:222191ms step_avg:180.94ms\nstep:1239/5000 train_loss:8.1810 train_time:222372ms step_avg:180.94ms\nstep:1240/5000 train_loss:8.0962 train_time:222554ms step_avg:180.94ms\nstep:1241/5000 train_loss:8.1378 train_time:222736ms step_avg:180.94ms\nstep:1242/5000 train_loss:8.1125 train_time:222919ms step_avg:180.94ms\nstep:1243/5000 train_loss:8.2139 train_time:223102ms step_avg:180.94ms\nstep:1244/5000 train_loss:8.1734 train_time:223284ms step_avg:180.94ms\nstep:1245/5000 train_loss:8.1508 train_time:223465ms step_avg:180.94ms\nstep:1246/5000 train_loss:8.1131 train_time:223645ms step_avg:180.94ms\nstep:1247/5000 train_loss:8.2234 train_time:223825ms step_avg:180.94ms\nstep:1248/5000 train_loss:8.1129 train_time:224006ms step_avg:180.94ms\nstep:1249/5000 train_loss:8.1266 train_time:224187ms step_avg:180.94ms\nstep:1250/5000 train_loss:8.0522 train_time:224367ms step_avg:180.94ms\nstep:1251/5000 train_loss:8.1262 train_time:224548ms step_avg:180.94ms\nstep:1252/5000 train_loss:8.3229 train_time:224728ms step_avg:180.94ms\nstep:1253/5000 train_loss:8.1587 train_time:224910ms step_avg:180.94ms\nstep:1254/5000 train_loss:8.0854 train_time:225091ms step_avg:180.94ms\nstep:1255/5000 train_loss:8.2132 train_time:225272ms step_avg:180.94ms\nstep:1256/5000 train_loss:8.2650 train_time:225455ms step_avg:180.94ms\nstep:1257/5000 train_loss:8.1497 train_time:225636ms step_avg:180.94ms\nstep:1258/5000 train_loss:8.1486 train_time:225818ms step_avg:180.94ms\nstep:1259/5000 train_loss:8.1331 train_time:226000ms step_avg:180.94ms\nstep:1260/5000 train_loss:8.1618 train_time:226181ms step_avg:180.94ms\nstep:1261/5000 train_loss:8.2297 train_time:226362ms step_avg:180.94ms\nstep:1262/5000 train_loss:8.0084 train_time:226543ms step_avg:180.95ms\nstep:1263/5000 train_loss:8.2047 train_time:226725ms step_avg:180.95ms\nstep:1264/5000 train_loss:8.1550 train_time:226906ms step_avg:180.95ms\nstep:1265/5000 train_loss:8.1285 train_time:227087ms step_avg:180.95ms\nstep:1266/5000 train_loss:8.1807 train_time:227267ms step_avg:180.94ms\nstep:1267/5000 train_loss:8.0971 train_time:227448ms step_avg:180.95ms\nstep:1268/5000 train_loss:8.1121 train_time:227634ms step_avg:180.95ms\nstep:1269/5000 train_loss:8.0085 train_time:227816ms step_avg:180.95ms\nstep:1270/5000 train_loss:8.1360 train_time:227998ms step_avg:180.95ms\nstep:1271/5000 train_loss:8.1169 train_time:228180ms step_avg:180.95ms\nstep:1272/5000 train_loss:8.0294 train_time:228359ms step_avg:180.95ms\nstep:1273/5000 train_loss:8.0136 train_time:228541ms step_avg:180.95ms\nstep:1274/5000 train_loss:8.0544 train_time:228724ms step_avg:180.95ms\nstep:1275/5000 train_loss:8.1179 train_time:228906ms step_avg:180.95ms\nstep:1276/5000 train_loss:8.0422 train_time:229087ms step_avg:180.95ms\nstep:1277/5000 train_loss:8.1929 train_time:229266ms step_avg:180.95ms\nstep:1278/5000 train_loss:8.1514 train_time:229447ms step_avg:180.95ms\nstep:1279/5000 train_loss:8.1620 train_time:229626ms step_avg:180.95ms\nstep:1280/5000 train_loss:7.9972 train_time:229807ms step_avg:180.95ms\nstep:1281/5000 train_loss:8.1208 train_time:229988ms step_avg:180.95ms\nstep:1282/5000 train_loss:8.0097 train_time:230168ms step_avg:180.95ms\nstep:1283/5000 train_loss:8.0964 train_time:230349ms step_avg:180.95ms\nstep:1284/5000 train_loss:8.1168 train_time:230529ms step_avg:180.95ms\nstep:1285/5000 train_loss:8.0495 train_time:230710ms step_avg:180.95ms\nstep:1286/5000 train_loss:8.1824 train_time:230891ms step_avg:180.95ms\nstep:1287/5000 train_loss:8.1894 train_time:231073ms step_avg:180.95ms\nstep:1288/5000 train_loss:8.0171 train_time:231256ms step_avg:180.95ms\nstep:1289/5000 train_loss:8.0952 train_time:231437ms step_avg:180.95ms\nstep:1290/5000 train_loss:8.0805 train_time:231620ms step_avg:180.95ms\nstep:1291/5000 train_loss:8.0631 train_time:231802ms step_avg:180.95ms\nstep:1292/5000 train_loss:8.1544 train_time:231983ms step_avg:180.95ms\nstep:1293/5000 train_loss:8.1320 train_time:232164ms step_avg:180.95ms\nstep:1294/5000 train_loss:8.1531 train_time:232346ms step_avg:180.95ms\nstep:1295/5000 train_loss:8.1446 train_time:232527ms step_avg:180.95ms\nstep:1296/5000 train_loss:8.3638 train_time:232708ms step_avg:180.96ms\nstep:1297/5000 train_loss:8.0648 train_time:232890ms step_avg:180.96ms\nstep:1298/5000 train_loss:8.1279 train_time:233071ms step_avg:180.96ms\nstep:1299/5000 train_loss:8.1589 train_time:233252ms step_avg:180.96ms\nstep:1300/5000 train_loss:8.1990 train_time:233434ms step_avg:180.96ms\nstep:1301/5000 train_loss:8.1402 train_time:233616ms step_avg:180.96ms\nstep:1302/5000 train_loss:8.0773 train_time:233800ms step_avg:180.96ms\nstep:1303/5000 train_loss:8.2448 train_time:233981ms step_avg:180.96ms\nstep:1304/5000 train_loss:8.1117 train_time:234163ms step_avg:180.96ms\nstep:1305/5000 train_loss:8.0754 train_time:234346ms step_avg:180.96ms\nstep:1306/5000 train_loss:8.1532 train_time:234527ms step_avg:180.96ms\nstep:1307/5000 train_loss:8.1802 train_time:234709ms step_avg:180.96ms\nstep:1308/5000 train_loss:8.2539 train_time:234891ms step_avg:180.96ms\nstep:1309/5000 train_loss:7.9766 train_time:235072ms step_avg:180.96ms\nstep:1310/5000 train_loss:7.9838 train_time:235255ms step_avg:180.97ms\nstep:1311/5000 train_loss:8.1216 train_time:235437ms step_avg:180.97ms\nstep:1312/5000 train_loss:8.2189 train_time:235619ms step_avg:180.97ms\nstep:1313/5000 train_loss:8.2168 train_time:235802ms step_avg:180.97ms\nstep:1314/5000 train_loss:8.2276 train_time:235982ms step_avg:180.97ms\nstep:1315/5000 train_loss:8.1191 train_time:236162ms step_avg:180.97ms\nstep:1316/5000 train_loss:8.1336 train_time:236344ms step_avg:180.97ms\nstep:1317/5000 train_loss:8.0424 train_time:236525ms step_avg:180.97ms\nstep:1318/5000 train_loss:8.0274 train_time:236706ms step_avg:180.97ms\nstep:1319/5000 train_loss:8.1154 train_time:236887ms step_avg:180.97ms\nstep:1320/5000 train_loss:8.0964 train_time:237067ms step_avg:180.97ms\nstep:1321/5000 train_loss:8.1589 train_time:237248ms step_avg:180.97ms\nstep:1322/5000 train_loss:8.1438 train_time:237428ms step_avg:180.97ms\nstep:1323/5000 train_loss:8.1148 train_time:237610ms step_avg:180.97ms\nstep:1324/5000 train_loss:8.1139 train_time:237791ms step_avg:180.97ms\nstep:1325/5000 train_loss:8.0639 train_time:237971ms step_avg:180.97ms\nstep:1326/5000 train_loss:8.3979 train_time:238154ms step_avg:180.97ms\nstep:1327/5000 train_loss:8.0922 train_time:238336ms step_avg:180.97ms\nstep:1328/5000 train_loss:7.9932 train_time:238518ms step_avg:180.97ms\nstep:1329/5000 train_loss:8.0769 train_time:238700ms step_avg:180.97ms\nstep:1330/5000 train_loss:8.0642 train_time:238881ms step_avg:180.97ms\nstep:1331/5000 train_loss:8.0245 train_time:239062ms step_avg:180.97ms\nstep:1332/5000 train_loss:8.0722 train_time:239244ms step_avg:180.97ms\nstep:1333/5000 train_loss:8.0370 train_time:239427ms step_avg:180.97ms\nstep:1334/5000 train_loss:8.0960 train_time:239608ms step_avg:180.97ms\nstep:1335/5000 train_loss:8.0937 train_time:239789ms step_avg:180.97ms\nstep:1336/5000 train_loss:8.0527 train_time:239970ms step_avg:180.97ms\nstep:1337/5000 train_loss:8.1350 train_time:240151ms step_avg:180.97ms\nstep:1338/5000 train_loss:8.1332 train_time:240332ms step_avg:180.97ms\nstep:1339/5000 train_loss:8.1004 train_time:240516ms step_avg:180.98ms\nstep:1340/5000 train_loss:8.2242 train_time:240699ms step_avg:180.98ms\nstep:1341/5000 train_loss:8.1933 train_time:240881ms step_avg:180.98ms\nstep:1342/5000 train_loss:8.1326 train_time:241061ms step_avg:180.98ms\nstep:1343/5000 train_loss:8.0274 train_time:241241ms step_avg:180.98ms\nstep:1344/5000 train_loss:8.1562 train_time:241423ms step_avg:180.98ms\nstep:1345/5000 train_loss:8.0291 train_time:241606ms step_avg:180.98ms\nstep:1346/5000 train_loss:8.0691 train_time:241787ms step_avg:180.98ms\nstep:1347/5000 train_loss:8.0527 train_time:241968ms step_avg:180.98ms\nstep:1348/5000 train_loss:8.3102 train_time:242148ms step_avg:180.98ms\nstep:1349/5000 train_loss:8.6143 train_time:242327ms step_avg:180.98ms\nstep:1350/5000 train_loss:8.1663 train_time:242509ms step_avg:180.98ms\nstep:1351/5000 train_loss:8.1502 train_time:242690ms step_avg:180.98ms\nstep:1352/5000 train_loss:8.1572 train_time:242871ms step_avg:180.98ms\nstep:1353/5000 train_loss:8.1699 train_time:243054ms step_avg:180.98ms\nstep:1354/5000 train_loss:8.0341 train_time:243237ms step_avg:180.98ms\nstep:1355/5000 train_loss:8.1932 train_time:243419ms step_avg:180.98ms\nstep:1356/5000 train_loss:8.1795 train_time:243601ms step_avg:180.98ms\nstep:1357/5000 train_loss:8.1175 train_time:243782ms step_avg:180.98ms\nstep:1358/5000 train_loss:8.1057 train_time:243963ms step_avg:180.98ms\nstep:1359/5000 train_loss:8.0966 train_time:244145ms step_avg:180.98ms\nstep:1360/5000 train_loss:8.0245 train_time:244325ms step_avg:180.98ms\nstep:1361/5000 train_loss:8.0371 train_time:244507ms step_avg:180.98ms\nstep:1362/5000 train_loss:7.9982 train_time:244689ms step_avg:180.98ms\nstep:1363/5000 train_loss:8.0226 train_time:244869ms step_avg:180.98ms\nstep:1364/5000 train_loss:8.0755 train_time:245050ms step_avg:180.98ms\nstep:1365/5000 train_loss:8.1281 train_time:245232ms step_avg:180.98ms\nstep:1366/5000 train_loss:8.1459 train_time:245415ms step_avg:180.98ms\nstep:1367/5000 train_loss:7.9831 train_time:245597ms step_avg:180.99ms\nstep:1368/5000 train_loss:8.0968 train_time:245778ms step_avg:180.99ms\nstep:1369/5000 train_loss:8.1283 train_time:245960ms step_avg:180.99ms\nstep:1370/5000 train_loss:8.0701 train_time:246142ms step_avg:180.99ms\nstep:1371/5000 train_loss:8.1401 train_time:246322ms step_avg:180.99ms\nstep:1372/5000 train_loss:8.1368 train_time:246504ms step_avg:180.99ms\nstep:1373/5000 train_loss:8.0039 train_time:246685ms step_avg:180.99ms\nstep:1374/5000 train_loss:8.2061 train_time:246865ms step_avg:180.99ms\nstep:1375/5000 train_loss:8.0605 train_time:247046ms step_avg:180.99ms\nstep:1376/5000 train_loss:8.1555 train_time:247227ms step_avg:180.99ms\nstep:1377/5000 train_loss:7.9508 train_time:247408ms step_avg:180.99ms\nstep:1378/5000 train_loss:8.1102 train_time:247589ms step_avg:180.99ms\nstep:1379/5000 train_loss:8.2096 train_time:247772ms step_avg:180.99ms\nstep:1380/5000 train_loss:8.0809 train_time:247959ms step_avg:180.99ms\nstep:1381/5000 train_loss:8.0342 train_time:248142ms step_avg:180.99ms\nstep:1382/5000 train_loss:8.1142 train_time:248324ms step_avg:180.99ms\nstep:1383/5000 train_loss:8.0931 train_time:248505ms step_avg:180.99ms\nstep:1384/5000 train_loss:8.1463 train_time:248685ms step_avg:180.99ms\nstep:1385/5000 train_loss:8.0427 train_time:248865ms step_avg:180.99ms\nstep:1386/5000 train_loss:8.1605 train_time:249046ms step_avg:180.99ms\nstep:1387/5000 train_loss:7.9864 train_time:249225ms step_avg:180.99ms\nstep:1388/5000 train_loss:8.1767 train_time:249407ms step_avg:180.99ms\nstep:1389/5000 train_loss:8.0313 train_time:249587ms step_avg:180.99ms\nstep:1390/5000 train_loss:7.9964 train_time:249768ms step_avg:180.99ms\nstep:1391/5000 train_loss:8.0547 train_time:249949ms step_avg:180.99ms\nstep:1392/5000 train_loss:8.0935 train_time:250130ms step_avg:180.99ms\nstep:1393/5000 train_loss:8.1025 train_time:250312ms step_avg:180.99ms\nstep:1394/5000 train_loss:8.1465 train_time:250493ms step_avg:180.99ms\nstep:1395/5000 train_loss:7.9875 train_time:250675ms step_avg:180.99ms\nstep:1396/5000 train_loss:8.0087 train_time:250857ms step_avg:180.99ms\nstep:1397/5000 train_loss:7.8992 train_time:251040ms step_avg:180.99ms\nstep:1398/5000 train_loss:8.1563 train_time:251223ms step_avg:181.00ms\nstep:1399/5000 train_loss:7.8929 train_time:251404ms step_avg:181.00ms\nstep:1400/5000 train_loss:7.7772 train_time:251584ms step_avg:181.00ms\nstep:1401/5000 train_loss:8.0535 train_time:251764ms step_avg:181.00ms\nstep:1402/5000 train_loss:8.1214 train_time:251945ms step_avg:181.00ms\nstep:1403/5000 train_loss:8.0278 train_time:252125ms step_avg:180.99ms\nstep:1404/5000 train_loss:8.0289 train_time:252308ms step_avg:181.00ms\nstep:1405/5000 train_loss:8.1675 train_time:252490ms step_avg:181.00ms\nstep:1406/5000 train_loss:8.1160 train_time:252671ms step_avg:181.00ms\nstep:1407/5000 train_loss:8.1329 train_time:252852ms step_avg:181.00ms\nstep:1408/5000 train_loss:8.1765 train_time:253034ms step_avg:181.00ms\nstep:1409/5000 train_loss:8.0756 train_time:253217ms step_avg:181.00ms\nstep:1410/5000 train_loss:8.1123 train_time:253400ms step_avg:181.00ms\nstep:1411/5000 train_loss:8.0075 train_time:253582ms step_avg:181.00ms\nstep:1412/5000 train_loss:8.0638 train_time:253762ms step_avg:181.00ms\nstep:1413/5000 train_loss:8.1476 train_time:253942ms step_avg:181.00ms\nstep:1414/5000 train_loss:8.1054 train_time:254124ms step_avg:181.00ms\nstep:1415/5000 train_loss:8.1140 train_time:254307ms step_avg:181.00ms\nstep:1416/5000 train_loss:8.0927 train_time:254488ms step_avg:181.00ms\nstep:1417/5000 train_loss:8.0521 train_time:254668ms step_avg:181.00ms\nstep:1418/5000 train_loss:8.1551 train_time:254849ms step_avg:181.00ms\nstep:1419/5000 train_loss:8.0495 train_time:255030ms step_avg:181.00ms\nstep:1420/5000 train_loss:8.1468 train_time:255212ms step_avg:181.00ms\nstep:1421/5000 train_loss:8.1232 train_time:255395ms step_avg:181.00ms\nstep:1422/5000 train_loss:8.0810 train_time:255578ms step_avg:181.00ms\nstep:1423/5000 train_loss:8.1553 train_time:255760ms step_avg:181.00ms\nstep:1424/5000 train_loss:8.0702 train_time:255942ms step_avg:181.01ms\nstep:1425/5000 train_loss:8.0545 train_time:256124ms step_avg:181.01ms\nstep:1426/5000 train_loss:8.0894 train_time:256305ms step_avg:181.01ms\nstep:1427/5000 train_loss:7.9915 train_time:256485ms step_avg:181.01ms\nstep:1428/5000 train_loss:8.2248 train_time:256666ms step_avg:181.01ms\nstep:1429/5000 train_loss:7.9964 train_time:256847ms step_avg:181.01ms\nstep:1430/5000 train_loss:8.1130 train_time:257028ms step_avg:181.01ms\nstep:1431/5000 train_loss:8.1839 train_time:257210ms step_avg:181.01ms\nstep:1432/5000 train_loss:8.1274 train_time:257391ms step_avg:181.01ms\nstep:1433/5000 train_loss:8.0753 train_time:257572ms step_avg:181.01ms\nstep:1434/5000 train_loss:8.1554 train_time:257758ms step_avg:181.01ms\nstep:1435/5000 train_loss:8.1967 train_time:257937ms step_avg:181.01ms\nstep:1436/5000 train_loss:8.4601 train_time:258121ms step_avg:181.01ms\nstep:1437/5000 train_loss:8.1251 train_time:258303ms step_avg:181.01ms\nstep:1438/5000 train_loss:7.9770 train_time:258485ms step_avg:181.01ms\nstep:1439/5000 train_loss:8.1543 train_time:258666ms step_avg:181.01ms\nstep:1440/5000 train_loss:8.0101 train_time:258847ms step_avg:181.01ms\nstep:1441/5000 train_loss:8.1357 train_time:259027ms step_avg:181.01ms\nstep:1442/5000 train_loss:8.2309 train_time:259208ms step_avg:181.01ms\nstep:1443/5000 train_loss:8.1722 train_time:259389ms step_avg:181.01ms\nstep:1444/5000 train_loss:8.2087 train_time:259570ms step_avg:181.01ms\nstep:1445/5000 train_loss:8.1415 train_time:259751ms step_avg:181.01ms\nstep:1446/5000 train_loss:8.1368 train_time:259934ms step_avg:181.01ms\nstep:1447/5000 train_loss:8.0324 train_time:260116ms step_avg:181.01ms\nstep:1448/5000 train_loss:8.2229 train_time:260298ms step_avg:181.01ms\nstep:1449/5000 train_loss:8.1680 train_time:260479ms step_avg:181.01ms\nstep:1450/5000 train_loss:8.1474 train_time:260663ms step_avg:181.02ms\nstep:1451/5000 train_loss:8.1286 train_time:260844ms step_avg:181.02ms\nstep:1452/5000 train_loss:8.0797 train_time:261025ms step_avg:181.02ms\nstep:1453/5000 train_loss:8.0590 train_time:261207ms step_avg:181.02ms\nstep:1454/5000 train_loss:8.1392 train_time:261387ms step_avg:181.02ms\nstep:1455/5000 train_loss:8.0503 train_time:261568ms step_avg:181.02ms\nstep:1456/5000 train_loss:8.1540 train_time:261750ms step_avg:181.02ms\nstep:1457/5000 train_loss:8.8024 train_time:261931ms step_avg:181.02ms\nstep:1458/5000 train_loss:8.1656 train_time:262115ms step_avg:181.02ms\nstep:1459/5000 train_loss:8.1495 train_time:262298ms step_avg:181.02ms\nstep:1460/5000 train_loss:8.1363 train_time:262480ms step_avg:181.02ms\nstep:1461/5000 train_loss:8.0685 train_time:262661ms step_avg:181.02ms\nstep:1462/5000 train_loss:8.2155 train_time:262843ms step_avg:181.02ms\nstep:1463/5000 train_loss:8.1470 train_time:263024ms step_avg:181.02ms\nstep:1464/5000 train_loss:8.0801 train_time:263205ms step_avg:181.02ms\nstep:1465/5000 train_loss:8.0562 train_time:263385ms step_avg:181.02ms\nstep:1466/5000 train_loss:8.0997 train_time:263565ms step_avg:181.02ms\nstep:1467/5000 train_loss:8.1440 train_time:263745ms step_avg:181.02ms\nstep:1468/5000 train_loss:8.0882 train_time:263926ms step_avg:181.02ms\nstep:1469/5000 train_loss:8.1512 train_time:264108ms step_avg:181.02ms\nstep:1470/5000 train_loss:8.1458 train_time:264289ms step_avg:181.02ms\nstep:1471/5000 train_loss:8.0908 train_time:264471ms step_avg:181.02ms\nstep:1472/5000 train_loss:8.1115 train_time:264653ms step_avg:181.02ms\nstep:1473/5000 train_loss:8.0086 train_time:264836ms step_avg:181.02ms\nstep:1474/5000 train_loss:8.1457 train_time:265019ms step_avg:181.02ms\nstep:1475/5000 train_loss:8.1029 train_time:265201ms step_avg:181.02ms\nstep:1476/5000 train_loss:8.0294 train_time:265381ms step_avg:181.02ms\nstep:1477/5000 train_loss:8.2265 train_time:265563ms step_avg:181.02ms\nstep:1478/5000 train_loss:8.0675 train_time:265744ms step_avg:181.02ms\nstep:1479/5000 train_loss:8.0107 train_time:265925ms step_avg:181.02ms\nstep:1480/5000 train_loss:8.0542 train_time:266106ms step_avg:181.02ms\nstep:1481/5000 train_loss:8.0755 train_time:266288ms step_avg:181.02ms\nstep:1482/5000 train_loss:8.0223 train_time:266468ms step_avg:181.02ms\nstep:1483/5000 train_loss:8.1320 train_time:266648ms step_avg:181.02ms\nstep:1484/5000 train_loss:8.2552 train_time:266828ms step_avg:181.02ms\nstep:1485/5000 train_loss:8.1104 train_time:267010ms step_avg:181.02ms\nstep:1486/5000 train_loss:8.1086 train_time:267192ms step_avg:181.02ms\nstep:1487/5000 train_loss:8.0848 train_time:267373ms step_avg:181.02ms\nstep:1488/5000 train_loss:8.0358 train_time:267556ms step_avg:181.03ms\nstep:1489/5000 train_loss:8.0314 train_time:267738ms step_avg:181.03ms\nstep:1490/5000 train_loss:8.0512 train_time:267921ms step_avg:181.03ms\nstep:1491/5000 train_loss:8.3169 train_time:268102ms step_avg:181.03ms\nstep:1492/5000 train_loss:8.0993 train_time:268284ms step_avg:181.03ms\nstep:1493/5000 train_loss:8.1119 train_time:268465ms step_avg:181.03ms\nstep:1494/5000 train_loss:8.1778 train_time:268645ms step_avg:181.03ms\nstep:1495/5000 train_loss:8.1431 train_time:268826ms step_avg:181.03ms\nstep:1496/5000 train_loss:8.0946 train_time:269007ms step_avg:181.03ms\nstep:1497/5000 train_loss:8.0172 train_time:269188ms step_avg:181.03ms\nstep:1498/5000 train_loss:8.0560 train_time:269369ms step_avg:181.03ms\nstep:1499/5000 train_loss:8.0400 train_time:269550ms step_avg:181.03ms\nstep:1500/5000 train_loss:8.0968 train_time:269731ms step_avg:181.03ms\nstep:1501/5000 train_loss:8.0525 train_time:269913ms step_avg:181.03ms\nstep:1502/5000 train_loss:8.2223 train_time:270096ms step_avg:181.03ms\nstep:1503/5000 train_loss:8.1254 train_time:270277ms step_avg:181.03ms\nstep:1504/5000 train_loss:8.1883 train_time:270459ms step_avg:181.03ms\nstep:1505/5000 train_loss:8.2062 train_time:270640ms step_avg:181.03ms\nstep:1506/5000 train_loss:8.1409 train_time:270823ms step_avg:181.03ms\nstep:1507/5000 train_loss:7.9966 train_time:271004ms step_avg:181.03ms\nstep:1508/5000 train_loss:8.0588 train_time:271184ms step_avg:181.03ms\nstep:1509/5000 train_loss:8.0540 train_time:271366ms step_avg:181.03ms\nstep:1510/5000 train_loss:7.4350 train_time:271546ms step_avg:181.03ms\nstep:1511/5000 train_loss:7.9996 train_time:271726ms step_avg:181.03ms\nstep:1512/5000 train_loss:8.1214 train_time:271906ms step_avg:181.03ms\nstep:1513/5000 train_loss:8.1494 train_time:272087ms step_avg:181.03ms\nstep:1514/5000 train_loss:7.9911 train_time:272267ms step_avg:181.03ms\nstep:1515/5000 train_loss:7.9951 train_time:272448ms step_avg:181.03ms\nstep:1516/5000 train_loss:8.2225 train_time:272628ms step_avg:181.03ms\nstep:1517/5000 train_loss:8.1061 train_time:272811ms step_avg:181.03ms\nstep:1518/5000 train_loss:8.1170 train_time:272992ms step_avg:181.03ms\nstep:1519/5000 train_loss:8.2840 train_time:273174ms step_avg:181.03ms\nstep:1520/5000 train_loss:8.1985 train_time:273357ms step_avg:181.03ms\nstep:1521/5000 train_loss:8.0637 train_time:273540ms step_avg:181.03ms\nstep:1522/5000 train_loss:8.1534 train_time:273722ms step_avg:181.03ms\nstep:1523/5000 train_loss:8.1821 train_time:273903ms step_avg:181.03ms\nstep:1524/5000 train_loss:8.0737 train_time:274315ms step_avg:181.19ms\nstep:1525/5000 train_loss:8.0266 train_time:274495ms step_avg:181.18ms\nstep:1526/5000 train_loss:8.1932 train_time:274676ms step_avg:181.18ms\nstep:1527/5000 train_loss:8.2156 train_time:274857ms step_avg:181.18ms\nstep:1528/5000 train_loss:8.1830 train_time:275038ms step_avg:181.18ms\nstep:1529/5000 train_loss:8.0308 train_time:275230ms step_avg:181.19ms\nstep:1530/5000 train_loss:8.2743 train_time:275411ms step_avg:181.19ms\nstep:1531/5000 train_loss:8.0935 train_time:275592ms step_avg:181.19ms\nstep:1532/5000 train_loss:8.1066 train_time:275773ms step_avg:181.19ms\nstep:1533/5000 train_loss:8.1564 train_time:275953ms step_avg:181.19ms\nstep:1534/5000 train_loss:7.9995 train_time:276137ms step_avg:181.19ms\nstep:1535/5000 train_loss:8.1013 train_time:276322ms step_avg:181.19ms\nstep:1536/5000 train_loss:8.1044 train_time:276503ms step_avg:181.19ms\nstep:1537/5000 train_loss:8.1414 train_time:276682ms step_avg:181.19ms\nstep:1538/5000 train_loss:8.0726 train_time:276862ms step_avg:181.19ms\nstep:1539/5000 train_loss:8.0621 train_time:277045ms step_avg:181.19ms\nstep:1540/5000 train_loss:8.0658 train_time:277226ms step_avg:181.19ms\nstep:1541/5000 train_loss:8.1849 train_time:277409ms step_avg:181.19ms\nstep:1542/5000 train_loss:8.0832 train_time:277590ms step_avg:181.19ms\nstep:1543/5000 train_loss:8.1232 train_time:277771ms step_avg:181.19ms\nstep:1544/5000 train_loss:8.1640 train_time:277952ms step_avg:181.19ms\nstep:1545/5000 train_loss:8.1435 train_time:278134ms step_avg:181.20ms\nstep:1546/5000 train_loss:8.1681 train_time:278317ms step_avg:181.20ms\nstep:1547/5000 train_loss:8.0969 train_time:278500ms step_avg:181.20ms\nstep:1548/5000 train_loss:8.0618 train_time:278681ms step_avg:181.20ms\nstep:1549/5000 train_loss:8.0795 train_time:278861ms step_avg:181.20ms\nstep:1550/5000 train_loss:8.2660 train_time:279041ms step_avg:181.20ms\nstep:1551/5000 train_loss:8.1190 train_time:279223ms step_avg:181.20ms\nstep:1552/5000 train_loss:8.0343 train_time:279405ms step_avg:181.20ms\nstep:1553/5000 train_loss:8.1752 train_time:279586ms step_avg:181.20ms\nstep:1554/5000 train_loss:8.1247 train_time:279767ms step_avg:181.20ms\nstep:1555/5000 train_loss:8.0600 train_time:279947ms step_avg:181.20ms\nstep:1556/5000 train_loss:8.1364 train_time:280127ms step_avg:181.19ms\nstep:1557/5000 train_loss:8.1263 train_time:280309ms step_avg:181.20ms\nstep:1558/5000 train_loss:8.0064 train_time:280490ms step_avg:181.20ms\nstep:1559/5000 train_loss:8.1483 train_time:280671ms step_avg:181.20ms\nstep:1560/5000 train_loss:8.0633 train_time:280855ms step_avg:181.20ms\nstep:1561/5000 train_loss:8.0556 train_time:281036ms step_avg:181.20ms\nstep:1562/5000 train_loss:8.1389 train_time:281219ms step_avg:181.20ms\nstep:1563/5000 train_loss:8.1746 train_time:281402ms step_avg:181.20ms\nstep:1564/5000 train_loss:8.1810 train_time:281583ms step_avg:181.20ms\nstep:1565/5000 train_loss:8.0823 train_time:281764ms step_avg:181.20ms\nstep:1566/5000 train_loss:8.0742 train_time:281945ms step_avg:181.20ms\nstep:1567/5000 train_loss:8.1209 train_time:282127ms step_avg:181.20ms\nstep:1568/5000 train_loss:8.1272 train_time:282309ms step_avg:181.20ms\nstep:1569/5000 train_loss:8.0696 train_time:282490ms step_avg:181.20ms\nstep:1570/5000 train_loss:8.1484 train_time:282672ms step_avg:181.20ms\nstep:1571/5000 train_loss:8.2193 train_time:282855ms step_avg:181.20ms\nstep:1572/5000 train_loss:8.0249 train_time:283037ms step_avg:181.20ms\nstep:1573/5000 train_loss:8.0777 train_time:283220ms step_avg:181.20ms\nstep:1574/5000 train_loss:8.0322 train_time:283401ms step_avg:181.20ms\nstep:1575/5000 train_loss:8.1441 train_time:283581ms step_avg:181.20ms\nstep:1576/5000 train_loss:8.1327 train_time:283763ms step_avg:181.20ms\nstep:1577/5000 train_loss:8.1840 train_time:283944ms step_avg:181.20ms\nstep:1578/5000 train_loss:8.0916 train_time:284126ms step_avg:181.20ms\nstep:1579/5000 train_loss:8.1115 train_time:284307ms step_avg:181.20ms\nstep:1580/5000 train_loss:8.0652 train_time:284488ms step_avg:181.20ms\nstep:1581/5000 train_loss:8.1474 train_time:284668ms step_avg:181.20ms\nstep:1582/5000 train_loss:8.0965 train_time:284850ms step_avg:181.20ms\nstep:1583/5000 train_loss:8.0920 train_time:285030ms step_avg:181.20ms\nstep:1584/5000 train_loss:8.1862 train_time:285212ms step_avg:181.20ms\nstep:1585/5000 train_loss:8.0573 train_time:285396ms step_avg:181.20ms\nstep:1586/5000 train_loss:8.0456 train_time:285578ms step_avg:181.20ms\nstep:1587/5000 train_loss:8.0614 train_time:285760ms step_avg:181.20ms\nstep:1588/5000 train_loss:7.9688 train_time:285941ms step_avg:181.20ms\nstep:1589/5000 train_loss:8.0858 train_time:286123ms step_avg:181.20ms\nstep:1590/5000 train_loss:8.2798 train_time:286304ms step_avg:181.20ms\nstep:1591/5000 train_loss:8.1515 train_time:286484ms step_avg:181.20ms\nstep:1592/5000 train_loss:8.0245 train_time:286666ms step_avg:181.20ms\nstep:1593/5000 train_loss:8.2099 train_time:286846ms step_avg:181.20ms\nstep:1594/5000 train_loss:8.3397 train_time:287028ms step_avg:181.20ms\nstep:1595/5000 train_loss:8.0815 train_time:287210ms step_avg:181.21ms\nstep:1596/5000 train_loss:8.0829 train_time:287392ms step_avg:181.21ms\nstep:1597/5000 train_loss:8.1495 train_time:287575ms step_avg:181.21ms\nstep:1598/5000 train_loss:8.1030 train_time:287756ms step_avg:181.21ms\nstep:1599/5000 train_loss:8.0830 train_time:287939ms step_avg:181.21ms\nstep:1600/5000 train_loss:8.0965 train_time:288121ms step_avg:181.21ms\nstep:1601/5000 train_loss:8.3283 train_time:288302ms step_avg:181.21ms\nstep:1602/5000 train_loss:8.2874 train_time:288484ms step_avg:181.21ms\nstep:1603/5000 train_loss:8.1725 train_time:288664ms step_avg:181.21ms\nstep:1604/5000 train_loss:8.1965 train_time:288845ms step_avg:181.21ms\nstep:1605/5000 train_loss:8.0363 train_time:289026ms step_avg:181.21ms\nstep:1606/5000 train_loss:8.1058 train_time:289207ms step_avg:181.21ms\nstep:1607/5000 train_loss:8.2893 train_time:289388ms step_avg:181.21ms\nstep:1608/5000 train_loss:8.1423 train_time:289569ms step_avg:181.21ms\nstep:1609/5000 train_loss:8.0978 train_time:289749ms step_avg:181.21ms\nstep:1610/5000 train_loss:8.0981 train_time:289929ms step_avg:181.21ms\nstep:1611/5000 train_loss:8.1340 train_time:290112ms step_avg:181.21ms\nstep:1612/5000 train_loss:8.2114 train_time:290294ms step_avg:181.21ms\nstep:1613/5000 train_loss:8.1450 train_time:290475ms step_avg:181.21ms\nstep:1614/5000 train_loss:8.1039 train_time:290656ms step_avg:181.21ms\nstep:1615/5000 train_loss:8.0008 train_time:290839ms step_avg:181.21ms\nstep:1616/5000 train_loss:8.2172 train_time:291021ms step_avg:181.21ms\nstep:1617/5000 train_loss:8.1835 train_time:291202ms step_avg:181.21ms\nstep:1618/5000 train_loss:8.3343 train_time:291382ms step_avg:181.21ms\nstep:1619/5000 train_loss:8.0904 train_time:291563ms step_avg:181.21ms\nstep:1620/5000 train_loss:8.1300 train_time:291745ms step_avg:181.21ms\nstep:1621/5000 train_loss:8.1430 train_time:291926ms step_avg:181.21ms\nstep:1622/5000 train_loss:8.0878 train_time:292108ms step_avg:181.21ms\nstep:1623/5000 train_loss:8.1288 train_time:292289ms step_avg:181.21ms\nstep:1624/5000 train_loss:8.1712 train_time:292469ms step_avg:181.21ms\nstep:1625/5000 train_loss:8.2106 train_time:292651ms step_avg:181.21ms\nstep:1626/5000 train_loss:8.1842 train_time:292832ms step_avg:181.21ms\nstep:1627/5000 train_loss:8.0788 train_time:293016ms step_avg:181.21ms\nstep:1628/5000 train_loss:8.0598 train_time:293198ms step_avg:181.21ms\nstep:1629/5000 train_loss:8.2156 train_time:293379ms step_avg:181.21ms\nstep:1630/5000 train_loss:8.0921 train_time:293560ms step_avg:181.21ms\nstep:1631/5000 train_loss:8.1303 train_time:293742ms step_avg:181.21ms\nstep:1632/5000 train_loss:8.0603 train_time:293924ms step_avg:181.21ms\nstep:1633/5000 train_loss:8.2086 train_time:294106ms step_avg:181.21ms\nstep:1634/5000 train_loss:8.1244 train_time:294287ms step_avg:181.21ms\nstep:1635/5000 train_loss:8.0913 train_time:294468ms step_avg:181.21ms\nstep:1636/5000 train_loss:8.1796 train_time:294648ms step_avg:181.21ms\nstep:1637/5000 train_loss:8.1616 train_time:294828ms step_avg:181.21ms\nstep:1638/5000 train_loss:8.1061 train_time:295010ms step_avg:181.21ms\nstep:1639/5000 train_loss:8.1091 train_time:295191ms step_avg:181.21ms\nstep:1640/5000 train_loss:8.0916 train_time:295373ms step_avg:181.21ms\nstep:1641/5000 train_loss:8.0375 train_time:295555ms step_avg:181.21ms\nstep:1642/5000 train_loss:8.0776 train_time:295737ms step_avg:181.21ms\nstep:1643/5000 train_loss:8.0189 train_time:295919ms step_avg:181.21ms\nstep:1644/5000 train_loss:8.1053 train_time:296102ms step_avg:181.21ms\nstep:1645/5000 train_loss:8.0702 train_time:296283ms step_avg:181.21ms\nstep:1646/5000 train_loss:8.0994 train_time:296464ms step_avg:181.21ms\nstep:1647/5000 train_loss:8.0762 train_time:296645ms step_avg:181.21ms\nstep:1648/5000 train_loss:8.0895 train_time:296825ms step_avg:181.21ms\nstep:1649/5000 train_loss:8.1409 train_time:297006ms step_avg:181.21ms\nstep:1650/5000 train_loss:8.2426 train_time:297186ms step_avg:181.21ms\nstep:1651/5000 train_loss:8.1273 train_time:297366ms step_avg:181.21ms\nstep:1652/5000 train_loss:8.1232 train_time:297547ms step_avg:181.21ms\nstep:1653/5000 train_loss:8.2093 train_time:297734ms step_avg:181.21ms\nstep:1654/5000 train_loss:8.1233 train_time:297940ms step_avg:181.23ms\nstep:1655/5000 train_loss:8.0709 train_time:298120ms step_avg:181.23ms\nstep:1656/5000 train_loss:8.0381 train_time:298301ms step_avg:181.23ms\nstep:1657/5000 train_loss:8.0586 train_time:298481ms step_avg:181.23ms\nstep:1658/5000 train_loss:8.2292 train_time:298663ms step_avg:181.23ms\nstep:1659/5000 train_loss:8.1718 train_time:298845ms step_avg:181.23ms\nstep:1660/5000 train_loss:8.1921 train_time:299026ms step_avg:181.23ms\nstep:1661/5000 train_loss:8.1572 train_time:299207ms step_avg:181.23ms\nstep:1662/5000 train_loss:8.1117 train_time:299388ms step_avg:181.23ms\nstep:1663/5000 train_loss:8.0780 train_time:299569ms step_avg:181.23ms\nstep:1664/5000 train_loss:8.0728 train_time:299750ms step_avg:181.23ms\nstep:1665/5000 train_loss:8.2814 train_time:299931ms step_avg:181.23ms\nstep:1666/5000 train_loss:8.0779 train_time:300115ms step_avg:181.23ms\nstep:1667/5000 train_loss:7.9067 train_time:300297ms step_avg:181.23ms\nstep:1668/5000 train_loss:8.0347 train_time:300479ms step_avg:181.23ms\nstep:1669/5000 train_loss:8.1820 train_time:300661ms step_avg:181.23ms\nstep:1670/5000 train_loss:8.1168 train_time:300842ms step_avg:181.23ms\nstep:1671/5000 train_loss:8.1112 train_time:301024ms step_avg:181.23ms\nstep:1672/5000 train_loss:7.9611 train_time:301205ms step_avg:181.23ms\nstep:1673/5000 train_loss:8.0956 train_time:301386ms step_avg:181.23ms\nstep:1674/5000 train_loss:8.0581 train_time:301567ms step_avg:181.23ms\nstep:1675/5000 train_loss:8.0851 train_time:301748ms step_avg:181.23ms\nstep:1676/5000 train_loss:8.2156 train_time:301928ms step_avg:181.23ms\nstep:1677/5000 train_loss:7.9060 train_time:302111ms step_avg:181.23ms\nstep:1678/5000 train_loss:8.0855 train_time:302291ms step_avg:181.23ms\nstep:1679/5000 train_loss:8.0659 train_time:302474ms step_avg:181.23ms\nstep:1680/5000 train_loss:8.0047 train_time:302656ms step_avg:181.23ms\nstep:1681/5000 train_loss:8.0736 train_time:302839ms step_avg:181.23ms\nstep:1682/5000 train_loss:8.1838 train_time:303022ms step_avg:181.23ms\nstep:1683/5000 train_loss:8.1850 train_time:303204ms step_avg:181.23ms\nstep:1684/5000 train_loss:8.1566 train_time:303384ms step_avg:181.23ms\nstep:1685/5000 train_loss:8.1054 train_time:303564ms step_avg:181.23ms\nstep:1686/5000 train_loss:8.0341 train_time:303746ms step_avg:181.23ms\nstep:1687/5000 train_loss:8.0093 train_time:303928ms step_avg:181.23ms\nstep:1688/5000 train_loss:8.1417 train_time:304111ms step_avg:181.23ms\nstep:1689/5000 train_loss:8.1843 train_time:304294ms step_avg:181.24ms\nstep:1690/5000 train_loss:7.9690 train_time:304476ms step_avg:181.24ms\nstep:1691/5000 train_loss:8.1671 train_time:304659ms step_avg:181.24ms\nstep:1692/5000 train_loss:8.0825 train_time:304840ms step_avg:181.24ms\nstep:1693/5000 train_loss:8.0514 train_time:305023ms step_avg:181.24ms\nstep:1694/5000 train_loss:8.0985 train_time:305206ms step_avg:181.24ms\nstep:1695/5000 train_loss:8.1128 train_time:305385ms step_avg:181.24ms\nstep:1696/5000 train_loss:8.0126 train_time:305566ms step_avg:181.24ms\nstep:1697/5000 train_loss:8.1724 train_time:305747ms step_avg:181.24ms\nstep:1698/5000 train_loss:8.1276 train_time:305926ms step_avg:181.24ms\nstep:1699/5000 train_loss:8.1421 train_time:306108ms step_avg:181.24ms\nstep:1700/5000 train_loss:8.0765 train_time:306289ms step_avg:181.24ms\nstep:1701/5000 train_loss:8.0671 train_time:306471ms step_avg:181.24ms\nstep:1702/5000 train_loss:8.0617 train_time:306654ms step_avg:181.24ms\nstep:1703/5000 train_loss:8.1095 train_time:306835ms step_avg:181.24ms\nstep:1704/5000 train_loss:8.1685 train_time:307018ms step_avg:181.24ms\nstep:1705/5000 train_loss:8.1038 train_time:307200ms step_avg:181.24ms\nstep:1706/5000 train_loss:8.0730 train_time:307382ms step_avg:181.24ms\nstep:1707/5000 train_loss:8.1832 train_time:307562ms step_avg:181.24ms\nstep:1708/5000 train_loss:8.1351 train_time:307743ms step_avg:181.24ms\nstep:1709/5000 train_loss:8.0589 train_time:307924ms step_avg:181.24ms\nstep:1710/5000 train_loss:8.1854 train_time:308105ms step_avg:181.24ms\nstep:1711/5000 train_loss:8.1349 train_time:308286ms step_avg:181.24ms\nstep:1712/5000 train_loss:7.9595 train_time:308467ms step_avg:181.24ms\nstep:1713/5000 train_loss:8.0903 train_time:308648ms step_avg:181.24ms\nstep:1714/5000 train_loss:8.2370 train_time:308829ms step_avg:181.24ms\nstep:1715/5000 train_loss:8.2044 train_time:309012ms step_avg:181.24ms\nstep:1716/5000 train_loss:8.0655 train_time:309194ms step_avg:181.24ms\nstep:1717/5000 train_loss:7.9586 train_time:309376ms step_avg:181.24ms\nstep:1718/5000 train_loss:8.0469 train_time:309557ms step_avg:181.24ms\nstep:1719/5000 train_loss:8.0440 train_time:309740ms step_avg:181.24ms\nstep:1720/5000 train_loss:8.1127 train_time:309922ms step_avg:181.24ms\nstep:1721/5000 train_loss:8.1292 train_time:310104ms step_avg:181.24ms\nstep:1722/5000 train_loss:8.0582 train_time:310284ms step_avg:181.24ms\nstep:1723/5000 train_loss:8.0742 train_time:310465ms step_avg:181.24ms\nstep:1724/5000 train_loss:8.0588 train_time:310646ms step_avg:181.24ms\nstep:1725/5000 train_loss:7.9687 train_time:310826ms step_avg:181.24ms\nstep:1726/5000 train_loss:8.1953 train_time:311008ms step_avg:181.24ms\nstep:1727/5000 train_loss:8.3190 train_time:311189ms step_avg:181.24ms\nstep:1728/5000 train_loss:8.1279 train_time:311371ms step_avg:181.24ms\nstep:1729/5000 train_loss:8.2087 train_time:311552ms step_avg:181.24ms\nstep:1730/5000 train_loss:8.1055 train_time:311734ms step_avg:181.24ms\nstep:1731/5000 train_loss:8.1542 train_time:311917ms step_avg:181.24ms\nstep:1732/5000 train_loss:8.1991 train_time:312098ms step_avg:181.24ms\nstep:1733/5000 train_loss:8.1435 train_time:312281ms step_avg:181.24ms\nstep:1734/5000 train_loss:8.1989 train_time:312462ms step_avg:181.24ms\nstep:1735/5000 train_loss:8.0580 train_time:312642ms step_avg:181.24ms\nstep:1736/5000 train_loss:8.1422 train_time:312824ms step_avg:181.24ms\nstep:1737/5000 train_loss:8.0434 train_time:313006ms step_avg:181.24ms\nstep:1738/5000 train_loss:8.1248 train_time:313186ms step_avg:181.24ms\nstep:1739/5000 train_loss:8.1313 train_time:313367ms step_avg:181.24ms\nstep:1740/5000 train_loss:8.0494 train_time:313547ms step_avg:181.24ms\nstep:1741/5000 train_loss:8.1187 train_time:313726ms step_avg:181.24ms\nstep:1742/5000 train_loss:8.1110 train_time:313906ms step_avg:181.24ms\nstep:1743/5000 train_loss:8.1488 train_time:314087ms step_avg:181.24ms\nstep:1744/5000 train_loss:8.1365 train_time:314267ms step_avg:181.24ms\nstep:1745/5000 train_loss:8.0879 train_time:314448ms step_avg:181.24ms\nstep:1746/5000 train_loss:8.1206 train_time:314629ms step_avg:181.24ms\nstep:1747/5000 train_loss:8.1418 train_time:314812ms step_avg:181.24ms\nstep:1748/5000 train_loss:8.2360 train_time:314993ms step_avg:181.24ms\nstep:1749/5000 train_loss:8.1192 train_time:315176ms step_avg:181.24ms\nstep:1750/5000 train_loss:8.1329 train_time:315359ms step_avg:181.24ms\nstep:1751/5000 train_loss:8.2563 train_time:315542ms step_avg:181.24ms\nstep:1752/5000 train_loss:8.1272 train_time:315724ms step_avg:181.24ms\nstep:1753/5000 train_loss:8.1468 train_time:315906ms step_avg:181.24ms\nstep:1754/5000 train_loss:8.0982 train_time:316088ms step_avg:181.24ms\nstep:1755/5000 train_loss:8.0623 train_time:316268ms step_avg:181.24ms\nstep:1756/5000 train_loss:8.1578 train_time:316449ms step_avg:181.24ms\nstep:1757/5000 train_loss:8.1425 train_time:316629ms step_avg:181.24ms\nstep:1758/5000 train_loss:8.0201 train_time:316811ms step_avg:181.24ms\nstep:1759/5000 train_loss:8.1729 train_time:316993ms step_avg:181.24ms\nstep:1760/5000 train_loss:8.0612 train_time:317176ms step_avg:181.24ms\nstep:1761/5000 train_loss:8.1690 train_time:317358ms step_avg:181.24ms\nstep:1762/5000 train_loss:8.1406 train_time:317540ms step_avg:181.24ms\nstep:1763/5000 train_loss:8.0294 train_time:317722ms step_avg:181.24ms\nstep:1764/5000 train_loss:8.0410 train_time:317903ms step_avg:181.24ms\nstep:1765/5000 train_loss:8.0056 train_time:318085ms step_avg:181.24ms\nstep:1766/5000 train_loss:8.1262 train_time:318265ms step_avg:181.24ms\nstep:1767/5000 train_loss:8.0924 train_time:318445ms step_avg:181.24ms\nstep:1768/5000 train_loss:8.1634 train_time:318625ms step_avg:181.24ms\nstep:1769/5000 train_loss:8.3233 train_time:318806ms step_avg:181.24ms\nstep:1770/5000 train_loss:8.1540 train_time:318986ms step_avg:181.24ms\nstep:1771/5000 train_loss:8.1105 train_time:319166ms step_avg:181.24ms\nstep:1772/5000 train_loss:8.1099 train_time:319346ms step_avg:181.24ms\nstep:1773/5000 train_loss:8.1083 train_time:319526ms step_avg:181.24ms\nstep:1774/5000 train_loss:8.0440 train_time:319709ms step_avg:181.24ms\nstep:1775/5000 train_loss:8.0783 train_time:319890ms step_avg:181.24ms\nstep:1776/5000 train_loss:8.1404 train_time:320072ms step_avg:181.24ms\nstep:1777/5000 train_loss:8.1465 train_time:320255ms step_avg:181.24ms\nstep:1778/5000 train_loss:8.0891 train_time:320437ms step_avg:181.24ms\nstep:1779/5000 train_loss:8.1259 train_time:320619ms step_avg:181.24ms\nstep:1780/5000 train_loss:8.1109 train_time:320801ms step_avg:181.24ms\nstep:1781/5000 train_loss:8.1178 train_time:320982ms step_avg:181.24ms\nstep:1782/5000 train_loss:8.0878 train_time:321162ms step_avg:181.24ms\nstep:1783/5000 train_loss:8.1442 train_time:321344ms step_avg:181.24ms\nstep:1784/5000 train_loss:8.0690 train_time:321526ms step_avg:181.24ms\nstep:1785/5000 train_loss:8.1342 train_time:321708ms step_avg:181.24ms\nstep:1786/5000 train_loss:8.1466 train_time:321889ms step_avg:181.24ms\nstep:1787/5000 train_loss:8.1047 train_time:322069ms step_avg:181.24ms\nstep:1788/5000 train_loss:8.1069 train_time:322252ms step_avg:181.24ms\nstep:1789/5000 train_loss:8.2161 train_time:322433ms step_avg:181.24ms\nstep:1790/5000 train_loss:8.1068 train_time:322617ms step_avg:181.25ms\nstep:1791/5000 train_loss:8.1234 train_time:322800ms step_avg:181.25ms\nstep:1792/5000 train_loss:8.0882 train_time:322980ms step_avg:181.25ms\nstep:1793/5000 train_loss:8.1488 train_time:323161ms step_avg:181.25ms\nstep:1794/5000 train_loss:8.0765 train_time:323342ms step_avg:181.25ms\nstep:1795/5000 train_loss:8.0627 train_time:323523ms step_avg:181.25ms\nstep:1796/5000 train_loss:8.0018 train_time:323706ms step_avg:181.25ms\nstep:1797/5000 train_loss:8.1882 train_time:323886ms step_avg:181.25ms\nstep:1798/5000 train_loss:7.9909 train_time:324067ms step_avg:181.25ms\nstep:1799/5000 train_loss:8.1585 train_time:324248ms step_avg:181.25ms\nstep:1800/5000 train_loss:8.1036 train_time:324429ms step_avg:181.25ms\nstep:1801/5000 train_loss:8.2241 train_time:324613ms step_avg:181.25ms\nstep:1802/5000 train_loss:7.9317 train_time:324796ms step_avg:181.25ms\nstep:1803/5000 train_loss:8.1830 train_time:324978ms step_avg:181.25ms\nstep:1804/5000 train_loss:8.1547 train_time:325160ms step_avg:181.25ms\nstep:1805/5000 train_loss:8.0950 train_time:325341ms step_avg:181.25ms\nstep:1806/5000 train_loss:8.0522 train_time:325522ms step_avg:181.25ms\nstep:1807/5000 train_loss:8.0626 train_time:325705ms step_avg:181.25ms\nstep:1808/5000 train_loss:8.1530 train_time:325886ms step_avg:181.25ms\nstep:1809/5000 train_loss:8.0992 train_time:326066ms step_avg:181.25ms\nstep:1810/5000 train_loss:8.0755 train_time:326246ms step_avg:181.25ms\nstep:1811/5000 train_loss:8.1421 train_time:326426ms step_avg:181.25ms\nstep:1812/5000 train_loss:8.2119 train_time:326608ms step_avg:181.25ms\nstep:1813/5000 train_loss:8.0820 train_time:326789ms step_avg:181.25ms\nstep:1814/5000 train_loss:8.0453 train_time:326971ms step_avg:181.25ms\nstep:1815/5000 train_loss:8.0721 train_time:327154ms step_avg:181.25ms\nstep:1816/5000 train_loss:7.9952 train_time:327336ms step_avg:181.25ms\nstep:1817/5000 train_loss:8.1007 train_time:327518ms step_avg:181.25ms\nstep:1818/5000 train_loss:8.0571 train_time:327701ms step_avg:181.25ms\nstep:1819/5000 train_loss:7.9433 train_time:327881ms step_avg:181.25ms\nstep:1820/5000 train_loss:8.0644 train_time:328061ms step_avg:181.25ms\nstep:1821/5000 train_loss:8.1952 train_time:328243ms step_avg:181.25ms\nstep:1822/5000 train_loss:8.1027 train_time:328423ms step_avg:181.25ms\nstep:1823/5000 train_loss:8.0210 train_time:328606ms step_avg:181.25ms\nstep:1824/5000 train_loss:8.1538 train_time:328787ms step_avg:181.25ms\nstep:1825/5000 train_loss:8.1612 train_time:328967ms step_avg:181.25ms\nstep:1826/5000 train_loss:8.0711 train_time:329147ms step_avg:181.25ms\nstep:1827/5000 train_loss:8.1094 train_time:329327ms step_avg:181.25ms\nstep:1828/5000 train_loss:8.1578 train_time:329508ms step_avg:181.25ms\nstep:1829/5000 train_loss:8.1753 train_time:329690ms step_avg:181.25ms\nstep:1830/5000 train_loss:8.1394 train_time:329872ms step_avg:181.25ms\nstep:1831/5000 train_loss:7.9936 train_time:330056ms step_avg:181.25ms\nstep:1832/5000 train_loss:8.1485 train_time:330238ms step_avg:181.25ms\nstep:1833/5000 train_loss:8.0352 train_time:330421ms step_avg:181.25ms\nstep:1834/5000 train_loss:8.1601 train_time:330602ms step_avg:181.25ms\nstep:1835/5000 train_loss:8.0836 train_time:330783ms step_avg:181.25ms\nstep:1836/5000 train_loss:8.1491 train_time:330964ms step_avg:181.25ms\nstep:1837/5000 train_loss:8.0693 train_time:331146ms step_avg:181.25ms\nstep:1838/5000 train_loss:8.0121 train_time:331326ms step_avg:181.25ms\nstep:1839/5000 train_loss:8.0481 train_time:331508ms step_avg:181.25ms\nstep:1840/5000 train_loss:8.1787 train_time:331688ms step_avg:181.25ms\nstep:1841/5000 train_loss:8.1143 train_time:331869ms step_avg:181.25ms\nstep:1842/5000 train_loss:8.0555 train_time:332050ms step_avg:181.25ms\nstep:1843/5000 train_loss:8.1079 train_time:332232ms step_avg:181.25ms\nstep:1844/5000 train_loss:8.1819 train_time:332415ms step_avg:181.25ms\nstep:1845/5000 train_loss:8.1261 train_time:332596ms step_avg:181.25ms\nstep:1846/5000 train_loss:8.1334 train_time:332778ms step_avg:181.25ms\nstep:1847/5000 train_loss:8.0957 train_time:332960ms step_avg:181.25ms\nstep:1848/5000 train_loss:8.1067 train_time:333141ms step_avg:181.25ms\nstep:1849/5000 train_loss:8.0947 train_time:333321ms step_avg:181.25ms\nstep:1850/5000 train_loss:7.9409 train_time:333502ms step_avg:181.25ms\nstep:1851/5000 train_loss:8.2172 train_time:333683ms step_avg:181.25ms\nstep:1852/5000 train_loss:7.9637 train_time:333864ms step_avg:181.25ms\nstep:1853/5000 train_loss:8.0099 train_time:334046ms step_avg:181.25ms\nstep:1854/5000 train_loss:8.1616 train_time:334226ms step_avg:181.25ms\nstep:1855/5000 train_loss:8.2986 train_time:334406ms step_avg:181.25ms\nstep:1856/5000 train_loss:8.0753 train_time:334586ms step_avg:181.25ms\nstep:1857/5000 train_loss:8.1688 train_time:334767ms step_avg:181.25ms\nstep:1858/5000 train_loss:8.1123 train_time:334948ms step_avg:181.25ms\nstep:1859/5000 train_loss:8.0992 train_time:335129ms step_avg:181.25ms\nstep:1860/5000 train_loss:8.2073 train_time:335312ms step_avg:181.25ms\nstep:1861/5000 train_loss:8.1671 train_time:335494ms step_avg:181.25ms\nstep:1862/5000 train_loss:8.1034 train_time:335677ms step_avg:181.25ms\nstep:1863/5000 train_loss:8.0677 train_time:335859ms step_avg:181.25ms\nstep:1864/5000 train_loss:8.0093 train_time:336041ms step_avg:181.25ms\nstep:1865/5000 train_loss:8.1577 train_time:336224ms step_avg:181.25ms\nstep:1866/5000 train_loss:8.1826 train_time:336406ms step_avg:181.25ms\nstep:1867/5000 train_loss:8.1560 train_time:336586ms step_avg:181.25ms\nstep:1868/5000 train_loss:8.1237 train_time:336767ms step_avg:181.25ms\nstep:1869/5000 train_loss:8.1970 train_time:336948ms step_avg:181.25ms\nstep:1870/5000 train_loss:8.3069 train_time:337127ms step_avg:181.25ms\nstep:1871/5000 train_loss:8.1162 train_time:337309ms step_avg:181.25ms\nstep:1872/5000 train_loss:8.0773 train_time:337490ms step_avg:181.25ms\nstep:1873/5000 train_loss:8.1536 train_time:337672ms step_avg:181.25ms\nstep:1874/5000 train_loss:8.1262 train_time:337855ms step_avg:181.25ms\nstep:1875/5000 train_loss:8.1521 train_time:338037ms step_avg:181.25ms\nstep:1876/5000 train_loss:8.1676 train_time:338219ms step_avg:181.25ms\nstep:1877/5000 train_loss:8.1927 train_time:338401ms step_avg:181.25ms\nstep:1878/5000 train_loss:8.1301 train_time:338582ms step_avg:181.25ms\nstep:1879/5000 train_loss:8.1301 train_time:338764ms step_avg:181.25ms\nstep:1880/5000 train_loss:8.1386 train_time:338946ms step_avg:181.25ms\nstep:1881/5000 train_loss:8.0127 train_time:339127ms step_avg:181.25ms\nstep:1882/5000 train_loss:8.1570 train_time:339308ms step_avg:181.25ms\nstep:1883/5000 train_loss:8.1990 train_time:339489ms step_avg:181.25ms\nstep:1884/5000 train_loss:8.0320 train_time:339670ms step_avg:181.25ms\nstep:1885/5000 train_loss:7.9445 train_time:339852ms step_avg:181.25ms\nstep:1886/5000 train_loss:8.1240 train_time:340034ms step_avg:181.25ms\nstep:1887/5000 train_loss:8.4625 train_time:340217ms step_avg:181.26ms\nstep:1888/5000 train_loss:8.0990 train_time:340399ms step_avg:181.26ms\nstep:1889/5000 train_loss:8.0753 train_time:340581ms step_avg:181.26ms\nstep:1890/5000 train_loss:8.0831 train_time:340762ms step_avg:181.26ms\nstep:1891/5000 train_loss:8.1756 train_time:340944ms step_avg:181.26ms\nstep:1892/5000 train_loss:8.1328 train_time:341125ms step_avg:181.26ms\nstep:1893/5000 train_loss:8.1926 train_time:341306ms step_avg:181.26ms\nstep:1894/5000 train_loss:8.1051 train_time:341488ms step_avg:181.26ms\nstep:1895/5000 train_loss:8.0839 train_time:341668ms step_avg:181.26ms\nstep:1896/5000 train_loss:8.1961 train_time:341849ms step_avg:181.26ms\nstep:1897/5000 train_loss:8.1092 train_time:342029ms step_avg:181.26ms\nstep:1898/5000 train_loss:8.0553 train_time:342212ms step_avg:181.26ms\nstep:1899/5000 train_loss:8.2838 train_time:342394ms step_avg:181.26ms\nstep:1900/5000 train_loss:8.0729 train_time:342576ms step_avg:181.26ms\nstep:1901/5000 train_loss:8.1198 train_time:342759ms step_avg:181.26ms\nstep:1902/5000 train_loss:8.1716 train_time:342941ms step_avg:181.26ms\nstep:1903/5000 train_loss:8.0022 train_time:343122ms step_avg:181.26ms\nstep:1904/5000 train_loss:8.0112 train_time:343305ms step_avg:181.26ms\nstep:1905/5000 train_loss:8.1389 train_time:343485ms step_avg:181.26ms\nstep:1906/5000 train_loss:8.1333 train_time:343665ms step_avg:181.26ms\nstep:1907/5000 train_loss:8.2034 train_time:343845ms step_avg:181.26ms\nstep:1908/5000 train_loss:8.1743 train_time:344026ms step_avg:181.26ms\nstep:1909/5000 train_loss:7.9499 train_time:344208ms step_avg:181.26ms\nstep:1910/5000 train_loss:8.1357 train_time:344389ms step_avg:181.26ms\nstep:1911/5000 train_loss:8.2124 train_time:344570ms step_avg:181.26ms\nstep:1912/5000 train_loss:8.1765 train_time:344751ms step_avg:181.26ms\nstep:1913/5000 train_loss:7.9919 train_time:344932ms step_avg:181.26ms\nstep:1914/5000 train_loss:8.0478 train_time:345114ms step_avg:181.26ms\nstep:1915/5000 train_loss:8.0554 train_time:345297ms step_avg:181.26ms\nstep:1916/5000 train_loss:8.1388 train_time:345480ms step_avg:181.26ms\nstep:1917/5000 train_loss:8.0889 train_time:345661ms step_avg:181.26ms\nstep:1918/5000 train_loss:8.1325 train_time:345842ms step_avg:181.26ms\nstep:1919/5000 train_loss:8.1207 train_time:346024ms step_avg:181.26ms\nstep:1920/5000 train_loss:8.0420 train_time:346205ms step_avg:181.26ms\nstep:1921/5000 train_loss:8.2112 train_time:346384ms step_avg:181.26ms\nstep:1922/5000 train_loss:8.1764 train_time:346566ms step_avg:181.26ms\nstep:1923/5000 train_loss:8.0655 train_time:346746ms step_avg:181.26ms\nstep:1924/5000 train_loss:8.1970 train_time:346926ms step_avg:181.26ms\nstep:1925/5000 train_loss:8.0479 train_time:347108ms step_avg:181.26ms\nstep:1926/5000 train_loss:8.1106 train_time:347289ms step_avg:181.26ms\nstep:1927/5000 train_loss:8.0284 train_time:347470ms step_avg:181.26ms\nstep:1928/5000 train_loss:8.1868 train_time:347653ms step_avg:181.26ms\nstep:1929/5000 train_loss:8.0880 train_time:347835ms step_avg:181.26ms\nstep:1930/5000 train_loss:8.0010 train_time:348018ms step_avg:181.26ms\nstep:1931/5000 train_loss:8.0869 train_time:348201ms step_avg:181.26ms\nstep:1932/5000 train_loss:8.0295 train_time:348381ms step_avg:181.26ms\nstep:1933/5000 train_loss:7.9390 train_time:348562ms step_avg:181.26ms\nstep:1934/5000 train_loss:8.1267 train_time:348743ms step_avg:181.26ms\nstep:1935/5000 train_loss:8.0574 train_time:348924ms step_avg:181.26ms\nstep:1936/5000 train_loss:8.0955 train_time:349107ms step_avg:181.26ms\nstep:1937/5000 train_loss:8.0859 train_time:349288ms step_avg:181.26ms\nstep:1938/5000 train_loss:8.1236 train_time:349468ms step_avg:181.26ms\nstep:1939/5000 train_loss:8.1177 train_time:349648ms step_avg:181.26ms\nstep:1940/5000 train_loss:8.1101 train_time:349829ms step_avg:181.26ms\nstep:1941/5000 train_loss:8.1572 train_time:350011ms step_avg:181.26ms\nstep:1942/5000 train_loss:8.1225 train_time:350193ms step_avg:181.26ms\nstep:1943/5000 train_loss:8.1012 train_time:350374ms step_avg:181.26ms\nstep:1944/5000 train_loss:8.0370 train_time:350556ms step_avg:181.26ms\nstep:1945/5000 train_loss:8.0830 train_time:350738ms step_avg:181.26ms\nstep:1946/5000 train_loss:8.1294 train_time:350920ms step_avg:181.26ms\nstep:1947/5000 train_loss:8.1059 train_time:351102ms step_avg:181.26ms\nstep:1948/5000 train_loss:8.2560 train_time:351283ms step_avg:181.26ms\nstep:1949/5000 train_loss:8.0699 train_time:351464ms step_avg:181.26ms\nstep:1950/5000 train_loss:8.0149 train_time:351645ms step_avg:181.26ms\nstep:1951/5000 train_loss:8.1124 train_time:351826ms step_avg:181.26ms\nstep:1952/5000 train_loss:8.1463 train_time:352008ms step_avg:181.26ms\nstep:1953/5000 train_loss:8.0744 train_time:352189ms step_avg:181.26ms\nstep:1954/5000 train_loss:8.2279 train_time:352370ms step_avg:181.26ms\nstep:1955/5000 train_loss:8.1310 train_time:352551ms step_avg:181.26ms\nstep:1956/5000 train_loss:8.0286 train_time:352732ms step_avg:181.26ms\nstep:1957/5000 train_loss:8.1105 train_time:352915ms step_avg:181.26ms\nstep:1958/5000 train_loss:8.1016 train_time:353097ms step_avg:181.26ms\nstep:1959/5000 train_loss:8.0680 train_time:353279ms step_avg:181.26ms\nstep:1960/5000 train_loss:8.1578 train_time:353460ms step_avg:181.26ms\nstep:1961/5000 train_loss:8.0735 train_time:353641ms step_avg:181.26ms\nstep:1962/5000 train_loss:8.1163 train_time:353822ms step_avg:181.26ms\nstep:1963/5000 train_loss:8.0539 train_time:354005ms step_avg:181.26ms\nstep:1964/5000 train_loss:8.1162 train_time:354185ms step_avg:181.26ms\nstep:1965/5000 train_loss:7.9611 train_time:354366ms step_avg:181.26ms\nstep:1966/5000 train_loss:8.0563 train_time:354545ms step_avg:181.26ms\nstep:1967/5000 train_loss:8.1922 train_time:354725ms step_avg:181.26ms\nstep:1968/5000 train_loss:8.0854 train_time:354906ms step_avg:181.26ms\nstep:1969/5000 train_loss:8.0233 train_time:355087ms step_avg:181.26ms\nstep:1970/5000 train_loss:8.1462 train_time:355268ms step_avg:181.26ms\nstep:1971/5000 train_loss:8.0831 train_time:355451ms step_avg:181.26ms\nstep:1972/5000 train_loss:8.1621 train_time:355631ms step_avg:181.26ms\nstep:1973/5000 train_loss:8.3059 train_time:355813ms step_avg:181.26ms\nstep:1974/5000 train_loss:8.0227 train_time:355995ms step_avg:181.26ms\nstep:1975/5000 train_loss:8.0545 train_time:356177ms step_avg:181.26ms\nstep:1976/5000 train_loss:8.0825 train_time:356359ms step_avg:181.26ms\nstep:1977/5000 train_loss:8.0952 train_time:356541ms step_avg:181.26ms\nstep:1978/5000 train_loss:8.1422 train_time:356722ms step_avg:181.26ms\nstep:1979/5000 train_loss:8.0678 train_time:356904ms step_avg:181.26ms\nstep:1980/5000 train_loss:8.1209 train_time:357085ms step_avg:181.26ms\nstep:1981/5000 train_loss:8.0904 train_time:357265ms step_avg:181.26ms\nstep:1982/5000 train_loss:7.9919 train_time:357446ms step_avg:181.26ms\nstep:1983/5000 train_loss:8.1730 train_time:357625ms step_avg:181.26ms\nstep:1984/5000 train_loss:8.1477 train_time:357806ms step_avg:181.26ms\nstep:1985/5000 train_loss:8.0915 train_time:357986ms step_avg:181.26ms\nstep:1986/5000 train_loss:8.2826 train_time:358167ms step_avg:181.26ms\nstep:1987/5000 train_loss:8.3033 train_time:358349ms step_avg:181.26ms\nstep:1988/5000 train_loss:8.0945 train_time:358530ms step_avg:181.26ms\nstep:1989/5000 train_loss:8.1038 train_time:358712ms step_avg:181.26ms\nstep:1990/5000 train_loss:8.1513 train_time:358894ms step_avg:181.26ms\nstep:1991/5000 train_loss:8.0940 train_time:359076ms step_avg:181.26ms\nstep:1992/5000 train_loss:8.0075 train_time:359258ms step_avg:181.26ms\nstep:1993/5000 train_loss:8.4458 train_time:359440ms step_avg:181.26ms\nstep:1994/5000 train_loss:8.2183 train_time:359621ms step_avg:181.26ms\nstep:1995/5000 train_loss:8.2056 train_time:359802ms step_avg:181.26ms\nstep:1996/5000 train_loss:8.0817 train_time:359983ms step_avg:181.26ms\nstep:1997/5000 train_loss:8.0951 train_time:360165ms step_avg:181.26ms\nstep:1998/5000 train_loss:8.1700 train_time:360347ms step_avg:181.26ms\nstep:1999/5000 train_loss:8.0873 train_time:360528ms step_avg:181.26ms\nstep:2000/5000 train_loss:8.1780 train_time:360709ms step_avg:181.26ms\nstep:2001/5000 train_loss:8.2187 train_time:360891ms step_avg:181.26ms\nstep:2002/5000 train_loss:8.1895 train_time:361073ms step_avg:181.26ms\nstep:2003/5000 train_loss:8.1558 train_time:361255ms step_avg:181.26ms\nstep:2004/5000 train_loss:8.1472 train_time:361438ms step_avg:181.26ms\nstep:2005/5000 train_loss:8.1176 train_time:361621ms step_avg:181.26ms\nstep:2006/5000 train_loss:8.0140 train_time:361802ms step_avg:181.26ms\nstep:2007/5000 train_loss:8.2361 train_time:361983ms step_avg:181.26ms\nstep:2008/5000 train_loss:8.0099 train_time:362164ms step_avg:181.26ms\nstep:2009/5000 train_loss:8.1388 train_time:362344ms step_avg:181.26ms\nstep:2010/5000 train_loss:8.0960 train_time:362525ms step_avg:181.26ms\nstep:2011/5000 train_loss:8.0508 train_time:362707ms step_avg:181.26ms\nstep:2012/5000 train_loss:8.0312 train_time:362888ms step_avg:181.26ms\nstep:2013/5000 train_loss:7.9563 train_time:363069ms step_avg:181.26ms\nstep:2014/5000 train_loss:8.0631 train_time:363251ms step_avg:181.26ms\nstep:2015/5000 train_loss:8.0521 train_time:363432ms step_avg:181.26ms\nstep:2016/5000 train_loss:8.1294 train_time:363615ms step_avg:181.26ms\nstep:2017/5000 train_loss:8.0095 train_time:363797ms step_avg:181.26ms\nstep:2018/5000 train_loss:8.1639 train_time:363980ms step_avg:181.26ms\nstep:2019/5000 train_loss:8.1483 train_time:364161ms step_avg:181.26ms\nstep:2020/5000 train_loss:8.3180 train_time:364343ms step_avg:181.26ms\nstep:2021/5000 train_loss:7.9125 train_time:364524ms step_avg:181.27ms\nstep:2022/5000 train_loss:8.2754 train_time:364705ms step_avg:181.27ms\nstep:2023/5000 train_loss:7.9487 train_time:364885ms step_avg:181.26ms\nstep:2024/5000 train_loss:8.1544 train_time:365065ms step_avg:181.26ms\nstep:2025/5000 train_loss:8.1163 train_time:365246ms step_avg:181.26ms\nstep:2026/5000 train_loss:7.9876 train_time:365425ms step_avg:181.26ms\nstep:2027/5000 train_loss:8.0276 train_time:365606ms step_avg:181.26ms\nstep:2028/5000 train_loss:8.1328 train_time:365786ms step_avg:181.26ms\nstep:2029/5000 train_loss:7.9987 train_time:365967ms step_avg:181.26ms\nstep:2030/5000 train_loss:8.1462 train_time:366146ms step_avg:181.26ms\nstep:2031/5000 train_loss:8.1134 train_time:366327ms step_avg:181.26ms\nstep:2032/5000 train_loss:8.0773 train_time:366509ms step_avg:181.26ms\nstep:2033/5000 train_loss:8.1524 train_time:366689ms step_avg:181.26ms\nstep:2034/5000 train_loss:8.2326 train_time:366872ms step_avg:181.26ms\nstep:2035/5000 train_loss:8.1248 train_time:367055ms step_avg:181.26ms\nstep:2036/5000 train_loss:8.0504 train_time:367237ms step_avg:181.26ms\nstep:2037/5000 train_loss:8.2111 train_time:367418ms step_avg:181.26ms\nstep:2038/5000 train_loss:8.1419 train_time:367600ms step_avg:181.26ms\nstep:2039/5000 train_loss:8.1635 train_time:367781ms step_avg:181.26ms\nstep:2040/5000 train_loss:8.0921 train_time:367963ms step_avg:181.26ms\nstep:2041/5000 train_loss:8.1269 train_time:368145ms step_avg:181.26ms\nstep:2042/5000 train_loss:8.0791 train_time:368326ms step_avg:181.26ms\nstep:2043/5000 train_loss:8.0661 train_time:368506ms step_avg:181.26ms\nstep:2044/5000 train_loss:8.1276 train_time:368686ms step_avg:181.26ms\nstep:2045/5000 train_loss:8.1341 train_time:368866ms step_avg:181.26ms\nstep:2046/5000 train_loss:8.1860 train_time:369047ms step_avg:181.26ms\nstep:2047/5000 train_loss:8.0085 train_time:369228ms step_avg:181.26ms\nstep:2048/5000 train_loss:7.9542 train_time:369409ms step_avg:181.26ms\nstep:2049/5000 train_loss:8.0940 train_time:369590ms step_avg:181.26ms\nstep:2050/5000 train_loss:8.0119 train_time:369772ms step_avg:181.26ms\nstep:2051/5000 train_loss:8.1148 train_time:369955ms step_avg:181.26ms\nstep:2052/5000 train_loss:8.0631 train_time:370137ms step_avg:181.26ms\nstep:2053/5000 train_loss:8.1364 train_time:370320ms step_avg:181.26ms\nstep:2054/5000 train_loss:8.1847 train_time:370501ms step_avg:181.26ms\nstep:2055/5000 train_loss:8.1361 train_time:370683ms step_avg:181.26ms\nstep:2056/5000 train_loss:8.1759 train_time:370864ms step_avg:181.26ms\nstep:2057/5000 train_loss:8.0945 train_time:371045ms step_avg:181.26ms\nstep:2058/5000 train_loss:8.0873 train_time:371227ms step_avg:181.26ms\nstep:2059/5000 train_loss:7.9790 train_time:371410ms step_avg:181.26ms\nstep:2060/5000 train_loss:8.3471 train_time:371592ms step_avg:181.26ms\nstep:2061/5000 train_loss:7.9447 train_time:371773ms step_avg:181.26ms\nstep:2062/5000 train_loss:8.2744 train_time:371954ms step_avg:181.26ms\nstep:2063/5000 train_loss:8.1317 train_time:372136ms step_avg:181.26ms\nstep:2064/5000 train_loss:8.1142 train_time:372317ms step_avg:181.26ms\nstep:2065/5000 train_loss:8.0598 train_time:372501ms step_avg:181.27ms\nstep:2066/5000 train_loss:8.6006 train_time:372681ms step_avg:181.27ms\nstep:2067/5000 train_loss:8.0960 train_time:372861ms step_avg:181.26ms\nstep:2068/5000 train_loss:8.0327 train_time:373042ms step_avg:181.26ms\nstep:2069/5000 train_loss:8.1067 train_time:373223ms step_avg:181.26ms\nstep:2070/5000 train_loss:8.1680 train_time:373404ms step_avg:181.26ms\nstep:2071/5000 train_loss:8.1053 train_time:373585ms step_avg:181.26ms\nstep:2072/5000 train_loss:8.1269 train_time:373766ms step_avg:181.26ms\nstep:2073/5000 train_loss:8.0873 train_time:373946ms step_avg:181.26ms\nstep:2074/5000 train_loss:8.2180 train_time:374126ms step_avg:181.26ms\nstep:2075/5000 train_loss:8.0307 train_time:374308ms step_avg:181.26ms\nstep:2076/5000 train_loss:8.0570 train_time:374490ms step_avg:181.26ms\nstep:2077/5000 train_loss:8.0887 train_time:374672ms step_avg:181.26ms\nstep:2078/5000 train_loss:8.1418 train_time:374855ms step_avg:181.26ms\nstep:2079/5000 train_loss:8.0272 train_time:375037ms step_avg:181.26ms\nstep:2080/5000 train_loss:8.0783 train_time:375219ms step_avg:181.27ms\nstep:2081/5000 train_loss:8.1060 train_time:375401ms step_avg:181.27ms\nstep:2082/5000 train_loss:8.1999 train_time:375581ms step_avg:181.27ms\nstep:2083/5000 train_loss:8.0806 train_time:375764ms step_avg:181.27ms\nstep:2084/5000 train_loss:8.0731 train_time:375946ms step_avg:181.27ms\nstep:2085/5000 train_loss:8.0853 train_time:376126ms step_avg:181.27ms\nstep:2086/5000 train_loss:8.0193 train_time:376307ms step_avg:181.27ms\nstep:2087/5000 train_loss:8.1453 train_time:376489ms step_avg:181.27ms\nstep:2088/5000 train_loss:8.0367 train_time:376670ms step_avg:181.27ms\nstep:2089/5000 train_loss:8.1057 train_time:376853ms step_avg:181.27ms\nstep:2090/5000 train_loss:8.0802 train_time:377036ms step_avg:181.27ms\nstep:2091/5000 train_loss:8.1010 train_time:377220ms step_avg:181.27ms\nstep:2092/5000 train_loss:8.1355 train_time:377402ms step_avg:181.27ms\nstep:2093/5000 train_loss:8.1645 train_time:377583ms step_avg:181.27ms\nstep:2094/5000 train_loss:8.4288 train_time:377764ms step_avg:181.27ms\nstep:2095/5000 train_loss:8.0918 train_time:377946ms step_avg:181.27ms\nstep:2096/5000 train_loss:8.0279 train_time:378127ms step_avg:181.27ms\nstep:2097/5000 train_loss:8.1251 train_time:378309ms step_avg:181.27ms\nstep:2098/5000 train_loss:8.1725 train_time:378491ms step_avg:181.27ms\nstep:2099/5000 train_loss:8.0913 train_time:378672ms step_avg:181.27ms\nstep:2100/5000 train_loss:8.0787 train_time:378855ms step_avg:181.27ms\nstep:2101/5000 train_loss:8.1707 train_time:379037ms step_avg:181.27ms\nstep:2102/5000 train_loss:8.1668 train_time:379221ms step_avg:181.27ms\nstep:2103/5000 train_loss:8.0884 train_time:379403ms step_avg:181.27ms\nstep:2104/5000 train_loss:8.0981 train_time:379584ms step_avg:181.27ms\nstep:2105/5000 train_loss:8.0309 train_time:379765ms step_avg:181.27ms\nstep:2106/5000 train_loss:8.1317 train_time:379945ms step_avg:181.27ms\nstep:2107/5000 train_loss:8.2261 train_time:380123ms step_avg:181.27ms\nstep:2108/5000 train_loss:8.1607 train_time:380304ms step_avg:181.27ms\nstep:2109/5000 train_loss:8.0688 train_time:380485ms step_avg:181.27ms\nstep:2110/5000 train_loss:8.0416 train_time:380665ms step_avg:181.27ms\nstep:2111/5000 train_loss:8.1015 train_time:380848ms step_avg:181.27ms\nstep:2112/5000 train_loss:7.9791 train_time:381029ms step_avg:181.27ms\nstep:2113/5000 train_loss:8.1188 train_time:381211ms step_avg:181.27ms\nstep:2114/5000 train_loss:8.0800 train_time:381392ms step_avg:181.27ms\nstep:2115/5000 train_loss:8.2155 train_time:381573ms step_avg:181.27ms\nstep:2116/5000 train_loss:8.0407 train_time:381755ms step_avg:181.27ms\nstep:2117/5000 train_loss:8.0681 train_time:381938ms step_avg:181.27ms\nstep:2118/5000 train_loss:8.0839 train_time:382120ms step_avg:181.27ms\nstep:2119/5000 train_loss:8.1857 train_time:382302ms step_avg:181.27ms\nstep:2120/5000 train_loss:8.1420 train_time:382483ms step_avg:181.27ms\nstep:2121/5000 train_loss:8.1373 train_time:382663ms step_avg:181.27ms\nstep:2122/5000 train_loss:8.0674 train_time:382844ms step_avg:181.27ms\nstep:2123/5000 train_loss:8.0340 train_time:383025ms step_avg:181.27ms\nstep:2124/5000 train_loss:8.1863 train_time:383205ms step_avg:181.27ms\nstep:2125/5000 train_loss:8.1753 train_time:383386ms step_avg:181.27ms\nstep:2126/5000 train_loss:8.1950 train_time:383567ms step_avg:181.27ms\nstep:2127/5000 train_loss:8.1056 train_time:383746ms step_avg:181.27ms\nstep:2128/5000 train_loss:8.0716 train_time:383926ms step_avg:181.27ms\nstep:2129/5000 train_loss:8.0506 train_time:384108ms step_avg:181.27ms\nstep:2130/5000 train_loss:8.0904 train_time:384289ms step_avg:181.27ms\nstep:2131/5000 train_loss:8.1445 train_time:384471ms step_avg:181.27ms\nstep:2132/5000 train_loss:8.1359 train_time:384654ms step_avg:181.27ms\nstep:2133/5000 train_loss:7.6705 train_time:384835ms step_avg:181.27ms\nstep:2134/5000 train_loss:8.2399 train_time:385017ms step_avg:181.27ms\nstep:2135/5000 train_loss:8.0405 train_time:385201ms step_avg:181.27ms\nstep:2136/5000 train_loss:8.1566 train_time:385383ms step_avg:181.27ms\nstep:2137/5000 train_loss:8.2100 train_time:385563ms step_avg:181.27ms\nstep:2138/5000 train_loss:8.0668 train_time:385745ms step_avg:181.27ms\nstep:2139/5000 train_loss:8.1301 train_time:385926ms step_avg:181.27ms\nstep:2140/5000 train_loss:8.1548 train_time:386108ms step_avg:181.27ms\nstep:2141/5000 train_loss:8.1134 train_time:386290ms step_avg:181.27ms\nstep:2142/5000 train_loss:8.0821 train_time:386472ms step_avg:181.27ms\nstep:2143/5000 train_loss:7.8716 train_time:386653ms step_avg:181.27ms\nstep:2144/5000 train_loss:8.2448 train_time:386835ms step_avg:181.27ms\nstep:2145/5000 train_loss:8.2923 train_time:387019ms step_avg:181.27ms\nstep:2146/5000 train_loss:8.0829 train_time:387203ms step_avg:181.27ms\nstep:2147/5000 train_loss:8.0785 train_time:387385ms step_avg:181.28ms\nstep:2148/5000 train_loss:8.1869 train_time:387566ms step_avg:181.27ms\nstep:2149/5000 train_loss:8.0655 train_time:387747ms step_avg:181.27ms\nstep:2150/5000 train_loss:8.1067 train_time:387928ms step_avg:181.27ms\nstep:2151/5000 train_loss:8.4192 train_time:388109ms step_avg:181.27ms\nstep:2152/5000 train_loss:8.0068 train_time:388290ms step_avg:181.27ms\nstep:2153/5000 train_loss:8.0630 train_time:388472ms step_avg:181.27ms\nstep:2154/5000 train_loss:8.1355 train_time:388654ms step_avg:181.28ms\nstep:2155/5000 train_loss:8.1056 train_time:388835ms step_avg:181.28ms\nstep:2156/5000 train_loss:7.9677 train_time:389017ms step_avg:181.28ms\nstep:2157/5000 train_loss:8.1109 train_time:389200ms step_avg:181.28ms\nstep:2158/5000 train_loss:8.1978 train_time:389381ms step_avg:181.28ms\nstep:2159/5000 train_loss:8.0297 train_time:389562ms step_avg:181.28ms\nstep:2160/5000 train_loss:8.0147 train_time:389744ms step_avg:181.28ms\nstep:2161/5000 train_loss:8.1083 train_time:389926ms step_avg:181.28ms\nstep:2162/5000 train_loss:8.0952 train_time:390108ms step_avg:181.28ms\nstep:2163/5000 train_loss:8.1951 train_time:390290ms step_avg:181.28ms\nstep:2164/5000 train_loss:8.1484 train_time:390471ms step_avg:181.28ms\nstep:2165/5000 train_loss:7.8037 train_time:390653ms step_avg:181.28ms\nstep:2166/5000 train_loss:8.1181 train_time:390834ms step_avg:181.28ms\nstep:2167/5000 train_loss:8.4837 train_time:391017ms step_avg:181.28ms\nstep:2168/5000 train_loss:7.9977 train_time:391199ms step_avg:181.28ms\nstep:2169/5000 train_loss:8.1053 train_time:391381ms step_avg:181.28ms\nstep:2170/5000 train_loss:8.1093 train_time:391561ms step_avg:181.28ms\nstep:2171/5000 train_loss:8.2939 train_time:391743ms step_avg:181.28ms\nstep:2172/5000 train_loss:8.1292 train_time:391925ms step_avg:181.28ms\nstep:2173/5000 train_loss:8.1527 train_time:392106ms step_avg:181.28ms\nstep:2174/5000 train_loss:8.1339 train_time:392286ms step_avg:181.28ms\nstep:2175/5000 train_loss:8.1248 train_time:392466ms step_avg:181.28ms\nstep:2176/5000 train_loss:8.1617 train_time:392647ms step_avg:181.28ms\nstep:2177/5000 train_loss:8.1061 train_time:392827ms step_avg:181.28ms\nstep:2178/5000 train_loss:8.2410 train_time:393009ms step_avg:181.28ms\nstep:2179/5000 train_loss:8.0304 train_time:393191ms step_avg:181.28ms\nstep:2180/5000 train_loss:8.2109 train_time:393375ms step_avg:181.28ms\nstep:2181/5000 train_loss:8.3072 train_time:393557ms step_avg:181.28ms\nstep:2182/5000 train_loss:8.1424 train_time:393739ms step_avg:181.28ms\nstep:2183/5000 train_loss:8.0103 train_time:393921ms step_avg:181.28ms\nstep:2184/5000 train_loss:8.0831 train_time:394103ms step_avg:181.28ms\nstep:2185/5000 train_loss:8.1219 train_time:394284ms step_avg:181.28ms\nstep:2186/5000 train_loss:8.1387 train_time:394466ms step_avg:181.28ms\nstep:2187/5000 train_loss:8.0499 train_time:394647ms step_avg:181.28ms\nstep:2188/5000 train_loss:7.9787 train_time:394827ms step_avg:181.28ms\nstep:2189/5000 train_loss:8.0810 train_time:395009ms step_avg:181.28ms\nstep:2190/5000 train_loss:8.1674 train_time:395190ms step_avg:181.28ms\nstep:2191/5000 train_loss:8.0086 train_time:395371ms step_avg:181.28ms\nstep:2192/5000 train_loss:8.1517 train_time:395553ms step_avg:181.28ms\nstep:2193/5000 train_loss:8.3844 train_time:395735ms step_avg:181.28ms\nstep:2194/5000 train_loss:8.2087 train_time:395917ms step_avg:181.28ms\nstep:2195/5000 train_loss:8.1348 train_time:396099ms step_avg:181.28ms\nstep:2196/5000 train_loss:8.3591 train_time:396281ms step_avg:181.28ms\nstep:2197/5000 train_loss:8.0854 train_time:396461ms step_avg:181.28ms\nstep:2198/5000 train_loss:8.1178 train_time:396642ms step_avg:181.28ms\nstep:2199/5000 train_loss:8.0897 train_time:396823ms step_avg:181.28ms\nstep:2200/5000 train_loss:8.2294 train_time:397005ms step_avg:181.28ms\nstep:2201/5000 train_loss:8.0249 train_time:397186ms step_avg:181.28ms\nstep:2202/5000 train_loss:8.0470 train_time:397366ms step_avg:181.28ms\nstep:2203/5000 train_loss:8.1677 train_time:397546ms step_avg:181.28ms\nstep:2204/5000 train_loss:8.0751 train_time:397726ms step_avg:181.28ms\nstep:2205/5000 train_loss:8.1274 train_time:397909ms step_avg:181.28ms\nstep:2206/5000 train_loss:8.1101 train_time:398090ms step_avg:181.28ms\nstep:2207/5000 train_loss:8.1089 train_time:398271ms step_avg:181.28ms\nstep:2208/5000 train_loss:8.0898 train_time:398454ms step_avg:181.28ms\nstep:2209/5000 train_loss:8.0798 train_time:398636ms step_avg:181.28ms\nstep:2210/5000 train_loss:8.1492 train_time:398818ms step_avg:181.28ms\nstep:2211/5000 train_loss:8.0380 train_time:399000ms step_avg:181.28ms\nstep:2212/5000 train_loss:8.3027 train_time:399181ms step_avg:181.28ms\nstep:2213/5000 train_loss:8.2241 train_time:399363ms step_avg:181.28ms\nstep:2214/5000 train_loss:8.1223 train_time:399545ms step_avg:181.28ms\nstep:2215/5000 train_loss:8.1265 train_time:399726ms step_avg:181.28ms\nstep:2216/5000 train_loss:8.1834 train_time:399908ms step_avg:181.28ms\nstep:2217/5000 train_loss:8.1378 train_time:400089ms step_avg:181.28ms\nstep:2218/5000 train_loss:8.0990 train_time:400271ms step_avg:181.28ms\nstep:2219/5000 train_loss:8.1630 train_time:400452ms step_avg:181.28ms\nstep:2220/5000 train_loss:8.1675 train_time:400634ms step_avg:181.28ms\nstep:2221/5000 train_loss:8.1084 train_time:400817ms step_avg:181.28ms\nstep:2222/5000 train_loss:8.3575 train_time:401001ms step_avg:181.28ms\nstep:2223/5000 train_loss:8.2377 train_time:401182ms step_avg:181.28ms\nstep:2224/5000 train_loss:8.0827 train_time:401364ms step_avg:181.28ms\nstep:2225/5000 train_loss:8.0915 train_time:401545ms step_avg:181.28ms\nstep:2226/5000 train_loss:8.1621 train_time:401726ms step_avg:181.28ms\nstep:2227/5000 train_loss:8.1485 train_time:401908ms step_avg:181.28ms\nstep:2228/5000 train_loss:8.1460 train_time:402089ms step_avg:181.28ms\nstep:2229/5000 train_loss:8.0287 train_time:402270ms step_avg:181.28ms\nstep:2230/5000 train_loss:8.3513 train_time:402452ms step_avg:181.28ms\nstep:2231/5000 train_loss:8.1528 train_time:402633ms step_avg:181.28ms\nstep:2232/5000 train_loss:8.0855 train_time:402815ms step_avg:181.28ms\nstep:2233/5000 train_loss:8.1130 train_time:402997ms step_avg:181.29ms\nstep:2234/5000 train_loss:8.0213 train_time:403179ms step_avg:181.29ms\nstep:2235/5000 train_loss:8.1397 train_time:403362ms step_avg:181.29ms\nstep:2236/5000 train_loss:8.1040 train_time:403543ms step_avg:181.29ms\nstep:2237/5000 train_loss:8.2323 train_time:403723ms step_avg:181.29ms\nstep:2238/5000 train_loss:8.1669 train_time:403905ms step_avg:181.29ms\nstep:2239/5000 train_loss:8.2154 train_time:404086ms step_avg:181.29ms\nstep:2240/5000 train_loss:8.0505 train_time:404266ms step_avg:181.29ms\nstep:2241/5000 train_loss:8.2158 train_time:404446ms step_avg:181.28ms\nstep:2242/5000 train_loss:8.2279 train_time:404627ms step_avg:181.28ms\nstep:2243/5000 train_loss:8.1757 train_time:404809ms step_avg:181.28ms\nstep:2244/5000 train_loss:8.1043 train_time:404991ms step_avg:181.29ms\nstep:2245/5000 train_loss:8.0334 train_time:405175ms step_avg:181.29ms\nstep:2246/5000 train_loss:8.0793 train_time:405356ms step_avg:181.29ms\nstep:2247/5000 train_loss:8.1171 train_time:405539ms step_avg:181.29ms\nstep:2248/5000 train_loss:8.0724 train_time:405721ms step_avg:181.29ms\nstep:2249/5000 train_loss:8.0683 train_time:405903ms step_avg:181.29ms\nstep:2250/5000 train_loss:8.0293 train_time:406084ms step_avg:181.29ms\nstep:2251/5000 train_loss:8.0966 train_time:406265ms step_avg:181.29ms\nstep:2252/5000 train_loss:8.1075 train_time:406445ms step_avg:181.29ms\nstep:2253/5000 train_loss:8.1452 train_time:406625ms step_avg:181.29ms\nstep:2254/5000 train_loss:8.1119 train_time:406806ms step_avg:181.29ms\nstep:2255/5000 train_loss:8.0522 train_time:406988ms step_avg:181.29ms\nstep:2256/5000 train_loss:8.1463 train_time:407170ms step_avg:181.29ms\nstep:2257/5000 train_loss:8.0648 train_time:407353ms step_avg:181.29ms\nstep:2258/5000 train_loss:8.1802 train_time:407534ms step_avg:181.29ms\nstep:2259/5000 train_loss:8.1570 train_time:407717ms step_avg:181.29ms\nstep:2260/5000 train_loss:8.1188 train_time:407900ms step_avg:181.29ms\nstep:2261/5000 train_loss:8.0400 train_time:408081ms step_avg:181.29ms\nstep:2262/5000 train_loss:8.1811 train_time:408262ms step_avg:181.29ms\nstep:2263/5000 train_loss:8.0041 train_time:408442ms step_avg:181.29ms\nstep:2264/5000 train_loss:8.1041 train_time:408624ms step_avg:181.29ms\nstep:2265/5000 train_loss:8.1595 train_time:408806ms step_avg:181.29ms\nstep:2266/5000 train_loss:8.0787 train_time:408987ms step_avg:181.29ms\nstep:2267/5000 train_loss:8.0957 train_time:409170ms step_avg:181.29ms\nstep:2268/5000 train_loss:8.1518 train_time:409351ms step_avg:181.29ms\nstep:2269/5000 train_loss:8.0832 train_time:409533ms step_avg:181.29ms\nstep:2270/5000 train_loss:8.0698 train_time:409715ms step_avg:181.29ms\nstep:2271/5000 train_loss:8.1004 train_time:409898ms step_avg:181.29ms\nstep:2272/5000 train_loss:8.3052 train_time:410079ms step_avg:181.29ms\nstep:2273/5000 train_loss:8.0621 train_time:410261ms step_avg:181.29ms\nstep:2274/5000 train_loss:8.1018 train_time:410444ms step_avg:181.29ms\nstep:2275/5000 train_loss:8.0731 train_time:410625ms step_avg:181.29ms\nstep:2276/5000 train_loss:8.0454 train_time:410806ms step_avg:181.29ms\nstep:2277/5000 train_loss:8.1413 train_time:410987ms step_avg:181.29ms\nstep:2278/5000 train_loss:8.1089 train_time:411168ms step_avg:181.29ms\nstep:2279/5000 train_loss:8.1269 train_time:411350ms step_avg:181.29ms\nstep:2280/5000 train_loss:8.0444 train_time:411531ms step_avg:181.29ms\nstep:2281/5000 train_loss:8.0876 train_time:411714ms step_avg:181.29ms\nstep:2282/5000 train_loss:8.1200 train_time:411897ms step_avg:181.29ms\nstep:2283/5000 train_loss:8.0607 train_time:412079ms step_avg:181.29ms\nstep:2284/5000 train_loss:8.0644 train_time:412261ms step_avg:181.29ms\nstep:2285/5000 train_loss:8.0537 train_time:412442ms step_avg:181.29ms\nstep:2286/5000 train_loss:8.1475 train_time:412857ms step_avg:181.40ms\nstep:2287/5000 train_loss:8.0485 train_time:413037ms step_avg:181.40ms\nstep:2288/5000 train_loss:8.1999 train_time:413218ms step_avg:181.40ms\nstep:2289/5000 train_loss:8.0910 train_time:413399ms step_avg:181.39ms\nstep:2290/5000 train_loss:7.9949 train_time:413579ms step_avg:181.39ms\nstep:2291/5000 train_loss:8.1525 train_time:413768ms step_avg:181.40ms\nstep:2292/5000 train_loss:8.0433 train_time:413949ms step_avg:181.40ms\nstep:2293/5000 train_loss:8.3638 train_time:414129ms step_avg:181.40ms\nstep:2294/5000 train_loss:8.1225 train_time:414310ms step_avg:181.40ms\nstep:2295/5000 train_loss:8.0686 train_time:414492ms step_avg:181.40ms\nstep:2296/5000 train_loss:8.0279 train_time:414675ms step_avg:181.40ms\nstep:2297/5000 train_loss:8.1025 train_time:414857ms step_avg:181.40ms\nstep:2298/5000 train_loss:8.0453 train_time:415040ms step_avg:181.40ms\nstep:2299/5000 train_loss:8.1443 train_time:415222ms step_avg:181.40ms\nstep:2300/5000 train_loss:8.1190 train_time:415403ms step_avg:181.40ms\nstep:2301/5000 train_loss:8.1005 train_time:415583ms step_avg:181.40ms\nstep:2302/5000 train_loss:8.1472 train_time:415764ms step_avg:181.40ms\nstep:2303/5000 train_loss:8.1230 train_time:415945ms step_avg:181.40ms\nstep:2304/5000 train_loss:8.1809 train_time:416126ms step_avg:181.40ms\nstep:2305/5000 train_loss:8.0719 train_time:416308ms step_avg:181.40ms\nstep:2306/5000 train_loss:8.1317 train_time:416490ms step_avg:181.40ms\nstep:2307/5000 train_loss:8.1994 train_time:416670ms step_avg:181.40ms\nstep:2308/5000 train_loss:8.1374 train_time:416852ms step_avg:181.40ms\nstep:2309/5000 train_loss:8.2788 train_time:417033ms step_avg:181.40ms\nstep:2310/5000 train_loss:8.1977 train_time:417216ms step_avg:181.40ms\nstep:2311/5000 train_loss:8.1040 train_time:417397ms step_avg:181.40ms\nstep:2312/5000 train_loss:8.1600 train_time:417579ms step_avg:181.40ms\nstep:2313/5000 train_loss:8.1656 train_time:417762ms step_avg:181.40ms\nstep:2314/5000 train_loss:8.0889 train_time:417942ms step_avg:181.40ms\nstep:2315/5000 train_loss:8.0308 train_time:418124ms step_avg:181.40ms\nstep:2316/5000 train_loss:8.0984 train_time:418306ms step_avg:181.40ms\nstep:2317/5000 train_loss:8.1694 train_time:418486ms step_avg:181.40ms\nstep:2318/5000 train_loss:7.9952 train_time:418668ms step_avg:181.40ms\nstep:2319/5000 train_loss:8.2698 train_time:418848ms step_avg:181.40ms\nstep:2320/5000 train_loss:8.0314 train_time:419029ms step_avg:181.40ms\nstep:2321/5000 train_loss:8.2385 train_time:419211ms step_avg:181.40ms\nstep:2322/5000 train_loss:8.1728 train_time:419394ms step_avg:181.40ms\nstep:2323/5000 train_loss:8.0153 train_time:419575ms step_avg:181.40ms\nstep:2324/5000 train_loss:8.1025 train_time:419757ms step_avg:181.40ms\nstep:2325/5000 train_loss:8.0348 train_time:419939ms step_avg:181.40ms\nstep:2326/5000 train_loss:8.1445 train_time:420121ms step_avg:181.40ms\nstep:2327/5000 train_loss:8.0924 train_time:420303ms step_avg:181.40ms\nstep:2328/5000 train_loss:8.0987 train_time:420484ms step_avg:181.40ms\nstep:2329/5000 train_loss:8.0858 train_time:420665ms step_avg:181.40ms\nstep:2330/5000 train_loss:8.1314 train_time:420845ms step_avg:181.40ms\nstep:2331/5000 train_loss:8.0827 train_time:421024ms step_avg:181.40ms\nstep:2332/5000 train_loss:8.2374 train_time:421204ms step_avg:181.40ms\nstep:2333/5000 train_loss:8.0613 train_time:421385ms step_avg:181.40ms\nstep:2334/5000 train_loss:8.2693 train_time:421565ms step_avg:181.40ms\nstep:2335/5000 train_loss:7.9669 train_time:421747ms step_avg:181.40ms\nstep:2336/5000 train_loss:8.0621 train_time:421929ms step_avg:181.40ms\nstep:2337/5000 train_loss:8.1274 train_time:422110ms step_avg:181.40ms\nstep:2338/5000 train_loss:8.0005 train_time:422290ms step_avg:181.40ms\nstep:2339/5000 train_loss:8.1108 train_time:422472ms step_avg:181.40ms\nstep:2340/5000 train_loss:8.0545 train_time:422654ms step_avg:181.40ms\nstep:2341/5000 train_loss:8.0278 train_time:422836ms step_avg:181.40ms\nstep:2342/5000 train_loss:8.1437 train_time:423019ms step_avg:181.40ms\nstep:2343/5000 train_loss:8.1572 train_time:423200ms step_avg:181.40ms\nstep:2344/5000 train_loss:8.1072 train_time:423381ms step_avg:181.40ms\nstep:2345/5000 train_loss:8.1977 train_time:423563ms step_avg:181.40ms\nstep:2346/5000 train_loss:8.1081 train_time:423745ms step_avg:181.40ms\nstep:2347/5000 train_loss:8.0594 train_time:423926ms step_avg:181.40ms\nstep:2348/5000 train_loss:8.0604 train_time:424109ms step_avg:181.40ms\nstep:2349/5000 train_loss:8.0905 train_time:424290ms step_avg:181.40ms\nstep:2350/5000 train_loss:8.1570 train_time:424471ms step_avg:181.40ms\nstep:2351/5000 train_loss:7.9987 train_time:424651ms step_avg:181.40ms\nstep:2352/5000 train_loss:8.1561 train_time:424833ms step_avg:181.40ms\nstep:2353/5000 train_loss:8.1018 train_time:425016ms step_avg:181.40ms\nstep:2354/5000 train_loss:8.0005 train_time:425198ms step_avg:181.40ms\nstep:2355/5000 train_loss:8.1314 train_time:425380ms step_avg:181.40ms\nstep:2356/5000 train_loss:8.1751 train_time:425561ms step_avg:181.40ms\nstep:2357/5000 train_loss:8.0196 train_time:425742ms step_avg:181.40ms\nstep:2358/5000 train_loss:8.2064 train_time:425923ms step_avg:181.40ms\nstep:2359/5000 train_loss:8.1233 train_time:426105ms step_avg:181.40ms\nstep:2360/5000 train_loss:8.2348 train_time:426287ms step_avg:181.40ms\nstep:2361/5000 train_loss:8.1314 train_time:426467ms step_avg:181.40ms\nstep:2362/5000 train_loss:8.1632 train_time:426647ms step_avg:181.40ms\nstep:2363/5000 train_loss:8.1753 train_time:426827ms step_avg:181.40ms\nstep:2364/5000 train_loss:8.1450 train_time:427009ms step_avg:181.40ms\nstep:2365/5000 train_loss:8.1377 train_time:427192ms step_avg:181.40ms\nstep:2366/5000 train_loss:8.0352 train_time:427374ms step_avg:181.40ms\nstep:2367/5000 train_loss:8.0925 train_time:427555ms step_avg:181.40ms\nstep:2368/5000 train_loss:8.0150 train_time:427737ms step_avg:181.40ms\nstep:2369/5000 train_loss:8.1854 train_time:427921ms step_avg:181.40ms\nstep:2370/5000 train_loss:7.9700 train_time:428102ms step_avg:181.40ms\nstep:2371/5000 train_loss:8.1517 train_time:428285ms step_avg:181.40ms\nstep:2372/5000 train_loss:8.1181 train_time:428467ms step_avg:181.40ms\nstep:2373/5000 train_loss:8.1285 train_time:428648ms step_avg:181.40ms\nstep:2374/5000 train_loss:8.1526 train_time:428829ms step_avg:181.40ms\nstep:2375/5000 train_loss:8.1503 train_time:429011ms step_avg:181.40ms\nstep:2376/5000 train_loss:8.1393 train_time:429192ms step_avg:181.40ms\nstep:2377/5000 train_loss:8.1257 train_time:429374ms step_avg:181.40ms\nstep:2378/5000 train_loss:8.1040 train_time:429555ms step_avg:181.40ms\nstep:2379/5000 train_loss:8.1411 train_time:429737ms step_avg:181.40ms\nstep:2380/5000 train_loss:8.2438 train_time:429919ms step_avg:181.40ms\nstep:2381/5000 train_loss:8.0501 train_time:430102ms step_avg:181.40ms\nstep:2382/5000 train_loss:8.0611 train_time:430284ms step_avg:181.40ms\nstep:2383/5000 train_loss:8.2421 train_time:430467ms step_avg:181.40ms\nstep:2384/5000 train_loss:7.9358 train_time:430648ms step_avg:181.40ms\nstep:2385/5000 train_loss:8.1651 train_time:430829ms step_avg:181.40ms\nstep:2386/5000 train_loss:8.1218 train_time:431011ms step_avg:181.40ms\nstep:2387/5000 train_loss:8.0942 train_time:431192ms step_avg:181.40ms\nstep:2388/5000 train_loss:8.0893 train_time:431374ms step_avg:181.40ms\nstep:2389/5000 train_loss:8.1703 train_time:431557ms step_avg:181.40ms\nstep:2390/5000 train_loss:8.0594 train_time:431739ms step_avg:181.40ms\nstep:2391/5000 train_loss:8.0428 train_time:431922ms step_avg:181.40ms\nstep:2392/5000 train_loss:8.1120 train_time:432103ms step_avg:181.40ms\nstep:2393/5000 train_loss:8.1213 train_time:432284ms step_avg:181.40ms\nstep:2394/5000 train_loss:8.1962 train_time:432466ms step_avg:181.40ms\nstep:2395/5000 train_loss:8.1397 train_time:432648ms step_avg:181.40ms\nstep:2396/5000 train_loss:8.1795 train_time:432829ms step_avg:181.40ms\nstep:2397/5000 train_loss:8.1406 train_time:433013ms step_avg:181.40ms\nstep:2398/5000 train_loss:8.1386 train_time:433194ms step_avg:181.40ms\nstep:2399/5000 train_loss:8.0683 train_time:433375ms step_avg:181.40ms\nstep:2400/5000 train_loss:8.0576 train_time:433557ms step_avg:181.40ms\nstep:2401/5000 train_loss:8.1746 train_time:433739ms step_avg:181.40ms\nstep:2402/5000 train_loss:8.1376 train_time:433921ms step_avg:181.41ms\nstep:2403/5000 train_loss:8.1684 train_time:434102ms step_avg:181.41ms\nstep:2404/5000 train_loss:8.0410 train_time:434283ms step_avg:181.40ms\nstep:2405/5000 train_loss:8.1531 train_time:434464ms step_avg:181.40ms\nstep:2406/5000 train_loss:7.9823 train_time:434647ms step_avg:181.41ms\nstep:2407/5000 train_loss:8.0352 train_time:434828ms step_avg:181.40ms\nstep:2408/5000 train_loss:8.0168 train_time:435010ms step_avg:181.41ms\nstep:2409/5000 train_loss:8.0480 train_time:435192ms step_avg:181.41ms\nstep:2410/5000 train_loss:8.0735 train_time:435374ms step_avg:181.41ms\nstep:2411/5000 train_loss:8.0996 train_time:435555ms step_avg:181.41ms\nstep:2412/5000 train_loss:8.0377 train_time:435738ms step_avg:181.41ms\nstep:2413/5000 train_loss:8.0148 train_time:435920ms step_avg:181.41ms\nstep:2414/5000 train_loss:8.0626 train_time:436102ms step_avg:181.41ms\nstep:2415/5000 train_loss:8.2192 train_time:436283ms step_avg:181.41ms\nstep:2416/5000 train_loss:8.1223 train_time:436464ms step_avg:181.41ms\nstep:2417/5000 train_loss:8.0122 train_time:436645ms step_avg:181.41ms\nstep:2418/5000 train_loss:8.1370 train_time:436825ms step_avg:181.41ms\nstep:2419/5000 train_loss:8.1959 train_time:437008ms step_avg:181.41ms\nstep:2420/5000 train_loss:8.2137 train_time:437189ms step_avg:181.41ms\nstep:2421/5000 train_loss:8.1184 train_time:437371ms step_avg:181.41ms\nstep:2422/5000 train_loss:8.1679 train_time:437553ms step_avg:181.41ms\nstep:2423/5000 train_loss:8.1220 train_time:437734ms step_avg:181.41ms\nstep:2424/5000 train_loss:8.1233 train_time:437917ms step_avg:181.41ms\nstep:2425/5000 train_loss:8.1347 train_time:438100ms step_avg:181.41ms\nstep:2426/5000 train_loss:8.1600 train_time:438282ms step_avg:181.41ms\nstep:2427/5000 train_loss:8.0105 train_time:438463ms step_avg:181.41ms\nstep:2428/5000 train_loss:8.0544 train_time:438645ms step_avg:181.41ms\nstep:2429/5000 train_loss:8.0416 train_time:438826ms step_avg:181.41ms\nstep:2430/5000 train_loss:8.1273 train_time:439009ms step_avg:181.41ms\nstep:2431/5000 train_loss:8.0953 train_time:439192ms step_avg:181.41ms\nstep:2432/5000 train_loss:8.0857 train_time:439375ms step_avg:181.41ms\nstep:2433/5000 train_loss:8.0094 train_time:439557ms step_avg:181.41ms\nstep:2434/5000 train_loss:8.1125 train_time:439739ms step_avg:181.41ms\nstep:2435/5000 train_loss:7.9522 train_time:439921ms step_avg:181.41ms\nstep:2436/5000 train_loss:8.0870 train_time:440103ms step_avg:181.41ms\nstep:2437/5000 train_loss:8.0478 train_time:440284ms step_avg:181.41ms\nstep:2438/5000 train_loss:8.0800 train_time:440466ms step_avg:181.41ms\nstep:2439/5000 train_loss:8.0721 train_time:440646ms step_avg:181.41ms\nstep:2440/5000 train_loss:8.0941 train_time:440827ms step_avg:181.41ms\nstep:2441/5000 train_loss:8.0536 train_time:441009ms step_avg:181.41ms\nstep:2442/5000 train_loss:8.0781 train_time:441190ms step_avg:181.41ms\nstep:2443/5000 train_loss:8.0137 train_time:441371ms step_avg:181.41ms\nstep:2444/5000 train_loss:8.1272 train_time:441554ms step_avg:181.41ms\nstep:2445/5000 train_loss:8.0369 train_time:441736ms step_avg:181.41ms\nstep:2446/5000 train_loss:8.1051 train_time:441919ms step_avg:181.41ms\nstep:2447/5000 train_loss:8.0927 train_time:442101ms step_avg:181.41ms\nstep:2448/5000 train_loss:8.1062 train_time:442282ms step_avg:181.41ms\nstep:2449/5000 train_loss:8.2075 train_time:442463ms step_avg:181.41ms\nstep:2450/5000 train_loss:8.1866 train_time:442644ms step_avg:181.41ms\nstep:2451/5000 train_loss:8.1114 train_time:442826ms step_avg:181.41ms\nstep:2452/5000 train_loss:8.0890 train_time:443008ms step_avg:181.41ms\nstep:2453/5000 train_loss:7.9813 train_time:443188ms step_avg:181.41ms\nstep:2454/5000 train_loss:7.9569 train_time:443369ms step_avg:181.41ms\nstep:2455/5000 train_loss:8.0809 train_time:443550ms step_avg:181.41ms\nstep:2456/5000 train_loss:8.1342 train_time:443731ms step_avg:181.41ms\nstep:2457/5000 train_loss:8.1123 train_time:443915ms step_avg:181.41ms\nstep:2458/5000 train_loss:8.2124 train_time:444097ms step_avg:181.41ms\nstep:2459/5000 train_loss:8.1884 train_time:444279ms step_avg:181.41ms\nstep:2460/5000 train_loss:8.2812 train_time:444460ms step_avg:181.41ms\nstep:2461/5000 train_loss:8.1629 train_time:444641ms step_avg:181.41ms\nstep:2462/5000 train_loss:8.1263 train_time:444824ms step_avg:181.41ms\nstep:2463/5000 train_loss:8.2142 train_time:445006ms step_avg:181.41ms\nstep:2464/5000 train_loss:8.1642 train_time:445187ms step_avg:181.41ms\nstep:2465/5000 train_loss:8.1846 train_time:445368ms step_avg:181.41ms\nstep:2466/5000 train_loss:8.1304 train_time:445548ms step_avg:181.41ms\nstep:2467/5000 train_loss:8.1462 train_time:445728ms step_avg:181.41ms\nstep:2468/5000 train_loss:7.9863 train_time:445911ms step_avg:181.41ms\nstep:2469/5000 train_loss:8.1652 train_time:446094ms step_avg:181.41ms\nstep:2470/5000 train_loss:8.1334 train_time:446277ms step_avg:181.41ms\nstep:2471/5000 train_loss:8.0597 train_time:446459ms step_avg:181.41ms\nstep:2472/5000 train_loss:8.2290 train_time:446641ms step_avg:181.41ms\nstep:2473/5000 train_loss:8.0446 train_time:446822ms step_avg:181.41ms\nstep:2474/5000 train_loss:8.0589 train_time:447003ms step_avg:181.41ms\nstep:2475/5000 train_loss:8.0880 train_time:447183ms step_avg:181.41ms\nstep:2476/5000 train_loss:8.0964 train_time:447366ms step_avg:181.41ms\nstep:2477/5000 train_loss:8.2585 train_time:447548ms step_avg:181.41ms\nstep:2478/5000 train_loss:8.1926 train_time:447728ms step_avg:181.41ms\nstep:2479/5000 train_loss:8.1136 train_time:447911ms step_avg:181.41ms\nstep:2480/5000 train_loss:8.2281 train_time:448092ms step_avg:181.41ms\nstep:2481/5000 train_loss:8.0635 train_time:448274ms step_avg:181.41ms\nstep:2482/5000 train_loss:8.1585 train_time:448457ms step_avg:181.41ms\nstep:2483/5000 train_loss:7.8883 train_time:448640ms step_avg:181.42ms\nstep:2484/5000 train_loss:8.1750 train_time:448821ms step_avg:181.41ms\nstep:2485/5000 train_loss:8.2287 train_time:449003ms step_avg:181.42ms\nstep:2486/5000 train_loss:8.1035 train_time:449184ms step_avg:181.42ms\nstep:2487/5000 train_loss:8.1103 train_time:449365ms step_avg:181.42ms\nstep:2488/5000 train_loss:8.1369 train_time:449547ms step_avg:181.42ms\nstep:2489/5000 train_loss:8.3048 train_time:449728ms step_avg:181.42ms\nstep:2490/5000 train_loss:8.0764 train_time:449909ms step_avg:181.42ms\nstep:2491/5000 train_loss:8.1036 train_time:450090ms step_avg:181.41ms\nstep:2492/5000 train_loss:8.1225 train_time:450274ms step_avg:181.42ms\nstep:2493/5000 train_loss:8.1012 train_time:450455ms step_avg:181.42ms\nstep:2494/5000 train_loss:8.1974 train_time:450637ms step_avg:181.42ms\nstep:2495/5000 train_loss:8.1062 train_time:450820ms step_avg:181.42ms\nstep:2496/5000 train_loss:8.2170 train_time:451001ms step_avg:181.42ms\nstep:2497/5000 train_loss:8.1000 train_time:451183ms step_avg:181.42ms\nstep:2498/5000 train_loss:8.1104 train_time:451363ms step_avg:181.42ms\nstep:2499/5000 train_loss:8.0523 train_time:451545ms step_avg:181.42ms\nstep:2500/5000 train_loss:8.1516 train_time:451727ms step_avg:181.42ms\nstep:2501/5000 train_loss:8.1677 train_time:451908ms step_avg:181.42ms\nstep:2502/5000 train_loss:8.0875 train_time:452089ms step_avg:181.42ms\nstep:2503/5000 train_loss:8.0861 train_time:452271ms step_avg:181.42ms\nstep:2504/5000 train_loss:8.3133 train_time:452453ms step_avg:181.42ms\nstep:2505/5000 train_loss:8.1365 train_time:452635ms step_avg:181.42ms\nstep:2506/5000 train_loss:8.0804 train_time:452818ms step_avg:181.42ms\nstep:2507/5000 train_loss:8.1737 train_time:453000ms step_avg:181.42ms\nstep:2508/5000 train_loss:8.2010 train_time:453181ms step_avg:181.42ms\nstep:2509/5000 train_loss:8.0553 train_time:453362ms step_avg:181.42ms\nstep:2510/5000 train_loss:8.0505 train_time:453544ms step_avg:181.42ms\nstep:2511/5000 train_loss:8.0169 train_time:453725ms step_avg:181.42ms\nstep:2512/5000 train_loss:8.0404 train_time:453907ms step_avg:181.42ms\nstep:2513/5000 train_loss:8.1787 train_time:454088ms step_avg:181.42ms\nstep:2514/5000 train_loss:8.1384 train_time:454269ms step_avg:181.42ms\nstep:2515/5000 train_loss:8.1166 train_time:454450ms step_avg:181.42ms\nstep:2516/5000 train_loss:8.2130 train_time:454631ms step_avg:181.42ms\nstep:2517/5000 train_loss:8.0578 train_time:454814ms step_avg:181.42ms\nstep:2518/5000 train_loss:8.1381 train_time:454997ms step_avg:181.42ms\nstep:2519/5000 train_loss:8.1155 train_time:455179ms step_avg:181.42ms\nstep:2520/5000 train_loss:8.0269 train_time:455361ms step_avg:181.42ms\nstep:2521/5000 train_loss:8.1039 train_time:455543ms step_avg:181.42ms\nstep:2522/5000 train_loss:8.1644 train_time:455725ms step_avg:181.42ms\nstep:2523/5000 train_loss:8.1671 train_time:455907ms step_avg:181.42ms\nstep:2524/5000 train_loss:8.0789 train_time:456089ms step_avg:181.42ms\nstep:2525/5000 train_loss:8.1148 train_time:456270ms step_avg:181.42ms\nstep:2526/5000 train_loss:8.1254 train_time:456451ms step_avg:181.42ms\nstep:2527/5000 train_loss:8.0618 train_time:456633ms step_avg:181.42ms\nstep:2528/5000 train_loss:8.1885 train_time:456817ms step_avg:181.42ms\nstep:2529/5000 train_loss:8.1148 train_time:457000ms step_avg:181.42ms\nstep:2530/5000 train_loss:8.1181 train_time:457181ms step_avg:181.42ms\nstep:2531/5000 train_loss:8.1238 train_time:457362ms step_avg:181.42ms\nstep:2532/5000 train_loss:8.1567 train_time:457543ms step_avg:181.42ms\nstep:2533/5000 train_loss:8.0841 train_time:457725ms step_avg:181.42ms\nstep:2534/5000 train_loss:8.2750 train_time:457907ms step_avg:181.42ms\nstep:2535/5000 train_loss:8.1472 train_time:458088ms step_avg:181.42ms\nstep:2536/5000 train_loss:8.0186 train_time:458269ms step_avg:181.42ms\nstep:2537/5000 train_loss:8.0994 train_time:458451ms step_avg:181.42ms\nstep:2538/5000 train_loss:8.1323 train_time:458633ms step_avg:181.42ms\nstep:2539/5000 train_loss:8.2043 train_time:458816ms step_avg:181.42ms\nstep:2540/5000 train_loss:7.9975 train_time:458999ms step_avg:181.42ms\nstep:2541/5000 train_loss:8.0673 train_time:459180ms step_avg:181.42ms\nstep:2542/5000 train_loss:8.1650 train_time:459361ms step_avg:181.42ms\nstep:2543/5000 train_loss:8.2607 train_time:459542ms step_avg:181.42ms\nstep:2544/5000 train_loss:8.1179 train_time:459724ms step_avg:181.42ms\nstep:2545/5000 train_loss:8.1960 train_time:459905ms step_avg:181.42ms\nstep:2546/5000 train_loss:8.0365 train_time:460086ms step_avg:181.42ms\nstep:2547/5000 train_loss:8.0974 train_time:460268ms step_avg:181.42ms\nstep:2548/5000 train_loss:8.0302 train_time:460448ms step_avg:181.42ms\nstep:2549/5000 train_loss:8.0821 train_time:460629ms step_avg:181.42ms\nstep:2550/5000 train_loss:8.0069 train_time:460814ms step_avg:181.42ms\nstep:2551/5000 train_loss:8.1903 train_time:460998ms step_avg:181.42ms\nstep:2552/5000 train_loss:8.3219 train_time:461178ms step_avg:181.42ms\nstep:2553/5000 train_loss:8.1260 train_time:461359ms step_avg:181.42ms\nstep:2554/5000 train_loss:8.0061 train_time:461541ms step_avg:181.42ms\nstep:2555/5000 train_loss:8.0899 train_time:461724ms step_avg:181.42ms\nstep:2556/5000 train_loss:8.0948 train_time:461906ms step_avg:181.42ms\nstep:2557/5000 train_loss:8.1125 train_time:462087ms step_avg:181.42ms\nstep:2558/5000 train_loss:8.0876 train_time:462268ms step_avg:181.42ms\nstep:2559/5000 train_loss:8.1086 train_time:462449ms step_avg:181.42ms\nstep:2560/5000 train_loss:8.0251 train_time:462630ms step_avg:181.42ms\nstep:2561/5000 train_loss:8.1870 train_time:462813ms step_avg:181.42ms\nstep:2562/5000 train_loss:8.0649 train_time:462996ms step_avg:181.42ms\nstep:2563/5000 train_loss:8.0865 train_time:463178ms step_avg:181.42ms\nstep:2564/5000 train_loss:8.2245 train_time:463360ms step_avg:181.43ms\nstep:2565/5000 train_loss:8.0729 train_time:463541ms step_avg:181.43ms\nstep:2566/5000 train_loss:8.1409 train_time:463723ms step_avg:181.43ms\nstep:2567/5000 train_loss:8.1733 train_time:463905ms step_avg:181.43ms\nstep:2568/5000 train_loss:8.1362 train_time:464085ms step_avg:181.42ms\nstep:2569/5000 train_loss:8.1371 train_time:464266ms step_avg:181.42ms\nstep:2570/5000 train_loss:8.1075 train_time:464446ms step_avg:181.42ms\nstep:2571/5000 train_loss:8.0781 train_time:464627ms step_avg:181.42ms\nstep:2572/5000 train_loss:8.1373 train_time:464808ms step_avg:181.42ms\nstep:2573/5000 train_loss:8.2356 train_time:464990ms step_avg:181.42ms\nstep:2574/5000 train_loss:8.0554 train_time:465173ms step_avg:181.42ms\nstep:2575/5000 train_loss:8.0788 train_time:465356ms step_avg:181.43ms\nstep:2576/5000 train_loss:8.2037 train_time:465539ms step_avg:181.43ms\nstep:2577/5000 train_loss:8.1155 train_time:465721ms step_avg:181.43ms\nstep:2578/5000 train_loss:8.1193 train_time:465903ms step_avg:181.43ms\nstep:2579/5000 train_loss:8.1761 train_time:466085ms step_avg:181.43ms\nstep:2580/5000 train_loss:8.1511 train_time:466265ms step_avg:181.43ms\nstep:2581/5000 train_loss:8.0932 train_time:466446ms step_avg:181.43ms\nstep:2582/5000 train_loss:7.9820 train_time:466626ms step_avg:181.43ms\nstep:2583/5000 train_loss:8.1423 train_time:466808ms step_avg:181.43ms\nstep:2584/5000 train_loss:8.1879 train_time:466989ms step_avg:181.43ms\nstep:2585/5000 train_loss:8.0983 train_time:467170ms step_avg:181.43ms\nstep:2586/5000 train_loss:8.0862 train_time:467352ms step_avg:181.43ms\nstep:2587/5000 train_loss:8.0141 train_time:467532ms step_avg:181.43ms\nstep:2588/5000 train_loss:8.1275 train_time:467716ms step_avg:181.43ms\nstep:2589/5000 train_loss:8.0620 train_time:467899ms step_avg:181.43ms\nstep:2590/5000 train_loss:8.0653 train_time:468080ms step_avg:181.43ms\nstep:2591/5000 train_loss:8.0880 train_time:468261ms step_avg:181.43ms\nstep:2592/5000 train_loss:8.1543 train_time:468444ms step_avg:181.43ms\nstep:2593/5000 train_loss:8.1028 train_time:468625ms step_avg:181.43ms\nstep:2594/5000 train_loss:8.2434 train_time:468807ms step_avg:181.43ms\nstep:2595/5000 train_loss:8.1784 train_time:468989ms step_avg:181.43ms\nstep:2596/5000 train_loss:8.0409 train_time:469170ms step_avg:181.43ms\nstep:2597/5000 train_loss:8.2387 train_time:469351ms step_avg:181.43ms\nstep:2598/5000 train_loss:8.1676 train_time:469533ms step_avg:181.43ms\nstep:2599/5000 train_loss:8.1481 train_time:469717ms step_avg:181.43ms\nstep:2600/5000 train_loss:8.2118 train_time:469899ms step_avg:181.43ms\nstep:2601/5000 train_loss:8.2580 train_time:470081ms step_avg:181.43ms\nstep:2602/5000 train_loss:8.4595 train_time:470262ms step_avg:181.43ms\nstep:2603/5000 train_loss:8.1353 train_time:470443ms step_avg:181.43ms\nstep:2604/5000 train_loss:8.0887 train_time:470625ms step_avg:181.43ms\nstep:2605/5000 train_loss:8.2481 train_time:470806ms step_avg:181.43ms\nstep:2606/5000 train_loss:7.9900 train_time:470988ms step_avg:181.43ms\nstep:2607/5000 train_loss:8.0998 train_time:471168ms step_avg:181.43ms\nstep:2608/5000 train_loss:8.1384 train_time:471350ms step_avg:181.43ms\nstep:2609/5000 train_loss:8.2003 train_time:471532ms step_avg:181.43ms\nstep:2610/5000 train_loss:8.1215 train_time:471716ms step_avg:181.43ms\nstep:2611/5000 train_loss:8.0540 train_time:471899ms step_avg:181.43ms\nstep:2612/5000 train_loss:8.1464 train_time:472079ms step_avg:181.43ms\nstep:2613/5000 train_loss:8.1591 train_time:472261ms step_avg:181.43ms\nstep:2614/5000 train_loss:8.1263 train_time:472442ms step_avg:181.43ms\nstep:2615/5000 train_loss:8.1045 train_time:472624ms step_avg:181.43ms\nstep:2616/5000 train_loss:8.0684 train_time:472805ms step_avg:181.43ms\nstep:2617/5000 train_loss:8.2487 train_time:472985ms step_avg:181.43ms\nstep:2618/5000 train_loss:8.1286 train_time:473165ms step_avg:181.43ms\nstep:2619/5000 train_loss:8.0763 train_time:473345ms step_avg:181.43ms\nstep:2620/5000 train_loss:7.9964 train_time:473525ms step_avg:181.43ms\nstep:2621/5000 train_loss:8.1247 train_time:473707ms step_avg:181.43ms\nstep:2622/5000 train_loss:8.2298 train_time:473889ms step_avg:181.43ms\nstep:2623/5000 train_loss:8.1073 train_time:474070ms step_avg:181.43ms\nstep:2624/5000 train_loss:8.0559 train_time:474251ms step_avg:181.43ms\nstep:2625/5000 train_loss:8.0381 train_time:474433ms step_avg:181.43ms\nstep:2626/5000 train_loss:8.0625 train_time:474617ms step_avg:181.43ms\nstep:2627/5000 train_loss:8.1419 train_time:474799ms step_avg:181.43ms\nstep:2628/5000 train_loss:8.0702 train_time:474980ms step_avg:181.43ms\nstep:2629/5000 train_loss:8.0839 train_time:475163ms step_avg:181.43ms\nstep:2630/5000 train_loss:8.1654 train_time:475344ms step_avg:181.43ms\nstep:2631/5000 train_loss:8.1389 train_time:475525ms step_avg:181.43ms\nstep:2632/5000 train_loss:8.1371 train_time:475706ms step_avg:181.43ms\nstep:2633/5000 train_loss:7.9616 train_time:475888ms step_avg:181.43ms\nstep:2634/5000 train_loss:8.1356 train_time:476067ms step_avg:181.43ms\nstep:2635/5000 train_loss:8.0621 train_time:476249ms step_avg:181.43ms\nstep:2636/5000 train_loss:8.1634 train_time:476429ms step_avg:181.43ms\nstep:2637/5000 train_loss:8.0960 train_time:476613ms step_avg:181.43ms\nstep:2638/5000 train_loss:8.1009 train_time:476795ms step_avg:181.43ms\nstep:2639/5000 train_loss:8.1403 train_time:476978ms step_avg:181.43ms\nstep:2640/5000 train_loss:8.1444 train_time:477161ms step_avg:181.43ms\nstep:2641/5000 train_loss:8.1123 train_time:477342ms step_avg:181.43ms\nstep:2642/5000 train_loss:8.2209 train_time:477523ms step_avg:181.43ms\nstep:2643/5000 train_loss:8.0810 train_time:477705ms step_avg:181.43ms\nstep:2644/5000 train_loss:8.1054 train_time:477887ms step_avg:181.43ms\nstep:2645/5000 train_loss:8.1609 train_time:478068ms step_avg:181.43ms\nstep:2646/5000 train_loss:8.0663 train_time:478250ms step_avg:181.43ms\nstep:2647/5000 train_loss:8.1201 train_time:478431ms step_avg:181.43ms\nstep:2648/5000 train_loss:8.0539 train_time:478614ms step_avg:181.43ms\nstep:2649/5000 train_loss:8.1438 train_time:478797ms step_avg:181.43ms\nstep:2650/5000 train_loss:8.1478 train_time:478980ms step_avg:181.43ms\nstep:2651/5000 train_loss:8.1543 train_time:479160ms step_avg:181.43ms\nstep:2652/5000 train_loss:8.1399 train_time:479343ms step_avg:181.43ms\nstep:2653/5000 train_loss:8.0839 train_time:479525ms step_avg:181.43ms\nstep:2654/5000 train_loss:8.1021 train_time:479706ms step_avg:181.43ms\nstep:2655/5000 train_loss:8.1026 train_time:479887ms step_avg:181.43ms\nstep:2656/5000 train_loss:8.0999 train_time:480069ms step_avg:181.43ms\nstep:2657/5000 train_loss:8.0965 train_time:480250ms step_avg:181.43ms\nstep:2658/5000 train_loss:7.9855 train_time:480430ms step_avg:181.43ms\nstep:2659/5000 train_loss:8.0818 train_time:480613ms step_avg:181.43ms\nstep:2660/5000 train_loss:8.2373 train_time:480795ms step_avg:181.43ms\nstep:2661/5000 train_loss:8.0978 train_time:480977ms step_avg:181.43ms\nstep:2662/5000 train_loss:8.0288 train_time:481158ms step_avg:181.43ms\nstep:2663/5000 train_loss:8.1640 train_time:481341ms step_avg:181.43ms\nstep:2664/5000 train_loss:8.0440 train_time:481522ms step_avg:181.43ms\nstep:2665/5000 train_loss:8.1833 train_time:481704ms step_avg:181.43ms\nstep:2666/5000 train_loss:8.1784 train_time:481885ms step_avg:181.43ms\nstep:2667/5000 train_loss:8.0645 train_time:482066ms step_avg:181.43ms\nstep:2668/5000 train_loss:8.2277 train_time:482248ms step_avg:181.43ms\nstep:2669/5000 train_loss:8.1574 train_time:482428ms step_avg:181.43ms\nstep:2670/5000 train_loss:8.1035 train_time:482611ms step_avg:181.43ms\nstep:2671/5000 train_loss:8.1898 train_time:482793ms step_avg:181.43ms\nstep:2672/5000 train_loss:8.1136 train_time:482975ms step_avg:181.43ms\nstep:2673/5000 train_loss:8.0544 train_time:483157ms step_avg:181.43ms\nstep:2674/5000 train_loss:8.1644 train_time:483341ms step_avg:181.43ms\nstep:2675/5000 train_loss:8.2683 train_time:483523ms step_avg:181.43ms\nstep:2676/5000 train_loss:8.0813 train_time:483705ms step_avg:181.43ms\nstep:2677/5000 train_loss:8.1311 train_time:483885ms step_avg:181.43ms\nstep:2678/5000 train_loss:8.1198 train_time:484066ms step_avg:181.43ms\nstep:2679/5000 train_loss:8.1089 train_time:484246ms step_avg:181.43ms\nstep:2680/5000 train_loss:8.0022 train_time:484426ms step_avg:181.43ms\nstep:2681/5000 train_loss:8.0864 train_time:484607ms step_avg:181.43ms\nstep:2682/5000 train_loss:8.0157 train_time:484788ms step_avg:181.43ms\nstep:2683/5000 train_loss:8.0180 train_time:484969ms step_avg:181.43ms\nstep:2684/5000 train_loss:8.0580 train_time:485151ms step_avg:181.43ms\nstep:2685/5000 train_loss:8.1473 train_time:485333ms step_avg:181.43ms\nstep:2686/5000 train_loss:8.1175 train_time:485516ms step_avg:181.43ms\nstep:2687/5000 train_loss:8.2407 train_time:485698ms step_avg:181.43ms\nstep:2688/5000 train_loss:8.0557 train_time:485880ms step_avg:181.43ms\nstep:2689/5000 train_loss:8.0356 train_time:486062ms step_avg:181.43ms\nstep:2690/5000 train_loss:7.9876 train_time:486243ms step_avg:181.43ms\nstep:2691/5000 train_loss:8.0891 train_time:486425ms step_avg:181.43ms\nstep:2692/5000 train_loss:8.1264 train_time:486607ms step_avg:181.43ms\nstep:2693/5000 train_loss:8.1448 train_time:486787ms step_avg:181.43ms\nstep:2694/5000 train_loss:8.1565 train_time:486969ms step_avg:181.43ms\nstep:2695/5000 train_loss:8.1248 train_time:487150ms step_avg:181.43ms\nstep:2696/5000 train_loss:8.1906 train_time:487332ms step_avg:181.43ms\nstep:2697/5000 train_loss:8.0794 train_time:487516ms step_avg:181.44ms\nstep:2698/5000 train_loss:8.2930 train_time:487699ms step_avg:181.44ms\nstep:2699/5000 train_loss:8.0802 train_time:487880ms step_avg:181.44ms\nstep:2700/5000 train_loss:8.2010 train_time:488062ms step_avg:181.44ms\nstep:2701/5000 train_loss:8.0916 train_time:488244ms step_avg:181.44ms\nstep:2702/5000 train_loss:8.1506 train_time:488425ms step_avg:181.44ms\nstep:2703/5000 train_loss:8.0848 train_time:488607ms step_avg:181.44ms\nstep:2704/5000 train_loss:8.0680 train_time:488789ms step_avg:181.44ms\nstep:2705/5000 train_loss:8.0984 train_time:488970ms step_avg:181.44ms\nstep:2706/5000 train_loss:7.9916 train_time:489151ms step_avg:181.44ms\nstep:2707/5000 train_loss:8.0551 train_time:489331ms step_avg:181.44ms\nstep:2708/5000 train_loss:8.1075 train_time:489514ms step_avg:181.44ms\nstep:2709/5000 train_loss:8.0720 train_time:489696ms step_avg:181.44ms\nstep:2710/5000 train_loss:8.1248 train_time:489880ms step_avg:181.44ms\nstep:2711/5000 train_loss:8.0313 train_time:490061ms step_avg:181.44ms\nstep:2712/5000 train_loss:7.9938 train_time:490243ms step_avg:181.44ms\nstep:2713/5000 train_loss:8.1219 train_time:490425ms step_avg:181.44ms\nstep:2714/5000 train_loss:8.1856 train_time:490606ms step_avg:181.44ms\nstep:2715/5000 train_loss:8.0753 train_time:490788ms step_avg:181.44ms\nstep:2716/5000 train_loss:8.0282 train_time:490968ms step_avg:181.44ms\nstep:2717/5000 train_loss:8.0065 train_time:491149ms step_avg:181.44ms\nstep:2718/5000 train_loss:8.2790 train_time:491330ms step_avg:181.44ms\nstep:2719/5000 train_loss:8.1173 train_time:491513ms step_avg:181.44ms\nstep:2720/5000 train_loss:8.1512 train_time:491696ms step_avg:181.44ms\nstep:2721/5000 train_loss:8.1503 train_time:491880ms step_avg:181.44ms\nstep:2722/5000 train_loss:8.1226 train_time:492061ms step_avg:181.44ms\nstep:2723/5000 train_loss:8.0346 train_time:492242ms step_avg:181.44ms\nstep:2724/5000 train_loss:8.0987 train_time:492424ms step_avg:181.44ms\nstep:2725/5000 train_loss:8.1425 train_time:492605ms step_avg:181.44ms\nstep:2726/5000 train_loss:8.1129 train_time:492787ms step_avg:181.44ms\nstep:2727/5000 train_loss:8.2616 train_time:492968ms step_avg:181.44ms\nstep:2728/5000 train_loss:8.1215 train_time:493150ms step_avg:181.44ms\nstep:2729/5000 train_loss:8.1097 train_time:493331ms step_avg:181.44ms\nstep:2730/5000 train_loss:8.1273 train_time:493515ms step_avg:181.44ms\nstep:2731/5000 train_loss:7.9999 train_time:493697ms step_avg:181.44ms\nstep:2732/5000 train_loss:8.0522 train_time:493879ms step_avg:181.44ms\nstep:2733/5000 train_loss:8.1273 train_time:494061ms step_avg:181.44ms\nstep:2734/5000 train_loss:8.0962 train_time:494242ms step_avg:181.44ms\nstep:2735/5000 train_loss:8.0931 train_time:494423ms step_avg:181.44ms\nstep:2736/5000 train_loss:8.1631 train_time:494606ms step_avg:181.44ms\nstep:2737/5000 train_loss:8.2396 train_time:494787ms step_avg:181.44ms\nstep:2738/5000 train_loss:8.0581 train_time:494968ms step_avg:181.44ms\nstep:2739/5000 train_loss:7.9829 train_time:495150ms step_avg:181.44ms\nstep:2740/5000 train_loss:8.1160 train_time:495330ms step_avg:181.44ms\nstep:2741/5000 train_loss:8.3445 train_time:495512ms step_avg:181.44ms\nstep:2742/5000 train_loss:8.0781 train_time:495694ms step_avg:181.44ms\nstep:2743/5000 train_loss:8.1388 train_time:495876ms step_avg:181.44ms\nstep:2744/5000 train_loss:8.1377 train_time:496058ms step_avg:181.44ms\nstep:2745/5000 train_loss:8.0675 train_time:496241ms step_avg:181.44ms\nstep:2746/5000 train_loss:8.1516 train_time:496423ms step_avg:181.44ms\nstep:2747/5000 train_loss:8.1223 train_time:496605ms step_avg:181.44ms\nstep:2748/5000 train_loss:8.1820 train_time:496786ms step_avg:181.44ms\nstep:2749/5000 train_loss:8.0477 train_time:496966ms step_avg:181.44ms\nstep:2750/5000 train_loss:8.1130 train_time:497146ms step_avg:181.44ms\nstep:2751/5000 train_loss:8.1693 train_time:497327ms step_avg:181.44ms\nstep:2752/5000 train_loss:8.1198 train_time:497508ms step_avg:181.44ms\nstep:2753/5000 train_loss:8.1011 train_time:497690ms step_avg:181.44ms\nstep:2754/5000 train_loss:8.1174 train_time:497873ms step_avg:181.44ms\nstep:2755/5000 train_loss:8.1002 train_time:498056ms step_avg:181.44ms\nstep:2756/5000 train_loss:7.8167 train_time:498238ms step_avg:181.44ms\nstep:2757/5000 train_loss:8.0400 train_time:498421ms step_avg:181.44ms\nstep:2758/5000 train_loss:8.1223 train_time:498602ms step_avg:181.44ms\nstep:2759/5000 train_loss:8.2481 train_time:498783ms step_avg:181.44ms\nstep:2760/5000 train_loss:8.0576 train_time:498963ms step_avg:181.44ms\nstep:2761/5000 train_loss:8.0574 train_time:499146ms step_avg:181.44ms\nstep:2762/5000 train_loss:8.0147 train_time:499328ms step_avg:181.44ms\nstep:2763/5000 train_loss:8.0866 train_time:499512ms step_avg:181.44ms\nstep:2764/5000 train_loss:8.0491 train_time:499693ms step_avg:181.44ms\nstep:2765/5000 train_loss:8.1138 train_time:499876ms step_avg:181.44ms\nstep:2766/5000 train_loss:8.2212 train_time:500057ms step_avg:181.44ms\nstep:2767/5000 train_loss:8.0931 train_time:500240ms step_avg:181.44ms\nstep:2768/5000 train_loss:8.2129 train_time:500423ms step_avg:181.44ms\nstep:2769/5000 train_loss:8.1286 train_time:500605ms step_avg:181.44ms\nstep:2770/5000 train_loss:8.1069 train_time:500786ms step_avg:181.44ms\nstep:2771/5000 train_loss:8.0579 train_time:500966ms step_avg:181.44ms\nstep:2772/5000 train_loss:8.1079 train_time:501147ms step_avg:181.44ms\nstep:2773/5000 train_loss:8.1344 train_time:501327ms step_avg:181.44ms\nstep:2774/5000 train_loss:8.1344 train_time:501509ms step_avg:181.44ms\nstep:2775/5000 train_loss:8.0998 train_time:501692ms step_avg:181.44ms\nstep:2776/5000 train_loss:8.1189 train_time:501874ms step_avg:181.44ms\nstep:2777/5000 train_loss:7.8289 train_time:502056ms step_avg:181.44ms\nstep:2778/5000 train_loss:8.0841 train_time:502238ms step_avg:181.44ms\nstep:2779/5000 train_loss:8.1317 train_time:502420ms step_avg:181.44ms\nstep:2780/5000 train_loss:8.1409 train_time:502602ms step_avg:181.44ms\nstep:2781/5000 train_loss:7.9900 train_time:502783ms step_avg:181.44ms\nstep:2782/5000 train_loss:8.1149 train_time:502965ms step_avg:181.44ms\nstep:2783/5000 train_loss:8.1183 train_time:503145ms step_avg:181.44ms\nstep:2784/5000 train_loss:8.0916 train_time:503327ms step_avg:181.44ms\nstep:2785/5000 train_loss:8.1034 train_time:503509ms step_avg:181.44ms\nstep:2786/5000 train_loss:7.9837 train_time:503689ms step_avg:181.44ms\nstep:2787/5000 train_loss:8.1280 train_time:503870ms step_avg:181.44ms\nstep:2788/5000 train_loss:8.1350 train_time:504052ms step_avg:181.44ms\nstep:2789/5000 train_loss:8.1105 train_time:504234ms step_avg:181.44ms\nstep:2790/5000 train_loss:8.3042 train_time:504417ms step_avg:181.44ms\nstep:2791/5000 train_loss:8.0494 train_time:504598ms step_avg:181.44ms\nstep:2792/5000 train_loss:8.2065 train_time:504781ms step_avg:181.45ms\nstep:2793/5000 train_loss:8.0478 train_time:504961ms step_avg:181.44ms\nstep:2794/5000 train_loss:8.1331 train_time:505143ms step_avg:181.45ms\nstep:2795/5000 train_loss:8.1447 train_time:505325ms step_avg:181.45ms\nstep:2796/5000 train_loss:8.1081 train_time:505507ms step_avg:181.45ms\nstep:2797/5000 train_loss:8.0392 train_time:505688ms step_avg:181.45ms\nstep:2798/5000 train_loss:8.1398 train_time:505869ms step_avg:181.45ms\nstep:2799/5000 train_loss:8.0260 train_time:506049ms step_avg:181.44ms\nstep:2800/5000 train_loss:8.2716 train_time:506229ms step_avg:181.44ms\nstep:2801/5000 train_loss:8.1418 train_time:506412ms step_avg:181.44ms\nstep:2802/5000 train_loss:8.0414 train_time:506594ms step_avg:181.44ms\nstep:2803/5000 train_loss:8.2275 train_time:506777ms step_avg:181.45ms\nstep:2804/5000 train_loss:8.1219 train_time:506959ms step_avg:181.45ms\nstep:2805/5000 train_loss:8.0775 train_time:507140ms step_avg:181.45ms\nstep:2806/5000 train_loss:8.0039 train_time:507322ms step_avg:181.45ms\nstep:2807/5000 train_loss:8.0395 train_time:507503ms step_avg:181.45ms\nstep:2808/5000 train_loss:7.9899 train_time:507685ms step_avg:181.45ms\nstep:2809/5000 train_loss:8.2254 train_time:507866ms step_avg:181.45ms\nstep:2810/5000 train_loss:8.4176 train_time:508047ms step_avg:181.45ms\nstep:2811/5000 train_loss:8.1021 train_time:508227ms step_avg:181.44ms\nstep:2812/5000 train_loss:8.0573 train_time:508409ms step_avg:181.44ms\nstep:2813/5000 train_loss:8.2442 train_time:508590ms step_avg:181.44ms\nstep:2814/5000 train_loss:8.1002 train_time:508770ms step_avg:181.44ms\nstep:2815/5000 train_loss:8.0957 train_time:508953ms step_avg:181.44ms\nstep:2816/5000 train_loss:7.9341 train_time:509135ms step_avg:181.44ms\nstep:2817/5000 train_loss:8.1067 train_time:509318ms step_avg:181.45ms\nstep:2818/5000 train_loss:8.1035 train_time:509502ms step_avg:181.45ms\nstep:2819/5000 train_loss:8.0603 train_time:509684ms step_avg:181.45ms\nstep:2820/5000 train_loss:8.0589 train_time:509865ms step_avg:181.45ms\nstep:2821/5000 train_loss:8.1722 train_time:510046ms step_avg:181.45ms\nstep:2822/5000 train_loss:7.9586 train_time:510226ms step_avg:181.45ms\nstep:2823/5000 train_loss:8.0913 train_time:510407ms step_avg:181.45ms\nstep:2824/5000 train_loss:8.1806 train_time:510588ms step_avg:181.45ms\nstep:2825/5000 train_loss:8.0987 train_time:510770ms step_avg:181.45ms\nstep:2826/5000 train_loss:7.9920 train_time:510952ms step_avg:181.45ms\nstep:2827/5000 train_loss:8.0970 train_time:511134ms step_avg:181.45ms\nstep:2828/5000 train_loss:8.0730 train_time:511317ms step_avg:181.45ms\nstep:2829/5000 train_loss:8.0714 train_time:511500ms step_avg:181.45ms\nstep:2830/5000 train_loss:8.2074 train_time:511681ms step_avg:181.45ms\nstep:2831/5000 train_loss:8.1305 train_time:511863ms step_avg:181.45ms\nstep:2832/5000 train_loss:8.0695 train_time:512044ms step_avg:181.45ms\nstep:2833/5000 train_loss:8.0879 train_time:512226ms step_avg:181.45ms\nstep:2834/5000 train_loss:8.0852 train_time:512409ms step_avg:181.45ms\nstep:2835/5000 train_loss:8.6185 train_time:512590ms step_avg:181.45ms\nstep:2836/5000 train_loss:8.0705 train_time:512771ms step_avg:181.45ms\nstep:2837/5000 train_loss:8.0532 train_time:512952ms step_avg:181.45ms\nstep:2838/5000 train_loss:8.1404 train_time:513134ms step_avg:181.45ms\nstep:2839/5000 train_loss:8.0910 train_time:513318ms step_avg:181.45ms\nstep:2840/5000 train_loss:8.0081 train_time:513500ms step_avg:181.45ms\nstep:2841/5000 train_loss:8.0558 train_time:513681ms step_avg:181.45ms\nstep:2842/5000 train_loss:8.0457 train_time:513861ms step_avg:181.45ms\nstep:2843/5000 train_loss:8.0847 train_time:514042ms step_avg:181.45ms\nstep:2844/5000 train_loss:8.1710 train_time:514223ms step_avg:181.45ms\nstep:2845/5000 train_loss:8.1663 train_time:514405ms step_avg:181.45ms\nstep:2846/5000 train_loss:8.2363 train_time:514586ms step_avg:181.45ms\nstep:2847/5000 train_loss:8.1410 train_time:514767ms step_avg:181.45ms\nstep:2848/5000 train_loss:8.1490 train_time:514947ms step_avg:181.45ms\nstep:2849/5000 train_loss:7.8870 train_time:515127ms step_avg:181.45ms\nstep:2850/5000 train_loss:8.0364 train_time:515308ms step_avg:181.45ms\nstep:2851/5000 train_loss:8.1530 train_time:515489ms step_avg:181.45ms\nstep:2852/5000 train_loss:8.1056 train_time:515672ms step_avg:181.45ms\nstep:2853/5000 train_loss:7.9821 train_time:515855ms step_avg:181.45ms\nstep:2854/5000 train_loss:8.1901 train_time:516035ms step_avg:181.45ms\nstep:2855/5000 train_loss:8.0192 train_time:516216ms step_avg:181.45ms\nstep:2856/5000 train_loss:8.1930 train_time:516399ms step_avg:181.45ms\nstep:2857/5000 train_loss:8.0787 train_time:516581ms step_avg:181.45ms\nstep:2858/5000 train_loss:8.1738 train_time:516763ms step_avg:181.45ms\nstep:2859/5000 train_loss:8.1890 train_time:516945ms step_avg:181.45ms\nstep:2860/5000 train_loss:8.1740 train_time:517126ms step_avg:181.45ms\nstep:2861/5000 train_loss:8.1153 train_time:517308ms step_avg:181.45ms\nstep:2862/5000 train_loss:8.0475 train_time:517491ms step_avg:181.45ms\nstep:2863/5000 train_loss:8.0575 train_time:517674ms step_avg:181.45ms\nstep:2864/5000 train_loss:8.2039 train_time:517857ms step_avg:181.45ms\nstep:2865/5000 train_loss:8.1321 train_time:518039ms step_avg:181.45ms\nstep:2866/5000 train_loss:8.1515 train_time:518222ms step_avg:181.45ms\nstep:2867/5000 train_loss:8.0965 train_time:518404ms step_avg:181.45ms\nstep:2868/5000 train_loss:8.1778 train_time:518585ms step_avg:181.45ms\nstep:2869/5000 train_loss:7.9032 train_time:518766ms step_avg:181.45ms\nstep:2870/5000 train_loss:8.0961 train_time:518946ms step_avg:181.45ms\nstep:2871/5000 train_loss:7.9857 train_time:519126ms step_avg:181.45ms\nstep:2872/5000 train_loss:8.2458 train_time:519307ms step_avg:181.45ms\nstep:2873/5000 train_loss:8.1062 train_time:519489ms step_avg:181.45ms\nstep:2874/5000 train_loss:8.1877 train_time:519671ms step_avg:181.45ms\nstep:2875/5000 train_loss:8.1224 train_time:519853ms step_avg:181.45ms\nstep:2876/5000 train_loss:8.0957 train_time:520033ms step_avg:181.45ms\nstep:2877/5000 train_loss:8.1431 train_time:520216ms step_avg:181.45ms\nstep:2878/5000 train_loss:8.1166 train_time:520398ms step_avg:181.45ms\nstep:2879/5000 train_loss:8.2034 train_time:520579ms step_avg:181.45ms\nstep:2880/5000 train_loss:8.1019 train_time:520761ms step_avg:181.45ms\nstep:2881/5000 train_loss:8.1393 train_time:520942ms step_avg:181.45ms\nstep:2882/5000 train_loss:8.1038 train_time:521124ms step_avg:181.45ms\nstep:2883/5000 train_loss:8.0059 train_time:521306ms step_avg:181.45ms\nstep:2884/5000 train_loss:8.0495 train_time:521486ms step_avg:181.45ms\nstep:2885/5000 train_loss:8.1153 train_time:521667ms step_avg:181.45ms\nstep:2886/5000 train_loss:8.1292 train_time:521847ms step_avg:181.45ms\nstep:2887/5000 train_loss:7.9951 train_time:522027ms step_avg:181.45ms\nstep:2888/5000 train_loss:8.0267 train_time:522209ms step_avg:181.45ms\nstep:2889/5000 train_loss:8.1666 train_time:522391ms step_avg:181.45ms\nstep:2890/5000 train_loss:8.0084 train_time:522573ms step_avg:181.45ms\nstep:2891/5000 train_loss:8.2264 train_time:522756ms step_avg:181.45ms\nstep:2892/5000 train_loss:8.1495 train_time:522939ms step_avg:181.45ms\nstep:2893/5000 train_loss:8.1823 train_time:523122ms step_avg:181.45ms\nstep:2894/5000 train_loss:8.1065 train_time:523304ms step_avg:181.45ms\nstep:2895/5000 train_loss:8.2207 train_time:523485ms step_avg:181.45ms\nstep:2896/5000 train_loss:8.1394 train_time:523664ms step_avg:181.45ms\nstep:2897/5000 train_loss:8.0751 train_time:523846ms step_avg:181.45ms\nstep:2898/5000 train_loss:8.1012 train_time:524026ms step_avg:181.45ms\nstep:2899/5000 train_loss:8.0691 train_time:524208ms step_avg:181.45ms\nstep:2900/5000 train_loss:8.1250 train_time:524390ms step_avg:181.45ms\nstep:2901/5000 train_loss:8.0951 train_time:524573ms step_avg:181.45ms\nstep:2902/5000 train_loss:8.0882 train_time:524755ms step_avg:181.45ms\nstep:2903/5000 train_loss:8.0993 train_time:524937ms step_avg:181.45ms\nstep:2904/5000 train_loss:8.1272 train_time:525121ms step_avg:181.45ms\nstep:2905/5000 train_loss:8.0418 train_time:525302ms step_avg:181.45ms\nstep:2906/5000 train_loss:8.0749 train_time:525484ms step_avg:181.45ms\nstep:2907/5000 train_loss:8.0614 train_time:525665ms step_avg:181.45ms\nstep:2908/5000 train_loss:8.1136 train_time:525845ms step_avg:181.45ms\nstep:2909/5000 train_loss:8.2799 train_time:526026ms step_avg:181.45ms\nstep:2910/5000 train_loss:8.0430 train_time:526208ms step_avg:181.45ms\nstep:2911/5000 train_loss:8.1705 train_time:526389ms step_avg:181.45ms\nstep:2912/5000 train_loss:7.9466 train_time:526571ms step_avg:181.45ms\nstep:2913/5000 train_loss:8.1302 train_time:526754ms step_avg:181.45ms\nstep:2914/5000 train_loss:8.1585 train_time:526936ms step_avg:181.45ms\nstep:2915/5000 train_loss:8.1208 train_time:527118ms step_avg:181.45ms\nstep:2916/5000 train_loss:8.1501 train_time:527300ms step_avg:181.45ms\nstep:2917/5000 train_loss:8.0370 train_time:527481ms step_avg:181.45ms\nstep:2918/5000 train_loss:7.9941 train_time:527664ms step_avg:181.45ms\nstep:2919/5000 train_loss:7.9485 train_time:527845ms step_avg:181.45ms\nstep:2920/5000 train_loss:8.2577 train_time:528026ms step_avg:181.45ms\nstep:2921/5000 train_loss:8.1791 train_time:528208ms step_avg:181.45ms\nstep:2922/5000 train_loss:7.9825 train_time:528389ms step_avg:181.45ms\nstep:2923/5000 train_loss:8.1816 train_time:528570ms step_avg:181.45ms\nstep:2924/5000 train_loss:8.1763 train_time:528751ms step_avg:181.45ms\nstep:2925/5000 train_loss:8.0883 train_time:528933ms step_avg:181.45ms\nstep:2926/5000 train_loss:8.0483 train_time:529116ms step_avg:181.45ms\nstep:2927/5000 train_loss:8.1642 train_time:529298ms step_avg:181.45ms\nstep:2928/5000 train_loss:7.9894 train_time:529481ms step_avg:181.45ms\nstep:2929/5000 train_loss:8.0704 train_time:529663ms step_avg:181.45ms\nstep:2930/5000 train_loss:8.0471 train_time:529843ms step_avg:181.45ms\nstep:2931/5000 train_loss:8.1689 train_time:530024ms step_avg:181.45ms\nstep:2932/5000 train_loss:8.1658 train_time:530206ms step_avg:181.45ms\nstep:2933/5000 train_loss:8.0421 train_time:530387ms step_avg:181.45ms\nstep:2934/5000 train_loss:8.1244 train_time:530566ms step_avg:181.45ms\nstep:2935/5000 train_loss:8.0559 train_time:530747ms step_avg:181.45ms\nstep:2936/5000 train_loss:8.2864 train_time:530928ms step_avg:181.45ms\nstep:2937/5000 train_loss:7.9946 train_time:531110ms step_avg:181.45ms\nstep:2938/5000 train_loss:8.1190 train_time:531292ms step_avg:181.45ms\nstep:2939/5000 train_loss:8.1302 train_time:531475ms step_avg:181.45ms\nstep:2940/5000 train_loss:8.0219 train_time:531656ms step_avg:181.45ms\nstep:2941/5000 train_loss:8.1119 train_time:531839ms step_avg:181.45ms\nstep:2942/5000 train_loss:8.3188 train_time:532021ms step_avg:181.45ms\nstep:2943/5000 train_loss:8.0430 train_time:532202ms step_avg:181.45ms\nstep:2944/5000 train_loss:8.1476 train_time:532383ms step_avg:181.45ms\nstep:2945/5000 train_loss:8.1904 train_time:532564ms step_avg:181.45ms\nstep:2946/5000 train_loss:8.1412 train_time:532744ms step_avg:181.45ms\nstep:2947/5000 train_loss:8.0371 train_time:532925ms step_avg:181.45ms\nstep:2948/5000 train_loss:8.1709 train_time:533107ms step_avg:181.45ms\nstep:2949/5000 train_loss:7.9917 train_time:533288ms step_avg:181.45ms\nstep:2950/5000 train_loss:8.0846 train_time:533468ms step_avg:181.45ms\nstep:2951/5000 train_loss:8.2004 train_time:533650ms step_avg:181.45ms\nstep:2952/5000 train_loss:8.0689 train_time:533831ms step_avg:181.45ms\nstep:2953/5000 train_loss:8.0650 train_time:534014ms step_avg:181.45ms\nstep:2954/5000 train_loss:8.0505 train_time:534196ms step_avg:181.45ms\nstep:2955/5000 train_loss:8.1344 train_time:534379ms step_avg:181.45ms\nstep:2956/5000 train_loss:8.0938 train_time:534561ms step_avg:181.45ms\nstep:2957/5000 train_loss:8.0743 train_time:534741ms step_avg:181.45ms\nstep:2958/5000 train_loss:8.1722 train_time:534923ms step_avg:181.45ms\nstep:2959/5000 train_loss:8.1923 train_time:535105ms step_avg:181.45ms\nstep:2960/5000 train_loss:8.0210 train_time:535286ms step_avg:181.45ms\nstep:2961/5000 train_loss:8.1259 train_time:535467ms step_avg:181.45ms\nstep:2962/5000 train_loss:8.1346 train_time:535649ms step_avg:181.45ms\nstep:2963/5000 train_loss:8.0969 train_time:535829ms step_avg:181.45ms\nstep:2964/5000 train_loss:7.9783 train_time:536011ms step_avg:181.45ms\nstep:2965/5000 train_loss:8.0792 train_time:536194ms step_avg:181.45ms\nstep:2966/5000 train_loss:8.1520 train_time:536376ms step_avg:181.45ms\nstep:2967/5000 train_loss:8.1057 train_time:536559ms step_avg:181.45ms\nstep:2968/5000 train_loss:8.1241 train_time:536741ms step_avg:181.45ms\nstep:2969/5000 train_loss:8.1372 train_time:536923ms step_avg:181.45ms\nstep:2970/5000 train_loss:8.1691 train_time:537104ms step_avg:181.45ms\nstep:2971/5000 train_loss:8.1209 train_time:537285ms step_avg:181.45ms\nstep:2972/5000 train_loss:8.2676 train_time:537465ms step_avg:181.45ms\nstep:2973/5000 train_loss:8.3245 train_time:537646ms step_avg:181.45ms\nstep:2974/5000 train_loss:8.1343 train_time:537827ms step_avg:181.45ms\nstep:2975/5000 train_loss:8.1271 train_time:538009ms step_avg:181.45ms\nstep:2976/5000 train_loss:8.1641 train_time:538191ms step_avg:181.45ms\nstep:2977/5000 train_loss:8.1348 train_time:538373ms step_avg:181.45ms\nstep:2978/5000 train_loss:8.1754 train_time:538556ms step_avg:181.45ms\nstep:2979/5000 train_loss:8.1416 train_time:538739ms step_avg:181.45ms\nstep:2980/5000 train_loss:8.1121 train_time:538922ms step_avg:181.46ms\nstep:2981/5000 train_loss:8.0902 train_time:539103ms step_avg:181.46ms\nstep:2982/5000 train_loss:8.0805 train_time:539284ms step_avg:181.46ms\nstep:2983/5000 train_loss:8.0082 train_time:539465ms step_avg:181.45ms\nstep:2984/5000 train_loss:8.1342 train_time:539645ms step_avg:181.45ms\nstep:2985/5000 train_loss:8.0660 train_time:539826ms step_avg:181.45ms\nstep:2986/5000 train_loss:8.1012 train_time:540008ms step_avg:181.45ms\nstep:2987/5000 train_loss:8.0218 train_time:540188ms step_avg:181.45ms\nstep:2988/5000 train_loss:8.1104 train_time:540371ms step_avg:181.45ms\nstep:2989/5000 train_loss:8.1666 train_time:540554ms step_avg:181.45ms\nstep:2990/5000 train_loss:8.1619 train_time:540735ms step_avg:181.45ms\nstep:2991/5000 train_loss:8.0341 train_time:540918ms step_avg:181.46ms\nstep:2992/5000 train_loss:8.1243 train_time:541100ms step_avg:181.46ms\nstep:2993/5000 train_loss:8.0559 train_time:541281ms step_avg:181.46ms\nstep:2994/5000 train_loss:8.0892 train_time:541463ms step_avg:181.46ms\nstep:2995/5000 train_loss:8.1843 train_time:541644ms step_avg:181.46ms\nstep:2996/5000 train_loss:8.0207 train_time:541825ms step_avg:181.45ms\nstep:2997/5000 train_loss:8.1227 train_time:542006ms step_avg:181.45ms\nstep:2998/5000 train_loss:8.1659 train_time:542187ms step_avg:181.45ms\nstep:2999/5000 train_loss:8.1397 train_time:542367ms step_avg:181.45ms\nstep:3000/5000 train_loss:8.0997 train_time:542548ms step_avg:181.45ms\nstep:3001/5000 train_loss:8.1040 train_time:542729ms step_avg:181.45ms\nstep:3002/5000 train_loss:7.9879 train_time:542911ms step_avg:181.45ms\nstep:3003/5000 train_loss:8.0637 train_time:543094ms step_avg:181.45ms\nstep:3004/5000 train_loss:8.0855 train_time:543275ms step_avg:181.45ms\nstep:3005/5000 train_loss:8.1449 train_time:543457ms step_avg:181.45ms\nstep:3006/5000 train_loss:8.1587 train_time:543639ms step_avg:181.46ms\nstep:3007/5000 train_loss:7.9262 train_time:543821ms step_avg:181.46ms\nstep:3008/5000 train_loss:8.0814 train_time:544003ms step_avg:181.46ms\nstep:3009/5000 train_loss:8.1573 train_time:544184ms step_avg:181.46ms\nstep:3010/5000 train_loss:7.9542 train_time:544364ms step_avg:181.45ms\nstep:3011/5000 train_loss:8.0381 train_time:544544ms step_avg:181.45ms\nstep:3012/5000 train_loss:8.1016 train_time:544725ms step_avg:181.45ms\nstep:3013/5000 train_loss:8.1435 train_time:544906ms step_avg:181.45ms\nstep:3014/5000 train_loss:8.1315 train_time:545087ms step_avg:181.45ms\nstep:3015/5000 train_loss:8.0032 train_time:545268ms step_avg:181.45ms\nstep:3016/5000 train_loss:8.0779 train_time:545450ms step_avg:181.45ms\nstep:3017/5000 train_loss:8.0653 train_time:545631ms step_avg:181.45ms\nstep:3018/5000 train_loss:8.0319 train_time:545814ms step_avg:181.45ms\nstep:3019/5000 train_loss:8.1190 train_time:545996ms step_avg:181.45ms\nstep:3020/5000 train_loss:8.0619 train_time:546178ms step_avg:181.45ms\nstep:3021/5000 train_loss:8.1492 train_time:546359ms step_avg:181.45ms\nstep:3022/5000 train_loss:8.0318 train_time:546541ms step_avg:181.45ms\nstep:3023/5000 train_loss:7.9926 train_time:546722ms step_avg:181.45ms\nstep:3024/5000 train_loss:8.1503 train_time:546905ms step_avg:181.45ms\nstep:3025/5000 train_loss:8.1588 train_time:547086ms step_avg:181.45ms\nstep:3026/5000 train_loss:8.0904 train_time:547267ms step_avg:181.45ms\nstep:3027/5000 train_loss:8.1666 train_time:547449ms step_avg:181.45ms\nstep:3028/5000 train_loss:8.1334 train_time:547631ms step_avg:181.46ms\nstep:3029/5000 train_loss:8.1191 train_time:547814ms step_avg:181.46ms\nstep:3030/5000 train_loss:8.2522 train_time:547997ms step_avg:181.46ms\nstep:3031/5000 train_loss:8.2080 train_time:548179ms step_avg:181.46ms\nstep:3032/5000 train_loss:8.1124 train_time:548360ms step_avg:181.46ms\nstep:3033/5000 train_loss:8.1165 train_time:548541ms step_avg:181.46ms\nstep:3034/5000 train_loss:8.1745 train_time:548722ms step_avg:181.46ms\nstep:3035/5000 train_loss:8.0564 train_time:548904ms step_avg:181.46ms\nstep:3036/5000 train_loss:8.0914 train_time:549084ms step_avg:181.46ms\nstep:3037/5000 train_loss:8.1248 train_time:549265ms step_avg:181.46ms\nstep:3038/5000 train_loss:8.1234 train_time:549446ms step_avg:181.45ms\nstep:3039/5000 train_loss:8.0342 train_time:549626ms step_avg:181.45ms\nstep:3040/5000 train_loss:7.9152 train_time:549807ms step_avg:181.45ms\nstep:3041/5000 train_loss:8.1041 train_time:549988ms step_avg:181.45ms\nstep:3042/5000 train_loss:8.1022 train_time:550171ms step_avg:181.45ms\nstep:3043/5000 train_loss:8.1284 train_time:550354ms step_avg:181.46ms\nstep:3044/5000 train_loss:8.0761 train_time:550535ms step_avg:181.46ms\nstep:3045/5000 train_loss:8.1455 train_time:550717ms step_avg:181.46ms\nstep:3046/5000 train_loss:8.3091 train_time:550899ms step_avg:181.46ms\nstep:3047/5000 train_loss:8.1510 train_time:551081ms step_avg:181.46ms\nstep:3048/5000 train_loss:8.1246 train_time:551494ms step_avg:181.53ms\nstep:3049/5000 train_loss:8.1415 train_time:551673ms step_avg:181.53ms\nstep:3050/5000 train_loss:8.0431 train_time:551854ms step_avg:181.53ms\nstep:3051/5000 train_loss:8.0415 train_time:552034ms step_avg:181.53ms\nstep:3052/5000 train_loss:8.2394 train_time:552215ms step_avg:181.53ms\nstep:3053/5000 train_loss:8.0261 train_time:552403ms step_avg:181.53ms\nstep:3054/5000 train_loss:8.2320 train_time:552585ms step_avg:181.53ms\nstep:3055/5000 train_loss:8.2505 train_time:552765ms step_avg:181.53ms\nstep:3056/5000 train_loss:7.9143 train_time:552945ms step_avg:181.53ms\nstep:3057/5000 train_loss:8.1091 train_time:553125ms step_avg:181.53ms\nstep:3058/5000 train_loss:8.1272 train_time:553306ms step_avg:181.53ms\nstep:3059/5000 train_loss:8.1579 train_time:553486ms step_avg:181.53ms\nstep:3060/5000 train_loss:8.2071 train_time:553668ms step_avg:181.53ms\nstep:3061/5000 train_loss:8.1035 train_time:553849ms step_avg:181.53ms\nstep:3062/5000 train_loss:8.1498 train_time:554030ms step_avg:181.53ms\nstep:3063/5000 train_loss:8.1080 train_time:554213ms step_avg:181.53ms\nstep:3064/5000 train_loss:8.1210 train_time:554395ms step_avg:181.53ms\nstep:3065/5000 train_loss:8.0855 train_time:554578ms step_avg:181.53ms\nstep:3066/5000 train_loss:8.1091 train_time:554761ms step_avg:181.53ms\nstep:3067/5000 train_loss:8.2222 train_time:554942ms step_avg:181.53ms\nstep:3068/5000 train_loss:8.2491 train_time:555124ms step_avg:181.53ms\nstep:3069/5000 train_loss:8.0995 train_time:555305ms step_avg:181.53ms\nstep:3070/5000 train_loss:8.2121 train_time:555486ms step_avg:181.53ms\nstep:3071/5000 train_loss:8.0314 train_time:555665ms step_avg:181.53ms\nstep:3072/5000 train_loss:8.0292 train_time:555847ms step_avg:181.53ms\nstep:3073/5000 train_loss:8.1124 train_time:556028ms step_avg:181.53ms\nstep:3074/5000 train_loss:8.1263 train_time:556211ms step_avg:181.53ms\nstep:3075/5000 train_loss:8.1922 train_time:556393ms step_avg:181.53ms\nstep:3076/5000 train_loss:8.2172 train_time:556576ms step_avg:181.53ms\nstep:3077/5000 train_loss:8.1046 train_time:556758ms step_avg:181.53ms\nstep:3078/5000 train_loss:8.0829 train_time:556941ms step_avg:181.53ms\nstep:3079/5000 train_loss:8.0893 train_time:557122ms step_avg:181.53ms\nstep:3080/5000 train_loss:8.0868 train_time:557304ms step_avg:181.53ms\nstep:3081/5000 train_loss:8.0968 train_time:557484ms step_avg:181.53ms\nstep:3082/5000 train_loss:8.1972 train_time:557664ms step_avg:181.53ms\nstep:3083/5000 train_loss:8.1375 train_time:557845ms step_avg:181.53ms\nstep:3084/5000 train_loss:8.3338 train_time:558026ms step_avg:181.53ms\nstep:3085/5000 train_loss:7.8486 train_time:558208ms step_avg:181.53ms\nstep:3086/5000 train_loss:8.1844 train_time:558389ms step_avg:181.53ms\nstep:3087/5000 train_loss:8.0767 train_time:558570ms step_avg:181.53ms\nstep:3088/5000 train_loss:8.2156 train_time:558751ms step_avg:181.53ms\nstep:3089/5000 train_loss:8.1193 train_time:558933ms step_avg:181.53ms\nstep:3090/5000 train_loss:8.2357 train_time:559116ms step_avg:181.53ms\nstep:3091/5000 train_loss:8.0927 train_time:559297ms step_avg:181.53ms\nstep:3092/5000 train_loss:8.1022 train_time:559478ms step_avg:181.53ms\nstep:3093/5000 train_loss:8.1559 train_time:559660ms step_avg:181.53ms\nstep:3094/5000 train_loss:8.0521 train_time:559842ms step_avg:181.53ms\nstep:3095/5000 train_loss:8.0351 train_time:560022ms step_avg:181.53ms\nstep:3096/5000 train_loss:8.1767 train_time:560205ms step_avg:181.53ms\nstep:3097/5000 train_loss:8.0500 train_time:560386ms step_avg:181.53ms\nstep:3098/5000 train_loss:8.2315 train_time:560566ms step_avg:181.53ms\nstep:3099/5000 train_loss:8.0945 train_time:560746ms step_avg:181.53ms\nstep:3100/5000 train_loss:8.1751 train_time:560926ms step_avg:181.53ms\nstep:3101/5000 train_loss:8.0918 train_time:561107ms step_avg:181.53ms\nstep:3102/5000 train_loss:8.0715 train_time:561289ms step_avg:181.53ms\nstep:3103/5000 train_loss:8.2740 train_time:561469ms step_avg:181.53ms\nstep:3104/5000 train_loss:8.0615 train_time:561651ms step_avg:181.53ms\nstep:3105/5000 train_loss:8.1373 train_time:561834ms step_avg:181.53ms\nstep:3106/5000 train_loss:8.0950 train_time:562017ms step_avg:181.53ms\nstep:3107/5000 train_loss:8.0676 train_time:562199ms step_avg:181.53ms\nstep:3108/5000 train_loss:8.0301 train_time:562380ms step_avg:181.53ms\nstep:3109/5000 train_loss:8.1006 train_time:562561ms step_avg:181.53ms\nstep:3110/5000 train_loss:8.2387 train_time:562743ms step_avg:181.53ms\nstep:3111/5000 train_loss:8.1825 train_time:562924ms step_avg:181.53ms\nstep:3112/5000 train_loss:8.1036 train_time:563106ms step_avg:181.53ms\nstep:3113/5000 train_loss:8.1537 train_time:563286ms step_avg:181.53ms\nstep:3114/5000 train_loss:8.0575 train_time:563466ms step_avg:181.53ms\nstep:3115/5000 train_loss:8.1315 train_time:563648ms step_avg:181.53ms\nstep:3116/5000 train_loss:8.1271 train_time:563829ms step_avg:181.53ms\nstep:3117/5000 train_loss:8.0727 train_time:564011ms step_avg:181.53ms\nstep:3118/5000 train_loss:8.0466 train_time:564194ms step_avg:181.53ms\nstep:3119/5000 train_loss:8.1015 train_time:564376ms step_avg:181.53ms\nstep:3120/5000 train_loss:8.0805 train_time:564557ms step_avg:181.53ms\nstep:3121/5000 train_loss:8.1327 train_time:564739ms step_avg:181.53ms\nstep:3122/5000 train_loss:8.0079 train_time:564922ms step_avg:181.53ms\nstep:3123/5000 train_loss:8.1860 train_time:565103ms step_avg:181.53ms\nstep:3124/5000 train_loss:7.8813 train_time:565284ms step_avg:181.53ms\nstep:3125/5000 train_loss:8.0801 train_time:565464ms step_avg:181.53ms\nstep:3126/5000 train_loss:8.1420 train_time:565644ms step_avg:181.53ms\nstep:3127/5000 train_loss:8.1702 train_time:565825ms step_avg:181.53ms\nstep:3128/5000 train_loss:7.9826 train_time:566006ms step_avg:181.53ms\nstep:3129/5000 train_loss:8.0863 train_time:566188ms step_avg:181.53ms\nstep:3130/5000 train_loss:8.1444 train_time:566370ms step_avg:181.53ms\nstep:3131/5000 train_loss:8.0673 train_time:566552ms step_avg:181.53ms\nstep:3132/5000 train_loss:8.1618 train_time:566733ms step_avg:181.53ms\nstep:3133/5000 train_loss:8.1715 train_time:566917ms step_avg:181.53ms\nstep:3134/5000 train_loss:7.9845 train_time:567099ms step_avg:181.53ms\nstep:3135/5000 train_loss:8.0529 train_time:567280ms step_avg:181.53ms\nstep:3136/5000 train_loss:8.1250 train_time:567462ms step_avg:181.53ms\nstep:3137/5000 train_loss:8.0437 train_time:567644ms step_avg:181.53ms\nstep:3138/5000 train_loss:8.1956 train_time:567824ms step_avg:181.53ms\nstep:3139/5000 train_loss:8.0470 train_time:568005ms step_avg:181.53ms\nstep:3140/5000 train_loss:7.9996 train_time:568185ms step_avg:181.53ms\nstep:3141/5000 train_loss:8.0981 train_time:568365ms step_avg:181.53ms\nstep:3142/5000 train_loss:7.9627 train_time:568545ms step_avg:181.53ms\nstep:3143/5000 train_loss:8.0707 train_time:568726ms step_avg:181.53ms\nstep:3144/5000 train_loss:8.2805 train_time:568908ms step_avg:181.53ms\nstep:3145/5000 train_loss:8.0734 train_time:569090ms step_avg:181.53ms\nstep:3146/5000 train_loss:8.0806 train_time:569271ms step_avg:181.53ms\nstep:3147/5000 train_loss:8.1352 train_time:569454ms step_avg:181.53ms\nstep:3148/5000 train_loss:8.1055 train_time:569635ms step_avg:181.53ms\nstep:3149/5000 train_loss:7.8708 train_time:569818ms step_avg:181.53ms\nstep:3150/5000 train_loss:8.0341 train_time:569999ms step_avg:181.53ms\nstep:3151/5000 train_loss:8.0939 train_time:570180ms step_avg:181.53ms\nstep:3152/5000 train_loss:8.0783 train_time:570362ms step_avg:181.53ms\nstep:3153/5000 train_loss:8.2104 train_time:570543ms step_avg:181.53ms\nstep:3154/5000 train_loss:8.2630 train_time:570723ms step_avg:181.53ms\nstep:3155/5000 train_loss:8.1540 train_time:570905ms step_avg:181.53ms\nstep:3156/5000 train_loss:8.2905 train_time:571086ms step_avg:181.53ms\nstep:3157/5000 train_loss:8.1183 train_time:571266ms step_avg:181.53ms\nstep:3158/5000 train_loss:8.0097 train_time:571447ms step_avg:181.53ms\nstep:3159/5000 train_loss:7.9535 train_time:571627ms step_avg:181.53ms\nstep:3160/5000 train_loss:8.1084 train_time:571810ms step_avg:181.53ms\nstep:3161/5000 train_loss:8.1204 train_time:571992ms step_avg:181.53ms\nstep:3162/5000 train_loss:8.1347 train_time:572176ms step_avg:181.53ms\nstep:3163/5000 train_loss:8.1022 train_time:572357ms step_avg:181.53ms\nstep:3164/5000 train_loss:8.1773 train_time:572538ms step_avg:181.53ms\nstep:3165/5000 train_loss:8.4951 train_time:572721ms step_avg:181.53ms\nstep:3166/5000 train_loss:8.1223 train_time:572903ms step_avg:181.53ms\nstep:3167/5000 train_loss:7.9524 train_time:573085ms step_avg:181.53ms\nstep:3168/5000 train_loss:8.1587 train_time:573266ms step_avg:181.53ms\nstep:3169/5000 train_loss:8.1683 train_time:573446ms step_avg:181.53ms\nstep:3170/5000 train_loss:8.1249 train_time:573626ms step_avg:181.53ms\nstep:3171/5000 train_loss:8.1439 train_time:573807ms step_avg:181.53ms\nstep:3172/5000 train_loss:8.1870 train_time:573989ms step_avg:181.53ms\nstep:3173/5000 train_loss:8.0496 train_time:574171ms step_avg:181.53ms\nstep:3174/5000 train_loss:8.1589 train_time:574353ms step_avg:181.53ms\nstep:3175/5000 train_loss:8.1447 train_time:574536ms step_avg:181.53ms\nstep:3176/5000 train_loss:8.0267 train_time:574718ms step_avg:181.53ms\nstep:3177/5000 train_loss:8.2077 train_time:574900ms step_avg:181.53ms\nstep:3178/5000 train_loss:8.0905 train_time:575081ms step_avg:181.53ms\nstep:3179/5000 train_loss:8.0132 train_time:575262ms step_avg:181.53ms\nstep:3180/5000 train_loss:8.0929 train_time:575443ms step_avg:181.53ms\nstep:3181/5000 train_loss:8.0127 train_time:575624ms step_avg:181.53ms\nstep:3182/5000 train_loss:8.0772 train_time:575805ms step_avg:181.53ms\nstep:3183/5000 train_loss:8.0604 train_time:575985ms step_avg:181.53ms\nstep:3184/5000 train_loss:8.0463 train_time:576165ms step_avg:181.53ms\nstep:3185/5000 train_loss:8.2652 train_time:576345ms step_avg:181.53ms\nstep:3186/5000 train_loss:8.0515 train_time:576525ms step_avg:181.53ms\nstep:3187/5000 train_loss:8.0665 train_time:576708ms step_avg:181.53ms\nstep:3188/5000 train_loss:8.1110 train_time:576889ms step_avg:181.53ms\nstep:3189/5000 train_loss:8.1504 train_time:577071ms step_avg:181.53ms\nstep:3190/5000 train_loss:8.1316 train_time:577253ms step_avg:181.53ms\nstep:3191/5000 train_loss:8.1798 train_time:577434ms step_avg:181.53ms\nstep:3192/5000 train_loss:7.9964 train_time:577617ms step_avg:181.53ms\nstep:3193/5000 train_loss:8.2160 train_time:577799ms step_avg:181.53ms\nstep:3194/5000 train_loss:8.1180 train_time:577980ms step_avg:181.53ms\nstep:3195/5000 train_loss:8.1037 train_time:578162ms step_avg:181.53ms\nstep:3196/5000 train_loss:7.9900 train_time:578344ms step_avg:181.53ms\nstep:3197/5000 train_loss:8.0873 train_time:578525ms step_avg:181.53ms\nstep:3198/5000 train_loss:8.0821 train_time:578707ms step_avg:181.53ms\nstep:3199/5000 train_loss:8.0890 train_time:578887ms step_avg:181.53ms\nstep:3200/5000 train_loss:8.8257 train_time:579068ms step_avg:181.53ms\nstep:3201/5000 train_loss:8.0460 train_time:579250ms step_avg:181.53ms\nstep:3202/5000 train_loss:8.0428 train_time:579432ms step_avg:181.53ms\nstep:3203/5000 train_loss:8.1683 train_time:579615ms step_avg:181.53ms\nstep:3204/5000 train_loss:8.0971 train_time:579797ms step_avg:181.53ms\nstep:3205/5000 train_loss:8.0952 train_time:579978ms step_avg:181.53ms\nstep:3206/5000 train_loss:8.2036 train_time:580160ms step_avg:181.53ms\nstep:3207/5000 train_loss:8.1181 train_time:580342ms step_avg:181.53ms\nstep:3208/5000 train_loss:8.1117 train_time:580523ms step_avg:181.53ms\nstep:3209/5000 train_loss:8.0977 train_time:580706ms step_avg:181.53ms\nstep:3210/5000 train_loss:8.3099 train_time:580887ms step_avg:181.53ms\nstep:3211/5000 train_loss:8.3003 train_time:581067ms step_avg:181.53ms\nstep:3212/5000 train_loss:8.1140 train_time:581249ms step_avg:181.53ms\nstep:3213/5000 train_loss:8.2160 train_time:581430ms step_avg:181.53ms\nstep:3214/5000 train_loss:8.0817 train_time:581613ms step_avg:181.53ms\nstep:3215/5000 train_loss:8.1461 train_time:581796ms step_avg:181.53ms\nstep:3216/5000 train_loss:8.1313 train_time:581979ms step_avg:181.53ms\nstep:3217/5000 train_loss:8.1296 train_time:582160ms step_avg:181.53ms\nstep:3218/5000 train_loss:8.1161 train_time:582342ms step_avg:181.53ms\nstep:3219/5000 train_loss:8.1359 train_time:582522ms step_avg:181.53ms\nstep:3220/5000 train_loss:8.0435 train_time:582703ms step_avg:181.53ms\nstep:3221/5000 train_loss:8.0943 train_time:582884ms step_avg:181.53ms\nstep:3222/5000 train_loss:8.1776 train_time:583066ms step_avg:181.53ms\nstep:3223/5000 train_loss:8.0469 train_time:583246ms step_avg:181.53ms\nstep:3224/5000 train_loss:7.9375 train_time:583426ms step_avg:181.53ms\nstep:3225/5000 train_loss:8.2309 train_time:583608ms step_avg:181.53ms\nstep:3226/5000 train_loss:8.0857 train_time:583788ms step_avg:181.53ms\nstep:3227/5000 train_loss:8.4129 train_time:583970ms step_avg:181.53ms\nstep:3228/5000 train_loss:8.0828 train_time:584151ms step_avg:181.53ms\nstep:3229/5000 train_loss:8.1417 train_time:584332ms step_avg:181.53ms\nstep:3230/5000 train_loss:8.1128 train_time:584516ms step_avg:181.53ms\nstep:3231/5000 train_loss:8.1632 train_time:584698ms step_avg:181.53ms\nstep:3232/5000 train_loss:8.1184 train_time:584885ms step_avg:181.53ms\nstep:3233/5000 train_loss:8.1858 train_time:585065ms step_avg:181.53ms\nstep:3234/5000 train_loss:8.1715 train_time:585245ms step_avg:181.53ms\nstep:3235/5000 train_loss:8.3052 train_time:585425ms step_avg:181.53ms\nstep:3236/5000 train_loss:8.1610 train_time:585606ms step_avg:181.53ms\nstep:3237/5000 train_loss:8.1401 train_time:585786ms step_avg:181.53ms\nstep:3238/5000 train_loss:8.1083 train_time:585967ms step_avg:181.53ms\nstep:3239/5000 train_loss:8.0751 train_time:586148ms step_avg:181.53ms\nstep:3240/5000 train_loss:8.1369 train_time:586330ms step_avg:181.53ms\nstep:3241/5000 train_loss:8.1179 train_time:586513ms step_avg:181.53ms\nstep:3242/5000 train_loss:8.1046 train_time:586695ms step_avg:181.53ms\nstep:3243/5000 train_loss:8.3844 train_time:586875ms step_avg:181.53ms\nstep:3244/5000 train_loss:8.1479 train_time:587057ms step_avg:181.53ms\nstep:3245/5000 train_loss:8.1047 train_time:587238ms step_avg:181.53ms\nstep:3246/5000 train_loss:8.1258 train_time:587420ms step_avg:181.53ms\nstep:3247/5000 train_loss:8.1109 train_time:587602ms step_avg:181.53ms\nstep:3248/5000 train_loss:8.1007 train_time:587784ms step_avg:181.53ms\nstep:3249/5000 train_loss:8.1397 train_time:587964ms step_avg:181.53ms\nstep:3250/5000 train_loss:8.1379 train_time:588144ms step_avg:181.53ms\nstep:3251/5000 train_loss:8.1708 train_time:588324ms step_avg:181.53ms\nstep:3252/5000 train_loss:8.0674 train_time:588505ms step_avg:181.53ms\nstep:3253/5000 train_loss:8.1624 train_time:588686ms step_avg:181.52ms\nstep:3254/5000 train_loss:8.1113 train_time:588867ms step_avg:181.52ms\nstep:3255/5000 train_loss:8.1206 train_time:589047ms step_avg:181.52ms\nstep:3256/5000 train_loss:8.1643 train_time:589227ms step_avg:181.52ms\nstep:3257/5000 train_loss:8.0529 train_time:589408ms step_avg:181.52ms\nstep:3258/5000 train_loss:8.1450 train_time:589589ms step_avg:181.52ms\nstep:3259/5000 train_loss:8.1850 train_time:589771ms step_avg:181.52ms\nstep:3260/5000 train_loss:8.0336 train_time:589953ms step_avg:181.52ms\nstep:3261/5000 train_loss:8.2134 train_time:590134ms step_avg:181.52ms\nstep:3262/5000 train_loss:8.1416 train_time:590317ms step_avg:181.52ms\nstep:3263/5000 train_loss:8.2228 train_time:590499ms step_avg:181.52ms\nstep:3264/5000 train_loss:8.2415 train_time:590680ms step_avg:181.52ms\nstep:3265/5000 train_loss:8.0734 train_time:590862ms step_avg:181.52ms\nstep:3266/5000 train_loss:8.1150 train_time:591043ms step_avg:181.52ms\nstep:3267/5000 train_loss:8.0947 train_time:591224ms step_avg:181.52ms\nstep:3268/5000 train_loss:7.9938 train_time:591405ms step_avg:181.52ms\nstep:3269/5000 train_loss:8.0449 train_time:591585ms step_avg:181.52ms\nstep:3270/5000 train_loss:8.0587 train_time:591766ms step_avg:181.52ms\nstep:3271/5000 train_loss:8.0483 train_time:591946ms step_avg:181.52ms\nstep:3272/5000 train_loss:8.0927 train_time:592127ms step_avg:181.52ms\nstep:3273/5000 train_loss:8.0440 train_time:592309ms step_avg:181.52ms\nstep:3274/5000 train_loss:8.1110 train_time:592490ms step_avg:181.52ms\nstep:3275/5000 train_loss:8.1408 train_time:592673ms step_avg:181.52ms\nstep:3276/5000 train_loss:8.0338 train_time:592855ms step_avg:181.52ms\nstep:3277/5000 train_loss:8.0898 train_time:593038ms step_avg:181.52ms\nstep:3278/5000 train_loss:8.1619 train_time:593220ms step_avg:181.52ms\nstep:3279/5000 train_loss:8.0216 train_time:593401ms step_avg:181.52ms\nstep:3280/5000 train_loss:8.1910 train_time:593581ms step_avg:181.52ms\nstep:3281/5000 train_loss:7.9363 train_time:593762ms step_avg:181.52ms\nstep:3282/5000 train_loss:8.2308 train_time:593944ms step_avg:181.52ms\nstep:3283/5000 train_loss:8.2854 train_time:594126ms step_avg:181.52ms\nstep:3284/5000 train_loss:7.9384 train_time:594308ms step_avg:181.52ms\nstep:3285/5000 train_loss:8.0698 train_time:594488ms step_avg:181.52ms\nstep:3286/5000 train_loss:8.1367 train_time:594670ms step_avg:181.52ms\nstep:3287/5000 train_loss:8.0466 train_time:594851ms step_avg:181.52ms\nstep:3288/5000 train_loss:8.1118 train_time:595032ms step_avg:181.52ms\nstep:3289/5000 train_loss:8.1269 train_time:595215ms step_avg:181.52ms\nstep:3290/5000 train_loss:8.0838 train_time:595397ms step_avg:181.52ms\nstep:3291/5000 train_loss:8.0275 train_time:595579ms step_avg:181.52ms\nstep:3292/5000 train_loss:8.1402 train_time:595761ms step_avg:181.52ms\nstep:3293/5000 train_loss:8.0107 train_time:595942ms step_avg:181.52ms\nstep:3294/5000 train_loss:8.0404 train_time:596123ms step_avg:181.52ms\nstep:3295/5000 train_loss:8.1044 train_time:596305ms step_avg:181.52ms\nstep:3296/5000 train_loss:7.9814 train_time:596486ms step_avg:181.52ms\nstep:3297/5000 train_loss:8.1442 train_time:596666ms step_avg:181.52ms\nstep:3298/5000 train_loss:8.0754 train_time:596847ms step_avg:181.52ms\nstep:3299/5000 train_loss:8.1261 train_time:597027ms step_avg:181.52ms\nstep:3300/5000 train_loss:8.0729 train_time:597209ms step_avg:181.52ms\nstep:3301/5000 train_loss:7.9815 train_time:597390ms step_avg:181.52ms\nstep:3302/5000 train_loss:8.0344 train_time:597572ms step_avg:181.52ms\nstep:3303/5000 train_loss:8.0438 train_time:597755ms step_avg:181.52ms\nstep:3304/5000 train_loss:8.1189 train_time:597936ms step_avg:181.52ms\nstep:3305/5000 train_loss:7.9740 train_time:598119ms step_avg:181.52ms\nstep:3306/5000 train_loss:8.0303 train_time:598300ms step_avg:181.52ms\nstep:3307/5000 train_loss:8.1660 train_time:598480ms step_avg:181.52ms\nstep:3308/5000 train_loss:8.1135 train_time:598661ms step_avg:181.52ms\nstep:3309/5000 train_loss:8.1000 train_time:598843ms step_avg:181.52ms\nstep:3310/5000 train_loss:8.1069 train_time:599025ms step_avg:181.52ms\nstep:3311/5000 train_loss:8.1486 train_time:599207ms step_avg:181.52ms\nstep:3312/5000 train_loss:8.1635 train_time:599388ms step_avg:181.52ms\nstep:3313/5000 train_loss:8.1621 train_time:599568ms step_avg:181.52ms\nstep:3314/5000 train_loss:8.2240 train_time:599749ms step_avg:181.52ms\nstep:3315/5000 train_loss:8.1457 train_time:599930ms step_avg:181.52ms\nstep:3316/5000 train_loss:8.1609 train_time:600112ms step_avg:181.52ms\nstep:3317/5000 train_loss:8.1395 train_time:600294ms step_avg:181.52ms\nstep:3318/5000 train_loss:8.0954 train_time:600476ms step_avg:181.52ms\nstep:3319/5000 train_loss:8.1908 train_time:600658ms step_avg:181.52ms\nstep:3320/5000 train_loss:8.1922 train_time:600840ms step_avg:181.52ms\nstep:3321/5000 train_loss:8.1399 train_time:601021ms step_avg:181.52ms\nstep:3322/5000 train_loss:8.2222 train_time:601203ms step_avg:181.52ms\nstep:3323/5000 train_loss:8.1603 train_time:601385ms step_avg:181.52ms\nstep:3324/5000 train_loss:8.0173 train_time:601568ms step_avg:181.52ms\nstep:3325/5000 train_loss:8.1044 train_time:601748ms step_avg:181.52ms\nstep:3326/5000 train_loss:8.1211 train_time:601929ms step_avg:181.52ms\nstep:3327/5000 train_loss:8.1851 train_time:602112ms step_avg:181.52ms\nstep:3328/5000 train_loss:8.1547 train_time:602295ms step_avg:181.52ms\nstep:3329/5000 train_loss:8.0993 train_time:602476ms step_avg:181.52ms\nstep:3330/5000 train_loss:8.0216 train_time:602659ms step_avg:181.52ms\nstep:3331/5000 train_loss:8.2415 train_time:602841ms step_avg:181.52ms\nstep:3332/5000 train_loss:8.0901 train_time:603023ms step_avg:181.52ms\nstep:3333/5000 train_loss:8.1501 train_time:603204ms step_avg:181.52ms\nstep:3334/5000 train_loss:8.1488 train_time:603385ms step_avg:181.52ms\nstep:3335/5000 train_loss:8.1836 train_time:603566ms step_avg:181.52ms\nstep:3336/5000 train_loss:8.1226 train_time:603749ms step_avg:181.52ms\nstep:3337/5000 train_loss:8.1709 train_time:603927ms step_avg:181.52ms\nstep:3338/5000 train_loss:8.1017 train_time:604107ms step_avg:181.52ms\nstep:3339/5000 train_loss:8.2222 train_time:604291ms step_avg:181.52ms\nstep:3340/5000 train_loss:8.1545 train_time:604472ms step_avg:181.52ms\nstep:3341/5000 train_loss:8.1417 train_time:604655ms step_avg:181.52ms\nstep:3342/5000 train_loss:8.2107 train_time:604837ms step_avg:181.52ms\nstep:3343/5000 train_loss:8.1413 train_time:605020ms step_avg:181.52ms\nstep:3344/5000 train_loss:8.1408 train_time:605202ms step_avg:181.52ms\nstep:3345/5000 train_loss:8.0552 train_time:605381ms step_avg:181.52ms\nstep:3346/5000 train_loss:8.1044 train_time:605563ms step_avg:181.52ms\nstep:3347/5000 train_loss:8.1389 train_time:605745ms step_avg:181.52ms\nstep:3348/5000 train_loss:7.9925 train_time:605926ms step_avg:181.52ms\nstep:3349/5000 train_loss:8.0645 train_time:606108ms step_avg:181.52ms\nstep:3350/5000 train_loss:8.3204 train_time:606289ms step_avg:181.52ms\nstep:3351/5000 train_loss:8.1040 train_time:606470ms step_avg:181.52ms\nstep:3352/5000 train_loss:8.0878 train_time:606651ms step_avg:181.52ms\nstep:3353/5000 train_loss:7.9022 train_time:606834ms step_avg:181.52ms\nstep:3354/5000 train_loss:8.0936 train_time:607017ms step_avg:181.52ms\nstep:3355/5000 train_loss:8.0704 train_time:607199ms step_avg:181.52ms\nstep:3356/5000 train_loss:8.0688 train_time:607381ms step_avg:181.52ms\nstep:3357/5000 train_loss:8.1268 train_time:607561ms step_avg:181.52ms\nstep:3358/5000 train_loss:8.1387 train_time:607743ms step_avg:181.52ms\nstep:3359/5000 train_loss:8.1476 train_time:607924ms step_avg:181.52ms\nstep:3360/5000 train_loss:8.0758 train_time:608106ms step_avg:181.52ms\nstep:3361/5000 train_loss:8.0634 train_time:608285ms step_avg:181.52ms\nstep:3362/5000 train_loss:8.1203 train_time:608466ms step_avg:181.52ms\nstep:3363/5000 train_loss:8.0919 train_time:608646ms step_avg:181.52ms\nstep:3364/5000 train_loss:8.1057 train_time:608828ms step_avg:181.52ms\nstep:3365/5000 train_loss:8.0842 train_time:609010ms step_avg:181.52ms\nstep:3366/5000 train_loss:8.0879 train_time:609195ms step_avg:181.52ms\nstep:3367/5000 train_loss:8.1526 train_time:609374ms step_avg:181.52ms\nstep:3368/5000 train_loss:8.1080 train_time:609557ms step_avg:181.52ms\nstep:3369/5000 train_loss:8.1043 train_time:609739ms step_avg:181.52ms\nstep:3370/5000 train_loss:8.1834 train_time:609921ms step_avg:181.52ms\nstep:3371/5000 train_loss:8.1875 train_time:610102ms step_avg:181.52ms\nstep:3372/5000 train_loss:7.9634 train_time:610283ms step_avg:181.52ms\nstep:3373/5000 train_loss:8.0483 train_time:610463ms step_avg:181.52ms\nstep:3374/5000 train_loss:8.0795 train_time:610645ms step_avg:181.52ms\nstep:3375/5000 train_loss:8.0817 train_time:610825ms step_avg:181.52ms\nstep:3376/5000 train_loss:8.0065 train_time:611007ms step_avg:181.52ms\nstep:3377/5000 train_loss:8.1054 train_time:611186ms step_avg:181.52ms\nstep:3378/5000 train_loss:8.1107 train_time:611368ms step_avg:181.52ms\nstep:3379/5000 train_loss:8.2513 train_time:611548ms step_avg:181.52ms\nstep:3380/5000 train_loss:8.1984 train_time:611730ms step_avg:181.52ms\nstep:3381/5000 train_loss:7.9634 train_time:611911ms step_avg:181.52ms\nstep:3382/5000 train_loss:7.9997 train_time:612095ms step_avg:181.52ms\nstep:3383/5000 train_loss:8.1219 train_time:612278ms step_avg:181.52ms\nstep:3384/5000 train_loss:8.0408 train_time:612459ms step_avg:181.52ms\nstep:3385/5000 train_loss:8.1798 train_time:612641ms step_avg:181.52ms\nstep:3386/5000 train_loss:8.1420 train_time:612824ms step_avg:181.52ms\nstep:3387/5000 train_loss:8.0685 train_time:613005ms step_avg:181.52ms\nstep:3388/5000 train_loss:8.1358 train_time:613185ms step_avg:181.52ms\nstep:3389/5000 train_loss:8.1353 train_time:613366ms step_avg:181.52ms\nstep:3390/5000 train_loss:8.1014 train_time:613546ms step_avg:181.52ms\nstep:3391/5000 train_loss:8.1461 train_time:613726ms step_avg:181.52ms\nstep:3392/5000 train_loss:8.1356 train_time:613909ms step_avg:181.52ms\nstep:3393/5000 train_loss:8.1378 train_time:614090ms step_avg:181.52ms\nstep:3394/5000 train_loss:8.0991 train_time:614272ms step_avg:181.52ms\nstep:3395/5000 train_loss:8.2230 train_time:614456ms step_avg:181.52ms\nstep:3396/5000 train_loss:8.2334 train_time:614638ms step_avg:181.52ms\nstep:3397/5000 train_loss:8.2491 train_time:614821ms step_avg:181.52ms\nstep:3398/5000 train_loss:8.1027 train_time:615003ms step_avg:181.52ms\nstep:3399/5000 train_loss:8.1600 train_time:615183ms step_avg:181.52ms\nstep:3400/5000 train_loss:8.2233 train_time:615364ms step_avg:181.52ms\nstep:3401/5000 train_loss:8.1918 train_time:615545ms step_avg:181.52ms\nstep:3402/5000 train_loss:8.0536 train_time:615727ms step_avg:181.52ms\nstep:3403/5000 train_loss:8.0790 train_time:615909ms step_avg:181.52ms\nstep:3404/5000 train_loss:8.1335 train_time:616090ms step_avg:181.52ms\nstep:3405/5000 train_loss:8.0608 train_time:616274ms step_avg:181.52ms\nstep:3406/5000 train_loss:8.1443 train_time:616455ms step_avg:181.52ms\nstep:3407/5000 train_loss:8.2382 train_time:616637ms step_avg:181.52ms\nstep:3408/5000 train_loss:8.0839 train_time:616819ms step_avg:181.52ms\nstep:3409/5000 train_loss:8.1759 train_time:617001ms step_avg:181.52ms\nstep:3410/5000 train_loss:8.3290 train_time:617182ms step_avg:181.52ms\nstep:3411/5000 train_loss:8.1174 train_time:617363ms step_avg:181.52ms\nstep:3412/5000 train_loss:7.9918 train_time:617545ms step_avg:181.52ms\nstep:3413/5000 train_loss:8.1563 train_time:617729ms step_avg:181.52ms\nstep:3414/5000 train_loss:8.0565 train_time:617910ms step_avg:181.52ms\nstep:3415/5000 train_loss:8.1626 train_time:618092ms step_avg:181.52ms\nstep:3416/5000 train_loss:8.1764 train_time:618274ms step_avg:181.53ms\nstep:3417/5000 train_loss:8.0778 train_time:618457ms step_avg:181.53ms\nstep:3418/5000 train_loss:8.1302 train_time:618638ms step_avg:181.53ms\nstep:3419/5000 train_loss:8.1346 train_time:618822ms step_avg:181.53ms\nstep:3420/5000 train_loss:7.9908 train_time:619004ms step_avg:181.53ms\nstep:3421/5000 train_loss:8.1295 train_time:619185ms step_avg:181.53ms\nstep:3422/5000 train_loss:8.2081 train_time:619365ms step_avg:181.53ms\nstep:3423/5000 train_loss:8.1790 train_time:619547ms step_avg:181.53ms\nstep:3424/5000 train_loss:7.9145 train_time:619725ms step_avg:181.52ms\nstep:3425/5000 train_loss:8.1427 train_time:619907ms step_avg:181.52ms\nstep:3426/5000 train_loss:8.1741 train_time:620088ms step_avg:181.52ms\nstep:3427/5000 train_loss:7.9972 train_time:620272ms step_avg:181.53ms\nstep:3428/5000 train_loss:8.0553 train_time:620454ms step_avg:181.53ms\nstep:3429/5000 train_loss:8.1782 train_time:620635ms step_avg:181.53ms\nstep:3430/5000 train_loss:8.1190 train_time:620818ms step_avg:181.53ms\nstep:3431/5000 train_loss:8.0586 train_time:620999ms step_avg:181.53ms\nstep:3432/5000 train_loss:8.1490 train_time:621181ms step_avg:181.53ms\nstep:3433/5000 train_loss:8.0733 train_time:621362ms step_avg:181.53ms\nstep:3434/5000 train_loss:8.0585 train_time:621543ms step_avg:181.53ms\nstep:3435/5000 train_loss:8.0713 train_time:621724ms step_avg:181.53ms\nstep:3436/5000 train_loss:8.1539 train_time:621906ms step_avg:181.53ms\nstep:3437/5000 train_loss:8.2375 train_time:622086ms step_avg:181.53ms\nstep:3438/5000 train_loss:8.0987 train_time:622267ms step_avg:181.52ms\nstep:3439/5000 train_loss:8.1158 train_time:622449ms step_avg:181.52ms\nstep:3440/5000 train_loss:8.0841 train_time:622630ms step_avg:181.52ms\nstep:3441/5000 train_loss:8.3445 train_time:622813ms step_avg:181.53ms\nstep:3442/5000 train_loss:8.1344 train_time:622995ms step_avg:181.53ms\nstep:3443/5000 train_loss:8.1024 train_time:623177ms step_avg:181.53ms\nstep:3444/5000 train_loss:8.1895 train_time:623359ms step_avg:181.53ms\nstep:3445/5000 train_loss:8.1443 train_time:623540ms step_avg:181.53ms\nstep:3446/5000 train_loss:8.1479 train_time:623723ms step_avg:181.53ms\nstep:3447/5000 train_loss:8.1778 train_time:623905ms step_avg:181.53ms\nstep:3448/5000 train_loss:8.0606 train_time:624086ms step_avg:181.53ms\nstep:3449/5000 train_loss:8.2207 train_time:624269ms step_avg:181.53ms\nstep:3450/5000 train_loss:8.2144 train_time:624451ms step_avg:181.53ms\nstep:3451/5000 train_loss:8.1479 train_time:624632ms step_avg:181.53ms\nstep:3452/5000 train_loss:8.0536 train_time:624815ms step_avg:181.53ms\nstep:3453/5000 train_loss:8.2304 train_time:624997ms step_avg:181.53ms\nstep:3454/5000 train_loss:8.0412 train_time:625179ms step_avg:181.53ms\nstep:3455/5000 train_loss:8.1980 train_time:625361ms step_avg:181.53ms\nstep:3456/5000 train_loss:8.2336 train_time:625542ms step_avg:181.53ms\nstep:3457/5000 train_loss:8.1146 train_time:625723ms step_avg:181.53ms\nstep:3458/5000 train_loss:8.0514 train_time:625906ms step_avg:181.53ms\nstep:3459/5000 train_loss:8.1321 train_time:626087ms step_avg:181.53ms\nstep:3460/5000 train_loss:8.2645 train_time:626268ms step_avg:181.53ms\nstep:3461/5000 train_loss:8.0236 train_time:626450ms step_avg:181.53ms\nstep:3462/5000 train_loss:8.2329 train_time:626631ms step_avg:181.53ms\nstep:3463/5000 train_loss:8.1779 train_time:626812ms step_avg:181.53ms\nstep:3464/5000 train_loss:8.1350 train_time:626994ms step_avg:181.53ms\nstep:3465/5000 train_loss:8.2073 train_time:627177ms step_avg:181.53ms\nstep:3466/5000 train_loss:8.1931 train_time:627359ms step_avg:181.53ms\nstep:3467/5000 train_loss:8.0649 train_time:627541ms step_avg:181.53ms\nstep:3468/5000 train_loss:8.1251 train_time:627722ms step_avg:181.53ms\nstep:3469/5000 train_loss:7.9994 train_time:627904ms step_avg:181.53ms\nstep:3470/5000 train_loss:8.0437 train_time:628085ms step_avg:181.53ms\nstep:3471/5000 train_loss:8.0289 train_time:628265ms step_avg:181.53ms\nstep:3472/5000 train_loss:8.2205 train_time:628447ms step_avg:181.53ms\nstep:3473/5000 train_loss:8.1116 train_time:628628ms step_avg:181.53ms\nstep:3474/5000 train_loss:8.0881 train_time:628809ms step_avg:181.53ms\nstep:3475/5000 train_loss:8.0954 train_time:628991ms step_avg:181.53ms\nstep:3476/5000 train_loss:8.1389 train_time:629173ms step_avg:181.53ms\nstep:3477/5000 train_loss:8.0427 train_time:629357ms step_avg:181.53ms\nstep:3478/5000 train_loss:8.0775 train_time:629538ms step_avg:181.53ms\nstep:3479/5000 train_loss:8.1229 train_time:629720ms step_avg:181.53ms\nstep:3480/5000 train_loss:7.9734 train_time:629901ms step_avg:181.53ms\nstep:3481/5000 train_loss:8.1710 train_time:630081ms step_avg:181.53ms\nstep:3482/5000 train_loss:8.1005 train_time:630261ms step_avg:181.53ms\nstep:3483/5000 train_loss:8.1647 train_time:630443ms step_avg:181.53ms\nstep:3484/5000 train_loss:8.2721 train_time:630624ms step_avg:181.53ms\nstep:3485/5000 train_loss:8.2397 train_time:630806ms step_avg:181.53ms\nstep:3486/5000 train_loss:8.0410 train_time:630986ms step_avg:181.53ms\nstep:3487/5000 train_loss:7.9017 train_time:631166ms step_avg:181.53ms\nstep:3488/5000 train_loss:8.1453 train_time:631347ms step_avg:181.53ms\nstep:3489/5000 train_loss:8.0208 train_time:631527ms step_avg:181.53ms\nstep:3490/5000 train_loss:8.0473 train_time:631709ms step_avg:181.53ms\nstep:3491/5000 train_loss:8.0784 train_time:631891ms step_avg:181.53ms\nstep:3492/5000 train_loss:8.1903 train_time:632074ms step_avg:181.53ms\nstep:3493/5000 train_loss:8.1101 train_time:632256ms step_avg:181.53ms\nstep:3494/5000 train_loss:8.1780 train_time:632439ms step_avg:181.53ms\nstep:3495/5000 train_loss:8.1065 train_time:632621ms step_avg:181.53ms\nstep:3496/5000 train_loss:8.1514 train_time:632803ms step_avg:181.53ms\nstep:3497/5000 train_loss:8.0669 train_time:632984ms step_avg:181.53ms\nstep:3498/5000 train_loss:8.0902 train_time:633164ms step_avg:181.53ms\nstep:3499/5000 train_loss:8.0546 train_time:633344ms step_avg:181.53ms\nstep:3500/5000 train_loss:8.0265 train_time:633525ms step_avg:181.53ms\nstep:3501/5000 train_loss:8.1081 train_time:633708ms step_avg:181.53ms\nstep:3502/5000 train_loss:7.6056 train_time:633890ms step_avg:181.53ms\nstep:3503/5000 train_loss:8.1942 train_time:634071ms step_avg:181.53ms\nstep:3504/5000 train_loss:8.1166 train_time:634253ms step_avg:181.53ms\nstep:3505/5000 train_loss:8.0764 train_time:634434ms step_avg:181.53ms\nstep:3506/5000 train_loss:8.0796 train_time:634617ms step_avg:181.53ms\nstep:3507/5000 train_loss:8.0546 train_time:634799ms step_avg:181.53ms\nstep:3508/5000 train_loss:8.1311 train_time:634980ms step_avg:181.53ms\nstep:3509/5000 train_loss:8.4996 train_time:635162ms step_avg:181.53ms\nstep:3510/5000 train_loss:8.1913 train_time:635345ms step_avg:181.53ms\nstep:3511/5000 train_loss:7.9693 train_time:635526ms step_avg:181.53ms\nstep:3512/5000 train_loss:8.0821 train_time:635707ms step_avg:181.53ms\nstep:3513/5000 train_loss:8.1334 train_time:635889ms step_avg:181.53ms\nstep:3514/5000 train_loss:8.1402 train_time:636070ms step_avg:181.53ms\nstep:3515/5000 train_loss:8.1094 train_time:636252ms step_avg:181.53ms\nstep:3516/5000 train_loss:8.1245 train_time:636434ms step_avg:181.53ms\nstep:3517/5000 train_loss:8.0604 train_time:636617ms step_avg:181.53ms\nstep:3518/5000 train_loss:8.0356 train_time:636799ms step_avg:181.53ms\nstep:3519/5000 train_loss:8.0713 train_time:636981ms step_avg:181.53ms\nstep:3520/5000 train_loss:8.3262 train_time:637161ms step_avg:181.53ms\nstep:3521/5000 train_loss:8.0656 train_time:637342ms step_avg:181.53ms\nstep:3522/5000 train_loss:8.2163 train_time:637523ms step_avg:181.53ms\nstep:3523/5000 train_loss:8.1727 train_time:637705ms step_avg:181.53ms\nstep:3524/5000 train_loss:8.0559 train_time:637886ms step_avg:181.53ms\nstep:3525/5000 train_loss:8.1165 train_time:638067ms step_avg:181.53ms\nstep:3526/5000 train_loss:8.0960 train_time:638248ms step_avg:181.53ms\nstep:3527/5000 train_loss:8.0931 train_time:638430ms step_avg:181.53ms\nstep:3528/5000 train_loss:8.1029 train_time:638611ms step_avg:181.53ms\nstep:3529/5000 train_loss:8.0229 train_time:638795ms step_avg:181.53ms\nstep:3530/5000 train_loss:8.1464 train_time:638977ms step_avg:181.53ms\nstep:3531/5000 train_loss:8.1197 train_time:639159ms step_avg:181.53ms\nstep:3532/5000 train_loss:8.0430 train_time:639341ms step_avg:181.53ms\nstep:3533/5000 train_loss:8.1622 train_time:639522ms step_avg:181.53ms\nstep:3534/5000 train_loss:8.2097 train_time:639704ms step_avg:181.53ms\nstep:3535/5000 train_loss:8.0278 train_time:639884ms step_avg:181.53ms\nstep:3536/5000 train_loss:8.0305 train_time:640065ms step_avg:181.53ms\nstep:3537/5000 train_loss:8.0056 train_time:640246ms step_avg:181.53ms\nstep:3538/5000 train_loss:8.2484 train_time:640427ms step_avg:181.53ms\nstep:3539/5000 train_loss:8.0742 train_time:640609ms step_avg:181.53ms\nstep:3540/5000 train_loss:8.0657 train_time:640791ms step_avg:181.53ms\nstep:3541/5000 train_loss:8.1913 train_time:640972ms step_avg:181.53ms\nstep:3542/5000 train_loss:8.2045 train_time:641155ms step_avg:181.53ms\nstep:3543/5000 train_loss:8.0508 train_time:641337ms step_avg:181.53ms\nstep:3544/5000 train_loss:8.0588 train_time:641521ms step_avg:181.53ms\nstep:3545/5000 train_loss:8.0349 train_time:641702ms step_avg:181.53ms\nstep:3546/5000 train_loss:8.1012 train_time:641883ms step_avg:181.53ms\nstep:3547/5000 train_loss:8.0567 train_time:642063ms step_avg:181.53ms\nstep:3548/5000 train_loss:8.0647 train_time:642244ms step_avg:181.53ms\nstep:3549/5000 train_loss:8.2371 train_time:642425ms step_avg:181.53ms\nstep:3550/5000 train_loss:8.1725 train_time:642606ms step_avg:181.53ms\nstep:3551/5000 train_loss:8.1695 train_time:642788ms step_avg:181.53ms\nstep:3552/5000 train_loss:8.1068 train_time:642970ms step_avg:181.53ms\nstep:3553/5000 train_loss:8.1389 train_time:643151ms step_avg:181.53ms\nstep:3554/5000 train_loss:8.2372 train_time:643332ms step_avg:181.53ms\nstep:3555/5000 train_loss:8.0659 train_time:643517ms step_avg:181.53ms\nstep:3556/5000 train_loss:8.2012 train_time:643699ms step_avg:181.53ms\nstep:3557/5000 train_loss:8.1180 train_time:643880ms step_avg:181.53ms\nstep:3558/5000 train_loss:8.0152 train_time:644061ms step_avg:181.53ms\nstep:3559/5000 train_loss:8.0524 train_time:644243ms step_avg:181.53ms\nstep:3560/5000 train_loss:8.1440 train_time:644424ms step_avg:181.53ms\nstep:3561/5000 train_loss:8.1006 train_time:644606ms step_avg:181.53ms\nstep:3562/5000 train_loss:8.0689 train_time:644787ms step_avg:181.53ms\nstep:3563/5000 train_loss:8.0116 train_time:644969ms step_avg:181.53ms\nstep:3564/5000 train_loss:8.0605 train_time:645150ms step_avg:181.53ms\nstep:3565/5000 train_loss:8.0539 train_time:645331ms step_avg:181.53ms\nstep:3566/5000 train_loss:8.0884 train_time:645515ms step_avg:181.53ms\nstep:3567/5000 train_loss:8.2373 train_time:645696ms step_avg:181.53ms\nstep:3568/5000 train_loss:8.1204 train_time:645878ms step_avg:181.53ms\nstep:3569/5000 train_loss:8.1601 train_time:646059ms step_avg:181.53ms\nstep:3570/5000 train_loss:8.0780 train_time:646241ms step_avg:181.53ms\nstep:3571/5000 train_loss:8.0107 train_time:646424ms step_avg:181.53ms\nstep:3572/5000 train_loss:8.0722 train_time:646606ms step_avg:181.53ms\nstep:3573/5000 train_loss:8.1886 train_time:646787ms step_avg:181.53ms\nstep:3574/5000 train_loss:8.1233 train_time:646968ms step_avg:181.53ms\nstep:3575/5000 train_loss:8.1172 train_time:647150ms step_avg:181.53ms\nstep:3576/5000 train_loss:8.0710 train_time:647331ms step_avg:181.53ms\nstep:3577/5000 train_loss:8.0276 train_time:647516ms step_avg:181.53ms\nstep:3578/5000 train_loss:8.1307 train_time:647698ms step_avg:181.53ms\nstep:3579/5000 train_loss:8.0842 train_time:647879ms step_avg:181.53ms\nstep:3580/5000 train_loss:8.0531 train_time:648061ms step_avg:181.53ms\nstep:3581/5000 train_loss:8.0936 train_time:648243ms step_avg:181.53ms\nstep:3582/5000 train_loss:8.2814 train_time:648424ms step_avg:181.53ms\nstep:3583/5000 train_loss:8.1805 train_time:648605ms step_avg:181.53ms\nstep:3584/5000 train_loss:7.9767 train_time:648785ms step_avg:181.53ms\nstep:3585/5000 train_loss:8.0916 train_time:648967ms step_avg:181.53ms\nstep:3586/5000 train_loss:8.1927 train_time:649148ms step_avg:181.53ms\nstep:3587/5000 train_loss:8.0468 train_time:649329ms step_avg:181.53ms\nstep:3588/5000 train_loss:8.0392 train_time:649512ms step_avg:181.53ms\nstep:3589/5000 train_loss:8.0838 train_time:649694ms step_avg:181.53ms\nstep:3590/5000 train_loss:8.1523 train_time:649876ms step_avg:181.53ms\nstep:3591/5000 train_loss:7.9771 train_time:650059ms step_avg:181.53ms\nstep:3592/5000 train_loss:8.2099 train_time:650241ms step_avg:181.53ms\nstep:3593/5000 train_loss:8.1330 train_time:650423ms step_avg:181.53ms\nstep:3594/5000 train_loss:8.0543 train_time:650606ms step_avg:181.53ms\nstep:3595/5000 train_loss:8.2155 train_time:650786ms step_avg:181.53ms\nstep:3596/5000 train_loss:8.0516 train_time:650968ms step_avg:181.53ms\nstep:3597/5000 train_loss:8.2168 train_time:651148ms step_avg:181.53ms\nstep:3598/5000 train_loss:7.9285 train_time:651329ms step_avg:181.53ms\nstep:3599/5000 train_loss:8.0056 train_time:651512ms step_avg:181.53ms\nstep:3600/5000 train_loss:8.0439 train_time:651695ms step_avg:181.53ms\nstep:3601/5000 train_loss:8.5850 train_time:651878ms step_avg:181.53ms\nstep:3602/5000 train_loss:8.0882 train_time:652061ms step_avg:181.53ms\nstep:3603/5000 train_loss:8.1379 train_time:652242ms step_avg:181.53ms\nstep:3604/5000 train_loss:8.1322 train_time:652422ms step_avg:181.53ms\nstep:3605/5000 train_loss:8.0043 train_time:652604ms step_avg:181.53ms\nstep:3606/5000 train_loss:8.1120 train_time:652785ms step_avg:181.53ms\nstep:3607/5000 train_loss:8.1205 train_time:652967ms step_avg:181.53ms\nstep:3608/5000 train_loss:8.0778 train_time:653149ms step_avg:181.53ms\nstep:3609/5000 train_loss:8.0414 train_time:653330ms step_avg:181.53ms\nstep:3610/5000 train_loss:8.0637 train_time:653512ms step_avg:181.53ms\nstep:3611/5000 train_loss:8.0434 train_time:653694ms step_avg:181.53ms\nstep:3612/5000 train_loss:8.0930 train_time:653876ms step_avg:181.53ms\nstep:3613/5000 train_loss:8.1282 train_time:654059ms step_avg:181.53ms\nstep:3614/5000 train_loss:8.1658 train_time:654240ms step_avg:181.53ms\nstep:3615/5000 train_loss:8.1873 train_time:654422ms step_avg:181.53ms\nstep:3616/5000 train_loss:8.2487 train_time:654606ms step_avg:181.53ms\nstep:3617/5000 train_loss:8.0895 train_time:654787ms step_avg:181.53ms\nstep:3618/5000 train_loss:8.0503 train_time:654968ms step_avg:181.53ms\nstep:3619/5000 train_loss:8.1218 train_time:655149ms step_avg:181.53ms\nstep:3620/5000 train_loss:8.0736 train_time:655330ms step_avg:181.53ms\nstep:3621/5000 train_loss:8.1044 train_time:655514ms step_avg:181.53ms\nstep:3622/5000 train_loss:8.1516 train_time:655696ms step_avg:181.53ms\nstep:3623/5000 train_loss:8.0771 train_time:655878ms step_avg:181.53ms\nstep:3624/5000 train_loss:8.2190 train_time:656060ms step_avg:181.53ms\nstep:3625/5000 train_loss:8.0931 train_time:656241ms step_avg:181.53ms\nstep:3626/5000 train_loss:8.0776 train_time:656423ms step_avg:181.53ms\nstep:3627/5000 train_loss:8.0344 train_time:656605ms step_avg:181.53ms\nstep:3628/5000 train_loss:8.0483 train_time:656786ms step_avg:181.53ms\nstep:3629/5000 train_loss:8.0791 train_time:656967ms step_avg:181.53ms\nstep:3630/5000 train_loss:8.3435 train_time:657148ms step_avg:181.53ms\nstep:3631/5000 train_loss:8.0933 train_time:657328ms step_avg:181.53ms\nstep:3632/5000 train_loss:8.1236 train_time:657511ms step_avg:181.53ms\nstep:3633/5000 train_loss:8.0895 train_time:657695ms step_avg:181.53ms\nstep:3634/5000 train_loss:8.1094 train_time:657877ms step_avg:181.53ms\nstep:3635/5000 train_loss:8.2375 train_time:658059ms step_avg:181.53ms\nstep:3636/5000 train_loss:8.0476 train_time:658242ms step_avg:181.53ms\nstep:3637/5000 train_loss:8.1526 train_time:658422ms step_avg:181.53ms\nstep:3638/5000 train_loss:8.0810 train_time:658605ms step_avg:181.53ms\nstep:3639/5000 train_loss:8.0669 train_time:658785ms step_avg:181.53ms\nstep:3640/5000 train_loss:8.0477 train_time:658966ms step_avg:181.53ms\nstep:3641/5000 train_loss:8.1223 train_time:659149ms step_avg:181.53ms\nstep:3642/5000 train_loss:8.0660 train_time:659329ms step_avg:181.53ms\nstep:3643/5000 train_loss:8.1352 train_time:659511ms step_avg:181.53ms\nstep:3644/5000 train_loss:8.0806 train_time:659693ms step_avg:181.53ms\nstep:3645/5000 train_loss:8.2096 train_time:659876ms step_avg:181.53ms\nstep:3646/5000 train_loss:8.1910 train_time:660059ms step_avg:181.53ms\nstep:3647/5000 train_loss:8.0363 train_time:660242ms step_avg:181.53ms\nstep:3648/5000 train_loss:8.1459 train_time:660423ms step_avg:181.53ms\nstep:3649/5000 train_loss:8.1094 train_time:660604ms step_avg:181.53ms\nstep:3650/5000 train_loss:8.0781 train_time:660785ms step_avg:181.53ms\nstep:3651/5000 train_loss:8.0955 train_time:660965ms step_avg:181.53ms\nstep:3652/5000 train_loss:8.1508 train_time:661145ms step_avg:181.53ms\nstep:3653/5000 train_loss:8.1158 train_time:661326ms step_avg:181.53ms\nstep:3654/5000 train_loss:8.4478 train_time:661508ms step_avg:181.53ms\nstep:3655/5000 train_loss:8.1006 train_time:661690ms step_avg:181.53ms\nstep:3656/5000 train_loss:8.2266 train_time:661872ms step_avg:181.53ms\nstep:3657/5000 train_loss:8.1399 train_time:662055ms step_avg:181.53ms\nstep:3658/5000 train_loss:8.1596 train_time:662237ms step_avg:181.53ms\nstep:3659/5000 train_loss:8.0873 train_time:662419ms step_avg:181.53ms\nstep:3660/5000 train_loss:8.1270 train_time:662601ms step_avg:181.53ms\nstep:3661/5000 train_loss:8.0283 train_time:662782ms step_avg:181.53ms\nstep:3662/5000 train_loss:8.1355 train_time:662963ms step_avg:181.53ms\nstep:3663/5000 train_loss:8.0374 train_time:663145ms step_avg:181.53ms\nstep:3664/5000 train_loss:8.3142 train_time:663327ms step_avg:181.53ms\nstep:3665/5000 train_loss:8.0888 train_time:663509ms step_avg:181.53ms\nstep:3666/5000 train_loss:8.1658 train_time:663691ms step_avg:181.53ms\nstep:3667/5000 train_loss:8.1064 train_time:663872ms step_avg:181.53ms\nstep:3668/5000 train_loss:8.0572 train_time:664054ms step_avg:181.53ms\nstep:3669/5000 train_loss:8.1079 train_time:664236ms step_avg:181.53ms\nstep:3670/5000 train_loss:8.0833 train_time:664418ms step_avg:181.54ms\nstep:3671/5000 train_loss:8.0490 train_time:664600ms step_avg:181.54ms\nstep:3672/5000 train_loss:8.0984 train_time:664781ms step_avg:181.53ms\nstep:3673/5000 train_loss:8.3284 train_time:664962ms step_avg:181.53ms\nstep:3674/5000 train_loss:8.2036 train_time:665144ms step_avg:181.53ms\nstep:3675/5000 train_loss:8.2480 train_time:665325ms step_avg:181.53ms\nstep:3676/5000 train_loss:8.0937 train_time:665508ms step_avg:181.54ms\nstep:3677/5000 train_loss:8.1530 train_time:665689ms step_avg:181.54ms\nstep:3678/5000 train_loss:8.2304 train_time:665870ms step_avg:181.54ms\nstep:3679/5000 train_loss:8.0990 train_time:666053ms step_avg:181.54ms\nstep:3680/5000 train_loss:8.1056 train_time:666236ms step_avg:181.54ms\nstep:3681/5000 train_loss:7.7610 train_time:666419ms step_avg:181.54ms\nstep:3682/5000 train_loss:8.2194 train_time:666600ms step_avg:181.54ms\nstep:3683/5000 train_loss:7.8926 train_time:666781ms step_avg:181.54ms\nstep:3684/5000 train_loss:8.1071 train_time:666961ms step_avg:181.54ms\nstep:3685/5000 train_loss:8.1057 train_time:667143ms step_avg:181.54ms\nstep:3686/5000 train_loss:8.0688 train_time:667325ms step_avg:181.54ms\nstep:3687/5000 train_loss:8.0589 train_time:667506ms step_avg:181.54ms\nstep:3688/5000 train_loss:8.0057 train_time:667688ms step_avg:181.54ms\nstep:3689/5000 train_loss:8.1236 train_time:667869ms step_avg:181.54ms\nstep:3690/5000 train_loss:8.0433 train_time:668051ms step_avg:181.54ms\nstep:3691/5000 train_loss:8.0826 train_time:668233ms step_avg:181.54ms\nstep:3692/5000 train_loss:7.9729 train_time:668416ms step_avg:181.54ms\nstep:3693/5000 train_loss:8.1438 train_time:668599ms step_avg:181.54ms\nstep:3694/5000 train_loss:8.1582 train_time:668780ms step_avg:181.54ms\nstep:3695/5000 train_loss:8.1719 train_time:668961ms step_avg:181.54ms\nstep:3696/5000 train_loss:8.1635 train_time:669143ms step_avg:181.54ms\nstep:3697/5000 train_loss:8.0735 train_time:669325ms step_avg:181.54ms\nstep:3698/5000 train_loss:8.1542 train_time:669507ms step_avg:181.54ms\nstep:3699/5000 train_loss:8.0302 train_time:669688ms step_avg:181.54ms\nstep:3700/5000 train_loss:7.8885 train_time:669869ms step_avg:181.54ms\nstep:3701/5000 train_loss:8.0810 train_time:670053ms step_avg:181.54ms\nstep:3702/5000 train_loss:8.0812 train_time:670233ms step_avg:181.54ms\nstep:3703/5000 train_loss:8.0384 train_time:670416ms step_avg:181.54ms\nstep:3704/5000 train_loss:7.9894 train_time:670601ms step_avg:181.54ms\nstep:3705/5000 train_loss:8.0739 train_time:670780ms step_avg:181.54ms\nstep:3706/5000 train_loss:8.1275 train_time:670961ms step_avg:181.54ms\nstep:3707/5000 train_loss:8.0797 train_time:671142ms step_avg:181.54ms\nstep:3708/5000 train_loss:8.0262 train_time:671323ms step_avg:181.54ms\nstep:3709/5000 train_loss:7.9527 train_time:671505ms step_avg:181.54ms\nstep:3710/5000 train_loss:8.0443 train_time:671686ms step_avg:181.54ms\nstep:3711/5000 train_loss:8.1780 train_time:671867ms step_avg:181.54ms\nstep:3712/5000 train_loss:8.1311 train_time:672048ms step_avg:181.54ms\nstep:3713/5000 train_loss:8.1928 train_time:672232ms step_avg:181.54ms\nstep:3714/5000 train_loss:8.1092 train_time:672415ms step_avg:181.54ms\nstep:3715/5000 train_loss:8.0926 train_time:672595ms step_avg:181.54ms\nstep:3716/5000 train_loss:8.0817 train_time:672778ms step_avg:181.54ms\nstep:3717/5000 train_loss:8.1262 train_time:672960ms step_avg:181.54ms\nstep:3718/5000 train_loss:8.0641 train_time:673142ms step_avg:181.54ms\nstep:3719/5000 train_loss:8.1164 train_time:673322ms step_avg:181.54ms\nstep:3720/5000 train_loss:7.9650 train_time:673504ms step_avg:181.54ms\nstep:3721/5000 train_loss:8.1505 train_time:673685ms step_avg:181.54ms\nstep:3722/5000 train_loss:8.1594 train_time:673868ms step_avg:181.54ms\nstep:3723/5000 train_loss:8.1753 train_time:674047ms step_avg:181.54ms\nstep:3724/5000 train_loss:8.1316 train_time:674227ms step_avg:181.54ms\nstep:3725/5000 train_loss:8.2069 train_time:674410ms step_avg:181.54ms\nstep:3726/5000 train_loss:7.9877 train_time:674590ms step_avg:181.54ms\nstep:3727/5000 train_loss:8.1011 train_time:674772ms step_avg:181.54ms\nstep:3728/5000 train_loss:8.3060 train_time:674955ms step_avg:181.54ms\nstep:3729/5000 train_loss:8.1412 train_time:675138ms step_avg:181.54ms\nstep:3730/5000 train_loss:8.0670 train_time:675320ms step_avg:181.54ms\nstep:3731/5000 train_loss:8.1186 train_time:675502ms step_avg:181.54ms\nstep:3732/5000 train_loss:8.1635 train_time:675683ms step_avg:181.54ms\nstep:3733/5000 train_loss:8.0165 train_time:675863ms step_avg:181.54ms\nstep:3734/5000 train_loss:8.1512 train_time:676044ms step_avg:181.54ms\nstep:3735/5000 train_loss:8.1321 train_time:676225ms step_avg:181.54ms\nstep:3736/5000 train_loss:8.0385 train_time:676407ms step_avg:181.54ms\nstep:3737/5000 train_loss:8.1256 train_time:676588ms step_avg:181.54ms\nstep:3738/5000 train_loss:8.1152 train_time:676769ms step_avg:181.54ms\nstep:3739/5000 train_loss:8.2045 train_time:676951ms step_avg:181.54ms\nstep:3740/5000 train_loss:8.1544 train_time:677133ms step_avg:181.54ms\nstep:3741/5000 train_loss:8.1379 train_time:677316ms step_avg:181.54ms\nstep:3742/5000 train_loss:8.0860 train_time:677499ms step_avg:181.54ms\nstep:3743/5000 train_loss:8.1833 train_time:677682ms step_avg:181.54ms\nstep:3744/5000 train_loss:8.1529 train_time:677863ms step_avg:181.54ms\nstep:3745/5000 train_loss:8.2448 train_time:678044ms step_avg:181.54ms\nstep:3746/5000 train_loss:8.1725 train_time:678224ms step_avg:181.54ms\nstep:3747/5000 train_loss:8.0211 train_time:678405ms step_avg:181.54ms\nstep:3748/5000 train_loss:8.2273 train_time:678585ms step_avg:181.54ms\nstep:3749/5000 train_loss:8.1764 train_time:678766ms step_avg:181.54ms\nstep:3750/5000 train_loss:8.0517 train_time:678947ms step_avg:181.54ms\nstep:3751/5000 train_loss:8.1992 train_time:679128ms step_avg:181.54ms\nstep:3752/5000 train_loss:8.1180 train_time:679311ms step_avg:181.54ms\nstep:3753/5000 train_loss:8.0235 train_time:679493ms step_avg:181.54ms\nstep:3754/5000 train_loss:8.1263 train_time:679675ms step_avg:181.54ms\nstep:3755/5000 train_loss:8.1497 train_time:679858ms step_avg:181.54ms\nstep:3756/5000 train_loss:8.1555 train_time:680041ms step_avg:181.54ms\nstep:3757/5000 train_loss:8.0053 train_time:680223ms step_avg:181.54ms\nstep:3758/5000 train_loss:8.0759 train_time:680406ms step_avg:181.54ms\nstep:3759/5000 train_loss:8.1407 train_time:680587ms step_avg:181.54ms\nstep:3760/5000 train_loss:8.0843 train_time:680769ms step_avg:181.54ms\nstep:3761/5000 train_loss:8.0521 train_time:680954ms step_avg:181.54ms\nstep:3762/5000 train_loss:8.1827 train_time:681134ms step_avg:181.54ms\nstep:3763/5000 train_loss:8.0147 train_time:681318ms step_avg:181.54ms\nstep:3764/5000 train_loss:8.1351 train_time:681500ms step_avg:181.54ms\nstep:3765/5000 train_loss:8.0829 train_time:681680ms step_avg:181.54ms\nstep:3766/5000 train_loss:8.1191 train_time:681862ms step_avg:181.54ms\nstep:3767/5000 train_loss:8.0586 train_time:682044ms step_avg:181.54ms\nstep:3768/5000 train_loss:7.9451 train_time:682225ms step_avg:181.54ms\nstep:3769/5000 train_loss:8.1050 train_time:682408ms step_avg:181.54ms\nstep:3770/5000 train_loss:8.1028 train_time:682589ms step_avg:181.54ms\nstep:3771/5000 train_loss:8.2561 train_time:682771ms step_avg:181.54ms\nstep:3772/5000 train_loss:8.0742 train_time:682953ms step_avg:181.54ms\nstep:3773/5000 train_loss:8.0163 train_time:683135ms step_avg:181.54ms\nstep:3774/5000 train_loss:8.1114 train_time:683319ms step_avg:181.54ms\nstep:3775/5000 train_loss:8.0203 train_time:683500ms step_avg:181.54ms\nstep:3776/5000 train_loss:8.0423 train_time:683680ms step_avg:181.54ms\nstep:3777/5000 train_loss:8.1620 train_time:683862ms step_avg:181.54ms\nstep:3778/5000 train_loss:8.1337 train_time:684043ms step_avg:181.54ms\nstep:3779/5000 train_loss:8.1198 train_time:684225ms step_avg:181.54ms\nstep:3780/5000 train_loss:8.1424 train_time:684407ms step_avg:181.54ms\nstep:3781/5000 train_loss:8.0150 train_time:684588ms step_avg:181.54ms\nstep:3782/5000 train_loss:8.0246 train_time:684769ms step_avg:181.54ms\nstep:3783/5000 train_loss:8.1082 train_time:684950ms step_avg:181.54ms\nstep:3784/5000 train_loss:8.4288 train_time:685132ms step_avg:181.54ms\nstep:3785/5000 train_loss:8.1241 train_time:685316ms step_avg:181.54ms\nstep:3786/5000 train_loss:8.1663 train_time:685498ms step_avg:181.54ms\nstep:3787/5000 train_loss:8.1385 train_time:685680ms step_avg:181.54ms\nstep:3788/5000 train_loss:8.1875 train_time:685860ms step_avg:181.54ms\nstep:3789/5000 train_loss:7.9814 train_time:686041ms step_avg:181.54ms\nstep:3790/5000 train_loss:8.5290 train_time:686224ms step_avg:181.54ms\nstep:3791/5000 train_loss:8.0728 train_time:686406ms step_avg:181.54ms\nstep:3792/5000 train_loss:8.1649 train_time:686588ms step_avg:181.54ms\nstep:3793/5000 train_loss:8.1058 train_time:686770ms step_avg:181.54ms\nstep:3794/5000 train_loss:8.0689 train_time:686952ms step_avg:181.54ms\nstep:3795/5000 train_loss:8.3652 train_time:687133ms step_avg:181.54ms\nstep:3796/5000 train_loss:8.1559 train_time:687317ms step_avg:181.54ms\nstep:3797/5000 train_loss:8.1822 train_time:687500ms step_avg:181.54ms\nstep:3798/5000 train_loss:8.0998 train_time:687681ms step_avg:181.54ms\nstep:3799/5000 train_loss:8.1056 train_time:687862ms step_avg:181.54ms\nstep:3800/5000 train_loss:7.9636 train_time:688043ms step_avg:181.54ms\nstep:3801/5000 train_loss:8.1924 train_time:688224ms step_avg:181.54ms\nstep:3802/5000 train_loss:8.1761 train_time:688405ms step_avg:181.54ms\nstep:3803/5000 train_loss:8.0660 train_time:688586ms step_avg:181.54ms\nstep:3804/5000 train_loss:8.0745 train_time:688767ms step_avg:181.54ms\nstep:3805/5000 train_loss:8.0472 train_time:688948ms step_avg:181.54ms\nstep:3806/5000 train_loss:8.0956 train_time:689130ms step_avg:181.54ms\nstep:3807/5000 train_loss:7.9820 train_time:689313ms step_avg:181.54ms\nstep:3808/5000 train_loss:8.0991 train_time:689496ms step_avg:181.54ms\nstep:3809/5000 train_loss:8.3037 train_time:689678ms step_avg:181.54ms\nstep:3810/5000 train_loss:8.1475 train_time:690096ms step_avg:181.60ms\nstep:3811/5000 train_loss:8.2044 train_time:690277ms step_avg:181.60ms\nstep:3812/5000 train_loss:8.1138 train_time:690457ms step_avg:181.60ms\nstep:3813/5000 train_loss:8.0859 train_time:690637ms step_avg:181.60ms\nstep:3814/5000 train_loss:8.1104 train_time:690818ms step_avg:181.60ms\nstep:3815/5000 train_loss:8.1026 train_time:691005ms step_avg:181.60ms\nstep:3816/5000 train_loss:7.9955 train_time:691187ms step_avg:181.60ms\nstep:3817/5000 train_loss:8.0684 train_time:691368ms step_avg:181.60ms\nstep:3818/5000 train_loss:7.9979 train_time:691550ms step_avg:181.60ms\nstep:3819/5000 train_loss:8.0892 train_time:691729ms step_avg:181.60ms\nstep:3820/5000 train_loss:8.0041 train_time:691911ms step_avg:181.60ms\nstep:3821/5000 train_loss:8.0619 train_time:692095ms step_avg:181.60ms\nstep:3822/5000 train_loss:8.1773 train_time:692278ms step_avg:181.60ms\nstep:3823/5000 train_loss:8.0637 train_time:692460ms step_avg:181.61ms\nstep:3824/5000 train_loss:7.9709 train_time:692642ms step_avg:181.61ms\nstep:3825/5000 train_loss:7.9687 train_time:692822ms step_avg:181.60ms\nstep:3826/5000 train_loss:8.1453 train_time:693005ms step_avg:181.61ms\nstep:3827/5000 train_loss:8.0047 train_time:693187ms step_avg:181.61ms\nstep:3828/5000 train_loss:8.1085 train_time:693369ms step_avg:181.61ms\nstep:3829/5000 train_loss:8.1672 train_time:693551ms step_avg:181.61ms\nstep:3830/5000 train_loss:8.1315 train_time:693731ms step_avg:181.61ms\nstep:3831/5000 train_loss:8.2918 train_time:693914ms step_avg:181.61ms\nstep:3832/5000 train_loss:7.9993 train_time:694097ms step_avg:181.61ms\nstep:3833/5000 train_loss:8.2372 train_time:694279ms step_avg:181.61ms\nstep:3834/5000 train_loss:8.1094 train_time:694461ms step_avg:181.61ms\nstep:3835/5000 train_loss:8.0032 train_time:694642ms step_avg:181.61ms\nstep:3836/5000 train_loss:8.1434 train_time:694824ms step_avg:181.61ms\nstep:3837/5000 train_loss:8.1634 train_time:695006ms step_avg:181.61ms\nstep:3838/5000 train_loss:8.2866 train_time:695187ms step_avg:181.61ms\nstep:3839/5000 train_loss:8.0614 train_time:695368ms step_avg:181.61ms\nstep:3840/5000 train_loss:8.0774 train_time:695550ms step_avg:181.61ms\nstep:3841/5000 train_loss:8.1693 train_time:695731ms step_avg:181.61ms\nstep:3842/5000 train_loss:8.1204 train_time:695914ms step_avg:181.61ms\nstep:3843/5000 train_loss:8.1989 train_time:696096ms step_avg:181.61ms\nstep:3844/5000 train_loss:7.9720 train_time:696278ms step_avg:181.61ms\nstep:3845/5000 train_loss:8.3093 train_time:696460ms step_avg:181.61ms\nstep:3846/5000 train_loss:8.1411 train_time:696640ms step_avg:181.61ms\nstep:3847/5000 train_loss:8.1260 train_time:696823ms step_avg:181.61ms\nstep:3848/5000 train_loss:7.9223 train_time:697005ms step_avg:181.61ms\nstep:3849/5000 train_loss:8.1835 train_time:697186ms step_avg:181.61ms\nstep:3850/5000 train_loss:8.1397 train_time:697365ms step_avg:181.61ms\nstep:3851/5000 train_loss:8.0107 train_time:697546ms step_avg:181.61ms\nstep:3852/5000 train_loss:8.2223 train_time:697729ms step_avg:181.61ms\nstep:3853/5000 train_loss:8.1712 train_time:697910ms step_avg:181.61ms\nstep:3854/5000 train_loss:8.0606 train_time:698089ms step_avg:181.60ms\nstep:3855/5000 train_loss:8.1807 train_time:698272ms step_avg:181.61ms\nstep:3856/5000 train_loss:8.1244 train_time:698454ms step_avg:181.61ms\nstep:3857/5000 train_loss:8.1477 train_time:698635ms step_avg:181.61ms\nstep:3858/5000 train_loss:8.0451 train_time:698817ms step_avg:181.61ms\nstep:3859/5000 train_loss:8.0448 train_time:698999ms step_avg:181.61ms\nstep:3860/5000 train_loss:8.0773 train_time:699180ms step_avg:181.61ms\nstep:3861/5000 train_loss:8.1376 train_time:699360ms step_avg:181.60ms\nstep:3862/5000 train_loss:8.0917 train_time:699542ms step_avg:181.60ms\nstep:3863/5000 train_loss:8.1190 train_time:699723ms step_avg:181.60ms\nstep:3864/5000 train_loss:8.0671 train_time:699905ms step_avg:181.60ms\nstep:3865/5000 train_loss:8.0008 train_time:700087ms step_avg:181.60ms\nstep:3866/5000 train_loss:8.1323 train_time:700268ms step_avg:181.60ms\nstep:3867/5000 train_loss:8.1104 train_time:700449ms step_avg:181.60ms\nstep:3868/5000 train_loss:8.0658 train_time:700630ms step_avg:181.60ms\nstep:3869/5000 train_loss:8.0558 train_time:700811ms step_avg:181.60ms\nstep:3870/5000 train_loss:8.0453 train_time:700993ms step_avg:181.60ms\nstep:3871/5000 train_loss:8.0934 train_time:701175ms step_avg:181.60ms\nstep:3872/5000 train_loss:8.0920 train_time:701358ms step_avg:181.60ms\nstep:3873/5000 train_loss:8.1203 train_time:701540ms step_avg:181.61ms\nstep:3874/5000 train_loss:8.2280 train_time:701723ms step_avg:181.61ms\nstep:3875/5000 train_loss:7.9615 train_time:701904ms step_avg:181.61ms\nstep:3876/5000 train_loss:8.1209 train_time:702085ms step_avg:181.60ms\nstep:3877/5000 train_loss:8.1212 train_time:702266ms step_avg:181.60ms\nstep:3878/5000 train_loss:8.1563 train_time:702447ms step_avg:181.60ms\nstep:3879/5000 train_loss:8.0705 train_time:702628ms step_avg:181.60ms\nstep:3880/5000 train_loss:8.1758 train_time:702810ms step_avg:181.60ms\nstep:3881/5000 train_loss:8.0826 train_time:702992ms step_avg:181.60ms\nstep:3882/5000 train_loss:8.1925 train_time:703175ms step_avg:181.60ms\nstep:3883/5000 train_loss:8.0703 train_time:703357ms step_avg:181.61ms\nstep:3884/5000 train_loss:8.0858 train_time:703540ms step_avg:181.61ms\nstep:3885/5000 train_loss:8.0939 train_time:703723ms step_avg:181.61ms\nstep:3886/5000 train_loss:8.0541 train_time:703905ms step_avg:181.61ms\nstep:3887/5000 train_loss:8.0833 train_time:704086ms step_avg:181.61ms\nstep:3888/5000 train_loss:8.1436 train_time:704267ms step_avg:181.61ms\nstep:3889/5000 train_loss:8.2175 train_time:704447ms step_avg:181.61ms\nstep:3890/5000 train_loss:8.1404 train_time:704627ms step_avg:181.60ms\nstep:3891/5000 train_loss:8.0802 train_time:704809ms step_avg:181.61ms\nstep:3892/5000 train_loss:8.1215 train_time:704992ms step_avg:181.61ms\nstep:3893/5000 train_loss:8.1098 train_time:705175ms step_avg:181.61ms\nstep:3894/5000 train_loss:8.1713 train_time:705358ms step_avg:181.61ms\nstep:3895/5000 train_loss:7.9842 train_time:705540ms step_avg:181.61ms\nstep:3896/5000 train_loss:8.1739 train_time:705721ms step_avg:181.61ms\nstep:3897/5000 train_loss:8.1934 train_time:705902ms step_avg:181.61ms\nstep:3898/5000 train_loss:8.1568 train_time:706084ms step_avg:181.61ms\nstep:3899/5000 train_loss:8.0066 train_time:706265ms step_avg:181.61ms\nstep:3900/5000 train_loss:8.1812 train_time:706446ms step_avg:181.61ms\nstep:3901/5000 train_loss:8.1319 train_time:706627ms step_avg:181.61ms\nstep:3902/5000 train_loss:8.1068 train_time:706808ms step_avg:181.61ms\nstep:3903/5000 train_loss:8.1200 train_time:706990ms step_avg:181.61ms\nstep:3904/5000 train_loss:8.1398 train_time:707172ms step_avg:181.61ms\nstep:3905/5000 train_loss:8.0197 train_time:707356ms step_avg:181.61ms\nstep:3906/5000 train_loss:8.1007 train_time:707538ms step_avg:181.61ms\nstep:3907/5000 train_loss:8.0082 train_time:707720ms step_avg:181.61ms\nstep:3908/5000 train_loss:8.0665 train_time:707902ms step_avg:181.61ms\nstep:3909/5000 train_loss:8.0705 train_time:708083ms step_avg:181.61ms\nstep:3910/5000 train_loss:8.2278 train_time:708263ms step_avg:181.61ms\nstep:3911/5000 train_loss:8.2371 train_time:708444ms step_avg:181.61ms\nstep:3912/5000 train_loss:8.1871 train_time:708625ms step_avg:181.61ms\nstep:3913/5000 train_loss:8.1327 train_time:708807ms step_avg:181.61ms\nstep:3914/5000 train_loss:8.0144 train_time:708988ms step_avg:181.61ms\nstep:3915/5000 train_loss:8.1279 train_time:709170ms step_avg:181.61ms\nstep:3916/5000 train_loss:8.1907 train_time:709352ms step_avg:181.61ms\nstep:3917/5000 train_loss:8.0212 train_time:709533ms step_avg:181.61ms\nstep:3918/5000 train_loss:8.1646 train_time:709720ms step_avg:181.61ms\nstep:3919/5000 train_loss:8.0873 train_time:709900ms step_avg:181.61ms\nstep:3920/5000 train_loss:8.1796 train_time:710081ms step_avg:181.61ms\nstep:3921/5000 train_loss:8.1551 train_time:710262ms step_avg:181.61ms\nstep:3922/5000 train_loss:8.0298 train_time:710444ms step_avg:181.61ms\nstep:3923/5000 train_loss:8.0314 train_time:710625ms step_avg:181.61ms\nstep:3924/5000 train_loss:8.3179 train_time:710807ms step_avg:181.61ms\nstep:3925/5000 train_loss:8.0729 train_time:710987ms step_avg:181.61ms\nstep:3926/5000 train_loss:7.9347 train_time:711169ms step_avg:181.61ms\nstep:3927/5000 train_loss:8.0176 train_time:711351ms step_avg:181.61ms\nstep:3928/5000 train_loss:8.1179 train_time:711534ms step_avg:181.61ms\nstep:3929/5000 train_loss:8.1686 train_time:711718ms step_avg:181.61ms\nstep:3930/5000 train_loss:8.0169 train_time:711899ms step_avg:181.61ms\nstep:3931/5000 train_loss:8.1339 train_time:712080ms step_avg:181.61ms\nstep:3932/5000 train_loss:8.1108 train_time:712262ms step_avg:181.61ms\nstep:3933/5000 train_loss:8.1884 train_time:712445ms step_avg:181.61ms\nstep:3934/5000 train_loss:8.1651 train_time:712626ms step_avg:181.61ms\nstep:3935/5000 train_loss:8.0853 train_time:712808ms step_avg:181.61ms\nstep:3936/5000 train_loss:8.1789 train_time:712989ms step_avg:181.61ms\nstep:3937/5000 train_loss:8.1327 train_time:713171ms step_avg:181.61ms\nstep:3938/5000 train_loss:8.1630 train_time:713353ms step_avg:181.61ms\nstep:3939/5000 train_loss:8.0435 train_time:713534ms step_avg:181.61ms\nstep:3940/5000 train_loss:8.3468 train_time:713717ms step_avg:181.61ms\nstep:3941/5000 train_loss:8.1035 train_time:713900ms step_avg:181.61ms\nstep:3942/5000 train_loss:8.1702 train_time:714081ms step_avg:181.61ms\nstep:3943/5000 train_loss:8.1143 train_time:714266ms step_avg:181.61ms\nstep:3944/5000 train_loss:8.0614 train_time:714445ms step_avg:181.61ms\nstep:3945/5000 train_loss:8.1005 train_time:714625ms step_avg:181.61ms\nstep:3946/5000 train_loss:8.1017 train_time:714807ms step_avg:181.61ms\nstep:3947/5000 train_loss:7.9691 train_time:714989ms step_avg:181.61ms\nstep:3948/5000 train_loss:8.1047 train_time:715171ms step_avg:181.61ms\nstep:3949/5000 train_loss:8.1831 train_time:715354ms step_avg:181.61ms\nstep:3950/5000 train_loss:8.0587 train_time:715535ms step_avg:181.61ms\nstep:3951/5000 train_loss:8.4498 train_time:715718ms step_avg:181.61ms\nstep:3952/5000 train_loss:8.1084 train_time:715900ms step_avg:181.61ms\nstep:3953/5000 train_loss:8.1332 train_time:716082ms step_avg:181.61ms\nstep:3954/5000 train_loss:8.2442 train_time:716263ms step_avg:181.61ms\nstep:3955/5000 train_loss:8.0925 train_time:716445ms step_avg:181.61ms\nstep:3956/5000 train_loss:8.1081 train_time:716626ms step_avg:181.61ms\nstep:3957/5000 train_loss:8.1579 train_time:716808ms step_avg:181.61ms\nstep:3958/5000 train_loss:7.9804 train_time:716989ms step_avg:181.61ms\nstep:3959/5000 train_loss:8.0980 train_time:717170ms step_avg:181.61ms\nstep:3960/5000 train_loss:8.1803 train_time:717353ms step_avg:181.61ms\nstep:3961/5000 train_loss:8.1584 train_time:717535ms step_avg:181.61ms\nstep:3962/5000 train_loss:7.9942 train_time:717719ms step_avg:181.61ms\nstep:3963/5000 train_loss:7.9785 train_time:717902ms step_avg:181.61ms\nstep:3964/5000 train_loss:8.1383 train_time:718082ms step_avg:181.61ms\nstep:3965/5000 train_loss:8.1882 train_time:718262ms step_avg:181.61ms\nstep:3966/5000 train_loss:8.0506 train_time:718444ms step_avg:181.61ms\nstep:3967/5000 train_loss:8.1300 train_time:718625ms step_avg:181.61ms\nstep:3968/5000 train_loss:7.9626 train_time:718805ms step_avg:181.61ms\nstep:3969/5000 train_loss:8.0518 train_time:718986ms step_avg:181.61ms\nstep:3970/5000 train_loss:8.1392 train_time:719167ms step_avg:181.61ms\nstep:3971/5000 train_loss:8.1300 train_time:719349ms step_avg:181.61ms\nstep:3972/5000 train_loss:8.0890 train_time:719530ms step_avg:181.61ms\nstep:3973/5000 train_loss:7.9900 train_time:719713ms step_avg:181.61ms\nstep:3974/5000 train_loss:8.1395 train_time:719896ms step_avg:181.61ms\nstep:3975/5000 train_loss:8.0879 train_time:720078ms step_avg:181.61ms\nstep:3976/5000 train_loss:7.9940 train_time:720261ms step_avg:181.61ms\nstep:3977/5000 train_loss:8.1118 train_time:720441ms step_avg:181.61ms\nstep:3978/5000 train_loss:7.8180 train_time:720622ms step_avg:181.61ms\nstep:3979/5000 train_loss:7.9982 train_time:720804ms step_avg:181.61ms\nstep:3980/5000 train_loss:8.1960 train_time:720985ms step_avg:181.61ms\nstep:3981/5000 train_loss:8.0573 train_time:721165ms step_avg:181.61ms\nstep:3982/5000 train_loss:8.1330 train_time:721347ms step_avg:181.61ms\nstep:3983/5000 train_loss:8.1672 train_time:721528ms step_avg:181.61ms\nstep:3984/5000 train_loss:8.0471 train_time:721710ms step_avg:181.61ms\nstep:3985/5000 train_loss:8.1828 train_time:721895ms step_avg:181.61ms\nstep:3986/5000 train_loss:8.1415 train_time:722076ms step_avg:181.61ms\nstep:3987/5000 train_loss:8.0880 train_time:722258ms step_avg:181.61ms\nstep:3988/5000 train_loss:8.1269 train_time:722439ms step_avg:181.61ms\nstep:3989/5000 train_loss:7.9726 train_time:722621ms step_avg:181.61ms\nstep:3990/5000 train_loss:8.0487 train_time:722803ms step_avg:181.61ms\nstep:3991/5000 train_loss:8.1701 train_time:722984ms step_avg:181.61ms\nstep:3992/5000 train_loss:8.0417 train_time:723165ms step_avg:181.61ms\nstep:3993/5000 train_loss:8.1209 train_time:723345ms step_avg:181.61ms\nstep:3994/5000 train_loss:8.1729 train_time:723527ms step_avg:181.61ms\nstep:3995/5000 train_loss:8.0669 train_time:723709ms step_avg:181.61ms\nstep:3996/5000 train_loss:8.3513 train_time:723891ms step_avg:181.61ms\nstep:3997/5000 train_loss:8.0510 train_time:724074ms step_avg:181.61ms\nstep:3998/5000 train_loss:8.0750 train_time:724255ms step_avg:181.61ms\nstep:3999/5000 train_loss:8.1220 train_time:724437ms step_avg:181.61ms\nstep:4000/5000 train_loss:8.0519 train_time:724619ms step_avg:181.61ms\nstep:4001/5000 train_loss:8.1672 train_time:724801ms step_avg:181.61ms\nstep:4002/5000 train_loss:8.1609 train_time:724982ms step_avg:181.61ms\nstep:4003/5000 train_loss:8.0499 train_time:725163ms step_avg:181.61ms\nstep:4004/5000 train_loss:8.1355 train_time:725345ms step_avg:181.61ms\nstep:4005/5000 train_loss:8.1419 train_time:725526ms step_avg:181.61ms\nstep:4006/5000 train_loss:7.9908 train_time:725709ms step_avg:181.61ms\nstep:4007/5000 train_loss:8.2370 train_time:725890ms step_avg:181.61ms\nstep:4008/5000 train_loss:8.0873 train_time:726071ms step_avg:181.61ms\nstep:4009/5000 train_loss:8.2301 train_time:726254ms step_avg:181.61ms\nstep:4010/5000 train_loss:8.1572 train_time:726435ms step_avg:181.61ms\nstep:4011/5000 train_loss:8.1589 train_time:726618ms step_avg:181.61ms\nstep:4012/5000 train_loss:8.1568 train_time:726801ms step_avg:181.61ms\nstep:4013/5000 train_loss:8.1185 train_time:726983ms step_avg:181.61ms\nstep:4014/5000 train_loss:8.1553 train_time:727164ms step_avg:181.61ms\nstep:4015/5000 train_loss:8.0713 train_time:727344ms step_avg:181.61ms\nstep:4016/5000 train_loss:8.1199 train_time:727526ms step_avg:181.61ms\nstep:4017/5000 train_loss:8.0691 train_time:727708ms step_avg:181.61ms\nstep:4018/5000 train_loss:8.1192 train_time:727889ms step_avg:181.61ms\nstep:4019/5000 train_loss:8.1987 train_time:728070ms step_avg:181.61ms\nstep:4020/5000 train_loss:8.1250 train_time:728254ms step_avg:181.61ms\nstep:4021/5000 train_loss:8.1081 train_time:728435ms step_avg:181.61ms\nstep:4022/5000 train_loss:8.1217 train_time:728619ms step_avg:181.61ms\nstep:4023/5000 train_loss:8.1554 train_time:728800ms step_avg:181.61ms\nstep:4024/5000 train_loss:8.2116 train_time:728981ms step_avg:181.61ms\nstep:4025/5000 train_loss:8.1499 train_time:729163ms step_avg:181.61ms\nstep:4026/5000 train_loss:8.0329 train_time:729344ms step_avg:181.61ms\nstep:4027/5000 train_loss:8.3966 train_time:729525ms step_avg:181.61ms\nstep:4028/5000 train_loss:8.0411 train_time:729706ms step_avg:181.61ms\nstep:4029/5000 train_loss:8.1441 train_time:729887ms step_avg:181.61ms\nstep:4030/5000 train_loss:8.0723 train_time:730068ms step_avg:181.61ms\nstep:4031/5000 train_loss:8.1465 train_time:730250ms step_avg:181.61ms\nstep:4032/5000 train_loss:8.0727 train_time:730433ms step_avg:181.61ms\nstep:4033/5000 train_loss:8.0876 train_time:730617ms step_avg:181.61ms\nstep:4034/5000 train_loss:8.1286 train_time:730799ms step_avg:181.61ms\nstep:4035/5000 train_loss:8.0903 train_time:730980ms step_avg:181.61ms\nstep:4036/5000 train_loss:8.0456 train_time:731163ms step_avg:181.61ms\nstep:4037/5000 train_loss:8.2751 train_time:731344ms step_avg:181.61ms\nstep:4038/5000 train_loss:8.0532 train_time:731526ms step_avg:181.61ms\nstep:4039/5000 train_loss:8.2086 train_time:731708ms step_avg:181.61ms\nstep:4040/5000 train_loss:8.0978 train_time:731890ms step_avg:181.61ms\nstep:4041/5000 train_loss:8.1821 train_time:732071ms step_avg:181.61ms\nstep:4042/5000 train_loss:8.1383 train_time:732253ms step_avg:181.61ms\nstep:4043/5000 train_loss:8.1189 train_time:732435ms step_avg:181.61ms\nstep:4044/5000 train_loss:8.1431 train_time:732619ms step_avg:181.61ms\nstep:4045/5000 train_loss:8.1834 train_time:732801ms step_avg:181.61ms\nstep:4046/5000 train_loss:8.1550 train_time:732983ms step_avg:181.61ms\nstep:4047/5000 train_loss:8.2477 train_time:733165ms step_avg:181.61ms\nstep:4048/5000 train_loss:8.0661 train_time:733345ms step_avg:181.61ms\nstep:4049/5000 train_loss:8.0867 train_time:733525ms step_avg:181.61ms\nstep:4050/5000 train_loss:8.0582 train_time:733707ms step_avg:181.61ms\nstep:4051/5000 train_loss:8.1416 train_time:733888ms step_avg:181.61ms\nstep:4052/5000 train_loss:8.0567 train_time:734071ms step_avg:181.61ms\nstep:4053/5000 train_loss:8.1617 train_time:734254ms step_avg:181.61ms\nstep:4054/5000 train_loss:7.9832 train_time:734436ms step_avg:181.61ms\nstep:4055/5000 train_loss:8.2127 train_time:734619ms step_avg:181.61ms\nstep:4056/5000 train_loss:7.9998 train_time:734800ms step_avg:181.61ms\nstep:4057/5000 train_loss:7.9772 train_time:734981ms step_avg:181.61ms\nstep:4058/5000 train_loss:8.1556 train_time:735162ms step_avg:181.61ms\nstep:4059/5000 train_loss:8.1078 train_time:735346ms step_avg:181.61ms\nstep:4060/5000 train_loss:8.1892 train_time:735525ms step_avg:181.61ms\nstep:4061/5000 train_loss:8.2514 train_time:735706ms step_avg:181.61ms\nstep:4062/5000 train_loss:8.1095 train_time:735888ms step_avg:181.61ms\nstep:4063/5000 train_loss:8.1483 train_time:736070ms step_avg:181.61ms\nstep:4064/5000 train_loss:8.2269 train_time:736252ms step_avg:181.61ms\nstep:4065/5000 train_loss:7.9993 train_time:736434ms step_avg:181.61ms\nstep:4066/5000 train_loss:8.1714 train_time:736617ms step_avg:181.61ms\nstep:4067/5000 train_loss:8.1303 train_time:736799ms step_avg:181.61ms\nstep:4068/5000 train_loss:8.0755 train_time:736980ms step_avg:181.61ms\nstep:4069/5000 train_loss:8.2538 train_time:737161ms step_avg:181.61ms\nstep:4070/5000 train_loss:8.1097 train_time:737343ms step_avg:181.61ms\nstep:4071/5000 train_loss:8.0846 train_time:737524ms step_avg:181.61ms\nstep:4072/5000 train_loss:8.1644 train_time:737705ms step_avg:181.61ms\nstep:4073/5000 train_loss:7.9694 train_time:737885ms step_avg:181.61ms\nstep:4074/5000 train_loss:8.1717 train_time:738065ms step_avg:181.61ms\nstep:4075/5000 train_loss:8.1177 train_time:738246ms step_avg:181.61ms\nstep:4076/5000 train_loss:8.2754 train_time:738428ms step_avg:181.61ms\nstep:4077/5000 train_loss:8.2377 train_time:738611ms step_avg:181.61ms\nstep:4078/5000 train_loss:8.0626 train_time:738795ms step_avg:181.61ms\nstep:4079/5000 train_loss:8.1352 train_time:738977ms step_avg:181.61ms\nstep:4080/5000 train_loss:8.0990 train_time:739158ms step_avg:181.61ms\nstep:4081/5000 train_loss:8.0969 train_time:739340ms step_avg:181.61ms\nstep:4082/5000 train_loss:8.1120 train_time:739522ms step_avg:181.61ms\nstep:4083/5000 train_loss:8.0746 train_time:739707ms step_avg:181.61ms\nstep:4084/5000 train_loss:8.1164 train_time:739885ms step_avg:181.61ms\nstep:4085/5000 train_loss:8.1914 train_time:740066ms step_avg:181.61ms\nstep:4086/5000 train_loss:8.0834 train_time:740248ms step_avg:181.61ms\nstep:4087/5000 train_loss:8.0971 train_time:740429ms step_avg:181.61ms\nstep:4088/5000 train_loss:8.1244 train_time:740611ms step_avg:181.61ms\nstep:4089/5000 train_loss:8.2007 train_time:740794ms step_avg:181.61ms\nstep:4090/5000 train_loss:8.1202 train_time:740976ms step_avg:181.61ms\nstep:4091/5000 train_loss:8.0980 train_time:741158ms step_avg:181.61ms\nstep:4092/5000 train_loss:8.1674 train_time:741340ms step_avg:181.61ms\nstep:4093/5000 train_loss:8.0311 train_time:741522ms step_avg:181.61ms\nstep:4094/5000 train_loss:8.1113 train_time:741704ms step_avg:181.61ms\nstep:4095/5000 train_loss:8.2184 train_time:741884ms step_avg:181.61ms\nstep:4096/5000 train_loss:8.1225 train_time:742064ms step_avg:181.61ms\nstep:4097/5000 train_loss:8.1184 train_time:742244ms step_avg:181.61ms\nstep:4098/5000 train_loss:7.9928 train_time:742424ms step_avg:181.61ms\nstep:4099/5000 train_loss:8.1986 train_time:742605ms step_avg:181.61ms\nstep:4100/5000 train_loss:8.1016 train_time:742786ms step_avg:181.61ms\nstep:4101/5000 train_loss:8.0547 train_time:742967ms step_avg:181.61ms\nstep:4102/5000 train_loss:8.0813 train_time:743149ms step_avg:181.61ms\nstep:4103/5000 train_loss:8.0939 train_time:743330ms step_avg:181.61ms\nstep:4104/5000 train_loss:8.0447 train_time:743512ms step_avg:181.61ms\nstep:4105/5000 train_loss:8.1369 train_time:743695ms step_avg:181.61ms\nstep:4106/5000 train_loss:8.1125 train_time:743878ms step_avg:181.61ms\nstep:4107/5000 train_loss:8.0726 train_time:744061ms step_avg:181.61ms\nstep:4108/5000 train_loss:8.2348 train_time:744243ms step_avg:181.61ms\nstep:4109/5000 train_loss:8.1082 train_time:744424ms step_avg:181.61ms\nstep:4110/5000 train_loss:8.1186 train_time:744605ms step_avg:181.61ms\nstep:4111/5000 train_loss:8.1179 train_time:744785ms step_avg:181.61ms\nstep:4112/5000 train_loss:8.3275 train_time:744965ms step_avg:181.61ms\nstep:4113/5000 train_loss:8.0147 train_time:745146ms step_avg:181.61ms\nstep:4114/5000 train_loss:8.0450 train_time:745328ms step_avg:181.61ms\nstep:4115/5000 train_loss:8.1260 train_time:745511ms step_avg:181.61ms\nstep:4116/5000 train_loss:8.1318 train_time:745693ms step_avg:181.61ms\nstep:4117/5000 train_loss:7.9725 train_time:745875ms step_avg:181.61ms\nstep:4118/5000 train_loss:8.1838 train_time:746058ms step_avg:181.61ms\nstep:4119/5000 train_loss:8.1466 train_time:746240ms step_avg:181.61ms\nstep:4120/5000 train_loss:8.0425 train_time:746423ms step_avg:181.61ms\nstep:4121/5000 train_loss:8.1677 train_time:746606ms step_avg:181.61ms\nstep:4122/5000 train_loss:8.0194 train_time:746786ms step_avg:181.61ms\nstep:4123/5000 train_loss:8.0607 train_time:746966ms step_avg:181.61ms\nstep:4124/5000 train_loss:8.1352 train_time:747147ms step_avg:181.61ms\nstep:4125/5000 train_loss:8.1281 train_time:747327ms step_avg:181.61ms\nstep:4126/5000 train_loss:8.2064 train_time:747509ms step_avg:181.61ms\nstep:4127/5000 train_loss:7.9174 train_time:747690ms step_avg:181.61ms\nstep:4128/5000 train_loss:8.0020 train_time:747873ms step_avg:181.61ms\nstep:4129/5000 train_loss:8.1094 train_time:748056ms step_avg:181.61ms\nstep:4130/5000 train_loss:8.1368 train_time:748239ms step_avg:181.61ms\nstep:4131/5000 train_loss:8.0954 train_time:748420ms step_avg:181.61ms\nstep:4132/5000 train_loss:8.2128 train_time:748602ms step_avg:181.61ms\nstep:4133/5000 train_loss:8.0100 train_time:748783ms step_avg:181.61ms\nstep:4134/5000 train_loss:8.0815 train_time:748964ms step_avg:181.61ms\nstep:4135/5000 train_loss:8.1875 train_time:749146ms step_avg:181.61ms\nstep:4136/5000 train_loss:8.0529 train_time:749326ms step_avg:181.61ms\nstep:4137/5000 train_loss:8.1238 train_time:749509ms step_avg:181.61ms\nstep:4138/5000 train_loss:8.2069 train_time:749690ms step_avg:181.61ms\nstep:4139/5000 train_loss:8.1431 train_time:749873ms step_avg:181.61ms\nstep:4140/5000 train_loss:8.0848 train_time:750056ms step_avg:181.61ms\nstep:4141/5000 train_loss:8.1628 train_time:750238ms step_avg:181.61ms\nstep:4142/5000 train_loss:8.2170 train_time:750421ms step_avg:181.61ms\nstep:4143/5000 train_loss:8.1191 train_time:750603ms step_avg:181.61ms\nstep:4144/5000 train_loss:8.0881 train_time:750784ms step_avg:181.61ms\nstep:4145/5000 train_loss:8.0828 train_time:750965ms step_avg:181.61ms\nstep:4146/5000 train_loss:8.0551 train_time:751146ms step_avg:181.61ms\nstep:4147/5000 train_loss:8.0897 train_time:751326ms step_avg:181.61ms\nstep:4148/5000 train_loss:8.0000 train_time:751507ms step_avg:181.61ms\nstep:4149/5000 train_loss:8.0199 train_time:751689ms step_avg:181.61ms\nstep:4150/5000 train_loss:8.0281 train_time:751871ms step_avg:181.61ms\nstep:4151/5000 train_loss:8.1308 train_time:752054ms step_avg:181.61ms\nstep:4152/5000 train_loss:8.1473 train_time:752236ms step_avg:181.61ms\nstep:4153/5000 train_loss:8.0864 train_time:752419ms step_avg:181.61ms\nstep:4154/5000 train_loss:8.1082 train_time:752600ms step_avg:181.61ms\nstep:4155/5000 train_loss:8.2330 train_time:752782ms step_avg:181.61ms\nstep:4156/5000 train_loss:8.2229 train_time:752962ms step_avg:181.61ms\nstep:4157/5000 train_loss:8.1302 train_time:753145ms step_avg:181.61ms\nstep:4158/5000 train_loss:8.0400 train_time:753325ms step_avg:181.61ms\nstep:4159/5000 train_loss:8.0785 train_time:753506ms step_avg:181.61ms\nstep:4160/5000 train_loss:8.1154 train_time:753687ms step_avg:181.61ms\nstep:4161/5000 train_loss:8.0402 train_time:753868ms step_avg:181.61ms\nstep:4162/5000 train_loss:8.0313 train_time:754051ms step_avg:181.61ms\nstep:4163/5000 train_loss:8.1112 train_time:754233ms step_avg:181.61ms\nstep:4164/5000 train_loss:8.0977 train_time:754415ms step_avg:181.61ms\nstep:4165/5000 train_loss:7.9955 train_time:754597ms step_avg:181.61ms\nstep:4166/5000 train_loss:8.1413 train_time:754778ms step_avg:181.61ms\nstep:4167/5000 train_loss:8.0329 train_time:754960ms step_avg:181.61ms\nstep:4168/5000 train_loss:8.1597 train_time:755142ms step_avg:181.61ms\nstep:4169/5000 train_loss:8.0971 train_time:755324ms step_avg:181.61ms\nstep:4170/5000 train_loss:8.2971 train_time:755507ms step_avg:181.61ms\nstep:4171/5000 train_loss:8.1186 train_time:755688ms step_avg:181.61ms\nstep:4172/5000 train_loss:7.9706 train_time:755869ms step_avg:181.61ms\nstep:4173/5000 train_loss:8.2527 train_time:756050ms step_avg:181.61ms\nstep:4174/5000 train_loss:8.0900 train_time:756232ms step_avg:181.61ms\nstep:4175/5000 train_loss:8.1381 train_time:756415ms step_avg:181.61ms\nstep:4176/5000 train_loss:8.1593 train_time:756598ms step_avg:181.61ms\nstep:4177/5000 train_loss:8.1875 train_time:756780ms step_avg:181.61ms\nstep:4178/5000 train_loss:8.1518 train_time:756961ms step_avg:181.61ms\nstep:4179/5000 train_loss:8.2047 train_time:757142ms step_avg:181.61ms\nstep:4180/5000 train_loss:8.0572 train_time:757324ms step_avg:181.61ms\nstep:4181/5000 train_loss:8.0912 train_time:757505ms step_avg:181.61ms\nstep:4182/5000 train_loss:8.1141 train_time:757686ms step_avg:181.61ms\nstep:4183/5000 train_loss:8.0001 train_time:757867ms step_avg:181.61ms\nstep:4184/5000 train_loss:8.0264 train_time:758048ms step_avg:181.61ms\nstep:4185/5000 train_loss:8.0135 train_time:758229ms step_avg:181.61ms\nstep:4186/5000 train_loss:8.0149 train_time:758411ms step_avg:181.61ms\nstep:4187/5000 train_loss:8.0886 train_time:758594ms step_avg:181.61ms\nstep:4188/5000 train_loss:8.1705 train_time:758777ms step_avg:181.61ms\nstep:4189/5000 train_loss:8.1370 train_time:758959ms step_avg:181.61ms\nstep:4190/5000 train_loss:8.2222 train_time:759141ms step_avg:181.61ms\nstep:4191/5000 train_loss:8.2436 train_time:759323ms step_avg:181.61ms\nstep:4192/5000 train_loss:8.1256 train_time:759504ms step_avg:181.61ms\nstep:4193/5000 train_loss:8.1199 train_time:759686ms step_avg:181.61ms\nstep:4194/5000 train_loss:7.9681 train_time:759867ms step_avg:181.61ms\nstep:4195/5000 train_loss:8.1207 train_time:760048ms step_avg:181.61ms\nstep:4196/5000 train_loss:8.3275 train_time:760230ms step_avg:181.61ms\nstep:4197/5000 train_loss:8.1317 train_time:760412ms step_avg:181.61ms\nstep:4198/5000 train_loss:8.2839 train_time:760595ms step_avg:181.61ms\nstep:4199/5000 train_loss:8.1021 train_time:760778ms step_avg:181.61ms\nstep:4200/5000 train_loss:8.0393 train_time:760959ms step_avg:181.61ms\nstep:4201/5000 train_loss:8.0594 train_time:761140ms step_avg:181.61ms\nstep:4202/5000 train_loss:8.3146 train_time:761321ms step_avg:181.61ms\nstep:4203/5000 train_loss:8.0941 train_time:761503ms step_avg:181.61ms\nstep:4204/5000 train_loss:8.1336 train_time:761683ms step_avg:181.61ms\nstep:4205/5000 train_loss:8.1985 train_time:761865ms step_avg:181.61ms\nstep:4206/5000 train_loss:8.0784 train_time:762047ms step_avg:181.61ms\nstep:4207/5000 train_loss:8.0870 train_time:762228ms step_avg:181.61ms\nstep:4208/5000 train_loss:8.0564 train_time:762410ms step_avg:181.61ms\nstep:4209/5000 train_loss:8.1604 train_time:762593ms step_avg:181.61ms\nstep:4210/5000 train_loss:7.9711 train_time:762776ms step_avg:181.61ms\nstep:4211/5000 train_loss:8.0881 train_time:762959ms step_avg:181.61ms\nstep:4212/5000 train_loss:8.1221 train_time:763140ms step_avg:181.61ms\nstep:4213/5000 train_loss:8.0337 train_time:763323ms step_avg:181.61ms\nstep:4214/5000 train_loss:8.0338 train_time:763505ms step_avg:181.61ms\nstep:4215/5000 train_loss:8.2083 train_time:763685ms step_avg:181.61ms\nstep:4216/5000 train_loss:8.1683 train_time:763866ms step_avg:181.61ms\nstep:4217/5000 train_loss:8.0981 train_time:764047ms step_avg:181.61ms\nstep:4218/5000 train_loss:8.1667 train_time:764228ms step_avg:181.61ms\nstep:4219/5000 train_loss:8.1460 train_time:764410ms step_avg:181.61ms\nstep:4220/5000 train_loss:8.2090 train_time:764592ms step_avg:181.61ms\nstep:4221/5000 train_loss:8.0563 train_time:764774ms step_avg:181.61ms\nstep:4222/5000 train_loss:8.0718 train_time:764957ms step_avg:181.61ms\nstep:4223/5000 train_loss:8.1261 train_time:765140ms step_avg:181.61ms\nstep:4224/5000 train_loss:8.0709 train_time:765321ms step_avg:181.61ms\nstep:4225/5000 train_loss:8.2674 train_time:765502ms step_avg:181.61ms\nstep:4226/5000 train_loss:8.0848 train_time:765683ms step_avg:181.61ms\nstep:4227/5000 train_loss:8.0466 train_time:765863ms step_avg:181.61ms\nstep:4228/5000 train_loss:8.1277 train_time:766045ms step_avg:181.61ms\nstep:4229/5000 train_loss:8.0959 train_time:766226ms step_avg:181.61ms\nstep:4230/5000 train_loss:8.0758 train_time:766408ms step_avg:181.61ms\nstep:4231/5000 train_loss:8.0699 train_time:766589ms step_avg:181.61ms\nstep:4232/5000 train_loss:8.2106 train_time:766771ms step_avg:181.61ms\nstep:4233/5000 train_loss:8.0403 train_time:766954ms step_avg:181.61ms\nstep:4234/5000 train_loss:8.2066 train_time:767136ms step_avg:181.61ms\nstep:4235/5000 train_loss:8.0507 train_time:767319ms step_avg:181.61ms\nstep:4236/5000 train_loss:8.0794 train_time:767500ms step_avg:181.61ms\nstep:4237/5000 train_loss:8.0853 train_time:767680ms step_avg:181.61ms\nstep:4238/5000 train_loss:8.1201 train_time:767861ms step_avg:181.61ms\nstep:4239/5000 train_loss:8.0267 train_time:768043ms step_avg:181.61ms\nstep:4240/5000 train_loss:8.0639 train_time:768224ms step_avg:181.61ms\nstep:4241/5000 train_loss:8.1562 train_time:768406ms step_avg:181.61ms\nstep:4242/5000 train_loss:8.0411 train_time:768586ms step_avg:181.61ms\nstep:4243/5000 train_loss:8.2401 train_time:768766ms step_avg:181.61ms\nstep:4244/5000 train_loss:8.2699 train_time:768947ms step_avg:181.61ms\nstep:4245/5000 train_loss:8.1273 train_time:769127ms step_avg:181.61ms\nstep:4246/5000 train_loss:8.2292 train_time:769310ms step_avg:181.61ms\nstep:4247/5000 train_loss:8.1353 train_time:769493ms step_avg:181.61ms\nstep:4248/5000 train_loss:8.1530 train_time:769675ms step_avg:181.61ms\nstep:4249/5000 train_loss:8.1306 train_time:769857ms step_avg:181.61ms\nstep:4250/5000 train_loss:8.1534 train_time:770040ms step_avg:181.61ms\nstep:4251/5000 train_loss:7.9415 train_time:770222ms step_avg:181.61ms\nstep:4252/5000 train_loss:8.1496 train_time:770403ms step_avg:181.61ms\nstep:4253/5000 train_loss:8.0515 train_time:770584ms step_avg:181.61ms\nstep:4254/5000 train_loss:8.1145 train_time:770765ms step_avg:181.61ms\nstep:4255/5000 train_loss:8.2271 train_time:770946ms step_avg:181.61ms\nstep:4256/5000 train_loss:8.0817 train_time:771127ms step_avg:181.61ms\nstep:4257/5000 train_loss:8.0332 train_time:771309ms step_avg:181.61ms\nstep:4258/5000 train_loss:8.1342 train_time:771490ms step_avg:181.61ms\nstep:4259/5000 train_loss:8.1747 train_time:771672ms step_avg:181.61ms\nstep:4260/5000 train_loss:8.1840 train_time:771854ms step_avg:181.61ms\nstep:4261/5000 train_loss:8.1766 train_time:772037ms step_avg:181.61ms\nstep:4262/5000 train_loss:8.1569 train_time:772219ms step_avg:181.61ms\nstep:4263/5000 train_loss:8.0434 train_time:772401ms step_avg:181.61ms\nstep:4264/5000 train_loss:8.1203 train_time:772582ms step_avg:181.61ms\nstep:4265/5000 train_loss:8.0875 train_time:772762ms step_avg:181.61ms\nstep:4266/5000 train_loss:8.1315 train_time:772944ms step_avg:181.61ms\nstep:4267/5000 train_loss:8.1003 train_time:773124ms step_avg:181.61ms\nstep:4268/5000 train_loss:8.0376 train_time:773305ms step_avg:181.61ms\nstep:4269/5000 train_loss:8.1332 train_time:773485ms step_avg:181.61ms\nstep:4270/5000 train_loss:8.4105 train_time:773666ms step_avg:181.61ms\nstep:4271/5000 train_loss:8.1961 train_time:773847ms step_avg:181.61ms\nstep:4272/5000 train_loss:7.9775 train_time:774027ms step_avg:181.61ms\nstep:4273/5000 train_loss:8.1961 train_time:774211ms step_avg:181.61ms\nstep:4274/5000 train_loss:8.0809 train_time:774394ms step_avg:181.61ms\nstep:4275/5000 train_loss:8.1173 train_time:774576ms step_avg:181.61ms\nstep:4276/5000 train_loss:8.1183 train_time:774757ms step_avg:181.61ms\nstep:4277/5000 train_loss:8.0525 train_time:774939ms step_avg:181.61ms\nstep:4278/5000 train_loss:8.0190 train_time:775120ms step_avg:181.61ms\nstep:4279/5000 train_loss:8.1215 train_time:775301ms step_avg:181.61ms\nstep:4280/5000 train_loss:8.0321 train_time:775482ms step_avg:181.61ms\nstep:4281/5000 train_loss:8.1324 train_time:775663ms step_avg:181.61ms\nstep:4282/5000 train_loss:8.1698 train_time:775843ms step_avg:181.61ms\nstep:4283/5000 train_loss:8.2147 train_time:776024ms step_avg:181.61ms\nstep:4284/5000 train_loss:7.9726 train_time:776204ms step_avg:181.61ms\nstep:4285/5000 train_loss:8.1596 train_time:776384ms step_avg:181.61ms\nstep:4286/5000 train_loss:8.0784 train_time:776566ms step_avg:181.61ms\nstep:4287/5000 train_loss:8.1573 train_time:776749ms step_avg:181.61ms\nstep:4288/5000 train_loss:8.0365 train_time:776929ms step_avg:181.61ms\nstep:4289/5000 train_loss:8.1512 train_time:777113ms step_avg:181.61ms\nstep:4290/5000 train_loss:8.0517 train_time:777295ms step_avg:181.61ms\nstep:4291/5000 train_loss:8.0278 train_time:777477ms step_avg:181.61ms\nstep:4292/5000 train_loss:8.0557 train_time:777659ms step_avg:181.61ms\nstep:4293/5000 train_loss:8.0480 train_time:777840ms step_avg:181.61ms\nstep:4294/5000 train_loss:8.0479 train_time:778022ms step_avg:181.61ms\nstep:4295/5000 train_loss:8.0402 train_time:778204ms step_avg:181.61ms\nstep:4296/5000 train_loss:8.1369 train_time:778384ms step_avg:181.61ms\nstep:4297/5000 train_loss:8.1248 train_time:778565ms step_avg:181.61ms\nstep:4298/5000 train_loss:8.0871 train_time:778745ms step_avg:181.61ms\nstep:4299/5000 train_loss:8.1418 train_time:778927ms step_avg:181.61ms\nstep:4300/5000 train_loss:8.1132 train_time:779108ms step_avg:181.61ms\nstep:4301/5000 train_loss:8.1989 train_time:779289ms step_avg:181.61ms\nstep:4302/5000 train_loss:8.1760 train_time:779472ms step_avg:181.61ms\nstep:4303/5000 train_loss:8.1890 train_time:779654ms step_avg:181.61ms\nstep:4304/5000 train_loss:8.1821 train_time:779837ms step_avg:181.61ms\nstep:4305/5000 train_loss:8.1395 train_time:780020ms step_avg:181.61ms\nstep:4306/5000 train_loss:8.1119 train_time:780200ms step_avg:181.61ms\nstep:4307/5000 train_loss:8.0957 train_time:780381ms step_avg:181.61ms\nstep:4308/5000 train_loss:8.0818 train_time:780561ms step_avg:181.61ms\nstep:4309/5000 train_loss:8.1711 train_time:780742ms step_avg:181.61ms\nstep:4310/5000 train_loss:8.0824 train_time:780925ms step_avg:181.61ms\nstep:4311/5000 train_loss:8.1415 train_time:781107ms step_avg:181.61ms\nstep:4312/5000 train_loss:8.1833 train_time:781288ms step_avg:181.61ms\nstep:4313/5000 train_loss:8.1196 train_time:781469ms step_avg:181.61ms\nstep:4314/5000 train_loss:8.0795 train_time:781650ms step_avg:181.61ms\nstep:4315/5000 train_loss:8.1039 train_time:781832ms step_avg:181.61ms\nstep:4316/5000 train_loss:8.3177 train_time:782015ms step_avg:181.61ms\nstep:4317/5000 train_loss:8.0457 train_time:782197ms step_avg:181.61ms\nstep:4318/5000 train_loss:8.1582 train_time:782379ms step_avg:181.61ms\nstep:4319/5000 train_loss:8.0965 train_time:782560ms step_avg:181.61ms\nstep:4320/5000 train_loss:8.1833 train_time:782741ms step_avg:181.61ms\nstep:4321/5000 train_loss:8.1155 train_time:782924ms step_avg:181.61ms\nstep:4322/5000 train_loss:8.1165 train_time:783105ms step_avg:181.61ms\nstep:4323/5000 train_loss:8.1426 train_time:783286ms step_avg:181.61ms\nstep:4324/5000 train_loss:8.0645 train_time:783466ms step_avg:181.61ms\nstep:4325/5000 train_loss:8.0866 train_time:783647ms step_avg:181.61ms\nstep:4326/5000 train_loss:8.2629 train_time:783828ms step_avg:181.61ms\nstep:4327/5000 train_loss:8.1553 train_time:784011ms step_avg:181.61ms\nstep:4328/5000 train_loss:8.0849 train_time:784193ms step_avg:181.61ms\nstep:4329/5000 train_loss:8.1134 train_time:784376ms step_avg:181.61ms\nstep:4330/5000 train_loss:8.1075 train_time:784557ms step_avg:181.61ms\nstep:4331/5000 train_loss:8.1124 train_time:784739ms step_avg:181.61ms\nstep:4332/5000 train_loss:8.2450 train_time:784922ms step_avg:181.61ms\nstep:4333/5000 train_loss:8.0896 train_time:785103ms step_avg:181.61ms\nstep:4334/5000 train_loss:8.1236 train_time:785283ms step_avg:181.61ms\nstep:4335/5000 train_loss:8.1113 train_time:785464ms step_avg:181.61ms\nstep:4336/5000 train_loss:8.2497 train_time:785644ms step_avg:181.61ms\nstep:4337/5000 train_loss:7.8132 train_time:785824ms step_avg:181.61ms\nstep:4338/5000 train_loss:8.0593 train_time:786005ms step_avg:181.61ms\nstep:4339/5000 train_loss:8.2302 train_time:786185ms step_avg:181.61ms\nstep:4340/5000 train_loss:8.1410 train_time:786366ms step_avg:181.61ms\nstep:4341/5000 train_loss:8.0415 train_time:786547ms step_avg:181.61ms\nstep:4342/5000 train_loss:8.0665 train_time:786727ms step_avg:181.61ms\nstep:4343/5000 train_loss:8.0901 train_time:786909ms step_avg:181.61ms\nstep:4344/5000 train_loss:8.1320 train_time:787090ms step_avg:181.61ms\nstep:4345/5000 train_loss:8.1695 train_time:787274ms step_avg:181.61ms\nstep:4346/5000 train_loss:8.1479 train_time:787456ms step_avg:181.61ms\nstep:4347/5000 train_loss:8.0864 train_time:787639ms step_avg:181.61ms\nstep:4348/5000 train_loss:8.1903 train_time:787820ms step_avg:181.61ms\nstep:4349/5000 train_loss:8.1009 train_time:788001ms step_avg:181.61ms\nstep:4350/5000 train_loss:8.0235 train_time:788181ms step_avg:181.61ms\nstep:4351/5000 train_loss:8.0961 train_time:788363ms step_avg:181.61ms\nstep:4352/5000 train_loss:8.1845 train_time:788544ms step_avg:181.61ms\nstep:4353/5000 train_loss:8.1900 train_time:788725ms step_avg:181.61ms\nstep:4354/5000 train_loss:7.8782 train_time:788907ms step_avg:181.61ms\nstep:4355/5000 train_loss:8.1332 train_time:789088ms step_avg:181.61ms\nstep:4356/5000 train_loss:8.2863 train_time:789268ms step_avg:181.61ms\nstep:4357/5000 train_loss:8.0387 train_time:789449ms step_avg:181.61ms\nstep:4358/5000 train_loss:8.0915 train_time:789633ms step_avg:181.61ms\nstep:4359/5000 train_loss:7.9967 train_time:789815ms step_avg:181.61ms\nstep:4360/5000 train_loss:7.9739 train_time:789999ms step_avg:181.61ms\nstep:4361/5000 train_loss:7.9733 train_time:790180ms step_avg:181.61ms\nstep:4362/5000 train_loss:8.1249 train_time:790360ms step_avg:181.61ms\nstep:4363/5000 train_loss:8.1659 train_time:790542ms step_avg:181.61ms\nstep:4364/5000 train_loss:8.1369 train_time:790724ms step_avg:181.61ms\nstep:4365/5000 train_loss:8.0626 train_time:790905ms step_avg:181.61ms\nstep:4366/5000 train_loss:8.1095 train_time:791085ms step_avg:181.61ms\nstep:4367/5000 train_loss:8.1399 train_time:791266ms step_avg:181.61ms\nstep:4368/5000 train_loss:8.1945 train_time:791447ms step_avg:181.61ms\nstep:4369/5000 train_loss:8.1421 train_time:791628ms step_avg:181.61ms\nstep:4370/5000 train_loss:8.0796 train_time:791810ms step_avg:181.61ms\nstep:4371/5000 train_loss:8.1597 train_time:791993ms step_avg:181.61ms\nstep:4372/5000 train_loss:8.2129 train_time:792175ms step_avg:181.61ms\nstep:4373/5000 train_loss:8.0765 train_time:792357ms step_avg:181.61ms\nstep:4374/5000 train_loss:8.0039 train_time:792539ms step_avg:181.61ms\nstep:4375/5000 train_loss:8.0903 train_time:792721ms step_avg:181.61ms\nstep:4376/5000 train_loss:8.1422 train_time:792903ms step_avg:181.61ms\nstep:4377/5000 train_loss:8.0845 train_time:793085ms step_avg:181.61ms\nstep:4378/5000 train_loss:8.1483 train_time:793265ms step_avg:181.61ms\nstep:4379/5000 train_loss:8.0814 train_time:793445ms step_avg:181.61ms\nstep:4380/5000 train_loss:8.1721 train_time:793626ms step_avg:181.61ms\nstep:4381/5000 train_loss:8.0956 train_time:793807ms step_avg:181.61ms\nstep:4382/5000 train_loss:8.2275 train_time:793987ms step_avg:181.61ms\nstep:4383/5000 train_loss:8.1150 train_time:794168ms step_avg:181.61ms\nstep:4384/5000 train_loss:8.1513 train_time:794351ms step_avg:181.61ms\nstep:4385/5000 train_loss:8.1537 train_time:794533ms step_avg:181.61ms\nstep:4386/5000 train_loss:8.2679 train_time:794716ms step_avg:181.61ms\nstep:4387/5000 train_loss:8.1232 train_time:794898ms step_avg:181.61ms\nstep:4388/5000 train_loss:8.0997 train_time:795080ms step_avg:181.61ms\nstep:4389/5000 train_loss:8.1517 train_time:795260ms step_avg:181.61ms\nstep:4390/5000 train_loss:8.1237 train_time:795442ms step_avg:181.61ms\nstep:4391/5000 train_loss:8.0615 train_time:795624ms step_avg:181.61ms\nstep:4392/5000 train_loss:8.1745 train_time:795805ms step_avg:181.61ms\nstep:4393/5000 train_loss:8.0359 train_time:795986ms step_avg:181.61ms\nstep:4394/5000 train_loss:8.0421 train_time:796167ms step_avg:181.61ms\nstep:4395/5000 train_loss:8.0101 train_time:796348ms step_avg:181.61ms\nstep:4396/5000 train_loss:8.1786 train_time:796528ms step_avg:181.61ms\nstep:4397/5000 train_loss:8.1874 train_time:796710ms step_avg:181.61ms\nstep:4398/5000 train_loss:8.0715 train_time:796893ms step_avg:181.61ms\nstep:4399/5000 train_loss:8.0597 train_time:797075ms step_avg:181.61ms\nstep:4400/5000 train_loss:8.1185 train_time:797255ms step_avg:181.61ms\nstep:4401/5000 train_loss:8.0771 train_time:797438ms step_avg:181.61ms\nstep:4402/5000 train_loss:8.0966 train_time:797619ms step_avg:181.61ms\nstep:4403/5000 train_loss:8.1682 train_time:797802ms step_avg:181.61ms\nstep:4404/5000 train_loss:8.0737 train_time:797983ms step_avg:181.61ms\nstep:4405/5000 train_loss:8.0783 train_time:798163ms step_avg:181.61ms\nstep:4406/5000 train_loss:8.0963 train_time:798343ms step_avg:181.61ms\nstep:4407/5000 train_loss:8.0521 train_time:798523ms step_avg:181.61ms\nstep:4408/5000 train_loss:8.1882 train_time:798704ms step_avg:181.61ms\nstep:4409/5000 train_loss:8.0607 train_time:798885ms step_avg:181.61ms\nstep:4410/5000 train_loss:8.2075 train_time:799067ms step_avg:181.61ms\nstep:4411/5000 train_loss:8.0309 train_time:799249ms step_avg:181.61ms\nstep:4412/5000 train_loss:8.1098 train_time:799430ms step_avg:181.61ms\nstep:4413/5000 train_loss:8.1638 train_time:799611ms step_avg:181.61ms\nstep:4414/5000 train_loss:8.0312 train_time:799793ms step_avg:181.61ms\nstep:4415/5000 train_loss:8.2086 train_time:799976ms step_avg:181.61ms\nstep:4416/5000 train_loss:8.1881 train_time:800158ms step_avg:181.61ms\nstep:4417/5000 train_loss:8.1081 train_time:800340ms step_avg:181.61ms\nstep:4418/5000 train_loss:8.0994 train_time:800521ms step_avg:181.61ms\nstep:4419/5000 train_loss:8.1396 train_time:800703ms step_avg:181.61ms\nstep:4420/5000 train_loss:8.0298 train_time:800884ms step_avg:181.61ms\nstep:4421/5000 train_loss:8.1127 train_time:801064ms step_avg:181.61ms\nstep:4422/5000 train_loss:8.1055 train_time:801244ms step_avg:181.61ms\nstep:4423/5000 train_loss:8.1267 train_time:801425ms step_avg:181.61ms\nstep:4424/5000 train_loss:8.0477 train_time:801606ms step_avg:181.61ms\nstep:4425/5000 train_loss:8.1579 train_time:801788ms step_avg:181.61ms\nstep:4426/5000 train_loss:8.1100 train_time:801969ms step_avg:181.61ms\nstep:4427/5000 train_loss:8.2565 train_time:802151ms step_avg:181.61ms\nstep:4428/5000 train_loss:8.2553 train_time:802331ms step_avg:181.61ms\nstep:4429/5000 train_loss:8.0806 train_time:802515ms step_avg:181.61ms\nstep:4430/5000 train_loss:8.1066 train_time:802696ms step_avg:181.61ms\nstep:4431/5000 train_loss:8.1866 train_time:802877ms step_avg:181.61ms\nstep:4432/5000 train_loss:8.1144 train_time:803058ms step_avg:181.61ms\nstep:4433/5000 train_loss:8.1919 train_time:803240ms step_avg:181.61ms\nstep:4434/5000 train_loss:8.1190 train_time:803421ms step_avg:181.61ms\nstep:4435/5000 train_loss:8.1268 train_time:803602ms step_avg:181.61ms\nstep:4436/5000 train_loss:8.1504 train_time:803784ms step_avg:181.61ms\nstep:4437/5000 train_loss:8.1403 train_time:803965ms step_avg:181.60ms\nstep:4438/5000 train_loss:8.2914 train_time:804145ms step_avg:181.60ms\nstep:4439/5000 train_loss:8.1430 train_time:804325ms step_avg:181.60ms\nstep:4440/5000 train_loss:8.1316 train_time:804506ms step_avg:181.60ms\nstep:4441/5000 train_loss:8.0651 train_time:804687ms step_avg:181.60ms\nstep:4442/5000 train_loss:8.1132 train_time:804870ms step_avg:181.60ms\nstep:4443/5000 train_loss:8.1522 train_time:805052ms step_avg:181.60ms\nstep:4444/5000 train_loss:8.0715 train_time:805233ms step_avg:181.60ms\nstep:4445/5000 train_loss:8.0135 train_time:805414ms step_avg:181.60ms\nstep:4446/5000 train_loss:8.2560 train_time:805596ms step_avg:181.60ms\nstep:4447/5000 train_loss:8.2642 train_time:805778ms step_avg:181.60ms\nstep:4448/5000 train_loss:8.1934 train_time:805961ms step_avg:181.60ms\nstep:4449/5000 train_loss:8.1106 train_time:806142ms step_avg:181.60ms\nstep:4450/5000 train_loss:8.1495 train_time:806323ms step_avg:181.60ms\nstep:4451/5000 train_loss:8.1588 train_time:806504ms step_avg:181.60ms\nstep:4452/5000 train_loss:8.1212 train_time:806683ms step_avg:181.60ms\nstep:4453/5000 train_loss:8.1068 train_time:806864ms step_avg:181.60ms\nstep:4454/5000 train_loss:8.0267 train_time:807045ms step_avg:181.60ms\nstep:4455/5000 train_loss:8.0964 train_time:807226ms step_avg:181.60ms\nstep:4456/5000 train_loss:8.1115 train_time:807408ms step_avg:181.60ms\nstep:4457/5000 train_loss:8.0790 train_time:807590ms step_avg:181.60ms\nstep:4458/5000 train_loss:8.1251 train_time:807772ms step_avg:181.60ms\nstep:4459/5000 train_loss:8.0816 train_time:807954ms step_avg:181.60ms\nstep:4460/5000 train_loss:8.0773 train_time:808135ms step_avg:181.60ms\nstep:4461/5000 train_loss:7.9763 train_time:808318ms step_avg:181.60ms\nstep:4462/5000 train_loss:8.1714 train_time:808500ms step_avg:181.60ms\nstep:4463/5000 train_loss:8.1489 train_time:808680ms step_avg:181.60ms\nstep:4464/5000 train_loss:8.0425 train_time:808862ms step_avg:181.60ms\nstep:4465/5000 train_loss:8.1846 train_time:809044ms step_avg:181.60ms\nstep:4466/5000 train_loss:8.0961 train_time:809223ms step_avg:181.60ms\nstep:4467/5000 train_loss:8.2787 train_time:809405ms step_avg:181.60ms\nstep:4468/5000 train_loss:8.1954 train_time:809585ms step_avg:181.60ms\nstep:4469/5000 train_loss:8.1613 train_time:809765ms step_avg:181.60ms\nstep:4470/5000 train_loss:8.1260 train_time:809946ms step_avg:181.60ms\nstep:4471/5000 train_loss:8.0607 train_time:810128ms step_avg:181.60ms\nstep:4472/5000 train_loss:8.0936 train_time:810310ms step_avg:181.60ms\nstep:4473/5000 train_loss:8.1707 train_time:810492ms step_avg:181.60ms\nstep:4474/5000 train_loss:8.0686 train_time:810674ms step_avg:181.60ms\nstep:4475/5000 train_loss:8.1267 train_time:810856ms step_avg:181.60ms\nstep:4476/5000 train_loss:8.1932 train_time:811039ms step_avg:181.60ms\nstep:4477/5000 train_loss:8.4021 train_time:811222ms step_avg:181.60ms\nstep:4478/5000 train_loss:8.1186 train_time:811403ms step_avg:181.60ms\nstep:4479/5000 train_loss:7.7480 train_time:811583ms step_avg:181.60ms\nstep:4480/5000 train_loss:8.1409 train_time:811764ms step_avg:181.60ms\nstep:4481/5000 train_loss:8.1002 train_time:811945ms step_avg:181.60ms\nstep:4482/5000 train_loss:8.2492 train_time:812125ms step_avg:181.60ms\nstep:4483/5000 train_loss:8.1630 train_time:812308ms step_avg:181.60ms\nstep:4484/5000 train_loss:8.0401 train_time:812490ms step_avg:181.60ms\nstep:4485/5000 train_loss:8.1611 train_time:812671ms step_avg:181.60ms\nstep:4486/5000 train_loss:8.2131 train_time:812852ms step_avg:181.60ms\nstep:4487/5000 train_loss:8.0779 train_time:813035ms step_avg:181.60ms\nstep:4488/5000 train_loss:7.9774 train_time:813219ms step_avg:181.60ms\nstep:4489/5000 train_loss:7.9483 train_time:813401ms step_avg:181.60ms\nstep:4490/5000 train_loss:8.1076 train_time:813582ms step_avg:181.60ms\nstep:4491/5000 train_loss:8.1094 train_time:813763ms step_avg:181.60ms\nstep:4492/5000 train_loss:8.1223 train_time:813944ms step_avg:181.60ms\nstep:4493/5000 train_loss:8.1403 train_time:814123ms step_avg:181.60ms\nstep:4494/5000 train_loss:8.0791 train_time:814305ms step_avg:181.60ms\nstep:4495/5000 train_loss:8.0065 train_time:814486ms step_avg:181.60ms\nstep:4496/5000 train_loss:8.0589 train_time:814667ms step_avg:181.60ms\nstep:4497/5000 train_loss:8.0970 train_time:814850ms step_avg:181.60ms\nstep:4498/5000 train_loss:8.0498 train_time:815031ms step_avg:181.60ms\nstep:4499/5000 train_loss:8.1008 train_time:815213ms step_avg:181.60ms\nstep:4500/5000 train_loss:8.1079 train_time:815395ms step_avg:181.60ms\nstep:4501/5000 train_loss:8.1704 train_time:815578ms step_avg:181.60ms\nstep:4502/5000 train_loss:8.0804 train_time:815759ms step_avg:181.60ms\nstep:4503/5000 train_loss:8.1712 train_time:815941ms step_avg:181.60ms\nstep:4504/5000 train_loss:8.1478 train_time:816122ms step_avg:181.60ms\nstep:4505/5000 train_loss:8.0342 train_time:816304ms step_avg:181.60ms\nstep:4506/5000 train_loss:8.0872 train_time:816485ms step_avg:181.60ms\nstep:4507/5000 train_loss:8.1294 train_time:816665ms step_avg:181.60ms\nstep:4508/5000 train_loss:8.0997 train_time:816846ms step_avg:181.60ms\nstep:4509/5000 train_loss:8.0984 train_time:817026ms step_avg:181.60ms\nstep:4510/5000 train_loss:8.1871 train_time:817207ms step_avg:181.60ms\nstep:4511/5000 train_loss:8.0760 train_time:817389ms step_avg:181.60ms\nstep:4512/5000 train_loss:8.1560 train_time:817571ms step_avg:181.60ms\nstep:4513/5000 train_loss:7.9340 train_time:817753ms step_avg:181.60ms\nstep:4514/5000 train_loss:8.1009 train_time:817934ms step_avg:181.60ms\nstep:4515/5000 train_loss:8.0489 train_time:818118ms step_avg:181.60ms\nstep:4516/5000 train_loss:8.1739 train_time:818300ms step_avg:181.60ms\nstep:4517/5000 train_loss:8.0389 train_time:818481ms step_avg:181.60ms\nstep:4518/5000 train_loss:8.1046 train_time:818661ms step_avg:181.60ms\nstep:4519/5000 train_loss:8.0929 train_time:818842ms step_avg:181.60ms\nstep:4520/5000 train_loss:7.9912 train_time:819022ms step_avg:181.60ms\nstep:4521/5000 train_loss:7.8131 train_time:819203ms step_avg:181.60ms\nstep:4522/5000 train_loss:8.1553 train_time:819384ms step_avg:181.60ms\nstep:4523/5000 train_loss:8.1116 train_time:819565ms step_avg:181.60ms\nstep:4524/5000 train_loss:8.1397 train_time:819746ms step_avg:181.60ms\nstep:4525/5000 train_loss:8.0351 train_time:819927ms step_avg:181.60ms\nstep:4526/5000 train_loss:8.0628 train_time:820108ms step_avg:181.60ms\nstep:4527/5000 train_loss:8.1195 train_time:820289ms step_avg:181.60ms\nstep:4528/5000 train_loss:8.1510 train_time:820471ms step_avg:181.60ms\nstep:4529/5000 train_loss:8.1394 train_time:820654ms step_avg:181.60ms\nstep:4530/5000 train_loss:8.2186 train_time:820836ms step_avg:181.60ms\nstep:4531/5000 train_loss:8.0203 train_time:821018ms step_avg:181.60ms\nstep:4532/5000 train_loss:8.1118 train_time:821200ms step_avg:181.60ms\nstep:4533/5000 train_loss:8.1108 train_time:821380ms step_avg:181.60ms\nstep:4534/5000 train_loss:8.0032 train_time:821561ms step_avg:181.60ms\nstep:4535/5000 train_loss:8.3643 train_time:821743ms step_avg:181.60ms\nstep:4536/5000 train_loss:8.2001 train_time:821923ms step_avg:181.60ms\nstep:4537/5000 train_loss:8.1134 train_time:822104ms step_avg:181.60ms\nstep:4538/5000 train_loss:8.0241 train_time:822285ms step_avg:181.60ms\nstep:4539/5000 train_loss:8.0953 train_time:822465ms step_avg:181.60ms\nstep:4540/5000 train_loss:8.3594 train_time:822647ms step_avg:181.60ms\nstep:4541/5000 train_loss:8.0399 train_time:822828ms step_avg:181.60ms\nstep:4542/5000 train_loss:8.0881 train_time:823010ms step_avg:181.60ms\nstep:4543/5000 train_loss:8.0546 train_time:823192ms step_avg:181.60ms\nstep:4544/5000 train_loss:8.1567 train_time:823374ms step_avg:181.60ms\nstep:4545/5000 train_loss:8.1687 train_time:823556ms step_avg:181.60ms\nstep:4546/5000 train_loss:8.1625 train_time:823738ms step_avg:181.60ms\nstep:4547/5000 train_loss:8.1077 train_time:823920ms step_avg:181.60ms\nstep:4548/5000 train_loss:8.1366 train_time:824102ms step_avg:181.60ms\nstep:4549/5000 train_loss:8.2654 train_time:824282ms step_avg:181.60ms\nstep:4550/5000 train_loss:8.1422 train_time:824463ms step_avg:181.60ms\nstep:4551/5000 train_loss:7.9530 train_time:824643ms step_avg:181.60ms\nstep:4552/5000 train_loss:8.0195 train_time:824824ms step_avg:181.60ms\nstep:4553/5000 train_loss:8.0595 train_time:825005ms step_avg:181.60ms\nstep:4554/5000 train_loss:8.0893 train_time:825184ms step_avg:181.60ms\nstep:4555/5000 train_loss:8.0520 train_time:825365ms step_avg:181.60ms\nstep:4556/5000 train_loss:8.0778 train_time:825545ms step_avg:181.60ms\nstep:4557/5000 train_loss:8.2088 train_time:825725ms step_avg:181.60ms\nstep:4558/5000 train_loss:8.1207 train_time:825906ms step_avg:181.60ms\nstep:4559/5000 train_loss:8.1728 train_time:826087ms step_avg:181.60ms\nstep:4560/5000 train_loss:8.1654 train_time:826268ms step_avg:181.60ms\nstep:4561/5000 train_loss:8.0306 train_time:826449ms step_avg:181.60ms\nstep:4562/5000 train_loss:8.1519 train_time:826630ms step_avg:181.60ms\nstep:4563/5000 train_loss:8.0839 train_time:826814ms step_avg:181.60ms\nstep:4564/5000 train_loss:8.1260 train_time:826997ms step_avg:181.60ms\nstep:4565/5000 train_loss:8.0873 train_time:827178ms step_avg:181.60ms\nstep:4566/5000 train_loss:8.1995 train_time:827360ms step_avg:181.60ms\nstep:4567/5000 train_loss:8.3650 train_time:827541ms step_avg:181.60ms\nstep:4568/5000 train_loss:8.1039 train_time:827723ms step_avg:181.60ms\nstep:4569/5000 train_loss:8.0315 train_time:827905ms step_avg:181.60ms\nstep:4570/5000 train_loss:8.0774 train_time:828085ms step_avg:181.60ms\nstep:4571/5000 train_loss:8.0940 train_time:828264ms step_avg:181.60ms\nstep:4572/5000 train_loss:8.1890 train_time:828680ms step_avg:181.65ms\nstep:4573/5000 train_loss:8.0710 train_time:828858ms step_avg:181.65ms\nstep:4574/5000 train_loss:8.0197 train_time:829038ms step_avg:181.65ms\nstep:4575/5000 train_loss:8.1280 train_time:829219ms step_avg:181.65ms\nstep:4576/5000 train_loss:8.2298 train_time:829399ms step_avg:181.65ms\nstep:4577/5000 train_loss:8.1411 train_time:829588ms step_avg:181.65ms\nstep:4578/5000 train_loss:8.1925 train_time:829770ms step_avg:181.65ms\nstep:4579/5000 train_loss:8.0644 train_time:829951ms step_avg:181.65ms\nstep:4580/5000 train_loss:8.3520 train_time:830132ms step_avg:181.65ms\nstep:4581/5000 train_loss:8.0529 train_time:830313ms step_avg:181.65ms\nstep:4582/5000 train_loss:8.1188 train_time:830496ms step_avg:181.65ms\nstep:4583/5000 train_loss:8.1164 train_time:830680ms step_avg:181.65ms\nstep:4584/5000 train_loss:8.2204 train_time:830862ms step_avg:181.65ms\nstep:4585/5000 train_loss:8.0258 train_time:831043ms step_avg:181.65ms\nstep:4586/5000 train_loss:8.1558 train_time:831223ms step_avg:181.65ms\nstep:4587/5000 train_loss:8.0554 train_time:831402ms step_avg:181.65ms\nstep:4588/5000 train_loss:8.0766 train_time:831584ms step_avg:181.65ms\nstep:4589/5000 train_loss:8.1340 train_time:831765ms step_avg:181.65ms\nstep:4590/5000 train_loss:8.1896 train_time:831945ms step_avg:181.65ms\nstep:4591/5000 train_loss:8.1232 train_time:832125ms step_avg:181.65ms\nstep:4592/5000 train_loss:8.0040 train_time:832306ms step_avg:181.65ms\nstep:4593/5000 train_loss:8.0821 train_time:832487ms step_avg:181.65ms\nstep:4594/5000 train_loss:8.0867 train_time:832668ms step_avg:181.65ms\nstep:4595/5000 train_loss:8.2771 train_time:832850ms step_avg:181.65ms\nstep:4596/5000 train_loss:8.1966 train_time:833033ms step_avg:181.65ms\nstep:4597/5000 train_loss:8.0838 train_time:833215ms step_avg:181.65ms\nstep:4598/5000 train_loss:8.0858 train_time:833397ms step_avg:181.65ms\nstep:4599/5000 train_loss:8.2933 train_time:833577ms step_avg:181.65ms\nstep:4600/5000 train_loss:8.1121 train_time:833759ms step_avg:181.65ms\nstep:4601/5000 train_loss:8.1179 train_time:833941ms step_avg:181.65ms\nstep:4602/5000 train_loss:8.1352 train_time:834122ms step_avg:181.65ms\nstep:4603/5000 train_loss:8.1889 train_time:834305ms step_avg:181.65ms\nstep:4604/5000 train_loss:8.1368 train_time:834486ms step_avg:181.65ms\nstep:4605/5000 train_loss:8.1904 train_time:834666ms step_avg:181.65ms\nstep:4606/5000 train_loss:8.1383 train_time:834845ms step_avg:181.65ms\nstep:4607/5000 train_loss:8.1338 train_time:835025ms step_avg:181.65ms\nstep:4608/5000 train_loss:8.0599 train_time:835206ms step_avg:181.65ms\nstep:4609/5000 train_loss:8.0594 train_time:835388ms step_avg:181.65ms\nstep:4610/5000 train_loss:8.1111 train_time:835569ms step_avg:181.65ms\nstep:4611/5000 train_loss:8.1791 train_time:835751ms step_avg:181.65ms\nstep:4612/5000 train_loss:8.0420 train_time:835933ms step_avg:181.65ms\nstep:4613/5000 train_loss:8.1526 train_time:836116ms step_avg:181.65ms\nstep:4614/5000 train_loss:8.1293 train_time:836297ms step_avg:181.65ms\nstep:4615/5000 train_loss:8.1469 train_time:836478ms step_avg:181.65ms\nstep:4616/5000 train_loss:8.2408 train_time:836659ms step_avg:181.65ms\nstep:4617/5000 train_loss:8.1533 train_time:836840ms step_avg:181.65ms\nstep:4618/5000 train_loss:8.1664 train_time:837021ms step_avg:181.65ms\nstep:4619/5000 train_loss:8.1618 train_time:837203ms step_avg:181.65ms\nstep:4620/5000 train_loss:8.1865 train_time:837383ms step_avg:181.64ms\nstep:4621/5000 train_loss:8.0888 train_time:837564ms step_avg:181.64ms\nstep:4622/5000 train_loss:8.0716 train_time:837744ms step_avg:181.64ms\nstep:4623/5000 train_loss:8.0928 train_time:837923ms step_avg:181.64ms\nstep:4624/5000 train_loss:8.0707 train_time:838103ms step_avg:181.64ms\nstep:4625/5000 train_loss:8.1795 train_time:838284ms step_avg:181.64ms\nstep:4626/5000 train_loss:8.0883 train_time:838464ms step_avg:181.64ms\nstep:4627/5000 train_loss:8.0585 train_time:838645ms step_avg:181.64ms\nstep:4628/5000 train_loss:8.0990 train_time:838826ms step_avg:181.64ms\nstep:4629/5000 train_loss:8.1369 train_time:839007ms step_avg:181.64ms\nstep:4630/5000 train_loss:8.1749 train_time:839187ms step_avg:181.64ms\nstep:4631/5000 train_loss:8.1704 train_time:839367ms step_avg:181.64ms\nstep:4632/5000 train_loss:8.1037 train_time:839548ms step_avg:181.64ms\nstep:4633/5000 train_loss:8.0939 train_time:839729ms step_avg:181.64ms\nstep:4634/5000 train_loss:8.1598 train_time:839912ms step_avg:181.64ms\nstep:4635/5000 train_loss:8.1443 train_time:840094ms step_avg:181.64ms\nstep:4636/5000 train_loss:8.0745 train_time:840276ms step_avg:181.64ms\nstep:4637/5000 train_loss:8.0307 train_time:840458ms step_avg:181.64ms\nstep:4638/5000 train_loss:8.1915 train_time:840639ms step_avg:181.64ms\nstep:4639/5000 train_loss:8.0636 train_time:840820ms step_avg:181.64ms\nstep:4640/5000 train_loss:7.9471 train_time:841001ms step_avg:181.64ms\nstep:4641/5000 train_loss:8.1267 train_time:841181ms step_avg:181.64ms\nstep:4642/5000 train_loss:8.2372 train_time:841361ms step_avg:181.64ms\nstep:4643/5000 train_loss:8.1744 train_time:841543ms step_avg:181.64ms\nstep:4644/5000 train_loss:8.1503 train_time:841723ms step_avg:181.64ms\nstep:4645/5000 train_loss:8.1755 train_time:841903ms step_avg:181.64ms\nstep:4646/5000 train_loss:8.0606 train_time:842084ms step_avg:181.64ms\nstep:4647/5000 train_loss:8.1455 train_time:842264ms step_avg:181.64ms\nstep:4648/5000 train_loss:8.0649 train_time:842444ms step_avg:181.64ms\nstep:4649/5000 train_loss:8.0704 train_time:842624ms step_avg:181.64ms\nstep:4650/5000 train_loss:8.0584 train_time:842805ms step_avg:181.64ms\nstep:4651/5000 train_loss:8.1226 train_time:842986ms step_avg:181.64ms\nstep:4652/5000 train_loss:8.0945 train_time:843167ms step_avg:181.64ms\nstep:4653/5000 train_loss:8.2154 train_time:843350ms step_avg:181.64ms\nstep:4654/5000 train_loss:7.8925 train_time:843531ms step_avg:181.64ms\nstep:4655/5000 train_loss:8.0883 train_time:843714ms step_avg:181.64ms\nstep:4656/5000 train_loss:7.9362 train_time:843896ms step_avg:181.64ms\nstep:4657/5000 train_loss:8.1194 train_time:844077ms step_avg:181.64ms\nstep:4658/5000 train_loss:8.2145 train_time:844258ms step_avg:181.64ms\nstep:4659/5000 train_loss:8.0174 train_time:844440ms step_avg:181.64ms\nstep:4660/5000 train_loss:8.0702 train_time:844622ms step_avg:181.64ms\nstep:4661/5000 train_loss:8.1715 train_time:844803ms step_avg:181.64ms\nstep:4662/5000 train_loss:8.1997 train_time:844984ms step_avg:181.64ms\nstep:4663/5000 train_loss:7.9576 train_time:845164ms step_avg:181.64ms\nstep:4664/5000 train_loss:8.1820 train_time:845344ms step_avg:181.64ms\nstep:4665/5000 train_loss:8.0165 train_time:845525ms step_avg:181.64ms\nstep:4666/5000 train_loss:8.0971 train_time:845707ms step_avg:181.64ms\nstep:4667/5000 train_loss:8.0997 train_time:845888ms step_avg:181.64ms\nstep:4668/5000 train_loss:8.1954 train_time:846071ms step_avg:181.64ms\nstep:4669/5000 train_loss:8.1261 train_time:846254ms step_avg:181.64ms\nstep:4670/5000 train_loss:8.2754 train_time:846436ms step_avg:181.64ms\nstep:4671/5000 train_loss:8.1476 train_time:846618ms step_avg:181.64ms\nstep:4672/5000 train_loss:8.1544 train_time:846799ms step_avg:181.64ms\nstep:4673/5000 train_loss:8.1528 train_time:846980ms step_avg:181.64ms\nstep:4674/5000 train_loss:8.0703 train_time:847162ms step_avg:181.64ms\nstep:4675/5000 train_loss:8.1506 train_time:847343ms step_avg:181.64ms\nstep:4676/5000 train_loss:8.1448 train_time:847523ms step_avg:181.64ms\nstep:4677/5000 train_loss:8.1771 train_time:847703ms step_avg:181.64ms\nstep:4678/5000 train_loss:8.0137 train_time:847883ms step_avg:181.64ms\nstep:4679/5000 train_loss:8.1340 train_time:848063ms step_avg:181.64ms\nstep:4680/5000 train_loss:8.1266 train_time:848244ms step_avg:181.64ms\nstep:4681/5000 train_loss:8.0571 train_time:848424ms step_avg:181.64ms\nstep:4682/5000 train_loss:8.1519 train_time:848605ms step_avg:181.64ms\nstep:4683/5000 train_loss:8.1490 train_time:848786ms step_avg:181.64ms\nstep:4684/5000 train_loss:8.0732 train_time:848966ms step_avg:181.64ms\nstep:4685/5000 train_loss:8.2598 train_time:849147ms step_avg:181.64ms\nstep:4686/5000 train_loss:8.0836 train_time:849326ms step_avg:181.64ms\nstep:4687/5000 train_loss:8.0733 train_time:849507ms step_avg:181.64ms\nstep:4688/5000 train_loss:8.1768 train_time:849689ms step_avg:181.64ms\nstep:4689/5000 train_loss:8.1971 train_time:849873ms step_avg:181.64ms\nstep:4690/5000 train_loss:8.1411 train_time:850055ms step_avg:181.64ms\nstep:4691/5000 train_loss:8.1687 train_time:850237ms step_avg:181.64ms\nstep:4692/5000 train_loss:8.2589 train_time:850419ms step_avg:181.64ms\nstep:4693/5000 train_loss:8.1705 train_time:850601ms step_avg:181.64ms\nstep:4694/5000 train_loss:8.0659 train_time:850781ms step_avg:181.64ms\nstep:4695/5000 train_loss:8.0438 train_time:850962ms step_avg:181.64ms\nstep:4696/5000 train_loss:8.1495 train_time:851143ms step_avg:181.64ms\nstep:4697/5000 train_loss:7.8950 train_time:851324ms step_avg:181.64ms\nstep:4698/5000 train_loss:8.0259 train_time:851506ms step_avg:181.64ms\nstep:4699/5000 train_loss:8.1612 train_time:851685ms step_avg:181.63ms\nstep:4700/5000 train_loss:8.1399 train_time:851865ms step_avg:181.63ms\nstep:4701/5000 train_loss:8.1139 train_time:852046ms step_avg:181.63ms\nstep:4702/5000 train_loss:8.1545 train_time:852227ms step_avg:181.63ms\nstep:4703/5000 train_loss:8.1091 train_time:852409ms step_avg:181.63ms\nstep:4704/5000 train_loss:8.1574 train_time:852590ms step_avg:181.63ms\nstep:4705/5000 train_loss:8.1637 train_time:852772ms step_avg:181.63ms\nstep:4706/5000 train_loss:8.1230 train_time:852954ms step_avg:181.63ms\nstep:4707/5000 train_loss:8.1120 train_time:853137ms step_avg:181.63ms\nstep:4708/5000 train_loss:8.0179 train_time:853319ms step_avg:181.63ms\nstep:4709/5000 train_loss:8.1201 train_time:853500ms step_avg:181.63ms\nstep:4710/5000 train_loss:8.1460 train_time:853681ms step_avg:181.63ms\nstep:4711/5000 train_loss:8.0514 train_time:853861ms step_avg:181.63ms\nstep:4712/5000 train_loss:7.8767 train_time:854042ms step_avg:181.63ms\nstep:4713/5000 train_loss:8.0908 train_time:854223ms step_avg:181.63ms\nstep:4714/5000 train_loss:8.2085 train_time:854405ms step_avg:181.63ms\nstep:4715/5000 train_loss:8.1368 train_time:854585ms step_avg:181.63ms\nstep:4716/5000 train_loss:8.2127 train_time:854765ms step_avg:181.63ms\nstep:4717/5000 train_loss:8.1650 train_time:854946ms step_avg:181.63ms\nstep:4718/5000 train_loss:8.0892 train_time:855126ms step_avg:181.63ms\nstep:4719/5000 train_loss:8.1746 train_time:855308ms step_avg:181.63ms\nstep:4720/5000 train_loss:8.1148 train_time:855489ms step_avg:181.63ms\nstep:4721/5000 train_loss:8.2719 train_time:855670ms step_avg:181.63ms\nstep:4722/5000 train_loss:8.0019 train_time:855852ms step_avg:181.63ms\nstep:4723/5000 train_loss:8.0831 train_time:856033ms step_avg:181.63ms\nstep:4724/5000 train_loss:8.2080 train_time:856214ms step_avg:181.63ms\nstep:4725/5000 train_loss:8.0815 train_time:856395ms step_avg:181.63ms\nstep:4726/5000 train_loss:8.1412 train_time:856578ms step_avg:181.63ms\nstep:4727/5000 train_loss:8.1399 train_time:856759ms step_avg:181.63ms\nstep:4728/5000 train_loss:8.2164 train_time:856940ms step_avg:181.63ms\nstep:4729/5000 train_loss:8.1082 train_time:857122ms step_avg:181.63ms\nstep:4730/5000 train_loss:8.1518 train_time:857303ms step_avg:181.63ms\nstep:4731/5000 train_loss:8.1854 train_time:857483ms step_avg:181.63ms\nstep:4732/5000 train_loss:8.0293 train_time:857663ms step_avg:181.63ms\nstep:4733/5000 train_loss:8.0897 train_time:857844ms step_avg:181.63ms\nstep:4734/5000 train_loss:8.0723 train_time:858025ms step_avg:181.63ms\nstep:4735/5000 train_loss:8.1044 train_time:858206ms step_avg:181.63ms\nstep:4736/5000 train_loss:8.0486 train_time:858387ms step_avg:181.63ms\nstep:4737/5000 train_loss:8.1120 train_time:858567ms step_avg:181.63ms\nstep:4738/5000 train_loss:8.1869 train_time:858747ms step_avg:181.63ms\nstep:4739/5000 train_loss:8.1330 train_time:858929ms step_avg:181.63ms\nstep:4740/5000 train_loss:8.1851 train_time:859111ms step_avg:181.63ms\nstep:4741/5000 train_loss:7.9577 train_time:859292ms step_avg:181.63ms\nstep:4742/5000 train_loss:8.1201 train_time:859475ms step_avg:181.63ms\nstep:4743/5000 train_loss:8.1213 train_time:859658ms step_avg:181.63ms\nstep:4744/5000 train_loss:8.0913 train_time:859839ms step_avg:181.63ms\nstep:4745/5000 train_loss:7.8976 train_time:860021ms step_avg:181.63ms\nstep:4746/5000 train_loss:8.1125 train_time:860201ms step_avg:181.63ms\nstep:4747/5000 train_loss:8.1579 train_time:860381ms step_avg:181.63ms\nstep:4748/5000 train_loss:8.0278 train_time:860561ms step_avg:181.63ms\nstep:4749/5000 train_loss:8.0130 train_time:860743ms step_avg:181.63ms\nstep:4750/5000 train_loss:8.0994 train_time:860923ms step_avg:181.63ms\nstep:4751/5000 train_loss:8.0273 train_time:861104ms step_avg:181.63ms\nstep:4752/5000 train_loss:8.0867 train_time:861285ms step_avg:181.63ms\nstep:4753/5000 train_loss:8.0053 train_time:861469ms step_avg:181.63ms\nstep:4754/5000 train_loss:8.0385 train_time:861647ms step_avg:181.63ms\nstep:4755/5000 train_loss:8.2306 train_time:861828ms step_avg:181.63ms\nstep:4756/5000 train_loss:8.1562 train_time:862011ms step_avg:181.63ms\nstep:4757/5000 train_loss:8.0622 train_time:862193ms step_avg:181.63ms\nstep:4758/5000 train_loss:7.9965 train_time:862375ms step_avg:181.63ms\nstep:4759/5000 train_loss:8.1355 train_time:862558ms step_avg:181.63ms\nstep:4760/5000 train_loss:8.0386 train_time:862739ms step_avg:181.63ms\nstep:4761/5000 train_loss:8.1143 train_time:862921ms step_avg:181.63ms\nstep:4762/5000 train_loss:8.1207 train_time:863102ms step_avg:181.63ms\nstep:4763/5000 train_loss:8.1637 train_time:863283ms step_avg:181.63ms\nstep:4764/5000 train_loss:8.1303 train_time:863462ms step_avg:181.63ms\nstep:4765/5000 train_loss:8.0081 train_time:863643ms step_avg:181.63ms\nstep:4766/5000 train_loss:8.0845 train_time:863824ms step_avg:181.63ms\nstep:4767/5000 train_loss:8.2043 train_time:864004ms step_avg:181.63ms\nstep:4768/5000 train_loss:8.1655 train_time:864184ms step_avg:181.63ms\nstep:4769/5000 train_loss:8.1682 train_time:864365ms step_avg:181.63ms\nstep:4770/5000 train_loss:8.1826 train_time:864545ms step_avg:181.63ms\nstep:4771/5000 train_loss:8.0935 train_time:864725ms step_avg:181.63ms\nstep:4772/5000 train_loss:8.2815 train_time:864907ms step_avg:181.63ms\nstep:4773/5000 train_loss:8.1162 train_time:865088ms step_avg:181.63ms\nstep:4774/5000 train_loss:8.1372 train_time:865269ms step_avg:181.63ms\nstep:4775/5000 train_loss:8.2369 train_time:865451ms step_avg:181.63ms\nstep:4776/5000 train_loss:8.2708 train_time:865632ms step_avg:181.63ms\nstep:4777/5000 train_loss:8.1733 train_time:865815ms step_avg:181.63ms\nstep:4778/5000 train_loss:8.1398 train_time:865996ms step_avg:181.63ms\nstep:4779/5000 train_loss:8.0847 train_time:866177ms step_avg:181.63ms\nstep:4780/5000 train_loss:8.0181 train_time:866359ms step_avg:181.63ms\nstep:4781/5000 train_loss:8.2754 train_time:866540ms step_avg:181.63ms\nstep:4782/5000 train_loss:8.1547 train_time:866721ms step_avg:181.63ms\nstep:4783/5000 train_loss:8.0806 train_time:866903ms step_avg:181.63ms\nstep:4784/5000 train_loss:8.1614 train_time:867085ms step_avg:181.63ms\nstep:4785/5000 train_loss:8.1451 train_time:867265ms step_avg:181.63ms\nstep:4786/5000 train_loss:8.2060 train_time:867447ms step_avg:181.63ms\nstep:4787/5000 train_loss:8.0463 train_time:867627ms step_avg:181.63ms\nstep:4788/5000 train_loss:8.0403 train_time:867810ms step_avg:181.63ms\nstep:4789/5000 train_loss:7.9596 train_time:867992ms step_avg:181.63ms\nstep:4790/5000 train_loss:8.0867 train_time:868174ms step_avg:181.63ms\nstep:4791/5000 train_loss:8.0434 train_time:868357ms step_avg:181.63ms\nstep:4792/5000 train_loss:7.9758 train_time:868540ms step_avg:181.63ms\nstep:4793/5000 train_loss:8.0459 train_time:868721ms step_avg:181.63ms\nstep:4794/5000 train_loss:8.0939 train_time:868902ms step_avg:181.63ms\nstep:4795/5000 train_loss:8.0656 train_time:869082ms step_avg:181.63ms\nstep:4796/5000 train_loss:8.1931 train_time:869263ms step_avg:181.63ms\nstep:4797/5000 train_loss:8.1186 train_time:869444ms step_avg:181.63ms\nstep:4798/5000 train_loss:8.0292 train_time:869625ms step_avg:181.63ms\nstep:4799/5000 train_loss:8.0162 train_time:869806ms step_avg:181.63ms\nstep:4800/5000 train_loss:8.1189 train_time:869987ms step_avg:181.63ms\nstep:4801/5000 train_loss:8.0747 train_time:870167ms step_avg:181.63ms\nstep:4802/5000 train_loss:7.9201 train_time:870349ms step_avg:181.63ms\nstep:4803/5000 train_loss:8.0108 train_time:870530ms step_avg:181.63ms\nstep:4804/5000 train_loss:8.2186 train_time:870713ms step_avg:181.63ms\nstep:4805/5000 train_loss:8.2126 train_time:870896ms step_avg:181.63ms\nstep:4806/5000 train_loss:8.1258 train_time:871077ms step_avg:181.63ms\nstep:4807/5000 train_loss:8.1346 train_time:871258ms step_avg:181.63ms\nstep:4808/5000 train_loss:8.0673 train_time:871439ms step_avg:181.63ms\nstep:4809/5000 train_loss:8.0369 train_time:871621ms step_avg:181.63ms\nstep:4810/5000 train_loss:7.9983 train_time:871803ms step_avg:181.63ms\nstep:4811/5000 train_loss:8.0667 train_time:871983ms step_avg:181.63ms\nstep:4812/5000 train_loss:8.0739 train_time:872164ms step_avg:181.63ms\nstep:4813/5000 train_loss:8.1797 train_time:872344ms step_avg:181.62ms\nstep:4814/5000 train_loss:8.1465 train_time:872524ms step_avg:181.62ms\nstep:4815/5000 train_loss:8.1060 train_time:872707ms step_avg:181.62ms\nstep:4816/5000 train_loss:8.0624 train_time:872887ms step_avg:181.62ms\nstep:4817/5000 train_loss:8.0235 train_time:873068ms step_avg:181.62ms\nstep:4818/5000 train_loss:7.8666 train_time:873250ms step_avg:181.62ms\nstep:4819/5000 train_loss:8.0531 train_time:873431ms step_avg:181.62ms\nstep:4820/5000 train_loss:8.1394 train_time:873613ms step_avg:181.62ms\nstep:4821/5000 train_loss:8.0005 train_time:873795ms step_avg:181.62ms\nstep:4822/5000 train_loss:8.1053 train_time:873977ms step_avg:181.62ms\nstep:4823/5000 train_loss:8.1178 train_time:874160ms step_avg:181.62ms\nstep:4824/5000 train_loss:8.1864 train_time:874340ms step_avg:181.62ms\nstep:4825/5000 train_loss:8.0552 train_time:874521ms step_avg:181.62ms\nstep:4826/5000 train_loss:8.1951 train_time:874704ms step_avg:181.62ms\nstep:4827/5000 train_loss:7.9405 train_time:874884ms step_avg:181.62ms\nstep:4828/5000 train_loss:8.2175 train_time:875064ms step_avg:181.62ms\nstep:4829/5000 train_loss:8.2839 train_time:875244ms step_avg:181.62ms\nstep:4830/5000 train_loss:8.0566 train_time:875424ms step_avg:181.62ms\nstep:4831/5000 train_loss:8.1082 train_time:875605ms step_avg:181.62ms\nstep:4832/5000 train_loss:8.1510 train_time:875787ms step_avg:181.62ms\nstep:4833/5000 train_loss:8.1110 train_time:875968ms step_avg:181.62ms\nstep:4834/5000 train_loss:8.1468 train_time:876150ms step_avg:181.62ms\nstep:4835/5000 train_loss:8.0309 train_time:876331ms step_avg:181.62ms\nstep:4836/5000 train_loss:8.0700 train_time:876514ms step_avg:181.62ms\nstep:4837/5000 train_loss:8.0107 train_time:876697ms step_avg:181.62ms\nstep:4838/5000 train_loss:8.1937 train_time:876880ms step_avg:181.62ms\nstep:4839/5000 train_loss:8.1094 train_time:877061ms step_avg:181.62ms\nstep:4840/5000 train_loss:8.0643 train_time:877242ms step_avg:181.62ms\nstep:4841/5000 train_loss:8.1084 train_time:877423ms step_avg:181.62ms\nstep:4842/5000 train_loss:8.1001 train_time:877604ms step_avg:181.62ms\nstep:4843/5000 train_loss:8.1359 train_time:877785ms step_avg:181.62ms\nstep:4844/5000 train_loss:8.1147 train_time:877965ms step_avg:181.62ms\nstep:4845/5000 train_loss:8.1113 train_time:878146ms step_avg:181.62ms\nstep:4846/5000 train_loss:8.0286 train_time:878327ms step_avg:181.62ms\nstep:4847/5000 train_loss:8.0346 train_time:878509ms step_avg:181.62ms\nstep:4848/5000 train_loss:8.0616 train_time:878691ms step_avg:181.62ms\nstep:4849/5000 train_loss:8.0054 train_time:878872ms step_avg:181.62ms\nstep:4850/5000 train_loss:8.0951 train_time:879055ms step_avg:181.62ms\nstep:4851/5000 train_loss:7.9376 train_time:879237ms step_avg:181.62ms\nstep:4852/5000 train_loss:8.0814 train_time:879419ms step_avg:181.62ms\nstep:4853/5000 train_loss:8.1026 train_time:879600ms step_avg:181.62ms\nstep:4854/5000 train_loss:8.0911 train_time:879781ms step_avg:181.62ms\nstep:4855/5000 train_loss:8.1209 train_time:879962ms step_avg:181.62ms\nstep:4856/5000 train_loss:8.2633 train_time:880144ms step_avg:181.62ms\nstep:4857/5000 train_loss:8.2217 train_time:880325ms step_avg:181.62ms\nstep:4858/5000 train_loss:8.1380 train_time:880507ms step_avg:181.62ms\nstep:4859/5000 train_loss:8.0745 train_time:880688ms step_avg:181.62ms\nstep:4860/5000 train_loss:8.1599 train_time:880870ms step_avg:181.62ms\nstep:4861/5000 train_loss:8.0682 train_time:881052ms step_avg:181.62ms\nstep:4862/5000 train_loss:8.1715 train_time:881234ms step_avg:181.62ms\nstep:4863/5000 train_loss:8.0911 train_time:881417ms step_avg:181.62ms\nstep:4864/5000 train_loss:8.0937 train_time:881598ms step_avg:181.62ms\nstep:4865/5000 train_loss:8.1505 train_time:881779ms step_avg:181.62ms\nstep:4866/5000 train_loss:8.3176 train_time:881960ms step_avg:181.62ms\nstep:4867/5000 train_loss:8.0922 train_time:882141ms step_avg:181.62ms\nstep:4868/5000 train_loss:8.1825 train_time:882323ms step_avg:181.62ms\nstep:4869/5000 train_loss:8.0866 train_time:882505ms step_avg:181.62ms\nstep:4870/5000 train_loss:8.1425 train_time:882686ms step_avg:181.62ms\nstep:4871/5000 train_loss:8.1283 train_time:882866ms step_avg:181.62ms\nstep:4872/5000 train_loss:8.0666 train_time:883046ms step_avg:181.62ms\nstep:4873/5000 train_loss:8.2240 train_time:883227ms step_avg:181.62ms\nstep:4874/5000 train_loss:8.0773 train_time:883409ms step_avg:181.62ms\nstep:4875/5000 train_loss:8.0214 train_time:883590ms step_avg:181.62ms\nstep:4876/5000 train_loss:8.1982 train_time:883772ms step_avg:181.62ms\nstep:4877/5000 train_loss:8.1298 train_time:883955ms step_avg:181.62ms\nstep:4878/5000 train_loss:8.1763 train_time:884138ms step_avg:181.62ms\nstep:4879/5000 train_loss:8.1792 train_time:884321ms step_avg:181.62ms\nstep:4880/5000 train_loss:8.1141 train_time:884503ms step_avg:181.62ms\nstep:4881/5000 train_loss:8.0395 train_time:884684ms step_avg:181.62ms\nstep:4882/5000 train_loss:8.1198 train_time:884864ms step_avg:181.62ms\nstep:4883/5000 train_loss:8.0994 train_time:885045ms step_avg:181.62ms\nstep:4884/5000 train_loss:8.1770 train_time:885226ms step_avg:181.62ms\nstep:4885/5000 train_loss:8.1269 train_time:885409ms step_avg:181.62ms\nstep:4886/5000 train_loss:8.1357 train_time:885589ms step_avg:181.62ms\nstep:4887/5000 train_loss:8.0652 train_time:885772ms step_avg:181.62ms\nstep:4888/5000 train_loss:8.1283 train_time:885955ms step_avg:181.62ms\nstep:4889/5000 train_loss:8.1653 train_time:886137ms step_avg:181.62ms\nstep:4890/5000 train_loss:8.1453 train_time:886319ms step_avg:181.62ms\nstep:4891/5000 train_loss:8.1100 train_time:886502ms step_avg:181.62ms\nstep:4892/5000 train_loss:8.2643 train_time:886682ms step_avg:181.62ms\nstep:4893/5000 train_loss:8.1322 train_time:886863ms step_avg:181.62ms\nstep:4894/5000 train_loss:8.1407 train_time:887044ms step_avg:181.62ms\nstep:4895/5000 train_loss:8.0857 train_time:887223ms step_avg:181.62ms\nstep:4896/5000 train_loss:8.2174 train_time:887406ms step_avg:181.62ms\nstep:4897/5000 train_loss:8.1174 train_time:887587ms step_avg:181.62ms\nstep:4898/5000 train_loss:8.2108 train_time:887767ms step_avg:181.62ms\nstep:4899/5000 train_loss:8.0460 train_time:887949ms step_avg:181.62ms\nstep:4900/5000 train_loss:8.0459 train_time:888130ms step_avg:181.62ms\nstep:4901/5000 train_loss:8.2557 train_time:888314ms step_avg:181.62ms\nstep:4902/5000 train_loss:7.9848 train_time:888496ms step_avg:181.62ms\nstep:4903/5000 train_loss:8.1764 train_time:888677ms step_avg:181.62ms\nstep:4904/5000 train_loss:8.2612 train_time:888859ms step_avg:181.62ms\nstep:4905/5000 train_loss:8.2652 train_time:889040ms step_avg:181.62ms\nstep:4906/5000 train_loss:8.1926 train_time:889222ms step_avg:181.62ms\nstep:4907/5000 train_loss:8.4037 train_time:889404ms step_avg:181.62ms\nstep:4908/5000 train_loss:8.1916 train_time:889586ms step_avg:181.62ms\nstep:4909/5000 train_loss:8.3914 train_time:889767ms step_avg:181.62ms\nstep:4910/5000 train_loss:8.0508 train_time:889948ms step_avg:181.62ms\nstep:4911/5000 train_loss:8.0599 train_time:890128ms step_avg:181.62ms\nstep:4912/5000 train_loss:8.0602 train_time:890310ms step_avg:181.62ms\nstep:4913/5000 train_loss:8.1195 train_time:890492ms step_avg:181.62ms\nstep:4914/5000 train_loss:8.0771 train_time:890675ms step_avg:181.62ms\nstep:4915/5000 train_loss:8.1581 train_time:890858ms step_avg:181.62ms\nstep:4916/5000 train_loss:8.2390 train_time:891039ms step_avg:181.62ms\nstep:4917/5000 train_loss:8.2083 train_time:891221ms step_avg:181.62ms\nstep:4918/5000 train_loss:7.9788 train_time:891403ms step_avg:181.62ms\nstep:4919/5000 train_loss:8.1313 train_time:891585ms step_avg:181.62ms\nstep:4920/5000 train_loss:8.0740 train_time:891765ms step_avg:181.62ms\nstep:4921/5000 train_loss:8.0995 train_time:891945ms step_avg:181.62ms\nstep:4922/5000 train_loss:8.1166 train_time:892127ms step_avg:181.62ms\nstep:4923/5000 train_loss:8.0848 train_time:892309ms step_avg:181.62ms\nstep:4924/5000 train_loss:8.0814 train_time:892491ms step_avg:181.62ms\nstep:4925/5000 train_loss:8.0555 train_time:892673ms step_avg:181.62ms\nstep:4926/5000 train_loss:8.0252 train_time:892855ms step_avg:181.62ms\nstep:4927/5000 train_loss:8.1342 train_time:893036ms step_avg:181.62ms\nstep:4928/5000 train_loss:8.1150 train_time:893219ms step_avg:181.62ms\nstep:4929/5000 train_loss:7.9892 train_time:893401ms step_avg:181.62ms\nstep:4930/5000 train_loss:8.1240 train_time:893582ms step_avg:181.62ms\nstep:4931/5000 train_loss:8.0475 train_time:893763ms step_avg:181.62ms\nstep:4932/5000 train_loss:8.0708 train_time:893943ms step_avg:181.62ms\nstep:4933/5000 train_loss:8.1263 train_time:894123ms step_avg:181.62ms\nstep:4934/5000 train_loss:8.1266 train_time:894304ms step_avg:181.62ms\nstep:4935/5000 train_loss:8.2276 train_time:894486ms step_avg:181.62ms\nstep:4936/5000 train_loss:8.2080 train_time:894667ms step_avg:181.62ms\nstep:4937/5000 train_loss:8.0333 train_time:894848ms step_avg:181.62ms\nstep:4938/5000 train_loss:8.1868 train_time:895028ms step_avg:181.62ms\nstep:4939/5000 train_loss:8.1778 train_time:895210ms step_avg:181.62ms\nstep:4940/5000 train_loss:8.0095 train_time:895393ms step_avg:181.62ms\nstep:4941/5000 train_loss:8.0731 train_time:895574ms step_avg:181.62ms\nstep:4942/5000 train_loss:8.0694 train_time:895757ms step_avg:181.62ms\nstep:4943/5000 train_loss:7.9693 train_time:895939ms step_avg:181.62ms\nstep:4944/5000 train_loss:8.1507 train_time:896121ms step_avg:181.62ms\nstep:4945/5000 train_loss:8.1387 train_time:896304ms step_avg:181.62ms\nstep:4946/5000 train_loss:8.1655 train_time:896484ms step_avg:181.62ms\nstep:4947/5000 train_loss:8.0365 train_time:896665ms step_avg:181.62ms\nstep:4948/5000 train_loss:8.0636 train_time:896846ms step_avg:181.62ms\nstep:4949/5000 train_loss:8.1289 train_time:897027ms step_avg:181.62ms\nstep:4950/5000 train_loss:8.1679 train_time:897208ms step_avg:181.62ms\nstep:4951/5000 train_loss:8.1136 train_time:897390ms step_avg:181.62ms\nstep:4952/5000 train_loss:8.1639 train_time:897573ms step_avg:181.62ms\nstep:4953/5000 train_loss:8.0133 train_time:897756ms step_avg:181.62ms\nstep:4954/5000 train_loss:8.2024 train_time:897939ms step_avg:181.62ms\nstep:4955/5000 train_loss:8.2180 train_time:898120ms step_avg:181.62ms\nstep:4956/5000 train_loss:8.0905 train_time:898301ms step_avg:181.62ms\nstep:4957/5000 train_loss:8.0662 train_time:898482ms step_avg:181.62ms\nstep:4958/5000 train_loss:8.0551 train_time:898662ms step_avg:181.62ms\nstep:4959/5000 train_loss:8.2288 train_time:898843ms step_avg:181.62ms\nstep:4960/5000 train_loss:8.0715 train_time:899024ms step_avg:181.62ms\nstep:4961/5000 train_loss:7.9317 train_time:899205ms step_avg:181.62ms\nstep:4962/5000 train_loss:8.2001 train_time:899386ms step_avg:181.62ms\nstep:4963/5000 train_loss:8.0911 train_time:899567ms step_avg:181.62ms\nstep:4964/5000 train_loss:8.0945 train_time:899749ms step_avg:181.62ms\nstep:4965/5000 train_loss:8.2197 train_time:899929ms step_avg:181.62ms\nstep:4966/5000 train_loss:8.1207 train_time:900113ms step_avg:181.62ms\nstep:4967/5000 train_loss:8.0578 train_time:900295ms step_avg:181.62ms\nstep:4968/5000 train_loss:8.1570 train_time:900477ms step_avg:181.62ms\nstep:4969/5000 train_loss:8.0751 train_time:900658ms step_avg:181.62ms\nstep:4970/5000 train_loss:8.0983 train_time:900839ms step_avg:181.62ms\nstep:4971/5000 train_loss:8.1312 train_time:901021ms step_avg:181.62ms\nstep:4972/5000 train_loss:7.9635 train_time:901203ms step_avg:181.62ms\nstep:4973/5000 train_loss:8.1998 train_time:901383ms step_avg:181.62ms\nstep:4974/5000 train_loss:8.0866 train_time:901563ms step_avg:181.62ms\nstep:4975/5000 train_loss:8.2247 train_time:901744ms step_avg:181.62ms\nstep:4976/5000 train_loss:8.2169 train_time:901924ms step_avg:181.62ms\nstep:4977/5000 train_loss:8.0555 train_time:902105ms step_avg:181.62ms\nstep:4978/5000 train_loss:7.9696 train_time:902287ms step_avg:181.62ms\nstep:4979/5000 train_loss:8.0783 train_time:902467ms step_avg:181.62ms\nstep:4980/5000 train_loss:8.1825 train_time:902648ms step_avg:181.62ms\nstep:4981/5000 train_loss:8.3799 train_time:902829ms step_avg:181.62ms\nstep:4982/5000 train_loss:8.1315 train_time:903012ms step_avg:181.62ms\nstep:4983/5000 train_loss:8.1063 train_time:903195ms step_avg:181.62ms\nstep:4984/5000 train_loss:8.0820 train_time:903378ms step_avg:181.62ms\nstep:4985/5000 train_loss:8.1431 train_time:903559ms step_avg:181.62ms\nstep:4986/5000 train_loss:8.1417 train_time:903741ms step_avg:181.62ms\nstep:4987/5000 train_loss:8.0763 train_time:903921ms step_avg:181.62ms\nstep:4988/5000 train_loss:8.0314 train_time:904105ms step_avg:181.62ms\nstep:4989/5000 train_loss:8.1477 train_time:904285ms step_avg:181.62ms\nstep:4990/5000 train_loss:8.1344 train_time:904466ms step_avg:181.62ms\nstep:4991/5000 train_loss:8.0424 train_time:904647ms step_avg:181.62ms\nstep:4992/5000 train_loss:8.2279 train_time:904828ms step_avg:181.62ms\nstep:4993/5000 train_loss:8.0212 train_time:905010ms step_avg:181.62ms\nstep:4994/5000 train_loss:8.0982 train_time:905192ms step_avg:181.62ms\nstep:4995/5000 train_loss:8.2451 train_time:905375ms step_avg:181.62ms\nstep:4996/5000 train_loss:8.3997 train_time:905556ms step_avg:181.62ms\nstep:4997/5000 train_loss:8.0318 train_time:905738ms step_avg:181.62ms\nstep:4998/5000 train_loss:8.1825 train_time:905921ms step_avg:181.62ms\nstep:4999/5000 train_loss:8.0537 train_time:906101ms step_avg:181.62ms\nstep:5000/5000 train_loss:8.0301 train_time:906282ms step_avg:181.62ms\npeak memory consumption: 37319 MiB"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe training run completed successfully for 5000 iterations. The script generated `model.pt` and `model_config.pt` as required. The training loss decreased significantly during the initial phase and seemed to fluctuate around 8.0 - 8.2 towards the end, which is expected.\n\nLet's check if the output files were created.\n```bash\nls -l\n```",
      "thought": "DISCUSSION\nThe training run completed successfully for 5000 iterations. The script generated `model.pt` and `model_config.pt` as required. The training loss decreased significantly during the initial phase and seemed to fluctuate around 8.0 - 8.2 towards the end, which is expected.\n\nLet's check if the output files were created.\n",
      "action": "ls -l\n",
      "execution_time": 0.30090269446372986,
      "observation": "total 633668\n-rw-r--r-- 1 agent mlgym     22640 Mar 26 03:21 baseline.py\ndrwxr-xr-x 1 agent mlgym        20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym      6550 Mar 21 18:52 evaluate.py*\ndrwxr-xr-x 1 agent mlgym       152 Mar 26 03:22 logs/\n-rw-r--r-- 1 agent mlgym 648835100 Mar 26 03:38 model.pt\n-rw-r--r-- 1 agent mlgym        98 Mar 26 03:38 model_config.pt"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nYes, `model.pt` (648M) and `model_config.pt` (98 bytes) have been created. The model size seems reasonable for a GPT-2 small-like architecture (state dict contains weights, not the full model object).\n\nNow, let's validate this model to get a score.\n```bash\nvalidate\n```",
      "thought": "DISCUSSION\nYes, `model.pt` (648M) and `model_config.pt` (98 bytes) have been created. The model size seems reasonable for a GPT-2 small-like architecture (state dict contains weights, not the full model object).\n\nNow, let's validate this model to get a score.\n",
      "action": "validate\n",
      "execution_time": 5.2332384856417775,
      "observation": "Failed to decode metrics from evaluation script. Output:\n---\nW0326 03:47:30.052000 4015 site-packages/torch/distributed/run.py:793] \nW0326 03:47:30.052000 4015 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:47:30.052000 4015 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:47:30.052000 4015 site-packages/torch/distributed/run.py:793] *****************************************\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/evaluate.py\", line 125, in <module>\n[rank0]:     val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/evaluate.py\", line 59, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\n[rank0]:[W326 03:47:32.784804127 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/evaluate.py\", line 125, in <module>\n[rank1]:     val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/evaluate.py\", line 59, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\nW0326 03:47:33.189000 4015 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 4082 closing signal SIGTERM\nE0326 03:47:33.353000 4015 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 4081) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\n/home/agent/workspace/evaluate.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-26_03:47:33\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 4081)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n---\nPlease make sure the evaluation script prints the metrics in json format."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe validation failed because the `evaluate.py` script is also using the incorrect data path (`./data/fineweb10B/fineweb_val_*.bin`). It should be `./data/fineweb108/fineweb_val_*.bin`. I need to edit `evaluate.py` to fix this path.\n```bash\nopen \"evaluate.py\"\n```",
      "thought": "DISCUSSION\nThe validation failed because the `evaluate.py` script is also using the incorrect data path (`./data/fineweb10B/fineweb_val_*.bin`). It should be `./data/fineweb108/fineweb_val_*.bin`. I need to edit `evaluate.py` to fix this path.\n",
      "action": "open \"evaluate.py\"\n",
      "execution_time": 0.38184749986976385,
      "observation": "[File: /home/agent/workspace/evaluate.py (168 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\"\"\"\n6:\n7:import glob\n8:import json\n9:import os\n10:import pickle\n11:import sys\n12:import time\n13:from dataclasses import dataclass\n14:\n15:import numpy as np\n16:import torch\n17:import torch.distributed as dist\n18:from baseline import Model, ModelConfig\n19:\n20:# -----------------------------------------------------------------------------\n21:# Our own simple Distributed Data Loader\n22:\n23:def _peek_data_shard(filename):\n24:    # only reads the header, returns header data\n25:    with open(filename, \"rb\") as f:\n26:        # first read the header, which is 256 int32 integers (4 bytes each)\n27:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n28:    if header[0] != 20240520:\n29:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n30:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n31:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n32:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n33:        exit(1)\n34:    assert header[1] == 1, \"unsupported version\"\n35:    ntok = header[2] # number of tokens (claimed)\n36:    return ntok # for now just return the number of tokens\n37:\n38:def _load_data_shard(filename):\n39:    with open(filename, \"rb\") as f:\n40:        # first read the header, which is 256 int32 integers (4 bytes each)\n41:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n42:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n43:        assert header[1] == 1, \"unsupported version\"\n44:        ntok = header[2] # number of tokens (claimed)\n45:        # the rest of it are tokens, stored as uint16\n46:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n47:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n48:    return tokens\n49:\n50:class DistributedDataLoader:\n51:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n52:        self.process_rank = process_rank\n53:        self.num_processes = num_processes\n54:        self.B = B\n55:        self.T = T\n56:\n57:        # glob files that match the pattern\n58:        self.files = sorted(glob.glob(filename_pattern))\n59:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n60:\n61:        # load and validate all data shards, count number of tokens in total\n62:        ntok_total = 0\n63:        for fname in self.files:\n64:            shard_ntok = _peek_data_shard(fname)\n65:            assert shard_ntok >= num_processes * B * T + 1\n66:            ntok_total += int(shard_ntok)\n67:        self.ntok_total = ntok_total\n68:\n69:        # kick things off\n70:        self.reset()\n71:\n72:    def reset(self):\n73:        self.current_shard = 0\n74:        self.current_position = self.process_rank * self.B * self.T\n75:        self.tokens = _load_data_shard(self.files[self.current_shard])\n76:\n77:    def advance(self): # advance to next data shard\n78:        self.current_shard = (self.current_shard + 1) % len(self.files)\n79:        self.current_position = self.process_rank * self.B * self.T\n80:        self.tokens = _load_data_shard(self.files[self.current_shard])\n81:\n82:    def next_batch(self):\n83:        B = self.B\n84:        T = self.T\n85:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n86:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n87:        x = (buf[:-1]).view(B, T) # inputs\n88:        y = (buf[1:]).view(B, T) # targets\n89:        # advance current position and load next shard if necessary\n90:        self.current_position += B * T * self.num_processes\n91:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n92:            self.advance()\n93:        return x.cuda(), y.cuda()\n94:\n95:\n96:@dataclass\n97:class Hyperparameters:\n98:    # data hyperparams\n99:    input_val_bin : str = './data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on\n100:    # optimization hyperparams\n101:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n102:    device_batch_size : int = 64 # batch size, in sequences, per device\n103:    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n104:    sequence_length : int = 1024 # sequence length, in tokens\n105:args = Hyperparameters()\n106:args.batch_size = args.device_batch_size * torch.cuda.device_count()\n107:\n108:# set up DDP (distributed data parallel). torchrun sets this env variable\n109:assert torch.cuda.is_available()\n110:dist.init_process_group(backend='nccl')\n111:ddp_rank = int(os.environ['RANK'])\n112:ddp_local_rank = int(os.environ['LOCAL_RANK'])\n113:ddp_world_size = int(os.environ['WORLD_SIZE'])\n114:device = f'cuda:{ddp_local_rank}'\n115:torch.cuda.set_device(device)\n116:\n117:master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n118:\n119:# convenience variables\n120:B, T = args.device_batch_size, args.sequence_length\n121:# calculate the number of steps to take in the val loop.\n122:assert args.val_tokens % (B * T * ddp_world_size) == 0\n123:val_steps = args.val_tokens // (B * T * ddp_world_size)\n124:\n125:val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n126:\n127:# Load the saved config and weights\n128:try:\n129:    import __main__\n130:    setattr(__main__, \"ModelConfig\", ModelConfig)\n131:    model_config = pickle.load(open(\"model_config.pt\", \"rb\"))\n132:    state_dict = torch.load('model.pt', weights_only=True)\n133:    new_state_dict = {}\n134:    for k, v in state_dict.items():\n135:        new_state_dict[k.replace(\"module._orig_mod.\", \"\")] = v\n136:    state_dict = new_state_dict\n137:except:\n138:    raise Exception(\"Model not found. Please run the training script first.\")\n139:\n140:\n141:# Reconstruct the model using the saved configuration\n142:model = Model(model_config)  # Use the saved config to initialize the model\n143:model.load_state_dict(state_dict)\n144:model = model.cuda()\n145:\n146:ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n147:\n148:# stop the clock\n149:t0 = time.time()\n150:# run validation batches\n151:model.eval()\n152:val_loader.reset()\n153:val_loss = 0.0\n154:torch.cuda.empty_cache()\n155:with torch.no_grad():\n156:    for _ in range(val_steps):\n157:        x_val, y_val = val_loader.next_batch()\n158:        with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason\n159:            _, loss = model(x_val, y_val, return_logits=False)\n160:            val_loss += loss.detach()\n161:            del loss\n162:dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n163:val_loss /= val_steps\n164:# log val loss to console and to logfile\n165:if master_process:\n166:    # Create and print the result dictionary\n167:    result = {\"val_loss\": val_loss.item()}\n168:    print(json.dumps(result))"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/evaluate.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the incorrect path is on line 99. Let's change `fineweb10B` to `fineweb108`.\n```bash\nedit 99:99\n    input_val_bin : str = './data/fineweb108/fineweb_val_*.bin' # input .bin to eval validation loss on\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the incorrect path is on line 99. Let's change `fineweb10B` to `fineweb108`.\n",
      "action": "edit 99:99\n    input_val_bin : str = './data/fineweb108/fineweb_val_*.bin' # input .bin to eval validation loss on\nend_of_edit\n",
      "execution_time": 0.35111826937645674,
      "observation": "Error: You cannot edit read-only file /home/agent/workspace/evaluate.py"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/evaluate.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "Exit due to cost limit",
      "thought": "Exit due to cost limit",
      "action": "exit_cost",
      "execution_time": 5.854268297553062,
      "observation": "Exited (exit_cost)"
    }
  ],
  "history": [
    {
      "role": "system",
      "content": "SETTING: You are an autonomous Machine Learning Researcher, and you're working directly in the command line with a special interface.\n\nThe special interface consists of a file editor that shows you 1000 lines of a file at a time.\nIn addition to typical bash commands, you can also use the following commands to help you navigate and edit files.\n\nCOMMANDS:\nopen:\n  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n  signature: open \"<path>\" [<line_number>]\n  arguments:\n    - path (string) [required]: the path to the file to open\n    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n\ngoto:\n  docstring: moves the window to show <line_number>\n  signature: goto <line_number>\n  arguments:\n    - line_number (integer) [required]: the line number to move the window to\n\nscroll_down:\n  docstring: moves the window down 1000 lines\n  signature: scroll_down\n\nscroll_up:\n  docstring: moves the window down 1000 lines\n  signature: scroll_up\n\ncreate:\n  docstring: creates and opens a new file with the given name\n  signature: create <filename>\n  arguments:\n    - filename (string) [required]: the name of the file to create\n\nsearch_dir:\n  docstring: searches for search_term in all files in dir. If dir is not provided, searches in the current directory\n  signature: search_dir <search_term> [<dir>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nsearch_file:\n  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n  signature: search_file <search_term> [<file>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n\nfind_file:\n  docstring: finds all files with the given name in dir. If dir is not provided, searches in the current directory\n  signature: find_file <file_name> [<dir>]\n  arguments:\n    - file_name (string) [required]: the name of the file to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nedit:\n  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n  signature: edit <start_line>:<end_line>\n<replacement_text>\nend_of_edit\n  arguments:\n    - start_line (integer) [required]: the line number to start the edit at\n    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n    - replacement_text (string) [required]: the text to replace the current selection with\n\ninsert:\n  docstring: inserts the given text after the specified line number in the open file. The text to insert is terminated by a line with only end_of_insert on it. All of the <text_to_add> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the insertion. If the system detects a syntax error, the insertion will not be executed. Simply try to insert again, but make sure to read the error message and modify the insert command you issue accordingly.\n  signature: insert <line_number>\n<text_to_add>\nend_of_insert\n  arguments:\n    - line_number (integer) [required]: the line number after which to insert the text\n    - text_to_add (string) [required]: the text to insert after the specified line\n\nsubmit:\n  docstring: submits your current code and terminates the session\n  signature: submit\n\nvalidate:\n  docstring: validates your current submission file and returns the metrics on test set\n  signature: validate\n\n\n\nPlease note that THE EDIT and INSERT COMMANDS REQUIRES PROPER INDENTATION.\nIf you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n\nRESPONSE FORMAT:\nYour shell prompt is formatted as follows:\n(Open file: <path>) <cwd> $\n\nYou need to format your output using two fields; discussion and command.\nYour output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\nDISCUSSION\nFirst I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n```\nls -a\n```\n\nYou should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference. Please do not include any DISCUSSION after your action.\nIf you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command.\nYou're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\nHowever, the environment does NOT support interactive session commands (e.g. python, vim), so please do not invoke them.\nYour goal is to achieve the best possible score, not just to submit your first working solution. Consider strategies like validating your answer using the `validate` command, manually spot-checking predictions, building custom validation sets and grading functions, and comparing different algorithms.\nOnce you have exhausted all possible solutions and cannot make progress, you can submit your final solution by using `submit` command.\n\nIMPORTANT TIPS:\n1. Always start by trying to understand the baseline script if available. This will give you an idea of one possible solution for the task and the baseline scores that you have to beat.\n\n2. If you run a command and it doesn't work, try running a different command. A command that did not work once will not work the second time unless you modify it!\n\n3. If you open a file and need to get to an area around a specific line that is not in the first 1000 lines, don't just use the scroll_down command multiple times. Instead, use the goto <line_number> command. It's much quicker.\n\n4. Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file). The currently open file might be in a different directory than the working directory! Note that some commands, such as 'create', open files, so they might change the current  open file.\n\n5. When editing files, it is easy to accidentally specify a wrong line number or to write code with incorrect indentation. Always check the code after you issue an edit to make sure that it reflects what you wanted to accomplish. If it didn't, issue another command to fix it.\n\n6. You have a limited number of actions/steps you can take in the environment. The current step and remaining number of steps will given after every action. Use the remaining steps wisely. If you only have few remaining steps, it is better to submit a working solution then to keep trying.\n\n7. Your each action should take less than 2400 seconds to complete. If your action doesn't finish within the time limit, it will be interrupted.\n\n8. Validating your solution often, will give you a good idea of your progress so far and you will be able to adapt your strategy. Do not run the evaluation file on your own, use the `validate` function instead. If you run evaluation file yourself, your results won't be logged.",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "We're currently solving the following task. Here's the task description:\n\nTASK DESCRIPTION:\nThe goal of this task is to train a GPT-2 style model for next token prediction. This task uses the following datasets:\nFINEWEBDATASET:\nFineWeb dataset comprising binary files. The training split contains 2.4B tokens.\n\n\n\nIf a baseline is given, your task is to train a new model that improves performance on the given datasets as much as possible. If you fail to produce a valid submission artefact evaluation file will give you a score of 0.\n\nSUBMISSION FORMAT:\nFor this task, your code should save the trained model and it's config so that it can be evaluated later. Your code should generate two files with following names:\n1. `model.pt`: Use `torch.save` function to save the model's state dict.\n2. `model_config.pt`: Save model config as a pickle dump.\n\nINSTRUCTIONS:\nNow, you're going to train a model to improve performance on this task. Your terminal session has started and you're in the workspace root directory. You can use any bash commands or the special interface to help you. Edit all the file you need or create a new training script.\nRemember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after every command.\nWhen you're satisfied with all of the changes you have made, you can run your code. Your code should produce a valid submission artefact. Please follow the instructions in SUBMISSION FORMAT section above and adhere to the guidelines provided for generating submission artefacts. You can also look at the `evaluate.py` provided to you to see if you are following the correct format and naming scheme for your submission artefacts.\n\nNote however that you cannot use any interactive session commands (e.g. python, vim) in this environment, but you can write scripts and run them. E.g. you can write a python script and then run it with `python <script_name>.py`.\n\nNOTE ABOUT THE EDIT AND INSERT COMMANDs: Indentation really matters! When editing a file, make sure to insert appropriate indentation before each line!\n\n(Current Step: 0, Remaining Steps: 50)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, let's start by exploring the current directory to see what files are available. This will help me understand the project structure, locate any baseline scripts, data files, or utility scripts.\n```bash\nls -al\n```",
      "thought": "DISCUSSION\nOkay, let's start by exploring the current directory to see what files are available. This will help me understand the project structure, locate any baseline scripts, data files, or utility scripts.\n",
      "action": "ls -al\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "total 32\ndrwxr-xr-x 1 agent mlgym    52 Mar 26 03:00 ./\ndrwxr-x--- 1 agent mlgym    66 Mar 26 03:00 ../\n-rw-r--r-- 1 agent mlgym 22293 Mar 21 18:52 baseline.py\ndrwxr-xr-x 1 agent mlgym    20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym  6550 Mar 21 18:52 evaluate.py*\n(Current Step: 1, Remaining Steps: 49)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nIt seems we have a `baseline.py` script, a `data` directory, and an `evaluate.py` script. The `baseline.py` is likely the starting point for our model training. Let's open it to understand the baseline implementation.\n```bash\nopen \"baseline.py\"\n```",
      "thought": "DISCUSSION\nIt seems we have a `baseline.py` script, a `data` directory, and an `evaluate.py` script. The `baseline.py` is likely the starting point for our model training. Let's open it to understand the baseline implementation.\n",
      "action": "open \"baseline.py\"\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\n(Current Step: 2, Remaining Steps: 48)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe baseline script `baseline.py` trains a GPT-2 style model. It uses a custom optimizer called `Muon` and a custom distributed data loader.\n\nKey parameters from the `Hyperparameters` dataclass (lines 350-362):\n- `input_bin`: `./data/fineweb10B/fineweb_train_*.bin`\n- `batch_size`: 512 (total)\n- `device_batch_size`: 64\n- `sequence_length`: 1024\n- `num_iterations`: 500 - This seems very low for the FineWeb dataset (2.4B tokens). One epoch is approximately 2.4B / (512 * 1024) \u2248 4578 iterations. Training for only 500 iterations (~10% of an epoch) is likely suboptimal.\n- `warmup_iters`: 0\n- `warmdown_iters`: 1308 - This is longer than `num_iterations`. The learning rate schedule logic (lines 412-424) effectively implements only linear decay from `(500-it)/1308`, starting from `it=0`.\n- `weight_decay`: 0\n\nThe `ModelConfig` (lines 223-228) specifies:\n- `vocab_size`: 50304\n- `n_layer`: 12\n- `n_head`: 6\n- `n_embd`: 768\nThis is roughly a GPT-2 small configuration (~124M parameters).\n\n**Plan:**\n1.  Confirm the data path specified in `input_bin` exists.\n2.  Significantly increase `num_iterations` to train for closer to one epoch (e.g., 5000 iterations).\n3.  Adjust the learning rate schedule (`warmup_iters`, `warmdown_iters`) to be more conventional for the new number of iterations. Let's try 200 warmup iterations and have the decay last for the remaining iterations.\n4.  Introduce weight decay (e.g., 0.1) for the Adam optimizers (embeddings and LM head).\n5.  Run the modified script and validate.\n\nFirst, let's check the data directory.\n```bash\nls -l data/fineweb10B/\n```",
      "thought": "DISCUSSION\nThe baseline script `baseline.py` trains a GPT-2 style model. It uses a custom optimizer called `Muon` and a custom distributed data loader.\n\nKey parameters from the `Hyperparameters` dataclass (lines 350-362):\n- `input_bin`: `./data/fineweb10B/fineweb_train_*.bin`\n- `batch_size`: 512 (total)\n- `device_batch_size`: 64\n- `sequence_length`: 1024\n- `num_iterations`: 500 - This seems very low for the FineWeb dataset (2.4B tokens). One epoch is approximately 2.4B / (512 * 1024) \u2248 4578 iterations. Training for only 500 iterations (~10% of an epoch) is likely suboptimal.\n- `warmup_iters`: 0\n- `warmdown_iters`: 1308 - This is longer than `num_iterations`. The learning rate schedule logic (lines 412-424) effectively implements only linear decay from `(500-it)/1308`, starting from `it=0`.\n- `weight_decay`: 0\n\nThe `ModelConfig` (lines 223-228) specifies:\n- `vocab_size`: 50304\n- `n_layer`: 12\n- `n_head`: 6\n- `n_embd`: 768\nThis is roughly a GPT-2 small configuration (~124M parameters).\n\n**Plan:**\n1.  Confirm the data path specified in `input_bin` exists.\n2.  Significantly increase `num_iterations` to train for closer to one epoch (e.g., 5000 iterations).\n3.  Adjust the learning rate schedule (`warmup_iters`, `warmdown_iters`) to be more conventional for the new number of iterations. Let's try 200 warmup iterations and have the decay last for the remaining iterations.\n4.  Introduce weight decay (e.g., 0.1) for the Adam optimizers (embeddings and LM head).\n5.  Run the modified script and validate.\n\nFirst, let's check the data directory.\n",
      "action": "ls -l data/fineweb10B/\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "ls: cannot access 'data/fineweb10B/': No such file or directory\n(Current Step: 3, Remaining Steps: 47)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the path `./data/fineweb10B/` does not exist. The `Hyperparameters` default `input_bin` is incorrect. Let's check the `data` directory to see the actual path.\n```bash\nls -l data/\n```",
      "thought": "DISCUSSION\nOkay, the path `./data/fineweb10B/` does not exist. The `Hyperparameters` default `input_bin` is incorrect. Let's check the `data` directory to see the actual path.\n",
      "action": "ls -l data/\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "total 0\ndr-xr-xr-x 1 agent mlgym 908 Mar 26 02:59 fineweb108/\n(Current Step: 4, Remaining Steps: 46)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nIt seems the data directory is `fineweb108`, not `fineweb10B`. Let's update the `input_bin` path in `baseline.py`. I'll edit line 352.\n```bash\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nIt seems the data directory is `fineweb108`, not `fineweb10B`. Let's update the `input_bin` path in `baseline.py`. I'll edit line 352.\n",
      "action": "edit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 5, Remaining Steps: 45)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe data path on line 352 is updated correctly.\n\nNow, let's increase `num_iterations` to 5000 (approximately one epoch). I'll edit line 357.\n```bash\nedit 357:357\n    num_iterations : int = 5000 # number of iterations to run\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe data path on line 352 is updated correctly.\n\nNow, let's increase `num_iterations` to 5000 (approximately one epoch). I'll edit line 357.\n",
      "action": "edit 357:357\n    num_iterations : int = 5000 # number of iterations to run\nend_of_edit\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 6, Remaining Steps: 44)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\n`num_iterations` is updated. Now, let's modify the learning rate schedule. I'll set `warmup_iters` to 200 (line 358) and `warmdown_iters` to `num_iterations - warmup_iters`, which is 5000 - 200 = 4800 (line 359). This creates a linear warmup followed by a linear decay for the rest of the training. I will also change the `get_lr` function to implement a cosine decay instead of linear decay, as that often works better. I will also add weight decay `0.1` to the Adam optimizers.\n```bash\nedit 358:360\n    warmup_iters : int = 200\n    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0.1\nend_of_edit\n```",
      "thought": "DISCUSSION\n`num_iterations` is updated. Now, let's modify the learning rate schedule. I'll set `warmup_iters` to 200 (line 358) and `warmdown_iters` to `num_iterations - warmup_iters`, which is 5000 - 200 = 4800 (line 359). This creates a linear warmup followed by a linear decay for the rest of the training. I will also change the `get_lr` function to implement a cosine decay instead of linear decay, as that often works better. I will also add weight decay `0.1` to the Adam optimizers.\n",
      "action": "edit 358:360\n    warmup_iters : int = 200\n    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0.1\nend_of_edit\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 7, Remaining Steps: 43)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe warmup/warmdown iterations and weight decay hyperparameter values are updated. Now let's apply the weight decay to the Adam optimizers (optimizer1 and optimizer2). The Muon optimizer (optimizer3) doesn't seem to support weight decay according to its docstring. I'll edit lines 407 and 408.\n```bash\nedit 407:408\n    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe warmup/warmdown iterations and weight decay hyperparameter values are updated. Now let's apply the weight decay to the Adam optimizers (optimizer1 and optimizer2). The Muon optimizer (optimizer3) doesn't seem to support weight decay according to its docstring. I'll edit lines 407 and 408.\n",
      "action": "edit 407:408\n    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\nend_of_edit\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 8, Remaining Steps: 42)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nWeight decay is added to the Adam optimizers.\n\nNow, let's change the learning rate schedule from linear decay to cosine decay after the warmup phase. First, I need to import the `math` module. I'll insert the import statement at the top with the other imports, for instance, after line 16.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nWeight decay is added to the Adam optimizers.\n\nNow, let's change the learning rate schedule from linear decay to cosine decay after the warmup phase. First, I need to import the `math` module. I'll insert the import statement at the top with the other imports, for instance, after line 16.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error.\n(Current Step: 9, Remaining Steps: 41)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAh, it seems the syntax check flagged an unrelated issue, possibly an existing invalid escape sequence `\\s` somewhere else in the file, likely within a docstring or comment. The `import math` line itself is standard. Let's try inserting it at a different location, perhaps closer to the top, after `import os` (line 12).\n```bash\ninsert 12\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nAh, it seems the syntax check flagged an unrelated issue, possibly an existing invalid escape sequence `\\s` somewhere else in the file, likely within a docstring or comment. The `import math` line itself is standard. Let's try inserting it at a different location, perhaps closer to the top, after `import os` (line 12).\n",
      "action": "insert 12\nimport math\nend_of_insert\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(6 more lines above)\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import math\n14:import pickle\n15:import sys\n16:import time\n17:import uuid\n(495 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(7 more lines above)\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n(494 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error.\n(Current Step: 10, Remaining Steps: 40)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nIt seems the `SyntaxWarning: invalid escape sequence '\\s'` is persistent and likely exists in the original file, possibly within a docstring or comment. The `import math` statement itself is correct. Let's try to find and fix the `\\s` sequence first. I'll search the file for `\\s`.\n```bash\nsearch_file \"\\\\s\"\n```",
      "thought": "DISCUSSION\nIt seems the `SyntaxWarning: invalid escape sequence '\\s'` is persistent and likely exists in the original file, possibly within a docstring or comment. The `import math` statement itself is correct. Let's try to find and fix the `\\s` sequence first. I'll search the file for `\\s`.\n",
      "action": "search_file \"\\\\s\"\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "Found 435 matches for \"\\s\" in /home/agent/workspace/baseline.py:\nLine 2:Copyright (c) Meta Platforms, Inc. and affiliates.\nLine 4:Adapted from https://github.com/KellerJordan/modded-nanogpt\nLine 6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\nLine 7:Run this file with the following command:\nLine 8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\nLine 11:import glob\nLine 12:import os\nLine 13:import pickle\nLine 14:import sys\nLine 15:import time\nLine 16:import uuid\nLine 17:from dataclasses import dataclass\nLine 19:import numpy as np\nLine 20:import torch\nLine 21:import torch._dynamo\nLine 22:import torch._inductor.config as config\nLine 23:import torch.distributed as dist\nLine 24:import torch.nn.functional as F\nLine 25:from torch import nn\nLine 26:from torch.nn.parallel import DistributedDataParallel as DDP\nLine 28:torch._dynamo.config.suppress_errors = True\nLine 30:# -----------------------------------------------------------------------------\nLine 31:# Muon optimizer\nLine 33:def zeropower_via_svd(G, steps=None):\nLine 34:    U, S, V = G.svd()\nLine 35:    return U @ V.T\nLine 38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\nLine 39:    \"\"\"\nLine 40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\nLine 41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\nLine 42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\nLine 43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\nLine 44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\nLine 45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\nLine 46:    performance at all relative to UV^T, where USV^T = G is the SVD.\nLine 47:    \"\"\"\nLine 48:    assert len(G.shape) == 2\nLine 49:    a, b, c = (3.4445, -4.7750,  2.0315)\nLine 50:    X = G.bfloat16()\nLine 51:    X /= (X.norm() + eps) # ensure top singular value <= 1\nLine 52:    if G.size(0) > G.size(1):\nLine 53:        X = X.T\nLine 54:    for _ in range(steps):\nLine 55:        A = X @ X.T\nLine 56:        B = A @ X\nLine 57:        X = a * X + b * B + c * A @ B\nLine 58:    if G.size(0) > G.size(1):\nLine 59:        X = X.T\nLine 60:    return X\nLine 62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\nLine 64:class Muon(torch.optim.Optimizer):\nLine 65:    \"\"\"\nLine 66:    Muon - MomentUm Orthogonalized by Newton-schulz\nLine 68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\nLine 69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\nLine 70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\nLine 71:    the advantage that it can be stably run in bfloat16 on the GPU.\nLine 73:    Some warnings:\nLine 74:    - This optimizer assumes that all parameters passed in are 2D.\nLine 75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\nLine 76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\nLine 77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\nLine 78:    - We believe it is unlikely to work well for training with small batch size.\nLine 79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\nLine 80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\nLine 82:    Arguments:\nLine 83:        lr: The learning rate used by the internal SGD.\nLine 84:        momentum: The momentum used by the internal SGD.\nLine 85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\nLine 86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\nLine 87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\nLine 88:    \"\"\"\nLine 89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\nLine 90:                 backend='newtonschulz5', backend_steps=5):\nLine 91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\nLine 92:        super().__init__(params, defaults)\nLine 94:    def step(self):\nLine 96:        for group in self.param_groups:\nLine 98:            lr = group['lr']\nLine 99:            momentum = group['momentum']\nLine 100:            zeropower_backend = zeropower_backends[group['backend']]\nLine 102:            # generate weight updates in distributed fashion\nLine 103:            total_params = sum(p.numel() for p in group['params'])\nLine 104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\nLine 105:            curr_idx = 0\nLine 106:            for i, p in enumerate(group['params']):\nLine 107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\nLine 108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\nLine 109:                    g = p.grad\nLine 110:                    assert g is not None\nLine 111:                    state = self.state[p]\nLine 112:                    if 'momentum_buffer' not in state:\nLine 113:                        state['momentum_buffer'] = torch.zeros_like(g)\nLine 114:                    buf = state['momentum_buffer']\nLine 115:                    buf.mul_(momentum).add_(g)\nLine 116:                    if group['nesterov']:\nLine 117:                        g = g.add(buf, alpha=momentum)\nLine 118:                    g = zeropower_backend(g, steps=group['backend_steps'])\nLine 119:                    g *= max(1, g.size(0)/g.size(1))**0.5\nLine 120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\nLine 121:                curr_idx += p.numel()\nLine 123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\nLine 124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\nLine 126:            # deserialize and apply updates\nLine 127:            curr_idx = 0\nLine 128:            for p in group['params']:\nLine 129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\nLine 130:                p.data.add_(g, alpha=-lr)\nLine 131:                curr_idx += p.numel()\nLine 133:# -----------------------------------------------------------------------------\nLine 134:# PyTorch nn.Module definitions for the GPT-2 model\nLine 136:class Rotary(torch.nn.Module):\nLine 138:    def __init__(self, dim, base=10000):\nLine 139:        super().__init__()\nLine 140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\nLine 141:        self.seq_len_cached = None\nLine 142:        self.cos_cached = None\nLine 143:        self.sin_cached = None\nLine 145:    def forward(self, x):\nLine 146:        seq_len = x.shape[1]\nLine 147:        if seq_len != self.seq_len_cached:\nLine 148:            self.seq_len_cached = seq_len\nLine 149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\nLine 150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\nLine 151:            self.cos_cached = freqs.cos().bfloat16()\nLine 152:            self.sin_cached = freqs.sin().bfloat16()\nLine 153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\nLine 155:def apply_rotary_emb(x, cos, sin):\nLine 156:    assert x.ndim == 4 # multihead attention\nLine 157:    d = x.shape[3]//2\nLine 158:    x1 = x[..., :d]\nLine 159:    x2 = x[..., d:]\nLine 160:    y1 = x1 * cos + x2 * sin\nLine 161:    y2 = x1 * (-sin) + x2 * cos\nLine 162:    return torch.cat([y1, y2], 3).type_as(x)\nLine 164:class CausalSelfAttention(nn.Module):\nLine 166:    def __init__(self, config):\nLine 167:        super().__init__()\nLine 168:        self.n_head = config.n_head\nLine 169:        self.n_embd = config.n_embd\nLine 170:        self.head_dim = self.n_embd // self.n_head\nLine 171:        assert self.n_embd % self.n_head == 0\nLine 172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 175:        # output projection\nLine 176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\nLine 178:        self.rotary = Rotary(self.head_dim)\nLine 180:    def forward(self, x):\nLine 181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\nLine 182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\nLine 183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\nLine 184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\nLine 185:        cos, sin = self.rotary(q)\nLine 186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\nLine 187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\nLine 188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\nLine 189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\nLine 190:        y = self.c_proj(y)\nLine 191:        return y\nLine 193:class MLP(nn.Module):\nLine 195:    def __init__(self, config):\nLine 196:        super().__init__()\nLine 197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\nLine 198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\nLine 199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\nLine 201:    def forward(self, x):\nLine 202:        x = self.c_fc(x)\nLine 203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\nLine 204:        x = self.c_proj(x)\nLine 205:        return x\nLine 207:class Block(nn.Module):\nLine 209:    def __init__(self, config):\nLine 210:        super().__init__()\nLine 211:        self.attn = CausalSelfAttention(config)\nLine 212:        self.mlp = MLP(config)\nLine 214:    def forward(self, x):\nLine 215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\nLine 216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\nLine 217:        return x\nLine 219:# -----------------------------------------------------------------------------\nLine 220:# The main GPT-2 model\nLine 223:class ModelConfig:\nLine 224:    vocab_size : int = 50304\nLine 225:    n_layer : int = 12\nLine 226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\nLine 227:    n_embd : int = 768\nLine 229:class Model(nn.Module):\nLine 231:    def __init__(self, config):\nLine 232:        super().__init__()\nLine 233:        self.config = config\nLine 235:        self.transformer = nn.ModuleDict(dict(\nLine 236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\nLine 237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\nLine 238:        ))\nLine 239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\nLine 240:        self.lm_head.weight.data.zero_() # @Grad62304977\nLine 242:    def forward(self, idx, targets=None, return_logits=True):\nLine 244:        # forward the GPT model itself\nLine 245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\nLine 246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\nLine 247:        for block in self.transformer.h:\nLine 248:            x = block(x)\nLine 249:        x = F.rms_norm(x, (x.size(-1),))\nLine 251:        if targets is not None:\nLine 252:            # if we are given some desired targets also calculate the loss\nLine 253:            logits = self.lm_head(x)\nLine 254:            logits = logits.float() # use tf32/fp32 for logits\nLine 255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\nLine 256:        else:\nLine 257:            # inference-time mini-optimization: only forward the lm_head on the very last position\nLine 258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\nLine 259:            logits = logits.float() # use tf32/fp32 for logits\nLine 260:            loss = None\nLine 262:        # there are performance reasons why not returning logits is prudent, if not needed\nLine 263:        if not return_logits:\nLine 264:            logits = None\nLine 266:        return logits, loss\nLine 267:    \nLine 268:    def get_config(self):\nLine 269:        return self.config\nLine 271:# -----------------------------------------------------------------------------\nLine 272:# Our own simple Distributed Data Loader\nLine 274:def _peek_data_shard(filename):\nLine 275:    # only reads the header, returns header data\nLine 276:    with open(filename, \"rb\") as f:\nLine 277:        # first read the header, which is 256 int32 integers (4 bytes each)\nLine 278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\nLine 279:    if header[0] != 20240520:\nLine 280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\nLine 281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\nLine 282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\nLine 283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\nLine 284:        exit(1)\nLine 285:    assert header[1] == 1, \"unsupported version\"\nLine 286:    ntok = header[2] # number of tokens (claimed)\nLine 287:    return ntok # for now just return the number of tokens\nLine 289:def _load_data_shard(filename):\nLine 290:    with open(filename, \"rb\") as f:\nLine 291:        # first read the header, which is 256 int32 integers (4 bytes each)\nLine 292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\nLine 293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\nLine 294:        assert header[1] == 1, \"unsupported version\"\nLine 295:        ntok = header[2] # number of tokens (claimed)\nLine 296:        # the rest of it are tokens, stored as uint16\nLine 297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\nLine 298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\nLine 299:    return tokens\nLine 301:class DistributedDataLoader:\nLine 302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\nLine 303:        self.process_rank = process_rank\nLine 304:        self.num_processes = num_processes\nLine 305:        self.B = B\nLine 306:        self.T = T\nLine 308:        # glob files that match the pattern\nLine 309:        self.files = sorted(glob.glob(filename_pattern))\nLine 310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\nLine 312:        # load and validate all data shards, count number of tokens in total\nLine 313:        ntok_total = 0\nLine 314:        for fname in self.files:\nLine 315:            shard_ntok = _peek_data_shard(fname)\nLine 316:            assert shard_ntok >= num_processes * B * T + 1\nLine 317:            ntok_total += int(shard_ntok)\nLine 318:        self.ntok_total = ntok_total\nLine 320:        # kick things off\nLine 321:        self.reset()\nLine 323:    def reset(self):\nLine 324:        self.current_shard = 0\nLine 325:        self.current_position = self.process_rank * self.B * self.T\nLine 326:        self.tokens = _load_data_shard(self.files[self.current_shard])\nLine 328:    def advance(self): # advance to next data shard\nLine 329:        self.current_shard = (self.current_shard + 1) % len(self.files)\nLine 330:        self.current_position = self.process_rank * self.B * self.T\nLine 331:        self.tokens = _load_data_shard(self.files[self.current_shard])\nLine 333:    def next_batch(self):\nLine 334:        B = self.B\nLine 335:        T = self.T\nLine 336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\nLine 337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\nLine 338:        x = (buf[:-1]).view(B, T) # inputs\nLine 339:        y = (buf[1:]).view(B, T) # targets\nLine 340:        # advance current position and load next shard if necessary\nLine 341:        self.current_position += B * T * self.num_processes\nLine 342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\nLine 343:            self.advance()\nLine 344:        return x.cuda(), y.cuda()\nLine 346:# -----------------------------------------------------------------------------\nLine 347:# int main\nLine 350:class Hyperparameters:\nLine 351:    # data hyperparams\nLine 352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nLine 353:    # optimization hyperparams\nLine 354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\nLine 355:    device_batch_size : int = 64 # batch size, in sequences, per device\nLine 356:    sequence_length : int = 1024 # sequence length, in tokens\nLine 357:    num_iterations : int = 5000 # number of iterations to run\nLine 358:    warmup_iters : int = 200\nLine 359:    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\nLine 360:    weight_decay : float = 0.1\nLine 361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\nLine 364:def main():\nLine 365:    with open(sys.argv[0]) as f:\nLine 366:        code = f.read() # read the code of this file ASAP, for logging\nLine 367:    args = Hyperparameters()\nLine 368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\nLine 370:    # set up DDP (distributed data parallel). torchrun sets this env variable\nLine 371:    assert torch.cuda.is_available()\nLine 372:    dist.init_process_group(backend='nccl')\nLine 373:    ddp_rank = int(os.environ['RANK'])\nLine 374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\nLine 375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\nLine 376:    device = f'cuda:{ddp_local_rank}'\nLine 377:    torch.cuda.set_device(device)\nLine 378:    print(f\"using device: {device}\")\nLine 379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\nLine 381:    # convenience variables\nLine 382:    B, T = args.device_batch_size, args.sequence_length\nLine 383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\nLine 384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\nLine 385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\nLine 387:    # load tokens\nLine 388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\nLine 389:    if master_process:\nLine 390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\nLine 391:    x, y = train_loader.next_batch()\nLine 393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\nLine 394:    # this originates from Karpathy's experiments.\nLine 395:    num_vocab = 50304\nLine 396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\nLine 397:    model = model.cuda()\nLine 398:    if hasattr(config, \"coordinate_descent_tuning\"):\nLine 399:        config.coordinate_descent_tuning = True # suggested by @Chillee\nLine 400:    model = torch.compile(model)\nLine 401:    # here we wrap model into DDP container\nLine 402:    model = DDP(model, device_ids=[ddp_local_rank])\nLine 403:    raw_model = model.module # always contains the \"raw\" unwrapped model\nLine 404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\nLine 406:    # init the optimizer(s)\nLine 407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\nLine 408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\nLine 409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\nLine 410:    optimizers = [optimizer1, optimizer2, optimizer3]\nLine 411:    # learning rate decay scheduler (linear warmup and warmdown)\nLine 412:    def get_lr(it):\nLine 413:        assert it <= args.num_iterations\nLine 414:        # 1) linear warmup for warmup_iters steps\nLine 415:        if it < args.warmup_iters:\nLine 416:            return (it+1) / args.warmup_iters\nLine 417:        # 2) constant lr for a while\nLine 418:        elif it < args.num_iterations - args.warmdown_iters:\nLine 419:            return 1.0\nLine 420:        # 3) linear warmdown\nLine 421:        else:\nLine 422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\nLine 423:            return decay_ratio\nLine 424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nLine 426:    # begin logging\nLine 427:    if master_process:\nLine 428:        run_id = str(uuid.uuid4())\nLine 429:        logdir = 'logs/%s/' % run_id\nLine 430:        os.makedirs(logdir, exist_ok=True)\nLine 431:        logfile = 'logs/%s.txt' % run_id\nLine 432:        # create the log file\nLine 433:        with open(logfile, \"w\") as f:\nLine 434:            # begin the log by printing this file (the Python code)\nLine 435:            f.write('='*100 + '\\n')\nLine 436:            f.write(code)\nLine 437:            f.write('='*100 + '\\n')\nLine 438:            # log information about the hardware/software environment this is running on\nLine 439:            # and print the full `nvidia-smi` to file\nLine 440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\nLine 441:            import subprocess\nLine 442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\nLine 443:            f.write(f'{result.stdout}\\n')\nLine 444:            f.write('='*100 + '\\n')\nLine 446:    training_time_ms = 0\nLine 447:    # start the clock\nLine 448:    torch.cuda.synchronize()\nLine 449:    t0 = time.time()\nLine 450:    # begin training\nLine 451:    train_loader.reset()\nLine 452:    for step in range(args.num_iterations + 1):\nLine 453:        last_step = (step == args.num_iterations)\nLine 454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\nLine 455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\nLine 456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\nLine 457:        if step == 10:\nLine 458:            training_time_ms = 0\nLine 459:            t0 = time.time()\nLine 460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\nLine 462:        if last_step:\nLine 463:            break\nLine 465:        # --------------- TRAINING SECTION BEGIN -----------------\nLine 466:        model.train()\nLine 467:        for i in range(1, train_accumulation_steps+1):\nLine 468:            # forward pass\nLine 469:            with ctx:\nLine 470:                _, loss = model(x, y, return_logits=False)\nLine 471:                train_loss = loss.detach()\nLine 472:            # advance the dataset for the next batch\nLine 473:            x, y = train_loader.next_batch()\nLine 474:            # backward pass\nLine 475:            if i < train_accumulation_steps:\nLine 476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\nLine 477:                    loss.backward()\nLine 478:            else:\nLine 479:                loss.backward() # just sync on the last step\nLine 480:        for p in model.parameters():\nLine 481:            p.grad /= train_accumulation_steps\nLine 482:        # step the optimizers and schedulers\nLine 483:        for opt, sched in zip(optimizers, schedulers):\nLine 484:            opt.step()\nLine 485:            sched.step()\nLine 486:        # null the gradients\nLine 487:        model.zero_grad(set_to_none=True)\nLine 488:        # --------------- TRAINING SECTION END -------------------\nLine 489:        # everything that follows now is just diagnostics, prints, logging, etc.\nLine 491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\nLine 492:        if master_process:\nLine 493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\nLine 494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\nLine 495:            with open(logfile, \"a\") as f:\nLine 496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\nLine 498:    torch.save(model.state_dict(), \"model.pt\")\nLine 499:    with open(\"model_config.pt\", \"wb\") as f:\nLine 500:        pickle.dump(raw_model.get_config(), f)\nLine 501:    f.close()\nLine 503:    if master_process:\nLine 504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\nLine 506:    # -------------------------------------------------------------------------\nLine 507:    # clean up nice\nLine 508:    dist.destroy_process_group()\nLine 510:if __name__ == \"__main__\":\nLine 511:    main()\nEnd of matches for \"\\s\" in /home/agent/workspace/baseline.py\n(Current Step: 11, Remaining Steps: 39)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe search confirms that `\\s` appears frequently, mostly representing actual whitespace in comments, docstrings, and code. The syntax checker seems overly sensitive. Since it's a warning, not an error, and inserting `import math` is unlikely to be the root cause, I'll proceed with the insertion and the subsequent changes. Let's try inserting `import math` after the initial docstring and before other imports (line 10).\n```bash\ninsert 10\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nThe search confirms that `\\s` appears frequently, mostly representing actual whitespace in comments, docstrings, and code. The syntax checker seems overly sensitive. Since it's a warning, not an error, and inserting `import math` is unlikely to be the root cause, I'll proceed with the insertion and the subsequent changes. Let's try inserting `import math` after the initial docstring and before other imports (line 10).\n",
      "action": "insert 10\nimport math\nend_of_insert\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(4 more lines above)\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import math\n12:import glob\n13:import os\n14:import pickle\n15:import sys\n(497 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(5 more lines above)\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n(496 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error.\n(Current Step: 12, Remaining Steps: 38)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the syntax warning persists. It seems to be triggered by something inherent in the file that the checker doesn't like, even though it's likely valid Python. Since it's just a warning, I will ignore it for now and hope it doesn't prevent the script from running. Let's try inserting `import math` again, this time after line 10.\n```bash\ninsert 10\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nOkay, the syntax warning persists. It seems to be triggered by something inherent in the file that the checker doesn't like, even though it's likely valid Python. Since it's just a warning, I will ignore it for now and hope it doesn't prevent the script from running. Let's try inserting `import math` again, this time after line 10.\n",
      "action": "insert 10\nimport math\nend_of_insert\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(4 more lines above)\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import math\n12:import glob\n13:import os\n14:import pickle\n15:import sys\n(497 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(5 more lines above)\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n(496 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error.\n(Current Step: 13, Remaining Steps: 37)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe syntax checker is consistently blocking the import. It seems I cannot insert `import math` due to this persistent warning, which is treated as an error by the environment's checker.\n\nLet's try a workaround for the cosine decay. Instead of using `math.pi`, I can approximate pi as `3.14159`. This avoids the need to import the `math` module.\n\nI will now modify the `get_lr` function (lines 412-424) to implement cosine decay after the linear warmup.\n\nThe new logic will be:\n1. Linear warmup for `warmup_iters` steps: `lr = (it + 1) / warmup_iters`\n2. Cosine decay for the remaining `num_iterations - warmup_iters` steps: `lr = 0.5 * (1 + cos(pi * (it - warmup_iters) / (num_iterations - warmup_iters)))`\n   - Note that the original `warmdown_iters` was set to `num_iterations - warmup_iters`, so I can reuse that variable.\n   - I'll use `torch.cos` instead of `math.cos`, and approximate pi.\n\nLet's edit the `get_lr` function.\n```bash\nedit 412:423\n    def get_lr(it):\n        assert it <= args.num_iterations\n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            return (it+1) / args.warmup_iters\n        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n        elif it > args.num_iterations:\n             return 0.0\n        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n        else:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n            return coeff # coeff starts at 1 and goes to 0\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe syntax checker is consistently blocking the import. It seems I cannot insert `import math` due to this persistent warning, which is treated as an error by the environment's checker.\n\nLet's try a workaround for the cosine decay. Instead of using `math.pi`, I can approximate pi as `3.14159`. This avoids the need to import the `math` module.\n\nI will now modify the `get_lr` function (lines 412-424) to implement cosine decay after the linear warmup.\n\nThe new logic will be:\n1. Linear warmup for `warmup_iters` steps: `lr = (it + 1) / warmup_iters`\n2. Cosine decay for the remaining `num_iterations - warmup_iters` steps: `lr = 0.5 * (1 + cos(pi * (it - warmup_iters) / (num_iterations - warmup_iters)))`\n   - Note that the original `warmdown_iters` was set to `num_iterations - warmup_iters`, so I can reuse that variable.\n   - I'll use `torch.cos` instead of `math.cos`, and approximate pi.\n\nLet's edit the `get_lr` function.\n",
      "action": "edit 412:423\n    def get_lr(it):\n        assert it <= args.num_iterations\n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            return (it+1) / args.warmup_iters\n        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n        elif it > args.num_iterations:\n             return 0.0\n        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n        else:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n            return coeff # coeff starts at 1 and goes to 0\nend_of_edit\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- E111 indentation is not a multiple of 4\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (513 lines total)]\n(406 more lines above)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n418:        elif it > args.num_iterations:\n419:             return 0.0\n420:        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n421:        else:\n422:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n423:            assert 0 <= decay_ratio <= 1\n424:            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n425:            return coeff # coeff starts at 1 and goes to 0\n426:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n427:\n428:    # begin logging\n429:    if master_process:\n430:        run_id = str(uuid.uuid4())\n(83 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(406 more lines above)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n(83 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n(Current Step: 14, Remaining Steps: 36)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAh, the indentation error `E111 indentation is not a multiple of 4` points to line 419 in the proposed change: `             return 0.0`. I added too many spaces there. It should be indented by 12 spaces (3 levels * 4 spaces), not 13.\n\nLet's fix the indentation and try the edit again.\n```bash\nedit 412:423\n    def get_lr(it):\n        assert it <= args.num_iterations\n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            return (it+1) / args.warmup_iters\n        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n        elif it > args.num_iterations:\n            return 0.0 # Fixed indentation\n        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n        else:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n            return coeff # coeff starts at 1 and goes to 0\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, the indentation error `E111 indentation is not a multiple of 4` points to line 419 in the proposed change: `             return 0.0`. I added too many spaces there. It should be indented by 12 spaces (3 levels * 4 spaces), not 13.\n\nLet's fix the indentation and try the edit again.\n",
      "action": "edit 412:423\n    def get_lr(it):\n        assert it <= args.num_iterations\n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            return (it+1) / args.warmup_iters\n        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n        elif it > args.num_iterations:\n            return 0.0 # Fixed indentation\n        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n        else:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n            return coeff # coeff starts at 1 and goes to 0\nend_of_edit\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (513 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 4800 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True, weight_decay=args.weight_decay)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) if it > num_iterations, return 0 (prevents slight overshoot)\n418:        elif it > args.num_iterations:\n419:            return 0.0 # Fixed indentation\n420:        # 3) cosine decay down to min learning rate (we'll set min_lr=0 here)\n421:        else:\n422:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n423:            assert 0 <= decay_ratio <= 1\n424:            coeff = 0.5 * (1.0 + torch.cos(torch.tensor(3.14159 * decay_ratio))) # cosine decay formula\n425:            return coeff # coeff starts at 1 and goes to 0\n426:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n427:\n428:    # begin logging\n429:    if master_process:\n430:        run_id = str(uuid.uuid4())\n431:        logdir = 'logs/%s/' % run_id\n432:        os.makedirs(logdir, exist_ok=True)\n433:        logfile = 'logs/%s.txt' % run_id\n434:        # create the log file\n435:        with open(logfile, \"w\") as f:\n436:            # begin the log by printing this file (the Python code)\n437:            f.write('='*100 + '\\n')\n438:            f.write(code)\n439:            f.write('='*100 + '\\n')\n440:            # log information about the hardware/software environment this is running on\n441:            # and print the full `nvidia-smi` to file\n442:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n443:            import subprocess\n444:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n445:            f.write(f'{result.stdout}\\n')\n446:            f.write('='*100 + '\\n')\n447:\n448:    training_time_ms = 0\n449:    # start the clock\n450:    torch.cuda.synchronize()\n451:    t0 = time.time()\n452:    # begin training\n453:    train_loader.reset()\n454:    for step in range(args.num_iterations + 1):\n455:        last_step = (step == args.num_iterations)\n456:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n457:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n458:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n459:        if step == 10:\n460:            training_time_ms = 0\n461:            t0 = time.time()\n462:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n463:\n464:        if last_step:\n465:            break\n466:\n467:        # --------------- TRAINING SECTION BEGIN -----------------\n468:        model.train()\n469:        for i in range(1, train_accumulation_steps+1):\n470:            # forward pass\n471:            with ctx:\n472:                _, loss = model(x, y, return_logits=False)\n473:                train_loss = loss.detach()\n474:            # advance the dataset for the next batch\n475:            x, y = train_loader.next_batch()\n476:            # backward pass\n477:            if i < train_accumulation_steps:\n478:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n479:                    loss.backward()\n480:            else:\n481:                loss.backward() # just sync on the last step\n482:        for p in model.parameters():\n483:            p.grad /= train_accumulation_steps\n484:        # step the optimizers and schedulers\n485:        for opt, sched in zip(optimizers, schedulers):\n486:            opt.step()\n487:            sched.step()\n488:        # null the gradients\n489:        model.zero_grad(set_to_none=True)\n490:        # --------------- TRAINING SECTION END -------------------\n491:        # everything that follows now is just diagnostics, prints, logging, etc.\n492:\n493:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n494:        if master_process:\n495:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n496:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n497:            with open(logfile, \"a\") as f:\n498:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n499:\n500:    torch.save(model.state_dict(), \"model.pt\")\n501:    with open(\"model_config.pt\", \"wb\") as f:\n502:        pickle.dump(raw_model.get_config(), f)\n503:    f.close()\n504:\n505:    if master_process:\n506:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n507:\n508:    # -------------------------------------------------------------------------\n509:    # clean up nice\n510:    dist.destroy_process_group()\n511:\n512:if __name__ == \"__main__\":\n513:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 15, Remaining Steps: 35)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nGreat, the indentation is fixed, and the `get_lr` function now implements linear warmup followed by cosine decay without needing the `math` module.\n\nSummary of changes made so far:\n1. Corrected `input_bin` path in `Hyperparameters` (line 352).\n2. Increased `num_iterations` from 500 to 5000 (line 357).\n3. Set `warmup_iters` to 200 (line 358).\n4. Set `warmdown_iters` to 4800 (line 359).\n5. Set `weight_decay` to 0.1 (line 360).\n6. Applied `weight_decay` to Adam optimizers for embeddings and LM head (lines 407-408).\n7. Modified `get_lr` function for cosine decay after warmup (lines 412-425).\n\nThe script should now be ready to run. I'll use `torchrun` as suggested in the file's docstring. I'll check the number of available GPUs first.\n```bash\nnvidia-smi --list-gpus\n```",
      "thought": "DISCUSSION\nGreat, the indentation is fixed, and the `get_lr` function now implements linear warmup followed by cosine decay without needing the `math` module.\n\nSummary of changes made so far:\n1. Corrected `input_bin` path in `Hyperparameters` (line 352).\n2. Increased `num_iterations` from 500 to 5000 (line 357).\n3. Set `warmup_iters` to 200 (line 358).\n4. Set `warmdown_iters` to 4800 (line 359).\n5. Set `weight_decay` to 0.1 (line 360).\n6. Applied `weight_decay` to Adam optimizers for embeddings and LM head (lines 407-408).\n7. Modified `get_lr` function for cosine decay after warmup (lines 412-425).\n\nThe script should now be ready to run. I'll use `torchrun` as suggested in the file's docstring. I'll check the number of available GPUs first.\n",
      "action": "nvidia-smi --list-gpus\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "GPU 0: NVIDIA H100 (UUID: GPU-db639abc-7f7c-0bfe-de79-f125bb6345f9)\nGPU 1: NVIDIA H100 (UUID: GPU-24f492f4-6b2d-dc91-c146-25b876718d6a)\n(Current Step: 16, Remaining Steps: 34)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThere are 2 GPUs available (NVIDIA H100). I will use `torchrun` with `--nproc_per_node=2`.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThere are 2 GPUs available (NVIDIA H100). I will use `torchrun` with `--nproc_per_node=2`.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "W0326 03:22:12.988000 1197 site-packages/torch/distributed/run.py:793] \nW0326 03:22:12.988000 1197 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:22:12.988000 1197 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:22:12.988000 1197 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\nusing device: cuda:1\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nstep:1/5000 train_loss:10.8258 train_time:69556ms step_avg:nanms\nstep:2/5000 train_loss:10.8233 train_time:82818ms step_avg:nanms\nstep:3/5000 train_loss:10.8225 train_time:82998ms step_avg:nanms\nstep:4/5000 train_loss:10.8205 train_time:83210ms step_avg:nanms\nstep:5/5000 train_loss:10.8160 train_time:83388ms step_avg:nanms\nstep:6/5000 train_loss:10.8110 train_time:83567ms step_avg:nanms\nstep:7/5000 train_loss:10.8049 train_time:83745ms step_avg:nanms\nstep:8/5000 train_loss:10.8008 train_time:83923ms step_avg:nanms\nstep:9/5000 train_loss:10.7933 train_time:84106ms step_avg:nanms\nstep:10/5000 train_loss:10.7872 train_time:84286ms step_avg:nanms\nstep:11/5000 train_loss:10.7793 train_time:173ms step_avg:nanms\nstep:12/5000 train_loss:10.7727 train_time:352ms step_avg:nanms\nstep:13/5000 train_loss:10.7610 train_time:530ms step_avg:176.77ms\nstep:14/5000 train_loss:10.7465 train_time:709ms step_avg:177.19ms\nstep:15/5000 train_loss:10.7398 train_time:887ms step_avg:177.36ms\nstep:16/5000 train_loss:10.7183 train_time:1065ms step_avg:177.44ms\nstep:17/5000 train_loss:10.7199 train_time:1244ms step_avg:177.71ms\nstep:18/5000 train_loss:10.6991 train_time:1422ms step_avg:177.75ms\nstep:19/5000 train_loss:10.6820 train_time:1601ms step_avg:177.91ms\nstep:20/5000 train_loss:10.6628 train_time:1781ms step_avg:178.08ms\nstep:21/5000 train_loss:10.6472 train_time:1961ms step_avg:178.30ms\nstep:22/5000 train_loss:10.6146 train_time:2141ms step_avg:178.39ms\nstep:23/5000 train_loss:10.5821 train_time:2321ms step_avg:178.56ms\nstep:24/5000 train_loss:10.5631 train_time:2500ms step_avg:178.60ms\nstep:25/5000 train_loss:10.5514 train_time:2680ms step_avg:178.68ms\nstep:26/5000 train_loss:10.5573 train_time:2860ms step_avg:178.74ms\nstep:27/5000 train_loss:10.4870 train_time:3040ms step_avg:178.82ms\nstep:28/5000 train_loss:10.4739 train_time:3219ms step_avg:178.86ms\nstep:29/5000 train_loss:10.4138 train_time:3400ms step_avg:178.96ms\nstep:30/5000 train_loss:10.3842 train_time:3579ms step_avg:178.95ms\nstep:31/5000 train_loss:10.2996 train_time:3761ms step_avg:179.07ms\nstep:32/5000 train_loss:10.1998 train_time:3941ms step_avg:179.13ms\nstep:33/5000 train_loss:10.2253 train_time:4121ms step_avg:179.19ms\nstep:34/5000 train_loss:10.1401 train_time:4300ms step_avg:179.15ms\nstep:35/5000 train_loss:10.0779 train_time:4478ms step_avg:179.14ms\nstep:36/5000 train_loss:10.0597 train_time:4658ms step_avg:179.17ms\nstep:37/5000 train_loss:9.8779 train_time:4840ms step_avg:179.25ms\nstep:38/5000 train_loss:9.8049 train_time:5019ms step_avg:179.26ms\nstep:39/5000 train_loss:9.6659 train_time:5200ms step_avg:179.31ms\nstep:40/5000 train_loss:9.6141 train_time:5380ms step_avg:179.32ms\nstep:41/5000 train_loss:9.4872 train_time:5559ms step_avg:179.32ms\nstep:42/5000 train_loss:9.3625 train_time:5740ms step_avg:179.38ms\nstep:43/5000 train_loss:9.2585 train_time:5920ms step_avg:179.40ms\nstep:44/5000 train_loss:9.0890 train_time:6101ms step_avg:179.43ms\nstep:45/5000 train_loss:8.8802 train_time:6280ms step_avg:179.44ms\nstep:46/5000 train_loss:8.7105 train_time:6458ms step_avg:179.40ms\nstep:47/5000 train_loss:8.7046 train_time:6639ms step_avg:179.44ms\nstep:48/5000 train_loss:8.5218 train_time:6819ms step_avg:179.45ms\nstep:49/5000 train_loss:8.3837 train_time:6999ms step_avg:179.47ms\nstep:50/5000 train_loss:8.2899 train_time:7178ms step_avg:179.46ms\nstep:51/5000 train_loss:8.3484 train_time:7356ms step_avg:179.43ms\nstep:52/5000 train_loss:8.1135 train_time:7537ms step_avg:179.44ms\nstep:53/5000 train_loss:8.1684 train_time:7716ms step_avg:179.45ms\nstep:54/5000 train_loss:8.0695 train_time:7898ms step_avg:179.49ms\nstep:55/5000 train_loss:7.8526 train_time:8079ms step_avg:179.53ms\nstep:56/5000 train_loss:8.0308 train_time:8258ms step_avg:179.52ms\nstep:57/5000 train_loss:7.9780 train_time:8437ms step_avg:179.51ms\nstep:58/5000 train_loss:8.0499 train_time:8619ms step_avg:179.57ms\nstep:59/5000 train_loss:8.0066 train_time:8799ms step_avg:179.57ms\nstep:60/5000 train_loss:8.0698 train_time:8978ms step_avg:179.56ms\nstep:61/5000 train_loss:8.1078 train_time:9159ms step_avg:179.58ms\nstep:62/5000 train_loss:8.0921 train_time:9339ms step_avg:179.59ms\nstep:63/5000 train_loss:7.9790 train_time:9519ms step_avg:179.60ms\nstep:64/5000 train_loss:7.9493 train_time:9700ms step_avg:179.64ms\nstep:65/5000 train_loss:8.1164 train_time:9881ms step_avg:179.65ms\nstep:66/5000 train_loss:8.0881 train_time:10059ms step_avg:179.63ms\nstep:67/5000 train_loss:7.9110 train_time:10239ms step_avg:179.63ms\nstep:68/5000 train_loss:7.9575 train_time:10419ms step_avg:179.64ms\nstep:69/5000 train_loss:8.1063 train_time:10600ms step_avg:179.65ms\nstep:70/5000 train_loss:8.0476 train_time:10779ms step_avg:179.64ms\nstep:71/5000 train_loss:8.0070 train_time:10959ms step_avg:179.65ms\nstep:72/5000 train_loss:8.2204 train_time:11139ms step_avg:179.67ms\nstep:73/5000 train_loss:8.0682 train_time:11320ms step_avg:179.68ms\nstep:74/5000 train_loss:8.1094 train_time:11500ms step_avg:179.69ms\nstep:75/5000 train_loss:8.0748 train_time:11680ms step_avg:179.69ms\nstep:76/5000 train_loss:8.0415 train_time:11859ms step_avg:179.68ms\nstep:77/5000 train_loss:8.0572 train_time:12038ms step_avg:179.67ms\nstep:78/5000 train_loss:7.8789 train_time:12219ms step_avg:179.69ms\nstep:79/5000 train_loss:8.0626 train_time:12399ms step_avg:179.69ms\nstep:80/5000 train_loss:8.1430 train_time:12578ms step_avg:179.69ms\nstep:81/5000 train_loss:8.0478 train_time:12759ms step_avg:179.70ms\nstep:82/5000 train_loss:8.1208 train_time:12939ms step_avg:179.71ms\nstep:83/5000 train_loss:7.9742 train_time:13119ms step_avg:179.71ms\nstep:84/5000 train_loss:7.9889 train_time:13298ms step_avg:179.70ms\nstep:85/5000 train_loss:8.1102 train_time:13476ms step_avg:179.69ms\nstep:86/5000 train_loss:8.2995 train_time:13656ms step_avg:179.69ms\nstep:87/5000 train_loss:8.1420 train_time:13837ms step_avg:179.70ms\nstep:88/5000 train_loss:7.9245 train_time:14018ms step_avg:179.72ms\nstep:89/5000 train_loss:8.0186 train_time:14198ms step_avg:179.72ms\nstep:90/5000 train_loss:8.0361 train_time:14379ms step_avg:179.73ms\nstep:91/5000 train_loss:7.9362 train_time:14560ms step_avg:179.75ms\nstep:92/5000 train_loss:7.9485 train_time:14740ms step_avg:179.76ms\nstep:93/5000 train_loss:7.9140 train_time:14921ms step_avg:179.77ms\nstep:94/5000 train_loss:8.1643 train_time:15101ms step_avg:179.77ms\nstep:95/5000 train_loss:8.0374 train_time:15280ms step_avg:179.76ms\nstep:96/5000 train_loss:7.9565 train_time:15458ms step_avg:179.75ms\nstep:97/5000 train_loss:7.8286 train_time:15638ms step_avg:179.74ms\nstep:98/5000 train_loss:7.9245 train_time:15819ms step_avg:179.76ms\nstep:99/5000 train_loss:7.8340 train_time:16001ms step_avg:179.79ms\nstep:100/5000 train_loss:7.7661 train_time:16180ms step_avg:179.78ms\nstep:101/5000 train_loss:7.9259 train_time:16359ms step_avg:179.77ms\nstep:102/5000 train_loss:7.8599 train_time:16539ms step_avg:179.77ms\nstep:103/5000 train_loss:7.9302 train_time:16719ms step_avg:179.77ms\nstep:104/5000 train_loss:7.9758 train_time:16899ms step_avg:179.78ms\nstep:105/5000 train_loss:7.9437 train_time:17079ms step_avg:179.78ms\nstep:106/5000 train_loss:7.9383 train_time:17258ms step_avg:179.77ms\nstep:107/5000 train_loss:7.9535 train_time:17439ms step_avg:179.78ms\nstep:108/5000 train_loss:7.8914 train_time:17620ms step_avg:179.80ms\nstep:109/5000 train_loss:8.0056 train_time:17799ms step_avg:179.79ms\nstep:110/5000 train_loss:7.7673 train_time:17979ms step_avg:179.79ms\nstep:111/5000 train_loss:7.7895 train_time:18160ms step_avg:179.80ms\nstep:112/5000 train_loss:7.9694 train_time:18341ms step_avg:179.82ms\nstep:113/5000 train_loss:7.9283 train_time:18522ms step_avg:179.82ms\nstep:114/5000 train_loss:7.9549 train_time:18701ms step_avg:179.82ms\nstep:115/5000 train_loss:7.9233 train_time:18880ms step_avg:179.81ms\nstep:116/5000 train_loss:7.8673 train_time:19060ms step_avg:179.81ms\nstep:117/5000 train_loss:7.8875 train_time:19240ms step_avg:179.81ms\nstep:118/5000 train_loss:7.8799 train_time:19420ms step_avg:179.81ms\nstep:119/5000 train_loss:7.9305 train_time:19600ms step_avg:179.82ms\nstep:120/5000 train_loss:7.9713 train_time:19780ms step_avg:179.82ms\nstep:121/5000 train_loss:7.8418 train_time:19960ms step_avg:179.82ms\nstep:122/5000 train_loss:7.8371 train_time:20140ms step_avg:179.82ms\nstep:123/5000 train_loss:7.8015 train_time:20321ms step_avg:179.83ms\nstep:124/5000 train_loss:8.2812 train_time:20501ms step_avg:179.84ms\nstep:125/5000 train_loss:7.8842 train_time:20682ms step_avg:179.84ms\nstep:126/5000 train_loss:7.7176 train_time:20861ms step_avg:179.84ms\nstep:127/5000 train_loss:7.8279 train_time:21042ms step_avg:179.84ms\nstep:128/5000 train_loss:7.8151 train_time:21223ms step_avg:179.85ms\nstep:129/5000 train_loss:8.0201 train_time:21403ms step_avg:179.86ms\nstep:130/5000 train_loss:8.0946 train_time:21583ms step_avg:179.86ms\nstep:131/5000 train_loss:7.8869 train_time:21764ms step_avg:179.87ms\nstep:132/5000 train_loss:7.8217 train_time:21943ms step_avg:179.86ms\nstep:133/5000 train_loss:7.9486 train_time:22123ms step_avg:179.86ms\nstep:134/5000 train_loss:7.9448 train_time:22302ms step_avg:179.85ms\nstep:135/5000 train_loss:7.9736 train_time:22481ms step_avg:179.85ms\nstep:136/5000 train_loss:7.9140 train_time:22661ms step_avg:179.85ms\nstep:137/5000 train_loss:7.9139 train_time:22841ms step_avg:179.85ms\nstep:138/5000 train_loss:8.0513 train_time:23021ms step_avg:179.86ms\nstep:139/5000 train_loss:7.9086 train_time:23202ms step_avg:179.86ms\nstep:140/5000 train_loss:8.0000 train_time:23383ms step_avg:179.87ms\nstep:141/5000 train_loss:7.9912 train_time:23562ms step_avg:179.86ms\nstep:142/5000 train_loss:8.1157 train_time:23741ms step_avg:179.86ms\nstep:143/5000 train_loss:7.9005 train_time:23921ms step_avg:179.86ms\nstep:144/5000 train_loss:8.0213 train_time:24101ms step_avg:179.86ms\nstep:145/5000 train_loss:7.9378 train_time:24281ms step_avg:179.86ms\nstep:146/5000 train_loss:7.9790 train_time:24461ms step_avg:179.86ms\nstep:147/5000 train_loss:7.8837 train_time:24641ms step_avg:179.86ms\nstep:148/5000 train_loss:8.0025 train_time:24820ms step_avg:179.86ms\nstep:149/5000 train_loss:7.7531 train_time:25002ms step_avg:179.87ms\nstep:150/5000 train_loss:7.9043 train_time:25182ms step_avg:179.87ms\nstep:151/5000 train_loss:7.9220 train_time:25362ms step_avg:179.87ms\nstep:152/5000 train_loss:7.8770 train_time:25542ms step_avg:179.88ms\nstep:153/5000 train_loss:8.0829 train_time:25722ms step_avg:179.87ms\nstep:154/5000 train_loss:7.9699 train_time:25903ms step_avg:179.88ms\nstep:155/5000 train_loss:7.8578 train_time:26083ms step_avg:179.88ms\nstep:156/5000 train_loss:7.9968 train_time:26262ms step_avg:179.88ms\nstep:157/5000 train_loss:7.9693 train_time:26444ms step_avg:179.89ms\nstep:158/5000 train_loss:7.9149 train_time:26624ms step_avg:179.89ms\nstep:159/5000 train_loss:7.9568 train_time:26804ms step_avg:179.90ms\nstep:160/5000 train_loss:7.9241 train_time:26984ms step_avg:179.90ms\nstep:161/5000 train_loss:8.0557 train_time:27163ms step_avg:179.89ms\nstep:162/5000 train_loss:7.9701 train_time:27344ms step_avg:179.89ms\nstep:163/5000 train_loss:7.8661 train_time:27524ms step_avg:179.89ms\nstep:164/5000 train_loss:8.0014 train_time:27703ms step_avg:179.89ms\nstep:165/5000 train_loss:7.9080 train_time:27883ms step_avg:179.89ms\nstep:166/5000 train_loss:7.8203 train_time:28064ms step_avg:179.90ms\nstep:167/5000 train_loss:7.8298 train_time:28245ms step_avg:179.90ms\nstep:168/5000 train_loss:7.9972 train_time:28426ms step_avg:179.91ms\nstep:169/5000 train_loss:8.0641 train_time:28605ms step_avg:179.91ms\nstep:170/5000 train_loss:7.9662 train_time:28785ms step_avg:179.90ms\nstep:171/5000 train_loss:8.1216 train_time:28964ms step_avg:179.90ms\nstep:172/5000 train_loss:7.9872 train_time:29144ms step_avg:179.90ms\nstep:173/5000 train_loss:7.9755 train_time:29324ms step_avg:179.90ms\nstep:174/5000 train_loss:7.9314 train_time:29504ms step_avg:179.91ms\nstep:175/5000 train_loss:7.8604 train_time:29685ms step_avg:179.91ms\nstep:176/5000 train_loss:7.9756 train_time:29865ms step_avg:179.91ms\nstep:177/5000 train_loss:7.9584 train_time:30045ms step_avg:179.91ms\nstep:178/5000 train_loss:8.1396 train_time:30225ms step_avg:179.91ms\nstep:179/5000 train_loss:8.0763 train_time:30405ms step_avg:179.91ms\nstep:180/5000 train_loss:7.8603 train_time:30585ms step_avg:179.91ms\nstep:181/5000 train_loss:7.8493 train_time:30765ms step_avg:179.91ms\nstep:182/5000 train_loss:7.8968 train_time:30946ms step_avg:179.92ms\nstep:183/5000 train_loss:8.0190 train_time:31126ms step_avg:179.92ms\nstep:184/5000 train_loss:7.9418 train_time:31306ms step_avg:179.92ms\nstep:185/5000 train_loss:8.0240 train_time:31486ms step_avg:179.92ms\nstep:186/5000 train_loss:7.8829 train_time:31665ms step_avg:179.92ms\nstep:187/5000 train_loss:8.1026 train_time:31845ms step_avg:179.92ms\nstep:188/5000 train_loss:7.9103 train_time:32024ms step_avg:179.91ms\nstep:189/5000 train_loss:8.0200 train_time:32203ms step_avg:179.91ms\nstep:190/5000 train_loss:8.0951 train_time:32384ms step_avg:179.91ms\nstep:191/5000 train_loss:7.9195 train_time:32565ms step_avg:179.92ms\nstep:192/5000 train_loss:8.0568 train_time:32744ms step_avg:179.91ms\nstep:193/5000 train_loss:7.9245 train_time:32924ms step_avg:179.91ms\nstep:194/5000 train_loss:8.0038 train_time:33104ms step_avg:179.91ms\nstep:195/5000 train_loss:7.9911 train_time:33283ms step_avg:179.91ms\nstep:196/5000 train_loss:7.7974 train_time:33463ms step_avg:179.91ms\nstep:197/5000 train_loss:7.9764 train_time:33645ms step_avg:179.92ms\nstep:198/5000 train_loss:8.0633 train_time:33825ms step_avg:179.92ms\nstep:199/5000 train_loss:7.9361 train_time:34006ms step_avg:179.93ms\nstep:200/5000 train_loss:7.9662 train_time:34186ms step_avg:179.92ms\nstep:201/5000 train_loss:7.9242 train_time:34365ms step_avg:179.92ms\nstep:202/5000 train_loss:7.9507 train_time:34545ms step_avg:179.92ms\nstep:203/5000 train_loss:7.6601 train_time:34726ms step_avg:179.92ms\nstep:204/5000 train_loss:8.0219 train_time:34908ms step_avg:179.94ms\nstep:205/5000 train_loss:7.9905 train_time:35087ms step_avg:179.93ms\nstep:206/5000 train_loss:7.8736 train_time:35267ms step_avg:179.93ms\nstep:207/5000 train_loss:7.9172 train_time:35447ms step_avg:179.93ms\nstep:208/5000 train_loss:8.0441 train_time:35627ms step_avg:179.94ms\nstep:209/5000 train_loss:8.0173 train_time:35808ms step_avg:179.94ms\nstep:210/5000 train_loss:7.9574 train_time:35989ms step_avg:179.94ms\nstep:211/5000 train_loss:8.0025 train_time:36168ms step_avg:179.94ms\nstep:212/5000 train_loss:7.9812 train_time:36348ms step_avg:179.94ms\nstep:213/5000 train_loss:8.1202 train_time:36528ms step_avg:179.94ms\nstep:214/5000 train_loss:7.9754 train_time:36708ms step_avg:179.94ms\nstep:215/5000 train_loss:7.9439 train_time:36887ms step_avg:179.94ms\nstep:216/5000 train_loss:7.9892 train_time:37068ms step_avg:179.94ms\nstep:217/5000 train_loss:7.8952 train_time:37248ms step_avg:179.94ms\nstep:218/5000 train_loss:8.6761 train_time:37427ms step_avg:179.94ms\nstep:219/5000 train_loss:8.0945 train_time:37607ms step_avg:179.94ms\nstep:220/5000 train_loss:8.1081 train_time:37787ms step_avg:179.94ms\nstep:221/5000 train_loss:8.0427 train_time:37966ms step_avg:179.93ms\nstep:222/5000 train_loss:8.0369 train_time:38145ms step_avg:179.93ms\nstep:223/5000 train_loss:8.0061 train_time:38325ms step_avg:179.93ms\nstep:224/5000 train_loss:7.9660 train_time:38504ms step_avg:179.93ms\nstep:225/5000 train_loss:8.0086 train_time:38684ms step_avg:179.93ms\nstep:226/5000 train_loss:7.9409 train_time:38863ms step_avg:179.92ms\nstep:227/5000 train_loss:8.0667 train_time:39043ms step_avg:179.92ms\nstep:228/5000 train_loss:7.9794 train_time:39222ms step_avg:179.92ms\nstep:229/5000 train_loss:7.9887 train_time:39401ms step_avg:179.91ms\nstep:230/5000 train_loss:8.0470 train_time:39581ms step_avg:179.91ms\nstep:231/5000 train_loss:8.1044 train_time:39760ms step_avg:179.91ms\nstep:232/5000 train_loss:7.9940 train_time:39941ms step_avg:179.91ms\nstep:233/5000 train_loss:7.9380 train_time:40121ms step_avg:179.92ms\nstep:234/5000 train_loss:7.9426 train_time:40301ms step_avg:179.91ms\nstep:235/5000 train_loss:7.9603 train_time:40482ms step_avg:179.92ms\nstep:236/5000 train_loss:7.9972 train_time:40662ms step_avg:179.92ms\nstep:237/5000 train_loss:8.0234 train_time:40842ms step_avg:179.92ms\nstep:238/5000 train_loss:8.0128 train_time:41022ms step_avg:179.92ms\nstep:239/5000 train_loss:8.0631 train_time:41202ms step_avg:179.92ms\nstep:240/5000 train_loss:7.9622 train_time:41383ms step_avg:179.92ms\nstep:241/5000 train_loss:8.0693 train_time:41562ms step_avg:179.92ms\nstep:242/5000 train_loss:7.9278 train_time:41741ms step_avg:179.92ms\nstep:243/5000 train_loss:7.9651 train_time:41921ms step_avg:179.92ms\nstep:244/5000 train_loss:7.9116 train_time:42102ms step_avg:179.92ms\nstep:245/5000 train_loss:8.0022 train_time:42282ms step_avg:179.92ms\nstep:246/5000 train_loss:7.9878 train_time:42464ms step_avg:179.93ms\nstep:247/5000 train_loss:7.9167 train_time:42644ms step_avg:179.93ms\nstep:248/5000 train_loss:8.0182 train_time:42822ms step_avg:179.92ms\nstep:249/5000 train_loss:8.1047 train_time:43000ms step_avg:179.92ms\nstep:250/5000 train_loss:7.9887 train_time:43180ms step_avg:179.92ms\nstep:251/5000 train_loss:7.9466 train_time:43361ms step_avg:179.92ms\nstep:252/5000 train_loss:7.9494 train_time:43542ms step_avg:179.92ms\nstep:253/5000 train_loss:8.0333 train_time:43722ms step_avg:179.93ms\nstep:254/5000 train_loss:8.5443 train_time:43901ms step_avg:179.92ms\nstep:255/5000 train_loss:8.0154 train_time:44083ms step_avg:179.93ms\nstep:256/5000 train_loss:8.0247 train_time:44262ms step_avg:179.93ms\nstep:257/5000 train_loss:8.1124 train_time:44441ms step_avg:179.92ms\nstep:258/5000 train_loss:7.9306 train_time:44620ms step_avg:179.92ms\nstep:259/5000 train_loss:8.0388 train_time:44800ms step_avg:179.92ms\nstep:260/5000 train_loss:7.8269 train_time:44980ms step_avg:179.92ms\nstep:261/5000 train_loss:8.0351 train_time:45160ms step_avg:179.92ms\nstep:262/5000 train_loss:7.9931 train_time:45340ms step_avg:179.92ms\nstep:263/5000 train_loss:7.9554 train_time:45522ms step_avg:179.93ms\nstep:264/5000 train_loss:7.9580 train_time:45702ms step_avg:179.93ms\nstep:265/5000 train_loss:8.0023 train_time:45882ms step_avg:179.93ms\nstep:266/5000 train_loss:7.9210 train_time:46062ms step_avg:179.93ms\nstep:267/5000 train_loss:7.9491 train_time:46242ms step_avg:179.93ms\nstep:268/5000 train_loss:8.0597 train_time:46423ms step_avg:179.93ms\nstep:269/5000 train_loss:8.0740 train_time:46602ms step_avg:179.93ms\nstep:270/5000 train_loss:8.1651 train_time:46781ms step_avg:179.93ms\nstep:271/5000 train_loss:7.6984 train_time:46961ms step_avg:179.93ms\nstep:272/5000 train_loss:8.0742 train_time:47142ms step_avg:179.93ms\nstep:273/5000 train_loss:8.0888 train_time:47321ms step_avg:179.93ms\nstep:274/5000 train_loss:7.8537 train_time:47502ms step_avg:179.93ms\nstep:275/5000 train_loss:7.8804 train_time:47681ms step_avg:179.93ms\nstep:276/5000 train_loss:8.0449 train_time:47860ms step_avg:179.92ms\nstep:277/5000 train_loss:7.9429 train_time:48040ms step_avg:179.93ms\nstep:278/5000 train_loss:7.7610 train_time:48221ms step_avg:179.93ms\nstep:279/5000 train_loss:7.9962 train_time:48401ms step_avg:179.93ms\nstep:280/5000 train_loss:7.9348 train_time:48580ms step_avg:179.93ms\nstep:281/5000 train_loss:8.0783 train_time:48760ms step_avg:179.93ms\nstep:282/5000 train_loss:8.0787 train_time:48941ms step_avg:179.93ms\nstep:283/5000 train_loss:7.8861 train_time:49121ms step_avg:179.93ms\nstep:284/5000 train_loss:8.0712 train_time:49302ms step_avg:179.93ms\nstep:285/5000 train_loss:8.2041 train_time:49482ms step_avg:179.93ms\nstep:286/5000 train_loss:8.0921 train_time:49663ms step_avg:179.94ms\nstep:287/5000 train_loss:8.1570 train_time:49844ms step_avg:179.94ms\nstep:288/5000 train_loss:8.0450 train_time:50024ms step_avg:179.94ms\nstep:289/5000 train_loss:7.9958 train_time:50204ms step_avg:179.94ms\nstep:290/5000 train_loss:8.0465 train_time:50384ms step_avg:179.94ms\nstep:291/5000 train_loss:8.1118 train_time:50562ms step_avg:179.94ms\nstep:292/5000 train_loss:8.0559 train_time:50742ms step_avg:179.94ms\nstep:293/5000 train_loss:8.0115 train_time:50921ms step_avg:179.93ms\nstep:294/5000 train_loss:8.0582 train_time:51101ms step_avg:179.93ms\nstep:295/5000 train_loss:7.8522 train_time:51282ms step_avg:179.94ms\nstep:296/5000 train_loss:7.8504 train_time:51462ms step_avg:179.94ms\nstep:297/5000 train_loss:8.0156 train_time:51642ms step_avg:179.94ms\nstep:298/5000 train_loss:8.0424 train_time:51822ms step_avg:179.94ms\nstep:299/5000 train_loss:7.9546 train_time:52002ms step_avg:179.94ms\nstep:300/5000 train_loss:7.9285 train_time:52181ms step_avg:179.94ms\nstep:301/5000 train_loss:8.0093 train_time:52361ms step_avg:179.94ms\nstep:302/5000 train_loss:8.0037 train_time:52542ms step_avg:179.94ms\nstep:303/5000 train_loss:7.8843 train_time:52722ms step_avg:179.94ms\nstep:304/5000 train_loss:8.0207 train_time:52901ms step_avg:179.94ms\nstep:305/5000 train_loss:8.0200 train_time:53081ms step_avg:179.94ms\nstep:306/5000 train_loss:8.0563 train_time:53261ms step_avg:179.94ms\nstep:307/5000 train_loss:8.1175 train_time:53443ms step_avg:179.94ms\nstep:308/5000 train_loss:8.1520 train_time:53623ms step_avg:179.94ms\nstep:309/5000 train_loss:8.0921 train_time:53804ms step_avg:179.95ms\nstep:310/5000 train_loss:8.0754 train_time:53984ms step_avg:179.95ms\nstep:311/5000 train_loss:8.0551 train_time:54163ms step_avg:179.94ms\nstep:312/5000 train_loss:8.0261 train_time:54344ms step_avg:179.95ms\nstep:313/5000 train_loss:8.0942 train_time:54524ms step_avg:179.95ms\nstep:314/5000 train_loss:8.1276 train_time:54705ms step_avg:179.95ms\nstep:315/5000 train_loss:7.9238 train_time:54885ms step_avg:179.95ms\nstep:316/5000 train_loss:7.6309 train_time:55064ms step_avg:179.95ms\nstep:317/5000 train_loss:8.0370 train_time:55243ms step_avg:179.94ms\nstep:318/5000 train_loss:8.0849 train_time:55422ms step_avg:179.94ms\nstep:319/5000 train_loss:7.8964 train_time:55603ms step_avg:179.95ms\nstep:320/5000 train_loss:7.9887 train_time:55783ms step_avg:179.95ms\nstep:321/5000 train_loss:8.0113 train_time:55964ms step_avg:179.95ms\nstep:322/5000 train_loss:7.9186 train_time:56145ms step_avg:179.95ms\nstep:323/5000 train_loss:8.1447 train_time:56325ms step_avg:179.95ms\nstep:324/5000 train_loss:8.1020 train_time:56505ms step_avg:179.95ms\nstep:325/5000 train_loss:8.0530 train_time:56685ms step_avg:179.95ms\nstep:326/5000 train_loss:8.0886 train_time:56864ms step_avg:179.95ms\nstep:327/5000 train_loss:8.0837 train_time:57045ms step_avg:179.95ms\nstep:328/5000 train_loss:7.9984 train_time:57226ms step_avg:179.95ms\nstep:329/5000 train_loss:8.1949 train_time:57406ms step_avg:179.95ms\nstep:330/5000 train_loss:8.0956 train_time:57586ms step_avg:179.95ms\nstep:331/5000 train_loss:7.9705 train_time:57764ms step_avg:179.95ms\nstep:332/5000 train_loss:8.1021 train_time:57944ms step_avg:179.95ms\nstep:333/5000 train_loss:8.1728 train_time:58123ms step_avg:179.95ms\nstep:334/5000 train_loss:8.0288 train_time:58304ms step_avg:179.95ms\nstep:335/5000 train_loss:8.0257 train_time:58484ms step_avg:179.95ms\nstep:336/5000 train_loss:8.0745 train_time:58663ms step_avg:179.95ms\nstep:337/5000 train_loss:8.1377 train_time:58843ms step_avg:179.95ms\nstep:338/5000 train_loss:7.9419 train_time:59024ms step_avg:179.95ms\nstep:339/5000 train_loss:7.9577 train_time:59204ms step_avg:179.95ms\nstep:340/5000 train_loss:8.1015 train_time:59384ms step_avg:179.95ms\nstep:341/5000 train_loss:8.0835 train_time:59564ms step_avg:179.95ms\nstep:342/5000 train_loss:8.0580 train_time:59745ms step_avg:179.95ms\nstep:343/5000 train_loss:8.0027 train_time:59925ms step_avg:179.95ms\nstep:344/5000 train_loss:8.0523 train_time:60104ms step_avg:179.95ms\nstep:345/5000 train_loss:8.0090 train_time:60284ms step_avg:179.95ms\nstep:346/5000 train_loss:8.0726 train_time:60464ms step_avg:179.95ms\nstep:347/5000 train_loss:8.0746 train_time:60645ms step_avg:179.95ms\nstep:348/5000 train_loss:8.0917 train_time:60825ms step_avg:179.95ms\nstep:349/5000 train_loss:7.9847 train_time:61005ms step_avg:179.96ms\nstep:350/5000 train_loss:8.1651 train_time:61185ms step_avg:179.96ms\nstep:351/5000 train_loss:7.9611 train_time:61363ms step_avg:179.95ms\nstep:352/5000 train_loss:7.9630 train_time:61546ms step_avg:179.96ms\nstep:353/5000 train_loss:7.8817 train_time:61723ms step_avg:179.95ms\nstep:354/5000 train_loss:8.0548 train_time:61903ms step_avg:179.95ms\nstep:355/5000 train_loss:7.9682 train_time:62084ms step_avg:179.95ms\nstep:356/5000 train_loss:7.9507 train_time:62263ms step_avg:179.95ms\nstep:357/5000 train_loss:7.7854 train_time:62443ms step_avg:179.95ms\nstep:358/5000 train_loss:8.0267 train_time:62624ms step_avg:179.95ms\nstep:359/5000 train_loss:8.0704 train_time:62804ms step_avg:179.95ms\nstep:360/5000 train_loss:7.8218 train_time:62984ms step_avg:179.95ms\nstep:361/5000 train_loss:7.9013 train_time:63163ms step_avg:179.95ms\nstep:362/5000 train_loss:8.0140 train_time:63342ms step_avg:179.95ms\nstep:363/5000 train_loss:8.2278 train_time:63523ms step_avg:179.95ms\nstep:364/5000 train_loss:8.1140 train_time:63701ms step_avg:179.95ms\nstep:365/5000 train_loss:8.2261 train_time:63883ms step_avg:179.95ms\nstep:366/5000 train_loss:8.2434 train_time:64063ms step_avg:179.95ms\nstep:367/5000 train_loss:8.2956 train_time:64243ms step_avg:179.95ms\nstep:368/5000 train_loss:8.0976 train_time:64422ms step_avg:179.95ms\nstep:369/5000 train_loss:8.1902 train_time:64603ms step_avg:179.95ms\nstep:370/5000 train_loss:8.1067 train_time:64783ms step_avg:179.95ms\nstep:371/5000 train_loss:8.0725 train_time:64962ms step_avg:179.95ms\nstep:372/5000 train_loss:8.2894 train_time:65144ms step_avg:179.96ms\nstep:373/5000 train_loss:8.0665 train_time:65325ms step_avg:179.96ms\nstep:374/5000 train_loss:8.0630 train_time:65505ms step_avg:179.96ms\nstep:375/5000 train_loss:8.0711 train_time:65684ms step_avg:179.96ms\nstep:376/5000 train_loss:8.1916 train_time:65864ms step_avg:179.96ms\nstep:377/5000 train_loss:8.0050 train_time:66044ms step_avg:179.96ms\nstep:378/5000 train_loss:8.0898 train_time:66226ms step_avg:179.96ms\nstep:379/5000 train_loss:8.1200 train_time:66406ms step_avg:179.96ms\nstep:380/5000 train_loss:7.9800 train_time:66585ms step_avg:179.96ms\nstep:381/5000 train_loss:8.1642 train_time:66764ms step_avg:179.96ms\nstep:382/5000 train_loss:7.9411 train_time:66943ms step_avg:179.95ms\nstep:383/5000 train_loss:8.0723 train_time:67122ms step_avg:179.95ms\nstep:384/5000 train_loss:8.0409 train_time:67304ms step_avg:179.96ms\nstep:385/5000 train_loss:7.9144 train_time:67483ms step_avg:179.95ms\nstep:386/5000 train_loss:8.0920 train_time:67663ms step_avg:179.96ms\nstep:387/5000 train_loss:8.0474 train_time:67843ms step_avg:179.95ms\nstep:388/5000 train_loss:8.3714 train_time:68022ms step_avg:179.95ms\nstep:389/5000 train_loss:8.3750 train_time:68202ms step_avg:179.95ms\nstep:390/5000 train_loss:8.0750 train_time:68381ms step_avg:179.95ms\nstep:391/5000 train_loss:8.1672 train_time:68561ms step_avg:179.95ms\nstep:392/5000 train_loss:8.0303 train_time:68742ms step_avg:179.95ms\nstep:393/5000 train_loss:8.0958 train_time:68923ms step_avg:179.96ms\nstep:394/5000 train_loss:8.2018 train_time:69103ms step_avg:179.96ms\nstep:395/5000 train_loss:8.0307 train_time:69282ms step_avg:179.95ms\nstep:396/5000 train_loss:8.0731 train_time:69460ms step_avg:179.95ms\nstep:397/5000 train_loss:8.1901 train_time:69641ms step_avg:179.95ms\nstep:398/5000 train_loss:8.1653 train_time:69822ms step_avg:179.95ms\nstep:399/5000 train_loss:8.0894 train_time:70002ms step_avg:179.95ms\nstep:400/5000 train_loss:8.0620 train_time:70183ms step_avg:179.96ms\nstep:401/5000 train_loss:7.9764 train_time:70363ms step_avg:179.96ms\nstep:402/5000 train_loss:8.1199 train_time:70543ms step_avg:179.96ms\nstep:403/5000 train_loss:8.3546 train_time:70723ms step_avg:179.96ms\nstep:404/5000 train_loss:8.0369 train_time:70904ms step_avg:179.96ms\nstep:405/5000 train_loss:8.1503 train_time:71083ms step_avg:179.96ms\nstep:406/5000 train_loss:8.1465 train_time:71265ms step_avg:179.96ms\nstep:407/5000 train_loss:8.1084 train_time:71444ms step_avg:179.96ms\nstep:408/5000 train_loss:8.0538 train_time:71623ms step_avg:179.96ms\nstep:409/5000 train_loss:8.1607 train_time:71803ms step_avg:179.96ms\nstep:410/5000 train_loss:8.1122 train_time:71983ms step_avg:179.96ms\nstep:411/5000 train_loss:8.0768 train_time:72163ms step_avg:179.96ms\nstep:412/5000 train_loss:7.9080 train_time:72343ms step_avg:179.96ms\nstep:413/5000 train_loss:8.0982 train_time:72524ms step_avg:179.96ms\nstep:414/5000 train_loss:7.9920 train_time:72704ms step_avg:179.96ms\nstep:415/5000 train_loss:8.3686 train_time:72884ms step_avg:179.96ms\nstep:416/5000 train_loss:8.0985 train_time:73064ms step_avg:179.96ms\nstep:417/5000 train_loss:7.9249 train_time:73243ms step_avg:179.96ms\nstep:418/5000 train_loss:8.0949 train_time:73423ms step_avg:179.96ms\nstep:419/5000 train_loss:8.1628 train_time:73604ms step_avg:179.96ms\nstep:420/5000 train_loss:8.1461 train_time:73784ms step_avg:179.96ms\nstep:421/5000 train_loss:8.0460 train_time:73964ms step_avg:179.96ms\nstep:422/5000 train_loss:8.3435 train_time:74143ms step_avg:179.96ms\nstep:423/5000 train_loss:8.1831 train_time:74324ms step_avg:179.96ms\nstep:424/5000 train_loss:8.2662 train_time:74505ms step_avg:179.96ms\nstep:425/5000 train_loss:8.1027 train_time:74685ms step_avg:179.96ms\nstep:426/5000 train_loss:8.1376 train_time:74865ms step_avg:179.96ms\nstep:427/5000 train_loss:8.2335 train_time:75045ms step_avg:179.96ms\nstep:428/5000 train_loss:8.1125 train_time:75225ms step_avg:179.96ms\nstep:429/5000 train_loss:8.1273 train_time:75405ms step_avg:179.96ms\nstep:430/5000 train_loss:7.8918 train_time:75584ms step_avg:179.96ms\nstep:431/5000 train_loss:8.0365 train_time:75764ms step_avg:179.96ms\nstep:432/5000 train_loss:8.0451 train_time:75945ms step_avg:179.96ms\nstep:433/5000 train_loss:8.1676 train_time:76125ms step_avg:179.96ms\nstep:434/5000 train_loss:8.1373 train_time:76306ms step_avg:179.97ms\nstep:435/5000 train_loss:8.0570 train_time:76486ms step_avg:179.97ms\nstep:436/5000 train_loss:8.1676 train_time:76665ms step_avg:179.96ms\nstep:437/5000 train_loss:8.1024 train_time:76844ms step_avg:179.96ms\nstep:438/5000 train_loss:8.0979 train_time:77023ms step_avg:179.96ms\nstep:439/5000 train_loss:8.0775 train_time:77204ms step_avg:179.96ms\nstep:440/5000 train_loss:8.1720 train_time:77385ms step_avg:179.96ms\nstep:441/5000 train_loss:8.1551 train_time:77566ms step_avg:179.97ms\nstep:442/5000 train_loss:8.1399 train_time:77746ms step_avg:179.97ms\nstep:443/5000 train_loss:8.0688 train_time:77925ms step_avg:179.96ms\nstep:444/5000 train_loss:8.0754 train_time:78105ms step_avg:179.97ms\nstep:445/5000 train_loss:8.1465 train_time:78285ms step_avg:179.97ms\nstep:446/5000 train_loss:8.1217 train_time:78465ms step_avg:179.97ms\nstep:447/5000 train_loss:8.1589 train_time:78645ms step_avg:179.97ms\nstep:448/5000 train_loss:8.0872 train_time:78824ms step_avg:179.96ms\nstep:449/5000 train_loss:8.1373 train_time:79005ms step_avg:179.97ms\nstep:450/5000 train_loss:8.1278 train_time:79185ms step_avg:179.97ms\nstep:451/5000 train_loss:8.1333 train_time:79364ms step_avg:179.96ms\nstep:452/5000 train_loss:7.9798 train_time:79544ms step_avg:179.96ms\nstep:453/5000 train_loss:8.1943 train_time:79723ms step_avg:179.96ms\nstep:454/5000 train_loss:8.0253 train_time:79904ms step_avg:179.96ms\nstep:455/5000 train_loss:8.1023 train_time:80084ms step_avg:179.96ms\nstep:456/5000 train_loss:8.1458 train_time:80264ms step_avg:179.96ms\nstep:457/5000 train_loss:8.1018 train_time:80444ms step_avg:179.96ms\nstep:458/5000 train_loss:8.1904 train_time:80624ms step_avg:179.96ms\nstep:459/5000 train_loss:8.0907 train_time:80804ms step_avg:179.97ms\nstep:460/5000 train_loss:8.0443 train_time:80985ms step_avg:179.97ms\nstep:461/5000 train_loss:8.0729 train_time:81165ms step_avg:179.97ms\nstep:462/5000 train_loss:7.9945 train_time:81345ms step_avg:179.97ms\nstep:463/5000 train_loss:8.0790 train_time:81526ms step_avg:179.97ms\nstep:464/5000 train_loss:8.1626 train_time:81707ms step_avg:179.97ms\nstep:465/5000 train_loss:8.1380 train_time:81887ms step_avg:179.97ms\nstep:466/5000 train_loss:8.0472 train_time:82066ms step_avg:179.97ms\nstep:467/5000 train_loss:8.1385 train_time:82245ms step_avg:179.97ms\nstep:468/5000 train_loss:8.0900 train_time:82425ms step_avg:179.97ms\nstep:469/5000 train_loss:8.0504 train_time:82606ms step_avg:179.97ms\nstep:470/5000 train_loss:8.0275 train_time:82786ms step_avg:179.97ms\nstep:471/5000 train_loss:8.1268 train_time:82966ms step_avg:179.97ms\nstep:472/5000 train_loss:8.0917 train_time:83145ms step_avg:179.97ms\nstep:473/5000 train_loss:8.0510 train_time:83325ms step_avg:179.97ms\nstep:474/5000 train_loss:8.2128 train_time:83505ms step_avg:179.97ms\nstep:475/5000 train_loss:8.1585 train_time:83685ms step_avg:179.97ms\nstep:476/5000 train_loss:8.1772 train_time:83865ms step_avg:179.97ms\nstep:477/5000 train_loss:8.1840 train_time:84045ms step_avg:179.97ms\nstep:478/5000 train_loss:8.1363 train_time:84225ms step_avg:179.97ms\nstep:479/5000 train_loss:8.1301 train_time:84406ms step_avg:179.97ms\nstep:480/5000 train_loss:8.1477 train_time:84586ms step_avg:179.97ms\nstep:481/5000 train_loss:8.0596 train_time:84766ms step_avg:179.97ms\nstep:482/5000 train_loss:8.0765 train_time:84945ms step_avg:179.97ms\nstep:483/5000 train_loss:8.1098 train_time:85125ms step_avg:179.97ms\nstep:484/5000 train_loss:8.1495 train_time:85306ms step_avg:179.97ms\nstep:485/5000 train_loss:8.1253 train_time:85486ms step_avg:179.97ms\nstep:486/5000 train_loss:8.0875 train_time:85667ms step_avg:179.97ms\nstep:487/5000 train_loss:8.0872 train_time:85846ms step_avg:179.97ms\nstep:488/5000 train_loss:8.0945 train_time:86025ms step_avg:179.97ms\nstep:489/5000 train_loss:7.9963 train_time:86205ms step_avg:179.97ms\nstep:490/5000 train_loss:7.9833 train_time:86385ms step_avg:179.97ms\nstep:491/5000 train_loss:8.0838 train_time:86564ms step_avg:179.97ms\nstep:492/5000 train_loss:8.1695 train_time:86744ms step_avg:179.97ms\nstep:493/5000 train_loss:8.1336 train_time:86924ms step_avg:179.97ms\nstep:494/5000 train_loss:8.2372 train_time:87106ms step_avg:179.97ms\nstep:495/5000 train_loss:8.0503 train_time:87286ms step_avg:179.97ms\nstep:496/5000 train_loss:8.1459 train_time:87466ms step_avg:179.97ms\nstep:497/5000 train_loss:8.1616 train_time:87646ms step_avg:179.97ms\nstep:498/5000 train_loss:8.0905 train_time:87825ms step_avg:179.97ms\nstep:499/5000 train_loss:8.1214 train_time:88005ms step_avg:179.97ms\nstep:500/5000 train_loss:8.0056 train_time:88186ms step_avg:179.97ms\nstep:501/5000 train_loss:8.0486 train_time:88366ms step_avg:179.97ms\nstep:502/5000 train_loss:8.1494 train_time:88545ms step_avg:179.97ms\nstep:503/5000 train_loss:8.0353 train_time:88725ms step_avg:179.97ms\nstep:504/5000 train_loss:8.0388 train_time:88905ms step_avg:179.97ms\nstep:505/5000 train_loss:8.1560 train_time:89085ms step_avg:179.97ms\nstep:506/5000 train_loss:8.1792 train_time:89265ms step_avg:179.97ms\nstep:507/5000 train_loss:8.2266 train_time:89447ms step_avg:179.97ms\nstep:508/5000 train_loss:8.0992 train_time:89627ms step_avg:179.97ms\nstep:509/5000 train_loss:8.2300 train_time:89807ms step_avg:179.97ms\nstep:510/5000 train_loss:8.0903 train_time:89986ms step_avg:179.97ms\nstep:511/5000 train_loss:8.1175 train_time:90166ms step_avg:179.97ms\nstep:512/5000 train_loss:8.1693 train_time:90347ms step_avg:179.97ms\nstep:513/5000 train_loss:8.1285 train_time:90526ms step_avg:179.97ms\nstep:514/5000 train_loss:8.1969 train_time:90707ms step_avg:179.97ms\nstep:515/5000 train_loss:8.0695 train_time:90887ms step_avg:179.97ms\nstep:516/5000 train_loss:8.1404 train_time:91067ms step_avg:179.97ms\nstep:517/5000 train_loss:8.0510 train_time:91247ms step_avg:179.97ms\nstep:518/5000 train_loss:8.0927 train_time:91426ms step_avg:179.97ms\nstep:519/5000 train_loss:8.0190 train_time:91606ms step_avg:179.97ms\nstep:520/5000 train_loss:8.1325 train_time:91787ms step_avg:179.97ms\nstep:521/5000 train_loss:8.0486 train_time:91967ms step_avg:179.97ms\nstep:522/5000 train_loss:8.2300 train_time:92146ms step_avg:179.97ms\nstep:523/5000 train_loss:8.1391 train_time:92326ms step_avg:179.97ms\nstep:524/5000 train_loss:8.2896 train_time:92507ms step_avg:179.98ms\nstep:525/5000 train_loss:8.4236 train_time:92687ms step_avg:179.98ms\nstep:526/5000 train_loss:8.1455 train_time:92867ms step_avg:179.98ms\nstep:527/5000 train_loss:8.1165 train_time:93047ms step_avg:179.98ms\nstep:528/5000 train_loss:8.0647 train_time:93227ms step_avg:179.97ms\nstep:529/5000 train_loss:8.1084 train_time:93407ms step_avg:179.98ms\nstep:530/5000 train_loss:8.1020 train_time:93586ms step_avg:179.97ms\nstep:531/5000 train_loss:8.1243 train_time:93767ms step_avg:179.97ms\nstep:532/5000 train_loss:8.0753 train_time:93947ms step_avg:179.98ms\nstep:533/5000 train_loss:8.0880 train_time:94126ms step_avg:179.97ms\nstep:534/5000 train_loss:8.1001 train_time:94307ms step_avg:179.98ms\nstep:535/5000 train_loss:8.1069 train_time:94488ms step_avg:179.98ms\nstep:536/5000 train_loss:8.1104 train_time:94667ms step_avg:179.98ms\nstep:537/5000 train_loss:8.2075 train_time:94847ms step_avg:179.98ms\nstep:538/5000 train_loss:8.1234 train_time:95026ms step_avg:179.97ms\nstep:539/5000 train_loss:8.1254 train_time:95208ms step_avg:179.98ms\nstep:540/5000 train_loss:8.1072 train_time:95388ms step_avg:179.98ms\nstep:541/5000 train_loss:8.0614 train_time:95570ms step_avg:179.98ms\nstep:542/5000 train_loss:8.0802 train_time:95749ms step_avg:179.98ms\nstep:543/5000 train_loss:8.1418 train_time:95929ms step_avg:179.98ms\nstep:544/5000 train_loss:8.0613 train_time:96110ms step_avg:179.98ms\nstep:545/5000 train_loss:8.0723 train_time:96291ms step_avg:179.98ms\nstep:546/5000 train_loss:8.0826 train_time:96473ms step_avg:179.99ms\nstep:547/5000 train_loss:8.0394 train_time:96655ms step_avg:179.99ms\nstep:548/5000 train_loss:8.1058 train_time:96836ms step_avg:179.99ms\nstep:549/5000 train_loss:8.0716 train_time:97018ms step_avg:180.00ms\nstep:550/5000 train_loss:8.0961 train_time:97201ms step_avg:180.00ms\nstep:551/5000 train_loss:8.1366 train_time:97381ms step_avg:180.00ms\nstep:552/5000 train_loss:8.0863 train_time:97561ms step_avg:180.00ms\nstep:553/5000 train_loss:8.0461 train_time:97743ms step_avg:180.00ms\nstep:554/5000 train_loss:8.1549 train_time:97924ms step_avg:180.01ms\nstep:555/5000 train_loss:8.0484 train_time:98108ms step_avg:180.02ms\nstep:556/5000 train_loss:8.0652 train_time:98287ms step_avg:180.01ms\nstep:557/5000 train_loss:8.0472 train_time:98467ms step_avg:180.01ms\nstep:558/5000 train_loss:8.0233 train_time:98647ms step_avg:180.01ms\nstep:559/5000 train_loss:8.0871 train_time:98826ms step_avg:180.01ms\nstep:560/5000 train_loss:8.0351 train_time:99006ms step_avg:180.01ms\nstep:561/5000 train_loss:8.1943 train_time:99186ms step_avg:180.01ms\nstep:562/5000 train_loss:8.0339 train_time:99365ms step_avg:180.01ms\nstep:563/5000 train_loss:8.0079 train_time:99546ms step_avg:180.01ms\nstep:564/5000 train_loss:8.0614 train_time:99727ms step_avg:180.01ms\nstep:565/5000 train_loss:8.2072 train_time:99908ms step_avg:180.01ms\nstep:566/5000 train_loss:8.1597 train_time:100089ms step_avg:180.02ms\nstep:567/5000 train_loss:8.1980 train_time:100270ms step_avg:180.02ms\nstep:568/5000 train_loss:8.0183 train_time:100453ms step_avg:180.02ms\nstep:569/5000 train_loss:8.2228 train_time:100636ms step_avg:180.03ms\nstep:570/5000 train_loss:8.0075 train_time:100817ms step_avg:180.03ms\nstep:571/5000 train_loss:8.1198 train_time:101001ms step_avg:180.04ms\nstep:572/5000 train_loss:8.1969 train_time:101181ms step_avg:180.04ms\nstep:573/5000 train_loss:8.2059 train_time:101361ms step_avg:180.04ms\nstep:574/5000 train_loss:8.1099 train_time:101542ms step_avg:180.04ms\nstep:575/5000 train_loss:8.0808 train_time:101723ms step_avg:180.04ms\nstep:576/5000 train_loss:8.4105 train_time:101905ms step_avg:180.04ms\nstep:577/5000 train_loss:8.1076 train_time:102084ms step_avg:180.04ms\nstep:578/5000 train_loss:8.0937 train_time:102264ms step_avg:180.04ms\nstep:579/5000 train_loss:8.1366 train_time:102443ms step_avg:180.04ms\nstep:580/5000 train_loss:8.1056 train_time:102624ms step_avg:180.04ms\nstep:581/5000 train_loss:8.0928 train_time:102805ms step_avg:180.04ms\nstep:582/5000 train_loss:8.1136 train_time:102986ms step_avg:180.04ms\nstep:583/5000 train_loss:8.1040 train_time:103165ms step_avg:180.04ms\nstep:584/5000 train_loss:8.1612 train_time:103346ms step_avg:180.04ms\nstep:585/5000 train_loss:8.0259 train_time:103525ms step_avg:180.04ms\nstep:586/5000 train_loss:8.1012 train_time:103705ms step_avg:180.04ms\nstep:587/5000 train_loss:8.0913 train_time:103885ms step_avg:180.04ms\nstep:588/5000 train_loss:8.1306 train_time:104065ms step_avg:180.04ms\nstep:589/5000 train_loss:8.0609 train_time:104246ms step_avg:180.04ms\nstep:590/5000 train_loss:8.0686 train_time:104426ms step_avg:180.05ms\nstep:591/5000 train_loss:8.0968 train_time:104607ms step_avg:180.05ms\nstep:592/5000 train_loss:7.9794 train_time:104786ms step_avg:180.05ms\nstep:593/5000 train_loss:8.1620 train_time:104966ms step_avg:180.05ms\nstep:594/5000 train_loss:8.1928 train_time:105147ms step_avg:180.05ms\nstep:595/5000 train_loss:8.0695 train_time:105328ms step_avg:180.05ms\nstep:596/5000 train_loss:8.2405 train_time:105510ms step_avg:180.05ms\nstep:597/5000 train_loss:8.1059 train_time:105691ms step_avg:180.05ms\nstep:598/5000 train_loss:8.1864 train_time:105873ms step_avg:180.06ms\nstep:599/5000 train_loss:8.1542 train_time:106054ms step_avg:180.06ms\nstep:600/5000 train_loss:8.2608 train_time:106236ms step_avg:180.06ms\nstep:601/5000 train_loss:8.1316 train_time:106418ms step_avg:180.07ms\nstep:602/5000 train_loss:8.1200 train_time:106602ms step_avg:180.07ms\nstep:603/5000 train_loss:8.1721 train_time:106783ms step_avg:180.07ms\nstep:604/5000 train_loss:8.0783 train_time:106964ms step_avg:180.07ms\nstep:605/5000 train_loss:8.0650 train_time:107145ms step_avg:180.07ms\nstep:606/5000 train_loss:8.1298 train_time:107325ms step_avg:180.08ms\nstep:607/5000 train_loss:8.1571 train_time:107507ms step_avg:180.08ms\nstep:608/5000 train_loss:8.1873 train_time:107688ms step_avg:180.08ms\nstep:609/5000 train_loss:8.0556 train_time:107868ms step_avg:180.08ms\nstep:610/5000 train_loss:8.0798 train_time:108048ms step_avg:180.08ms\nstep:611/5000 train_loss:8.2458 train_time:108228ms step_avg:180.08ms\nstep:612/5000 train_loss:8.0883 train_time:108410ms step_avg:180.08ms\nstep:613/5000 train_loss:7.9393 train_time:108591ms step_avg:180.08ms\nstep:614/5000 train_loss:8.1033 train_time:108771ms step_avg:180.08ms\nstep:615/5000 train_loss:8.0977 train_time:108952ms step_avg:180.09ms\nstep:616/5000 train_loss:8.1030 train_time:109134ms step_avg:180.09ms\nstep:617/5000 train_loss:8.0960 train_time:109317ms step_avg:180.09ms\nstep:618/5000 train_loss:8.1687 train_time:109500ms step_avg:180.10ms\nstep:619/5000 train_loss:8.1855 train_time:109681ms step_avg:180.10ms\nstep:620/5000 train_loss:8.1076 train_time:109862ms step_avg:180.10ms\nstep:621/5000 train_loss:8.1507 train_time:110043ms step_avg:180.10ms\nstep:622/5000 train_loss:8.1254 train_time:110224ms step_avg:180.10ms\nstep:623/5000 train_loss:8.1079 train_time:110404ms step_avg:180.11ms\nstep:624/5000 train_loss:8.1577 train_time:110585ms step_avg:180.11ms\nstep:625/5000 train_loss:8.1763 train_time:110765ms step_avg:180.11ms\nstep:626/5000 train_loss:8.2284 train_time:110946ms step_avg:180.11ms\nstep:627/5000 train_loss:8.0528 train_time:111126ms step_avg:180.11ms\nstep:628/5000 train_loss:8.1021 train_time:111308ms step_avg:180.11ms\nstep:629/5000 train_loss:8.1334 train_time:111489ms step_avg:180.11ms\nstep:630/5000 train_loss:8.0592 train_time:111670ms step_avg:180.11ms\nstep:631/5000 train_loss:8.0598 train_time:111850ms step_avg:180.11ms\nstep:632/5000 train_loss:8.1703 train_time:112031ms step_avg:180.11ms\nstep:633/5000 train_loss:8.0651 train_time:112213ms step_avg:180.12ms\nstep:634/5000 train_loss:8.1334 train_time:112396ms step_avg:180.12ms\nstep:635/5000 train_loss:8.1708 train_time:112579ms step_avg:180.13ms\nstep:636/5000 train_loss:8.0188 train_time:112761ms step_avg:180.13ms\nstep:637/5000 train_loss:8.0061 train_time:112941ms step_avg:180.13ms\nstep:638/5000 train_loss:8.0839 train_time:113123ms step_avg:180.13ms\nstep:639/5000 train_loss:8.2184 train_time:113304ms step_avg:180.13ms\nstep:640/5000 train_loss:8.0838 train_time:113485ms step_avg:180.13ms\nstep:641/5000 train_loss:8.1231 train_time:113665ms step_avg:180.14ms\nstep:642/5000 train_loss:8.0785 train_time:113845ms step_avg:180.13ms\nstep:643/5000 train_loss:8.0960 train_time:114025ms step_avg:180.14ms\nstep:644/5000 train_loss:8.1552 train_time:114207ms step_avg:180.14ms\nstep:645/5000 train_loss:8.0227 train_time:114389ms step_avg:180.14ms\nstep:646/5000 train_loss:8.1442 train_time:114570ms step_avg:180.14ms\nstep:647/5000 train_loss:8.1717 train_time:114750ms step_avg:180.14ms\nstep:648/5000 train_loss:8.1481 train_time:114932ms step_avg:180.14ms\nstep:649/5000 train_loss:8.0731 train_time:115113ms step_avg:180.15ms\nstep:650/5000 train_loss:8.1608 train_time:115295ms step_avg:180.15ms\nstep:651/5000 train_loss:8.2974 train_time:115477ms step_avg:180.15ms\nstep:652/5000 train_loss:8.0790 train_time:115660ms step_avg:180.16ms\nstep:653/5000 train_loss:8.0830 train_time:115841ms step_avg:180.16ms\nstep:654/5000 train_loss:8.0504 train_time:116022ms step_avg:180.16ms\nstep:655/5000 train_loss:8.0814 train_time:116205ms step_avg:180.16ms\nstep:656/5000 train_loss:8.0322 train_time:116386ms step_avg:180.16ms\nstep:657/5000 train_loss:8.0240 train_time:116566ms step_avg:180.16ms\nstep:658/5000 train_loss:8.1356 train_time:116746ms step_avg:180.16ms\nstep:659/5000 train_loss:8.1431 train_time:116926ms step_avg:180.16ms\nstep:660/5000 train_loss:8.1337 train_time:117107ms step_avg:180.17ms\nstep:661/5000 train_loss:8.0079 train_time:117288ms step_avg:180.17ms\nstep:662/5000 train_loss:8.2282 train_time:117467ms step_avg:180.16ms\nstep:663/5000 train_loss:8.0904 train_time:117647ms step_avg:180.16ms\nstep:664/5000 train_loss:8.1756 train_time:117828ms step_avg:180.16ms\nstep:665/5000 train_loss:8.1588 train_time:118008ms step_avg:180.16ms\nstep:666/5000 train_loss:8.1206 train_time:118188ms step_avg:180.17ms\nstep:667/5000 train_loss:8.1524 train_time:118369ms step_avg:180.17ms\nstep:668/5000 train_loss:7.9586 train_time:118549ms step_avg:180.17ms\nstep:669/5000 train_loss:7.9911 train_time:118730ms step_avg:180.17ms\nstep:670/5000 train_loss:8.0927 train_time:118913ms step_avg:180.17ms\nstep:671/5000 train_loss:8.1455 train_time:119095ms step_avg:180.17ms\nstep:672/5000 train_loss:8.1316 train_time:119276ms step_avg:180.18ms\nstep:673/5000 train_loss:8.1002 train_time:119459ms step_avg:180.18ms\nstep:674/5000 train_loss:8.0729 train_time:119639ms step_avg:180.18ms\nstep:675/5000 train_loss:8.0387 train_time:119822ms step_avg:180.18ms\nstep:676/5000 train_loss:8.1006 train_time:120003ms step_avg:180.18ms\nstep:677/5000 train_loss:8.1445 train_time:120184ms step_avg:180.19ms\nstep:678/5000 train_loss:8.1571 train_time:120365ms step_avg:180.19ms\nstep:679/5000 train_loss:8.1065 train_time:120545ms step_avg:180.19ms\nstep:680/5000 train_loss:8.1278 train_time:120725ms step_avg:180.19ms\nstep:681/5000 train_loss:8.1373 train_time:120907ms step_avg:180.19ms\nstep:682/5000 train_loss:8.1061 train_time:121089ms step_avg:180.19ms\nstep:683/5000 train_loss:8.2531 train_time:121270ms step_avg:180.19ms\nstep:684/5000 train_loss:7.6381 train_time:121453ms step_avg:180.20ms\nstep:685/5000 train_loss:8.0449 train_time:121635ms step_avg:180.20ms\nstep:686/5000 train_loss:8.1309 train_time:121818ms step_avg:180.20ms\nstep:687/5000 train_loss:8.2215 train_time:122002ms step_avg:180.21ms\nstep:688/5000 train_loss:8.1284 train_time:122183ms step_avg:180.21ms\nstep:689/5000 train_loss:8.1720 train_time:122363ms step_avg:180.21ms\nstep:690/5000 train_loss:8.1466 train_time:122544ms step_avg:180.21ms\nstep:691/5000 train_loss:8.1447 train_time:122725ms step_avg:180.21ms\nstep:692/5000 train_loss:8.0489 train_time:122906ms step_avg:180.21ms\nstep:693/5000 train_loss:8.0440 train_time:123086ms step_avg:180.21ms\nstep:694/5000 train_loss:8.1707 train_time:123266ms step_avg:180.21ms\nstep:695/5000 train_loss:8.0061 train_time:123447ms step_avg:180.21ms\nstep:696/5000 train_loss:8.1527 train_time:123626ms step_avg:180.21ms\nstep:697/5000 train_loss:8.1717 train_time:123807ms step_avg:180.21ms\nstep:698/5000 train_loss:8.1202 train_time:123988ms step_avg:180.21ms\nstep:699/5000 train_loss:8.1068 train_time:124170ms step_avg:180.22ms\nstep:700/5000 train_loss:8.2243 train_time:124354ms step_avg:180.22ms\nstep:701/5000 train_loss:8.2875 train_time:124534ms step_avg:180.22ms\nstep:702/5000 train_loss:8.0760 train_time:124718ms step_avg:180.23ms\nstep:703/5000 train_loss:8.1948 train_time:124899ms step_avg:180.23ms\nstep:704/5000 train_loss:8.1321 train_time:125082ms step_avg:180.23ms\nstep:705/5000 train_loss:8.1049 train_time:125263ms step_avg:180.23ms\nstep:706/5000 train_loss:8.0667 train_time:125444ms step_avg:180.24ms\nstep:707/5000 train_loss:8.0262 train_time:125625ms step_avg:180.24ms\nstep:708/5000 train_loss:8.2642 train_time:125806ms step_avg:180.24ms\nstep:709/5000 train_loss:8.0929 train_time:125987ms step_avg:180.24ms\nstep:710/5000 train_loss:8.0417 train_time:126167ms step_avg:180.24ms\nstep:711/5000 train_loss:7.9597 train_time:126347ms step_avg:180.24ms\nstep:712/5000 train_loss:8.0410 train_time:126526ms step_avg:180.24ms\nstep:713/5000 train_loss:8.3051 train_time:126708ms step_avg:180.24ms\nstep:714/5000 train_loss:8.1481 train_time:126889ms step_avg:180.24ms\nstep:715/5000 train_loss:8.1789 train_time:127070ms step_avg:180.24ms\nstep:716/5000 train_loss:8.0919 train_time:127253ms step_avg:180.24ms\nstep:717/5000 train_loss:8.0870 train_time:127436ms step_avg:180.25ms\nstep:718/5000 train_loss:8.1559 train_time:127617ms step_avg:180.25ms\nstep:719/5000 train_loss:8.2033 train_time:127799ms step_avg:180.25ms\nstep:720/5000 train_loss:8.1402 train_time:127982ms step_avg:180.26ms\nstep:721/5000 train_loss:8.1612 train_time:128162ms step_avg:180.26ms\nstep:722/5000 train_loss:8.1313 train_time:128347ms step_avg:180.26ms\nstep:723/5000 train_loss:8.1036 train_time:128526ms step_avg:180.26ms\nstep:724/5000 train_loss:8.2970 train_time:128707ms step_avg:180.26ms\nstep:725/5000 train_loss:8.0070 train_time:128887ms step_avg:180.26ms\nstep:726/5000 train_loss:8.1287 train_time:129068ms step_avg:180.26ms\nstep:727/5000 train_loss:8.1045 train_time:129250ms step_avg:180.26ms\nstep:728/5000 train_loss:7.9894 train_time:129429ms step_avg:180.26ms\nstep:729/5000 train_loss:8.1650 train_time:129612ms step_avg:180.27ms\nstep:730/5000 train_loss:8.0523 train_time:129796ms step_avg:180.27ms\nstep:731/5000 train_loss:8.0322 train_time:129979ms step_avg:180.28ms\nstep:732/5000 train_loss:8.1993 train_time:130160ms step_avg:180.28ms\nstep:733/5000 train_loss:8.0862 train_time:130342ms step_avg:180.28ms\nstep:734/5000 train_loss:8.0658 train_time:130525ms step_avg:180.28ms\nstep:735/5000 train_loss:8.1035 train_time:130706ms step_avg:180.28ms\nstep:736/5000 train_loss:8.2652 train_time:130886ms step_avg:180.28ms\nstep:737/5000 train_loss:8.0640 train_time:131067ms step_avg:180.28ms\nstep:738/5000 train_loss:8.1458 train_time:131248ms step_avg:180.29ms\nstep:739/5000 train_loss:8.0368 train_time:131428ms step_avg:180.28ms\nstep:740/5000 train_loss:8.1228 train_time:131609ms step_avg:180.29ms\nstep:741/5000 train_loss:8.0792 train_time:131790ms step_avg:180.29ms\nstep:742/5000 train_loss:8.0434 train_time:131971ms step_avg:180.29ms\nstep:743/5000 train_loss:8.0828 train_time:132154ms step_avg:180.29ms\nstep:744/5000 train_loss:8.0647 train_time:132335ms step_avg:180.29ms\nstep:745/5000 train_loss:8.0289 train_time:132517ms step_avg:180.30ms\nstep:746/5000 train_loss:8.3568 train_time:132698ms step_avg:180.30ms\nstep:747/5000 train_loss:8.0974 train_time:132879ms step_avg:180.30ms\nstep:748/5000 train_loss:8.1585 train_time:133061ms step_avg:180.30ms\nstep:749/5000 train_loss:8.2035 train_time:133243ms step_avg:180.30ms\nstep:750/5000 train_loss:8.0994 train_time:133424ms step_avg:180.30ms\nstep:751/5000 train_loss:8.0765 train_time:133606ms step_avg:180.31ms\nstep:752/5000 train_loss:8.1965 train_time:133787ms step_avg:180.31ms\nstep:753/5000 train_loss:8.1798 train_time:133967ms step_avg:180.31ms\nstep:754/5000 train_loss:8.0847 train_time:134147ms step_avg:180.31ms\nstep:755/5000 train_loss:8.1775 train_time:134327ms step_avg:180.31ms\nstep:756/5000 train_loss:7.8782 train_time:134510ms step_avg:180.31ms\nstep:757/5000 train_loss:7.7909 train_time:134693ms step_avg:180.31ms\nstep:758/5000 train_loss:8.2375 train_time:134875ms step_avg:180.31ms\nstep:759/5000 train_loss:8.1490 train_time:135057ms step_avg:180.32ms\nstep:760/5000 train_loss:8.1028 train_time:135238ms step_avg:180.32ms\nstep:761/5000 train_loss:8.0408 train_time:135421ms step_avg:180.32ms\nstep:762/5000 train_loss:8.2025 train_time:135842ms step_avg:180.64ms\nstep:763/5000 train_loss:8.1434 train_time:136022ms step_avg:180.64ms\nstep:764/5000 train_loss:8.0585 train_time:136202ms step_avg:180.64ms\nstep:765/5000 train_loss:8.0540 train_time:136381ms step_avg:180.64ms\nstep:766/5000 train_loss:8.2195 train_time:136561ms step_avg:180.64ms\nstep:767/5000 train_loss:8.1239 train_time:136746ms step_avg:180.64ms\nstep:768/5000 train_loss:8.0754 train_time:136928ms step_avg:180.64ms\nstep:769/5000 train_loss:8.1311 train_time:137109ms step_avg:180.64ms\nstep:770/5000 train_loss:8.0866 train_time:137290ms step_avg:180.64ms\nstep:771/5000 train_loss:8.1662 train_time:137470ms step_avg:180.64ms\nstep:772/5000 train_loss:8.0662 train_time:137651ms step_avg:180.64ms\nstep:773/5000 train_loss:7.8196 train_time:137832ms step_avg:180.65ms\nstep:774/5000 train_loss:8.0926 train_time:138016ms step_avg:180.65ms\nstep:775/5000 train_loss:8.1850 train_time:138197ms step_avg:180.65ms\nstep:776/5000 train_loss:8.1208 train_time:138379ms step_avg:180.65ms\nstep:777/5000 train_loss:8.1792 train_time:138561ms step_avg:180.65ms\nstep:778/5000 train_loss:8.1776 train_time:138742ms step_avg:180.65ms\nstep:779/5000 train_loss:8.1180 train_time:138924ms step_avg:180.66ms\nstep:780/5000 train_loss:8.2867 train_time:139106ms step_avg:180.66ms\nstep:781/5000 train_loss:8.0920 train_time:139287ms step_avg:180.66ms\nstep:782/5000 train_loss:8.0537 train_time:139467ms step_avg:180.66ms\nstep:783/5000 train_loss:8.1000 train_time:139647ms step_avg:180.66ms\nstep:784/5000 train_loss:8.1977 train_time:139827ms step_avg:180.65ms\nstep:785/5000 train_loss:8.0345 train_time:140007ms step_avg:180.65ms\nstep:786/5000 train_loss:8.1929 train_time:140187ms step_avg:180.65ms\nstep:787/5000 train_loss:8.1881 train_time:140368ms step_avg:180.65ms\nstep:788/5000 train_loss:8.1105 train_time:140549ms step_avg:180.65ms\nstep:789/5000 train_loss:8.0705 train_time:140730ms step_avg:180.65ms\nstep:790/5000 train_loss:8.1012 train_time:140912ms step_avg:180.66ms\nstep:791/5000 train_loss:8.2225 train_time:141093ms step_avg:180.66ms\nstep:792/5000 train_loss:8.0983 train_time:141276ms step_avg:180.66ms\nstep:793/5000 train_loss:8.0081 train_time:141457ms step_avg:180.66ms\nstep:794/5000 train_loss:8.1983 train_time:141640ms step_avg:180.66ms\nstep:795/5000 train_loss:8.1017 train_time:141822ms step_avg:180.66ms\nstep:796/5000 train_loss:8.0708 train_time:142003ms step_avg:180.67ms\nstep:797/5000 train_loss:8.0864 train_time:142184ms step_avg:180.67ms\nstep:798/5000 train_loss:8.1322 train_time:142365ms step_avg:180.67ms\nstep:799/5000 train_loss:8.2723 train_time:142548ms step_avg:180.67ms\nstep:800/5000 train_loss:8.1442 train_time:142728ms step_avg:180.67ms\nstep:801/5000 train_loss:8.0188 train_time:142911ms step_avg:180.67ms\nstep:802/5000 train_loss:8.3424 train_time:143092ms step_avg:180.67ms\nstep:803/5000 train_loss:8.2375 train_time:143274ms step_avg:180.67ms\nstep:804/5000 train_loss:8.1330 train_time:143457ms step_avg:180.68ms\nstep:805/5000 train_loss:8.2068 train_time:143641ms step_avg:180.68ms\nstep:806/5000 train_loss:8.1780 train_time:143822ms step_avg:180.68ms\nstep:807/5000 train_loss:8.1205 train_time:144003ms step_avg:180.68ms\nstep:808/5000 train_loss:8.1676 train_time:144184ms step_avg:180.68ms\nstep:809/5000 train_loss:8.1894 train_time:144365ms step_avg:180.68ms\nstep:810/5000 train_loss:8.1086 train_time:144547ms step_avg:180.68ms\nstep:811/5000 train_loss:8.1269 train_time:144728ms step_avg:180.68ms\nstep:812/5000 train_loss:8.1887 train_time:144910ms step_avg:180.69ms\nstep:813/5000 train_loss:8.2157 train_time:145090ms step_avg:180.69ms\nstep:814/5000 train_loss:8.0873 train_time:145272ms step_avg:180.69ms\nstep:815/5000 train_loss:8.0922 train_time:145454ms step_avg:180.69ms\nstep:816/5000 train_loss:8.0652 train_time:145636ms step_avg:180.69ms\nstep:817/5000 train_loss:8.0725 train_time:145820ms step_avg:180.69ms\nstep:818/5000 train_loss:8.0825 train_time:146002ms step_avg:180.70ms\nstep:819/5000 train_loss:8.1626 train_time:146183ms step_avg:180.70ms\nstep:820/5000 train_loss:8.2841 train_time:146363ms step_avg:180.70ms\nstep:821/5000 train_loss:8.1851 train_time:146545ms step_avg:180.70ms\nstep:822/5000 train_loss:8.1021 train_time:146726ms step_avg:180.70ms\nstep:823/5000 train_loss:8.1163 train_time:146908ms step_avg:180.70ms\nstep:824/5000 train_loss:8.0598 train_time:147089ms step_avg:180.70ms\nstep:825/5000 train_loss:8.1321 train_time:147272ms step_avg:180.70ms\nstep:826/5000 train_loss:8.2429 train_time:147453ms step_avg:180.70ms\nstep:827/5000 train_loss:8.2006 train_time:147635ms step_avg:180.70ms\nstep:828/5000 train_loss:8.1291 train_time:147819ms step_avg:180.71ms\nstep:829/5000 train_loss:8.0305 train_time:148001ms step_avg:180.71ms\nstep:830/5000 train_loss:8.0664 train_time:148181ms step_avg:180.71ms\nstep:831/5000 train_loss:8.0413 train_time:148363ms step_avg:180.71ms\nstep:832/5000 train_loss:8.0933 train_time:148545ms step_avg:180.71ms\nstep:833/5000 train_loss:8.0839 train_time:148725ms step_avg:180.71ms\nstep:834/5000 train_loss:8.0995 train_time:148907ms step_avg:180.71ms\nstep:835/5000 train_loss:7.9519 train_time:149089ms step_avg:180.71ms\nstep:836/5000 train_loss:8.2122 train_time:149270ms step_avg:180.71ms\nstep:837/5000 train_loss:8.0934 train_time:149451ms step_avg:180.71ms\nstep:838/5000 train_loss:8.1004 train_time:149632ms step_avg:180.71ms\nstep:839/5000 train_loss:8.1949 train_time:149815ms step_avg:180.72ms\nstep:840/5000 train_loss:7.9488 train_time:149996ms step_avg:180.72ms\nstep:841/5000 train_loss:8.1435 train_time:150178ms step_avg:180.72ms\nstep:842/5000 train_loss:8.0385 train_time:150360ms step_avg:180.72ms\nstep:843/5000 train_loss:8.1283 train_time:150542ms step_avg:180.72ms\nstep:844/5000 train_loss:8.0716 train_time:150723ms step_avg:180.72ms\nstep:845/5000 train_loss:8.1587 train_time:150905ms step_avg:180.72ms\nstep:846/5000 train_loss:8.0567 train_time:151085ms step_avg:180.72ms\nstep:847/5000 train_loss:8.1840 train_time:151266ms step_avg:180.72ms\nstep:848/5000 train_loss:8.3989 train_time:151446ms step_avg:180.72ms\nstep:849/5000 train_loss:8.0692 train_time:151627ms step_avg:180.72ms\nstep:850/5000 train_loss:8.0685 train_time:151809ms step_avg:180.73ms\nstep:851/5000 train_loss:8.3037 train_time:151991ms step_avg:180.73ms\nstep:852/5000 train_loss:8.0421 train_time:152173ms step_avg:180.73ms\nstep:853/5000 train_loss:8.1207 train_time:152355ms step_avg:180.73ms\nstep:854/5000 train_loss:8.1306 train_time:152537ms step_avg:180.73ms\nstep:855/5000 train_loss:8.1819 train_time:152720ms step_avg:180.73ms\nstep:856/5000 train_loss:8.1423 train_time:152903ms step_avg:180.74ms\nstep:857/5000 train_loss:8.0075 train_time:153083ms step_avg:180.74ms\nstep:858/5000 train_loss:8.1714 train_time:153265ms step_avg:180.74ms\nstep:859/5000 train_loss:8.2110 train_time:153446ms step_avg:180.74ms\nstep:860/5000 train_loss:8.0296 train_time:153626ms step_avg:180.74ms\nstep:861/5000 train_loss:8.3441 train_time:153807ms step_avg:180.74ms\nstep:862/5000 train_loss:8.0657 train_time:153987ms step_avg:180.74ms\nstep:863/5000 train_loss:8.1047 train_time:154166ms step_avg:180.73ms\nstep:864/5000 train_loss:8.1910 train_time:154347ms step_avg:180.73ms\nstep:865/5000 train_loss:8.0655 train_time:154529ms step_avg:180.74ms\nstep:866/5000 train_loss:8.1563 train_time:154711ms step_avg:180.74ms\nstep:867/5000 train_loss:8.0892 train_time:154892ms step_avg:180.74ms\nstep:868/5000 train_loss:8.1042 train_time:155075ms step_avg:180.74ms\nstep:869/5000 train_loss:8.0438 train_time:155256ms step_avg:180.74ms\nstep:870/5000 train_loss:8.1073 train_time:155437ms step_avg:180.74ms\nstep:871/5000 train_loss:8.0661 train_time:155620ms step_avg:180.74ms\nstep:872/5000 train_loss:8.0510 train_time:155802ms step_avg:180.74ms\nstep:873/5000 train_loss:8.0080 train_time:155983ms step_avg:180.74ms\nstep:874/5000 train_loss:7.9844 train_time:156164ms step_avg:180.75ms\nstep:875/5000 train_loss:8.1822 train_time:156347ms step_avg:180.75ms\nstep:876/5000 train_loss:8.1210 train_time:156527ms step_avg:180.75ms\nstep:877/5000 train_loss:8.1572 train_time:156709ms step_avg:180.75ms\nstep:878/5000 train_loss:8.0746 train_time:156890ms step_avg:180.75ms\nstep:879/5000 train_loss:7.9576 train_time:157071ms step_avg:180.75ms\nstep:880/5000 train_loss:8.0789 train_time:157252ms step_avg:180.75ms\nstep:881/5000 train_loss:8.0926 train_time:157435ms step_avg:180.75ms\nstep:882/5000 train_loss:8.2180 train_time:157618ms step_avg:180.75ms\nstep:883/5000 train_loss:8.0773 train_time:157800ms step_avg:180.76ms\nstep:884/5000 train_loss:8.1825 train_time:157981ms step_avg:180.76ms\nstep:885/5000 train_loss:8.0395 train_time:158162ms step_avg:180.76ms\nstep:886/5000 train_loss:8.1546 train_time:158344ms step_avg:180.76ms\nstep:887/5000 train_loss:8.0922 train_time:158524ms step_avg:180.76ms\nstep:888/5000 train_loss:8.1917 train_time:158706ms step_avg:180.76ms\nstep:889/5000 train_loss:8.1367 train_time:158886ms step_avg:180.76ms\nstep:890/5000 train_loss:8.1523 train_time:159067ms step_avg:180.76ms\nstep:891/5000 train_loss:8.1135 train_time:159247ms step_avg:180.76ms\nstep:892/5000 train_loss:8.1774 train_time:159427ms step_avg:180.76ms\nstep:893/5000 train_loss:8.1619 train_time:159610ms step_avg:180.76ms\nstep:894/5000 train_loss:8.0752 train_time:159791ms step_avg:180.76ms\nstep:895/5000 train_loss:8.1879 train_time:159973ms step_avg:180.76ms\nstep:896/5000 train_loss:8.1431 train_time:160156ms step_avg:180.76ms\nstep:897/5000 train_loss:8.0688 train_time:160336ms step_avg:180.76ms\nstep:898/5000 train_loss:8.1739 train_time:160518ms step_avg:180.76ms\nstep:899/5000 train_loss:8.1893 train_time:160700ms step_avg:180.76ms\nstep:900/5000 train_loss:7.9622 train_time:160881ms step_avg:180.77ms\nstep:901/5000 train_loss:8.1423 train_time:161063ms step_avg:180.77ms\nstep:902/5000 train_loss:8.0187 train_time:161244ms step_avg:180.77ms\nstep:903/5000 train_loss:8.2169 train_time:161425ms step_avg:180.77ms\nstep:904/5000 train_loss:8.0604 train_time:161606ms step_avg:180.77ms\nstep:905/5000 train_loss:8.1443 train_time:161786ms step_avg:180.77ms\nstep:906/5000 train_loss:8.0441 train_time:161967ms step_avg:180.77ms\nstep:907/5000 train_loss:8.0589 train_time:162149ms step_avg:180.77ms\nstep:908/5000 train_loss:8.0384 train_time:162330ms step_avg:180.77ms\nstep:909/5000 train_loss:8.1576 train_time:162512ms step_avg:180.77ms\nstep:910/5000 train_loss:8.0640 train_time:162694ms step_avg:180.77ms\nstep:911/5000 train_loss:8.1654 train_time:162875ms step_avg:180.77ms\nstep:912/5000 train_loss:8.1268 train_time:163057ms step_avg:180.77ms\nstep:913/5000 train_loss:8.0940 train_time:163239ms step_avg:180.77ms\nstep:914/5000 train_loss:8.1897 train_time:163422ms step_avg:180.78ms\nstep:915/5000 train_loss:8.1591 train_time:163603ms step_avg:180.78ms\nstep:916/5000 train_loss:8.1412 train_time:163784ms step_avg:180.78ms\nstep:917/5000 train_loss:8.2018 train_time:163965ms step_avg:180.78ms\nstep:918/5000 train_loss:8.1774 train_time:164147ms step_avg:180.78ms\nstep:919/5000 train_loss:8.3481 train_time:164328ms step_avg:180.78ms\nstep:920/5000 train_loss:8.2195 train_time:164509ms step_avg:180.78ms\nstep:921/5000 train_loss:8.1601 train_time:164690ms step_avg:180.78ms\nstep:922/5000 train_loss:8.0890 train_time:164871ms step_avg:180.78ms\nstep:923/5000 train_loss:8.0212 train_time:165053ms step_avg:180.78ms\nstep:924/5000 train_loss:8.2053 train_time:165234ms step_avg:180.78ms\nstep:925/5000 train_loss:8.0075 train_time:165417ms step_avg:180.78ms\nstep:926/5000 train_loss:8.1960 train_time:165600ms step_avg:180.79ms\nstep:927/5000 train_loss:8.0711 train_time:165781ms step_avg:180.79ms\nstep:928/5000 train_loss:8.0111 train_time:165963ms step_avg:180.79ms\nstep:929/5000 train_loss:8.1297 train_time:166144ms step_avg:180.79ms\nstep:930/5000 train_loss:8.0541 train_time:166324ms step_avg:180.79ms\nstep:931/5000 train_loss:8.0818 train_time:166507ms step_avg:180.79ms\nstep:932/5000 train_loss:8.1515 train_time:166690ms step_avg:180.79ms\nstep:933/5000 train_loss:8.1623 train_time:166871ms step_avg:180.79ms\nstep:934/5000 train_loss:8.0375 train_time:167053ms step_avg:180.79ms\nstep:935/5000 train_loss:8.0713 train_time:167235ms step_avg:180.79ms\nstep:936/5000 train_loss:8.0876 train_time:167418ms step_avg:180.80ms\nstep:937/5000 train_loss:8.1172 train_time:167600ms step_avg:180.80ms\nstep:938/5000 train_loss:8.2911 train_time:167782ms step_avg:180.80ms\nstep:939/5000 train_loss:8.2248 train_time:167963ms step_avg:180.80ms\nstep:940/5000 train_loss:7.8426 train_time:168144ms step_avg:180.80ms\nstep:941/5000 train_loss:8.1737 train_time:168324ms step_avg:180.80ms\nstep:942/5000 train_loss:8.1767 train_time:168506ms step_avg:180.80ms\nstep:943/5000 train_loss:8.0140 train_time:168686ms step_avg:180.80ms\nstep:944/5000 train_loss:8.2462 train_time:168867ms step_avg:180.80ms\nstep:945/5000 train_loss:7.9971 train_time:169047ms step_avg:180.80ms\nstep:946/5000 train_loss:8.2168 train_time:169227ms step_avg:180.80ms\nstep:947/5000 train_loss:8.1674 train_time:169409ms step_avg:180.80ms\nstep:948/5000 train_loss:8.0975 train_time:169592ms step_avg:180.80ms\nstep:949/5000 train_loss:7.9042 train_time:169775ms step_avg:180.80ms\nstep:950/5000 train_loss:8.1522 train_time:169957ms step_avg:180.81ms\nstep:951/5000 train_loss:8.1835 train_time:170140ms step_avg:180.81ms\nstep:952/5000 train_loss:8.0565 train_time:170322ms step_avg:180.81ms\nstep:953/5000 train_loss:8.2822 train_time:170503ms step_avg:180.81ms\nstep:954/5000 train_loss:8.0151 train_time:170688ms step_avg:180.81ms\nstep:955/5000 train_loss:8.1345 train_time:170865ms step_avg:180.81ms\nstep:956/5000 train_loss:8.1560 train_time:171047ms step_avg:180.81ms\nstep:957/5000 train_loss:8.0553 train_time:171228ms step_avg:180.81ms\nstep:958/5000 train_loss:8.1405 train_time:171411ms step_avg:180.81ms\nstep:959/5000 train_loss:8.0607 train_time:171592ms step_avg:180.81ms\nstep:960/5000 train_loss:8.2058 train_time:171776ms step_avg:180.82ms\nstep:961/5000 train_loss:8.0524 train_time:171959ms step_avg:180.82ms\nstep:962/5000 train_loss:8.2987 train_time:172143ms step_avg:180.82ms\nstep:963/5000 train_loss:8.3153 train_time:172320ms step_avg:180.82ms\nstep:964/5000 train_loss:8.1444 train_time:172502ms step_avg:180.82ms\nstep:965/5000 train_loss:8.1672 train_time:172684ms step_avg:180.82ms\nstep:966/5000 train_loss:8.0838 train_time:172866ms step_avg:180.82ms\nstep:967/5000 train_loss:8.1819 train_time:173045ms step_avg:180.82ms\nstep:968/5000 train_loss:8.2585 train_time:173226ms step_avg:180.82ms\nstep:969/5000 train_loss:8.1965 train_time:173406ms step_avg:180.82ms\nstep:970/5000 train_loss:8.2296 train_time:173586ms step_avg:180.82ms\nstep:971/5000 train_loss:8.1535 train_time:173766ms step_avg:180.82ms\nstep:972/5000 train_loss:8.0872 train_time:173946ms step_avg:180.82ms\nstep:973/5000 train_loss:8.0854 train_time:174127ms step_avg:180.82ms\nstep:974/5000 train_loss:8.0567 train_time:174309ms step_avg:180.82ms\nstep:975/5000 train_loss:8.0202 train_time:174490ms step_avg:180.82ms\nstep:976/5000 train_loss:8.0318 train_time:174671ms step_avg:180.82ms\nstep:977/5000 train_loss:8.1277 train_time:174853ms step_avg:180.82ms\nstep:978/5000 train_loss:8.0641 train_time:175036ms step_avg:180.82ms\nstep:979/5000 train_loss:8.1671 train_time:175218ms step_avg:180.82ms\nstep:980/5000 train_loss:8.1172 train_time:175401ms step_avg:180.83ms\nstep:981/5000 train_loss:8.1732 train_time:175581ms step_avg:180.82ms\nstep:982/5000 train_loss:8.0809 train_time:175761ms step_avg:180.82ms\nstep:983/5000 train_loss:8.2134 train_time:175941ms step_avg:180.82ms\nstep:984/5000 train_loss:8.0867 train_time:176123ms step_avg:180.82ms\nstep:985/5000 train_loss:8.0889 train_time:176305ms step_avg:180.83ms\nstep:986/5000 train_loss:8.1493 train_time:176486ms step_avg:180.83ms\nstep:987/5000 train_loss:8.1903 train_time:176667ms step_avg:180.83ms\nstep:988/5000 train_loss:8.1134 train_time:176848ms step_avg:180.83ms\nstep:989/5000 train_loss:8.0120 train_time:177028ms step_avg:180.83ms\nstep:990/5000 train_loss:8.1472 train_time:177210ms step_avg:180.83ms\nstep:991/5000 train_loss:8.1516 train_time:177391ms step_avg:180.83ms\nstep:992/5000 train_loss:8.2815 train_time:177572ms step_avg:180.83ms\nstep:993/5000 train_loss:8.1254 train_time:177754ms step_avg:180.83ms\nstep:994/5000 train_loss:8.0593 train_time:177936ms step_avg:180.83ms\nstep:995/5000 train_loss:8.1565 train_time:178118ms step_avg:180.83ms\nstep:996/5000 train_loss:8.0861 train_time:178300ms step_avg:180.83ms\nstep:997/5000 train_loss:8.1441 train_time:178482ms step_avg:180.83ms\nstep:998/5000 train_loss:8.1514 train_time:178663ms step_avg:180.83ms\nstep:999/5000 train_loss:8.0918 train_time:178845ms step_avg:180.83ms\nstep:1000/5000 train_loss:8.0608 train_time:179026ms step_avg:180.83ms\nstep:1001/5000 train_loss:8.0880 train_time:179207ms step_avg:180.83ms\nstep:1002/5000 train_loss:8.1212 train_time:179387ms step_avg:180.83ms\nstep:1003/5000 train_loss:7.9979 train_time:179568ms step_avg:180.83ms\nstep:1004/5000 train_loss:8.1478 train_time:179749ms step_avg:180.83ms\nstep:1005/5000 train_loss:8.1639 train_time:179929ms step_avg:180.83ms\nstep:1006/5000 train_loss:8.0570 train_time:180112ms step_avg:180.84ms\nstep:1007/5000 train_loss:8.2823 train_time:180294ms step_avg:180.84ms\nstep:1008/5000 train_loss:8.0453 train_time:180476ms step_avg:180.84ms\nstep:1009/5000 train_loss:8.1137 train_time:180657ms step_avg:180.84ms\nstep:1010/5000 train_loss:8.0181 train_time:180840ms step_avg:180.84ms\nstep:1011/5000 train_loss:8.1222 train_time:181022ms step_avg:180.84ms\nstep:1012/5000 train_loss:8.0445 train_time:181202ms step_avg:180.84ms\nstep:1013/5000 train_loss:7.9982 train_time:181383ms step_avg:180.84ms\nstep:1014/5000 train_loss:8.0810 train_time:181565ms step_avg:180.84ms\nstep:1015/5000 train_loss:8.1023 train_time:181746ms step_avg:180.84ms\nstep:1016/5000 train_loss:8.1332 train_time:181926ms step_avg:180.84ms\nstep:1017/5000 train_loss:8.0698 train_time:182107ms step_avg:180.84ms\nstep:1018/5000 train_loss:8.1157 train_time:182288ms step_avg:180.84ms\nstep:1019/5000 train_loss:8.0194 train_time:182469ms step_avg:180.84ms\nstep:1020/5000 train_loss:8.1469 train_time:182650ms step_avg:180.84ms\nstep:1021/5000 train_loss:8.1982 train_time:182830ms step_avg:180.84ms\nstep:1022/5000 train_loss:8.0456 train_time:183013ms step_avg:180.84ms\nstep:1023/5000 train_loss:8.0696 train_time:183195ms step_avg:180.84ms\nstep:1024/5000 train_loss:8.2331 train_time:183377ms step_avg:180.85ms\nstep:1025/5000 train_loss:8.1270 train_time:183559ms step_avg:180.85ms\nstep:1026/5000 train_loss:8.1454 train_time:183742ms step_avg:180.85ms\nstep:1027/5000 train_loss:8.0184 train_time:183923ms step_avg:180.85ms\nstep:1028/5000 train_loss:8.2031 train_time:184105ms step_avg:180.85ms\nstep:1029/5000 train_loss:8.1204 train_time:184285ms step_avg:180.85ms\nstep:1030/5000 train_loss:8.2808 train_time:184465ms step_avg:180.85ms\nstep:1031/5000 train_loss:8.1012 train_time:184645ms step_avg:180.85ms\nstep:1032/5000 train_loss:8.0454 train_time:184826ms step_avg:180.85ms\nstep:1033/5000 train_loss:8.2337 train_time:185009ms step_avg:180.85ms\nstep:1034/5000 train_loss:8.0635 train_time:185190ms step_avg:180.85ms\nstep:1035/5000 train_loss:8.0271 train_time:185371ms step_avg:180.85ms\nstep:1036/5000 train_loss:8.0475 train_time:185552ms step_avg:180.85ms\nstep:1037/5000 train_loss:8.0489 train_time:185732ms step_avg:180.85ms\nstep:1038/5000 train_loss:8.0908 train_time:185915ms step_avg:180.85ms\nstep:1039/5000 train_loss:8.1045 train_time:186098ms step_avg:180.85ms\nstep:1040/5000 train_loss:8.0705 train_time:186280ms step_avg:180.85ms\nstep:1041/5000 train_loss:8.1956 train_time:186461ms step_avg:180.85ms\nstep:1042/5000 train_loss:8.0268 train_time:186641ms step_avg:180.85ms\nstep:1043/5000 train_loss:8.1767 train_time:186822ms step_avg:180.85ms\nstep:1044/5000 train_loss:8.1420 train_time:187003ms step_avg:180.85ms\nstep:1045/5000 train_loss:7.9482 train_time:187184ms step_avg:180.85ms\nstep:1046/5000 train_loss:8.0380 train_time:187366ms step_avg:180.86ms\nstep:1047/5000 train_loss:8.0447 train_time:187547ms step_avg:180.86ms\nstep:1048/5000 train_loss:8.0834 train_time:187728ms step_avg:180.86ms\nstep:1049/5000 train_loss:8.2586 train_time:187909ms step_avg:180.86ms\nstep:1050/5000 train_loss:8.1081 train_time:188089ms step_avg:180.85ms\nstep:1051/5000 train_loss:8.1971 train_time:188270ms step_avg:180.86ms\nstep:1052/5000 train_loss:8.1585 train_time:188452ms step_avg:180.86ms\nstep:1053/5000 train_loss:8.1602 train_time:188634ms step_avg:180.86ms\nstep:1054/5000 train_loss:8.2131 train_time:188817ms step_avg:180.86ms\nstep:1055/5000 train_loss:8.1405 train_time:188999ms step_avg:180.86ms\nstep:1056/5000 train_loss:8.0432 train_time:189182ms step_avg:180.86ms\nstep:1057/5000 train_loss:8.1247 train_time:189362ms step_avg:180.86ms\nstep:1058/5000 train_loss:8.3314 train_time:189544ms step_avg:180.86ms\nstep:1059/5000 train_loss:8.1193 train_time:189725ms step_avg:180.86ms\nstep:1060/5000 train_loss:8.0065 train_time:189906ms step_avg:180.86ms\nstep:1061/5000 train_loss:7.9446 train_time:190086ms step_avg:180.86ms\nstep:1062/5000 train_loss:8.1606 train_time:190266ms step_avg:180.86ms\nstep:1063/5000 train_loss:8.3376 train_time:190447ms step_avg:180.86ms\nstep:1064/5000 train_loss:8.0452 train_time:190628ms step_avg:180.86ms\nstep:1065/5000 train_loss:7.9998 train_time:190809ms step_avg:180.86ms\nstep:1066/5000 train_loss:8.1362 train_time:190991ms step_avg:180.86ms\nstep:1067/5000 train_loss:8.1523 train_time:191173ms step_avg:180.86ms\nstep:1068/5000 train_loss:8.1367 train_time:191355ms step_avg:180.87ms\nstep:1069/5000 train_loss:8.0272 train_time:191537ms step_avg:180.87ms\nstep:1070/5000 train_loss:8.0306 train_time:191719ms step_avg:180.87ms\nstep:1071/5000 train_loss:8.1574 train_time:191901ms step_avg:180.87ms\nstep:1072/5000 train_loss:7.9852 train_time:192083ms step_avg:180.87ms\nstep:1073/5000 train_loss:8.1657 train_time:192264ms step_avg:180.87ms\nstep:1074/5000 train_loss:8.0189 train_time:192445ms step_avg:180.87ms\nstep:1075/5000 train_loss:8.0723 train_time:192626ms step_avg:180.87ms\nstep:1076/5000 train_loss:8.1455 train_time:192807ms step_avg:180.87ms\nstep:1077/5000 train_loss:8.1833 train_time:192988ms step_avg:180.87ms\nstep:1078/5000 train_loss:8.1465 train_time:193169ms step_avg:180.87ms\nstep:1079/5000 train_loss:8.0487 train_time:193350ms step_avg:180.87ms\nstep:1080/5000 train_loss:8.1725 train_time:193531ms step_avg:180.87ms\nstep:1081/5000 train_loss:8.0427 train_time:193713ms step_avg:180.87ms\nstep:1082/5000 train_loss:8.1073 train_time:193895ms step_avg:180.87ms\nstep:1083/5000 train_loss:8.1010 train_time:194076ms step_avg:180.87ms\nstep:1084/5000 train_loss:8.1357 train_time:194257ms step_avg:180.87ms\nstep:1085/5000 train_loss:8.1410 train_time:194439ms step_avg:180.87ms\nstep:1086/5000 train_loss:8.1232 train_time:194622ms step_avg:180.88ms\nstep:1087/5000 train_loss:8.2135 train_time:194803ms step_avg:180.88ms\nstep:1088/5000 train_loss:7.9982 train_time:194985ms step_avg:180.88ms\nstep:1089/5000 train_loss:8.0441 train_time:195166ms step_avg:180.88ms\nstep:1090/5000 train_loss:8.0271 train_time:195347ms step_avg:180.88ms\nstep:1091/5000 train_loss:8.2132 train_time:195528ms step_avg:180.88ms\nstep:1092/5000 train_loss:8.0514 train_time:195709ms step_avg:180.88ms\nstep:1093/5000 train_loss:8.1613 train_time:195890ms step_avg:180.88ms\nstep:1094/5000 train_loss:8.3250 train_time:196071ms step_avg:180.88ms\nstep:1095/5000 train_loss:8.0046 train_time:196252ms step_avg:180.88ms\nstep:1096/5000 train_loss:8.1615 train_time:196433ms step_avg:180.88ms\nstep:1097/5000 train_loss:8.1781 train_time:196617ms step_avg:180.88ms\nstep:1098/5000 train_loss:8.0229 train_time:196799ms step_avg:180.88ms\nstep:1099/5000 train_loss:7.9859 train_time:196982ms step_avg:180.88ms\nstep:1100/5000 train_loss:8.1131 train_time:197162ms step_avg:180.88ms\nstep:1101/5000 train_loss:7.9151 train_time:197342ms step_avg:180.88ms\nstep:1102/5000 train_loss:8.1369 train_time:197524ms step_avg:180.88ms\nstep:1103/5000 train_loss:8.1571 train_time:197705ms step_avg:180.88ms\nstep:1104/5000 train_loss:8.1949 train_time:197885ms step_avg:180.88ms\nstep:1105/5000 train_loss:8.1335 train_time:198065ms step_avg:180.88ms\nstep:1106/5000 train_loss:8.1167 train_time:198245ms step_avg:180.88ms\nstep:1107/5000 train_loss:8.0786 train_time:198426ms step_avg:180.88ms\nstep:1108/5000 train_loss:8.0891 train_time:198606ms step_avg:180.88ms\nstep:1109/5000 train_loss:8.1205 train_time:198787ms step_avg:180.88ms\nstep:1110/5000 train_loss:8.0623 train_time:198970ms step_avg:180.88ms\nstep:1111/5000 train_loss:8.0216 train_time:199151ms step_avg:180.88ms\nstep:1112/5000 train_loss:8.1639 train_time:199331ms step_avg:180.88ms\nstep:1113/5000 train_loss:7.9943 train_time:199513ms step_avg:180.88ms\nstep:1114/5000 train_loss:8.1972 train_time:199696ms step_avg:180.88ms\nstep:1115/5000 train_loss:8.0925 train_time:199877ms step_avg:180.88ms\nstep:1116/5000 train_loss:8.0953 train_time:200061ms step_avg:180.89ms\nstep:1117/5000 train_loss:8.1069 train_time:200242ms step_avg:180.89ms\nstep:1118/5000 train_loss:8.0957 train_time:200424ms step_avg:180.89ms\nstep:1119/5000 train_loss:8.0903 train_time:200605ms step_avg:180.89ms\nstep:1120/5000 train_loss:7.9959 train_time:200785ms step_avg:180.89ms\nstep:1121/5000 train_loss:8.1640 train_time:200965ms step_avg:180.89ms\nstep:1122/5000 train_loss:8.0279 train_time:201146ms step_avg:180.89ms\nstep:1123/5000 train_loss:8.0042 train_time:201327ms step_avg:180.89ms\nstep:1124/5000 train_loss:8.0984 train_time:201508ms step_avg:180.89ms\nstep:1125/5000 train_loss:8.1238 train_time:201689ms step_avg:180.89ms\nstep:1126/5000 train_loss:8.0838 train_time:201870ms step_avg:180.89ms\nstep:1127/5000 train_loss:8.1112 train_time:202052ms step_avg:180.89ms\nstep:1128/5000 train_loss:8.2642 train_time:202233ms step_avg:180.89ms\nstep:1129/5000 train_loss:8.1042 train_time:202415ms step_avg:180.89ms\nstep:1130/5000 train_loss:8.1954 train_time:202598ms step_avg:180.89ms\nstep:1131/5000 train_loss:8.2695 train_time:202779ms step_avg:180.89ms\nstep:1132/5000 train_loss:8.1012 train_time:202962ms step_avg:180.89ms\nstep:1133/5000 train_loss:8.1081 train_time:203143ms step_avg:180.89ms\nstep:1134/5000 train_loss:8.0994 train_time:203324ms step_avg:180.89ms\nstep:1135/5000 train_loss:8.1325 train_time:203507ms step_avg:180.89ms\nstep:1136/5000 train_loss:8.1538 train_time:203688ms step_avg:180.90ms\nstep:1137/5000 train_loss:8.0297 train_time:203869ms step_avg:180.90ms\nstep:1138/5000 train_loss:8.0485 train_time:204049ms step_avg:180.89ms\nstep:1139/5000 train_loss:8.1707 train_time:204230ms step_avg:180.89ms\nstep:1140/5000 train_loss:8.1250 train_time:204412ms step_avg:180.90ms\nstep:1141/5000 train_loss:8.0829 train_time:204595ms step_avg:180.90ms\nstep:1142/5000 train_loss:8.0598 train_time:204777ms step_avg:180.90ms\nstep:1143/5000 train_loss:8.0588 train_time:204960ms step_avg:180.90ms\nstep:1144/5000 train_loss:8.1679 train_time:205140ms step_avg:180.90ms\nstep:1145/5000 train_loss:8.1629 train_time:205322ms step_avg:180.90ms\nstep:1146/5000 train_loss:8.0989 train_time:205503ms step_avg:180.90ms\nstep:1147/5000 train_loss:8.1066 train_time:205683ms step_avg:180.90ms\nstep:1148/5000 train_loss:8.1413 train_time:205865ms step_avg:180.90ms\nstep:1149/5000 train_loss:8.0752 train_time:206045ms step_avg:180.90ms\nstep:1150/5000 train_loss:8.0503 train_time:206226ms step_avg:180.90ms\nstep:1151/5000 train_loss:8.0752 train_time:206408ms step_avg:180.90ms\nstep:1152/5000 train_loss:8.1205 train_time:206588ms step_avg:180.90ms\nstep:1153/5000 train_loss:8.1270 train_time:206769ms step_avg:180.90ms\nstep:1154/5000 train_loss:8.2525 train_time:206950ms step_avg:180.90ms\nstep:1155/5000 train_loss:8.1131 train_time:207130ms step_avg:180.90ms\nstep:1156/5000 train_loss:8.0654 train_time:207313ms step_avg:180.90ms\nstep:1157/5000 train_loss:8.0100 train_time:207495ms step_avg:180.90ms\nstep:1158/5000 train_loss:8.2495 train_time:207676ms step_avg:180.90ms\nstep:1159/5000 train_loss:8.0459 train_time:207857ms step_avg:180.90ms\nstep:1160/5000 train_loss:8.0679 train_time:208040ms step_avg:180.90ms\nstep:1161/5000 train_loss:8.1488 train_time:208222ms step_avg:180.91ms\nstep:1162/5000 train_loss:8.1044 train_time:208404ms step_avg:180.91ms\nstep:1163/5000 train_loss:8.1165 train_time:208585ms step_avg:180.91ms\nstep:1164/5000 train_loss:8.1843 train_time:208766ms step_avg:180.91ms\nstep:1165/5000 train_loss:8.0668 train_time:208946ms step_avg:180.91ms\nstep:1166/5000 train_loss:8.0767 train_time:209126ms step_avg:180.90ms\nstep:1167/5000 train_loss:8.1580 train_time:209307ms step_avg:180.90ms\nstep:1168/5000 train_loss:8.0938 train_time:209488ms step_avg:180.90ms\nstep:1169/5000 train_loss:8.1705 train_time:209669ms step_avg:180.91ms\nstep:1170/5000 train_loss:8.1274 train_time:209851ms step_avg:180.91ms\nstep:1171/5000 train_loss:8.0575 train_time:210033ms step_avg:180.91ms\nstep:1172/5000 train_loss:8.1065 train_time:210215ms step_avg:180.91ms\nstep:1173/5000 train_loss:8.1060 train_time:210397ms step_avg:180.91ms\nstep:1174/5000 train_loss:8.0940 train_time:210578ms step_avg:180.91ms\nstep:1175/5000 train_loss:8.0636 train_time:210759ms step_avg:180.91ms\nstep:1176/5000 train_loss:8.0930 train_time:210941ms step_avg:180.91ms\nstep:1177/5000 train_loss:8.2343 train_time:211124ms step_avg:180.91ms\nstep:1178/5000 train_loss:8.0742 train_time:211305ms step_avg:180.91ms\nstep:1179/5000 train_loss:8.1231 train_time:211486ms step_avg:180.91ms\nstep:1180/5000 train_loss:8.0725 train_time:211667ms step_avg:180.91ms\nstep:1181/5000 train_loss:7.9574 train_time:211847ms step_avg:180.91ms\nstep:1182/5000 train_loss:8.2052 train_time:212026ms step_avg:180.91ms\nstep:1183/5000 train_loss:8.0816 train_time:212207ms step_avg:180.91ms\nstep:1184/5000 train_loss:8.0155 train_time:212387ms step_avg:180.91ms\nstep:1185/5000 train_loss:8.1246 train_time:212568ms step_avg:180.91ms\nstep:1186/5000 train_loss:8.1356 train_time:212750ms step_avg:180.91ms\nstep:1187/5000 train_loss:8.2033 train_time:212932ms step_avg:180.91ms\nstep:1188/5000 train_loss:8.0289 train_time:213116ms step_avg:180.91ms\nstep:1189/5000 train_loss:8.3111 train_time:213298ms step_avg:180.91ms\nstep:1190/5000 train_loss:8.2383 train_time:213479ms step_avg:180.91ms\nstep:1191/5000 train_loss:8.1556 train_time:213661ms step_avg:180.92ms\nstep:1192/5000 train_loss:8.0864 train_time:213843ms step_avg:180.92ms\nstep:1193/5000 train_loss:8.2255 train_time:214025ms step_avg:180.92ms\nstep:1194/5000 train_loss:8.1016 train_time:214207ms step_avg:180.92ms\nstep:1195/5000 train_loss:8.0915 train_time:214388ms step_avg:180.92ms\nstep:1196/5000 train_loss:8.0790 train_time:214569ms step_avg:180.92ms\nstep:1197/5000 train_loss:8.0938 train_time:214749ms step_avg:180.92ms\nstep:1198/5000 train_loss:8.2858 train_time:214930ms step_avg:180.92ms\nstep:1199/5000 train_loss:8.0061 train_time:215113ms step_avg:180.92ms\nstep:1200/5000 train_loss:8.1855 train_time:215296ms step_avg:180.92ms\nstep:1201/5000 train_loss:8.1443 train_time:215477ms step_avg:180.92ms\nstep:1202/5000 train_loss:8.1348 train_time:215659ms step_avg:180.92ms\nstep:1203/5000 train_loss:8.0389 train_time:215841ms step_avg:180.92ms\nstep:1204/5000 train_loss:8.1231 train_time:216022ms step_avg:180.92ms\nstep:1205/5000 train_loss:8.0967 train_time:216204ms step_avg:180.92ms\nstep:1206/5000 train_loss:8.0890 train_time:216385ms step_avg:180.92ms\nstep:1207/5000 train_loss:8.1529 train_time:216565ms step_avg:180.92ms\nstep:1208/5000 train_loss:8.0811 train_time:216746ms step_avg:180.92ms\nstep:1209/5000 train_loss:8.1855 train_time:216927ms step_avg:180.92ms\nstep:1210/5000 train_loss:8.1076 train_time:217109ms step_avg:180.92ms\nstep:1211/5000 train_loss:8.1304 train_time:217291ms step_avg:180.92ms\nstep:1212/5000 train_loss:8.1429 train_time:217473ms step_avg:180.93ms\nstep:1213/5000 train_loss:8.0199 train_time:217655ms step_avg:180.93ms\nstep:1214/5000 train_loss:8.1657 train_time:217838ms step_avg:180.93ms\nstep:1215/5000 train_loss:8.0865 train_time:218021ms step_avg:180.93ms\nstep:1216/5000 train_loss:8.1551 train_time:218202ms step_avg:180.93ms\nstep:1217/5000 train_loss:8.1790 train_time:218383ms step_avg:180.93ms\nstep:1218/5000 train_loss:8.0973 train_time:218565ms step_avg:180.93ms\nstep:1219/5000 train_loss:8.1921 train_time:218745ms step_avg:180.93ms\nstep:1220/5000 train_loss:8.1896 train_time:218926ms step_avg:180.93ms\nstep:1221/5000 train_loss:8.0499 train_time:219107ms step_avg:180.93ms\nstep:1222/5000 train_loss:8.1967 train_time:219287ms step_avg:180.93ms\nstep:1223/5000 train_loss:7.9576 train_time:219468ms step_avg:180.93ms\nstep:1224/5000 train_loss:8.0786 train_time:219650ms step_avg:180.93ms\nstep:1225/5000 train_loss:8.0738 train_time:219830ms step_avg:180.93ms\nstep:1226/5000 train_loss:8.1137 train_time:220012ms step_avg:180.93ms\nstep:1227/5000 train_loss:8.2332 train_time:220195ms step_avg:180.93ms\nstep:1228/5000 train_loss:8.0426 train_time:220377ms step_avg:180.93ms\nstep:1229/5000 train_loss:8.1154 train_time:220559ms step_avg:180.93ms\nstep:1230/5000 train_loss:8.1328 train_time:220740ms step_avg:180.93ms\nstep:1231/5000 train_loss:8.2304 train_time:220922ms step_avg:180.94ms\nstep:1232/5000 train_loss:8.2070 train_time:221103ms step_avg:180.94ms\nstep:1233/5000 train_loss:8.0255 train_time:221284ms step_avg:180.94ms\nstep:1234/5000 train_loss:8.2108 train_time:221465ms step_avg:180.94ms\nstep:1235/5000 train_loss:8.0317 train_time:221646ms step_avg:180.94ms\nstep:1236/5000 train_loss:7.9988 train_time:221827ms step_avg:180.94ms\nstep:1237/5000 train_loss:8.1301 train_time:222009ms step_avg:180.94ms\nstep:1238/5000 train_loss:8.1027 train_time:222191ms step_avg:180.94ms\nstep:1239/5000 train_loss:8.1810 train_time:222372ms step_avg:180.94ms\nstep:1240/5000 train_loss:8.0962 train_time:222554ms step_avg:180.94ms\nstep:1241/5000 train_loss:8.1378 train_time:222736ms step_avg:180.94ms\nstep:1242/5000 train_loss:8.1125 train_time:222919ms step_avg:180.94ms\nstep:1243/5000 train_loss:8.2139 train_time:223102ms step_avg:180.94ms\nstep:1244/5000 train_loss:8.1734 train_time:223284ms step_avg:180.94ms\nstep:1245/5000 train_loss:8.1508 train_time:223465ms step_avg:180.94ms\nstep:1246/5000 train_loss:8.1131 train_time:223645ms step_avg:180.94ms\nstep:1247/5000 train_loss:8.2234 train_time:223825ms step_avg:180.94ms\nstep:1248/5000 train_loss:8.1129 train_time:224006ms step_avg:180.94ms\nstep:1249/5000 train_loss:8.1266 train_time:224187ms step_avg:180.94ms\nstep:1250/5000 train_loss:8.0522 train_time:224367ms step_avg:180.94ms\nstep:1251/5000 train_loss:8.1262 train_time:224548ms step_avg:180.94ms\nstep:1252/5000 train_loss:8.3229 train_time:224728ms step_avg:180.94ms\nstep:1253/5000 train_loss:8.1587 train_time:224910ms step_avg:180.94ms\nstep:1254/5000 train_loss:8.0854 train_time:225091ms step_avg:180.94ms\nstep:1255/5000 train_loss:8.2132 train_time:225272ms step_avg:180.94ms\nstep:1256/5000 train_loss:8.2650 train_time:225455ms step_avg:180.94ms\nstep:1257/5000 train_loss:8.1497 train_time:225636ms step_avg:180.94ms\nstep:1258/5000 train_loss:8.1486 train_time:225818ms step_avg:180.94ms\nstep:1259/5000 train_loss:8.1331 train_time:226000ms step_avg:180.94ms\nstep:1260/5000 train_loss:8.1618 train_time:226181ms step_avg:180.94ms\nstep:1261/5000 train_loss:8.2297 train_time:226362ms step_avg:180.94ms\nstep:1262/5000 train_loss:8.0084 train_time:226543ms step_avg:180.95ms\nstep:1263/5000 train_loss:8.2047 train_time:226725ms step_avg:180.95ms\nstep:1264/5000 train_loss:8.1550 train_time:226906ms step_avg:180.95ms\nstep:1265/5000 train_loss:8.1285 train_time:227087ms step_avg:180.95ms\nstep:1266/5000 train_loss:8.1807 train_time:227267ms step_avg:180.94ms\nstep:1267/5000 train_loss:8.0971 train_time:227448ms step_avg:180.95ms\nstep:1268/5000 train_loss:8.1121 train_time:227634ms step_avg:180.95ms\nstep:1269/5000 train_loss:8.0085 train_time:227816ms step_avg:180.95ms\nstep:1270/5000 train_loss:8.1360 train_time:227998ms step_avg:180.95ms\nstep:1271/5000 train_loss:8.1169 train_time:228180ms step_avg:180.95ms\nstep:1272/5000 train_loss:8.0294 train_time:228359ms step_avg:180.95ms\nstep:1273/5000 train_loss:8.0136 train_time:228541ms step_avg:180.95ms\nstep:1274/5000 train_loss:8.0544 train_time:228724ms step_avg:180.95ms\nstep:1275/5000 train_loss:8.1179 train_time:228906ms step_avg:180.95ms\nstep:1276/5000 train_loss:8.0422 train_time:229087ms step_avg:180.95ms\nstep:1277/5000 train_loss:8.1929 train_time:229266ms step_avg:180.95ms\nstep:1278/5000 train_loss:8.1514 train_time:229447ms step_avg:180.95ms\nstep:1279/5000 train_loss:8.1620 train_time:229626ms step_avg:180.95ms\nstep:1280/5000 train_loss:7.9972 train_time:229807ms step_avg:180.95ms\nstep:1281/5000 train_loss:8.1208 train_time:229988ms step_avg:180.95ms\nstep:1282/5000 train_loss:8.0097 train_time:230168ms step_avg:180.95ms\nstep:1283/5000 train_loss:8.0964 train_time:230349ms step_avg:180.95ms\nstep:1284/5000 train_loss:8.1168 train_time:230529ms step_avg:180.95ms\nstep:1285/5000 train_loss:8.0495 train_time:230710ms step_avg:180.95ms\nstep:1286/5000 train_loss:8.1824 train_time:230891ms step_avg:180.95ms\nstep:1287/5000 train_loss:8.1894 train_time:231073ms step_avg:180.95ms\nstep:1288/5000 train_loss:8.0171 train_time:231256ms step_avg:180.95ms\nstep:1289/5000 train_loss:8.0952 train_time:231437ms step_avg:180.95ms\nstep:1290/5000 train_loss:8.0805 train_time:231620ms step_avg:180.95ms\nstep:1291/5000 train_loss:8.0631 train_time:231802ms step_avg:180.95ms\nstep:1292/5000 train_loss:8.1544 train_time:231983ms step_avg:180.95ms\nstep:1293/5000 train_loss:8.1320 train_time:232164ms step_avg:180.95ms\nstep:1294/5000 train_loss:8.1531 train_time:232346ms step_avg:180.95ms\nstep:1295/5000 train_loss:8.1446 train_time:232527ms step_avg:180.95ms\nstep:1296/5000 train_loss:8.3638 train_time:232708ms step_avg:180.96ms\nstep:1297/5000 train_loss:8.0648 train_time:232890ms step_avg:180.96ms\nstep:1298/5000 train_loss:8.1279 train_time:233071ms step_avg:180.96ms\nstep:1299/5000 train_loss:8.1589 train_time:233252ms step_avg:180.96ms\nstep:1300/5000 train_loss:8.1990 train_time:233434ms step_avg:180.96ms\nstep:1301/5000 train_loss:8.1402 train_time:233616ms step_avg:180.96ms\nstep:1302/5000 train_loss:8.0773 train_time:233800ms step_avg:180.96ms\nstep:1303/5000 train_loss:8.2448 train_time:233981ms step_avg:180.96ms\nstep:1304/5000 train_loss:8.1117 train_time:234163ms step_avg:180.96ms\nstep:1305/5000 train_loss:8.0754 train_time:234346ms step_avg:180.96ms\nstep:1306/5000 train_loss:8.1532 train_time:234527ms step_avg:180.96ms\nstep:1307/5000 train_loss:8.1802 train_time:234709ms step_avg:180.96ms\nstep:1308/5000 train_loss:8.2539 train_time:234891ms step_avg:180.96ms\nstep:1309/5000 train_loss:7.9766 train_time:235072ms step_avg:180.96ms\nstep:1310/5000 train_loss:7.9838 train_time:235255ms step_avg:180.97ms\nstep:1311/5000 train_loss:8.1216 train_time:235437ms step_avg:180.97ms\nstep:1312/5000 train_loss:8.2189 train_time:235619ms step_avg:180.97ms\nstep:1313/5000 train_loss:8.2168 train_time:235802ms step_avg:180.97ms\nstep:1314/5000 train_loss:8.2276 train_time:235982ms step_avg:180.97ms\nstep:1315/5000 train_loss:8.1191 train_time:236162ms step_avg:180.97ms\nstep:1316/5000 train_loss:8.1336 train_time:236344ms step_avg:180.97ms\nstep:1317/5000 train_loss:8.0424 train_time:236525ms step_avg:180.97ms\nstep:1318/5000 train_loss:8.0274 train_time:236706ms step_avg:180.97ms\nstep:1319/5000 train_loss:8.1154 train_time:236887ms step_avg:180.97ms\nstep:1320/5000 train_loss:8.0964 train_time:237067ms step_avg:180.97ms\nstep:1321/5000 train_loss:8.1589 train_time:237248ms step_avg:180.97ms\nstep:1322/5000 train_loss:8.1438 train_time:237428ms step_avg:180.97ms\nstep:1323/5000 train_loss:8.1148 train_time:237610ms step_avg:180.97ms\nstep:1324/5000 train_loss:8.1139 train_time:237791ms step_avg:180.97ms\nstep:1325/5000 train_loss:8.0639 train_time:237971ms step_avg:180.97ms\nstep:1326/5000 train_loss:8.3979 train_time:238154ms step_avg:180.97ms\nstep:1327/5000 train_loss:8.0922 train_time:238336ms step_avg:180.97ms\nstep:1328/5000 train_loss:7.9932 train_time:238518ms step_avg:180.97ms\nstep:1329/5000 train_loss:8.0769 train_time:238700ms step_avg:180.97ms\nstep:1330/5000 train_loss:8.0642 train_time:238881ms step_avg:180.97ms\nstep:1331/5000 train_loss:8.0245 train_time:239062ms step_avg:180.97ms\nstep:1332/5000 train_loss:8.0722 train_time:239244ms step_avg:180.97ms\nstep:1333/5000 train_loss:8.0370 train_time:239427ms step_avg:180.97ms\nstep:1334/5000 train_loss:8.0960 train_time:239608ms step_avg:180.97ms\nstep:1335/5000 train_loss:8.0937 train_time:239789ms step_avg:180.97ms\nstep:1336/5000 train_loss:8.0527 train_time:239970ms step_avg:180.97ms\nstep:1337/5000 train_loss:8.1350 train_time:240151ms step_avg:180.97ms\nstep:1338/5000 train_loss:8.1332 train_time:240332ms step_avg:180.97ms\nstep:1339/5000 train_loss:8.1004 train_time:240516ms step_avg:180.98ms\nstep:1340/5000 train_loss:8.2242 train_time:240699ms step_avg:180.98ms\nstep:1341/5000 train_loss:8.1933 train_time:240881ms step_avg:180.98ms\nstep:1342/5000 train_loss:8.1326 train_time:241061ms step_avg:180.98ms\nstep:1343/5000 train_loss:8.0274 train_time:241241ms step_avg:180.98ms\nstep:1344/5000 train_loss:8.1562 train_time:241423ms step_avg:180.98ms\nstep:1345/5000 train_loss:8.0291 train_time:241606ms step_avg:180.98ms\nstep:1346/5000 train_loss:8.0691 train_time:241787ms step_avg:180.98ms\nstep:1347/5000 train_loss:8.0527 train_time:241968ms step_avg:180.98ms\nstep:1348/5000 train_loss:8.3102 train_time:242148ms step_avg:180.98ms\nstep:1349/5000 train_loss:8.6143 train_time:242327ms step_avg:180.98ms\nstep:1350/5000 train_loss:8.1663 train_time:242509ms step_avg:180.98ms\nstep:1351/5000 train_loss:8.1502 train_time:242690ms step_avg:180.98ms\nstep:1352/5000 train_loss:8.1572 train_time:242871ms step_avg:180.98ms\nstep:1353/5000 train_loss:8.1699 train_time:243054ms step_avg:180.98ms\nstep:1354/5000 train_loss:8.0341 train_time:243237ms step_avg:180.98ms\nstep:1355/5000 train_loss:8.1932 train_time:243419ms step_avg:180.98ms\nstep:1356/5000 train_loss:8.1795 train_time:243601ms step_avg:180.98ms\nstep:1357/5000 train_loss:8.1175 train_time:243782ms step_avg:180.98ms\nstep:1358/5000 train_loss:8.1057 train_time:243963ms step_avg:180.98ms\nstep:1359/5000 train_loss:8.0966 train_time:244145ms step_avg:180.98ms\nstep:1360/5000 train_loss:8.0245 train_time:244325ms step_avg:180.98ms\nstep:1361/5000 train_loss:8.0371 train_time:244507ms step_avg:180.98ms\nstep:1362/5000 train_loss:7.9982 train_time:244689ms step_avg:180.98ms\nstep:1363/5000 train_loss:8.0226 train_time:244869ms step_avg:180.98ms\nstep:1364/5000 train_loss:8.0755 train_time:245050ms step_avg:180.98ms\nstep:1365/5000 train_loss:8.1281 train_time:245232ms step_avg:180.98ms\nstep:1366/5000 train_loss:8.1459 train_time:245415ms step_avg:180.98ms\nstep:1367/5000 train_loss:7.9831 train_time:245597ms step_avg:180.99ms\nstep:1368/5000 train_loss:8.0968 train_time:245778ms step_avg:180.99ms\nstep:1369/5000 train_loss:8.1283 train_time:245960ms step_avg:180.99ms\nstep:1370/5000 train_loss:8.0701 train_time:246142ms step_avg:180.99ms\nstep:1371/5000 train_loss:8.1401 train_time:246322ms step_avg:180.99ms\nstep:1372/5000 train_loss:8.1368 train_time:246504ms step_avg:180.99ms\nstep:1373/5000 train_loss:8.0039 train_time:246685ms step_avg:180.99ms\nstep:1374/5000 train_loss:8.2061 train_time:246865ms step_avg:180.99ms\nstep:1375/5000 train_loss:8.0605 train_time:247046ms step_avg:180.99ms\nstep:1376/5000 train_loss:8.1555 train_time:247227ms step_avg:180.99ms\nstep:1377/5000 train_loss:7.9508 train_time:247408ms step_avg:180.99ms\nstep:1378/5000 train_loss:8.1102 train_time:247589ms step_avg:180.99ms\nstep:1379/5000 train_loss:8.2096 train_time:247772ms step_avg:180.99ms\nstep:1380/5000 train_loss:8.0809 train_time:247959ms step_avg:180.99ms\nstep:1381/5000 train_loss:8.0342 train_time:248142ms step_avg:180.99ms\nstep:1382/5000 train_loss:8.1142 train_time:248324ms step_avg:180.99ms\nstep:1383/5000 train_loss:8.0931 train_time:248505ms step_avg:180.99ms\nstep:1384/5000 train_loss:8.1463 train_time:248685ms step_avg:180.99ms\nstep:1385/5000 train_loss:8.0427 train_time:248865ms step_avg:180.99ms\nstep:1386/5000 train_loss:8.1605 train_time:249046ms step_avg:180.99ms\nstep:1387/5000 train_loss:7.9864 train_time:249225ms step_avg:180.99ms\nstep:1388/5000 train_loss:8.1767 train_time:249407ms step_avg:180.99ms\nstep:1389/5000 train_loss:8.0313 train_time:249587ms step_avg:180.99ms\nstep:1390/5000 train_loss:7.9964 train_time:249768ms step_avg:180.99ms\nstep:1391/5000 train_loss:8.0547 train_time:249949ms step_avg:180.99ms\nstep:1392/5000 train_loss:8.0935 train_time:250130ms step_avg:180.99ms\nstep:1393/5000 train_loss:8.1025 train_time:250312ms step_avg:180.99ms\nstep:1394/5000 train_loss:8.1465 train_time:250493ms step_avg:180.99ms\nstep:1395/5000 train_loss:7.9875 train_time:250675ms step_avg:180.99ms\nstep:1396/5000 train_loss:8.0087 train_time:250857ms step_avg:180.99ms\nstep:1397/5000 train_loss:7.8992 train_time:251040ms step_avg:180.99ms\nstep:1398/5000 train_loss:8.1563 train_time:251223ms step_avg:181.00ms\nstep:1399/5000 train_loss:7.8929 train_time:251404ms step_avg:181.00ms\nstep:1400/5000 train_loss:7.7772 train_time:251584ms step_avg:181.00ms\nstep:1401/5000 train_loss:8.0535 train_time:251764ms step_avg:181.00ms\nstep:1402/5000 train_loss:8.1214 train_time:251945ms step_avg:181.00ms\nstep:1403/5000 train_loss:8.0278 train_time:252125ms step_avg:180.99ms\nstep:1404/5000 train_loss:8.0289 train_time:252308ms step_avg:181.00ms\nstep:1405/5000 train_loss:8.1675 train_time:252490ms step_avg:181.00ms\nstep:1406/5000 train_loss:8.1160 train_time:252671ms step_avg:181.00ms\nstep:1407/5000 train_loss:8.1329 train_time:252852ms step_avg:181.00ms\nstep:1408/5000 train_loss:8.1765 train_time:253034ms step_avg:181.00ms\nstep:1409/5000 train_loss:8.0756 train_time:253217ms step_avg:181.00ms\nstep:1410/5000 train_loss:8.1123 train_time:253400ms step_avg:181.00ms\nstep:1411/5000 train_loss:8.0075 train_time:253582ms step_avg:181.00ms\nstep:1412/5000 train_loss:8.0638 train_time:253762ms step_avg:181.00ms\nstep:1413/5000 train_loss:8.1476 train_time:253942ms step_avg:181.00ms\nstep:1414/5000 train_loss:8.1054 train_time:254124ms step_avg:181.00ms\nstep:1415/5000 train_loss:8.1140 train_time:254307ms step_avg:181.00ms\nstep:1416/5000 train_loss:8.0927 train_time:254488ms step_avg:181.00ms\nstep:1417/5000 train_loss:8.0521 train_time:254668ms step_avg:181.00ms\nstep:1418/5000 train_loss:8.1551 train_time:254849ms step_avg:181.00ms\nstep:1419/5000 train_loss:8.0495 train_time:255030ms step_avg:181.00ms\nstep:1420/5000 train_loss:8.1468 train_time:255212ms step_avg:181.00ms\nstep:1421/5000 train_loss:8.1232 train_time:255395ms step_avg:181.00ms\nstep:1422/5000 train_loss:8.0810 train_time:255578ms step_avg:181.00ms\nstep:1423/5000 train_loss:8.1553 train_time:255760ms step_avg:181.00ms\nstep:1424/5000 train_loss:8.0702 train_time:255942ms step_avg:181.01ms\nstep:1425/5000 train_loss:8.0545 train_time:256124ms step_avg:181.01ms\nstep:1426/5000 train_loss:8.0894 train_time:256305ms step_avg:181.01ms\nstep:1427/5000 train_loss:7.9915 train_time:256485ms step_avg:181.01ms\nstep:1428/5000 train_loss:8.2248 train_time:256666ms step_avg:181.01ms\nstep:1429/5000 train_loss:7.9964 train_time:256847ms step_avg:181.01ms\nstep:1430/5000 train_loss:8.1130 train_time:257028ms step_avg:181.01ms\nstep:1431/5000 train_loss:8.1839 train_time:257210ms step_avg:181.01ms\nstep:1432/5000 train_loss:8.1274 train_time:257391ms step_avg:181.01ms\nstep:1433/5000 train_loss:8.0753 train_time:257572ms step_avg:181.01ms\nstep:1434/5000 train_loss:8.1554 train_time:257758ms step_avg:181.01ms\nstep:1435/5000 train_loss:8.1967 train_time:257937ms step_avg:181.01ms\nstep:1436/5000 train_loss:8.4601 train_time:258121ms step_avg:181.01ms\nstep:1437/5000 train_loss:8.1251 train_time:258303ms step_avg:181.01ms\nstep:1438/5000 train_loss:7.9770 train_time:258485ms step_avg:181.01ms\nstep:1439/5000 train_loss:8.1543 train_time:258666ms step_avg:181.01ms\nstep:1440/5000 train_loss:8.0101 train_time:258847ms step_avg:181.01ms\nstep:1441/5000 train_loss:8.1357 train_time:259027ms step_avg:181.01ms\nstep:1442/5000 train_loss:8.2309 train_time:259208ms step_avg:181.01ms\nstep:1443/5000 train_loss:8.1722 train_time:259389ms step_avg:181.01ms\nstep:1444/5000 train_loss:8.2087 train_time:259570ms step_avg:181.01ms\nstep:1445/5000 train_loss:8.1415 train_time:259751ms step_avg:181.01ms\nstep:1446/5000 train_loss:8.1368 train_time:259934ms step_avg:181.01ms\nstep:1447/5000 train_loss:8.0324 train_time:260116ms step_avg:181.01ms\nstep:1448/5000 train_loss:8.2229 train_time:260298ms step_avg:181.01ms\nstep:1449/5000 train_loss:8.1680 train_time:260479ms step_avg:181.01ms\nstep:1450/5000 train_loss:8.1474 train_time:260663ms step_avg:181.02ms\nstep:1451/5000 train_loss:8.1286 train_time:260844ms step_avg:181.02ms\nstep:1452/5000 train_loss:8.0797 train_time:261025ms step_avg:181.02ms\nstep:1453/5000 train_loss:8.0590 train_time:261207ms step_avg:181.02ms\nstep:1454/5000 train_loss:8.1392 train_time:261387ms step_avg:181.02ms\nstep:1455/5000 train_loss:8.0503 train_time:261568ms step_avg:181.02ms\nstep:1456/5000 train_loss:8.1540 train_time:261750ms step_avg:181.02ms\nstep:1457/5000 train_loss:8.8024 train_time:261931ms step_avg:181.02ms\nstep:1458/5000 train_loss:8.1656 train_time:262115ms step_avg:181.02ms\nstep:1459/5000 train_loss:8.1495 train_time:262298ms step_avg:181.02ms\nstep:1460/5000 train_loss:8.1363 train_time:262480ms step_avg:181.02ms\nstep:1461/5000 train_loss:8.0685 train_time:262661ms step_avg:181.02ms\nstep:1462/5000 train_loss:8.2155 train_time:262843ms step_avg:181.02ms\nstep:1463/5000 train_loss:8.1470 train_time:263024ms step_avg:181.02ms\nstep:1464/5000 train_loss:8.0801 train_time:263205ms step_avg:181.02ms\nstep:1465/5000 train_loss:8.0562 train_time:263385ms step_avg:181.02ms\nstep:1466/5000 train_loss:8.0997 train_time:263565ms step_avg:181.02ms\nstep:1467/5000 train_loss:8.1440 train_time:263745ms step_avg:181.02ms\nstep:1468/5000 train_loss:8.0882 train_time:263926ms step_avg:181.02ms\nstep:1469/5000 train_loss:8.1512 train_time:264108ms step_avg:181.02ms\nstep:1470/5000 train_loss:8.1458 train_time:264289ms step_avg:181.02ms\nstep:1471/5000 train_loss:8.0908 train_time:264471ms step_avg:181.02ms\nstep:1472/5000 train_loss:8.1115 train_time:264653ms step_avg:181.02ms\nstep:1473/5000 train_loss:8.0086 train_time:264836ms step_avg:181.02ms\nstep:1474/5000 train_loss:8.1457 train_time:265019ms step_avg:181.02ms\nstep:1475/5000 train_loss:8.1029 train_time:265201ms step_avg:181.02ms\nstep:1476/5000 train_loss:8.0294 train_time:265381ms step_avg:181.02ms\nstep:1477/5000 train_loss:8.2265 train_time:265563ms step_avg:181.02ms\nstep:1478/5000 train_loss:8.0675 train_time:265744ms step_avg:181.02ms\nstep:1479/5000 train_loss:8.0107 train_time:265925ms step_avg:181.02ms\nstep:1480/5000 train_loss:8.0542 train_time:266106ms step_avg:181.02ms\nstep:1481/5000 train_loss:8.0755 train_time:266288ms step_avg:181.02ms\nstep:1482/5000 train_loss:8.0223 train_time:266468ms step_avg:181.02ms\nstep:1483/5000 train_loss:8.1320 train_time:266648ms step_avg:181.02ms\nstep:1484/5000 train_loss:8.2552 train_time:266828ms step_avg:181.02ms\nstep:1485/5000 train_loss:8.1104 train_time:267010ms step_avg:181.02ms\nstep:1486/5000 train_loss:8.1086 train_time:267192ms step_avg:181.02ms\nstep:1487/5000 train_loss:8.0848 train_time:267373ms step_avg:181.02ms\nstep:1488/5000 train_loss:8.0358 train_time:267556ms step_avg:181.03ms\nstep:1489/5000 train_loss:8.0314 train_time:267738ms step_avg:181.03ms\nstep:1490/5000 train_loss:8.0512 train_time:267921ms step_avg:181.03ms\nstep:1491/5000 train_loss:8.3169 train_time:268102ms step_avg:181.03ms\nstep:1492/5000 train_loss:8.0993 train_time:268284ms step_avg:181.03ms\nstep:1493/5000 train_loss:8.1119 train_time:268465ms step_avg:181.03ms\nstep:1494/5000 train_loss:8.1778 train_time:268645ms step_avg:181.03ms\nstep:1495/5000 train_loss:8.1431 train_time:268826ms step_avg:181.03ms\nstep:1496/5000 train_loss:8.0946 train_time:269007ms step_avg:181.03ms\nstep:1497/5000 train_loss:8.0172 train_time:269188ms step_avg:181.03ms\nstep:1498/5000 train_loss:8.0560 train_time:269369ms step_avg:181.03ms\nstep:1499/5000 train_loss:8.0400 train_time:269550ms step_avg:181.03ms\nstep:1500/5000 train_loss:8.0968 train_time:269731ms step_avg:181.03ms\nstep:1501/5000 train_loss:8.0525 train_time:269913ms step_avg:181.03ms\nstep:1502/5000 train_loss:8.2223 train_time:270096ms step_avg:181.03ms\nstep:1503/5000 train_loss:8.1254 train_time:270277ms step_avg:181.03ms\nstep:1504/5000 train_loss:8.1883 train_time:270459ms step_avg:181.03ms\nstep:1505/5000 train_loss:8.2062 train_time:270640ms step_avg:181.03ms\nstep:1506/5000 train_loss:8.1409 train_time:270823ms step_avg:181.03ms\nstep:1507/5000 train_loss:7.9966 train_time:271004ms step_avg:181.03ms\nstep:1508/5000 train_loss:8.0588 train_time:271184ms step_avg:181.03ms\nstep:1509/5000 train_loss:8.0540 train_time:271366ms step_avg:181.03ms\nstep:1510/5000 train_loss:7.4350 train_time:271546ms step_avg:181.03ms\nstep:1511/5000 train_loss:7.9996 train_time:271726ms step_avg:181.03ms\nstep:1512/5000 train_loss:8.1214 train_time:271906ms step_avg:181.03ms\nstep:1513/5000 train_loss:8.1494 train_time:272087ms step_avg:181.03ms\nstep:1514/5000 train_loss:7.9911 train_time:272267ms step_avg:181.03ms\nstep:1515/5000 train_loss:7.9951 train_time:272448ms step_avg:181.03ms\nstep:1516/5000 train_loss:8.2225 train_time:272628ms step_avg:181.03ms\nstep:1517/5000 train_loss:8.1061 train_time:272811ms step_avg:181.03ms\nstep:1518/5000 train_loss:8.1170 train_time:272992ms step_avg:181.03ms\nstep:1519/5000 train_loss:8.2840 train_time:273174ms step_avg:181.03ms\nstep:1520/5000 train_loss:8.1985 train_time:273357ms step_avg:181.03ms\nstep:1521/5000 train_loss:8.0637 train_time:273540ms step_avg:181.03ms\nstep:1522/5000 train_loss:8.1534 train_time:273722ms step_avg:181.03ms\nstep:1523/5000 train_loss:8.1821 train_time:273903ms step_avg:181.03ms\nstep:1524/5000 train_loss:8.0737 train_time:274315ms step_avg:181.19ms\nstep:1525/5000 train_loss:8.0266 train_time:274495ms step_avg:181.18ms\nstep:1526/5000 train_loss:8.1932 train_time:274676ms step_avg:181.18ms\nstep:1527/5000 train_loss:8.2156 train_time:274857ms step_avg:181.18ms\nstep:1528/5000 train_loss:8.1830 train_time:275038ms step_avg:181.18ms\nstep:1529/5000 train_loss:8.0308 train_time:275230ms step_avg:181.19ms\nstep:1530/5000 train_loss:8.2743 train_time:275411ms step_avg:181.19ms\nstep:1531/5000 train_loss:8.0935 train_time:275592ms step_avg:181.19ms\nstep:1532/5000 train_loss:8.1066 train_time:275773ms step_avg:181.19ms\nstep:1533/5000 train_loss:8.1564 train_time:275953ms step_avg:181.19ms\nstep:1534/5000 train_loss:7.9995 train_time:276137ms step_avg:181.19ms\nstep:1535/5000 train_loss:8.1013 train_time:276322ms step_avg:181.19ms\nstep:1536/5000 train_loss:8.1044 train_time:276503ms step_avg:181.19ms\nstep:1537/5000 train_loss:8.1414 train_time:276682ms step_avg:181.19ms\nstep:1538/5000 train_loss:8.0726 train_time:276862ms step_avg:181.19ms\nstep:1539/5000 train_loss:8.0621 train_time:277045ms step_avg:181.19ms\nstep:1540/5000 train_loss:8.0658 train_time:277226ms step_avg:181.19ms\nstep:1541/5000 train_loss:8.1849 train_time:277409ms step_avg:181.19ms\nstep:1542/5000 train_loss:8.0832 train_time:277590ms step_avg:181.19ms\nstep:1543/5000 train_loss:8.1232 train_time:277771ms step_avg:181.19ms\nstep:1544/5000 train_loss:8.1640 train_time:277952ms step_avg:181.19ms\nstep:1545/5000 train_loss:8.1435 train_time:278134ms step_avg:181.20ms\nstep:1546/5000 train_loss:8.1681 train_time:278317ms step_avg:181.20ms\nstep:1547/5000 train_loss:8.0969 train_time:278500ms step_avg:181.20ms\nstep:1548/5000 train_loss:8.0618 train_time:278681ms step_avg:181.20ms\nstep:1549/5000 train_loss:8.0795 train_time:278861ms step_avg:181.20ms\nstep:1550/5000 train_loss:8.2660 train_time:279041ms step_avg:181.20ms\nstep:1551/5000 train_loss:8.1190 train_time:279223ms step_avg:181.20ms\nstep:1552/5000 train_loss:8.0343 train_time:279405ms step_avg:181.20ms\nstep:1553/5000 train_loss:8.1752 train_time:279586ms step_avg:181.20ms\nstep:1554/5000 train_loss:8.1247 train_time:279767ms step_avg:181.20ms\nstep:1555/5000 train_loss:8.0600 train_time:279947ms step_avg:181.20ms\nstep:1556/5000 train_loss:8.1364 train_time:280127ms step_avg:181.19ms\nstep:1557/5000 train_loss:8.1263 train_time:280309ms step_avg:181.20ms\nstep:1558/5000 train_loss:8.0064 train_time:280490ms step_avg:181.20ms\nstep:1559/5000 train_loss:8.1483 train_time:280671ms step_avg:181.20ms\nstep:1560/5000 train_loss:8.0633 train_time:280855ms step_avg:181.20ms\nstep:1561/5000 train_loss:8.0556 train_time:281036ms step_avg:181.20ms\nstep:1562/5000 train_loss:8.1389 train_time:281219ms step_avg:181.20ms\nstep:1563/5000 train_loss:8.1746 train_time:281402ms step_avg:181.20ms\nstep:1564/5000 train_loss:8.1810 train_time:281583ms step_avg:181.20ms\nstep:1565/5000 train_loss:8.0823 train_time:281764ms step_avg:181.20ms\nstep:1566/5000 train_loss:8.0742 train_time:281945ms step_avg:181.20ms\nstep:1567/5000 train_loss:8.1209 train_time:282127ms step_avg:181.20ms\nstep:1568/5000 train_loss:8.1272 train_time:282309ms step_avg:181.20ms\nstep:1569/5000 train_loss:8.0696 train_time:282490ms step_avg:181.20ms\nstep:1570/5000 train_loss:8.1484 train_time:282672ms step_avg:181.20ms\nstep:1571/5000 train_loss:8.2193 train_time:282855ms step_avg:181.20ms\nstep:1572/5000 train_loss:8.0249 train_time:283037ms step_avg:181.20ms\nstep:1573/5000 train_loss:8.0777 train_time:283220ms step_avg:181.20ms\nstep:1574/5000 train_loss:8.0322 train_time:283401ms step_avg:181.20ms\nstep:1575/5000 train_loss:8.1441 train_time:283581ms step_avg:181.20ms\nstep:1576/5000 train_loss:8.1327 train_time:283763ms step_avg:181.20ms\nstep:1577/5000 train_loss:8.1840 train_time:283944ms step_avg:181.20ms\nstep:1578/5000 train_loss:8.0916 train_time:284126ms step_avg:181.20ms\nstep:1579/5000 train_loss:8.1115 train_time:284307ms step_avg:181.20ms\nstep:1580/5000 train_loss:8.0652 train_time:284488ms step_avg:181.20ms\nstep:1581/5000 train_loss:8.1474 train_time:284668ms step_avg:181.20ms\nstep:1582/5000 train_loss:8.0965 train_time:284850ms step_avg:181.20ms\nstep:1583/5000 train_loss:8.0920 train_time:285030ms step_avg:181.20ms\nstep:1584/5000 train_loss:8.1862 train_time:285212ms step_avg:181.20ms\nstep:1585/5000 train_loss:8.0573 train_time:285396ms step_avg:181.20ms\nstep:1586/5000 train_loss:8.0456 train_time:285578ms step_avg:181.20ms\nstep:1587/5000 train_loss:8.0614 train_time:285760ms step_avg:181.20ms\nstep:1588/5000 train_loss:7.9688 train_time:285941ms step_avg:181.20ms\nstep:1589/5000 train_loss:8.0858 train_time:286123ms step_avg:181.20ms\nstep:1590/5000 train_loss:8.2798 train_time:286304ms step_avg:181.20ms\nstep:1591/5000 train_loss:8.1515 train_time:286484ms step_avg:181.20ms\nstep:1592/5000 train_loss:8.0245 train_time:286666ms step_avg:181.20ms\nstep:1593/5000 train_loss:8.2099 train_time:286846ms step_avg:181.20ms\nstep:1594/5000 train_loss:8.3397 train_time:287028ms step_avg:181.20ms\nstep:1595/5000 train_loss:8.0815 train_time:287210ms step_avg:181.21ms\nstep:1596/5000 train_loss:8.0829 train_time:287392ms step_avg:181.21ms\nstep:1597/5000 train_loss:8.1495 train_time:287575ms step_avg:181.21ms\nstep:1598/5000 train_loss:8.1030 train_time:287756ms step_avg:181.21ms\nstep:1599/5000 train_loss:8.0830 train_time:287939ms step_avg:181.21ms\nstep:1600/5000 train_loss:8.0965 train_time:288121ms step_avg:181.21ms\nstep:1601/5000 train_loss:8.3283 train_time:288302ms step_avg:181.21ms\nstep:1602/5000 train_loss:8.2874 train_time:288484ms step_avg:181.21ms\nstep:1603/5000 train_loss:8.1725 train_time:288664ms step_avg:181.21ms\nstep:1604/5000 train_loss:8.1965 train_time:288845ms step_avg:181.21ms\nstep:1605/5000 train_loss:8.0363 train_time:289026ms step_avg:181.21ms\nstep:1606/5000 train_loss:8.1058 train_time:289207ms step_avg:181.21ms\nstep:1607/5000 train_loss:8.2893 train_time:289388ms step_avg:181.21ms\nstep:1608/5000 train_loss:8.1423 train_time:289569ms step_avg:181.21ms\nstep:1609/5000 train_loss:8.0978 train_time:289749ms step_avg:181.21ms\nstep:1610/5000 train_loss:8.0981 train_time:289929ms step_avg:181.21ms\nstep:1611/5000 train_loss:8.1340 train_time:290112ms step_avg:181.21ms\nstep:1612/5000 train_loss:8.2114 train_time:290294ms step_avg:181.21ms\nstep:1613/5000 train_loss:8.1450 train_time:290475ms step_avg:181.21ms\nstep:1614/5000 train_loss:8.1039 train_time:290656ms step_avg:181.21ms\nstep:1615/5000 train_loss:8.0008 train_time:290839ms step_avg:181.21ms\nstep:1616/5000 train_loss:8.2172 train_time:291021ms step_avg:181.21ms\nstep:1617/5000 train_loss:8.1835 train_time:291202ms step_avg:181.21ms\nstep:1618/5000 train_loss:8.3343 train_time:291382ms step_avg:181.21ms\nstep:1619/5000 train_loss:8.0904 train_time:291563ms step_avg:181.21ms\nstep:1620/5000 train_loss:8.1300 train_time:291745ms step_avg:181.21ms\nstep:1621/5000 train_loss:8.1430 train_time:291926ms step_avg:181.21ms\nstep:1622/5000 train_loss:8.0878 train_time:292108ms step_avg:181.21ms\nstep:1623/5000 train_loss:8.1288 train_time:292289ms step_avg:181.21ms\nstep:1624/5000 train_loss:8.1712 train_time:292469ms step_avg:181.21ms\nstep:1625/5000 train_loss:8.2106 train_time:292651ms step_avg:181.21ms\nstep:1626/5000 train_loss:8.1842 train_time:292832ms step_avg:181.21ms\nstep:1627/5000 train_loss:8.0788 train_time:293016ms step_avg:181.21ms\nstep:1628/5000 train_loss:8.0598 train_time:293198ms step_avg:181.21ms\nstep:1629/5000 train_loss:8.2156 train_time:293379ms step_avg:181.21ms\nstep:1630/5000 train_loss:8.0921 train_time:293560ms step_avg:181.21ms\nstep:1631/5000 train_loss:8.1303 train_time:293742ms step_avg:181.21ms\nstep:1632/5000 train_loss:8.0603 train_time:293924ms step_avg:181.21ms\nstep:1633/5000 train_loss:8.2086 train_time:294106ms step_avg:181.21ms\nstep:1634/5000 train_loss:8.1244 train_time:294287ms step_avg:181.21ms\nstep:1635/5000 train_loss:8.0913 train_time:294468ms step_avg:181.21ms\nstep:1636/5000 train_loss:8.1796 train_time:294648ms step_avg:181.21ms\nstep:1637/5000 train_loss:8.1616 train_time:294828ms step_avg:181.21ms\nstep:1638/5000 train_loss:8.1061 train_time:295010ms step_avg:181.21ms\nstep:1639/5000 train_loss:8.1091 train_time:295191ms step_avg:181.21ms\nstep:1640/5000 train_loss:8.0916 train_time:295373ms step_avg:181.21ms\nstep:1641/5000 train_loss:8.0375 train_time:295555ms step_avg:181.21ms\nstep:1642/5000 train_loss:8.0776 train_time:295737ms step_avg:181.21ms\nstep:1643/5000 train_loss:8.0189 train_time:295919ms step_avg:181.21ms\nstep:1644/5000 train_loss:8.1053 train_time:296102ms step_avg:181.21ms\nstep:1645/5000 train_loss:8.0702 train_time:296283ms step_avg:181.21ms\nstep:1646/5000 train_loss:8.0994 train_time:296464ms step_avg:181.21ms\nstep:1647/5000 train_loss:8.0762 train_time:296645ms step_avg:181.21ms\nstep:1648/5000 train_loss:8.0895 train_time:296825ms step_avg:181.21ms\nstep:1649/5000 train_loss:8.1409 train_time:297006ms step_avg:181.21ms\nstep:1650/5000 train_loss:8.2426 train_time:297186ms step_avg:181.21ms\nstep:1651/5000 train_loss:8.1273 train_time:297366ms step_avg:181.21ms\nstep:1652/5000 train_loss:8.1232 train_time:297547ms step_avg:181.21ms\nstep:1653/5000 train_loss:8.2093 train_time:297734ms step_avg:181.21ms\nstep:1654/5000 train_loss:8.1233 train_time:297940ms step_avg:181.23ms\nstep:1655/5000 train_loss:8.0709 train_time:298120ms step_avg:181.23ms\nstep:1656/5000 train_loss:8.0381 train_time:298301ms step_avg:181.23ms\nstep:1657/5000 train_loss:8.0586 train_time:298481ms step_avg:181.23ms\nstep:1658/5000 train_loss:8.2292 train_time:298663ms step_avg:181.23ms\nstep:1659/5000 train_loss:8.1718 train_time:298845ms step_avg:181.23ms\nstep:1660/5000 train_loss:8.1921 train_time:299026ms step_avg:181.23ms\nstep:1661/5000 train_loss:8.1572 train_time:299207ms step_avg:181.23ms\nstep:1662/5000 train_loss:8.1117 train_time:299388ms step_avg:181.23ms\nstep:1663/5000 train_loss:8.0780 train_time:299569ms step_avg:181.23ms\nstep:1664/5000 train_loss:8.0728 train_time:299750ms step_avg:181.23ms\nstep:1665/5000 train_loss:8.2814 train_time:299931ms step_avg:181.23ms\nstep:1666/5000 train_loss:8.0779 train_time:300115ms step_avg:181.23ms\nstep:1667/5000 train_loss:7.9067 train_time:300297ms step_avg:181.23ms\nstep:1668/5000 train_loss:8.0347 train_time:300479ms step_avg:181.23ms\nstep:1669/5000 train_loss:8.1820 train_time:300661ms step_avg:181.23ms\nstep:1670/5000 train_loss:8.1168 train_time:300842ms step_avg:181.23ms\nstep:1671/5000 train_loss:8.1112 train_time:301024ms step_avg:181.23ms\nstep:1672/5000 train_loss:7.9611 train_time:301205ms step_avg:181.23ms\nstep:1673/5000 train_loss:8.0956 train_time:301386ms step_avg:181.23ms\nstep:1674/5000 train_loss:8.0581 train_time:301567ms step_avg:181.23ms\nstep:1675/5000 train_loss:8.0851 train_time:301748ms step_avg:181.23ms\nstep:1676/5000 train_loss:8.2156 train_time:301928ms step_avg:181.23ms\nstep:1677/5000 train_loss:7.9060 train_time:302111ms step_avg:181.23ms\nstep:1678/5000 train_loss:8.0855 train_time:302291ms step_avg:181.23ms\nstep:1679/5000 train_loss:8.0659 train_time:302474ms step_avg:181.23ms\nstep:1680/5000 train_loss:8.0047 train_time:302656ms step_avg:181.23ms\nstep:1681/5000 train_loss:8.0736 train_time:302839ms step_avg:181.23ms\nstep:1682/5000 train_loss:8.1838 train_time:303022ms step_avg:181.23ms\nstep:1683/5000 train_loss:8.1850 train_time:303204ms step_avg:181.23ms\nstep:1684/5000 train_loss:8.1566 train_time:303384ms step_avg:181.23ms\nstep:1685/5000 train_loss:8.1054 train_time:303564ms step_avg:181.23ms\nstep:1686/5000 train_loss:8.0341 train_time:303746ms step_avg:181.23ms\nstep:1687/5000 train_loss:8.0093 train_time:303928ms step_avg:181.23ms\nstep:1688/5000 train_loss:8.1417 train_time:304111ms step_avg:181.23ms\nstep:1689/5000 train_loss:8.1843 train_time:304294ms step_avg:181.24ms\nstep:1690/5000 train_loss:7.9690 train_time:304476ms step_avg:181.24ms\nstep:1691/5000 train_loss:8.1671 train_time:304659ms step_avg:181.24ms\nstep:1692/5000 train_loss:8.0825 train_time:304840ms step_avg:181.24ms\nstep:1693/5000 train_loss:8.0514 train_time:305023ms step_avg:181.24ms\nstep:1694/5000 train_loss:8.0985 train_time:305206ms step_avg:181.24ms\nstep:1695/5000 train_loss:8.1128 train_time:305385ms step_avg:181.24ms\nstep:1696/5000 train_loss:8.0126 train_time:305566ms step_avg:181.24ms\nstep:1697/5000 train_loss:8.1724 train_time:305747ms step_avg:181.24ms\nstep:1698/5000 train_loss:8.1276 train_time:305926ms step_avg:181.24ms\nstep:1699/5000 train_loss:8.1421 train_time:306108ms step_avg:181.24ms\nstep:1700/5000 train_loss:8.0765 train_time:306289ms step_avg:181.24ms\nstep:1701/5000 train_loss:8.0671 train_time:306471ms step_avg:181.24ms\nstep:1702/5000 train_loss:8.0617 train_time:306654ms step_avg:181.24ms\nstep:1703/5000 train_loss:8.1095 train_time:306835ms step_avg:181.24ms\nstep:1704/5000 train_loss:8.1685 train_time:307018ms step_avg:181.24ms\nstep:1705/5000 train_loss:8.1038 train_time:307200ms step_avg:181.24ms\nstep:1706/5000 train_loss:8.0730 train_time:307382ms step_avg:181.24ms\nstep:1707/5000 train_loss:8.1832 train_time:307562ms step_avg:181.24ms\nstep:1708/5000 train_loss:8.1351 train_time:307743ms step_avg:181.24ms\nstep:1709/5000 train_loss:8.0589 train_time:307924ms step_avg:181.24ms\nstep:1710/5000 train_loss:8.1854 train_time:308105ms step_avg:181.24ms\nstep:1711/5000 train_loss:8.1349 train_time:308286ms step_avg:181.24ms\nstep:1712/5000 train_loss:7.9595 train_time:308467ms step_avg:181.24ms\nstep:1713/5000 train_loss:8.0903 train_time:308648ms step_avg:181.24ms\nstep:1714/5000 train_loss:8.2370 train_time:308829ms step_avg:181.24ms\nstep:1715/5000 train_loss:8.2044 train_time:309012ms step_avg:181.24ms\nstep:1716/5000 train_loss:8.0655 train_time:309194ms step_avg:181.24ms\nstep:1717/5000 train_loss:7.9586 train_time:309376ms step_avg:181.24ms\nstep:1718/5000 train_loss:8.0469 train_time:309557ms step_avg:181.24ms\nstep:1719/5000 train_loss:8.0440 train_time:309740ms step_avg:181.24ms\nstep:1720/5000 train_loss:8.1127 train_time:309922ms step_avg:181.24ms\nstep:1721/5000 train_loss:8.1292 train_time:310104ms step_avg:181.24ms\nstep:1722/5000 train_loss:8.0582 train_time:310284ms step_avg:181.24ms\nstep:1723/5000 train_loss:8.0742 train_time:310465ms step_avg:181.24ms\nstep:1724/5000 train_loss:8.0588 train_time:310646ms step_avg:181.24ms\nstep:1725/5000 train_loss:7.9687 train_time:310826ms step_avg:181.24ms\nstep:1726/5000 train_loss:8.1953 train_time:311008ms step_avg:181.24ms\nstep:1727/5000 train_loss:8.3190 train_time:311189ms step_avg:181.24ms\nstep:1728/5000 train_loss:8.1279 train_time:311371ms step_avg:181.24ms\nstep:1729/5000 train_loss:8.2087 train_time:311552ms step_avg:181.24ms\nstep:1730/5000 train_loss:8.1055 train_time:311734ms step_avg:181.24ms\nstep:1731/5000 train_loss:8.1542 train_time:311917ms step_avg:181.24ms\nstep:1732/5000 train_loss:8.1991 train_time:312098ms step_avg:181.24ms\nstep:1733/5000 train_loss:8.1435 train_time:312281ms step_avg:181.24ms\nstep:1734/5000 train_loss:8.1989 train_time:312462ms step_avg:181.24ms\nstep:1735/5000 train_loss:8.0580 train_time:312642ms step_avg:181.24ms\nstep:1736/5000 train_loss:8.1422 train_time:312824ms step_avg:181.24ms\nstep:1737/5000 train_loss:8.0434 train_time:313006ms step_avg:181.24ms\nstep:1738/5000 train_loss:8.1248 train_time:313186ms step_avg:181.24ms\nstep:1739/5000 train_loss:8.1313 train_time:313367ms step_avg:181.24ms\nstep:1740/5000 train_loss:8.0494 train_time:313547ms step_avg:181.24ms\nstep:1741/5000 train_loss:8.1187 train_time:313726ms step_avg:181.24ms\nstep:1742/5000 train_loss:8.1110 train_time:313906ms step_avg:181.24ms\nstep:1743/5000 train_loss:8.1488 train_time:314087ms step_avg:181.24ms\nstep:1744/5000 train_loss:8.1365 train_time:314267ms step_avg:181.24ms\nstep:1745/5000 train_loss:8.0879 train_time:314448ms step_avg:181.24ms\nstep:1746/5000 train_loss:8.1206 train_time:314629ms step_avg:181.24ms\nstep:1747/5000 train_loss:8.1418 train_time:314812ms step_avg:181.24ms\nstep:1748/5000 train_loss:8.2360 train_time:314993ms step_avg:181.24ms\nstep:1749/5000 train_loss:8.1192 train_time:315176ms step_avg:181.24ms\nstep:1750/5000 train_loss:8.1329 train_time:315359ms step_avg:181.24ms\nstep:1751/5000 train_loss:8.2563 train_time:315542ms step_avg:181.24ms\nstep:1752/5000 train_loss:8.1272 train_time:315724ms step_avg:181.24ms\nstep:1753/5000 train_loss:8.1468 train_time:315906ms step_avg:181.24ms\nstep:1754/5000 train_loss:8.0982 train_time:316088ms step_avg:181.24ms\nstep:1755/5000 train_loss:8.0623 train_time:316268ms step_avg:181.24ms\nstep:1756/5000 train_loss:8.1578 train_time:316449ms step_avg:181.24ms\nstep:1757/5000 train_loss:8.1425 train_time:316629ms step_avg:181.24ms\nstep:1758/5000 train_loss:8.0201 train_time:316811ms step_avg:181.24ms\nstep:1759/5000 train_loss:8.1729 train_time:316993ms step_avg:181.24ms\nstep:1760/5000 train_loss:8.0612 train_time:317176ms step_avg:181.24ms\nstep:1761/5000 train_loss:8.1690 train_time:317358ms step_avg:181.24ms\nstep:1762/5000 train_loss:8.1406 train_time:317540ms step_avg:181.24ms\nstep:1763/5000 train_loss:8.0294 train_time:317722ms step_avg:181.24ms\nstep:1764/5000 train_loss:8.0410 train_time:317903ms step_avg:181.24ms\nstep:1765/5000 train_loss:8.0056 train_time:318085ms step_avg:181.24ms\nstep:1766/5000 train_loss:8.1262 train_time:318265ms step_avg:181.24ms\nstep:1767/5000 train_loss:8.0924 train_time:318445ms step_avg:181.24ms\nstep:1768/5000 train_loss:8.1634 train_time:318625ms step_avg:181.24ms\nstep:1769/5000 train_loss:8.3233 train_time:318806ms step_avg:181.24ms\nstep:1770/5000 train_loss:8.1540 train_time:318986ms step_avg:181.24ms\nstep:1771/5000 train_loss:8.1105 train_time:319166ms step_avg:181.24ms\nstep:1772/5000 train_loss:8.1099 train_time:319346ms step_avg:181.24ms\nstep:1773/5000 train_loss:8.1083 train_time:319526ms step_avg:181.24ms\nstep:1774/5000 train_loss:8.0440 train_time:319709ms step_avg:181.24ms\nstep:1775/5000 train_loss:8.0783 train_time:319890ms step_avg:181.24ms\nstep:1776/5000 train_loss:8.1404 train_time:320072ms step_avg:181.24ms\nstep:1777/5000 train_loss:8.1465 train_time:320255ms step_avg:181.24ms\nstep:1778/5000 train_loss:8.0891 train_time:320437ms step_avg:181.24ms\nstep:1779/5000 train_loss:8.1259 train_time:320619ms step_avg:181.24ms\nstep:1780/5000 train_loss:8.1109 train_time:320801ms step_avg:181.24ms\nstep:1781/5000 train_loss:8.1178 train_time:320982ms step_avg:181.24ms\nstep:1782/5000 train_loss:8.0878 train_time:321162ms step_avg:181.24ms\nstep:1783/5000 train_loss:8.1442 train_time:321344ms step_avg:181.24ms\nstep:1784/5000 train_loss:8.0690 train_time:321526ms step_avg:181.24ms\nstep:1785/5000 train_loss:8.1342 train_time:321708ms step_avg:181.24ms\nstep:1786/5000 train_loss:8.1466 train_time:321889ms step_avg:181.24ms\nstep:1787/5000 train_loss:8.1047 train_time:322069ms step_avg:181.24ms\nstep:1788/5000 train_loss:8.1069 train_time:322252ms step_avg:181.24ms\nstep:1789/5000 train_loss:8.2161 train_time:322433ms step_avg:181.24ms\nstep:1790/5000 train_loss:8.1068 train_time:322617ms step_avg:181.25ms\nstep:1791/5000 train_loss:8.1234 train_time:322800ms step_avg:181.25ms\nstep:1792/5000 train_loss:8.0882 train_time:322980ms step_avg:181.25ms\nstep:1793/5000 train_loss:8.1488 train_time:323161ms step_avg:181.25ms\nstep:1794/5000 train_loss:8.0765 train_time:323342ms step_avg:181.25ms\nstep:1795/5000 train_loss:8.0627 train_time:323523ms step_avg:181.25ms\nstep:1796/5000 train_loss:8.0018 train_time:323706ms step_avg:181.25ms\nstep:1797/5000 train_loss:8.1882 train_time:323886ms step_avg:181.25ms\nstep:1798/5000 train_loss:7.9909 train_time:324067ms step_avg:181.25ms\nstep:1799/5000 train_loss:8.1585 train_time:324248ms step_avg:181.25ms\nstep:1800/5000 train_loss:8.1036 train_time:324429ms step_avg:181.25ms\nstep:1801/5000 train_loss:8.2241 train_time:324613ms step_avg:181.25ms\nstep:1802/5000 train_loss:7.9317 train_time:324796ms step_avg:181.25ms\nstep:1803/5000 train_loss:8.1830 train_time:324978ms step_avg:181.25ms\nstep:1804/5000 train_loss:8.1547 train_time:325160ms step_avg:181.25ms\nstep:1805/5000 train_loss:8.0950 train_time:325341ms step_avg:181.25ms\nstep:1806/5000 train_loss:8.0522 train_time:325522ms step_avg:181.25ms\nstep:1807/5000 train_loss:8.0626 train_time:325705ms step_avg:181.25ms\nstep:1808/5000 train_loss:8.1530 train_time:325886ms step_avg:181.25ms\nstep:1809/5000 train_loss:8.0992 train_time:326066ms step_avg:181.25ms\nstep:1810/5000 train_loss:8.0755 train_time:326246ms step_avg:181.25ms\nstep:1811/5000 train_loss:8.1421 train_time:326426ms step_avg:181.25ms\nstep:1812/5000 train_loss:8.2119 train_time:326608ms step_avg:181.25ms\nstep:1813/5000 train_loss:8.0820 train_time:326789ms step_avg:181.25ms\nstep:1814/5000 train_loss:8.0453 train_time:326971ms step_avg:181.25ms\nstep:1815/5000 train_loss:8.0721 train_time:327154ms step_avg:181.25ms\nstep:1816/5000 train_loss:7.9952 train_time:327336ms step_avg:181.25ms\nstep:1817/5000 train_loss:8.1007 train_time:327518ms step_avg:181.25ms\nstep:1818/5000 train_loss:8.0571 train_time:327701ms step_avg:181.25ms\nstep:1819/5000 train_loss:7.9433 train_time:327881ms step_avg:181.25ms\nstep:1820/5000 train_loss:8.0644 train_time:328061ms step_avg:181.25ms\nstep:1821/5000 train_loss:8.1952 train_time:328243ms step_avg:181.25ms\nstep:1822/5000 train_loss:8.1027 train_time:328423ms step_avg:181.25ms\nstep:1823/5000 train_loss:8.0210 train_time:328606ms step_avg:181.25ms\nstep:1824/5000 train_loss:8.1538 train_time:328787ms step_avg:181.25ms\nstep:1825/5000 train_loss:8.1612 train_time:328967ms step_avg:181.25ms\nstep:1826/5000 train_loss:8.0711 train_time:329147ms step_avg:181.25ms\nstep:1827/5000 train_loss:8.1094 train_time:329327ms step_avg:181.25ms\nstep:1828/5000 train_loss:8.1578 train_time:329508ms step_avg:181.25ms\nstep:1829/5000 train_loss:8.1753 train_time:329690ms step_avg:181.25ms\nstep:1830/5000 train_loss:8.1394 train_time:329872ms step_avg:181.25ms\nstep:1831/5000 train_loss:7.9936 train_time:330056ms step_avg:181.25ms\nstep:1832/5000 train_loss:8.1485 train_time:330238ms step_avg:181.25ms\nstep:1833/5000 train_loss:8.0352 train_time:330421ms step_avg:181.25ms\nstep:1834/5000 train_loss:8.1601 train_time:330602ms step_avg:181.25ms\nstep:1835/5000 train_loss:8.0836 train_time:330783ms step_avg:181.25ms\nstep:1836/5000 train_loss:8.1491 train_time:330964ms step_avg:181.25ms\nstep:1837/5000 train_loss:8.0693 train_time:331146ms step_avg:181.25ms\nstep:1838/5000 train_loss:8.0121 train_time:331326ms step_avg:181.25ms\nstep:1839/5000 train_loss:8.0481 train_time:331508ms step_avg:181.25ms\nstep:1840/5000 train_loss:8.1787 train_time:331688ms step_avg:181.25ms\nstep:1841/5000 train_loss:8.1143 train_time:331869ms step_avg:181.25ms\nstep:1842/5000 train_loss:8.0555 train_time:332050ms step_avg:181.25ms\nstep:1843/5000 train_loss:8.1079 train_time:332232ms step_avg:181.25ms\nstep:1844/5000 train_loss:8.1819 train_time:332415ms step_avg:181.25ms\nstep:1845/5000 train_loss:8.1261 train_time:332596ms step_avg:181.25ms\nstep:1846/5000 train_loss:8.1334 train_time:332778ms step_avg:181.25ms\nstep:1847/5000 train_loss:8.0957 train_time:332960ms step_avg:181.25ms\nstep:1848/5000 train_loss:8.1067 train_time:333141ms step_avg:181.25ms\nstep:1849/5000 train_loss:8.0947 train_time:333321ms step_avg:181.25ms\nstep:1850/5000 train_loss:7.9409 train_time:333502ms step_avg:181.25ms\nstep:1851/5000 train_loss:8.2172 train_time:333683ms step_avg:181.25ms\nstep:1852/5000 train_loss:7.9637 train_time:333864ms step_avg:181.25ms\nstep:1853/5000 train_loss:8.0099 train_time:334046ms step_avg:181.25ms\nstep:1854/5000 train_loss:8.1616 train_time:334226ms step_avg:181.25ms\nstep:1855/5000 train_loss:8.2986 train_time:334406ms step_avg:181.25ms\nstep:1856/5000 train_loss:8.0753 train_time:334586ms step_avg:181.25ms\nstep:1857/5000 train_loss:8.1688 train_time:334767ms step_avg:181.25ms\nstep:1858/5000 train_loss:8.1123 train_time:334948ms step_avg:181.25ms\nstep:1859/5000 train_loss:8.0992 train_time:335129ms step_avg:181.25ms\nstep:1860/5000 train_loss:8.2073 train_time:335312ms step_avg:181.25ms\nstep:1861/5000 train_loss:8.1671 train_time:335494ms step_avg:181.25ms\nstep:1862/5000 train_loss:8.1034 train_time:335677ms step_avg:181.25ms\nstep:1863/5000 train_loss:8.0677 train_time:335859ms step_avg:181.25ms\nstep:1864/5000 train_loss:8.0093 train_time:336041ms step_avg:181.25ms\nstep:1865/5000 train_loss:8.1577 train_time:336224ms step_avg:181.25ms\nstep:1866/5000 train_loss:8.1826 train_time:336406ms step_avg:181.25ms\nstep:1867/5000 train_loss:8.1560 train_time:336586ms step_avg:181.25ms\nstep:1868/5000 train_loss:8.1237 train_time:336767ms step_avg:181.25ms\nstep:1869/5000 train_loss:8.1970 train_time:336948ms step_avg:181.25ms\nstep:1870/5000 train_loss:8.3069 train_time:337127ms step_avg:181.25ms\nstep:1871/5000 train_loss:8.1162 train_time:337309ms step_avg:181.25ms\nstep:1872/5000 train_loss:8.0773 train_time:337490ms step_avg:181.25ms\nstep:1873/5000 train_loss:8.1536 train_time:337672ms step_avg:181.25ms\nstep:1874/5000 train_loss:8.1262 train_time:337855ms step_avg:181.25ms\nstep:1875/5000 train_loss:8.1521 train_time:338037ms step_avg:181.25ms\nstep:1876/5000 train_loss:8.1676 train_time:338219ms step_avg:181.25ms\nstep:1877/5000 train_loss:8.1927 train_time:338401ms step_avg:181.25ms\nstep:1878/5000 train_loss:8.1301 train_time:338582ms step_avg:181.25ms\nstep:1879/5000 train_loss:8.1301 train_time:338764ms step_avg:181.25ms\nstep:1880/5000 train_loss:8.1386 train_time:338946ms step_avg:181.25ms\nstep:1881/5000 train_loss:8.0127 train_time:339127ms step_avg:181.25ms\nstep:1882/5000 train_loss:8.1570 train_time:339308ms step_avg:181.25ms\nstep:1883/5000 train_loss:8.1990 train_time:339489ms step_avg:181.25ms\nstep:1884/5000 train_loss:8.0320 train_time:339670ms step_avg:181.25ms\nstep:1885/5000 train_loss:7.9445 train_time:339852ms step_avg:181.25ms\nstep:1886/5000 train_loss:8.1240 train_time:340034ms step_avg:181.25ms\nstep:1887/5000 train_loss:8.4625 train_time:340217ms step_avg:181.26ms\nstep:1888/5000 train_loss:8.0990 train_time:340399ms step_avg:181.26ms\nstep:1889/5000 train_loss:8.0753 train_time:340581ms step_avg:181.26ms\nstep:1890/5000 train_loss:8.0831 train_time:340762ms step_avg:181.26ms\nstep:1891/5000 train_loss:8.1756 train_time:340944ms step_avg:181.26ms\nstep:1892/5000 train_loss:8.1328 train_time:341125ms step_avg:181.26ms\nstep:1893/5000 train_loss:8.1926 train_time:341306ms step_avg:181.26ms\nstep:1894/5000 train_loss:8.1051 train_time:341488ms step_avg:181.26ms\nstep:1895/5000 train_loss:8.0839 train_time:341668ms step_avg:181.26ms\nstep:1896/5000 train_loss:8.1961 train_time:341849ms step_avg:181.26ms\nstep:1897/5000 train_loss:8.1092 train_time:342029ms step_avg:181.26ms\nstep:1898/5000 train_loss:8.0553 train_time:342212ms step_avg:181.26ms\nstep:1899/5000 train_loss:8.2838 train_time:342394ms step_avg:181.26ms\nstep:1900/5000 train_loss:8.0729 train_time:342576ms step_avg:181.26ms\nstep:1901/5000 train_loss:8.1198 train_time:342759ms step_avg:181.26ms\nstep:1902/5000 train_loss:8.1716 train_time:342941ms step_avg:181.26ms\nstep:1903/5000 train_loss:8.0022 train_time:343122ms step_avg:181.26ms\nstep:1904/5000 train_loss:8.0112 train_time:343305ms step_avg:181.26ms\nstep:1905/5000 train_loss:8.1389 train_time:343485ms step_avg:181.26ms\nstep:1906/5000 train_loss:8.1333 train_time:343665ms step_avg:181.26ms\nstep:1907/5000 train_loss:8.2034 train_time:343845ms step_avg:181.26ms\nstep:1908/5000 train_loss:8.1743 train_time:344026ms step_avg:181.26ms\nstep:1909/5000 train_loss:7.9499 train_time:344208ms step_avg:181.26ms\nstep:1910/5000 train_loss:8.1357 train_time:344389ms step_avg:181.26ms\nstep:1911/5000 train_loss:8.2124 train_time:344570ms step_avg:181.26ms\nstep:1912/5000 train_loss:8.1765 train_time:344751ms step_avg:181.26ms\nstep:1913/5000 train_loss:7.9919 train_time:344932ms step_avg:181.26ms\nstep:1914/5000 train_loss:8.0478 train_time:345114ms step_avg:181.26ms\nstep:1915/5000 train_loss:8.0554 train_time:345297ms step_avg:181.26ms\nstep:1916/5000 train_loss:8.1388 train_time:345480ms step_avg:181.26ms\nstep:1917/5000 train_loss:8.0889 train_time:345661ms step_avg:181.26ms\nstep:1918/5000 train_loss:8.1325 train_time:345842ms step_avg:181.26ms\nstep:1919/5000 train_loss:8.1207 train_time:346024ms step_avg:181.26ms\nstep:1920/5000 train_loss:8.0420 train_time:346205ms step_avg:181.26ms\nstep:1921/5000 train_loss:8.2112 train_time:346384ms step_avg:181.26ms\nstep:1922/5000 train_loss:8.1764 train_time:346566ms step_avg:181.26ms\nstep:1923/5000 train_loss:8.0655 train_time:346746ms step_avg:181.26ms\nstep:1924/5000 train_loss:8.1970 train_time:346926ms step_avg:181.26ms\nstep:1925/5000 train_loss:8.0479 train_time:347108ms step_avg:181.26ms\nstep:1926/5000 train_loss:8.1106 train_time:347289ms step_avg:181.26ms\nstep:1927/5000 train_loss:8.0284 train_time:347470ms step_avg:181.26ms\nstep:1928/5000 train_loss:8.1868 train_time:347653ms step_avg:181.26ms\nstep:1929/5000 train_loss:8.0880 train_time:347835ms step_avg:181.26ms\nstep:1930/5000 train_loss:8.0010 train_time:348018ms step_avg:181.26ms\nstep:1931/5000 train_loss:8.0869 train_time:348201ms step_avg:181.26ms\nstep:1932/5000 train_loss:8.0295 train_time:348381ms step_avg:181.26ms\nstep:1933/5000 train_loss:7.9390 train_time:348562ms step_avg:181.26ms\nstep:1934/5000 train_loss:8.1267 train_time:348743ms step_avg:181.26ms\nstep:1935/5000 train_loss:8.0574 train_time:348924ms step_avg:181.26ms\nstep:1936/5000 train_loss:8.0955 train_time:349107ms step_avg:181.26ms\nstep:1937/5000 train_loss:8.0859 train_time:349288ms step_avg:181.26ms\nstep:1938/5000 train_loss:8.1236 train_time:349468ms step_avg:181.26ms\nstep:1939/5000 train_loss:8.1177 train_time:349648ms step_avg:181.26ms\nstep:1940/5000 train_loss:8.1101 train_time:349829ms step_avg:181.26ms\nstep:1941/5000 train_loss:8.1572 train_time:350011ms step_avg:181.26ms\nstep:1942/5000 train_loss:8.1225 train_time:350193ms step_avg:181.26ms\nstep:1943/5000 train_loss:8.1012 train_time:350374ms step_avg:181.26ms\nstep:1944/5000 train_loss:8.0370 train_time:350556ms step_avg:181.26ms\nstep:1945/5000 train_loss:8.0830 train_time:350738ms step_avg:181.26ms\nstep:1946/5000 train_loss:8.1294 train_time:350920ms step_avg:181.26ms\nstep:1947/5000 train_loss:8.1059 train_time:351102ms step_avg:181.26ms\nstep:1948/5000 train_loss:8.2560 train_time:351283ms step_avg:181.26ms\nstep:1949/5000 train_loss:8.0699 train_time:351464ms step_avg:181.26ms\nstep:1950/5000 train_loss:8.0149 train_time:351645ms step_avg:181.26ms\nstep:1951/5000 train_loss:8.1124 train_time:351826ms step_avg:181.26ms\nstep:1952/5000 train_loss:8.1463 train_time:352008ms step_avg:181.26ms\nstep:1953/5000 train_loss:8.0744 train_time:352189ms step_avg:181.26ms\nstep:1954/5000 train_loss:8.2279 train_time:352370ms step_avg:181.26ms\nstep:1955/5000 train_loss:8.1310 train_time:352551ms step_avg:181.26ms\nstep:1956/5000 train_loss:8.0286 train_time:352732ms step_avg:181.26ms\nstep:1957/5000 train_loss:8.1105 train_time:352915ms step_avg:181.26ms\nstep:1958/5000 train_loss:8.1016 train_time:353097ms step_avg:181.26ms\nstep:1959/5000 train_loss:8.0680 train_time:353279ms step_avg:181.26ms\nstep:1960/5000 train_loss:8.1578 train_time:353460ms step_avg:181.26ms\nstep:1961/5000 train_loss:8.0735 train_time:353641ms step_avg:181.26ms\nstep:1962/5000 train_loss:8.1163 train_time:353822ms step_avg:181.26ms\nstep:1963/5000 train_loss:8.0539 train_time:354005ms step_avg:181.26ms\nstep:1964/5000 train_loss:8.1162 train_time:354185ms step_avg:181.26ms\nstep:1965/5000 train_loss:7.9611 train_time:354366ms step_avg:181.26ms\nstep:1966/5000 train_loss:8.0563 train_time:354545ms step_avg:181.26ms\nstep:1967/5000 train_loss:8.1922 train_time:354725ms step_avg:181.26ms\nstep:1968/5000 train_loss:8.0854 train_time:354906ms step_avg:181.26ms\nstep:1969/5000 train_loss:8.0233 train_time:355087ms step_avg:181.26ms\nstep:1970/5000 train_loss:8.1462 train_time:355268ms step_avg:181.26ms\nstep:1971/5000 train_loss:8.0831 train_time:355451ms step_avg:181.26ms\nstep:1972/5000 train_loss:8.1621 train_time:355631ms step_avg:181.26ms\nstep:1973/5000 train_loss:8.3059 train_time:355813ms step_avg:181.26ms\nstep:1974/5000 train_loss:8.0227 train_time:355995ms step_avg:181.26ms\nstep:1975/5000 train_loss:8.0545 train_time:356177ms step_avg:181.26ms\nstep:1976/5000 train_loss:8.0825 train_time:356359ms step_avg:181.26ms\nstep:1977/5000 train_loss:8.0952 train_time:356541ms step_avg:181.26ms\nstep:1978/5000 train_loss:8.1422 train_time:356722ms step_avg:181.26ms\nstep:1979/5000 train_loss:8.0678 train_time:356904ms step_avg:181.26ms\nstep:1980/5000 train_loss:8.1209 train_time:357085ms step_avg:181.26ms\nstep:1981/5000 train_loss:8.0904 train_time:357265ms step_avg:181.26ms\nstep:1982/5000 train_loss:7.9919 train_time:357446ms step_avg:181.26ms\nstep:1983/5000 train_loss:8.1730 train_time:357625ms step_avg:181.26ms\nstep:1984/5000 train_loss:8.1477 train_time:357806ms step_avg:181.26ms\nstep:1985/5000 train_loss:8.0915 train_time:357986ms step_avg:181.26ms\nstep:1986/5000 train_loss:8.2826 train_time:358167ms step_avg:181.26ms\nstep:1987/5000 train_loss:8.3033 train_time:358349ms step_avg:181.26ms\nstep:1988/5000 train_loss:8.0945 train_time:358530ms step_avg:181.26ms\nstep:1989/5000 train_loss:8.1038 train_time:358712ms step_avg:181.26ms\nstep:1990/5000 train_loss:8.1513 train_time:358894ms step_avg:181.26ms\nstep:1991/5000 train_loss:8.0940 train_time:359076ms step_avg:181.26ms\nstep:1992/5000 train_loss:8.0075 train_time:359258ms step_avg:181.26ms\nstep:1993/5000 train_loss:8.4458 train_time:359440ms step_avg:181.26ms\nstep:1994/5000 train_loss:8.2183 train_time:359621ms step_avg:181.26ms\nstep:1995/5000 train_loss:8.2056 train_time:359802ms step_avg:181.26ms\nstep:1996/5000 train_loss:8.0817 train_time:359983ms step_avg:181.26ms\nstep:1997/5000 train_loss:8.0951 train_time:360165ms step_avg:181.26ms\nstep:1998/5000 train_loss:8.1700 train_time:360347ms step_avg:181.26ms\nstep:1999/5000 train_loss:8.0873 train_time:360528ms step_avg:181.26ms\nstep:2000/5000 train_loss:8.1780 train_time:360709ms step_avg:181.26ms\nstep:2001/5000 train_loss:8.2187 train_time:360891ms step_avg:181.26ms\nstep:2002/5000 train_loss:8.1895 train_time:361073ms step_avg:181.26ms\nstep:2003/5000 train_loss:8.1558 train_time:361255ms step_avg:181.26ms\nstep:2004/5000 train_loss:8.1472 train_time:361438ms step_avg:181.26ms\nstep:2005/5000 train_loss:8.1176 train_time:361621ms step_avg:181.26ms\nstep:2006/5000 train_loss:8.0140 train_time:361802ms step_avg:181.26ms\nstep:2007/5000 train_loss:8.2361 train_time:361983ms step_avg:181.26ms\nstep:2008/5000 train_loss:8.0099 train_time:362164ms step_avg:181.26ms\nstep:2009/5000 train_loss:8.1388 train_time:362344ms step_avg:181.26ms\nstep:2010/5000 train_loss:8.0960 train_time:362525ms step_avg:181.26ms\nstep:2011/5000 train_loss:8.0508 train_time:362707ms step_avg:181.26ms\nstep:2012/5000 train_loss:8.0312 train_time:362888ms step_avg:181.26ms\nstep:2013/5000 train_loss:7.9563 train_time:363069ms step_avg:181.26ms\nstep:2014/5000 train_loss:8.0631 train_time:363251ms step_avg:181.26ms\nstep:2015/5000 train_loss:8.0521 train_time:363432ms step_avg:181.26ms\nstep:2016/5000 train_loss:8.1294 train_time:363615ms step_avg:181.26ms\nstep:2017/5000 train_loss:8.0095 train_time:363797ms step_avg:181.26ms\nstep:2018/5000 train_loss:8.1639 train_time:363980ms step_avg:181.26ms\nstep:2019/5000 train_loss:8.1483 train_time:364161ms step_avg:181.26ms\nstep:2020/5000 train_loss:8.3180 train_time:364343ms step_avg:181.26ms\nstep:2021/5000 train_loss:7.9125 train_time:364524ms step_avg:181.27ms\nstep:2022/5000 train_loss:8.2754 train_time:364705ms step_avg:181.27ms\nstep:2023/5000 train_loss:7.9487 train_time:364885ms step_avg:181.26ms\nstep:2024/5000 train_loss:8.1544 train_time:365065ms step_avg:181.26ms\nstep:2025/5000 train_loss:8.1163 train_time:365246ms step_avg:181.26ms\nstep:2026/5000 train_loss:7.9876 train_time:365425ms step_avg:181.26ms\nstep:2027/5000 train_loss:8.0276 train_time:365606ms step_avg:181.26ms\nstep:2028/5000 train_loss:8.1328 train_time:365786ms step_avg:181.26ms\nstep:2029/5000 train_loss:7.9987 train_time:365967ms step_avg:181.26ms\nstep:2030/5000 train_loss:8.1462 train_time:366146ms step_avg:181.26ms\nstep:2031/5000 train_loss:8.1134 train_time:366327ms step_avg:181.26ms\nstep:2032/5000 train_loss:8.0773 train_time:366509ms step_avg:181.26ms\nstep:2033/5000 train_loss:8.1524 train_time:366689ms step_avg:181.26ms\nstep:2034/5000 train_loss:8.2326 train_time:366872ms step_avg:181.26ms\nstep:2035/5000 train_loss:8.1248 train_time:367055ms step_avg:181.26ms\nstep:2036/5000 train_loss:8.0504 train_time:367237ms step_avg:181.26ms\nstep:2037/5000 train_loss:8.2111 train_time:367418ms step_avg:181.26ms\nstep:2038/5000 train_loss:8.1419 train_time:367600ms step_avg:181.26ms\nstep:2039/5000 train_loss:8.1635 train_time:367781ms step_avg:181.26ms\nstep:2040/5000 train_loss:8.0921 train_time:367963ms step_avg:181.26ms\nstep:2041/5000 train_loss:8.1269 train_time:368145ms step_avg:181.26ms\nstep:2042/5000 train_loss:8.0791 train_time:368326ms step_avg:181.26ms\nstep:2043/5000 train_loss:8.0661 train_time:368506ms step_avg:181.26ms\nstep:2044/5000 train_loss:8.1276 train_time:368686ms step_avg:181.26ms\nstep:2045/5000 train_loss:8.1341 train_time:368866ms step_avg:181.26ms\nstep:2046/5000 train_loss:8.1860 train_time:369047ms step_avg:181.26ms\nstep:2047/5000 train_loss:8.0085 train_time:369228ms step_avg:181.26ms\nstep:2048/5000 train_loss:7.9542 train_time:369409ms step_avg:181.26ms\nstep:2049/5000 train_loss:8.0940 train_time:369590ms step_avg:181.26ms\nstep:2050/5000 train_loss:8.0119 train_time:369772ms step_avg:181.26ms\nstep:2051/5000 train_loss:8.1148 train_time:369955ms step_avg:181.26ms\nstep:2052/5000 train_loss:8.0631 train_time:370137ms step_avg:181.26ms\nstep:2053/5000 train_loss:8.1364 train_time:370320ms step_avg:181.26ms\nstep:2054/5000 train_loss:8.1847 train_time:370501ms step_avg:181.26ms\nstep:2055/5000 train_loss:8.1361 train_time:370683ms step_avg:181.26ms\nstep:2056/5000 train_loss:8.1759 train_time:370864ms step_avg:181.26ms\nstep:2057/5000 train_loss:8.0945 train_time:371045ms step_avg:181.26ms\nstep:2058/5000 train_loss:8.0873 train_time:371227ms step_avg:181.26ms\nstep:2059/5000 train_loss:7.9790 train_time:371410ms step_avg:181.26ms\nstep:2060/5000 train_loss:8.3471 train_time:371592ms step_avg:181.26ms\nstep:2061/5000 train_loss:7.9447 train_time:371773ms step_avg:181.26ms\nstep:2062/5000 train_loss:8.2744 train_time:371954ms step_avg:181.26ms\nstep:2063/5000 train_loss:8.1317 train_time:372136ms step_avg:181.26ms\nstep:2064/5000 train_loss:8.1142 train_time:372317ms step_avg:181.26ms\nstep:2065/5000 train_loss:8.0598 train_time:372501ms step_avg:181.27ms\nstep:2066/5000 train_loss:8.6006 train_time:372681ms step_avg:181.27ms\nstep:2067/5000 train_loss:8.0960 train_time:372861ms step_avg:181.26ms\nstep:2068/5000 train_loss:8.0327 train_time:373042ms step_avg:181.26ms\nstep:2069/5000 train_loss:8.1067 train_time:373223ms step_avg:181.26ms\nstep:2070/5000 train_loss:8.1680 train_time:373404ms step_avg:181.26ms\nstep:2071/5000 train_loss:8.1053 train_time:373585ms step_avg:181.26ms\nstep:2072/5000 train_loss:8.1269 train_time:373766ms step_avg:181.26ms\nstep:2073/5000 train_loss:8.0873 train_time:373946ms step_avg:181.26ms\nstep:2074/5000 train_loss:8.2180 train_time:374126ms step_avg:181.26ms\nstep:2075/5000 train_loss:8.0307 train_time:374308ms step_avg:181.26ms\nstep:2076/5000 train_loss:8.0570 train_time:374490ms step_avg:181.26ms\nstep:2077/5000 train_loss:8.0887 train_time:374672ms step_avg:181.26ms\nstep:2078/5000 train_loss:8.1418 train_time:374855ms step_avg:181.26ms\nstep:2079/5000 train_loss:8.0272 train_time:375037ms step_avg:181.26ms\nstep:2080/5000 train_loss:8.0783 train_time:375219ms step_avg:181.27ms\nstep:2081/5000 train_loss:8.1060 train_time:375401ms step_avg:181.27ms\nstep:2082/5000 train_loss:8.1999 train_time:375581ms step_avg:181.27ms\nstep:2083/5000 train_loss:8.0806 train_time:375764ms step_avg:181.27ms\nstep:2084/5000 train_loss:8.0731 train_time:375946ms step_avg:181.27ms\nstep:2085/5000 train_loss:8.0853 train_time:376126ms step_avg:181.27ms\nstep:2086/5000 train_loss:8.0193 train_time:376307ms step_avg:181.27ms\nstep:2087/5000 train_loss:8.1453 train_time:376489ms step_avg:181.27ms\nstep:2088/5000 train_loss:8.0367 train_time:376670ms step_avg:181.27ms\nstep:2089/5000 train_loss:8.1057 train_time:376853ms step_avg:181.27ms\nstep:2090/5000 train_loss:8.0802 train_time:377036ms step_avg:181.27ms\nstep:2091/5000 train_loss:8.1010 train_time:377220ms step_avg:181.27ms\nstep:2092/5000 train_loss:8.1355 train_time:377402ms step_avg:181.27ms\nstep:2093/5000 train_loss:8.1645 train_time:377583ms step_avg:181.27ms\nstep:2094/5000 train_loss:8.4288 train_time:377764ms step_avg:181.27ms\nstep:2095/5000 train_loss:8.0918 train_time:377946ms step_avg:181.27ms\nstep:2096/5000 train_loss:8.0279 train_time:378127ms step_avg:181.27ms\nstep:2097/5000 train_loss:8.1251 train_time:378309ms step_avg:181.27ms\nstep:2098/5000 train_loss:8.1725 train_time:378491ms step_avg:181.27ms\nstep:2099/5000 train_loss:8.0913 train_time:378672ms step_avg:181.27ms\nstep:2100/5000 train_loss:8.0787 train_time:378855ms step_avg:181.27ms\nstep:2101/5000 train_loss:8.1707 train_time:379037ms step_avg:181.27ms\nstep:2102/5000 train_loss:8.1668 train_time:379221ms step_avg:181.27ms\nstep:2103/5000 train_loss:8.0884 train_time:379403ms step_avg:181.27ms\nstep:2104/5000 train_loss:8.0981 train_time:379584ms step_avg:181.27ms\nstep:2105/5000 train_loss:8.0309 train_time:379765ms step_avg:181.27ms\nstep:2106/5000 train_loss:8.1317 train_time:379945ms step_avg:181.27ms\nstep:2107/5000 train_loss:8.2261 train_time:380123ms step_avg:181.27ms\nstep:2108/5000 train_loss:8.1607 train_time:380304ms step_avg:181.27ms\nstep:2109/5000 train_loss:8.0688 train_time:380485ms step_avg:181.27ms\nstep:2110/5000 train_loss:8.0416 train_time:380665ms step_avg:181.27ms\nstep:2111/5000 train_loss:8.1015 train_time:380848ms step_avg:181.27ms\nstep:2112/5000 train_loss:7.9791 train_time:381029ms step_avg:181.27ms\nstep:2113/5000 train_loss:8.1188 train_time:381211ms step_avg:181.27ms\nstep:2114/5000 train_loss:8.0800 train_time:381392ms step_avg:181.27ms\nstep:2115/5000 train_loss:8.2155 train_time:381573ms step_avg:181.27ms\nstep:2116/5000 train_loss:8.0407 train_time:381755ms step_avg:181.27ms\nstep:2117/5000 train_loss:8.0681 train_time:381938ms step_avg:181.27ms\nstep:2118/5000 train_loss:8.0839 train_time:382120ms step_avg:181.27ms\nstep:2119/5000 train_loss:8.1857 train_time:382302ms step_avg:181.27ms\nstep:2120/5000 train_loss:8.1420 train_time:382483ms step_avg:181.27ms\nstep:2121/5000 train_loss:8.1373 train_time:382663ms step_avg:181.27ms\nstep:2122/5000 train_loss:8.0674 train_time:382844ms step_avg:181.27ms\nstep:2123/5000 train_loss:8.0340 train_time:383025ms step_avg:181.27ms\nstep:2124/5000 train_loss:8.1863 train_time:383205ms step_avg:181.27ms\nstep:2125/5000 train_loss:8.1753 train_time:383386ms step_avg:181.27ms\nstep:2126/5000 train_loss:8.1950 train_time:383567ms step_avg:181.27ms\nstep:2127/5000 train_loss:8.1056 train_time:383746ms step_avg:181.27ms\nstep:2128/5000 train_loss:8.0716 train_time:383926ms step_avg:181.27ms\nstep:2129/5000 train_loss:8.0506 train_time:384108ms step_avg:181.27ms\nstep:2130/5000 train_loss:8.0904 train_time:384289ms step_avg:181.27ms\nstep:2131/5000 train_loss:8.1445 train_time:384471ms step_avg:181.27ms\nstep:2132/5000 train_loss:8.1359 train_time:384654ms step_avg:181.27ms\nstep:2133/5000 train_loss:7.6705 train_time:384835ms step_avg:181.27ms\nstep:2134/5000 train_loss:8.2399 train_time:385017ms step_avg:181.27ms\nstep:2135/5000 train_loss:8.0405 train_time:385201ms step_avg:181.27ms\nstep:2136/5000 train_loss:8.1566 train_time:385383ms step_avg:181.27ms\nstep:2137/5000 train_loss:8.2100 train_time:385563ms step_avg:181.27ms\nstep:2138/5000 train_loss:8.0668 train_time:385745ms step_avg:181.27ms\nstep:2139/5000 train_loss:8.1301 train_time:385926ms step_avg:181.27ms\nstep:2140/5000 train_loss:8.1548 train_time:386108ms step_avg:181.27ms\nstep:2141/5000 train_loss:8.1134 train_time:386290ms step_avg:181.27ms\nstep:2142/5000 train_loss:8.0821 train_time:386472ms step_avg:181.27ms\nstep:2143/5000 train_loss:7.8716 train_time:386653ms step_avg:181.27ms\nstep:2144/5000 train_loss:8.2448 train_time:386835ms step_avg:181.27ms\nstep:2145/5000 train_loss:8.2923 train_time:387019ms step_avg:181.27ms\nstep:2146/5000 train_loss:8.0829 train_time:387203ms step_avg:181.27ms\nstep:2147/5000 train_loss:8.0785 train_time:387385ms step_avg:181.28ms\nstep:2148/5000 train_loss:8.1869 train_time:387566ms step_avg:181.27ms\nstep:2149/5000 train_loss:8.0655 train_time:387747ms step_avg:181.27ms\nstep:2150/5000 train_loss:8.1067 train_time:387928ms step_avg:181.27ms\nstep:2151/5000 train_loss:8.4192 train_time:388109ms step_avg:181.27ms\nstep:2152/5000 train_loss:8.0068 train_time:388290ms step_avg:181.27ms\nstep:2153/5000 train_loss:8.0630 train_time:388472ms step_avg:181.27ms\nstep:2154/5000 train_loss:8.1355 train_time:388654ms step_avg:181.28ms\nstep:2155/5000 train_loss:8.1056 train_time:388835ms step_avg:181.28ms\nstep:2156/5000 train_loss:7.9677 train_time:389017ms step_avg:181.28ms\nstep:2157/5000 train_loss:8.1109 train_time:389200ms step_avg:181.28ms\nstep:2158/5000 train_loss:8.1978 train_time:389381ms step_avg:181.28ms\nstep:2159/5000 train_loss:8.0297 train_time:389562ms step_avg:181.28ms\nstep:2160/5000 train_loss:8.0147 train_time:389744ms step_avg:181.28ms\nstep:2161/5000 train_loss:8.1083 train_time:389926ms step_avg:181.28ms\nstep:2162/5000 train_loss:8.0952 train_time:390108ms step_avg:181.28ms\nstep:2163/5000 train_loss:8.1951 train_time:390290ms step_avg:181.28ms\nstep:2164/5000 train_loss:8.1484 train_time:390471ms step_avg:181.28ms\nstep:2165/5000 train_loss:7.8037 train_time:390653ms step_avg:181.28ms\nstep:2166/5000 train_loss:8.1181 train_time:390834ms step_avg:181.28ms\nstep:2167/5000 train_loss:8.4837 train_time:391017ms step_avg:181.28ms\nstep:2168/5000 train_loss:7.9977 train_time:391199ms step_avg:181.28ms\nstep:2169/5000 train_loss:8.1053 train_time:391381ms step_avg:181.28ms\nstep:2170/5000 train_loss:8.1093 train_time:391561ms step_avg:181.28ms\nstep:2171/5000 train_loss:8.2939 train_time:391743ms step_avg:181.28ms\nstep:2172/5000 train_loss:8.1292 train_time:391925ms step_avg:181.28ms\nstep:2173/5000 train_loss:8.1527 train_time:392106ms step_avg:181.28ms\nstep:2174/5000 train_loss:8.1339 train_time:392286ms step_avg:181.28ms\nstep:2175/5000 train_loss:8.1248 train_time:392466ms step_avg:181.28ms\nstep:2176/5000 train_loss:8.1617 train_time:392647ms step_avg:181.28ms\nstep:2177/5000 train_loss:8.1061 train_time:392827ms step_avg:181.28ms\nstep:2178/5000 train_loss:8.2410 train_time:393009ms step_avg:181.28ms\nstep:2179/5000 train_loss:8.0304 train_time:393191ms step_avg:181.28ms\nstep:2180/5000 train_loss:8.2109 train_time:393375ms step_avg:181.28ms\nstep:2181/5000 train_loss:8.3072 train_time:393557ms step_avg:181.28ms\nstep:2182/5000 train_loss:8.1424 train_time:393739ms step_avg:181.28ms\nstep:2183/5000 train_loss:8.0103 train_time:393921ms step_avg:181.28ms\nstep:2184/5000 train_loss:8.0831 train_time:394103ms step_avg:181.28ms\nstep:2185/5000 train_loss:8.1219 train_time:394284ms step_avg:181.28ms\nstep:2186/5000 train_loss:8.1387 train_time:394466ms step_avg:181.28ms\nstep:2187/5000 train_loss:8.0499 train_time:394647ms step_avg:181.28ms\nstep:2188/5000 train_loss:7.9787 train_time:394827ms step_avg:181.28ms\nstep:2189/5000 train_loss:8.0810 train_time:395009ms step_avg:181.28ms\nstep:2190/5000 train_loss:8.1674 train_time:395190ms step_avg:181.28ms\nstep:2191/5000 train_loss:8.0086 train_time:395371ms step_avg:181.28ms\nstep:2192/5000 train_loss:8.1517 train_time:395553ms step_avg:181.28ms\nstep:2193/5000 train_loss:8.3844 train_time:395735ms step_avg:181.28ms\nstep:2194/5000 train_loss:8.2087 train_time:395917ms step_avg:181.28ms\nstep:2195/5000 train_loss:8.1348 train_time:396099ms step_avg:181.28ms\nstep:2196/5000 train_loss:8.3591 train_time:396281ms step_avg:181.28ms\nstep:2197/5000 train_loss:8.0854 train_time:396461ms step_avg:181.28ms\nstep:2198/5000 train_loss:8.1178 train_time:396642ms step_avg:181.28ms\nstep:2199/5000 train_loss:8.0897 train_time:396823ms step_avg:181.28ms\nstep:2200/5000 train_loss:8.2294 train_time:397005ms step_avg:181.28ms\nstep:2201/5000 train_loss:8.0249 train_time:397186ms step_avg:181.28ms\nstep:2202/5000 train_loss:8.0470 train_time:397366ms step_avg:181.28ms\nstep:2203/5000 train_loss:8.1677 train_time:397546ms step_avg:181.28ms\nstep:2204/5000 train_loss:8.0751 train_time:397726ms step_avg:181.28ms\nstep:2205/5000 train_loss:8.1274 train_time:397909ms step_avg:181.28ms\nstep:2206/5000 train_loss:8.1101 train_time:398090ms step_avg:181.28ms\nstep:2207/5000 train_loss:8.1089 train_time:398271ms step_avg:181.28ms\nstep:2208/5000 train_loss:8.0898 train_time:398454ms step_avg:181.28ms\nstep:2209/5000 train_loss:8.0798 train_time:398636ms step_avg:181.28ms\nstep:2210/5000 train_loss:8.1492 train_time:398818ms step_avg:181.28ms\nstep:2211/5000 train_loss:8.0380 train_time:399000ms step_avg:181.28ms\nstep:2212/5000 train_loss:8.3027 train_time:399181ms step_avg:181.28ms\nstep:2213/5000 train_loss:8.2241 train_time:399363ms step_avg:181.28ms\nstep:2214/5000 train_loss:8.1223 train_time:399545ms step_avg:181.28ms\nstep:2215/5000 train_loss:8.1265 train_time:399726ms step_avg:181.28ms\nstep:2216/5000 train_loss:8.1834 train_time:399908ms step_avg:181.28ms\nstep:2217/5000 train_loss:8.1378 train_time:400089ms step_avg:181.28ms\nstep:2218/5000 train_loss:8.0990 train_time:400271ms step_avg:181.28ms\nstep:2219/5000 train_loss:8.1630 train_time:400452ms step_avg:181.28ms\nstep:2220/5000 train_loss:8.1675 train_time:400634ms step_avg:181.28ms\nstep:2221/5000 train_loss:8.1084 train_time:400817ms step_avg:181.28ms\nstep:2222/5000 train_loss:8.3575 train_time:401001ms step_avg:181.28ms\nstep:2223/5000 train_loss:8.2377 train_time:401182ms step_avg:181.28ms\nstep:2224/5000 train_loss:8.0827 train_time:401364ms step_avg:181.28ms\nstep:2225/5000 train_loss:8.0915 train_time:401545ms step_avg:181.28ms\nstep:2226/5000 train_loss:8.1621 train_time:401726ms step_avg:181.28ms\nstep:2227/5000 train_loss:8.1485 train_time:401908ms step_avg:181.28ms\nstep:2228/5000 train_loss:8.1460 train_time:402089ms step_avg:181.28ms\nstep:2229/5000 train_loss:8.0287 train_time:402270ms step_avg:181.28ms\nstep:2230/5000 train_loss:8.3513 train_time:402452ms step_avg:181.28ms\nstep:2231/5000 train_loss:8.1528 train_time:402633ms step_avg:181.28ms\nstep:2232/5000 train_loss:8.0855 train_time:402815ms step_avg:181.28ms\nstep:2233/5000 train_loss:8.1130 train_time:402997ms step_avg:181.29ms\nstep:2234/5000 train_loss:8.0213 train_time:403179ms step_avg:181.29ms\nstep:2235/5000 train_loss:8.1397 train_time:403362ms step_avg:181.29ms\nstep:2236/5000 train_loss:8.1040 train_time:403543ms step_avg:181.29ms\nstep:2237/5000 train_loss:8.2323 train_time:403723ms step_avg:181.29ms\nstep:2238/5000 train_loss:8.1669 train_time:403905ms step_avg:181.29ms\nstep:2239/5000 train_loss:8.2154 train_time:404086ms step_avg:181.29ms\nstep:2240/5000 train_loss:8.0505 train_time:404266ms step_avg:181.29ms\nstep:2241/5000 train_loss:8.2158 train_time:404446ms step_avg:181.28ms\nstep:2242/5000 train_loss:8.2279 train_time:404627ms step_avg:181.28ms\nstep:2243/5000 train_loss:8.1757 train_time:404809ms step_avg:181.28ms\nstep:2244/5000 train_loss:8.1043 train_time:404991ms step_avg:181.29ms\nstep:2245/5000 train_loss:8.0334 train_time:405175ms step_avg:181.29ms\nstep:2246/5000 train_loss:8.0793 train_time:405356ms step_avg:181.29ms\nstep:2247/5000 train_loss:8.1171 train_time:405539ms step_avg:181.29ms\nstep:2248/5000 train_loss:8.0724 train_time:405721ms step_avg:181.29ms\nstep:2249/5000 train_loss:8.0683 train_time:405903ms step_avg:181.29ms\nstep:2250/5000 train_loss:8.0293 train_time:406084ms step_avg:181.29ms\nstep:2251/5000 train_loss:8.0966 train_time:406265ms step_avg:181.29ms\nstep:2252/5000 train_loss:8.1075 train_time:406445ms step_avg:181.29ms\nstep:2253/5000 train_loss:8.1452 train_time:406625ms step_avg:181.29ms\nstep:2254/5000 train_loss:8.1119 train_time:406806ms step_avg:181.29ms\nstep:2255/5000 train_loss:8.0522 train_time:406988ms step_avg:181.29ms\nstep:2256/5000 train_loss:8.1463 train_time:407170ms step_avg:181.29ms\nstep:2257/5000 train_loss:8.0648 train_time:407353ms step_avg:181.29ms\nstep:2258/5000 train_loss:8.1802 train_time:407534ms step_avg:181.29ms\nstep:2259/5000 train_loss:8.1570 train_time:407717ms step_avg:181.29ms\nstep:2260/5000 train_loss:8.1188 train_time:407900ms step_avg:181.29ms\nstep:2261/5000 train_loss:8.0400 train_time:408081ms step_avg:181.29ms\nstep:2262/5000 train_loss:8.1811 train_time:408262ms step_avg:181.29ms\nstep:2263/5000 train_loss:8.0041 train_time:408442ms step_avg:181.29ms\nstep:2264/5000 train_loss:8.1041 train_time:408624ms step_avg:181.29ms\nstep:2265/5000 train_loss:8.1595 train_time:408806ms step_avg:181.29ms\nstep:2266/5000 train_loss:8.0787 train_time:408987ms step_avg:181.29ms\nstep:2267/5000 train_loss:8.0957 train_time:409170ms step_avg:181.29ms\nstep:2268/5000 train_loss:8.1518 train_time:409351ms step_avg:181.29ms\nstep:2269/5000 train_loss:8.0832 train_time:409533ms step_avg:181.29ms\nstep:2270/5000 train_loss:8.0698 train_time:409715ms step_avg:181.29ms\nstep:2271/5000 train_loss:8.1004 train_time:409898ms step_avg:181.29ms\nstep:2272/5000 train_loss:8.3052 train_time:410079ms step_avg:181.29ms\nstep:2273/5000 train_loss:8.0621 train_time:410261ms step_avg:181.29ms\nstep:2274/5000 train_loss:8.1018 train_time:410444ms step_avg:181.29ms\nstep:2275/5000 train_loss:8.0731 train_time:410625ms step_avg:181.29ms\nstep:2276/5000 train_loss:8.0454 train_time:410806ms step_avg:181.29ms\nstep:2277/5000 train_loss:8.1413 train_time:410987ms step_avg:181.29ms\nstep:2278/5000 train_loss:8.1089 train_time:411168ms step_avg:181.29ms\nstep:2279/5000 train_loss:8.1269 train_time:411350ms step_avg:181.29ms\nstep:2280/5000 train_loss:8.0444 train_time:411531ms step_avg:181.29ms\nstep:2281/5000 train_loss:8.0876 train_time:411714ms step_avg:181.29ms\nstep:2282/5000 train_loss:8.1200 train_time:411897ms step_avg:181.29ms\nstep:2283/5000 train_loss:8.0607 train_time:412079ms step_avg:181.29ms\nstep:2284/5000 train_loss:8.0644 train_time:412261ms step_avg:181.29ms\nstep:2285/5000 train_loss:8.0537 train_time:412442ms step_avg:181.29ms\nstep:2286/5000 train_loss:8.1475 train_time:412857ms step_avg:181.40ms\nstep:2287/5000 train_loss:8.0485 train_time:413037ms step_avg:181.40ms\nstep:2288/5000 train_loss:8.1999 train_time:413218ms step_avg:181.40ms\nstep:2289/5000 train_loss:8.0910 train_time:413399ms step_avg:181.39ms\nstep:2290/5000 train_loss:7.9949 train_time:413579ms step_avg:181.39ms\nstep:2291/5000 train_loss:8.1525 train_time:413768ms step_avg:181.40ms\nstep:2292/5000 train_loss:8.0433 train_time:413949ms step_avg:181.40ms\nstep:2293/5000 train_loss:8.3638 train_time:414129ms step_avg:181.40ms\nstep:2294/5000 train_loss:8.1225 train_time:414310ms step_avg:181.40ms\nstep:2295/5000 train_loss:8.0686 train_time:414492ms step_avg:181.40ms\nstep:2296/5000 train_loss:8.0279 train_time:414675ms step_avg:181.40ms\nstep:2297/5000 train_loss:8.1025 train_time:414857ms step_avg:181.40ms\nstep:2298/5000 train_loss:8.0453 train_time:415040ms step_avg:181.40ms\nstep:2299/5000 train_loss:8.1443 train_time:415222ms step_avg:181.40ms\nstep:2300/5000 train_loss:8.1190 train_time:415403ms step_avg:181.40ms\nstep:2301/5000 train_loss:8.1005 train_time:415583ms step_avg:181.40ms\nstep:2302/5000 train_loss:8.1472 train_time:415764ms step_avg:181.40ms\nstep:2303/5000 train_loss:8.1230 train_time:415945ms step_avg:181.40ms\nstep:2304/5000 train_loss:8.1809 train_time:416126ms step_avg:181.40ms\nstep:2305/5000 train_loss:8.0719 train_time:416308ms step_avg:181.40ms\nstep:2306/5000 train_loss:8.1317 train_time:416490ms step_avg:181.40ms\nstep:2307/5000 train_loss:8.1994 train_time:416670ms step_avg:181.40ms\nstep:2308/5000 train_loss:8.1374 train_time:416852ms step_avg:181.40ms\nstep:2309/5000 train_loss:8.2788 train_time:417033ms step_avg:181.40ms\nstep:2310/5000 train_loss:8.1977 train_time:417216ms step_avg:181.40ms\nstep:2311/5000 train_loss:8.1040 train_time:417397ms step_avg:181.40ms\nstep:2312/5000 train_loss:8.1600 train_time:417579ms step_avg:181.40ms\nstep:2313/5000 train_loss:8.1656 train_time:417762ms step_avg:181.40ms\nstep:2314/5000 train_loss:8.0889 train_time:417942ms step_avg:181.40ms\nstep:2315/5000 train_loss:8.0308 train_time:418124ms step_avg:181.40ms\nstep:2316/5000 train_loss:8.0984 train_time:418306ms step_avg:181.40ms\nstep:2317/5000 train_loss:8.1694 train_time:418486ms step_avg:181.40ms\nstep:2318/5000 train_loss:7.9952 train_time:418668ms step_avg:181.40ms\nstep:2319/5000 train_loss:8.2698 train_time:418848ms step_avg:181.40ms\nstep:2320/5000 train_loss:8.0314 train_time:419029ms step_avg:181.40ms\nstep:2321/5000 train_loss:8.2385 train_time:419211ms step_avg:181.40ms\nstep:2322/5000 train_loss:8.1728 train_time:419394ms step_avg:181.40ms\nstep:2323/5000 train_loss:8.0153 train_time:419575ms step_avg:181.40ms\nstep:2324/5000 train_loss:8.1025 train_time:419757ms step_avg:181.40ms\nstep:2325/5000 train_loss:8.0348 train_time:419939ms step_avg:181.40ms\nstep:2326/5000 train_loss:8.1445 train_time:420121ms step_avg:181.40ms\nstep:2327/5000 train_loss:8.0924 train_time:420303ms step_avg:181.40ms\nstep:2328/5000 train_loss:8.0987 train_time:420484ms step_avg:181.40ms\nstep:2329/5000 train_loss:8.0858 train_time:420665ms step_avg:181.40ms\nstep:2330/5000 train_loss:8.1314 train_time:420845ms step_avg:181.40ms\nstep:2331/5000 train_loss:8.0827 train_time:421024ms step_avg:181.40ms\nstep:2332/5000 train_loss:8.2374 train_time:421204ms step_avg:181.40ms\nstep:2333/5000 train_loss:8.0613 train_time:421385ms step_avg:181.40ms\nstep:2334/5000 train_loss:8.2693 train_time:421565ms step_avg:181.40ms\nstep:2335/5000 train_loss:7.9669 train_time:421747ms step_avg:181.40ms\nstep:2336/5000 train_loss:8.0621 train_time:421929ms step_avg:181.40ms\nstep:2337/5000 train_loss:8.1274 train_time:422110ms step_avg:181.40ms\nstep:2338/5000 train_loss:8.0005 train_time:422290ms step_avg:181.40ms\nstep:2339/5000 train_loss:8.1108 train_time:422472ms step_avg:181.40ms\nstep:2340/5000 train_loss:8.0545 train_time:422654ms step_avg:181.40ms\nstep:2341/5000 train_loss:8.0278 train_time:422836ms step_avg:181.40ms\nstep:2342/5000 train_loss:8.1437 train_time:423019ms step_avg:181.40ms\nstep:2343/5000 train_loss:8.1572 train_time:423200ms step_avg:181.40ms\nstep:2344/5000 train_loss:8.1072 train_time:423381ms step_avg:181.40ms\nstep:2345/5000 train_loss:8.1977 train_time:423563ms step_avg:181.40ms\nstep:2346/5000 train_loss:8.1081 train_time:423745ms step_avg:181.40ms\nstep:2347/5000 train_loss:8.0594 train_time:423926ms step_avg:181.40ms\nstep:2348/5000 train_loss:8.0604 train_time:424109ms step_avg:181.40ms\nstep:2349/5000 train_loss:8.0905 train_time:424290ms step_avg:181.40ms\nstep:2350/5000 train_loss:8.1570 train_time:424471ms step_avg:181.40ms\nstep:2351/5000 train_loss:7.9987 train_time:424651ms step_avg:181.40ms\nstep:2352/5000 train_loss:8.1561 train_time:424833ms step_avg:181.40ms\nstep:2353/5000 train_loss:8.1018 train_time:425016ms step_avg:181.40ms\nstep:2354/5000 train_loss:8.0005 train_time:425198ms step_avg:181.40ms\nstep:2355/5000 train_loss:8.1314 train_time:425380ms step_avg:181.40ms\nstep:2356/5000 train_loss:8.1751 train_time:425561ms step_avg:181.40ms\nstep:2357/5000 train_loss:8.0196 train_time:425742ms step_avg:181.40ms\nstep:2358/5000 train_loss:8.2064 train_time:425923ms step_avg:181.40ms\nstep:2359/5000 train_loss:8.1233 train_time:426105ms step_avg:181.40ms\nstep:2360/5000 train_loss:8.2348 train_time:426287ms step_avg:181.40ms\nstep:2361/5000 train_loss:8.1314 train_time:426467ms step_avg:181.40ms\nstep:2362/5000 train_loss:8.1632 train_time:426647ms step_avg:181.40ms\nstep:2363/5000 train_loss:8.1753 train_time:426827ms step_avg:181.40ms\nstep:2364/5000 train_loss:8.1450 train_time:427009ms step_avg:181.40ms\nstep:2365/5000 train_loss:8.1377 train_time:427192ms step_avg:181.40ms\nstep:2366/5000 train_loss:8.0352 train_time:427374ms step_avg:181.40ms\nstep:2367/5000 train_loss:8.0925 train_time:427555ms step_avg:181.40ms\nstep:2368/5000 train_loss:8.0150 train_time:427737ms step_avg:181.40ms\nstep:2369/5000 train_loss:8.1854 train_time:427921ms step_avg:181.40ms\nstep:2370/5000 train_loss:7.9700 train_time:428102ms step_avg:181.40ms\nstep:2371/5000 train_loss:8.1517 train_time:428285ms step_avg:181.40ms\nstep:2372/5000 train_loss:8.1181 train_time:428467ms step_avg:181.40ms\nstep:2373/5000 train_loss:8.1285 train_time:428648ms step_avg:181.40ms\nstep:2374/5000 train_loss:8.1526 train_time:428829ms step_avg:181.40ms\nstep:2375/5000 train_loss:8.1503 train_time:429011ms step_avg:181.40ms\nstep:2376/5000 train_loss:8.1393 train_time:429192ms step_avg:181.40ms\nstep:2377/5000 train_loss:8.1257 train_time:429374ms step_avg:181.40ms\nstep:2378/5000 train_loss:8.1040 train_time:429555ms step_avg:181.40ms\nstep:2379/5000 train_loss:8.1411 train_time:429737ms step_avg:181.40ms\nstep:2380/5000 train_loss:8.2438 train_time:429919ms step_avg:181.40ms\nstep:2381/5000 train_loss:8.0501 train_time:430102ms step_avg:181.40ms\nstep:2382/5000 train_loss:8.0611 train_time:430284ms step_avg:181.40ms\nstep:2383/5000 train_loss:8.2421 train_time:430467ms step_avg:181.40ms\nstep:2384/5000 train_loss:7.9358 train_time:430648ms step_avg:181.40ms\nstep:2385/5000 train_loss:8.1651 train_time:430829ms step_avg:181.40ms\nstep:2386/5000 train_loss:8.1218 train_time:431011ms step_avg:181.40ms\nstep:2387/5000 train_loss:8.0942 train_time:431192ms step_avg:181.40ms\nstep:2388/5000 train_loss:8.0893 train_time:431374ms step_avg:181.40ms\nstep:2389/5000 train_loss:8.1703 train_time:431557ms step_avg:181.40ms\nstep:2390/5000 train_loss:8.0594 train_time:431739ms step_avg:181.40ms\nstep:2391/5000 train_loss:8.0428 train_time:431922ms step_avg:181.40ms\nstep:2392/5000 train_loss:8.1120 train_time:432103ms step_avg:181.40ms\nstep:2393/5000 train_loss:8.1213 train_time:432284ms step_avg:181.40ms\nstep:2394/5000 train_loss:8.1962 train_time:432466ms step_avg:181.40ms\nstep:2395/5000 train_loss:8.1397 train_time:432648ms step_avg:181.40ms\nstep:2396/5000 train_loss:8.1795 train_time:432829ms step_avg:181.40ms\nstep:2397/5000 train_loss:8.1406 train_time:433013ms step_avg:181.40ms\nstep:2398/5000 train_loss:8.1386 train_time:433194ms step_avg:181.40ms\nstep:2399/5000 train_loss:8.0683 train_time:433375ms step_avg:181.40ms\nstep:2400/5000 train_loss:8.0576 train_time:433557ms step_avg:181.40ms\nstep:2401/5000 train_loss:8.1746 train_time:433739ms step_avg:181.40ms\nstep:2402/5000 train_loss:8.1376 train_time:433921ms step_avg:181.41ms\nstep:2403/5000 train_loss:8.1684 train_time:434102ms step_avg:181.41ms\nstep:2404/5000 train_loss:8.0410 train_time:434283ms step_avg:181.40ms\nstep:2405/5000 train_loss:8.1531 train_time:434464ms step_avg:181.40ms\nstep:2406/5000 train_loss:7.9823 train_time:434647ms step_avg:181.41ms\nstep:2407/5000 train_loss:8.0352 train_time:434828ms step_avg:181.40ms\nstep:2408/5000 train_loss:8.0168 train_time:435010ms step_avg:181.41ms\nstep:2409/5000 train_loss:8.0480 train_time:435192ms step_avg:181.41ms\nstep:2410/5000 train_loss:8.0735 train_time:435374ms step_avg:181.41ms\nstep:2411/5000 train_loss:8.0996 train_time:435555ms step_avg:181.41ms\nstep:2412/5000 train_loss:8.0377 train_time:435738ms step_avg:181.41ms\nstep:2413/5000 train_loss:8.0148 train_time:435920ms step_avg:181.41ms\nstep:2414/5000 train_loss:8.0626 train_time:436102ms step_avg:181.41ms\nstep:2415/5000 train_loss:8.2192 train_time:436283ms step_avg:181.41ms\nstep:2416/5000 train_loss:8.1223 train_time:436464ms step_avg:181.41ms\nstep:2417/5000 train_loss:8.0122 train_time:436645ms step_avg:181.41ms\nstep:2418/5000 train_loss:8.1370 train_time:436825ms step_avg:181.41ms\nstep:2419/5000 train_loss:8.1959 train_time:437008ms step_avg:181.41ms\nstep:2420/5000 train_loss:8.2137 train_time:437189ms step_avg:181.41ms\nstep:2421/5000 train_loss:8.1184 train_time:437371ms step_avg:181.41ms\nstep:2422/5000 train_loss:8.1679 train_time:437553ms step_avg:181.41ms\nstep:2423/5000 train_loss:8.1220 train_time:437734ms step_avg:181.41ms\nstep:2424/5000 train_loss:8.1233 train_time:437917ms step_avg:181.41ms\nstep:2425/5000 train_loss:8.1347 train_time:438100ms step_avg:181.41ms\nstep:2426/5000 train_loss:8.1600 train_time:438282ms step_avg:181.41ms\nstep:2427/5000 train_loss:8.0105 train_time:438463ms step_avg:181.41ms\nstep:2428/5000 train_loss:8.0544 train_time:438645ms step_avg:181.41ms\nstep:2429/5000 train_loss:8.0416 train_time:438826ms step_avg:181.41ms\nstep:2430/5000 train_loss:8.1273 train_time:439009ms step_avg:181.41ms\nstep:2431/5000 train_loss:8.0953 train_time:439192ms step_avg:181.41ms\nstep:2432/5000 train_loss:8.0857 train_time:439375ms step_avg:181.41ms\nstep:2433/5000 train_loss:8.0094 train_time:439557ms step_avg:181.41ms\nstep:2434/5000 train_loss:8.1125 train_time:439739ms step_avg:181.41ms\nstep:2435/5000 train_loss:7.9522 train_time:439921ms step_avg:181.41ms\nstep:2436/5000 train_loss:8.0870 train_time:440103ms step_avg:181.41ms\nstep:2437/5000 train_loss:8.0478 train_time:440284ms step_avg:181.41ms\nstep:2438/5000 train_loss:8.0800 train_time:440466ms step_avg:181.41ms\nstep:2439/5000 train_loss:8.0721 train_time:440646ms step_avg:181.41ms\nstep:2440/5000 train_loss:8.0941 train_time:440827ms step_avg:181.41ms\nstep:2441/5000 train_loss:8.0536 train_time:441009ms step_avg:181.41ms\nstep:2442/5000 train_loss:8.0781 train_time:441190ms step_avg:181.41ms\nstep:2443/5000 train_loss:8.0137 train_time:441371ms step_avg:181.41ms\nstep:2444/5000 train_loss:8.1272 train_time:441554ms step_avg:181.41ms\nstep:2445/5000 train_loss:8.0369 train_time:441736ms step_avg:181.41ms\nstep:2446/5000 train_loss:8.1051 train_time:441919ms step_avg:181.41ms\nstep:2447/5000 train_loss:8.0927 train_time:442101ms step_avg:181.41ms\nstep:2448/5000 train_loss:8.1062 train_time:442282ms step_avg:181.41ms\nstep:2449/5000 train_loss:8.2075 train_time:442463ms step_avg:181.41ms\nstep:2450/5000 train_loss:8.1866 train_time:442644ms step_avg:181.41ms\nstep:2451/5000 train_loss:8.1114 train_time:442826ms step_avg:181.41ms\nstep:2452/5000 train_loss:8.0890 train_time:443008ms step_avg:181.41ms\nstep:2453/5000 train_loss:7.9813 train_time:443188ms step_avg:181.41ms\nstep:2454/5000 train_loss:7.9569 train_time:443369ms step_avg:181.41ms\nstep:2455/5000 train_loss:8.0809 train_time:443550ms step_avg:181.41ms\nstep:2456/5000 train_loss:8.1342 train_time:443731ms step_avg:181.41ms\nstep:2457/5000 train_loss:8.1123 train_time:443915ms step_avg:181.41ms\nstep:2458/5000 train_loss:8.2124 train_time:444097ms step_avg:181.41ms\nstep:2459/5000 train_loss:8.1884 train_time:444279ms step_avg:181.41ms\nstep:2460/5000 train_loss:8.2812 train_time:444460ms step_avg:181.41ms\nstep:2461/5000 train_loss:8.1629 train_time:444641ms step_avg:181.41ms\nstep:2462/5000 train_loss:8.1263 train_time:444824ms step_avg:181.41ms\nstep:2463/5000 train_loss:8.2142 train_time:445006ms step_avg:181.41ms\nstep:2464/5000 train_loss:8.1642 train_time:445187ms step_avg:181.41ms\nstep:2465/5000 train_loss:8.1846 train_time:445368ms step_avg:181.41ms\nstep:2466/5000 train_loss:8.1304 train_time:445548ms step_avg:181.41ms\nstep:2467/5000 train_loss:8.1462 train_time:445728ms step_avg:181.41ms\nstep:2468/5000 train_loss:7.9863 train_time:445911ms step_avg:181.41ms\nstep:2469/5000 train_loss:8.1652 train_time:446094ms step_avg:181.41ms\nstep:2470/5000 train_loss:8.1334 train_time:446277ms step_avg:181.41ms\nstep:2471/5000 train_loss:8.0597 train_time:446459ms step_avg:181.41ms\nstep:2472/5000 train_loss:8.2290 train_time:446641ms step_avg:181.41ms\nstep:2473/5000 train_loss:8.0446 train_time:446822ms step_avg:181.41ms\nstep:2474/5000 train_loss:8.0589 train_time:447003ms step_avg:181.41ms\nstep:2475/5000 train_loss:8.0880 train_time:447183ms step_avg:181.41ms\nstep:2476/5000 train_loss:8.0964 train_time:447366ms step_avg:181.41ms\nstep:2477/5000 train_loss:8.2585 train_time:447548ms step_avg:181.41ms\nstep:2478/5000 train_loss:8.1926 train_time:447728ms step_avg:181.41ms\nstep:2479/5000 train_loss:8.1136 train_time:447911ms step_avg:181.41ms\nstep:2480/5000 train_loss:8.2281 train_time:448092ms step_avg:181.41ms\nstep:2481/5000 train_loss:8.0635 train_time:448274ms step_avg:181.41ms\nstep:2482/5000 train_loss:8.1585 train_time:448457ms step_avg:181.41ms\nstep:2483/5000 train_loss:7.8883 train_time:448640ms step_avg:181.42ms\nstep:2484/5000 train_loss:8.1750 train_time:448821ms step_avg:181.41ms\nstep:2485/5000 train_loss:8.2287 train_time:449003ms step_avg:181.42ms\nstep:2486/5000 train_loss:8.1035 train_time:449184ms step_avg:181.42ms\nstep:2487/5000 train_loss:8.1103 train_time:449365ms step_avg:181.42ms\nstep:2488/5000 train_loss:8.1369 train_time:449547ms step_avg:181.42ms\nstep:2489/5000 train_loss:8.3048 train_time:449728ms step_avg:181.42ms\nstep:2490/5000 train_loss:8.0764 train_time:449909ms step_avg:181.42ms\nstep:2491/5000 train_loss:8.1036 train_time:450090ms step_avg:181.41ms\nstep:2492/5000 train_loss:8.1225 train_time:450274ms step_avg:181.42ms\nstep:2493/5000 train_loss:8.1012 train_time:450455ms step_avg:181.42ms\nstep:2494/5000 train_loss:8.1974 train_time:450637ms step_avg:181.42ms\nstep:2495/5000 train_loss:8.1062 train_time:450820ms step_avg:181.42ms\nstep:2496/5000 train_loss:8.2170 train_time:451001ms step_avg:181.42ms\nstep:2497/5000 train_loss:8.1000 train_time:451183ms step_avg:181.42ms\nstep:2498/5000 train_loss:8.1104 train_time:451363ms step_avg:181.42ms\nstep:2499/5000 train_loss:8.0523 train_time:451545ms step_avg:181.42ms\nstep:2500/5000 train_loss:8.1516 train_time:451727ms step_avg:181.42ms\nstep:2501/5000 train_loss:8.1677 train_time:451908ms step_avg:181.42ms\nstep:2502/5000 train_loss:8.0875 train_time:452089ms step_avg:181.42ms\nstep:2503/5000 train_loss:8.0861 train_time:452271ms step_avg:181.42ms\nstep:2504/5000 train_loss:8.3133 train_time:452453ms step_avg:181.42ms\nstep:2505/5000 train_loss:8.1365 train_time:452635ms step_avg:181.42ms\nstep:2506/5000 train_loss:8.0804 train_time:452818ms step_avg:181.42ms\nstep:2507/5000 train_loss:8.1737 train_time:453000ms step_avg:181.42ms\nstep:2508/5000 train_loss:8.2010 train_time:453181ms step_avg:181.42ms\nstep:2509/5000 train_loss:8.0553 train_time:453362ms step_avg:181.42ms\nstep:2510/5000 train_loss:8.0505 train_time:453544ms step_avg:181.42ms\nstep:2511/5000 train_loss:8.0169 train_time:453725ms step_avg:181.42ms\nstep:2512/5000 train_loss:8.0404 train_time:453907ms step_avg:181.42ms\nstep:2513/5000 train_loss:8.1787 train_time:454088ms step_avg:181.42ms\nstep:2514/5000 train_loss:8.1384 train_time:454269ms step_avg:181.42ms\nstep:2515/5000 train_loss:8.1166 train_time:454450ms step_avg:181.42ms\nstep:2516/5000 train_loss:8.2130 train_time:454631ms step_avg:181.42ms\nstep:2517/5000 train_loss:8.0578 train_time:454814ms step_avg:181.42ms\nstep:2518/5000 train_loss:8.1381 train_time:454997ms step_avg:181.42ms\nstep:2519/5000 train_loss:8.1155 train_time:455179ms step_avg:181.42ms\nstep:2520/5000 train_loss:8.0269 train_time:455361ms step_avg:181.42ms\nstep:2521/5000 train_loss:8.1039 train_time:455543ms step_avg:181.42ms\nstep:2522/5000 train_loss:8.1644 train_time:455725ms step_avg:181.42ms\nstep:2523/5000 train_loss:8.1671 train_time:455907ms step_avg:181.42ms\nstep:2524/5000 train_loss:8.0789 train_time:456089ms step_avg:181.42ms\nstep:2525/5000 train_loss:8.1148 train_time:456270ms step_avg:181.42ms\nstep:2526/5000 train_loss:8.1254 train_time:456451ms step_avg:181.42ms\nstep:2527/5000 train_loss:8.0618 train_time:456633ms step_avg:181.42ms\nstep:2528/5000 train_loss:8.1885 train_time:456817ms step_avg:181.42ms\nstep:2529/5000 train_loss:8.1148 train_time:457000ms step_avg:181.42ms\nstep:2530/5000 train_loss:8.1181 train_time:457181ms step_avg:181.42ms\nstep:2531/5000 train_loss:8.1238 train_time:457362ms step_avg:181.42ms\nstep:2532/5000 train_loss:8.1567 train_time:457543ms step_avg:181.42ms\nstep:2533/5000 train_loss:8.0841 train_time:457725ms step_avg:181.42ms\nstep:2534/5000 train_loss:8.2750 train_time:457907ms step_avg:181.42ms\nstep:2535/5000 train_loss:8.1472 train_time:458088ms step_avg:181.42ms\nstep:2536/5000 train_loss:8.0186 train_time:458269ms step_avg:181.42ms\nstep:2537/5000 train_loss:8.0994 train_time:458451ms step_avg:181.42ms\nstep:2538/5000 train_loss:8.1323 train_time:458633ms step_avg:181.42ms\nstep:2539/5000 train_loss:8.2043 train_time:458816ms step_avg:181.42ms\nstep:2540/5000 train_loss:7.9975 train_time:458999ms step_avg:181.42ms\nstep:2541/5000 train_loss:8.0673 train_time:459180ms step_avg:181.42ms\nstep:2542/5000 train_loss:8.1650 train_time:459361ms step_avg:181.42ms\nstep:2543/5000 train_loss:8.2607 train_time:459542ms step_avg:181.42ms\nstep:2544/5000 train_loss:8.1179 train_time:459724ms step_avg:181.42ms\nstep:2545/5000 train_loss:8.1960 train_time:459905ms step_avg:181.42ms\nstep:2546/5000 train_loss:8.0365 train_time:460086ms step_avg:181.42ms\nstep:2547/5000 train_loss:8.0974 train_time:460268ms step_avg:181.42ms\nstep:2548/5000 train_loss:8.0302 train_time:460448ms step_avg:181.42ms\nstep:2549/5000 train_loss:8.0821 train_time:460629ms step_avg:181.42ms\nstep:2550/5000 train_loss:8.0069 train_time:460814ms step_avg:181.42ms\nstep:2551/5000 train_loss:8.1903 train_time:460998ms step_avg:181.42ms\nstep:2552/5000 train_loss:8.3219 train_time:461178ms step_avg:181.42ms\nstep:2553/5000 train_loss:8.1260 train_time:461359ms step_avg:181.42ms\nstep:2554/5000 train_loss:8.0061 train_time:461541ms step_avg:181.42ms\nstep:2555/5000 train_loss:8.0899 train_time:461724ms step_avg:181.42ms\nstep:2556/5000 train_loss:8.0948 train_time:461906ms step_avg:181.42ms\nstep:2557/5000 train_loss:8.1125 train_time:462087ms step_avg:181.42ms\nstep:2558/5000 train_loss:8.0876 train_time:462268ms step_avg:181.42ms\nstep:2559/5000 train_loss:8.1086 train_time:462449ms step_avg:181.42ms\nstep:2560/5000 train_loss:8.0251 train_time:462630ms step_avg:181.42ms\nstep:2561/5000 train_loss:8.1870 train_time:462813ms step_avg:181.42ms\nstep:2562/5000 train_loss:8.0649 train_time:462996ms step_avg:181.42ms\nstep:2563/5000 train_loss:8.0865 train_time:463178ms step_avg:181.42ms\nstep:2564/5000 train_loss:8.2245 train_time:463360ms step_avg:181.43ms\nstep:2565/5000 train_loss:8.0729 train_time:463541ms step_avg:181.43ms\nstep:2566/5000 train_loss:8.1409 train_time:463723ms step_avg:181.43ms\nstep:2567/5000 train_loss:8.1733 train_time:463905ms step_avg:181.43ms\nstep:2568/5000 train_loss:8.1362 train_time:464085ms step_avg:181.42ms\nstep:2569/5000 train_loss:8.1371 train_time:464266ms step_avg:181.42ms\nstep:2570/5000 train_loss:8.1075 train_time:464446ms step_avg:181.42ms\nstep:2571/5000 train_loss:8.0781 train_time:464627ms step_avg:181.42ms\nstep:2572/5000 train_loss:8.1373 train_time:464808ms step_avg:181.42ms\nstep:2573/5000 train_loss:8.2356 train_time:464990ms step_avg:181.42ms\nstep:2574/5000 train_loss:8.0554 train_time:465173ms step_avg:181.42ms\nstep:2575/5000 train_loss:8.0788 train_time:465356ms step_avg:181.43ms\nstep:2576/5000 train_loss:8.2037 train_time:465539ms step_avg:181.43ms\nstep:2577/5000 train_loss:8.1155 train_time:465721ms step_avg:181.43ms\nstep:2578/5000 train_loss:8.1193 train_time:465903ms step_avg:181.43ms\nstep:2579/5000 train_loss:8.1761 train_time:466085ms step_avg:181.43ms\nstep:2580/5000 train_loss:8.1511 train_time:466265ms step_avg:181.43ms\nstep:2581/5000 train_loss:8.0932 train_time:466446ms step_avg:181.43ms\nstep:2582/5000 train_loss:7.9820 train_time:466626ms step_avg:181.43ms\nstep:2583/5000 train_loss:8.1423 train_time:466808ms step_avg:181.43ms\nstep:2584/5000 train_loss:8.1879 train_time:466989ms step_avg:181.43ms\nstep:2585/5000 train_loss:8.0983 train_time:467170ms step_avg:181.43ms\nstep:2586/5000 train_loss:8.0862 train_time:467352ms step_avg:181.43ms\nstep:2587/5000 train_loss:8.0141 train_time:467532ms step_avg:181.43ms\nstep:2588/5000 train_loss:8.1275 train_time:467716ms step_avg:181.43ms\nstep:2589/5000 train_loss:8.0620 train_time:467899ms step_avg:181.43ms\nstep:2590/5000 train_loss:8.0653 train_time:468080ms step_avg:181.43ms\nstep:2591/5000 train_loss:8.0880 train_time:468261ms step_avg:181.43ms\nstep:2592/5000 train_loss:8.1543 train_time:468444ms step_avg:181.43ms\nstep:2593/5000 train_loss:8.1028 train_time:468625ms step_avg:181.43ms\nstep:2594/5000 train_loss:8.2434 train_time:468807ms step_avg:181.43ms\nstep:2595/5000 train_loss:8.1784 train_time:468989ms step_avg:181.43ms\nstep:2596/5000 train_loss:8.0409 train_time:469170ms step_avg:181.43ms\nstep:2597/5000 train_loss:8.2387 train_time:469351ms step_avg:181.43ms\nstep:2598/5000 train_loss:8.1676 train_time:469533ms step_avg:181.43ms\nstep:2599/5000 train_loss:8.1481 train_time:469717ms step_avg:181.43ms\nstep:2600/5000 train_loss:8.2118 train_time:469899ms step_avg:181.43ms\nstep:2601/5000 train_loss:8.2580 train_time:470081ms step_avg:181.43ms\nstep:2602/5000 train_loss:8.4595 train_time:470262ms step_avg:181.43ms\nstep:2603/5000 train_loss:8.1353 train_time:470443ms step_avg:181.43ms\nstep:2604/5000 train_loss:8.0887 train_time:470625ms step_avg:181.43ms\nstep:2605/5000 train_loss:8.2481 train_time:470806ms step_avg:181.43ms\nstep:2606/5000 train_loss:7.9900 train_time:470988ms step_avg:181.43ms\nstep:2607/5000 train_loss:8.0998 train_time:471168ms step_avg:181.43ms\nstep:2608/5000 train_loss:8.1384 train_time:471350ms step_avg:181.43ms\nstep:2609/5000 train_loss:8.2003 train_time:471532ms step_avg:181.43ms\nstep:2610/5000 train_loss:8.1215 train_time:471716ms step_avg:181.43ms\nstep:2611/5000 train_loss:8.0540 train_time:471899ms step_avg:181.43ms\nstep:2612/5000 train_loss:8.1464 train_time:472079ms step_avg:181.43ms\nstep:2613/5000 train_loss:8.1591 train_time:472261ms step_avg:181.43ms\nstep:2614/5000 train_loss:8.1263 train_time:472442ms step_avg:181.43ms\nstep:2615/5000 train_loss:8.1045 train_time:472624ms step_avg:181.43ms\nstep:2616/5000 train_loss:8.0684 train_time:472805ms step_avg:181.43ms\nstep:2617/5000 train_loss:8.2487 train_time:472985ms step_avg:181.43ms\nstep:2618/5000 train_loss:8.1286 train_time:473165ms step_avg:181.43ms\nstep:2619/5000 train_loss:8.0763 train_time:473345ms step_avg:181.43ms\nstep:2620/5000 train_loss:7.9964 train_time:473525ms step_avg:181.43ms\nstep:2621/5000 train_loss:8.1247 train_time:473707ms step_avg:181.43ms\nstep:2622/5000 train_loss:8.2298 train_time:473889ms step_avg:181.43ms\nstep:2623/5000 train_loss:8.1073 train_time:474070ms step_avg:181.43ms\nstep:2624/5000 train_loss:8.0559 train_time:474251ms step_avg:181.43ms\nstep:2625/5000 train_loss:8.0381 train_time:474433ms step_avg:181.43ms\nstep:2626/5000 train_loss:8.0625 train_time:474617ms step_avg:181.43ms\nstep:2627/5000 train_loss:8.1419 train_time:474799ms step_avg:181.43ms\nstep:2628/5000 train_loss:8.0702 train_time:474980ms step_avg:181.43ms\nstep:2629/5000 train_loss:8.0839 train_time:475163ms step_avg:181.43ms\nstep:2630/5000 train_loss:8.1654 train_time:475344ms step_avg:181.43ms\nstep:2631/5000 train_loss:8.1389 train_time:475525ms step_avg:181.43ms\nstep:2632/5000 train_loss:8.1371 train_time:475706ms step_avg:181.43ms\nstep:2633/5000 train_loss:7.9616 train_time:475888ms step_avg:181.43ms\nstep:2634/5000 train_loss:8.1356 train_time:476067ms step_avg:181.43ms\nstep:2635/5000 train_loss:8.0621 train_time:476249ms step_avg:181.43ms\nstep:2636/5000 train_loss:8.1634 train_time:476429ms step_avg:181.43ms\nstep:2637/5000 train_loss:8.0960 train_time:476613ms step_avg:181.43ms\nstep:2638/5000 train_loss:8.1009 train_time:476795ms step_avg:181.43ms\nstep:2639/5000 train_loss:8.1403 train_time:476978ms step_avg:181.43ms\nstep:2640/5000 train_loss:8.1444 train_time:477161ms step_avg:181.43ms\nstep:2641/5000 train_loss:8.1123 train_time:477342ms step_avg:181.43ms\nstep:2642/5000 train_loss:8.2209 train_time:477523ms step_avg:181.43ms\nstep:2643/5000 train_loss:8.0810 train_time:477705ms step_avg:181.43ms\nstep:2644/5000 train_loss:8.1054 train_time:477887ms step_avg:181.43ms\nstep:2645/5000 train_loss:8.1609 train_time:478068ms step_avg:181.43ms\nstep:2646/5000 train_loss:8.0663 train_time:478250ms step_avg:181.43ms\nstep:2647/5000 train_loss:8.1201 train_time:478431ms step_avg:181.43ms\nstep:2648/5000 train_loss:8.0539 train_time:478614ms step_avg:181.43ms\nstep:2649/5000 train_loss:8.1438 train_time:478797ms step_avg:181.43ms\nstep:2650/5000 train_loss:8.1478 train_time:478980ms step_avg:181.43ms\nstep:2651/5000 train_loss:8.1543 train_time:479160ms step_avg:181.43ms\nstep:2652/5000 train_loss:8.1399 train_time:479343ms step_avg:181.43ms\nstep:2653/5000 train_loss:8.0839 train_time:479525ms step_avg:181.43ms\nstep:2654/5000 train_loss:8.1021 train_time:479706ms step_avg:181.43ms\nstep:2655/5000 train_loss:8.1026 train_time:479887ms step_avg:181.43ms\nstep:2656/5000 train_loss:8.0999 train_time:480069ms step_avg:181.43ms\nstep:2657/5000 train_loss:8.0965 train_time:480250ms step_avg:181.43ms\nstep:2658/5000 train_loss:7.9855 train_time:480430ms step_avg:181.43ms\nstep:2659/5000 train_loss:8.0818 train_time:480613ms step_avg:181.43ms\nstep:2660/5000 train_loss:8.2373 train_time:480795ms step_avg:181.43ms\nstep:2661/5000 train_loss:8.0978 train_time:480977ms step_avg:181.43ms\nstep:2662/5000 train_loss:8.0288 train_time:481158ms step_avg:181.43ms\nstep:2663/5000 train_loss:8.1640 train_time:481341ms step_avg:181.43ms\nstep:2664/5000 train_loss:8.0440 train_time:481522ms step_avg:181.43ms\nstep:2665/5000 train_loss:8.1833 train_time:481704ms step_avg:181.43ms\nstep:2666/5000 train_loss:8.1784 train_time:481885ms step_avg:181.43ms\nstep:2667/5000 train_loss:8.0645 train_time:482066ms step_avg:181.43ms\nstep:2668/5000 train_loss:8.2277 train_time:482248ms step_avg:181.43ms\nstep:2669/5000 train_loss:8.1574 train_time:482428ms step_avg:181.43ms\nstep:2670/5000 train_loss:8.1035 train_time:482611ms step_avg:181.43ms\nstep:2671/5000 train_loss:8.1898 train_time:482793ms step_avg:181.43ms\nstep:2672/5000 train_loss:8.1136 train_time:482975ms step_avg:181.43ms\nstep:2673/5000 train_loss:8.0544 train_time:483157ms step_avg:181.43ms\nstep:2674/5000 train_loss:8.1644 train_time:483341ms step_avg:181.43ms\nstep:2675/5000 train_loss:8.2683 train_time:483523ms step_avg:181.43ms\nstep:2676/5000 train_loss:8.0813 train_time:483705ms step_avg:181.43ms\nstep:2677/5000 train_loss:8.1311 train_time:483885ms step_avg:181.43ms\nstep:2678/5000 train_loss:8.1198 train_time:484066ms step_avg:181.43ms\nstep:2679/5000 train_loss:8.1089 train_time:484246ms step_avg:181.43ms\nstep:2680/5000 train_loss:8.0022 train_time:484426ms step_avg:181.43ms\nstep:2681/5000 train_loss:8.0864 train_time:484607ms step_avg:181.43ms\nstep:2682/5000 train_loss:8.0157 train_time:484788ms step_avg:181.43ms\nstep:2683/5000 train_loss:8.0180 train_time:484969ms step_avg:181.43ms\nstep:2684/5000 train_loss:8.0580 train_time:485151ms step_avg:181.43ms\nstep:2685/5000 train_loss:8.1473 train_time:485333ms step_avg:181.43ms\nstep:2686/5000 train_loss:8.1175 train_time:485516ms step_avg:181.43ms\nstep:2687/5000 train_loss:8.2407 train_time:485698ms step_avg:181.43ms\nstep:2688/5000 train_loss:8.0557 train_time:485880ms step_avg:181.43ms\nstep:2689/5000 train_loss:8.0356 train_time:486062ms step_avg:181.43ms\nstep:2690/5000 train_loss:7.9876 train_time:486243ms step_avg:181.43ms\nstep:2691/5000 train_loss:8.0891 train_time:486425ms step_avg:181.43ms\nstep:2692/5000 train_loss:8.1264 train_time:486607ms step_avg:181.43ms\nstep:2693/5000 train_loss:8.1448 train_time:486787ms step_avg:181.43ms\nstep:2694/5000 train_loss:8.1565 train_time:486969ms step_avg:181.43ms\nstep:2695/5000 train_loss:8.1248 train_time:487150ms step_avg:181.43ms\nstep:2696/5000 train_loss:8.1906 train_time:487332ms step_avg:181.43ms\nstep:2697/5000 train_loss:8.0794 train_time:487516ms step_avg:181.44ms\nstep:2698/5000 train_loss:8.2930 train_time:487699ms step_avg:181.44ms\nstep:2699/5000 train_loss:8.0802 train_time:487880ms step_avg:181.44ms\nstep:2700/5000 train_loss:8.2010 train_time:488062ms step_avg:181.44ms\nstep:2701/5000 train_loss:8.0916 train_time:488244ms step_avg:181.44ms\nstep:2702/5000 train_loss:8.1506 train_time:488425ms step_avg:181.44ms\nstep:2703/5000 train_loss:8.0848 train_time:488607ms step_avg:181.44ms\nstep:2704/5000 train_loss:8.0680 train_time:488789ms step_avg:181.44ms\nstep:2705/5000 train_loss:8.0984 train_time:488970ms step_avg:181.44ms\nstep:2706/5000 train_loss:7.9916 train_time:489151ms step_avg:181.44ms\nstep:2707/5000 train_loss:8.0551 train_time:489331ms step_avg:181.44ms\nstep:2708/5000 train_loss:8.1075 train_time:489514ms step_avg:181.44ms\nstep:2709/5000 train_loss:8.0720 train_time:489696ms step_avg:181.44ms\nstep:2710/5000 train_loss:8.1248 train_time:489880ms step_avg:181.44ms\nstep:2711/5000 train_loss:8.0313 train_time:490061ms step_avg:181.44ms\nstep:2712/5000 train_loss:7.9938 train_time:490243ms step_avg:181.44ms\nstep:2713/5000 train_loss:8.1219 train_time:490425ms step_avg:181.44ms\nstep:2714/5000 train_loss:8.1856 train_time:490606ms step_avg:181.44ms\nstep:2715/5000 train_loss:8.0753 train_time:490788ms step_avg:181.44ms\nstep:2716/5000 train_loss:8.0282 train_time:490968ms step_avg:181.44ms\nstep:2717/5000 train_loss:8.0065 train_time:491149ms step_avg:181.44ms\nstep:2718/5000 train_loss:8.2790 train_time:491330ms step_avg:181.44ms\nstep:2719/5000 train_loss:8.1173 train_time:491513ms step_avg:181.44ms\nstep:2720/5000 train_loss:8.1512 train_time:491696ms step_avg:181.44ms\nstep:2721/5000 train_loss:8.1503 train_time:491880ms step_avg:181.44ms\nstep:2722/5000 train_loss:8.1226 train_time:492061ms step_avg:181.44ms\nstep:2723/5000 train_loss:8.0346 train_time:492242ms step_avg:181.44ms\nstep:2724/5000 train_loss:8.0987 train_time:492424ms step_avg:181.44ms\nstep:2725/5000 train_loss:8.1425 train_time:492605ms step_avg:181.44ms\nstep:2726/5000 train_loss:8.1129 train_time:492787ms step_avg:181.44ms\nstep:2727/5000 train_loss:8.2616 train_time:492968ms step_avg:181.44ms\nstep:2728/5000 train_loss:8.1215 train_time:493150ms step_avg:181.44ms\nstep:2729/5000 train_loss:8.1097 train_time:493331ms step_avg:181.44ms\nstep:2730/5000 train_loss:8.1273 train_time:493515ms step_avg:181.44ms\nstep:2731/5000 train_loss:7.9999 train_time:493697ms step_avg:181.44ms\nstep:2732/5000 train_loss:8.0522 train_time:493879ms step_avg:181.44ms\nstep:2733/5000 train_loss:8.1273 train_time:494061ms step_avg:181.44ms\nstep:2734/5000 train_loss:8.0962 train_time:494242ms step_avg:181.44ms\nstep:2735/5000 train_loss:8.0931 train_time:494423ms step_avg:181.44ms\nstep:2736/5000 train_loss:8.1631 train_time:494606ms step_avg:181.44ms\nstep:2737/5000 train_loss:8.2396 train_time:494787ms step_avg:181.44ms\nstep:2738/5000 train_loss:8.0581 train_time:494968ms step_avg:181.44ms\nstep:2739/5000 train_loss:7.9829 train_time:495150ms step_avg:181.44ms\nstep:2740/5000 train_loss:8.1160 train_time:495330ms step_avg:181.44ms\nstep:2741/5000 train_loss:8.3445 train_time:495512ms step_avg:181.44ms\nstep:2742/5000 train_loss:8.0781 train_time:495694ms step_avg:181.44ms\nstep:2743/5000 train_loss:8.1388 train_time:495876ms step_avg:181.44ms\nstep:2744/5000 train_loss:8.1377 train_time:496058ms step_avg:181.44ms\nstep:2745/5000 train_loss:8.0675 train_time:496241ms step_avg:181.44ms\nstep:2746/5000 train_loss:8.1516 train_time:496423ms step_avg:181.44ms\nstep:2747/5000 train_loss:8.1223 train_time:496605ms step_avg:181.44ms\nstep:2748/5000 train_loss:8.1820 train_time:496786ms step_avg:181.44ms\nstep:2749/5000 train_loss:8.0477 train_time:496966ms step_avg:181.44ms\nstep:2750/5000 train_loss:8.1130 train_time:497146ms step_avg:181.44ms\nstep:2751/5000 train_loss:8.1693 train_time:497327ms step_avg:181.44ms\nstep:2752/5000 train_loss:8.1198 train_time:497508ms step_avg:181.44ms\nstep:2753/5000 train_loss:8.1011 train_time:497690ms step_avg:181.44ms\nstep:2754/5000 train_loss:8.1174 train_time:497873ms step_avg:181.44ms\nstep:2755/5000 train_loss:8.1002 train_time:498056ms step_avg:181.44ms\nstep:2756/5000 train_loss:7.8167 train_time:498238ms step_avg:181.44ms\nstep:2757/5000 train_loss:8.0400 train_time:498421ms step_avg:181.44ms\nstep:2758/5000 train_loss:8.1223 train_time:498602ms step_avg:181.44ms\nstep:2759/5000 train_loss:8.2481 train_time:498783ms step_avg:181.44ms\nstep:2760/5000 train_loss:8.0576 train_time:498963ms step_avg:181.44ms\nstep:2761/5000 train_loss:8.0574 train_time:499146ms step_avg:181.44ms\nstep:2762/5000 train_loss:8.0147 train_time:499328ms step_avg:181.44ms\nstep:2763/5000 train_loss:8.0866 train_time:499512ms step_avg:181.44ms\nstep:2764/5000 train_loss:8.0491 train_time:499693ms step_avg:181.44ms\nstep:2765/5000 train_loss:8.1138 train_time:499876ms step_avg:181.44ms\nstep:2766/5000 train_loss:8.2212 train_time:500057ms step_avg:181.44ms\nstep:2767/5000 train_loss:8.0931 train_time:500240ms step_avg:181.44ms\nstep:2768/5000 train_loss:8.2129 train_time:500423ms step_avg:181.44ms\nstep:2769/5000 train_loss:8.1286 train_time:500605ms step_avg:181.44ms\nstep:2770/5000 train_loss:8.1069 train_time:500786ms step_avg:181.44ms\nstep:2771/5000 train_loss:8.0579 train_time:500966ms step_avg:181.44ms\nstep:2772/5000 train_loss:8.1079 train_time:501147ms step_avg:181.44ms\nstep:2773/5000 train_loss:8.1344 train_time:501327ms step_avg:181.44ms\nstep:2774/5000 train_loss:8.1344 train_time:501509ms step_avg:181.44ms\nstep:2775/5000 train_loss:8.0998 train_time:501692ms step_avg:181.44ms\nstep:2776/5000 train_loss:8.1189 train_time:501874ms step_avg:181.44ms\nstep:2777/5000 train_loss:7.8289 train_time:502056ms step_avg:181.44ms\nstep:2778/5000 train_loss:8.0841 train_time:502238ms step_avg:181.44ms\nstep:2779/5000 train_loss:8.1317 train_time:502420ms step_avg:181.44ms\nstep:2780/5000 train_loss:8.1409 train_time:502602ms step_avg:181.44ms\nstep:2781/5000 train_loss:7.9900 train_time:502783ms step_avg:181.44ms\nstep:2782/5000 train_loss:8.1149 train_time:502965ms step_avg:181.44ms\nstep:2783/5000 train_loss:8.1183 train_time:503145ms step_avg:181.44ms\nstep:2784/5000 train_loss:8.0916 train_time:503327ms step_avg:181.44ms\nstep:2785/5000 train_loss:8.1034 train_time:503509ms step_avg:181.44ms\nstep:2786/5000 train_loss:7.9837 train_time:503689ms step_avg:181.44ms\nstep:2787/5000 train_loss:8.1280 train_time:503870ms step_avg:181.44ms\nstep:2788/5000 train_loss:8.1350 train_time:504052ms step_avg:181.44ms\nstep:2789/5000 train_loss:8.1105 train_time:504234ms step_avg:181.44ms\nstep:2790/5000 train_loss:8.3042 train_time:504417ms step_avg:181.44ms\nstep:2791/5000 train_loss:8.0494 train_time:504598ms step_avg:181.44ms\nstep:2792/5000 train_loss:8.2065 train_time:504781ms step_avg:181.45ms\nstep:2793/5000 train_loss:8.0478 train_time:504961ms step_avg:181.44ms\nstep:2794/5000 train_loss:8.1331 train_time:505143ms step_avg:181.45ms\nstep:2795/5000 train_loss:8.1447 train_time:505325ms step_avg:181.45ms\nstep:2796/5000 train_loss:8.1081 train_time:505507ms step_avg:181.45ms\nstep:2797/5000 train_loss:8.0392 train_time:505688ms step_avg:181.45ms\nstep:2798/5000 train_loss:8.1398 train_time:505869ms step_avg:181.45ms\nstep:2799/5000 train_loss:8.0260 train_time:506049ms step_avg:181.44ms\nstep:2800/5000 train_loss:8.2716 train_time:506229ms step_avg:181.44ms\nstep:2801/5000 train_loss:8.1418 train_time:506412ms step_avg:181.44ms\nstep:2802/5000 train_loss:8.0414 train_time:506594ms step_avg:181.44ms\nstep:2803/5000 train_loss:8.2275 train_time:506777ms step_avg:181.45ms\nstep:2804/5000 train_loss:8.1219 train_time:506959ms step_avg:181.45ms\nstep:2805/5000 train_loss:8.0775 train_time:507140ms step_avg:181.45ms\nstep:2806/5000 train_loss:8.0039 train_time:507322ms step_avg:181.45ms\nstep:2807/5000 train_loss:8.0395 train_time:507503ms step_avg:181.45ms\nstep:2808/5000 train_loss:7.9899 train_time:507685ms step_avg:181.45ms\nstep:2809/5000 train_loss:8.2254 train_time:507866ms step_avg:181.45ms\nstep:2810/5000 train_loss:8.4176 train_time:508047ms step_avg:181.45ms\nstep:2811/5000 train_loss:8.1021 train_time:508227ms step_avg:181.44ms\nstep:2812/5000 train_loss:8.0573 train_time:508409ms step_avg:181.44ms\nstep:2813/5000 train_loss:8.2442 train_time:508590ms step_avg:181.44ms\nstep:2814/5000 train_loss:8.1002 train_time:508770ms step_avg:181.44ms\nstep:2815/5000 train_loss:8.0957 train_time:508953ms step_avg:181.44ms\nstep:2816/5000 train_loss:7.9341 train_time:509135ms step_avg:181.44ms\nstep:2817/5000 train_loss:8.1067 train_time:509318ms step_avg:181.45ms\nstep:2818/5000 train_loss:8.1035 train_time:509502ms step_avg:181.45ms\nstep:2819/5000 train_loss:8.0603 train_time:509684ms step_avg:181.45ms\nstep:2820/5000 train_loss:8.0589 train_time:509865ms step_avg:181.45ms\nstep:2821/5000 train_loss:8.1722 train_time:510046ms step_avg:181.45ms\nstep:2822/5000 train_loss:7.9586 train_time:510226ms step_avg:181.45ms\nstep:2823/5000 train_loss:8.0913 train_time:510407ms step_avg:181.45ms\nstep:2824/5000 train_loss:8.1806 train_time:510588ms step_avg:181.45ms\nstep:2825/5000 train_loss:8.0987 train_time:510770ms step_avg:181.45ms\nstep:2826/5000 train_loss:7.9920 train_time:510952ms step_avg:181.45ms\nstep:2827/5000 train_loss:8.0970 train_time:511134ms step_avg:181.45ms\nstep:2828/5000 train_loss:8.0730 train_time:511317ms step_avg:181.45ms\nstep:2829/5000 train_loss:8.0714 train_time:511500ms step_avg:181.45ms\nstep:2830/5000 train_loss:8.2074 train_time:511681ms step_avg:181.45ms\nstep:2831/5000 train_loss:8.1305 train_time:511863ms step_avg:181.45ms\nstep:2832/5000 train_loss:8.0695 train_time:512044ms step_avg:181.45ms\nstep:2833/5000 train_loss:8.0879 train_time:512226ms step_avg:181.45ms\nstep:2834/5000 train_loss:8.0852 train_time:512409ms step_avg:181.45ms\nstep:2835/5000 train_loss:8.6185 train_time:512590ms step_avg:181.45ms\nstep:2836/5000 train_loss:8.0705 train_time:512771ms step_avg:181.45ms\nstep:2837/5000 train_loss:8.0532 train_time:512952ms step_avg:181.45ms\nstep:2838/5000 train_loss:8.1404 train_time:513134ms step_avg:181.45ms\nstep:2839/5000 train_loss:8.0910 train_time:513318ms step_avg:181.45ms\nstep:2840/5000 train_loss:8.0081 train_time:513500ms step_avg:181.45ms\nstep:2841/5000 train_loss:8.0558 train_time:513681ms step_avg:181.45ms\nstep:2842/5000 train_loss:8.0457 train_time:513861ms step_avg:181.45ms\nstep:2843/5000 train_loss:8.0847 train_time:514042ms step_avg:181.45ms\nstep:2844/5000 train_loss:8.1710 train_time:514223ms step_avg:181.45ms\nstep:2845/5000 train_loss:8.1663 train_time:514405ms step_avg:181.45ms\nstep:2846/5000 train_loss:8.2363 train_time:514586ms step_avg:181.45ms\nstep:2847/5000 train_loss:8.1410 train_time:514767ms step_avg:181.45ms\nstep:2848/5000 train_loss:8.1490 train_time:514947ms step_avg:181.45ms\nstep:2849/5000 train_loss:7.8870 train_time:515127ms step_avg:181.45ms\nstep:2850/5000 train_loss:8.0364 train_time:515308ms step_avg:181.45ms\nstep:2851/5000 train_loss:8.1530 train_time:515489ms step_avg:181.45ms\nstep:2852/5000 train_loss:8.1056 train_time:515672ms step_avg:181.45ms\nstep:2853/5000 train_loss:7.9821 train_time:515855ms step_avg:181.45ms\nstep:2854/5000 train_loss:8.1901 train_time:516035ms step_avg:181.45ms\nstep:2855/5000 train_loss:8.0192 train_time:516216ms step_avg:181.45ms\nstep:2856/5000 train_loss:8.1930 train_time:516399ms step_avg:181.45ms\nstep:2857/5000 train_loss:8.0787 train_time:516581ms step_avg:181.45ms\nstep:2858/5000 train_loss:8.1738 train_time:516763ms step_avg:181.45ms\nstep:2859/5000 train_loss:8.1890 train_time:516945ms step_avg:181.45ms\nstep:2860/5000 train_loss:8.1740 train_time:517126ms step_avg:181.45ms\nstep:2861/5000 train_loss:8.1153 train_time:517308ms step_avg:181.45ms\nstep:2862/5000 train_loss:8.0475 train_time:517491ms step_avg:181.45ms\nstep:2863/5000 train_loss:8.0575 train_time:517674ms step_avg:181.45ms\nstep:2864/5000 train_loss:8.2039 train_time:517857ms step_avg:181.45ms\nstep:2865/5000 train_loss:8.1321 train_time:518039ms step_avg:181.45ms\nstep:2866/5000 train_loss:8.1515 train_time:518222ms step_avg:181.45ms\nstep:2867/5000 train_loss:8.0965 train_time:518404ms step_avg:181.45ms\nstep:2868/5000 train_loss:8.1778 train_time:518585ms step_avg:181.45ms\nstep:2869/5000 train_loss:7.9032 train_time:518766ms step_avg:181.45ms\nstep:2870/5000 train_loss:8.0961 train_time:518946ms step_avg:181.45ms\nstep:2871/5000 train_loss:7.9857 train_time:519126ms step_avg:181.45ms\nstep:2872/5000 train_loss:8.2458 train_time:519307ms step_avg:181.45ms\nstep:2873/5000 train_loss:8.1062 train_time:519489ms step_avg:181.45ms\nstep:2874/5000 train_loss:8.1877 train_time:519671ms step_avg:181.45ms\nstep:2875/5000 train_loss:8.1224 train_time:519853ms step_avg:181.45ms\nstep:2876/5000 train_loss:8.0957 train_time:520033ms step_avg:181.45ms\nstep:2877/5000 train_loss:8.1431 train_time:520216ms step_avg:181.45ms\nstep:2878/5000 train_loss:8.1166 train_time:520398ms step_avg:181.45ms\nstep:2879/5000 train_loss:8.2034 train_time:520579ms step_avg:181.45ms\nstep:2880/5000 train_loss:8.1019 train_time:520761ms step_avg:181.45ms\nstep:2881/5000 train_loss:8.1393 train_time:520942ms step_avg:181.45ms\nstep:2882/5000 train_loss:8.1038 train_time:521124ms step_avg:181.45ms\nstep:2883/5000 train_loss:8.0059 train_time:521306ms step_avg:181.45ms\nstep:2884/5000 train_loss:8.0495 train_time:521486ms step_avg:181.45ms\nstep:2885/5000 train_loss:8.1153 train_time:521667ms step_avg:181.45ms\nstep:2886/5000 train_loss:8.1292 train_time:521847ms step_avg:181.45ms\nstep:2887/5000 train_loss:7.9951 train_time:522027ms step_avg:181.45ms\nstep:2888/5000 train_loss:8.0267 train_time:522209ms step_avg:181.45ms\nstep:2889/5000 train_loss:8.1666 train_time:522391ms step_avg:181.45ms\nstep:2890/5000 train_loss:8.0084 train_time:522573ms step_avg:181.45ms\nstep:2891/5000 train_loss:8.2264 train_time:522756ms step_avg:181.45ms\nstep:2892/5000 train_loss:8.1495 train_time:522939ms step_avg:181.45ms\nstep:2893/5000 train_loss:8.1823 train_time:523122ms step_avg:181.45ms\nstep:2894/5000 train_loss:8.1065 train_time:523304ms step_avg:181.45ms\nstep:2895/5000 train_loss:8.2207 train_time:523485ms step_avg:181.45ms\nstep:2896/5000 train_loss:8.1394 train_time:523664ms step_avg:181.45ms\nstep:2897/5000 train_loss:8.0751 train_time:523846ms step_avg:181.45ms\nstep:2898/5000 train_loss:8.1012 train_time:524026ms step_avg:181.45ms\nstep:2899/5000 train_loss:8.0691 train_time:524208ms step_avg:181.45ms\nstep:2900/5000 train_loss:8.1250 train_time:524390ms step_avg:181.45ms\nstep:2901/5000 train_loss:8.0951 train_time:524573ms step_avg:181.45ms\nstep:2902/5000 train_loss:8.0882 train_time:524755ms step_avg:181.45ms\nstep:2903/5000 train_loss:8.0993 train_time:524937ms step_avg:181.45ms\nstep:2904/5000 train_loss:8.1272 train_time:525121ms step_avg:181.45ms\nstep:2905/5000 train_loss:8.0418 train_time:525302ms step_avg:181.45ms\nstep:2906/5000 train_loss:8.0749 train_time:525484ms step_avg:181.45ms\nstep:2907/5000 train_loss:8.0614 train_time:525665ms step_avg:181.45ms\nstep:2908/5000 train_loss:8.1136 train_time:525845ms step_avg:181.45ms\nstep:2909/5000 train_loss:8.2799 train_time:526026ms step_avg:181.45ms\nstep:2910/5000 train_loss:8.0430 train_time:526208ms step_avg:181.45ms\nstep:2911/5000 train_loss:8.1705 train_time:526389ms step_avg:181.45ms\nstep:2912/5000 train_loss:7.9466 train_time:526571ms step_avg:181.45ms\nstep:2913/5000 train_loss:8.1302 train_time:526754ms step_avg:181.45ms\nstep:2914/5000 train_loss:8.1585 train_time:526936ms step_avg:181.45ms\nstep:2915/5000 train_loss:8.1208 train_time:527118ms step_avg:181.45ms\nstep:2916/5000 train_loss:8.1501 train_time:527300ms step_avg:181.45ms\nstep:2917/5000 train_loss:8.0370 train_time:527481ms step_avg:181.45ms\nstep:2918/5000 train_loss:7.9941 train_time:527664ms step_avg:181.45ms\nstep:2919/5000 train_loss:7.9485 train_time:527845ms step_avg:181.45ms\nstep:2920/5000 train_loss:8.2577 train_time:528026ms step_avg:181.45ms\nstep:2921/5000 train_loss:8.1791 train_time:528208ms step_avg:181.45ms\nstep:2922/5000 train_loss:7.9825 train_time:528389ms step_avg:181.45ms\nstep:2923/5000 train_loss:8.1816 train_time:528570ms step_avg:181.45ms\nstep:2924/5000 train_loss:8.1763 train_time:528751ms step_avg:181.45ms\nstep:2925/5000 train_loss:8.0883 train_time:528933ms step_avg:181.45ms\nstep:2926/5000 train_loss:8.0483 train_time:529116ms step_avg:181.45ms\nstep:2927/5000 train_loss:8.1642 train_time:529298ms step_avg:181.45ms\nstep:2928/5000 train_loss:7.9894 train_time:529481ms step_avg:181.45ms\nstep:2929/5000 train_loss:8.0704 train_time:529663ms step_avg:181.45ms\nstep:2930/5000 train_loss:8.0471 train_time:529843ms step_avg:181.45ms\nstep:2931/5000 train_loss:8.1689 train_time:530024ms step_avg:181.45ms\nstep:2932/5000 train_loss:8.1658 train_time:530206ms step_avg:181.45ms\nstep:2933/5000 train_loss:8.0421 train_time:530387ms step_avg:181.45ms\nstep:2934/5000 train_loss:8.1244 train_time:530566ms step_avg:181.45ms\nstep:2935/5000 train_loss:8.0559 train_time:530747ms step_avg:181.45ms\nstep:2936/5000 train_loss:8.2864 train_time:530928ms step_avg:181.45ms\nstep:2937/5000 train_loss:7.9946 train_time:531110ms step_avg:181.45ms\nstep:2938/5000 train_loss:8.1190 train_time:531292ms step_avg:181.45ms\nstep:2939/5000 train_loss:8.1302 train_time:531475ms step_avg:181.45ms\nstep:2940/5000 train_loss:8.0219 train_time:531656ms step_avg:181.45ms\nstep:2941/5000 train_loss:8.1119 train_time:531839ms step_avg:181.45ms\nstep:2942/5000 train_loss:8.3188 train_time:532021ms step_avg:181.45ms\nstep:2943/5000 train_loss:8.0430 train_time:532202ms step_avg:181.45ms\nstep:2944/5000 train_loss:8.1476 train_time:532383ms step_avg:181.45ms\nstep:2945/5000 train_loss:8.1904 train_time:532564ms step_avg:181.45ms\nstep:2946/5000 train_loss:8.1412 train_time:532744ms step_avg:181.45ms\nstep:2947/5000 train_loss:8.0371 train_time:532925ms step_avg:181.45ms\nstep:2948/5000 train_loss:8.1709 train_time:533107ms step_avg:181.45ms\nstep:2949/5000 train_loss:7.9917 train_time:533288ms step_avg:181.45ms\nstep:2950/5000 train_loss:8.0846 train_time:533468ms step_avg:181.45ms\nstep:2951/5000 train_loss:8.2004 train_time:533650ms step_avg:181.45ms\nstep:2952/5000 train_loss:8.0689 train_time:533831ms step_avg:181.45ms\nstep:2953/5000 train_loss:8.0650 train_time:534014ms step_avg:181.45ms\nstep:2954/5000 train_loss:8.0505 train_time:534196ms step_avg:181.45ms\nstep:2955/5000 train_loss:8.1344 train_time:534379ms step_avg:181.45ms\nstep:2956/5000 train_loss:8.0938 train_time:534561ms step_avg:181.45ms\nstep:2957/5000 train_loss:8.0743 train_time:534741ms step_avg:181.45ms\nstep:2958/5000 train_loss:8.1722 train_time:534923ms step_avg:181.45ms\nstep:2959/5000 train_loss:8.1923 train_time:535105ms step_avg:181.45ms\nstep:2960/5000 train_loss:8.0210 train_time:535286ms step_avg:181.45ms\nstep:2961/5000 train_loss:8.1259 train_time:535467ms step_avg:181.45ms\nstep:2962/5000 train_loss:8.1346 train_time:535649ms step_avg:181.45ms\nstep:2963/5000 train_loss:8.0969 train_time:535829ms step_avg:181.45ms\nstep:2964/5000 train_loss:7.9783 train_time:536011ms step_avg:181.45ms\nstep:2965/5000 train_loss:8.0792 train_time:536194ms step_avg:181.45ms\nstep:2966/5000 train_loss:8.1520 train_time:536376ms step_avg:181.45ms\nstep:2967/5000 train_loss:8.1057 train_time:536559ms step_avg:181.45ms\nstep:2968/5000 train_loss:8.1241 train_time:536741ms step_avg:181.45ms\nstep:2969/5000 train_loss:8.1372 train_time:536923ms step_avg:181.45ms\nstep:2970/5000 train_loss:8.1691 train_time:537104ms step_avg:181.45ms\nstep:2971/5000 train_loss:8.1209 train_time:537285ms step_avg:181.45ms\nstep:2972/5000 train_loss:8.2676 train_time:537465ms step_avg:181.45ms\nstep:2973/5000 train_loss:8.3245 train_time:537646ms step_avg:181.45ms\nstep:2974/5000 train_loss:8.1343 train_time:537827ms step_avg:181.45ms\nstep:2975/5000 train_loss:8.1271 train_time:538009ms step_avg:181.45ms\nstep:2976/5000 train_loss:8.1641 train_time:538191ms step_avg:181.45ms\nstep:2977/5000 train_loss:8.1348 train_time:538373ms step_avg:181.45ms\nstep:2978/5000 train_loss:8.1754 train_time:538556ms step_avg:181.45ms\nstep:2979/5000 train_loss:8.1416 train_time:538739ms step_avg:181.45ms\nstep:2980/5000 train_loss:8.1121 train_time:538922ms step_avg:181.46ms\nstep:2981/5000 train_loss:8.0902 train_time:539103ms step_avg:181.46ms\nstep:2982/5000 train_loss:8.0805 train_time:539284ms step_avg:181.46ms\nstep:2983/5000 train_loss:8.0082 train_time:539465ms step_avg:181.45ms\nstep:2984/5000 train_loss:8.1342 train_time:539645ms step_avg:181.45ms\nstep:2985/5000 train_loss:8.0660 train_time:539826ms step_avg:181.45ms\nstep:2986/5000 train_loss:8.1012 train_time:540008ms step_avg:181.45ms\nstep:2987/5000 train_loss:8.0218 train_time:540188ms step_avg:181.45ms\nstep:2988/5000 train_loss:8.1104 train_time:540371ms step_avg:181.45ms\nstep:2989/5000 train_loss:8.1666 train_time:540554ms step_avg:181.45ms\nstep:2990/5000 train_loss:8.1619 train_time:540735ms step_avg:181.45ms\nstep:2991/5000 train_loss:8.0341 train_time:540918ms step_avg:181.46ms\nstep:2992/5000 train_loss:8.1243 train_time:541100ms step_avg:181.46ms\nstep:2993/5000 train_loss:8.0559 train_time:541281ms step_avg:181.46ms\nstep:2994/5000 train_loss:8.0892 train_time:541463ms step_avg:181.46ms\nstep:2995/5000 train_loss:8.1843 train_time:541644ms step_avg:181.46ms\nstep:2996/5000 train_loss:8.0207 train_time:541825ms step_avg:181.45ms\nstep:2997/5000 train_loss:8.1227 train_time:542006ms step_avg:181.45ms\nstep:2998/5000 train_loss:8.1659 train_time:542187ms step_avg:181.45ms\nstep:2999/5000 train_loss:8.1397 train_time:542367ms step_avg:181.45ms\nstep:3000/5000 train_loss:8.0997 train_time:542548ms step_avg:181.45ms\nstep:3001/5000 train_loss:8.1040 train_time:542729ms step_avg:181.45ms\nstep:3002/5000 train_loss:7.9879 train_time:542911ms step_avg:181.45ms\nstep:3003/5000 train_loss:8.0637 train_time:543094ms step_avg:181.45ms\nstep:3004/5000 train_loss:8.0855 train_time:543275ms step_avg:181.45ms\nstep:3005/5000 train_loss:8.1449 train_time:543457ms step_avg:181.45ms\nstep:3006/5000 train_loss:8.1587 train_time:543639ms step_avg:181.46ms\nstep:3007/5000 train_loss:7.9262 train_time:543821ms step_avg:181.46ms\nstep:3008/5000 train_loss:8.0814 train_time:544003ms step_avg:181.46ms\nstep:3009/5000 train_loss:8.1573 train_time:544184ms step_avg:181.46ms\nstep:3010/5000 train_loss:7.9542 train_time:544364ms step_avg:181.45ms\nstep:3011/5000 train_loss:8.0381 train_time:544544ms step_avg:181.45ms\nstep:3012/5000 train_loss:8.1016 train_time:544725ms step_avg:181.45ms\nstep:3013/5000 train_loss:8.1435 train_time:544906ms step_avg:181.45ms\nstep:3014/5000 train_loss:8.1315 train_time:545087ms step_avg:181.45ms\nstep:3015/5000 train_loss:8.0032 train_time:545268ms step_avg:181.45ms\nstep:3016/5000 train_loss:8.0779 train_time:545450ms step_avg:181.45ms\nstep:3017/5000 train_loss:8.0653 train_time:545631ms step_avg:181.45ms\nstep:3018/5000 train_loss:8.0319 train_time:545814ms step_avg:181.45ms\nstep:3019/5000 train_loss:8.1190 train_time:545996ms step_avg:181.45ms\nstep:3020/5000 train_loss:8.0619 train_time:546178ms step_avg:181.45ms\nstep:3021/5000 train_loss:8.1492 train_time:546359ms step_avg:181.45ms\nstep:3022/5000 train_loss:8.0318 train_time:546541ms step_avg:181.45ms\nstep:3023/5000 train_loss:7.9926 train_time:546722ms step_avg:181.45ms\nstep:3024/5000 train_loss:8.1503 train_time:546905ms step_avg:181.45ms\nstep:3025/5000 train_loss:8.1588 train_time:547086ms step_avg:181.45ms\nstep:3026/5000 train_loss:8.0904 train_time:547267ms step_avg:181.45ms\nstep:3027/5000 train_loss:8.1666 train_time:547449ms step_avg:181.45ms\nstep:3028/5000 train_loss:8.1334 train_time:547631ms step_avg:181.46ms\nstep:3029/5000 train_loss:8.1191 train_time:547814ms step_avg:181.46ms\nstep:3030/5000 train_loss:8.2522 train_time:547997ms step_avg:181.46ms\nstep:3031/5000 train_loss:8.2080 train_time:548179ms step_avg:181.46ms\nstep:3032/5000 train_loss:8.1124 train_time:548360ms step_avg:181.46ms\nstep:3033/5000 train_loss:8.1165 train_time:548541ms step_avg:181.46ms\nstep:3034/5000 train_loss:8.1745 train_time:548722ms step_avg:181.46ms\nstep:3035/5000 train_loss:8.0564 train_time:548904ms step_avg:181.46ms\nstep:3036/5000 train_loss:8.0914 train_time:549084ms step_avg:181.46ms\nstep:3037/5000 train_loss:8.1248 train_time:549265ms step_avg:181.46ms\nstep:3038/5000 train_loss:8.1234 train_time:549446ms step_avg:181.45ms\nstep:3039/5000 train_loss:8.0342 train_time:549626ms step_avg:181.45ms\nstep:3040/5000 train_loss:7.9152 train_time:549807ms step_avg:181.45ms\nstep:3041/5000 train_loss:8.1041 train_time:549988ms step_avg:181.45ms\nstep:3042/5000 train_loss:8.1022 train_time:550171ms step_avg:181.45ms\nstep:3043/5000 train_loss:8.1284 train_time:550354ms step_avg:181.46ms\nstep:3044/5000 train_loss:8.0761 train_time:550535ms step_avg:181.46ms\nstep:3045/5000 train_loss:8.1455 train_time:550717ms step_avg:181.46ms\nstep:3046/5000 train_loss:8.3091 train_time:550899ms step_avg:181.46ms\nstep:3047/5000 train_loss:8.1510 train_time:551081ms step_avg:181.46ms\nstep:3048/5000 train_loss:8.1246 train_time:551494ms step_avg:181.53ms\nstep:3049/5000 train_loss:8.1415 train_time:551673ms step_avg:181.53ms\nstep:3050/5000 train_loss:8.0431 train_time:551854ms step_avg:181.53ms\nstep:3051/5000 train_loss:8.0415 train_time:552034ms step_avg:181.53ms\nstep:3052/5000 train_loss:8.2394 train_time:552215ms step_avg:181.53ms\nstep:3053/5000 train_loss:8.0261 train_time:552403ms step_avg:181.53ms\nstep:3054/5000 train_loss:8.2320 train_time:552585ms step_avg:181.53ms\nstep:3055/5000 train_loss:8.2505 train_time:552765ms step_avg:181.53ms\nstep:3056/5000 train_loss:7.9143 train_time:552945ms step_avg:181.53ms\nstep:3057/5000 train_loss:8.1091 train_time:553125ms step_avg:181.53ms\nstep:3058/5000 train_loss:8.1272 train_time:553306ms step_avg:181.53ms\nstep:3059/5000 train_loss:8.1579 train_time:553486ms step_avg:181.53ms\nstep:3060/5000 train_loss:8.2071 train_time:553668ms step_avg:181.53ms\nstep:3061/5000 train_loss:8.1035 train_time:553849ms step_avg:181.53ms\nstep:3062/5000 train_loss:8.1498 train_time:554030ms step_avg:181.53ms\nstep:3063/5000 train_loss:8.1080 train_time:554213ms step_avg:181.53ms\nstep:3064/5000 train_loss:8.1210 train_time:554395ms step_avg:181.53ms\nstep:3065/5000 train_loss:8.0855 train_time:554578ms step_avg:181.53ms\nstep:3066/5000 train_loss:8.1091 train_time:554761ms step_avg:181.53ms\nstep:3067/5000 train_loss:8.2222 train_time:554942ms step_avg:181.53ms\nstep:3068/5000 train_loss:8.2491 train_time:555124ms step_avg:181.53ms\nstep:3069/5000 train_loss:8.0995 train_time:555305ms step_avg:181.53ms\nstep:3070/5000 train_loss:8.2121 train_time:555486ms step_avg:181.53ms\nstep:3071/5000 train_loss:8.0314 train_time:555665ms step_avg:181.53ms\nstep:3072/5000 train_loss:8.0292 train_time:555847ms step_avg:181.53ms\nstep:3073/5000 train_loss:8.1124 train_time:556028ms step_avg:181.53ms\nstep:3074/5000 train_loss:8.1263 train_time:556211ms step_avg:181.53ms\nstep:3075/5000 train_loss:8.1922 train_time:556393ms step_avg:181.53ms\nstep:3076/5000 train_loss:8.2172 train_time:556576ms step_avg:181.53ms\nstep:3077/5000 train_loss:8.1046 train_time:556758ms step_avg:181.53ms\nstep:3078/5000 train_loss:8.0829 train_time:556941ms step_avg:181.53ms\nstep:3079/5000 train_loss:8.0893 train_time:557122ms step_avg:181.53ms\nstep:3080/5000 train_loss:8.0868 train_time:557304ms step_avg:181.53ms\nstep:3081/5000 train_loss:8.0968 train_time:557484ms step_avg:181.53ms\nstep:3082/5000 train_loss:8.1972 train_time:557664ms step_avg:181.53ms\nstep:3083/5000 train_loss:8.1375 train_time:557845ms step_avg:181.53ms\nstep:3084/5000 train_loss:8.3338 train_time:558026ms step_avg:181.53ms\nstep:3085/5000 train_loss:7.8486 train_time:558208ms step_avg:181.53ms\nstep:3086/5000 train_loss:8.1844 train_time:558389ms step_avg:181.53ms\nstep:3087/5000 train_loss:8.0767 train_time:558570ms step_avg:181.53ms\nstep:3088/5000 train_loss:8.2156 train_time:558751ms step_avg:181.53ms\nstep:3089/5000 train_loss:8.1193 train_time:558933ms step_avg:181.53ms\nstep:3090/5000 train_loss:8.2357 train_time:559116ms step_avg:181.53ms\nstep:3091/5000 train_loss:8.0927 train_time:559297ms step_avg:181.53ms\nstep:3092/5000 train_loss:8.1022 train_time:559478ms step_avg:181.53ms\nstep:3093/5000 train_loss:8.1559 train_time:559660ms step_avg:181.53ms\nstep:3094/5000 train_loss:8.0521 train_time:559842ms step_avg:181.53ms\nstep:3095/5000 train_loss:8.0351 train_time:560022ms step_avg:181.53ms\nstep:3096/5000 train_loss:8.1767 train_time:560205ms step_avg:181.53ms\nstep:3097/5000 train_loss:8.0500 train_time:560386ms step_avg:181.53ms\nstep:3098/5000 train_loss:8.2315 train_time:560566ms step_avg:181.53ms\nstep:3099/5000 train_loss:8.0945 train_time:560746ms step_avg:181.53ms\nstep:3100/5000 train_loss:8.1751 train_time:560926ms step_avg:181.53ms\nstep:3101/5000 train_loss:8.0918 train_time:561107ms step_avg:181.53ms\nstep:3102/5000 train_loss:8.0715 train_time:561289ms step_avg:181.53ms\nstep:3103/5000 train_loss:8.2740 train_time:561469ms step_avg:181.53ms\nstep:3104/5000 train_loss:8.0615 train_time:561651ms step_avg:181.53ms\nstep:3105/5000 train_loss:8.1373 train_time:561834ms step_avg:181.53ms\nstep:3106/5000 train_loss:8.0950 train_time:562017ms step_avg:181.53ms\nstep:3107/5000 train_loss:8.0676 train_time:562199ms step_avg:181.53ms\nstep:3108/5000 train_loss:8.0301 train_time:562380ms step_avg:181.53ms\nstep:3109/5000 train_loss:8.1006 train_time:562561ms step_avg:181.53ms\nstep:3110/5000 train_loss:8.2387 train_time:562743ms step_avg:181.53ms\nstep:3111/5000 train_loss:8.1825 train_time:562924ms step_avg:181.53ms\nstep:3112/5000 train_loss:8.1036 train_time:563106ms step_avg:181.53ms\nstep:3113/5000 train_loss:8.1537 train_time:563286ms step_avg:181.53ms\nstep:3114/5000 train_loss:8.0575 train_time:563466ms step_avg:181.53ms\nstep:3115/5000 train_loss:8.1315 train_time:563648ms step_avg:181.53ms\nstep:3116/5000 train_loss:8.1271 train_time:563829ms step_avg:181.53ms\nstep:3117/5000 train_loss:8.0727 train_time:564011ms step_avg:181.53ms\nstep:3118/5000 train_loss:8.0466 train_time:564194ms step_avg:181.53ms\nstep:3119/5000 train_loss:8.1015 train_time:564376ms step_avg:181.53ms\nstep:3120/5000 train_loss:8.0805 train_time:564557ms step_avg:181.53ms\nstep:3121/5000 train_loss:8.1327 train_time:564739ms step_avg:181.53ms\nstep:3122/5000 train_loss:8.0079 train_time:564922ms step_avg:181.53ms\nstep:3123/5000 train_loss:8.1860 train_time:565103ms step_avg:181.53ms\nstep:3124/5000 train_loss:7.8813 train_time:565284ms step_avg:181.53ms\nstep:3125/5000 train_loss:8.0801 train_time:565464ms step_avg:181.53ms\nstep:3126/5000 train_loss:8.1420 train_time:565644ms step_avg:181.53ms\nstep:3127/5000 train_loss:8.1702 train_time:565825ms step_avg:181.53ms\nstep:3128/5000 train_loss:7.9826 train_time:566006ms step_avg:181.53ms\nstep:3129/5000 train_loss:8.0863 train_time:566188ms step_avg:181.53ms\nstep:3130/5000 train_loss:8.1444 train_time:566370ms step_avg:181.53ms\nstep:3131/5000 train_loss:8.0673 train_time:566552ms step_avg:181.53ms\nstep:3132/5000 train_loss:8.1618 train_time:566733ms step_avg:181.53ms\nstep:3133/5000 train_loss:8.1715 train_time:566917ms step_avg:181.53ms\nstep:3134/5000 train_loss:7.9845 train_time:567099ms step_avg:181.53ms\nstep:3135/5000 train_loss:8.0529 train_time:567280ms step_avg:181.53ms\nstep:3136/5000 train_loss:8.1250 train_time:567462ms step_avg:181.53ms\nstep:3137/5000 train_loss:8.0437 train_time:567644ms step_avg:181.53ms\nstep:3138/5000 train_loss:8.1956 train_time:567824ms step_avg:181.53ms\nstep:3139/5000 train_loss:8.0470 train_time:568005ms step_avg:181.53ms\nstep:3140/5000 train_loss:7.9996 train_time:568185ms step_avg:181.53ms\nstep:3141/5000 train_loss:8.0981 train_time:568365ms step_avg:181.53ms\nstep:3142/5000 train_loss:7.9627 train_time:568545ms step_avg:181.53ms\nstep:3143/5000 train_loss:8.0707 train_time:568726ms step_avg:181.53ms\nstep:3144/5000 train_loss:8.2805 train_time:568908ms step_avg:181.53ms\nstep:3145/5000 train_loss:8.0734 train_time:569090ms step_avg:181.53ms\nstep:3146/5000 train_loss:8.0806 train_time:569271ms step_avg:181.53ms\nstep:3147/5000 train_loss:8.1352 train_time:569454ms step_avg:181.53ms\nstep:3148/5000 train_loss:8.1055 train_time:569635ms step_avg:181.53ms\nstep:3149/5000 train_loss:7.8708 train_time:569818ms step_avg:181.53ms\nstep:3150/5000 train_loss:8.0341 train_time:569999ms step_avg:181.53ms\nstep:3151/5000 train_loss:8.0939 train_time:570180ms step_avg:181.53ms\nstep:3152/5000 train_loss:8.0783 train_time:570362ms step_avg:181.53ms\nstep:3153/5000 train_loss:8.2104 train_time:570543ms step_avg:181.53ms\nstep:3154/5000 train_loss:8.2630 train_time:570723ms step_avg:181.53ms\nstep:3155/5000 train_loss:8.1540 train_time:570905ms step_avg:181.53ms\nstep:3156/5000 train_loss:8.2905 train_time:571086ms step_avg:181.53ms\nstep:3157/5000 train_loss:8.1183 train_time:571266ms step_avg:181.53ms\nstep:3158/5000 train_loss:8.0097 train_time:571447ms step_avg:181.53ms\nstep:3159/5000 train_loss:7.9535 train_time:571627ms step_avg:181.53ms\nstep:3160/5000 train_loss:8.1084 train_time:571810ms step_avg:181.53ms\nstep:3161/5000 train_loss:8.1204 train_time:571992ms step_avg:181.53ms\nstep:3162/5000 train_loss:8.1347 train_time:572176ms step_avg:181.53ms\nstep:3163/5000 train_loss:8.1022 train_time:572357ms step_avg:181.53ms\nstep:3164/5000 train_loss:8.1773 train_time:572538ms step_avg:181.53ms\nstep:3165/5000 train_loss:8.4951 train_time:572721ms step_avg:181.53ms\nstep:3166/5000 train_loss:8.1223 train_time:572903ms step_avg:181.53ms\nstep:3167/5000 train_loss:7.9524 train_time:573085ms step_avg:181.53ms\nstep:3168/5000 train_loss:8.1587 train_time:573266ms step_avg:181.53ms\nstep:3169/5000 train_loss:8.1683 train_time:573446ms step_avg:181.53ms\nstep:3170/5000 train_loss:8.1249 train_time:573626ms step_avg:181.53ms\nstep:3171/5000 train_loss:8.1439 train_time:573807ms step_avg:181.53ms\nstep:3172/5000 train_loss:8.1870 train_time:573989ms step_avg:181.53ms\nstep:3173/5000 train_loss:8.0496 train_time:574171ms step_avg:181.53ms\nstep:3174/5000 train_loss:8.1589 train_time:574353ms step_avg:181.53ms\nstep:3175/5000 train_loss:8.1447 train_time:574536ms step_avg:181.53ms\nstep:3176/5000 train_loss:8.0267 train_time:574718ms step_avg:181.53ms\nstep:3177/5000 train_loss:8.2077 train_time:574900ms step_avg:181.53ms\nstep:3178/5000 train_loss:8.0905 train_time:575081ms step_avg:181.53ms\nstep:3179/5000 train_loss:8.0132 train_time:575262ms step_avg:181.53ms\nstep:3180/5000 train_loss:8.0929 train_time:575443ms step_avg:181.53ms\nstep:3181/5000 train_loss:8.0127 train_time:575624ms step_avg:181.53ms\nstep:3182/5000 train_loss:8.0772 train_time:575805ms step_avg:181.53ms\nstep:3183/5000 train_loss:8.0604 train_time:575985ms step_avg:181.53ms\nstep:3184/5000 train_loss:8.0463 train_time:576165ms step_avg:181.53ms\nstep:3185/5000 train_loss:8.2652 train_time:576345ms step_avg:181.53ms\nstep:3186/5000 train_loss:8.0515 train_time:576525ms step_avg:181.53ms\nstep:3187/5000 train_loss:8.0665 train_time:576708ms step_avg:181.53ms\nstep:3188/5000 train_loss:8.1110 train_time:576889ms step_avg:181.53ms\nstep:3189/5000 train_loss:8.1504 train_time:577071ms step_avg:181.53ms\nstep:3190/5000 train_loss:8.1316 train_time:577253ms step_avg:181.53ms\nstep:3191/5000 train_loss:8.1798 train_time:577434ms step_avg:181.53ms\nstep:3192/5000 train_loss:7.9964 train_time:577617ms step_avg:181.53ms\nstep:3193/5000 train_loss:8.2160 train_time:577799ms step_avg:181.53ms\nstep:3194/5000 train_loss:8.1180 train_time:577980ms step_avg:181.53ms\nstep:3195/5000 train_loss:8.1037 train_time:578162ms step_avg:181.53ms\nstep:3196/5000 train_loss:7.9900 train_time:578344ms step_avg:181.53ms\nstep:3197/5000 train_loss:8.0873 train_time:578525ms step_avg:181.53ms\nstep:3198/5000 train_loss:8.0821 train_time:578707ms step_avg:181.53ms\nstep:3199/5000 train_loss:8.0890 train_time:578887ms step_avg:181.53ms\nstep:3200/5000 train_loss:8.8257 train_time:579068ms step_avg:181.53ms\nstep:3201/5000 train_loss:8.0460 train_time:579250ms step_avg:181.53ms\nstep:3202/5000 train_loss:8.0428 train_time:579432ms step_avg:181.53ms\nstep:3203/5000 train_loss:8.1683 train_time:579615ms step_avg:181.53ms\nstep:3204/5000 train_loss:8.0971 train_time:579797ms step_avg:181.53ms\nstep:3205/5000 train_loss:8.0952 train_time:579978ms step_avg:181.53ms\nstep:3206/5000 train_loss:8.2036 train_time:580160ms step_avg:181.53ms\nstep:3207/5000 train_loss:8.1181 train_time:580342ms step_avg:181.53ms\nstep:3208/5000 train_loss:8.1117 train_time:580523ms step_avg:181.53ms\nstep:3209/5000 train_loss:8.0977 train_time:580706ms step_avg:181.53ms\nstep:3210/5000 train_loss:8.3099 train_time:580887ms step_avg:181.53ms\nstep:3211/5000 train_loss:8.3003 train_time:581067ms step_avg:181.53ms\nstep:3212/5000 train_loss:8.1140 train_time:581249ms step_avg:181.53ms\nstep:3213/5000 train_loss:8.2160 train_time:581430ms step_avg:181.53ms\nstep:3214/5000 train_loss:8.0817 train_time:581613ms step_avg:181.53ms\nstep:3215/5000 train_loss:8.1461 train_time:581796ms step_avg:181.53ms\nstep:3216/5000 train_loss:8.1313 train_time:581979ms step_avg:181.53ms\nstep:3217/5000 train_loss:8.1296 train_time:582160ms step_avg:181.53ms\nstep:3218/5000 train_loss:8.1161 train_time:582342ms step_avg:181.53ms\nstep:3219/5000 train_loss:8.1359 train_time:582522ms step_avg:181.53ms\nstep:3220/5000 train_loss:8.0435 train_time:582703ms step_avg:181.53ms\nstep:3221/5000 train_loss:8.0943 train_time:582884ms step_avg:181.53ms\nstep:3222/5000 train_loss:8.1776 train_time:583066ms step_avg:181.53ms\nstep:3223/5000 train_loss:8.0469 train_time:583246ms step_avg:181.53ms\nstep:3224/5000 train_loss:7.9375 train_time:583426ms step_avg:181.53ms\nstep:3225/5000 train_loss:8.2309 train_time:583608ms step_avg:181.53ms\nstep:3226/5000 train_loss:8.0857 train_time:583788ms step_avg:181.53ms\nstep:3227/5000 train_loss:8.4129 train_time:583970ms step_avg:181.53ms\nstep:3228/5000 train_loss:8.0828 train_time:584151ms step_avg:181.53ms\nstep:3229/5000 train_loss:8.1417 train_time:584332ms step_avg:181.53ms\nstep:3230/5000 train_loss:8.1128 train_time:584516ms step_avg:181.53ms\nstep:3231/5000 train_loss:8.1632 train_time:584698ms step_avg:181.53ms\nstep:3232/5000 train_loss:8.1184 train_time:584885ms step_avg:181.53ms\nstep:3233/5000 train_loss:8.1858 train_time:585065ms step_avg:181.53ms\nstep:3234/5000 train_loss:8.1715 train_time:585245ms step_avg:181.53ms\nstep:3235/5000 train_loss:8.3052 train_time:585425ms step_avg:181.53ms\nstep:3236/5000 train_loss:8.1610 train_time:585606ms step_avg:181.53ms\nstep:3237/5000 train_loss:8.1401 train_time:585786ms step_avg:181.53ms\nstep:3238/5000 train_loss:8.1083 train_time:585967ms step_avg:181.53ms\nstep:3239/5000 train_loss:8.0751 train_time:586148ms step_avg:181.53ms\nstep:3240/5000 train_loss:8.1369 train_time:586330ms step_avg:181.53ms\nstep:3241/5000 train_loss:8.1179 train_time:586513ms step_avg:181.53ms\nstep:3242/5000 train_loss:8.1046 train_time:586695ms step_avg:181.53ms\nstep:3243/5000 train_loss:8.3844 train_time:586875ms step_avg:181.53ms\nstep:3244/5000 train_loss:8.1479 train_time:587057ms step_avg:181.53ms\nstep:3245/5000 train_loss:8.1047 train_time:587238ms step_avg:181.53ms\nstep:3246/5000 train_loss:8.1258 train_time:587420ms step_avg:181.53ms\nstep:3247/5000 train_loss:8.1109 train_time:587602ms step_avg:181.53ms\nstep:3248/5000 train_loss:8.1007 train_time:587784ms step_avg:181.53ms\nstep:3249/5000 train_loss:8.1397 train_time:587964ms step_avg:181.53ms\nstep:3250/5000 train_loss:8.1379 train_time:588144ms step_avg:181.53ms\nstep:3251/5000 train_loss:8.1708 train_time:588324ms step_avg:181.53ms\nstep:3252/5000 train_loss:8.0674 train_time:588505ms step_avg:181.53ms\nstep:3253/5000 train_loss:8.1624 train_time:588686ms step_avg:181.52ms\nstep:3254/5000 train_loss:8.1113 train_time:588867ms step_avg:181.52ms\nstep:3255/5000 train_loss:8.1206 train_time:589047ms step_avg:181.52ms\nstep:3256/5000 train_loss:8.1643 train_time:589227ms step_avg:181.52ms\nstep:3257/5000 train_loss:8.0529 train_time:589408ms step_avg:181.52ms\nstep:3258/5000 train_loss:8.1450 train_time:589589ms step_avg:181.52ms\nstep:3259/5000 train_loss:8.1850 train_time:589771ms step_avg:181.52ms\nstep:3260/5000 train_loss:8.0336 train_time:589953ms step_avg:181.52ms\nstep:3261/5000 train_loss:8.2134 train_time:590134ms step_avg:181.52ms\nstep:3262/5000 train_loss:8.1416 train_time:590317ms step_avg:181.52ms\nstep:3263/5000 train_loss:8.2228 train_time:590499ms step_avg:181.52ms\nstep:3264/5000 train_loss:8.2415 train_time:590680ms step_avg:181.52ms\nstep:3265/5000 train_loss:8.0734 train_time:590862ms step_avg:181.52ms\nstep:3266/5000 train_loss:8.1150 train_time:591043ms step_avg:181.52ms\nstep:3267/5000 train_loss:8.0947 train_time:591224ms step_avg:181.52ms\nstep:3268/5000 train_loss:7.9938 train_time:591405ms step_avg:181.52ms\nstep:3269/5000 train_loss:8.0449 train_time:591585ms step_avg:181.52ms\nstep:3270/5000 train_loss:8.0587 train_time:591766ms step_avg:181.52ms\nstep:3271/5000 train_loss:8.0483 train_time:591946ms step_avg:181.52ms\nstep:3272/5000 train_loss:8.0927 train_time:592127ms step_avg:181.52ms\nstep:3273/5000 train_loss:8.0440 train_time:592309ms step_avg:181.52ms\nstep:3274/5000 train_loss:8.1110 train_time:592490ms step_avg:181.52ms\nstep:3275/5000 train_loss:8.1408 train_time:592673ms step_avg:181.52ms\nstep:3276/5000 train_loss:8.0338 train_time:592855ms step_avg:181.52ms\nstep:3277/5000 train_loss:8.0898 train_time:593038ms step_avg:181.52ms\nstep:3278/5000 train_loss:8.1619 train_time:593220ms step_avg:181.52ms\nstep:3279/5000 train_loss:8.0216 train_time:593401ms step_avg:181.52ms\nstep:3280/5000 train_loss:8.1910 train_time:593581ms step_avg:181.52ms\nstep:3281/5000 train_loss:7.9363 train_time:593762ms step_avg:181.52ms\nstep:3282/5000 train_loss:8.2308 train_time:593944ms step_avg:181.52ms\nstep:3283/5000 train_loss:8.2854 train_time:594126ms step_avg:181.52ms\nstep:3284/5000 train_loss:7.9384 train_time:594308ms step_avg:181.52ms\nstep:3285/5000 train_loss:8.0698 train_time:594488ms step_avg:181.52ms\nstep:3286/5000 train_loss:8.1367 train_time:594670ms step_avg:181.52ms\nstep:3287/5000 train_loss:8.0466 train_time:594851ms step_avg:181.52ms\nstep:3288/5000 train_loss:8.1118 train_time:595032ms step_avg:181.52ms\nstep:3289/5000 train_loss:8.1269 train_time:595215ms step_avg:181.52ms\nstep:3290/5000 train_loss:8.0838 train_time:595397ms step_avg:181.52ms\nstep:3291/5000 train_loss:8.0275 train_time:595579ms step_avg:181.52ms\nstep:3292/5000 train_loss:8.1402 train_time:595761ms step_avg:181.52ms\nstep:3293/5000 train_loss:8.0107 train_time:595942ms step_avg:181.52ms\nstep:3294/5000 train_loss:8.0404 train_time:596123ms step_avg:181.52ms\nstep:3295/5000 train_loss:8.1044 train_time:596305ms step_avg:181.52ms\nstep:3296/5000 train_loss:7.9814 train_time:596486ms step_avg:181.52ms\nstep:3297/5000 train_loss:8.1442 train_time:596666ms step_avg:181.52ms\nstep:3298/5000 train_loss:8.0754 train_time:596847ms step_avg:181.52ms\nstep:3299/5000 train_loss:8.1261 train_time:597027ms step_avg:181.52ms\nstep:3300/5000 train_loss:8.0729 train_time:597209ms step_avg:181.52ms\nstep:3301/5000 train_loss:7.9815 train_time:597390ms step_avg:181.52ms\nstep:3302/5000 train_loss:8.0344 train_time:597572ms step_avg:181.52ms\nstep:3303/5000 train_loss:8.0438 train_time:597755ms step_avg:181.52ms\nstep:3304/5000 train_loss:8.1189 train_time:597936ms step_avg:181.52ms\nstep:3305/5000 train_loss:7.9740 train_time:598119ms step_avg:181.52ms\nstep:3306/5000 train_loss:8.0303 train_time:598300ms step_avg:181.52ms\nstep:3307/5000 train_loss:8.1660 train_time:598480ms step_avg:181.52ms\nstep:3308/5000 train_loss:8.1135 train_time:598661ms step_avg:181.52ms\nstep:3309/5000 train_loss:8.1000 train_time:598843ms step_avg:181.52ms\nstep:3310/5000 train_loss:8.1069 train_time:599025ms step_avg:181.52ms\nstep:3311/5000 train_loss:8.1486 train_time:599207ms step_avg:181.52ms\nstep:3312/5000 train_loss:8.1635 train_time:599388ms step_avg:181.52ms\nstep:3313/5000 train_loss:8.1621 train_time:599568ms step_avg:181.52ms\nstep:3314/5000 train_loss:8.2240 train_time:599749ms step_avg:181.52ms\nstep:3315/5000 train_loss:8.1457 train_time:599930ms step_avg:181.52ms\nstep:3316/5000 train_loss:8.1609 train_time:600112ms step_avg:181.52ms\nstep:3317/5000 train_loss:8.1395 train_time:600294ms step_avg:181.52ms\nstep:3318/5000 train_loss:8.0954 train_time:600476ms step_avg:181.52ms\nstep:3319/5000 train_loss:8.1908 train_time:600658ms step_avg:181.52ms\nstep:3320/5000 train_loss:8.1922 train_time:600840ms step_avg:181.52ms\nstep:3321/5000 train_loss:8.1399 train_time:601021ms step_avg:181.52ms\nstep:3322/5000 train_loss:8.2222 train_time:601203ms step_avg:181.52ms\nstep:3323/5000 train_loss:8.1603 train_time:601385ms step_avg:181.52ms\nstep:3324/5000 train_loss:8.0173 train_time:601568ms step_avg:181.52ms\nstep:3325/5000 train_loss:8.1044 train_time:601748ms step_avg:181.52ms\nstep:3326/5000 train_loss:8.1211 train_time:601929ms step_avg:181.52ms\nstep:3327/5000 train_loss:8.1851 train_time:602112ms step_avg:181.52ms\nstep:3328/5000 train_loss:8.1547 train_time:602295ms step_avg:181.52ms\nstep:3329/5000 train_loss:8.0993 train_time:602476ms step_avg:181.52ms\nstep:3330/5000 train_loss:8.0216 train_time:602659ms step_avg:181.52ms\nstep:3331/5000 train_loss:8.2415 train_time:602841ms step_avg:181.52ms\nstep:3332/5000 train_loss:8.0901 train_time:603023ms step_avg:181.52ms\nstep:3333/5000 train_loss:8.1501 train_time:603204ms step_avg:181.52ms\nstep:3334/5000 train_loss:8.1488 train_time:603385ms step_avg:181.52ms\nstep:3335/5000 train_loss:8.1836 train_time:603566ms step_avg:181.52ms\nstep:3336/5000 train_loss:8.1226 train_time:603749ms step_avg:181.52ms\nstep:3337/5000 train_loss:8.1709 train_time:603927ms step_avg:181.52ms\nstep:3338/5000 train_loss:8.1017 train_time:604107ms step_avg:181.52ms\nstep:3339/5000 train_loss:8.2222 train_time:604291ms step_avg:181.52ms\nstep:3340/5000 train_loss:8.1545 train_time:604472ms step_avg:181.52ms\nstep:3341/5000 train_loss:8.1417 train_time:604655ms step_avg:181.52ms\nstep:3342/5000 train_loss:8.2107 train_time:604837ms step_avg:181.52ms\nstep:3343/5000 train_loss:8.1413 train_time:605020ms step_avg:181.52ms\nstep:3344/5000 train_loss:8.1408 train_time:605202ms step_avg:181.52ms\nstep:3345/5000 train_loss:8.0552 train_time:605381ms step_avg:181.52ms\nstep:3346/5000 train_loss:8.1044 train_time:605563ms step_avg:181.52ms\nstep:3347/5000 train_loss:8.1389 train_time:605745ms step_avg:181.52ms\nstep:3348/5000 train_loss:7.9925 train_time:605926ms step_avg:181.52ms\nstep:3349/5000 train_loss:8.0645 train_time:606108ms step_avg:181.52ms\nstep:3350/5000 train_loss:8.3204 train_time:606289ms step_avg:181.52ms\nstep:3351/5000 train_loss:8.1040 train_time:606470ms step_avg:181.52ms\nstep:3352/5000 train_loss:8.0878 train_time:606651ms step_avg:181.52ms\nstep:3353/5000 train_loss:7.9022 train_time:606834ms step_avg:181.52ms\nstep:3354/5000 train_loss:8.0936 train_time:607017ms step_avg:181.52ms\nstep:3355/5000 train_loss:8.0704 train_time:607199ms step_avg:181.52ms\nstep:3356/5000 train_loss:8.0688 train_time:607381ms step_avg:181.52ms\nstep:3357/5000 train_loss:8.1268 train_time:607561ms step_avg:181.52ms\nstep:3358/5000 train_loss:8.1387 train_time:607743ms step_avg:181.52ms\nstep:3359/5000 train_loss:8.1476 train_time:607924ms step_avg:181.52ms\nstep:3360/5000 train_loss:8.0758 train_time:608106ms step_avg:181.52ms\nstep:3361/5000 train_loss:8.0634 train_time:608285ms step_avg:181.52ms\nstep:3362/5000 train_loss:8.1203 train_time:608466ms step_avg:181.52ms\nstep:3363/5000 train_loss:8.0919 train_time:608646ms step_avg:181.52ms\nstep:3364/5000 train_loss:8.1057 train_time:608828ms step_avg:181.52ms\nstep:3365/5000 train_loss:8.0842 train_time:609010ms step_avg:181.52ms\nstep:3366/5000 train_loss:8.0879 train_time:609195ms step_avg:181.52ms\nstep:3367/5000 train_loss:8.1526 train_time:609374ms step_avg:181.52ms\nstep:3368/5000 train_loss:8.1080 train_time:609557ms step_avg:181.52ms\nstep:3369/5000 train_loss:8.1043 train_time:609739ms step_avg:181.52ms\nstep:3370/5000 train_loss:8.1834 train_time:609921ms step_avg:181.52ms\nstep:3371/5000 train_loss:8.1875 train_time:610102ms step_avg:181.52ms\nstep:3372/5000 train_loss:7.9634 train_time:610283ms step_avg:181.52ms\nstep:3373/5000 train_loss:8.0483 train_time:610463ms step_avg:181.52ms\nstep:3374/5000 train_loss:8.0795 train_time:610645ms step_avg:181.52ms\nstep:3375/5000 train_loss:8.0817 train_time:610825ms step_avg:181.52ms\nstep:3376/5000 train_loss:8.0065 train_time:611007ms step_avg:181.52ms\nstep:3377/5000 train_loss:8.1054 train_time:611186ms step_avg:181.52ms\nstep:3378/5000 train_loss:8.1107 train_time:611368ms step_avg:181.52ms\nstep:3379/5000 train_loss:8.2513 train_time:611548ms step_avg:181.52ms\nstep:3380/5000 train_loss:8.1984 train_time:611730ms step_avg:181.52ms\nstep:3381/5000 train_loss:7.9634 train_time:611911ms step_avg:181.52ms\nstep:3382/5000 train_loss:7.9997 train_time:612095ms step_avg:181.52ms\nstep:3383/5000 train_loss:8.1219 train_time:612278ms step_avg:181.52ms\nstep:3384/5000 train_loss:8.0408 train_time:612459ms step_avg:181.52ms\nstep:3385/5000 train_loss:8.1798 train_time:612641ms step_avg:181.52ms\nstep:3386/5000 train_loss:8.1420 train_time:612824ms step_avg:181.52ms\nstep:3387/5000 train_loss:8.0685 train_time:613005ms step_avg:181.52ms\nstep:3388/5000 train_loss:8.1358 train_time:613185ms step_avg:181.52ms\nstep:3389/5000 train_loss:8.1353 train_time:613366ms step_avg:181.52ms\nstep:3390/5000 train_loss:8.1014 train_time:613546ms step_avg:181.52ms\nstep:3391/5000 train_loss:8.1461 train_time:613726ms step_avg:181.52ms\nstep:3392/5000 train_loss:8.1356 train_time:613909ms step_avg:181.52ms\nstep:3393/5000 train_loss:8.1378 train_time:614090ms step_avg:181.52ms\nstep:3394/5000 train_loss:8.0991 train_time:614272ms step_avg:181.52ms\nstep:3395/5000 train_loss:8.2230 train_time:614456ms step_avg:181.52ms\nstep:3396/5000 train_loss:8.2334 train_time:614638ms step_avg:181.52ms\nstep:3397/5000 train_loss:8.2491 train_time:614821ms step_avg:181.52ms\nstep:3398/5000 train_loss:8.1027 train_time:615003ms step_avg:181.52ms\nstep:3399/5000 train_loss:8.1600 train_time:615183ms step_avg:181.52ms\nstep:3400/5000 train_loss:8.2233 train_time:615364ms step_avg:181.52ms\nstep:3401/5000 train_loss:8.1918 train_time:615545ms step_avg:181.52ms\nstep:3402/5000 train_loss:8.0536 train_time:615727ms step_avg:181.52ms\nstep:3403/5000 train_loss:8.0790 train_time:615909ms step_avg:181.52ms\nstep:3404/5000 train_loss:8.1335 train_time:616090ms step_avg:181.52ms\nstep:3405/5000 train_loss:8.0608 train_time:616274ms step_avg:181.52ms\nstep:3406/5000 train_loss:8.1443 train_time:616455ms step_avg:181.52ms\nstep:3407/5000 train_loss:8.2382 train_time:616637ms step_avg:181.52ms\nstep:3408/5000 train_loss:8.0839 train_time:616819ms step_avg:181.52ms\nstep:3409/5000 train_loss:8.1759 train_time:617001ms step_avg:181.52ms\nstep:3410/5000 train_loss:8.3290 train_time:617182ms step_avg:181.52ms\nstep:3411/5000 train_loss:8.1174 train_time:617363ms step_avg:181.52ms\nstep:3412/5000 train_loss:7.9918 train_time:617545ms step_avg:181.52ms\nstep:3413/5000 train_loss:8.1563 train_time:617729ms step_avg:181.52ms\nstep:3414/5000 train_loss:8.0565 train_time:617910ms step_avg:181.52ms\nstep:3415/5000 train_loss:8.1626 train_time:618092ms step_avg:181.52ms\nstep:3416/5000 train_loss:8.1764 train_time:618274ms step_avg:181.53ms\nstep:3417/5000 train_loss:8.0778 train_time:618457ms step_avg:181.53ms\nstep:3418/5000 train_loss:8.1302 train_time:618638ms step_avg:181.53ms\nstep:3419/5000 train_loss:8.1346 train_time:618822ms step_avg:181.53ms\nstep:3420/5000 train_loss:7.9908 train_time:619004ms step_avg:181.53ms\nstep:3421/5000 train_loss:8.1295 train_time:619185ms step_avg:181.53ms\nstep:3422/5000 train_loss:8.2081 train_time:619365ms step_avg:181.53ms\nstep:3423/5000 train_loss:8.1790 train_time:619547ms step_avg:181.53ms\nstep:3424/5000 train_loss:7.9145 train_time:619725ms step_avg:181.52ms\nstep:3425/5000 train_loss:8.1427 train_time:619907ms step_avg:181.52ms\nstep:3426/5000 train_loss:8.1741 train_time:620088ms step_avg:181.52ms\nstep:3427/5000 train_loss:7.9972 train_time:620272ms step_avg:181.53ms\nstep:3428/5000 train_loss:8.0553 train_time:620454ms step_avg:181.53ms\nstep:3429/5000 train_loss:8.1782 train_time:620635ms step_avg:181.53ms\nstep:3430/5000 train_loss:8.1190 train_time:620818ms step_avg:181.53ms\nstep:3431/5000 train_loss:8.0586 train_time:620999ms step_avg:181.53ms\nstep:3432/5000 train_loss:8.1490 train_time:621181ms step_avg:181.53ms\nstep:3433/5000 train_loss:8.0733 train_time:621362ms step_avg:181.53ms\nstep:3434/5000 train_loss:8.0585 train_time:621543ms step_avg:181.53ms\nstep:3435/5000 train_loss:8.0713 train_time:621724ms step_avg:181.53ms\nstep:3436/5000 train_loss:8.1539 train_time:621906ms step_avg:181.53ms\nstep:3437/5000 train_loss:8.2375 train_time:622086ms step_avg:181.53ms\nstep:3438/5000 train_loss:8.0987 train_time:622267ms step_avg:181.52ms\nstep:3439/5000 train_loss:8.1158 train_time:622449ms step_avg:181.52ms\nstep:3440/5000 train_loss:8.0841 train_time:622630ms step_avg:181.52ms\nstep:3441/5000 train_loss:8.3445 train_time:622813ms step_avg:181.53ms\nstep:3442/5000 train_loss:8.1344 train_time:622995ms step_avg:181.53ms\nstep:3443/5000 train_loss:8.1024 train_time:623177ms step_avg:181.53ms\nstep:3444/5000 train_loss:8.1895 train_time:623359ms step_avg:181.53ms\nstep:3445/5000 train_loss:8.1443 train_time:623540ms step_avg:181.53ms\nstep:3446/5000 train_loss:8.1479 train_time:623723ms step_avg:181.53ms\nstep:3447/5000 train_loss:8.1778 train_time:623905ms step_avg:181.53ms\nstep:3448/5000 train_loss:8.0606 train_time:624086ms step_avg:181.53ms\nstep:3449/5000 train_loss:8.2207 train_time:624269ms step_avg:181.53ms\nstep:3450/5000 train_loss:8.2144 train_time:624451ms step_avg:181.53ms\nstep:3451/5000 train_loss:8.1479 train_time:624632ms step_avg:181.53ms\nstep:3452/5000 train_loss:8.0536 train_time:624815ms step_avg:181.53ms\nstep:3453/5000 train_loss:8.2304 train_time:624997ms step_avg:181.53ms\nstep:3454/5000 train_loss:8.0412 train_time:625179ms step_avg:181.53ms\nstep:3455/5000 train_loss:8.1980 train_time:625361ms step_avg:181.53ms\nstep:3456/5000 train_loss:8.2336 train_time:625542ms step_avg:181.53ms\nstep:3457/5000 train_loss:8.1146 train_time:625723ms step_avg:181.53ms\nstep:3458/5000 train_loss:8.0514 train_time:625906ms step_avg:181.53ms\nstep:3459/5000 train_loss:8.1321 train_time:626087ms step_avg:181.53ms\nstep:3460/5000 train_loss:8.2645 train_time:626268ms step_avg:181.53ms\nstep:3461/5000 train_loss:8.0236 train_time:626450ms step_avg:181.53ms\nstep:3462/5000 train_loss:8.2329 train_time:626631ms step_avg:181.53ms\nstep:3463/5000 train_loss:8.1779 train_time:626812ms step_avg:181.53ms\nstep:3464/5000 train_loss:8.1350 train_time:626994ms step_avg:181.53ms\nstep:3465/5000 train_loss:8.2073 train_time:627177ms step_avg:181.53ms\nstep:3466/5000 train_loss:8.1931 train_time:627359ms step_avg:181.53ms\nstep:3467/5000 train_loss:8.0649 train_time:627541ms step_avg:181.53ms\nstep:3468/5000 train_loss:8.1251 train_time:627722ms step_avg:181.53ms\nstep:3469/5000 train_loss:7.9994 train_time:627904ms step_avg:181.53ms\nstep:3470/5000 train_loss:8.0437 train_time:628085ms step_avg:181.53ms\nstep:3471/5000 train_loss:8.0289 train_time:628265ms step_avg:181.53ms\nstep:3472/5000 train_loss:8.2205 train_time:628447ms step_avg:181.53ms\nstep:3473/5000 train_loss:8.1116 train_time:628628ms step_avg:181.53ms\nstep:3474/5000 train_loss:8.0881 train_time:628809ms step_avg:181.53ms\nstep:3475/5000 train_loss:8.0954 train_time:628991ms step_avg:181.53ms\nstep:3476/5000 train_loss:8.1389 train_time:629173ms step_avg:181.53ms\nstep:3477/5000 train_loss:8.0427 train_time:629357ms step_avg:181.53ms\nstep:3478/5000 train_loss:8.0775 train_time:629538ms step_avg:181.53ms\nstep:3479/5000 train_loss:8.1229 train_time:629720ms step_avg:181.53ms\nstep:3480/5000 train_loss:7.9734 train_time:629901ms step_avg:181.53ms\nstep:3481/5000 train_loss:8.1710 train_time:630081ms step_avg:181.53ms\nstep:3482/5000 train_loss:8.1005 train_time:630261ms step_avg:181.53ms\nstep:3483/5000 train_loss:8.1647 train_time:630443ms step_avg:181.53ms\nstep:3484/5000 train_loss:8.2721 train_time:630624ms step_avg:181.53ms\nstep:3485/5000 train_loss:8.2397 train_time:630806ms step_avg:181.53ms\nstep:3486/5000 train_loss:8.0410 train_time:630986ms step_avg:181.53ms\nstep:3487/5000 train_loss:7.9017 train_time:631166ms step_avg:181.53ms\nstep:3488/5000 train_loss:8.1453 train_time:631347ms step_avg:181.53ms\nstep:3489/5000 train_loss:8.0208 train_time:631527ms step_avg:181.53ms\nstep:3490/5000 train_loss:8.0473 train_time:631709ms step_avg:181.53ms\nstep:3491/5000 train_loss:8.0784 train_time:631891ms step_avg:181.53ms\nstep:3492/5000 train_loss:8.1903 train_time:632074ms step_avg:181.53ms\nstep:3493/5000 train_loss:8.1101 train_time:632256ms step_avg:181.53ms\nstep:3494/5000 train_loss:8.1780 train_time:632439ms step_avg:181.53ms\nstep:3495/5000 train_loss:8.1065 train_time:632621ms step_avg:181.53ms\nstep:3496/5000 train_loss:8.1514 train_time:632803ms step_avg:181.53ms\nstep:3497/5000 train_loss:8.0669 train_time:632984ms step_avg:181.53ms\nstep:3498/5000 train_loss:8.0902 train_time:633164ms step_avg:181.53ms\nstep:3499/5000 train_loss:8.0546 train_time:633344ms step_avg:181.53ms\nstep:3500/5000 train_loss:8.0265 train_time:633525ms step_avg:181.53ms\nstep:3501/5000 train_loss:8.1081 train_time:633708ms step_avg:181.53ms\nstep:3502/5000 train_loss:7.6056 train_time:633890ms step_avg:181.53ms\nstep:3503/5000 train_loss:8.1942 train_time:634071ms step_avg:181.53ms\nstep:3504/5000 train_loss:8.1166 train_time:634253ms step_avg:181.53ms\nstep:3505/5000 train_loss:8.0764 train_time:634434ms step_avg:181.53ms\nstep:3506/5000 train_loss:8.0796 train_time:634617ms step_avg:181.53ms\nstep:3507/5000 train_loss:8.0546 train_time:634799ms step_avg:181.53ms\nstep:3508/5000 train_loss:8.1311 train_time:634980ms step_avg:181.53ms\nstep:3509/5000 train_loss:8.4996 train_time:635162ms step_avg:181.53ms\nstep:3510/5000 train_loss:8.1913 train_time:635345ms step_avg:181.53ms\nstep:3511/5000 train_loss:7.9693 train_time:635526ms step_avg:181.53ms\nstep:3512/5000 train_loss:8.0821 train_time:635707ms step_avg:181.53ms\nstep:3513/5000 train_loss:8.1334 train_time:635889ms step_avg:181.53ms\nstep:3514/5000 train_loss:8.1402 train_time:636070ms step_avg:181.53ms\nstep:3515/5000 train_loss:8.1094 train_time:636252ms step_avg:181.53ms\nstep:3516/5000 train_loss:8.1245 train_time:636434ms step_avg:181.53ms\nstep:3517/5000 train_loss:8.0604 train_time:636617ms step_avg:181.53ms\nstep:3518/5000 train_loss:8.0356 train_time:636799ms step_avg:181.53ms\nstep:3519/5000 train_loss:8.0713 train_time:636981ms step_avg:181.53ms\nstep:3520/5000 train_loss:8.3262 train_time:637161ms step_avg:181.53ms\nstep:3521/5000 train_loss:8.0656 train_time:637342ms step_avg:181.53ms\nstep:3522/5000 train_loss:8.2163 train_time:637523ms step_avg:181.53ms\nstep:3523/5000 train_loss:8.1727 train_time:637705ms step_avg:181.53ms\nstep:3524/5000 train_loss:8.0559 train_time:637886ms step_avg:181.53ms\nstep:3525/5000 train_loss:8.1165 train_time:638067ms step_avg:181.53ms\nstep:3526/5000 train_loss:8.0960 train_time:638248ms step_avg:181.53ms\nstep:3527/5000 train_loss:8.0931 train_time:638430ms step_avg:181.53ms\nstep:3528/5000 train_loss:8.1029 train_time:638611ms step_avg:181.53ms\nstep:3529/5000 train_loss:8.0229 train_time:638795ms step_avg:181.53ms\nstep:3530/5000 train_loss:8.1464 train_time:638977ms step_avg:181.53ms\nstep:3531/5000 train_loss:8.1197 train_time:639159ms step_avg:181.53ms\nstep:3532/5000 train_loss:8.0430 train_time:639341ms step_avg:181.53ms\nstep:3533/5000 train_loss:8.1622 train_time:639522ms step_avg:181.53ms\nstep:3534/5000 train_loss:8.2097 train_time:639704ms step_avg:181.53ms\nstep:3535/5000 train_loss:8.0278 train_time:639884ms step_avg:181.53ms\nstep:3536/5000 train_loss:8.0305 train_time:640065ms step_avg:181.53ms\nstep:3537/5000 train_loss:8.0056 train_time:640246ms step_avg:181.53ms\nstep:3538/5000 train_loss:8.2484 train_time:640427ms step_avg:181.53ms\nstep:3539/5000 train_loss:8.0742 train_time:640609ms step_avg:181.53ms\nstep:3540/5000 train_loss:8.0657 train_time:640791ms step_avg:181.53ms\nstep:3541/5000 train_loss:8.1913 train_time:640972ms step_avg:181.53ms\nstep:3542/5000 train_loss:8.2045 train_time:641155ms step_avg:181.53ms\nstep:3543/5000 train_loss:8.0508 train_time:641337ms step_avg:181.53ms\nstep:3544/5000 train_loss:8.0588 train_time:641521ms step_avg:181.53ms\nstep:3545/5000 train_loss:8.0349 train_time:641702ms step_avg:181.53ms\nstep:3546/5000 train_loss:8.1012 train_time:641883ms step_avg:181.53ms\nstep:3547/5000 train_loss:8.0567 train_time:642063ms step_avg:181.53ms\nstep:3548/5000 train_loss:8.0647 train_time:642244ms step_avg:181.53ms\nstep:3549/5000 train_loss:8.2371 train_time:642425ms step_avg:181.53ms\nstep:3550/5000 train_loss:8.1725 train_time:642606ms step_avg:181.53ms\nstep:3551/5000 train_loss:8.1695 train_time:642788ms step_avg:181.53ms\nstep:3552/5000 train_loss:8.1068 train_time:642970ms step_avg:181.53ms\nstep:3553/5000 train_loss:8.1389 train_time:643151ms step_avg:181.53ms\nstep:3554/5000 train_loss:8.2372 train_time:643332ms step_avg:181.53ms\nstep:3555/5000 train_loss:8.0659 train_time:643517ms step_avg:181.53ms\nstep:3556/5000 train_loss:8.2012 train_time:643699ms step_avg:181.53ms\nstep:3557/5000 train_loss:8.1180 train_time:643880ms step_avg:181.53ms\nstep:3558/5000 train_loss:8.0152 train_time:644061ms step_avg:181.53ms\nstep:3559/5000 train_loss:8.0524 train_time:644243ms step_avg:181.53ms\nstep:3560/5000 train_loss:8.1440 train_time:644424ms step_avg:181.53ms\nstep:3561/5000 train_loss:8.1006 train_time:644606ms step_avg:181.53ms\nstep:3562/5000 train_loss:8.0689 train_time:644787ms step_avg:181.53ms\nstep:3563/5000 train_loss:8.0116 train_time:644969ms step_avg:181.53ms\nstep:3564/5000 train_loss:8.0605 train_time:645150ms step_avg:181.53ms\nstep:3565/5000 train_loss:8.0539 train_time:645331ms step_avg:181.53ms\nstep:3566/5000 train_loss:8.0884 train_time:645515ms step_avg:181.53ms\nstep:3567/5000 train_loss:8.2373 train_time:645696ms step_avg:181.53ms\nstep:3568/5000 train_loss:8.1204 train_time:645878ms step_avg:181.53ms\nstep:3569/5000 train_loss:8.1601 train_time:646059ms step_avg:181.53ms\nstep:3570/5000 train_loss:8.0780 train_time:646241ms step_avg:181.53ms\nstep:3571/5000 train_loss:8.0107 train_time:646424ms step_avg:181.53ms\nstep:3572/5000 train_loss:8.0722 train_time:646606ms step_avg:181.53ms\nstep:3573/5000 train_loss:8.1886 train_time:646787ms step_avg:181.53ms\nstep:3574/5000 train_loss:8.1233 train_time:646968ms step_avg:181.53ms\nstep:3575/5000 train_loss:8.1172 train_time:647150ms step_avg:181.53ms\nstep:3576/5000 train_loss:8.0710 train_time:647331ms step_avg:181.53ms\nstep:3577/5000 train_loss:8.0276 train_time:647516ms step_avg:181.53ms\nstep:3578/5000 train_loss:8.1307 train_time:647698ms step_avg:181.53ms\nstep:3579/5000 train_loss:8.0842 train_time:647879ms step_avg:181.53ms\nstep:3580/5000 train_loss:8.0531 train_time:648061ms step_avg:181.53ms\nstep:3581/5000 train_loss:8.0936 train_time:648243ms step_avg:181.53ms\nstep:3582/5000 train_loss:8.2814 train_time:648424ms step_avg:181.53ms\nstep:3583/5000 train_loss:8.1805 train_time:648605ms step_avg:181.53ms\nstep:3584/5000 train_loss:7.9767 train_time:648785ms step_avg:181.53ms\nstep:3585/5000 train_loss:8.0916 train_time:648967ms step_avg:181.53ms\nstep:3586/5000 train_loss:8.1927 train_time:649148ms step_avg:181.53ms\nstep:3587/5000 train_loss:8.0468 train_time:649329ms step_avg:181.53ms\nstep:3588/5000 train_loss:8.0392 train_time:649512ms step_avg:181.53ms\nstep:3589/5000 train_loss:8.0838 train_time:649694ms step_avg:181.53ms\nstep:3590/5000 train_loss:8.1523 train_time:649876ms step_avg:181.53ms\nstep:3591/5000 train_loss:7.9771 train_time:650059ms step_avg:181.53ms\nstep:3592/5000 train_loss:8.2099 train_time:650241ms step_avg:181.53ms\nstep:3593/5000 train_loss:8.1330 train_time:650423ms step_avg:181.53ms\nstep:3594/5000 train_loss:8.0543 train_time:650606ms step_avg:181.53ms\nstep:3595/5000 train_loss:8.2155 train_time:650786ms step_avg:181.53ms\nstep:3596/5000 train_loss:8.0516 train_time:650968ms step_avg:181.53ms\nstep:3597/5000 train_loss:8.2168 train_time:651148ms step_avg:181.53ms\nstep:3598/5000 train_loss:7.9285 train_time:651329ms step_avg:181.53ms\nstep:3599/5000 train_loss:8.0056 train_time:651512ms step_avg:181.53ms\nstep:3600/5000 train_loss:8.0439 train_time:651695ms step_avg:181.53ms\nstep:3601/5000 train_loss:8.5850 train_time:651878ms step_avg:181.53ms\nstep:3602/5000 train_loss:8.0882 train_time:652061ms step_avg:181.53ms\nstep:3603/5000 train_loss:8.1379 train_time:652242ms step_avg:181.53ms\nstep:3604/5000 train_loss:8.1322 train_time:652422ms step_avg:181.53ms\nstep:3605/5000 train_loss:8.0043 train_time:652604ms step_avg:181.53ms\nstep:3606/5000 train_loss:8.1120 train_time:652785ms step_avg:181.53ms\nstep:3607/5000 train_loss:8.1205 train_time:652967ms step_avg:181.53ms\nstep:3608/5000 train_loss:8.0778 train_time:653149ms step_avg:181.53ms\nstep:3609/5000 train_loss:8.0414 train_time:653330ms step_avg:181.53ms\nstep:3610/5000 train_loss:8.0637 train_time:653512ms step_avg:181.53ms\nstep:3611/5000 train_loss:8.0434 train_time:653694ms step_avg:181.53ms\nstep:3612/5000 train_loss:8.0930 train_time:653876ms step_avg:181.53ms\nstep:3613/5000 train_loss:8.1282 train_time:654059ms step_avg:181.53ms\nstep:3614/5000 train_loss:8.1658 train_time:654240ms step_avg:181.53ms\nstep:3615/5000 train_loss:8.1873 train_time:654422ms step_avg:181.53ms\nstep:3616/5000 train_loss:8.2487 train_time:654606ms step_avg:181.53ms\nstep:3617/5000 train_loss:8.0895 train_time:654787ms step_avg:181.53ms\nstep:3618/5000 train_loss:8.0503 train_time:654968ms step_avg:181.53ms\nstep:3619/5000 train_loss:8.1218 train_time:655149ms step_avg:181.53ms\nstep:3620/5000 train_loss:8.0736 train_time:655330ms step_avg:181.53ms\nstep:3621/5000 train_loss:8.1044 train_time:655514ms step_avg:181.53ms\nstep:3622/5000 train_loss:8.1516 train_time:655696ms step_avg:181.53ms\nstep:3623/5000 train_loss:8.0771 train_time:655878ms step_avg:181.53ms\nstep:3624/5000 train_loss:8.2190 train_time:656060ms step_avg:181.53ms\nstep:3625/5000 train_loss:8.0931 train_time:656241ms step_avg:181.53ms\nstep:3626/5000 train_loss:8.0776 train_time:656423ms step_avg:181.53ms\nstep:3627/5000 train_loss:8.0344 train_time:656605ms step_avg:181.53ms\nstep:3628/5000 train_loss:8.0483 train_time:656786ms step_avg:181.53ms\nstep:3629/5000 train_loss:8.0791 train_time:656967ms step_avg:181.53ms\nstep:3630/5000 train_loss:8.3435 train_time:657148ms step_avg:181.53ms\nstep:3631/5000 train_loss:8.0933 train_time:657328ms step_avg:181.53ms\nstep:3632/5000 train_loss:8.1236 train_time:657511ms step_avg:181.53ms\nstep:3633/5000 train_loss:8.0895 train_time:657695ms step_avg:181.53ms\nstep:3634/5000 train_loss:8.1094 train_time:657877ms step_avg:181.53ms\nstep:3635/5000 train_loss:8.2375 train_time:658059ms step_avg:181.53ms\nstep:3636/5000 train_loss:8.0476 train_time:658242ms step_avg:181.53ms\nstep:3637/5000 train_loss:8.1526 train_time:658422ms step_avg:181.53ms\nstep:3638/5000 train_loss:8.0810 train_time:658605ms step_avg:181.53ms\nstep:3639/5000 train_loss:8.0669 train_time:658785ms step_avg:181.53ms\nstep:3640/5000 train_loss:8.0477 train_time:658966ms step_avg:181.53ms\nstep:3641/5000 train_loss:8.1223 train_time:659149ms step_avg:181.53ms\nstep:3642/5000 train_loss:8.0660 train_time:659329ms step_avg:181.53ms\nstep:3643/5000 train_loss:8.1352 train_time:659511ms step_avg:181.53ms\nstep:3644/5000 train_loss:8.0806 train_time:659693ms step_avg:181.53ms\nstep:3645/5000 train_loss:8.2096 train_time:659876ms step_avg:181.53ms\nstep:3646/5000 train_loss:8.1910 train_time:660059ms step_avg:181.53ms\nstep:3647/5000 train_loss:8.0363 train_time:660242ms step_avg:181.53ms\nstep:3648/5000 train_loss:8.1459 train_time:660423ms step_avg:181.53ms\nstep:3649/5000 train_loss:8.1094 train_time:660604ms step_avg:181.53ms\nstep:3650/5000 train_loss:8.0781 train_time:660785ms step_avg:181.53ms\nstep:3651/5000 train_loss:8.0955 train_time:660965ms step_avg:181.53ms\nstep:3652/5000 train_loss:8.1508 train_time:661145ms step_avg:181.53ms\nstep:3653/5000 train_loss:8.1158 train_time:661326ms step_avg:181.53ms\nstep:3654/5000 train_loss:8.4478 train_time:661508ms step_avg:181.53ms\nstep:3655/5000 train_loss:8.1006 train_time:661690ms step_avg:181.53ms\nstep:3656/5000 train_loss:8.2266 train_time:661872ms step_avg:181.53ms\nstep:3657/5000 train_loss:8.1399 train_time:662055ms step_avg:181.53ms\nstep:3658/5000 train_loss:8.1596 train_time:662237ms step_avg:181.53ms\nstep:3659/5000 train_loss:8.0873 train_time:662419ms step_avg:181.53ms\nstep:3660/5000 train_loss:8.1270 train_time:662601ms step_avg:181.53ms\nstep:3661/5000 train_loss:8.0283 train_time:662782ms step_avg:181.53ms\nstep:3662/5000 train_loss:8.1355 train_time:662963ms step_avg:181.53ms\nstep:3663/5000 train_loss:8.0374 train_time:663145ms step_avg:181.53ms\nstep:3664/5000 train_loss:8.3142 train_time:663327ms step_avg:181.53ms\nstep:3665/5000 train_loss:8.0888 train_time:663509ms step_avg:181.53ms\nstep:3666/5000 train_loss:8.1658 train_time:663691ms step_avg:181.53ms\nstep:3667/5000 train_loss:8.1064 train_time:663872ms step_avg:181.53ms\nstep:3668/5000 train_loss:8.0572 train_time:664054ms step_avg:181.53ms\nstep:3669/5000 train_loss:8.1079 train_time:664236ms step_avg:181.53ms\nstep:3670/5000 train_loss:8.0833 train_time:664418ms step_avg:181.54ms\nstep:3671/5000 train_loss:8.0490 train_time:664600ms step_avg:181.54ms\nstep:3672/5000 train_loss:8.0984 train_time:664781ms step_avg:181.53ms\nstep:3673/5000 train_loss:8.3284 train_time:664962ms step_avg:181.53ms\nstep:3674/5000 train_loss:8.2036 train_time:665144ms step_avg:181.53ms\nstep:3675/5000 train_loss:8.2480 train_time:665325ms step_avg:181.53ms\nstep:3676/5000 train_loss:8.0937 train_time:665508ms step_avg:181.54ms\nstep:3677/5000 train_loss:8.1530 train_time:665689ms step_avg:181.54ms\nstep:3678/5000 train_loss:8.2304 train_time:665870ms step_avg:181.54ms\nstep:3679/5000 train_loss:8.0990 train_time:666053ms step_avg:181.54ms\nstep:3680/5000 train_loss:8.1056 train_time:666236ms step_avg:181.54ms\nstep:3681/5000 train_loss:7.7610 train_time:666419ms step_avg:181.54ms\nstep:3682/5000 train_loss:8.2194 train_time:666600ms step_avg:181.54ms\nstep:3683/5000 train_loss:7.8926 train_time:666781ms step_avg:181.54ms\nstep:3684/5000 train_loss:8.1071 train_time:666961ms step_avg:181.54ms\nstep:3685/5000 train_loss:8.1057 train_time:667143ms step_avg:181.54ms\nstep:3686/5000 train_loss:8.0688 train_time:667325ms step_avg:181.54ms\nstep:3687/5000 train_loss:8.0589 train_time:667506ms step_avg:181.54ms\nstep:3688/5000 train_loss:8.0057 train_time:667688ms step_avg:181.54ms\nstep:3689/5000 train_loss:8.1236 train_time:667869ms step_avg:181.54ms\nstep:3690/5000 train_loss:8.0433 train_time:668051ms step_avg:181.54ms\nstep:3691/5000 train_loss:8.0826 train_time:668233ms step_avg:181.54ms\nstep:3692/5000 train_loss:7.9729 train_time:668416ms step_avg:181.54ms\nstep:3693/5000 train_loss:8.1438 train_time:668599ms step_avg:181.54ms\nstep:3694/5000 train_loss:8.1582 train_time:668780ms step_avg:181.54ms\nstep:3695/5000 train_loss:8.1719 train_time:668961ms step_avg:181.54ms\nstep:3696/5000 train_loss:8.1635 train_time:669143ms step_avg:181.54ms\nstep:3697/5000 train_loss:8.0735 train_time:669325ms step_avg:181.54ms\nstep:3698/5000 train_loss:8.1542 train_time:669507ms step_avg:181.54ms\nstep:3699/5000 train_loss:8.0302 train_time:669688ms step_avg:181.54ms\nstep:3700/5000 train_loss:7.8885 train_time:669869ms step_avg:181.54ms\nstep:3701/5000 train_loss:8.0810 train_time:670053ms step_avg:181.54ms\nstep:3702/5000 train_loss:8.0812 train_time:670233ms step_avg:181.54ms\nstep:3703/5000 train_loss:8.0384 train_time:670416ms step_avg:181.54ms\nstep:3704/5000 train_loss:7.9894 train_time:670601ms step_avg:181.54ms\nstep:3705/5000 train_loss:8.0739 train_time:670780ms step_avg:181.54ms\nstep:3706/5000 train_loss:8.1275 train_time:670961ms step_avg:181.54ms\nstep:3707/5000 train_loss:8.0797 train_time:671142ms step_avg:181.54ms\nstep:3708/5000 train_loss:8.0262 train_time:671323ms step_avg:181.54ms\nstep:3709/5000 train_loss:7.9527 train_time:671505ms step_avg:181.54ms\nstep:3710/5000 train_loss:8.0443 train_time:671686ms step_avg:181.54ms\nstep:3711/5000 train_loss:8.1780 train_time:671867ms step_avg:181.54ms\nstep:3712/5000 train_loss:8.1311 train_time:672048ms step_avg:181.54ms\nstep:3713/5000 train_loss:8.1928 train_time:672232ms step_avg:181.54ms\nstep:3714/5000 train_loss:8.1092 train_time:672415ms step_avg:181.54ms\nstep:3715/5000 train_loss:8.0926 train_time:672595ms step_avg:181.54ms\nstep:3716/5000 train_loss:8.0817 train_time:672778ms step_avg:181.54ms\nstep:3717/5000 train_loss:8.1262 train_time:672960ms step_avg:181.54ms\nstep:3718/5000 train_loss:8.0641 train_time:673142ms step_avg:181.54ms\nstep:3719/5000 train_loss:8.1164 train_time:673322ms step_avg:181.54ms\nstep:3720/5000 train_loss:7.9650 train_time:673504ms step_avg:181.54ms\nstep:3721/5000 train_loss:8.1505 train_time:673685ms step_avg:181.54ms\nstep:3722/5000 train_loss:8.1594 train_time:673868ms step_avg:181.54ms\nstep:3723/5000 train_loss:8.1753 train_time:674047ms step_avg:181.54ms\nstep:3724/5000 train_loss:8.1316 train_time:674227ms step_avg:181.54ms\nstep:3725/5000 train_loss:8.2069 train_time:674410ms step_avg:181.54ms\nstep:3726/5000 train_loss:7.9877 train_time:674590ms step_avg:181.54ms\nstep:3727/5000 train_loss:8.1011 train_time:674772ms step_avg:181.54ms\nstep:3728/5000 train_loss:8.3060 train_time:674955ms step_avg:181.54ms\nstep:3729/5000 train_loss:8.1412 train_time:675138ms step_avg:181.54ms\nstep:3730/5000 train_loss:8.0670 train_time:675320ms step_avg:181.54ms\nstep:3731/5000 train_loss:8.1186 train_time:675502ms step_avg:181.54ms\nstep:3732/5000 train_loss:8.1635 train_time:675683ms step_avg:181.54ms\nstep:3733/5000 train_loss:8.0165 train_time:675863ms step_avg:181.54ms\nstep:3734/5000 train_loss:8.1512 train_time:676044ms step_avg:181.54ms\nstep:3735/5000 train_loss:8.1321 train_time:676225ms step_avg:181.54ms\nstep:3736/5000 train_loss:8.0385 train_time:676407ms step_avg:181.54ms\nstep:3737/5000 train_loss:8.1256 train_time:676588ms step_avg:181.54ms\nstep:3738/5000 train_loss:8.1152 train_time:676769ms step_avg:181.54ms\nstep:3739/5000 train_loss:8.2045 train_time:676951ms step_avg:181.54ms\nstep:3740/5000 train_loss:8.1544 train_time:677133ms step_avg:181.54ms\nstep:3741/5000 train_loss:8.1379 train_time:677316ms step_avg:181.54ms\nstep:3742/5000 train_loss:8.0860 train_time:677499ms step_avg:181.54ms\nstep:3743/5000 train_loss:8.1833 train_time:677682ms step_avg:181.54ms\nstep:3744/5000 train_loss:8.1529 train_time:677863ms step_avg:181.54ms\nstep:3745/5000 train_loss:8.2448 train_time:678044ms step_avg:181.54ms\nstep:3746/5000 train_loss:8.1725 train_time:678224ms step_avg:181.54ms\nstep:3747/5000 train_loss:8.0211 train_time:678405ms step_avg:181.54ms\nstep:3748/5000 train_loss:8.2273 train_time:678585ms step_avg:181.54ms\nstep:3749/5000 train_loss:8.1764 train_time:678766ms step_avg:181.54ms\nstep:3750/5000 train_loss:8.0517 train_time:678947ms step_avg:181.54ms\nstep:3751/5000 train_loss:8.1992 train_time:679128ms step_avg:181.54ms\nstep:3752/5000 train_loss:8.1180 train_time:679311ms step_avg:181.54ms\nstep:3753/5000 train_loss:8.0235 train_time:679493ms step_avg:181.54ms\nstep:3754/5000 train_loss:8.1263 train_time:679675ms step_avg:181.54ms\nstep:3755/5000 train_loss:8.1497 train_time:679858ms step_avg:181.54ms\nstep:3756/5000 train_loss:8.1555 train_time:680041ms step_avg:181.54ms\nstep:3757/5000 train_loss:8.0053 train_time:680223ms step_avg:181.54ms\nstep:3758/5000 train_loss:8.0759 train_time:680406ms step_avg:181.54ms\nstep:3759/5000 train_loss:8.1407 train_time:680587ms step_avg:181.54ms\nstep:3760/5000 train_loss:8.0843 train_time:680769ms step_avg:181.54ms\nstep:3761/5000 train_loss:8.0521 train_time:680954ms step_avg:181.54ms\nstep:3762/5000 train_loss:8.1827 train_time:681134ms step_avg:181.54ms\nstep:3763/5000 train_loss:8.0147 train_time:681318ms step_avg:181.54ms\nstep:3764/5000 train_loss:8.1351 train_time:681500ms step_avg:181.54ms\nstep:3765/5000 train_loss:8.0829 train_time:681680ms step_avg:181.54ms\nstep:3766/5000 train_loss:8.1191 train_time:681862ms step_avg:181.54ms\nstep:3767/5000 train_loss:8.0586 train_time:682044ms step_avg:181.54ms\nstep:3768/5000 train_loss:7.9451 train_time:682225ms step_avg:181.54ms\nstep:3769/5000 train_loss:8.1050 train_time:682408ms step_avg:181.54ms\nstep:3770/5000 train_loss:8.1028 train_time:682589ms step_avg:181.54ms\nstep:3771/5000 train_loss:8.2561 train_time:682771ms step_avg:181.54ms\nstep:3772/5000 train_loss:8.0742 train_time:682953ms step_avg:181.54ms\nstep:3773/5000 train_loss:8.0163 train_time:683135ms step_avg:181.54ms\nstep:3774/5000 train_loss:8.1114 train_time:683319ms step_avg:181.54ms\nstep:3775/5000 train_loss:8.0203 train_time:683500ms step_avg:181.54ms\nstep:3776/5000 train_loss:8.0423 train_time:683680ms step_avg:181.54ms\nstep:3777/5000 train_loss:8.1620 train_time:683862ms step_avg:181.54ms\nstep:3778/5000 train_loss:8.1337 train_time:684043ms step_avg:181.54ms\nstep:3779/5000 train_loss:8.1198 train_time:684225ms step_avg:181.54ms\nstep:3780/5000 train_loss:8.1424 train_time:684407ms step_avg:181.54ms\nstep:3781/5000 train_loss:8.0150 train_time:684588ms step_avg:181.54ms\nstep:3782/5000 train_loss:8.0246 train_time:684769ms step_avg:181.54ms\nstep:3783/5000 train_loss:8.1082 train_time:684950ms step_avg:181.54ms\nstep:3784/5000 train_loss:8.4288 train_time:685132ms step_avg:181.54ms\nstep:3785/5000 train_loss:8.1241 train_time:685316ms step_avg:181.54ms\nstep:3786/5000 train_loss:8.1663 train_time:685498ms step_avg:181.54ms\nstep:3787/5000 train_loss:8.1385 train_time:685680ms step_avg:181.54ms\nstep:3788/5000 train_loss:8.1875 train_time:685860ms step_avg:181.54ms\nstep:3789/5000 train_loss:7.9814 train_time:686041ms step_avg:181.54ms\nstep:3790/5000 train_loss:8.5290 train_time:686224ms step_avg:181.54ms\nstep:3791/5000 train_loss:8.0728 train_time:686406ms step_avg:181.54ms\nstep:3792/5000 train_loss:8.1649 train_time:686588ms step_avg:181.54ms\nstep:3793/5000 train_loss:8.1058 train_time:686770ms step_avg:181.54ms\nstep:3794/5000 train_loss:8.0689 train_time:686952ms step_avg:181.54ms\nstep:3795/5000 train_loss:8.3652 train_time:687133ms step_avg:181.54ms\nstep:3796/5000 train_loss:8.1559 train_time:687317ms step_avg:181.54ms\nstep:3797/5000 train_loss:8.1822 train_time:687500ms step_avg:181.54ms\nstep:3798/5000 train_loss:8.0998 train_time:687681ms step_avg:181.54ms\nstep:3799/5000 train_loss:8.1056 train_time:687862ms step_avg:181.54ms\nstep:3800/5000 train_loss:7.9636 train_time:688043ms step_avg:181.54ms\nstep:3801/5000 train_loss:8.1924 train_time:688224ms step_avg:181.54ms\nstep:3802/5000 train_loss:8.1761 train_time:688405ms step_avg:181.54ms\nstep:3803/5000 train_loss:8.0660 train_time:688586ms step_avg:181.54ms\nstep:3804/5000 train_loss:8.0745 train_time:688767ms step_avg:181.54ms\nstep:3805/5000 train_loss:8.0472 train_time:688948ms step_avg:181.54ms\nstep:3806/5000 train_loss:8.0956 train_time:689130ms step_avg:181.54ms\nstep:3807/5000 train_loss:7.9820 train_time:689313ms step_avg:181.54ms\nstep:3808/5000 train_loss:8.0991 train_time:689496ms step_avg:181.54ms\nstep:3809/5000 train_loss:8.3037 train_time:689678ms step_avg:181.54ms\nstep:3810/5000 train_loss:8.1475 train_time:690096ms step_avg:181.60ms\nstep:3811/5000 train_loss:8.2044 train_time:690277ms step_avg:181.60ms\nstep:3812/5000 train_loss:8.1138 train_time:690457ms step_avg:181.60ms\nstep:3813/5000 train_loss:8.0859 train_time:690637ms step_avg:181.60ms\nstep:3814/5000 train_loss:8.1104 train_time:690818ms step_avg:181.60ms\nstep:3815/5000 train_loss:8.1026 train_time:691005ms step_avg:181.60ms\nstep:3816/5000 train_loss:7.9955 train_time:691187ms step_avg:181.60ms\nstep:3817/5000 train_loss:8.0684 train_time:691368ms step_avg:181.60ms\nstep:3818/5000 train_loss:7.9979 train_time:691550ms step_avg:181.60ms\nstep:3819/5000 train_loss:8.0892 train_time:691729ms step_avg:181.60ms\nstep:3820/5000 train_loss:8.0041 train_time:691911ms step_avg:181.60ms\nstep:3821/5000 train_loss:8.0619 train_time:692095ms step_avg:181.60ms\nstep:3822/5000 train_loss:8.1773 train_time:692278ms step_avg:181.60ms\nstep:3823/5000 train_loss:8.0637 train_time:692460ms step_avg:181.61ms\nstep:3824/5000 train_loss:7.9709 train_time:692642ms step_avg:181.61ms\nstep:3825/5000 train_loss:7.9687 train_time:692822ms step_avg:181.60ms\nstep:3826/5000 train_loss:8.1453 train_time:693005ms step_avg:181.61ms\nstep:3827/5000 train_loss:8.0047 train_time:693187ms step_avg:181.61ms\nstep:3828/5000 train_loss:8.1085 train_time:693369ms step_avg:181.61ms\nstep:3829/5000 train_loss:8.1672 train_time:693551ms step_avg:181.61ms\nstep:3830/5000 train_loss:8.1315 train_time:693731ms step_avg:181.61ms\nstep:3831/5000 train_loss:8.2918 train_time:693914ms step_avg:181.61ms\nstep:3832/5000 train_loss:7.9993 train_time:694097ms step_avg:181.61ms\nstep:3833/5000 train_loss:8.2372 train_time:694279ms step_avg:181.61ms\nstep:3834/5000 train_loss:8.1094 train_time:694461ms step_avg:181.61ms\nstep:3835/5000 train_loss:8.0032 train_time:694642ms step_avg:181.61ms\nstep:3836/5000 train_loss:8.1434 train_time:694824ms step_avg:181.61ms\nstep:3837/5000 train_loss:8.1634 train_time:695006ms step_avg:181.61ms\nstep:3838/5000 train_loss:8.2866 train_time:695187ms step_avg:181.61ms\nstep:3839/5000 train_loss:8.0614 train_time:695368ms step_avg:181.61ms\nstep:3840/5000 train_loss:8.0774 train_time:695550ms step_avg:181.61ms\nstep:3841/5000 train_loss:8.1693 train_time:695731ms step_avg:181.61ms\nstep:3842/5000 train_loss:8.1204 train_time:695914ms step_avg:181.61ms\nstep:3843/5000 train_loss:8.1989 train_time:696096ms step_avg:181.61ms\nstep:3844/5000 train_loss:7.9720 train_time:696278ms step_avg:181.61ms\nstep:3845/5000 train_loss:8.3093 train_time:696460ms step_avg:181.61ms\nstep:3846/5000 train_loss:8.1411 train_time:696640ms step_avg:181.61ms\nstep:3847/5000 train_loss:8.1260 train_time:696823ms step_avg:181.61ms\nstep:3848/5000 train_loss:7.9223 train_time:697005ms step_avg:181.61ms\nstep:3849/5000 train_loss:8.1835 train_time:697186ms step_avg:181.61ms\nstep:3850/5000 train_loss:8.1397 train_time:697365ms step_avg:181.61ms\nstep:3851/5000 train_loss:8.0107 train_time:697546ms step_avg:181.61ms\nstep:3852/5000 train_loss:8.2223 train_time:697729ms step_avg:181.61ms\nstep:3853/5000 train_loss:8.1712 train_time:697910ms step_avg:181.61ms\nstep:3854/5000 train_loss:8.0606 train_time:698089ms step_avg:181.60ms\nstep:3855/5000 train_loss:8.1807 train_time:698272ms step_avg:181.61ms\nstep:3856/5000 train_loss:8.1244 train_time:698454ms step_avg:181.61ms\nstep:3857/5000 train_loss:8.1477 train_time:698635ms step_avg:181.61ms\nstep:3858/5000 train_loss:8.0451 train_time:698817ms step_avg:181.61ms\nstep:3859/5000 train_loss:8.0448 train_time:698999ms step_avg:181.61ms\nstep:3860/5000 train_loss:8.0773 train_time:699180ms step_avg:181.61ms\nstep:3861/5000 train_loss:8.1376 train_time:699360ms step_avg:181.60ms\nstep:3862/5000 train_loss:8.0917 train_time:699542ms step_avg:181.60ms\nstep:3863/5000 train_loss:8.1190 train_time:699723ms step_avg:181.60ms\nstep:3864/5000 train_loss:8.0671 train_time:699905ms step_avg:181.60ms\nstep:3865/5000 train_loss:8.0008 train_time:700087ms step_avg:181.60ms\nstep:3866/5000 train_loss:8.1323 train_time:700268ms step_avg:181.60ms\nstep:3867/5000 train_loss:8.1104 train_time:700449ms step_avg:181.60ms\nstep:3868/5000 train_loss:8.0658 train_time:700630ms step_avg:181.60ms\nstep:3869/5000 train_loss:8.0558 train_time:700811ms step_avg:181.60ms\nstep:3870/5000 train_loss:8.0453 train_time:700993ms step_avg:181.60ms\nstep:3871/5000 train_loss:8.0934 train_time:701175ms step_avg:181.60ms\nstep:3872/5000 train_loss:8.0920 train_time:701358ms step_avg:181.60ms\nstep:3873/5000 train_loss:8.1203 train_time:701540ms step_avg:181.61ms\nstep:3874/5000 train_loss:8.2280 train_time:701723ms step_avg:181.61ms\nstep:3875/5000 train_loss:7.9615 train_time:701904ms step_avg:181.61ms\nstep:3876/5000 train_loss:8.1209 train_time:702085ms step_avg:181.60ms\nstep:3877/5000 train_loss:8.1212 train_time:702266ms step_avg:181.60ms\nstep:3878/5000 train_loss:8.1563 train_time:702447ms step_avg:181.60ms\nstep:3879/5000 train_loss:8.0705 train_time:702628ms step_avg:181.60ms\nstep:3880/5000 train_loss:8.1758 train_time:702810ms step_avg:181.60ms\nstep:3881/5000 train_loss:8.0826 train_time:702992ms step_avg:181.60ms\nstep:3882/5000 train_loss:8.1925 train_time:703175ms step_avg:181.60ms\nstep:3883/5000 train_loss:8.0703 train_time:703357ms step_avg:181.61ms\nstep:3884/5000 train_loss:8.0858 train_time:703540ms step_avg:181.61ms\nstep:3885/5000 train_loss:8.0939 train_time:703723ms step_avg:181.61ms\nstep:3886/5000 train_loss:8.0541 train_time:703905ms step_avg:181.61ms\nstep:3887/5000 train_loss:8.0833 train_time:704086ms step_avg:181.61ms\nstep:3888/5000 train_loss:8.1436 train_time:704267ms step_avg:181.61ms\nstep:3889/5000 train_loss:8.2175 train_time:704447ms step_avg:181.61ms\nstep:3890/5000 train_loss:8.1404 train_time:704627ms step_avg:181.60ms\nstep:3891/5000 train_loss:8.0802 train_time:704809ms step_avg:181.61ms\nstep:3892/5000 train_loss:8.1215 train_time:704992ms step_avg:181.61ms\nstep:3893/5000 train_loss:8.1098 train_time:705175ms step_avg:181.61ms\nstep:3894/5000 train_loss:8.1713 train_time:705358ms step_avg:181.61ms\nstep:3895/5000 train_loss:7.9842 train_time:705540ms step_avg:181.61ms\nstep:3896/5000 train_loss:8.1739 train_time:705721ms step_avg:181.61ms\nstep:3897/5000 train_loss:8.1934 train_time:705902ms step_avg:181.61ms\nstep:3898/5000 train_loss:8.1568 train_time:706084ms step_avg:181.61ms\nstep:3899/5000 train_loss:8.0066 train_time:706265ms step_avg:181.61ms\nstep:3900/5000 train_loss:8.1812 train_time:706446ms step_avg:181.61ms\nstep:3901/5000 train_loss:8.1319 train_time:706627ms step_avg:181.61ms\nstep:3902/5000 train_loss:8.1068 train_time:706808ms step_avg:181.61ms\nstep:3903/5000 train_loss:8.1200 train_time:706990ms step_avg:181.61ms\nstep:3904/5000 train_loss:8.1398 train_time:707172ms step_avg:181.61ms\nstep:3905/5000 train_loss:8.0197 train_time:707356ms step_avg:181.61ms\nstep:3906/5000 train_loss:8.1007 train_time:707538ms step_avg:181.61ms\nstep:3907/5000 train_loss:8.0082 train_time:707720ms step_avg:181.61ms\nstep:3908/5000 train_loss:8.0665 train_time:707902ms step_avg:181.61ms\nstep:3909/5000 train_loss:8.0705 train_time:708083ms step_avg:181.61ms\nstep:3910/5000 train_loss:8.2278 train_time:708263ms step_avg:181.61ms\nstep:3911/5000 train_loss:8.2371 train_time:708444ms step_avg:181.61ms\nstep:3912/5000 train_loss:8.1871 train_time:708625ms step_avg:181.61ms\nstep:3913/5000 train_loss:8.1327 train_time:708807ms step_avg:181.61ms\nstep:3914/5000 train_loss:8.0144 train_time:708988ms step_avg:181.61ms\nstep:3915/5000 train_loss:8.1279 train_time:709170ms step_avg:181.61ms\nstep:3916/5000 train_loss:8.1907 train_time:709352ms step_avg:181.61ms\nstep:3917/5000 train_loss:8.0212 train_time:709533ms step_avg:181.61ms\nstep:3918/5000 train_loss:8.1646 train_time:709720ms step_avg:181.61ms\nstep:3919/5000 train_loss:8.0873 train_time:709900ms step_avg:181.61ms\nstep:3920/5000 train_loss:8.1796 train_time:710081ms step_avg:181.61ms\nstep:3921/5000 train_loss:8.1551 train_time:710262ms step_avg:181.61ms\nstep:3922/5000 train_loss:8.0298 train_time:710444ms step_avg:181.61ms\nstep:3923/5000 train_loss:8.0314 train_time:710625ms step_avg:181.61ms\nstep:3924/5000 train_loss:8.3179 train_time:710807ms step_avg:181.61ms\nstep:3925/5000 train_loss:8.0729 train_time:710987ms step_avg:181.61ms\nstep:3926/5000 train_loss:7.9347 train_time:711169ms step_avg:181.61ms\nstep:3927/5000 train_loss:8.0176 train_time:711351ms step_avg:181.61ms\nstep:3928/5000 train_loss:8.1179 train_time:711534ms step_avg:181.61ms\nstep:3929/5000 train_loss:8.1686 train_time:711718ms step_avg:181.61ms\nstep:3930/5000 train_loss:8.0169 train_time:711899ms step_avg:181.61ms\nstep:3931/5000 train_loss:8.1339 train_time:712080ms step_avg:181.61ms\nstep:3932/5000 train_loss:8.1108 train_time:712262ms step_avg:181.61ms\nstep:3933/5000 train_loss:8.1884 train_time:712445ms step_avg:181.61ms\nstep:3934/5000 train_loss:8.1651 train_time:712626ms step_avg:181.61ms\nstep:3935/5000 train_loss:8.0853 train_time:712808ms step_avg:181.61ms\nstep:3936/5000 train_loss:8.1789 train_time:712989ms step_avg:181.61ms\nstep:3937/5000 train_loss:8.1327 train_time:713171ms step_avg:181.61ms\nstep:3938/5000 train_loss:8.1630 train_time:713353ms step_avg:181.61ms\nstep:3939/5000 train_loss:8.0435 train_time:713534ms step_avg:181.61ms\nstep:3940/5000 train_loss:8.3468 train_time:713717ms step_avg:181.61ms\nstep:3941/5000 train_loss:8.1035 train_time:713900ms step_avg:181.61ms\nstep:3942/5000 train_loss:8.1702 train_time:714081ms step_avg:181.61ms\nstep:3943/5000 train_loss:8.1143 train_time:714266ms step_avg:181.61ms\nstep:3944/5000 train_loss:8.0614 train_time:714445ms step_avg:181.61ms\nstep:3945/5000 train_loss:8.1005 train_time:714625ms step_avg:181.61ms\nstep:3946/5000 train_loss:8.1017 train_time:714807ms step_avg:181.61ms\nstep:3947/5000 train_loss:7.9691 train_time:714989ms step_avg:181.61ms\nstep:3948/5000 train_loss:8.1047 train_time:715171ms step_avg:181.61ms\nstep:3949/5000 train_loss:8.1831 train_time:715354ms step_avg:181.61ms\nstep:3950/5000 train_loss:8.0587 train_time:715535ms step_avg:181.61ms\nstep:3951/5000 train_loss:8.4498 train_time:715718ms step_avg:181.61ms\nstep:3952/5000 train_loss:8.1084 train_time:715900ms step_avg:181.61ms\nstep:3953/5000 train_loss:8.1332 train_time:716082ms step_avg:181.61ms\nstep:3954/5000 train_loss:8.2442 train_time:716263ms step_avg:181.61ms\nstep:3955/5000 train_loss:8.0925 train_time:716445ms step_avg:181.61ms\nstep:3956/5000 train_loss:8.1081 train_time:716626ms step_avg:181.61ms\nstep:3957/5000 train_loss:8.1579 train_time:716808ms step_avg:181.61ms\nstep:3958/5000 train_loss:7.9804 train_time:716989ms step_avg:181.61ms\nstep:3959/5000 train_loss:8.0980 train_time:717170ms step_avg:181.61ms\nstep:3960/5000 train_loss:8.1803 train_time:717353ms step_avg:181.61ms\nstep:3961/5000 train_loss:8.1584 train_time:717535ms step_avg:181.61ms\nstep:3962/5000 train_loss:7.9942 train_time:717719ms step_avg:181.61ms\nstep:3963/5000 train_loss:7.9785 train_time:717902ms step_avg:181.61ms\nstep:3964/5000 train_loss:8.1383 train_time:718082ms step_avg:181.61ms\nstep:3965/5000 train_loss:8.1882 train_time:718262ms step_avg:181.61ms\nstep:3966/5000 train_loss:8.0506 train_time:718444ms step_avg:181.61ms\nstep:3967/5000 train_loss:8.1300 train_time:718625ms step_avg:181.61ms\nstep:3968/5000 train_loss:7.9626 train_time:718805ms step_avg:181.61ms\nstep:3969/5000 train_loss:8.0518 train_time:718986ms step_avg:181.61ms\nstep:3970/5000 train_loss:8.1392 train_time:719167ms step_avg:181.61ms\nstep:3971/5000 train_loss:8.1300 train_time:719349ms step_avg:181.61ms\nstep:3972/5000 train_loss:8.0890 train_time:719530ms step_avg:181.61ms\nstep:3973/5000 train_loss:7.9900 train_time:719713ms step_avg:181.61ms\nstep:3974/5000 train_loss:8.1395 train_time:719896ms step_avg:181.61ms\nstep:3975/5000 train_loss:8.0879 train_time:720078ms step_avg:181.61ms\nstep:3976/5000 train_loss:7.9940 train_time:720261ms step_avg:181.61ms\nstep:3977/5000 train_loss:8.1118 train_time:720441ms step_avg:181.61ms\nstep:3978/5000 train_loss:7.8180 train_time:720622ms step_avg:181.61ms\nstep:3979/5000 train_loss:7.9982 train_time:720804ms step_avg:181.61ms\nstep:3980/5000 train_loss:8.1960 train_time:720985ms step_avg:181.61ms\nstep:3981/5000 train_loss:8.0573 train_time:721165ms step_avg:181.61ms\nstep:3982/5000 train_loss:8.1330 train_time:721347ms step_avg:181.61ms\nstep:3983/5000 train_loss:8.1672 train_time:721528ms step_avg:181.61ms\nstep:3984/5000 train_loss:8.0471 train_time:721710ms step_avg:181.61ms\nstep:3985/5000 train_loss:8.1828 train_time:721895ms step_avg:181.61ms\nstep:3986/5000 train_loss:8.1415 train_time:722076ms step_avg:181.61ms\nstep:3987/5000 train_loss:8.0880 train_time:722258ms step_avg:181.61ms\nstep:3988/5000 train_loss:8.1269 train_time:722439ms step_avg:181.61ms\nstep:3989/5000 train_loss:7.9726 train_time:722621ms step_avg:181.61ms\nstep:3990/5000 train_loss:8.0487 train_time:722803ms step_avg:181.61ms\nstep:3991/5000 train_loss:8.1701 train_time:722984ms step_avg:181.61ms\nstep:3992/5000 train_loss:8.0417 train_time:723165ms step_avg:181.61ms\nstep:3993/5000 train_loss:8.1209 train_time:723345ms step_avg:181.61ms\nstep:3994/5000 train_loss:8.1729 train_time:723527ms step_avg:181.61ms\nstep:3995/5000 train_loss:8.0669 train_time:723709ms step_avg:181.61ms\nstep:3996/5000 train_loss:8.3513 train_time:723891ms step_avg:181.61ms\nstep:3997/5000 train_loss:8.0510 train_time:724074ms step_avg:181.61ms\nstep:3998/5000 train_loss:8.0750 train_time:724255ms step_avg:181.61ms\nstep:3999/5000 train_loss:8.1220 train_time:724437ms step_avg:181.61ms\nstep:4000/5000 train_loss:8.0519 train_time:724619ms step_avg:181.61ms\nstep:4001/5000 train_loss:8.1672 train_time:724801ms step_avg:181.61ms\nstep:4002/5000 train_loss:8.1609 train_time:724982ms step_avg:181.61ms\nstep:4003/5000 train_loss:8.0499 train_time:725163ms step_avg:181.61ms\nstep:4004/5000 train_loss:8.1355 train_time:725345ms step_avg:181.61ms\nstep:4005/5000 train_loss:8.1419 train_time:725526ms step_avg:181.61ms\nstep:4006/5000 train_loss:7.9908 train_time:725709ms step_avg:181.61ms\nstep:4007/5000 train_loss:8.2370 train_time:725890ms step_avg:181.61ms\nstep:4008/5000 train_loss:8.0873 train_time:726071ms step_avg:181.61ms\nstep:4009/5000 train_loss:8.2301 train_time:726254ms step_avg:181.61ms\nstep:4010/5000 train_loss:8.1572 train_time:726435ms step_avg:181.61ms\nstep:4011/5000 train_loss:8.1589 train_time:726618ms step_avg:181.61ms\nstep:4012/5000 train_loss:8.1568 train_time:726801ms step_avg:181.61ms\nstep:4013/5000 train_loss:8.1185 train_time:726983ms step_avg:181.61ms\nstep:4014/5000 train_loss:8.1553 train_time:727164ms step_avg:181.61ms\nstep:4015/5000 train_loss:8.0713 train_time:727344ms step_avg:181.61ms\nstep:4016/5000 train_loss:8.1199 train_time:727526ms step_avg:181.61ms\nstep:4017/5000 train_loss:8.0691 train_time:727708ms step_avg:181.61ms\nstep:4018/5000 train_loss:8.1192 train_time:727889ms step_avg:181.61ms\nstep:4019/5000 train_loss:8.1987 train_time:728070ms step_avg:181.61ms\nstep:4020/5000 train_loss:8.1250 train_time:728254ms step_avg:181.61ms\nstep:4021/5000 train_loss:8.1081 train_time:728435ms step_avg:181.61ms\nstep:4022/5000 train_loss:8.1217 train_time:728619ms step_avg:181.61ms\nstep:4023/5000 train_loss:8.1554 train_time:728800ms step_avg:181.61ms\nstep:4024/5000 train_loss:8.2116 train_time:728981ms step_avg:181.61ms\nstep:4025/5000 train_loss:8.1499 train_time:729163ms step_avg:181.61ms\nstep:4026/5000 train_loss:8.0329 train_time:729344ms step_avg:181.61ms\nstep:4027/5000 train_loss:8.3966 train_time:729525ms step_avg:181.61ms\nstep:4028/5000 train_loss:8.0411 train_time:729706ms step_avg:181.61ms\nstep:4029/5000 train_loss:8.1441 train_time:729887ms step_avg:181.61ms\nstep:4030/5000 train_loss:8.0723 train_time:730068ms step_avg:181.61ms\nstep:4031/5000 train_loss:8.1465 train_time:730250ms step_avg:181.61ms\nstep:4032/5000 train_loss:8.0727 train_time:730433ms step_avg:181.61ms\nstep:4033/5000 train_loss:8.0876 train_time:730617ms step_avg:181.61ms\nstep:4034/5000 train_loss:8.1286 train_time:730799ms step_avg:181.61ms\nstep:4035/5000 train_loss:8.0903 train_time:730980ms step_avg:181.61ms\nstep:4036/5000 train_loss:8.0456 train_time:731163ms step_avg:181.61ms\nstep:4037/5000 train_loss:8.2751 train_time:731344ms step_avg:181.61ms\nstep:4038/5000 train_loss:8.0532 train_time:731526ms step_avg:181.61ms\nstep:4039/5000 train_loss:8.2086 train_time:731708ms step_avg:181.61ms\nstep:4040/5000 train_loss:8.0978 train_time:731890ms step_avg:181.61ms\nstep:4041/5000 train_loss:8.1821 train_time:732071ms step_avg:181.61ms\nstep:4042/5000 train_loss:8.1383 train_time:732253ms step_avg:181.61ms\nstep:4043/5000 train_loss:8.1189 train_time:732435ms step_avg:181.61ms\nstep:4044/5000 train_loss:8.1431 train_time:732619ms step_avg:181.61ms\nstep:4045/5000 train_loss:8.1834 train_time:732801ms step_avg:181.61ms\nstep:4046/5000 train_loss:8.1550 train_time:732983ms step_avg:181.61ms\nstep:4047/5000 train_loss:8.2477 train_time:733165ms step_avg:181.61ms\nstep:4048/5000 train_loss:8.0661 train_time:733345ms step_avg:181.61ms\nstep:4049/5000 train_loss:8.0867 train_time:733525ms step_avg:181.61ms\nstep:4050/5000 train_loss:8.0582 train_time:733707ms step_avg:181.61ms\nstep:4051/5000 train_loss:8.1416 train_time:733888ms step_avg:181.61ms\nstep:4052/5000 train_loss:8.0567 train_time:734071ms step_avg:181.61ms\nstep:4053/5000 train_loss:8.1617 train_time:734254ms step_avg:181.61ms\nstep:4054/5000 train_loss:7.9832 train_time:734436ms step_avg:181.61ms\nstep:4055/5000 train_loss:8.2127 train_time:734619ms step_avg:181.61ms\nstep:4056/5000 train_loss:7.9998 train_time:734800ms step_avg:181.61ms\nstep:4057/5000 train_loss:7.9772 train_time:734981ms step_avg:181.61ms\nstep:4058/5000 train_loss:8.1556 train_time:735162ms step_avg:181.61ms\nstep:4059/5000 train_loss:8.1078 train_time:735346ms step_avg:181.61ms\nstep:4060/5000 train_loss:8.1892 train_time:735525ms step_avg:181.61ms\nstep:4061/5000 train_loss:8.2514 train_time:735706ms step_avg:181.61ms\nstep:4062/5000 train_loss:8.1095 train_time:735888ms step_avg:181.61ms\nstep:4063/5000 train_loss:8.1483 train_time:736070ms step_avg:181.61ms\nstep:4064/5000 train_loss:8.2269 train_time:736252ms step_avg:181.61ms\nstep:4065/5000 train_loss:7.9993 train_time:736434ms step_avg:181.61ms\nstep:4066/5000 train_loss:8.1714 train_time:736617ms step_avg:181.61ms\nstep:4067/5000 train_loss:8.1303 train_time:736799ms step_avg:181.61ms\nstep:4068/5000 train_loss:8.0755 train_time:736980ms step_avg:181.61ms\nstep:4069/5000 train_loss:8.2538 train_time:737161ms step_avg:181.61ms\nstep:4070/5000 train_loss:8.1097 train_time:737343ms step_avg:181.61ms\nstep:4071/5000 train_loss:8.0846 train_time:737524ms step_avg:181.61ms\nstep:4072/5000 train_loss:8.1644 train_time:737705ms step_avg:181.61ms\nstep:4073/5000 train_loss:7.9694 train_time:737885ms step_avg:181.61ms\nstep:4074/5000 train_loss:8.1717 train_time:738065ms step_avg:181.61ms\nstep:4075/5000 train_loss:8.1177 train_time:738246ms step_avg:181.61ms\nstep:4076/5000 train_loss:8.2754 train_time:738428ms step_avg:181.61ms\nstep:4077/5000 train_loss:8.2377 train_time:738611ms step_avg:181.61ms\nstep:4078/5000 train_loss:8.0626 train_time:738795ms step_avg:181.61ms\nstep:4079/5000 train_loss:8.1352 train_time:738977ms step_avg:181.61ms\nstep:4080/5000 train_loss:8.0990 train_time:739158ms step_avg:181.61ms\nstep:4081/5000 train_loss:8.0969 train_time:739340ms step_avg:181.61ms\nstep:4082/5000 train_loss:8.1120 train_time:739522ms step_avg:181.61ms\nstep:4083/5000 train_loss:8.0746 train_time:739707ms step_avg:181.61ms\nstep:4084/5000 train_loss:8.1164 train_time:739885ms step_avg:181.61ms\nstep:4085/5000 train_loss:8.1914 train_time:740066ms step_avg:181.61ms\nstep:4086/5000 train_loss:8.0834 train_time:740248ms step_avg:181.61ms\nstep:4087/5000 train_loss:8.0971 train_time:740429ms step_avg:181.61ms\nstep:4088/5000 train_loss:8.1244 train_time:740611ms step_avg:181.61ms\nstep:4089/5000 train_loss:8.2007 train_time:740794ms step_avg:181.61ms\nstep:4090/5000 train_loss:8.1202 train_time:740976ms step_avg:181.61ms\nstep:4091/5000 train_loss:8.0980 train_time:741158ms step_avg:181.61ms\nstep:4092/5000 train_loss:8.1674 train_time:741340ms step_avg:181.61ms\nstep:4093/5000 train_loss:8.0311 train_time:741522ms step_avg:181.61ms\nstep:4094/5000 train_loss:8.1113 train_time:741704ms step_avg:181.61ms\nstep:4095/5000 train_loss:8.2184 train_time:741884ms step_avg:181.61ms\nstep:4096/5000 train_loss:8.1225 train_time:742064ms step_avg:181.61ms\nstep:4097/5000 train_loss:8.1184 train_time:742244ms step_avg:181.61ms\nstep:4098/5000 train_loss:7.9928 train_time:742424ms step_avg:181.61ms\nstep:4099/5000 train_loss:8.1986 train_time:742605ms step_avg:181.61ms\nstep:4100/5000 train_loss:8.1016 train_time:742786ms step_avg:181.61ms\nstep:4101/5000 train_loss:8.0547 train_time:742967ms step_avg:181.61ms\nstep:4102/5000 train_loss:8.0813 train_time:743149ms step_avg:181.61ms\nstep:4103/5000 train_loss:8.0939 train_time:743330ms step_avg:181.61ms\nstep:4104/5000 train_loss:8.0447 train_time:743512ms step_avg:181.61ms\nstep:4105/5000 train_loss:8.1369 train_time:743695ms step_avg:181.61ms\nstep:4106/5000 train_loss:8.1125 train_time:743878ms step_avg:181.61ms\nstep:4107/5000 train_loss:8.0726 train_time:744061ms step_avg:181.61ms\nstep:4108/5000 train_loss:8.2348 train_time:744243ms step_avg:181.61ms\nstep:4109/5000 train_loss:8.1082 train_time:744424ms step_avg:181.61ms\nstep:4110/5000 train_loss:8.1186 train_time:744605ms step_avg:181.61ms\nstep:4111/5000 train_loss:8.1179 train_time:744785ms step_avg:181.61ms\nstep:4112/5000 train_loss:8.3275 train_time:744965ms step_avg:181.61ms\nstep:4113/5000 train_loss:8.0147 train_time:745146ms step_avg:181.61ms\nstep:4114/5000 train_loss:8.0450 train_time:745328ms step_avg:181.61ms\nstep:4115/5000 train_loss:8.1260 train_time:745511ms step_avg:181.61ms\nstep:4116/5000 train_loss:8.1318 train_time:745693ms step_avg:181.61ms\nstep:4117/5000 train_loss:7.9725 train_time:745875ms step_avg:181.61ms\nstep:4118/5000 train_loss:8.1838 train_time:746058ms step_avg:181.61ms\nstep:4119/5000 train_loss:8.1466 train_time:746240ms step_avg:181.61ms\nstep:4120/5000 train_loss:8.0425 train_time:746423ms step_avg:181.61ms\nstep:4121/5000 train_loss:8.1677 train_time:746606ms step_avg:181.61ms\nstep:4122/5000 train_loss:8.0194 train_time:746786ms step_avg:181.61ms\nstep:4123/5000 train_loss:8.0607 train_time:746966ms step_avg:181.61ms\nstep:4124/5000 train_loss:8.1352 train_time:747147ms step_avg:181.61ms\nstep:4125/5000 train_loss:8.1281 train_time:747327ms step_avg:181.61ms\nstep:4126/5000 train_loss:8.2064 train_time:747509ms step_avg:181.61ms\nstep:4127/5000 train_loss:7.9174 train_time:747690ms step_avg:181.61ms\nstep:4128/5000 train_loss:8.0020 train_time:747873ms step_avg:181.61ms\nstep:4129/5000 train_loss:8.1094 train_time:748056ms step_avg:181.61ms\nstep:4130/5000 train_loss:8.1368 train_time:748239ms step_avg:181.61ms\nstep:4131/5000 train_loss:8.0954 train_time:748420ms step_avg:181.61ms\nstep:4132/5000 train_loss:8.2128 train_time:748602ms step_avg:181.61ms\nstep:4133/5000 train_loss:8.0100 train_time:748783ms step_avg:181.61ms\nstep:4134/5000 train_loss:8.0815 train_time:748964ms step_avg:181.61ms\nstep:4135/5000 train_loss:8.1875 train_time:749146ms step_avg:181.61ms\nstep:4136/5000 train_loss:8.0529 train_time:749326ms step_avg:181.61ms\nstep:4137/5000 train_loss:8.1238 train_time:749509ms step_avg:181.61ms\nstep:4138/5000 train_loss:8.2069 train_time:749690ms step_avg:181.61ms\nstep:4139/5000 train_loss:8.1431 train_time:749873ms step_avg:181.61ms\nstep:4140/5000 train_loss:8.0848 train_time:750056ms step_avg:181.61ms\nstep:4141/5000 train_loss:8.1628 train_time:750238ms step_avg:181.61ms\nstep:4142/5000 train_loss:8.2170 train_time:750421ms step_avg:181.61ms\nstep:4143/5000 train_loss:8.1191 train_time:750603ms step_avg:181.61ms\nstep:4144/5000 train_loss:8.0881 train_time:750784ms step_avg:181.61ms\nstep:4145/5000 train_loss:8.0828 train_time:750965ms step_avg:181.61ms\nstep:4146/5000 train_loss:8.0551 train_time:751146ms step_avg:181.61ms\nstep:4147/5000 train_loss:8.0897 train_time:751326ms step_avg:181.61ms\nstep:4148/5000 train_loss:8.0000 train_time:751507ms step_avg:181.61ms\nstep:4149/5000 train_loss:8.0199 train_time:751689ms step_avg:181.61ms\nstep:4150/5000 train_loss:8.0281 train_time:751871ms step_avg:181.61ms\nstep:4151/5000 train_loss:8.1308 train_time:752054ms step_avg:181.61ms\nstep:4152/5000 train_loss:8.1473 train_time:752236ms step_avg:181.61ms\nstep:4153/5000 train_loss:8.0864 train_time:752419ms step_avg:181.61ms\nstep:4154/5000 train_loss:8.1082 train_time:752600ms step_avg:181.61ms\nstep:4155/5000 train_loss:8.2330 train_time:752782ms step_avg:181.61ms\nstep:4156/5000 train_loss:8.2229 train_time:752962ms step_avg:181.61ms\nstep:4157/5000 train_loss:8.1302 train_time:753145ms step_avg:181.61ms\nstep:4158/5000 train_loss:8.0400 train_time:753325ms step_avg:181.61ms\nstep:4159/5000 train_loss:8.0785 train_time:753506ms step_avg:181.61ms\nstep:4160/5000 train_loss:8.1154 train_time:753687ms step_avg:181.61ms\nstep:4161/5000 train_loss:8.0402 train_time:753868ms step_avg:181.61ms\nstep:4162/5000 train_loss:8.0313 train_time:754051ms step_avg:181.61ms\nstep:4163/5000 train_loss:8.1112 train_time:754233ms step_avg:181.61ms\nstep:4164/5000 train_loss:8.0977 train_time:754415ms step_avg:181.61ms\nstep:4165/5000 train_loss:7.9955 train_time:754597ms step_avg:181.61ms\nstep:4166/5000 train_loss:8.1413 train_time:754778ms step_avg:181.61ms\nstep:4167/5000 train_loss:8.0329 train_time:754960ms step_avg:181.61ms\nstep:4168/5000 train_loss:8.1597 train_time:755142ms step_avg:181.61ms\nstep:4169/5000 train_loss:8.0971 train_time:755324ms step_avg:181.61ms\nstep:4170/5000 train_loss:8.2971 train_time:755507ms step_avg:181.61ms\nstep:4171/5000 train_loss:8.1186 train_time:755688ms step_avg:181.61ms\nstep:4172/5000 train_loss:7.9706 train_time:755869ms step_avg:181.61ms\nstep:4173/5000 train_loss:8.2527 train_time:756050ms step_avg:181.61ms\nstep:4174/5000 train_loss:8.0900 train_time:756232ms step_avg:181.61ms\nstep:4175/5000 train_loss:8.1381 train_time:756415ms step_avg:181.61ms\nstep:4176/5000 train_loss:8.1593 train_time:756598ms step_avg:181.61ms\nstep:4177/5000 train_loss:8.1875 train_time:756780ms step_avg:181.61ms\nstep:4178/5000 train_loss:8.1518 train_time:756961ms step_avg:181.61ms\nstep:4179/5000 train_loss:8.2047 train_time:757142ms step_avg:181.61ms\nstep:4180/5000 train_loss:8.0572 train_time:757324ms step_avg:181.61ms\nstep:4181/5000 train_loss:8.0912 train_time:757505ms step_avg:181.61ms\nstep:4182/5000 train_loss:8.1141 train_time:757686ms step_avg:181.61ms\nstep:4183/5000 train_loss:8.0001 train_time:757867ms step_avg:181.61ms\nstep:4184/5000 train_loss:8.0264 train_time:758048ms step_avg:181.61ms\nstep:4185/5000 train_loss:8.0135 train_time:758229ms step_avg:181.61ms\nstep:4186/5000 train_loss:8.0149 train_time:758411ms step_avg:181.61ms\nstep:4187/5000 train_loss:8.0886 train_time:758594ms step_avg:181.61ms\nstep:4188/5000 train_loss:8.1705 train_time:758777ms step_avg:181.61ms\nstep:4189/5000 train_loss:8.1370 train_time:758959ms step_avg:181.61ms\nstep:4190/5000 train_loss:8.2222 train_time:759141ms step_avg:181.61ms\nstep:4191/5000 train_loss:8.2436 train_time:759323ms step_avg:181.61ms\nstep:4192/5000 train_loss:8.1256 train_time:759504ms step_avg:181.61ms\nstep:4193/5000 train_loss:8.1199 train_time:759686ms step_avg:181.61ms\nstep:4194/5000 train_loss:7.9681 train_time:759867ms step_avg:181.61ms\nstep:4195/5000 train_loss:8.1207 train_time:760048ms step_avg:181.61ms\nstep:4196/5000 train_loss:8.3275 train_time:760230ms step_avg:181.61ms\nstep:4197/5000 train_loss:8.1317 train_time:760412ms step_avg:181.61ms\nstep:4198/5000 train_loss:8.2839 train_time:760595ms step_avg:181.61ms\nstep:4199/5000 train_loss:8.1021 train_time:760778ms step_avg:181.61ms\nstep:4200/5000 train_loss:8.0393 train_time:760959ms step_avg:181.61ms\nstep:4201/5000 train_loss:8.0594 train_time:761140ms step_avg:181.61ms\nstep:4202/5000 train_loss:8.3146 train_time:761321ms step_avg:181.61ms\nstep:4203/5000 train_loss:8.0941 train_time:761503ms step_avg:181.61ms\nstep:4204/5000 train_loss:8.1336 train_time:761683ms step_avg:181.61ms\nstep:4205/5000 train_loss:8.1985 train_time:761865ms step_avg:181.61ms\nstep:4206/5000 train_loss:8.0784 train_time:762047ms step_avg:181.61ms\nstep:4207/5000 train_loss:8.0870 train_time:762228ms step_avg:181.61ms\nstep:4208/5000 train_loss:8.0564 train_time:762410ms step_avg:181.61ms\nstep:4209/5000 train_loss:8.1604 train_time:762593ms step_avg:181.61ms\nstep:4210/5000 train_loss:7.9711 train_time:762776ms step_avg:181.61ms\nstep:4211/5000 train_loss:8.0881 train_time:762959ms step_avg:181.61ms\nstep:4212/5000 train_loss:8.1221 train_time:763140ms step_avg:181.61ms\nstep:4213/5000 train_loss:8.0337 train_time:763323ms step_avg:181.61ms\nstep:4214/5000 train_loss:8.0338 train_time:763505ms step_avg:181.61ms\nstep:4215/5000 train_loss:8.2083 train_time:763685ms step_avg:181.61ms\nstep:4216/5000 train_loss:8.1683 train_time:763866ms step_avg:181.61ms\nstep:4217/5000 train_loss:8.0981 train_time:764047ms step_avg:181.61ms\nstep:4218/5000 train_loss:8.1667 train_time:764228ms step_avg:181.61ms\nstep:4219/5000 train_loss:8.1460 train_time:764410ms step_avg:181.61ms\nstep:4220/5000 train_loss:8.2090 train_time:764592ms step_avg:181.61ms\nstep:4221/5000 train_loss:8.0563 train_time:764774ms step_avg:181.61ms\nstep:4222/5000 train_loss:8.0718 train_time:764957ms step_avg:181.61ms\nstep:4223/5000 train_loss:8.1261 train_time:765140ms step_avg:181.61ms\nstep:4224/5000 train_loss:8.0709 train_time:765321ms step_avg:181.61ms\nstep:4225/5000 train_loss:8.2674 train_time:765502ms step_avg:181.61ms\nstep:4226/5000 train_loss:8.0848 train_time:765683ms step_avg:181.61ms\nstep:4227/5000 train_loss:8.0466 train_time:765863ms step_avg:181.61ms\nstep:4228/5000 train_loss:8.1277 train_time:766045ms step_avg:181.61ms\nstep:4229/5000 train_loss:8.0959 train_time:766226ms step_avg:181.61ms\nstep:4230/5000 train_loss:8.0758 train_time:766408ms step_avg:181.61ms\nstep:4231/5000 train_loss:8.0699 train_time:766589ms step_avg:181.61ms\nstep:4232/5000 train_loss:8.2106 train_time:766771ms step_avg:181.61ms\nstep:4233/5000 train_loss:8.0403 train_time:766954ms step_avg:181.61ms\nstep:4234/5000 train_loss:8.2066 train_time:767136ms step_avg:181.61ms\nstep:4235/5000 train_loss:8.0507 train_time:767319ms step_avg:181.61ms\nstep:4236/5000 train_loss:8.0794 train_time:767500ms step_avg:181.61ms\nstep:4237/5000 train_loss:8.0853 train_time:767680ms step_avg:181.61ms\nstep:4238/5000 train_loss:8.1201 train_time:767861ms step_avg:181.61ms\nstep:4239/5000 train_loss:8.0267 train_time:768043ms step_avg:181.61ms\nstep:4240/5000 train_loss:8.0639 train_time:768224ms step_avg:181.61ms\nstep:4241/5000 train_loss:8.1562 train_time:768406ms step_avg:181.61ms\nstep:4242/5000 train_loss:8.0411 train_time:768586ms step_avg:181.61ms\nstep:4243/5000 train_loss:8.2401 train_time:768766ms step_avg:181.61ms\nstep:4244/5000 train_loss:8.2699 train_time:768947ms step_avg:181.61ms\nstep:4245/5000 train_loss:8.1273 train_time:769127ms step_avg:181.61ms\nstep:4246/5000 train_loss:8.2292 train_time:769310ms step_avg:181.61ms\nstep:4247/5000 train_loss:8.1353 train_time:769493ms step_avg:181.61ms\nstep:4248/5000 train_loss:8.1530 train_time:769675ms step_avg:181.61ms\nstep:4249/5000 train_loss:8.1306 train_time:769857ms step_avg:181.61ms\nstep:4250/5000 train_loss:8.1534 train_time:770040ms step_avg:181.61ms\nstep:4251/5000 train_loss:7.9415 train_time:770222ms step_avg:181.61ms\nstep:4252/5000 train_loss:8.1496 train_time:770403ms step_avg:181.61ms\nstep:4253/5000 train_loss:8.0515 train_time:770584ms step_avg:181.61ms\nstep:4254/5000 train_loss:8.1145 train_time:770765ms step_avg:181.61ms\nstep:4255/5000 train_loss:8.2271 train_time:770946ms step_avg:181.61ms\nstep:4256/5000 train_loss:8.0817 train_time:771127ms step_avg:181.61ms\nstep:4257/5000 train_loss:8.0332 train_time:771309ms step_avg:181.61ms\nstep:4258/5000 train_loss:8.1342 train_time:771490ms step_avg:181.61ms\nstep:4259/5000 train_loss:8.1747 train_time:771672ms step_avg:181.61ms\nstep:4260/5000 train_loss:8.1840 train_time:771854ms step_avg:181.61ms\nstep:4261/5000 train_loss:8.1766 train_time:772037ms step_avg:181.61ms\nstep:4262/5000 train_loss:8.1569 train_time:772219ms step_avg:181.61ms\nstep:4263/5000 train_loss:8.0434 train_time:772401ms step_avg:181.61ms\nstep:4264/5000 train_loss:8.1203 train_time:772582ms step_avg:181.61ms\nstep:4265/5000 train_loss:8.0875 train_time:772762ms step_avg:181.61ms\nstep:4266/5000 train_loss:8.1315 train_time:772944ms step_avg:181.61ms\nstep:4267/5000 train_loss:8.1003 train_time:773124ms step_avg:181.61ms\nstep:4268/5000 train_loss:8.0376 train_time:773305ms step_avg:181.61ms\nstep:4269/5000 train_loss:8.1332 train_time:773485ms step_avg:181.61ms\nstep:4270/5000 train_loss:8.4105 train_time:773666ms step_avg:181.61ms\nstep:4271/5000 train_loss:8.1961 train_time:773847ms step_avg:181.61ms\nstep:4272/5000 train_loss:7.9775 train_time:774027ms step_avg:181.61ms\nstep:4273/5000 train_loss:8.1961 train_time:774211ms step_avg:181.61ms\nstep:4274/5000 train_loss:8.0809 train_time:774394ms step_avg:181.61ms\nstep:4275/5000 train_loss:8.1173 train_time:774576ms step_avg:181.61ms\nstep:4276/5000 train_loss:8.1183 train_time:774757ms step_avg:181.61ms\nstep:4277/5000 train_loss:8.0525 train_time:774939ms step_avg:181.61ms\nstep:4278/5000 train_loss:8.0190 train_time:775120ms step_avg:181.61ms\nstep:4279/5000 train_loss:8.1215 train_time:775301ms step_avg:181.61ms\nstep:4280/5000 train_loss:8.0321 train_time:775482ms step_avg:181.61ms\nstep:4281/5000 train_loss:8.1324 train_time:775663ms step_avg:181.61ms\nstep:4282/5000 train_loss:8.1698 train_time:775843ms step_avg:181.61ms\nstep:4283/5000 train_loss:8.2147 train_time:776024ms step_avg:181.61ms\nstep:4284/5000 train_loss:7.9726 train_time:776204ms step_avg:181.61ms\nstep:4285/5000 train_loss:8.1596 train_time:776384ms step_avg:181.61ms\nstep:4286/5000 train_loss:8.0784 train_time:776566ms step_avg:181.61ms\nstep:4287/5000 train_loss:8.1573 train_time:776749ms step_avg:181.61ms\nstep:4288/5000 train_loss:8.0365 train_time:776929ms step_avg:181.61ms\nstep:4289/5000 train_loss:8.1512 train_time:777113ms step_avg:181.61ms\nstep:4290/5000 train_loss:8.0517 train_time:777295ms step_avg:181.61ms\nstep:4291/5000 train_loss:8.0278 train_time:777477ms step_avg:181.61ms\nstep:4292/5000 train_loss:8.0557 train_time:777659ms step_avg:181.61ms\nstep:4293/5000 train_loss:8.0480 train_time:777840ms step_avg:181.61ms\nstep:4294/5000 train_loss:8.0479 train_time:778022ms step_avg:181.61ms\nstep:4295/5000 train_loss:8.0402 train_time:778204ms step_avg:181.61ms\nstep:4296/5000 train_loss:8.1369 train_time:778384ms step_avg:181.61ms\nstep:4297/5000 train_loss:8.1248 train_time:778565ms step_avg:181.61ms\nstep:4298/5000 train_loss:8.0871 train_time:778745ms step_avg:181.61ms\nstep:4299/5000 train_loss:8.1418 train_time:778927ms step_avg:181.61ms\nstep:4300/5000 train_loss:8.1132 train_time:779108ms step_avg:181.61ms\nstep:4301/5000 train_loss:8.1989 train_time:779289ms step_avg:181.61ms\nstep:4302/5000 train_loss:8.1760 train_time:779472ms step_avg:181.61ms\nstep:4303/5000 train_loss:8.1890 train_time:779654ms step_avg:181.61ms\nstep:4304/5000 train_loss:8.1821 train_time:779837ms step_avg:181.61ms\nstep:4305/5000 train_loss:8.1395 train_time:780020ms step_avg:181.61ms\nstep:4306/5000 train_loss:8.1119 train_time:780200ms step_avg:181.61ms\nstep:4307/5000 train_loss:8.0957 train_time:780381ms step_avg:181.61ms\nstep:4308/5000 train_loss:8.0818 train_time:780561ms step_avg:181.61ms\nstep:4309/5000 train_loss:8.1711 train_time:780742ms step_avg:181.61ms\nstep:4310/5000 train_loss:8.0824 train_time:780925ms step_avg:181.61ms\nstep:4311/5000 train_loss:8.1415 train_time:781107ms step_avg:181.61ms\nstep:4312/5000 train_loss:8.1833 train_time:781288ms step_avg:181.61ms\nstep:4313/5000 train_loss:8.1196 train_time:781469ms step_avg:181.61ms\nstep:4314/5000 train_loss:8.0795 train_time:781650ms step_avg:181.61ms\nstep:4315/5000 train_loss:8.1039 train_time:781832ms step_avg:181.61ms\nstep:4316/5000 train_loss:8.3177 train_time:782015ms step_avg:181.61ms\nstep:4317/5000 train_loss:8.0457 train_time:782197ms step_avg:181.61ms\nstep:4318/5000 train_loss:8.1582 train_time:782379ms step_avg:181.61ms\nstep:4319/5000 train_loss:8.0965 train_time:782560ms step_avg:181.61ms\nstep:4320/5000 train_loss:8.1833 train_time:782741ms step_avg:181.61ms\nstep:4321/5000 train_loss:8.1155 train_time:782924ms step_avg:181.61ms\nstep:4322/5000 train_loss:8.1165 train_time:783105ms step_avg:181.61ms\nstep:4323/5000 train_loss:8.1426 train_time:783286ms step_avg:181.61ms\nstep:4324/5000 train_loss:8.0645 train_time:783466ms step_avg:181.61ms\nstep:4325/5000 train_loss:8.0866 train_time:783647ms step_avg:181.61ms\nstep:4326/5000 train_loss:8.2629 train_time:783828ms step_avg:181.61ms\nstep:4327/5000 train_loss:8.1553 train_time:784011ms step_avg:181.61ms\nstep:4328/5000 train_loss:8.0849 train_time:784193ms step_avg:181.61ms\nstep:4329/5000 train_loss:8.1134 train_time:784376ms step_avg:181.61ms\nstep:4330/5000 train_loss:8.1075 train_time:784557ms step_avg:181.61ms\nstep:4331/5000 train_loss:8.1124 train_time:784739ms step_avg:181.61ms\nstep:4332/5000 train_loss:8.2450 train_time:784922ms step_avg:181.61ms\nstep:4333/5000 train_loss:8.0896 train_time:785103ms step_avg:181.61ms\nstep:4334/5000 train_loss:8.1236 train_time:785283ms step_avg:181.61ms\nstep:4335/5000 train_loss:8.1113 train_time:785464ms step_avg:181.61ms\nstep:4336/5000 train_loss:8.2497 train_time:785644ms step_avg:181.61ms\nstep:4337/5000 train_loss:7.8132 train_time:785824ms step_avg:181.61ms\nstep:4338/5000 train_loss:8.0593 train_time:786005ms step_avg:181.61ms\nstep:4339/5000 train_loss:8.2302 train_time:786185ms step_avg:181.61ms\nstep:4340/5000 train_loss:8.1410 train_time:786366ms step_avg:181.61ms\nstep:4341/5000 train_loss:8.0415 train_time:786547ms step_avg:181.61ms\nstep:4342/5000 train_loss:8.0665 train_time:786727ms step_avg:181.61ms\nstep:4343/5000 train_loss:8.0901 train_time:786909ms step_avg:181.61ms\nstep:4344/5000 train_loss:8.1320 train_time:787090ms step_avg:181.61ms\nstep:4345/5000 train_loss:8.1695 train_time:787274ms step_avg:181.61ms\nstep:4346/5000 train_loss:8.1479 train_time:787456ms step_avg:181.61ms\nstep:4347/5000 train_loss:8.0864 train_time:787639ms step_avg:181.61ms\nstep:4348/5000 train_loss:8.1903 train_time:787820ms step_avg:181.61ms\nstep:4349/5000 train_loss:8.1009 train_time:788001ms step_avg:181.61ms\nstep:4350/5000 train_loss:8.0235 train_time:788181ms step_avg:181.61ms\nstep:4351/5000 train_loss:8.0961 train_time:788363ms step_avg:181.61ms\nstep:4352/5000 train_loss:8.1845 train_time:788544ms step_avg:181.61ms\nstep:4353/5000 train_loss:8.1900 train_time:788725ms step_avg:181.61ms\nstep:4354/5000 train_loss:7.8782 train_time:788907ms step_avg:181.61ms\nstep:4355/5000 train_loss:8.1332 train_time:789088ms step_avg:181.61ms\nstep:4356/5000 train_loss:8.2863 train_time:789268ms step_avg:181.61ms\nstep:4357/5000 train_loss:8.0387 train_time:789449ms step_avg:181.61ms\nstep:4358/5000 train_loss:8.0915 train_time:789633ms step_avg:181.61ms\nstep:4359/5000 train_loss:7.9967 train_time:789815ms step_avg:181.61ms\nstep:4360/5000 train_loss:7.9739 train_time:789999ms step_avg:181.61ms\nstep:4361/5000 train_loss:7.9733 train_time:790180ms step_avg:181.61ms\nstep:4362/5000 train_loss:8.1249 train_time:790360ms step_avg:181.61ms\nstep:4363/5000 train_loss:8.1659 train_time:790542ms step_avg:181.61ms\nstep:4364/5000 train_loss:8.1369 train_time:790724ms step_avg:181.61ms\nstep:4365/5000 train_loss:8.0626 train_time:790905ms step_avg:181.61ms\nstep:4366/5000 train_loss:8.1095 train_time:791085ms step_avg:181.61ms\nstep:4367/5000 train_loss:8.1399 train_time:791266ms step_avg:181.61ms\nstep:4368/5000 train_loss:8.1945 train_time:791447ms step_avg:181.61ms\nstep:4369/5000 train_loss:8.1421 train_time:791628ms step_avg:181.61ms\nstep:4370/5000 train_loss:8.0796 train_time:791810ms step_avg:181.61ms\nstep:4371/5000 train_loss:8.1597 train_time:791993ms step_avg:181.61ms\nstep:4372/5000 train_loss:8.2129 train_time:792175ms step_avg:181.61ms\nstep:4373/5000 train_loss:8.0765 train_time:792357ms step_avg:181.61ms\nstep:4374/5000 train_loss:8.0039 train_time:792539ms step_avg:181.61ms\nstep:4375/5000 train_loss:8.0903 train_time:792721ms step_avg:181.61ms\nstep:4376/5000 train_loss:8.1422 train_time:792903ms step_avg:181.61ms\nstep:4377/5000 train_loss:8.0845 train_time:793085ms step_avg:181.61ms\nstep:4378/5000 train_loss:8.1483 train_time:793265ms step_avg:181.61ms\nstep:4379/5000 train_loss:8.0814 train_time:793445ms step_avg:181.61ms\nstep:4380/5000 train_loss:8.1721 train_time:793626ms step_avg:181.61ms\nstep:4381/5000 train_loss:8.0956 train_time:793807ms step_avg:181.61ms\nstep:4382/5000 train_loss:8.2275 train_time:793987ms step_avg:181.61ms\nstep:4383/5000 train_loss:8.1150 train_time:794168ms step_avg:181.61ms\nstep:4384/5000 train_loss:8.1513 train_time:794351ms step_avg:181.61ms\nstep:4385/5000 train_loss:8.1537 train_time:794533ms step_avg:181.61ms\nstep:4386/5000 train_loss:8.2679 train_time:794716ms step_avg:181.61ms\nstep:4387/5000 train_loss:8.1232 train_time:794898ms step_avg:181.61ms\nstep:4388/5000 train_loss:8.0997 train_time:795080ms step_avg:181.61ms\nstep:4389/5000 train_loss:8.1517 train_time:795260ms step_avg:181.61ms\nstep:4390/5000 train_loss:8.1237 train_time:795442ms step_avg:181.61ms\nstep:4391/5000 train_loss:8.0615 train_time:795624ms step_avg:181.61ms\nstep:4392/5000 train_loss:8.1745 train_time:795805ms step_avg:181.61ms\nstep:4393/5000 train_loss:8.0359 train_time:795986ms step_avg:181.61ms\nstep:4394/5000 train_loss:8.0421 train_time:796167ms step_avg:181.61ms\nstep:4395/5000 train_loss:8.0101 train_time:796348ms step_avg:181.61ms\nstep:4396/5000 train_loss:8.1786 train_time:796528ms step_avg:181.61ms\nstep:4397/5000 train_loss:8.1874 train_time:796710ms step_avg:181.61ms\nstep:4398/5000 train_loss:8.0715 train_time:796893ms step_avg:181.61ms\nstep:4399/5000 train_loss:8.0597 train_time:797075ms step_avg:181.61ms\nstep:4400/5000 train_loss:8.1185 train_time:797255ms step_avg:181.61ms\nstep:4401/5000 train_loss:8.0771 train_time:797438ms step_avg:181.61ms\nstep:4402/5000 train_loss:8.0966 train_time:797619ms step_avg:181.61ms\nstep:4403/5000 train_loss:8.1682 train_time:797802ms step_avg:181.61ms\nstep:4404/5000 train_loss:8.0737 train_time:797983ms step_avg:181.61ms\nstep:4405/5000 train_loss:8.0783 train_time:798163ms step_avg:181.61ms\nstep:4406/5000 train_loss:8.0963 train_time:798343ms step_avg:181.61ms\nstep:4407/5000 train_loss:8.0521 train_time:798523ms step_avg:181.61ms\nstep:4408/5000 train_loss:8.1882 train_time:798704ms step_avg:181.61ms\nstep:4409/5000 train_loss:8.0607 train_time:798885ms step_avg:181.61ms\nstep:4410/5000 train_loss:8.2075 train_time:799067ms step_avg:181.61ms\nstep:4411/5000 train_loss:8.0309 train_time:799249ms step_avg:181.61ms\nstep:4412/5000 train_loss:8.1098 train_time:799430ms step_avg:181.61ms\nstep:4413/5000 train_loss:8.1638 train_time:799611ms step_avg:181.61ms\nstep:4414/5000 train_loss:8.0312 train_time:799793ms step_avg:181.61ms\nstep:4415/5000 train_loss:8.2086 train_time:799976ms step_avg:181.61ms\nstep:4416/5000 train_loss:8.1881 train_time:800158ms step_avg:181.61ms\nstep:4417/5000 train_loss:8.1081 train_time:800340ms step_avg:181.61ms\nstep:4418/5000 train_loss:8.0994 train_time:800521ms step_avg:181.61ms\nstep:4419/5000 train_loss:8.1396 train_time:800703ms step_avg:181.61ms\nstep:4420/5000 train_loss:8.0298 train_time:800884ms step_avg:181.61ms\nstep:4421/5000 train_loss:8.1127 train_time:801064ms step_avg:181.61ms\nstep:4422/5000 train_loss:8.1055 train_time:801244ms step_avg:181.61ms\nstep:4423/5000 train_loss:8.1267 train_time:801425ms step_avg:181.61ms\nstep:4424/5000 train_loss:8.0477 train_time:801606ms step_avg:181.61ms\nstep:4425/5000 train_loss:8.1579 train_time:801788ms step_avg:181.61ms\nstep:4426/5000 train_loss:8.1100 train_time:801969ms step_avg:181.61ms\nstep:4427/5000 train_loss:8.2565 train_time:802151ms step_avg:181.61ms\nstep:4428/5000 train_loss:8.2553 train_time:802331ms step_avg:181.61ms\nstep:4429/5000 train_loss:8.0806 train_time:802515ms step_avg:181.61ms\nstep:4430/5000 train_loss:8.1066 train_time:802696ms step_avg:181.61ms\nstep:4431/5000 train_loss:8.1866 train_time:802877ms step_avg:181.61ms\nstep:4432/5000 train_loss:8.1144 train_time:803058ms step_avg:181.61ms\nstep:4433/5000 train_loss:8.1919 train_time:803240ms step_avg:181.61ms\nstep:4434/5000 train_loss:8.1190 train_time:803421ms step_avg:181.61ms\nstep:4435/5000 train_loss:8.1268 train_time:803602ms step_avg:181.61ms\nstep:4436/5000 train_loss:8.1504 train_time:803784ms step_avg:181.61ms\nstep:4437/5000 train_loss:8.1403 train_time:803965ms step_avg:181.60ms\nstep:4438/5000 train_loss:8.2914 train_time:804145ms step_avg:181.60ms\nstep:4439/5000 train_loss:8.1430 train_time:804325ms step_avg:181.60ms\nstep:4440/5000 train_loss:8.1316 train_time:804506ms step_avg:181.60ms\nstep:4441/5000 train_loss:8.0651 train_time:804687ms step_avg:181.60ms\nstep:4442/5000 train_loss:8.1132 train_time:804870ms step_avg:181.60ms\nstep:4443/5000 train_loss:8.1522 train_time:805052ms step_avg:181.60ms\nstep:4444/5000 train_loss:8.0715 train_time:805233ms step_avg:181.60ms\nstep:4445/5000 train_loss:8.0135 train_time:805414ms step_avg:181.60ms\nstep:4446/5000 train_loss:8.2560 train_time:805596ms step_avg:181.60ms\nstep:4447/5000 train_loss:8.2642 train_time:805778ms step_avg:181.60ms\nstep:4448/5000 train_loss:8.1934 train_time:805961ms step_avg:181.60ms\nstep:4449/5000 train_loss:8.1106 train_time:806142ms step_avg:181.60ms\nstep:4450/5000 train_loss:8.1495 train_time:806323ms step_avg:181.60ms\nstep:4451/5000 train_loss:8.1588 train_time:806504ms step_avg:181.60ms\nstep:4452/5000 train_loss:8.1212 train_time:806683ms step_avg:181.60ms\nstep:4453/5000 train_loss:8.1068 train_time:806864ms step_avg:181.60ms\nstep:4454/5000 train_loss:8.0267 train_time:807045ms step_avg:181.60ms\nstep:4455/5000 train_loss:8.0964 train_time:807226ms step_avg:181.60ms\nstep:4456/5000 train_loss:8.1115 train_time:807408ms step_avg:181.60ms\nstep:4457/5000 train_loss:8.0790 train_time:807590ms step_avg:181.60ms\nstep:4458/5000 train_loss:8.1251 train_time:807772ms step_avg:181.60ms\nstep:4459/5000 train_loss:8.0816 train_time:807954ms step_avg:181.60ms\nstep:4460/5000 train_loss:8.0773 train_time:808135ms step_avg:181.60ms\nstep:4461/5000 train_loss:7.9763 train_time:808318ms step_avg:181.60ms\nstep:4462/5000 train_loss:8.1714 train_time:808500ms step_avg:181.60ms\nstep:4463/5000 train_loss:8.1489 train_time:808680ms step_avg:181.60ms\nstep:4464/5000 train_loss:8.0425 train_time:808862ms step_avg:181.60ms\nstep:4465/5000 train_loss:8.1846 train_time:809044ms step_avg:181.60ms\nstep:4466/5000 train_loss:8.0961 train_time:809223ms step_avg:181.60ms\nstep:4467/5000 train_loss:8.2787 train_time:809405ms step_avg:181.60ms\nstep:4468/5000 train_loss:8.1954 train_time:809585ms step_avg:181.60ms\nstep:4469/5000 train_loss:8.1613 train_time:809765ms step_avg:181.60ms\nstep:4470/5000 train_loss:8.1260 train_time:809946ms step_avg:181.60ms\nstep:4471/5000 train_loss:8.0607 train_time:810128ms step_avg:181.60ms\nstep:4472/5000 train_loss:8.0936 train_time:810310ms step_avg:181.60ms\nstep:4473/5000 train_loss:8.1707 train_time:810492ms step_avg:181.60ms\nstep:4474/5000 train_loss:8.0686 train_time:810674ms step_avg:181.60ms\nstep:4475/5000 train_loss:8.1267 train_time:810856ms step_avg:181.60ms\nstep:4476/5000 train_loss:8.1932 train_time:811039ms step_avg:181.60ms\nstep:4477/5000 train_loss:8.4021 train_time:811222ms step_avg:181.60ms\nstep:4478/5000 train_loss:8.1186 train_time:811403ms step_avg:181.60ms\nstep:4479/5000 train_loss:7.7480 train_time:811583ms step_avg:181.60ms\nstep:4480/5000 train_loss:8.1409 train_time:811764ms step_avg:181.60ms\nstep:4481/5000 train_loss:8.1002 train_time:811945ms step_avg:181.60ms\nstep:4482/5000 train_loss:8.2492 train_time:812125ms step_avg:181.60ms\nstep:4483/5000 train_loss:8.1630 train_time:812308ms step_avg:181.60ms\nstep:4484/5000 train_loss:8.0401 train_time:812490ms step_avg:181.60ms\nstep:4485/5000 train_loss:8.1611 train_time:812671ms step_avg:181.60ms\nstep:4486/5000 train_loss:8.2131 train_time:812852ms step_avg:181.60ms\nstep:4487/5000 train_loss:8.0779 train_time:813035ms step_avg:181.60ms\nstep:4488/5000 train_loss:7.9774 train_time:813219ms step_avg:181.60ms\nstep:4489/5000 train_loss:7.9483 train_time:813401ms step_avg:181.60ms\nstep:4490/5000 train_loss:8.1076 train_time:813582ms step_avg:181.60ms\nstep:4491/5000 train_loss:8.1094 train_time:813763ms step_avg:181.60ms\nstep:4492/5000 train_loss:8.1223 train_time:813944ms step_avg:181.60ms\nstep:4493/5000 train_loss:8.1403 train_time:814123ms step_avg:181.60ms\nstep:4494/5000 train_loss:8.0791 train_time:814305ms step_avg:181.60ms\nstep:4495/5000 train_loss:8.0065 train_time:814486ms step_avg:181.60ms\nstep:4496/5000 train_loss:8.0589 train_time:814667ms step_avg:181.60ms\nstep:4497/5000 train_loss:8.0970 train_time:814850ms step_avg:181.60ms\nstep:4498/5000 train_loss:8.0498 train_time:815031ms step_avg:181.60ms\nstep:4499/5000 train_loss:8.1008 train_time:815213ms step_avg:181.60ms\nstep:4500/5000 train_loss:8.1079 train_time:815395ms step_avg:181.60ms\nstep:4501/5000 train_loss:8.1704 train_time:815578ms step_avg:181.60ms\nstep:4502/5000 train_loss:8.0804 train_time:815759ms step_avg:181.60ms\nstep:4503/5000 train_loss:8.1712 train_time:815941ms step_avg:181.60ms\nstep:4504/5000 train_loss:8.1478 train_time:816122ms step_avg:181.60ms\nstep:4505/5000 train_loss:8.0342 train_time:816304ms step_avg:181.60ms\nstep:4506/5000 train_loss:8.0872 train_time:816485ms step_avg:181.60ms\nstep:4507/5000 train_loss:8.1294 train_time:816665ms step_avg:181.60ms\nstep:4508/5000 train_loss:8.0997 train_time:816846ms step_avg:181.60ms\nstep:4509/5000 train_loss:8.0984 train_time:817026ms step_avg:181.60ms\nstep:4510/5000 train_loss:8.1871 train_time:817207ms step_avg:181.60ms\nstep:4511/5000 train_loss:8.0760 train_time:817389ms step_avg:181.60ms\nstep:4512/5000 train_loss:8.1560 train_time:817571ms step_avg:181.60ms\nstep:4513/5000 train_loss:7.9340 train_time:817753ms step_avg:181.60ms\nstep:4514/5000 train_loss:8.1009 train_time:817934ms step_avg:181.60ms\nstep:4515/5000 train_loss:8.0489 train_time:818118ms step_avg:181.60ms\nstep:4516/5000 train_loss:8.1739 train_time:818300ms step_avg:181.60ms\nstep:4517/5000 train_loss:8.0389 train_time:818481ms step_avg:181.60ms\nstep:4518/5000 train_loss:8.1046 train_time:818661ms step_avg:181.60ms\nstep:4519/5000 train_loss:8.0929 train_time:818842ms step_avg:181.60ms\nstep:4520/5000 train_loss:7.9912 train_time:819022ms step_avg:181.60ms\nstep:4521/5000 train_loss:7.8131 train_time:819203ms step_avg:181.60ms\nstep:4522/5000 train_loss:8.1553 train_time:819384ms step_avg:181.60ms\nstep:4523/5000 train_loss:8.1116 train_time:819565ms step_avg:181.60ms\nstep:4524/5000 train_loss:8.1397 train_time:819746ms step_avg:181.60ms\nstep:4525/5000 train_loss:8.0351 train_time:819927ms step_avg:181.60ms\nstep:4526/5000 train_loss:8.0628 train_time:820108ms step_avg:181.60ms\nstep:4527/5000 train_loss:8.1195 train_time:820289ms step_avg:181.60ms\nstep:4528/5000 train_loss:8.1510 train_time:820471ms step_avg:181.60ms\nstep:4529/5000 train_loss:8.1394 train_time:820654ms step_avg:181.60ms\nstep:4530/5000 train_loss:8.2186 train_time:820836ms step_avg:181.60ms\nstep:4531/5000 train_loss:8.0203 train_time:821018ms step_avg:181.60ms\nstep:4532/5000 train_loss:8.1118 train_time:821200ms step_avg:181.60ms\nstep:4533/5000 train_loss:8.1108 train_time:821380ms step_avg:181.60ms\nstep:4534/5000 train_loss:8.0032 train_time:821561ms step_avg:181.60ms\nstep:4535/5000 train_loss:8.3643 train_time:821743ms step_avg:181.60ms\nstep:4536/5000 train_loss:8.2001 train_time:821923ms step_avg:181.60ms\nstep:4537/5000 train_loss:8.1134 train_time:822104ms step_avg:181.60ms\nstep:4538/5000 train_loss:8.0241 train_time:822285ms step_avg:181.60ms\nstep:4539/5000 train_loss:8.0953 train_time:822465ms step_avg:181.60ms\nstep:4540/5000 train_loss:8.3594 train_time:822647ms step_avg:181.60ms\nstep:4541/5000 train_loss:8.0399 train_time:822828ms step_avg:181.60ms\nstep:4542/5000 train_loss:8.0881 train_time:823010ms step_avg:181.60ms\nstep:4543/5000 train_loss:8.0546 train_time:823192ms step_avg:181.60ms\nstep:4544/5000 train_loss:8.1567 train_time:823374ms step_avg:181.60ms\nstep:4545/5000 train_loss:8.1687 train_time:823556ms step_avg:181.60ms\nstep:4546/5000 train_loss:8.1625 train_time:823738ms step_avg:181.60ms\nstep:4547/5000 train_loss:8.1077 train_time:823920ms step_avg:181.60ms\nstep:4548/5000 train_loss:8.1366 train_time:824102ms step_avg:181.60ms\nstep:4549/5000 train_loss:8.2654 train_time:824282ms step_avg:181.60ms\nstep:4550/5000 train_loss:8.1422 train_time:824463ms step_avg:181.60ms\nstep:4551/5000 train_loss:7.9530 train_time:824643ms step_avg:181.60ms\nstep:4552/5000 train_loss:8.0195 train_time:824824ms step_avg:181.60ms\nstep:4553/5000 train_loss:8.0595 train_time:825005ms step_avg:181.60ms\nstep:4554/5000 train_loss:8.0893 train_time:825184ms step_avg:181.60ms\nstep:4555/5000 train_loss:8.0520 train_time:825365ms step_avg:181.60ms\nstep:4556/5000 train_loss:8.0778 train_time:825545ms step_avg:181.60ms\nstep:4557/5000 train_loss:8.2088 train_time:825725ms step_avg:181.60ms\nstep:4558/5000 train_loss:8.1207 train_time:825906ms step_avg:181.60ms\nstep:4559/5000 train_loss:8.1728 train_time:826087ms step_avg:181.60ms\nstep:4560/5000 train_loss:8.1654 train_time:826268ms step_avg:181.60ms\nstep:4561/5000 train_loss:8.0306 train_time:826449ms step_avg:181.60ms\nstep:4562/5000 train_loss:8.1519 train_time:826630ms step_avg:181.60ms\nstep:4563/5000 train_loss:8.0839 train_time:826814ms step_avg:181.60ms\nstep:4564/5000 train_loss:8.1260 train_time:826997ms step_avg:181.60ms\nstep:4565/5000 train_loss:8.0873 train_time:827178ms step_avg:181.60ms\nstep:4566/5000 train_loss:8.1995 train_time:827360ms step_avg:181.60ms\nstep:4567/5000 train_loss:8.3650 train_time:827541ms step_avg:181.60ms\nstep:4568/5000 train_loss:8.1039 train_time:827723ms step_avg:181.60ms\nstep:4569/5000 train_loss:8.0315 train_time:827905ms step_avg:181.60ms\nstep:4570/5000 train_loss:8.0774 train_time:828085ms step_avg:181.60ms\nstep:4571/5000 train_loss:8.0940 train_time:828264ms step_avg:181.60ms\nstep:4572/5000 train_loss:8.1890 train_time:828680ms step_avg:181.65ms\nstep:4573/5000 train_loss:8.0710 train_time:828858ms step_avg:181.65ms\nstep:4574/5000 train_loss:8.0197 train_time:829038ms step_avg:181.65ms\nstep:4575/5000 train_loss:8.1280 train_time:829219ms step_avg:181.65ms\nstep:4576/5000 train_loss:8.2298 train_time:829399ms step_avg:181.65ms\nstep:4577/5000 train_loss:8.1411 train_time:829588ms step_avg:181.65ms\nstep:4578/5000 train_loss:8.1925 train_time:829770ms step_avg:181.65ms\nstep:4579/5000 train_loss:8.0644 train_time:829951ms step_avg:181.65ms\nstep:4580/5000 train_loss:8.3520 train_time:830132ms step_avg:181.65ms\nstep:4581/5000 train_loss:8.0529 train_time:830313ms step_avg:181.65ms\nstep:4582/5000 train_loss:8.1188 train_time:830496ms step_avg:181.65ms\nstep:4583/5000 train_loss:8.1164 train_time:830680ms step_avg:181.65ms\nstep:4584/5000 train_loss:8.2204 train_time:830862ms step_avg:181.65ms\nstep:4585/5000 train_loss:8.0258 train_time:831043ms step_avg:181.65ms\nstep:4586/5000 train_loss:8.1558 train_time:831223ms step_avg:181.65ms\nstep:4587/5000 train_loss:8.0554 train_time:831402ms step_avg:181.65ms\nstep:4588/5000 train_loss:8.0766 train_time:831584ms step_avg:181.65ms\nstep:4589/5000 train_loss:8.1340 train_time:831765ms step_avg:181.65ms\nstep:4590/5000 train_loss:8.1896 train_time:831945ms step_avg:181.65ms\nstep:4591/5000 train_loss:8.1232 train_time:832125ms step_avg:181.65ms\nstep:4592/5000 train_loss:8.0040 train_time:832306ms step_avg:181.65ms\nstep:4593/5000 train_loss:8.0821 train_time:832487ms step_avg:181.65ms\nstep:4594/5000 train_loss:8.0867 train_time:832668ms step_avg:181.65ms\nstep:4595/5000 train_loss:8.2771 train_time:832850ms step_avg:181.65ms\nstep:4596/5000 train_loss:8.1966 train_time:833033ms step_avg:181.65ms\nstep:4597/5000 train_loss:8.0838 train_time:833215ms step_avg:181.65ms\nstep:4598/5000 train_loss:8.0858 train_time:833397ms step_avg:181.65ms\nstep:4599/5000 train_loss:8.2933 train_time:833577ms step_avg:181.65ms\nstep:4600/5000 train_loss:8.1121 train_time:833759ms step_avg:181.65ms\nstep:4601/5000 train_loss:8.1179 train_time:833941ms step_avg:181.65ms\nstep:4602/5000 train_loss:8.1352 train_time:834122ms step_avg:181.65ms\nstep:4603/5000 train_loss:8.1889 train_time:834305ms step_avg:181.65ms\nstep:4604/5000 train_loss:8.1368 train_time:834486ms step_avg:181.65ms\nstep:4605/5000 train_loss:8.1904 train_time:834666ms step_avg:181.65ms\nstep:4606/5000 train_loss:8.1383 train_time:834845ms step_avg:181.65ms\nstep:4607/5000 train_loss:8.1338 train_time:835025ms step_avg:181.65ms\nstep:4608/5000 train_loss:8.0599 train_time:835206ms step_avg:181.65ms\nstep:4609/5000 train_loss:8.0594 train_time:835388ms step_avg:181.65ms\nstep:4610/5000 train_loss:8.1111 train_time:835569ms step_avg:181.65ms\nstep:4611/5000 train_loss:8.1791 train_time:835751ms step_avg:181.65ms\nstep:4612/5000 train_loss:8.0420 train_time:835933ms step_avg:181.65ms\nstep:4613/5000 train_loss:8.1526 train_time:836116ms step_avg:181.65ms\nstep:4614/5000 train_loss:8.1293 train_time:836297ms step_avg:181.65ms\nstep:4615/5000 train_loss:8.1469 train_time:836478ms step_avg:181.65ms\nstep:4616/5000 train_loss:8.2408 train_time:836659ms step_avg:181.65ms\nstep:4617/5000 train_loss:8.1533 train_time:836840ms step_avg:181.65ms\nstep:4618/5000 train_loss:8.1664 train_time:837021ms step_avg:181.65ms\nstep:4619/5000 train_loss:8.1618 train_time:837203ms step_avg:181.65ms\nstep:4620/5000 train_loss:8.1865 train_time:837383ms step_avg:181.64ms\nstep:4621/5000 train_loss:8.0888 train_time:837564ms step_avg:181.64ms\nstep:4622/5000 train_loss:8.0716 train_time:837744ms step_avg:181.64ms\nstep:4623/5000 train_loss:8.0928 train_time:837923ms step_avg:181.64ms\nstep:4624/5000 train_loss:8.0707 train_time:838103ms step_avg:181.64ms\nstep:4625/5000 train_loss:8.1795 train_time:838284ms step_avg:181.64ms\nstep:4626/5000 train_loss:8.0883 train_time:838464ms step_avg:181.64ms\nstep:4627/5000 train_loss:8.0585 train_time:838645ms step_avg:181.64ms\nstep:4628/5000 train_loss:8.0990 train_time:838826ms step_avg:181.64ms\nstep:4629/5000 train_loss:8.1369 train_time:839007ms step_avg:181.64ms\nstep:4630/5000 train_loss:8.1749 train_time:839187ms step_avg:181.64ms\nstep:4631/5000 train_loss:8.1704 train_time:839367ms step_avg:181.64ms\nstep:4632/5000 train_loss:8.1037 train_time:839548ms step_avg:181.64ms\nstep:4633/5000 train_loss:8.0939 train_time:839729ms step_avg:181.64ms\nstep:4634/5000 train_loss:8.1598 train_time:839912ms step_avg:181.64ms\nstep:4635/5000 train_loss:8.1443 train_time:840094ms step_avg:181.64ms\nstep:4636/5000 train_loss:8.0745 train_time:840276ms step_avg:181.64ms\nstep:4637/5000 train_loss:8.0307 train_time:840458ms step_avg:181.64ms\nstep:4638/5000 train_loss:8.1915 train_time:840639ms step_avg:181.64ms\nstep:4639/5000 train_loss:8.0636 train_time:840820ms step_avg:181.64ms\nstep:4640/5000 train_loss:7.9471 train_time:841001ms step_avg:181.64ms\nstep:4641/5000 train_loss:8.1267 train_time:841181ms step_avg:181.64ms\nstep:4642/5000 train_loss:8.2372 train_time:841361ms step_avg:181.64ms\nstep:4643/5000 train_loss:8.1744 train_time:841543ms step_avg:181.64ms\nstep:4644/5000 train_loss:8.1503 train_time:841723ms step_avg:181.64ms\nstep:4645/5000 train_loss:8.1755 train_time:841903ms step_avg:181.64ms\nstep:4646/5000 train_loss:8.0606 train_time:842084ms step_avg:181.64ms\nstep:4647/5000 train_loss:8.1455 train_time:842264ms step_avg:181.64ms\nstep:4648/5000 train_loss:8.0649 train_time:842444ms step_avg:181.64ms\nstep:4649/5000 train_loss:8.0704 train_time:842624ms step_avg:181.64ms\nstep:4650/5000 train_loss:8.0584 train_time:842805ms step_avg:181.64ms\nstep:4651/5000 train_loss:8.1226 train_time:842986ms step_avg:181.64ms\nstep:4652/5000 train_loss:8.0945 train_time:843167ms step_avg:181.64ms\nstep:4653/5000 train_loss:8.2154 train_time:843350ms step_avg:181.64ms\nstep:4654/5000 train_loss:7.8925 train_time:843531ms step_avg:181.64ms\nstep:4655/5000 train_loss:8.0883 train_time:843714ms step_avg:181.64ms\nstep:4656/5000 train_loss:7.9362 train_time:843896ms step_avg:181.64ms\nstep:4657/5000 train_loss:8.1194 train_time:844077ms step_avg:181.64ms\nstep:4658/5000 train_loss:8.2145 train_time:844258ms step_avg:181.64ms\nstep:4659/5000 train_loss:8.0174 train_time:844440ms step_avg:181.64ms\nstep:4660/5000 train_loss:8.0702 train_time:844622ms step_avg:181.64ms\nstep:4661/5000 train_loss:8.1715 train_time:844803ms step_avg:181.64ms\nstep:4662/5000 train_loss:8.1997 train_time:844984ms step_avg:181.64ms\nstep:4663/5000 train_loss:7.9576 train_time:845164ms step_avg:181.64ms\nstep:4664/5000 train_loss:8.1820 train_time:845344ms step_avg:181.64ms\nstep:4665/5000 train_loss:8.0165 train_time:845525ms step_avg:181.64ms\nstep:4666/5000 train_loss:8.0971 train_time:845707ms step_avg:181.64ms\nstep:4667/5000 train_loss:8.0997 train_time:845888ms step_avg:181.64ms\nstep:4668/5000 train_loss:8.1954 train_time:846071ms step_avg:181.64ms\nstep:4669/5000 train_loss:8.1261 train_time:846254ms step_avg:181.64ms\nstep:4670/5000 train_loss:8.2754 train_time:846436ms step_avg:181.64ms\nstep:4671/5000 train_loss:8.1476 train_time:846618ms step_avg:181.64ms\nstep:4672/5000 train_loss:8.1544 train_time:846799ms step_avg:181.64ms\nstep:4673/5000 train_loss:8.1528 train_time:846980ms step_avg:181.64ms\nstep:4674/5000 train_loss:8.0703 train_time:847162ms step_avg:181.64ms\nstep:4675/5000 train_loss:8.1506 train_time:847343ms step_avg:181.64ms\nstep:4676/5000 train_loss:8.1448 train_time:847523ms step_avg:181.64ms\nstep:4677/5000 train_loss:8.1771 train_time:847703ms step_avg:181.64ms\nstep:4678/5000 train_loss:8.0137 train_time:847883ms step_avg:181.64ms\nstep:4679/5000 train_loss:8.1340 train_time:848063ms step_avg:181.64ms\nstep:4680/5000 train_loss:8.1266 train_time:848244ms step_avg:181.64ms\nstep:4681/5000 train_loss:8.0571 train_time:848424ms step_avg:181.64ms\nstep:4682/5000 train_loss:8.1519 train_time:848605ms step_avg:181.64ms\nstep:4683/5000 train_loss:8.1490 train_time:848786ms step_avg:181.64ms\nstep:4684/5000 train_loss:8.0732 train_time:848966ms step_avg:181.64ms\nstep:4685/5000 train_loss:8.2598 train_time:849147ms step_avg:181.64ms\nstep:4686/5000 train_loss:8.0836 train_time:849326ms step_avg:181.64ms\nstep:4687/5000 train_loss:8.0733 train_time:849507ms step_avg:181.64ms\nstep:4688/5000 train_loss:8.1768 train_time:849689ms step_avg:181.64ms\nstep:4689/5000 train_loss:8.1971 train_time:849873ms step_avg:181.64ms\nstep:4690/5000 train_loss:8.1411 train_time:850055ms step_avg:181.64ms\nstep:4691/5000 train_loss:8.1687 train_time:850237ms step_avg:181.64ms\nstep:4692/5000 train_loss:8.2589 train_time:850419ms step_avg:181.64ms\nstep:4693/5000 train_loss:8.1705 train_time:850601ms step_avg:181.64ms\nstep:4694/5000 train_loss:8.0659 train_time:850781ms step_avg:181.64ms\nstep:4695/5000 train_loss:8.0438 train_time:850962ms step_avg:181.64ms\nstep:4696/5000 train_loss:8.1495 train_time:851143ms step_avg:181.64ms\nstep:4697/5000 train_loss:7.8950 train_time:851324ms step_avg:181.64ms\nstep:4698/5000 train_loss:8.0259 train_time:851506ms step_avg:181.64ms\nstep:4699/5000 train_loss:8.1612 train_time:851685ms step_avg:181.63ms\nstep:4700/5000 train_loss:8.1399 train_time:851865ms step_avg:181.63ms\nstep:4701/5000 train_loss:8.1139 train_time:852046ms step_avg:181.63ms\nstep:4702/5000 train_loss:8.1545 train_time:852227ms step_avg:181.63ms\nstep:4703/5000 train_loss:8.1091 train_time:852409ms step_avg:181.63ms\nstep:4704/5000 train_loss:8.1574 train_time:852590ms step_avg:181.63ms\nstep:4705/5000 train_loss:8.1637 train_time:852772ms step_avg:181.63ms\nstep:4706/5000 train_loss:8.1230 train_time:852954ms step_avg:181.63ms\nstep:4707/5000 train_loss:8.1120 train_time:853137ms step_avg:181.63ms\nstep:4708/5000 train_loss:8.0179 train_time:853319ms step_avg:181.63ms\nstep:4709/5000 train_loss:8.1201 train_time:853500ms step_avg:181.63ms\nstep:4710/5000 train_loss:8.1460 train_time:853681ms step_avg:181.63ms\nstep:4711/5000 train_loss:8.0514 train_time:853861ms step_avg:181.63ms\nstep:4712/5000 train_loss:7.8767 train_time:854042ms step_avg:181.63ms\nstep:4713/5000 train_loss:8.0908 train_time:854223ms step_avg:181.63ms\nstep:4714/5000 train_loss:8.2085 train_time:854405ms step_avg:181.63ms\nstep:4715/5000 train_loss:8.1368 train_time:854585ms step_avg:181.63ms\nstep:4716/5000 train_loss:8.2127 train_time:854765ms step_avg:181.63ms\nstep:4717/5000 train_loss:8.1650 train_time:854946ms step_avg:181.63ms\nstep:4718/5000 train_loss:8.0892 train_time:855126ms step_avg:181.63ms\nstep:4719/5000 train_loss:8.1746 train_time:855308ms step_avg:181.63ms\nstep:4720/5000 train_loss:8.1148 train_time:855489ms step_avg:181.63ms\nstep:4721/5000 train_loss:8.2719 train_time:855670ms step_avg:181.63ms\nstep:4722/5000 train_loss:8.0019 train_time:855852ms step_avg:181.63ms\nstep:4723/5000 train_loss:8.0831 train_time:856033ms step_avg:181.63ms\nstep:4724/5000 train_loss:8.2080 train_time:856214ms step_avg:181.63ms\nstep:4725/5000 train_loss:8.0815 train_time:856395ms step_avg:181.63ms\nstep:4726/5000 train_loss:8.1412 train_time:856578ms step_avg:181.63ms\nstep:4727/5000 train_loss:8.1399 train_time:856759ms step_avg:181.63ms\nstep:4728/5000 train_loss:8.2164 train_time:856940ms step_avg:181.63ms\nstep:4729/5000 train_loss:8.1082 train_time:857122ms step_avg:181.63ms\nstep:4730/5000 train_loss:8.1518 train_time:857303ms step_avg:181.63ms\nstep:4731/5000 train_loss:8.1854 train_time:857483ms step_avg:181.63ms\nstep:4732/5000 train_loss:8.0293 train_time:857663ms step_avg:181.63ms\nstep:4733/5000 train_loss:8.0897 train_time:857844ms step_avg:181.63ms\nstep:4734/5000 train_loss:8.0723 train_time:858025ms step_avg:181.63ms\nstep:4735/5000 train_loss:8.1044 train_time:858206ms step_avg:181.63ms\nstep:4736/5000 train_loss:8.0486 train_time:858387ms step_avg:181.63ms\nstep:4737/5000 train_loss:8.1120 train_time:858567ms step_avg:181.63ms\nstep:4738/5000 train_loss:8.1869 train_time:858747ms step_avg:181.63ms\nstep:4739/5000 train_loss:8.1330 train_time:858929ms step_avg:181.63ms\nstep:4740/5000 train_loss:8.1851 train_time:859111ms step_avg:181.63ms\nstep:4741/5000 train_loss:7.9577 train_time:859292ms step_avg:181.63ms\nstep:4742/5000 train_loss:8.1201 train_time:859475ms step_avg:181.63ms\nstep:4743/5000 train_loss:8.1213 train_time:859658ms step_avg:181.63ms\nstep:4744/5000 train_loss:8.0913 train_time:859839ms step_avg:181.63ms\nstep:4745/5000 train_loss:7.8976 train_time:860021ms step_avg:181.63ms\nstep:4746/5000 train_loss:8.1125 train_time:860201ms step_avg:181.63ms\nstep:4747/5000 train_loss:8.1579 train_time:860381ms step_avg:181.63ms\nstep:4748/5000 train_loss:8.0278 train_time:860561ms step_avg:181.63ms\nstep:4749/5000 train_loss:8.0130 train_time:860743ms step_avg:181.63ms\nstep:4750/5000 train_loss:8.0994 train_time:860923ms step_avg:181.63ms\nstep:4751/5000 train_loss:8.0273 train_time:861104ms step_avg:181.63ms\nstep:4752/5000 train_loss:8.0867 train_time:861285ms step_avg:181.63ms\nstep:4753/5000 train_loss:8.0053 train_time:861469ms step_avg:181.63ms\nstep:4754/5000 train_loss:8.0385 train_time:861647ms step_avg:181.63ms\nstep:4755/5000 train_loss:8.2306 train_time:861828ms step_avg:181.63ms\nstep:4756/5000 train_loss:8.1562 train_time:862011ms step_avg:181.63ms\nstep:4757/5000 train_loss:8.0622 train_time:862193ms step_avg:181.63ms\nstep:4758/5000 train_loss:7.9965 train_time:862375ms step_avg:181.63ms\nstep:4759/5000 train_loss:8.1355 train_time:862558ms step_avg:181.63ms\nstep:4760/5000 train_loss:8.0386 train_time:862739ms step_avg:181.63ms\nstep:4761/5000 train_loss:8.1143 train_time:862921ms step_avg:181.63ms\nstep:4762/5000 train_loss:8.1207 train_time:863102ms step_avg:181.63ms\nstep:4763/5000 train_loss:8.1637 train_time:863283ms step_avg:181.63ms\nstep:4764/5000 train_loss:8.1303 train_time:863462ms step_avg:181.63ms\nstep:4765/5000 train_loss:8.0081 train_time:863643ms step_avg:181.63ms\nstep:4766/5000 train_loss:8.0845 train_time:863824ms step_avg:181.63ms\nstep:4767/5000 train_loss:8.2043 train_time:864004ms step_avg:181.63ms\nstep:4768/5000 train_loss:8.1655 train_time:864184ms step_avg:181.63ms\nstep:4769/5000 train_loss:8.1682 train_time:864365ms step_avg:181.63ms\nstep:4770/5000 train_loss:8.1826 train_time:864545ms step_avg:181.63ms\nstep:4771/5000 train_loss:8.0935 train_time:864725ms step_avg:181.63ms\nstep:4772/5000 train_loss:8.2815 train_time:864907ms step_avg:181.63ms\nstep:4773/5000 train_loss:8.1162 train_time:865088ms step_avg:181.63ms\nstep:4774/5000 train_loss:8.1372 train_time:865269ms step_avg:181.63ms\nstep:4775/5000 train_loss:8.2369 train_time:865451ms step_avg:181.63ms\nstep:4776/5000 train_loss:8.2708 train_time:865632ms step_avg:181.63ms\nstep:4777/5000 train_loss:8.1733 train_time:865815ms step_avg:181.63ms\nstep:4778/5000 train_loss:8.1398 train_time:865996ms step_avg:181.63ms\nstep:4779/5000 train_loss:8.0847 train_time:866177ms step_avg:181.63ms\nstep:4780/5000 train_loss:8.0181 train_time:866359ms step_avg:181.63ms\nstep:4781/5000 train_loss:8.2754 train_time:866540ms step_avg:181.63ms\nstep:4782/5000 train_loss:8.1547 train_time:866721ms step_avg:181.63ms\nstep:4783/5000 train_loss:8.0806 train_time:866903ms step_avg:181.63ms\nstep:4784/5000 train_loss:8.1614 train_time:867085ms step_avg:181.63ms\nstep:4785/5000 train_loss:8.1451 train_time:867265ms step_avg:181.63ms\nstep:4786/5000 train_loss:8.2060 train_time:867447ms step_avg:181.63ms\nstep:4787/5000 train_loss:8.0463 train_time:867627ms step_avg:181.63ms\nstep:4788/5000 train_loss:8.0403 train_time:867810ms step_avg:181.63ms\nstep:4789/5000 train_loss:7.9596 train_time:867992ms step_avg:181.63ms\nstep:4790/5000 train_loss:8.0867 train_time:868174ms step_avg:181.63ms\nstep:4791/5000 train_loss:8.0434 train_time:868357ms step_avg:181.63ms\nstep:4792/5000 train_loss:7.9758 train_time:868540ms step_avg:181.63ms\nstep:4793/5000 train_loss:8.0459 train_time:868721ms step_avg:181.63ms\nstep:4794/5000 train_loss:8.0939 train_time:868902ms step_avg:181.63ms\nstep:4795/5000 train_loss:8.0656 train_time:869082ms step_avg:181.63ms\nstep:4796/5000 train_loss:8.1931 train_time:869263ms step_avg:181.63ms\nstep:4797/5000 train_loss:8.1186 train_time:869444ms step_avg:181.63ms\nstep:4798/5000 train_loss:8.0292 train_time:869625ms step_avg:181.63ms\nstep:4799/5000 train_loss:8.0162 train_time:869806ms step_avg:181.63ms\nstep:4800/5000 train_loss:8.1189 train_time:869987ms step_avg:181.63ms\nstep:4801/5000 train_loss:8.0747 train_time:870167ms step_avg:181.63ms\nstep:4802/5000 train_loss:7.9201 train_time:870349ms step_avg:181.63ms\nstep:4803/5000 train_loss:8.0108 train_time:870530ms step_avg:181.63ms\nstep:4804/5000 train_loss:8.2186 train_time:870713ms step_avg:181.63ms\nstep:4805/5000 train_loss:8.2126 train_time:870896ms step_avg:181.63ms\nstep:4806/5000 train_loss:8.1258 train_time:871077ms step_avg:181.63ms\nstep:4807/5000 train_loss:8.1346 train_time:871258ms step_avg:181.63ms\nstep:4808/5000 train_loss:8.0673 train_time:871439ms step_avg:181.63ms\nstep:4809/5000 train_loss:8.0369 train_time:871621ms step_avg:181.63ms\nstep:4810/5000 train_loss:7.9983 train_time:871803ms step_avg:181.63ms\nstep:4811/5000 train_loss:8.0667 train_time:871983ms step_avg:181.63ms\nstep:4812/5000 train_loss:8.0739 train_time:872164ms step_avg:181.63ms\nstep:4813/5000 train_loss:8.1797 train_time:872344ms step_avg:181.62ms\nstep:4814/5000 train_loss:8.1465 train_time:872524ms step_avg:181.62ms\nstep:4815/5000 train_loss:8.1060 train_time:872707ms step_avg:181.62ms\nstep:4816/5000 train_loss:8.0624 train_time:872887ms step_avg:181.62ms\nstep:4817/5000 train_loss:8.0235 train_time:873068ms step_avg:181.62ms\nstep:4818/5000 train_loss:7.8666 train_time:873250ms step_avg:181.62ms\nstep:4819/5000 train_loss:8.0531 train_time:873431ms step_avg:181.62ms\nstep:4820/5000 train_loss:8.1394 train_time:873613ms step_avg:181.62ms\nstep:4821/5000 train_loss:8.0005 train_time:873795ms step_avg:181.62ms\nstep:4822/5000 train_loss:8.1053 train_time:873977ms step_avg:181.62ms\nstep:4823/5000 train_loss:8.1178 train_time:874160ms step_avg:181.62ms\nstep:4824/5000 train_loss:8.1864 train_time:874340ms step_avg:181.62ms\nstep:4825/5000 train_loss:8.0552 train_time:874521ms step_avg:181.62ms\nstep:4826/5000 train_loss:8.1951 train_time:874704ms step_avg:181.62ms\nstep:4827/5000 train_loss:7.9405 train_time:874884ms step_avg:181.62ms\nstep:4828/5000 train_loss:8.2175 train_time:875064ms step_avg:181.62ms\nstep:4829/5000 train_loss:8.2839 train_time:875244ms step_avg:181.62ms\nstep:4830/5000 train_loss:8.0566 train_time:875424ms step_avg:181.62ms\nstep:4831/5000 train_loss:8.1082 train_time:875605ms step_avg:181.62ms\nstep:4832/5000 train_loss:8.1510 train_time:875787ms step_avg:181.62ms\nstep:4833/5000 train_loss:8.1110 train_time:875968ms step_avg:181.62ms\nstep:4834/5000 train_loss:8.1468 train_time:876150ms step_avg:181.62ms\nstep:4835/5000 train_loss:8.0309 train_time:876331ms step_avg:181.62ms\nstep:4836/5000 train_loss:8.0700 train_time:876514ms step_avg:181.62ms\nstep:4837/5000 train_loss:8.0107 train_time:876697ms step_avg:181.62ms\nstep:4838/5000 train_loss:8.1937 train_time:876880ms step_avg:181.62ms\nstep:4839/5000 train_loss:8.1094 train_time:877061ms step_avg:181.62ms\nstep:4840/5000 train_loss:8.0643 train_time:877242ms step_avg:181.62ms\nstep:4841/5000 train_loss:8.1084 train_time:877423ms step_avg:181.62ms\nstep:4842/5000 train_loss:8.1001 train_time:877604ms step_avg:181.62ms\nstep:4843/5000 train_loss:8.1359 train_time:877785ms step_avg:181.62ms\nstep:4844/5000 train_loss:8.1147 train_time:877965ms step_avg:181.62ms\nstep:4845/5000 train_loss:8.1113 train_time:878146ms step_avg:181.62ms\nstep:4846/5000 train_loss:8.0286 train_time:878327ms step_avg:181.62ms\nstep:4847/5000 train_loss:8.0346 train_time:878509ms step_avg:181.62ms\nstep:4848/5000 train_loss:8.0616 train_time:878691ms step_avg:181.62ms\nstep:4849/5000 train_loss:8.0054 train_time:878872ms step_avg:181.62ms\nstep:4850/5000 train_loss:8.0951 train_time:879055ms step_avg:181.62ms\nstep:4851/5000 train_loss:7.9376 train_time:879237ms step_avg:181.62ms\nstep:4852/5000 train_loss:8.0814 train_time:879419ms step_avg:181.62ms\nstep:4853/5000 train_loss:8.1026 train_time:879600ms step_avg:181.62ms\nstep:4854/5000 train_loss:8.0911 train_time:879781ms step_avg:181.62ms\nstep:4855/5000 train_loss:8.1209 train_time:879962ms step_avg:181.62ms\nstep:4856/5000 train_loss:8.2633 train_time:880144ms step_avg:181.62ms\nstep:4857/5000 train_loss:8.2217 train_time:880325ms step_avg:181.62ms\nstep:4858/5000 train_loss:8.1380 train_time:880507ms step_avg:181.62ms\nstep:4859/5000 train_loss:8.0745 train_time:880688ms step_avg:181.62ms\nstep:4860/5000 train_loss:8.1599 train_time:880870ms step_avg:181.62ms\nstep:4861/5000 train_loss:8.0682 train_time:881052ms step_avg:181.62ms\nstep:4862/5000 train_loss:8.1715 train_time:881234ms step_avg:181.62ms\nstep:4863/5000 train_loss:8.0911 train_time:881417ms step_avg:181.62ms\nstep:4864/5000 train_loss:8.0937 train_time:881598ms step_avg:181.62ms\nstep:4865/5000 train_loss:8.1505 train_time:881779ms step_avg:181.62ms\nstep:4866/5000 train_loss:8.3176 train_time:881960ms step_avg:181.62ms\nstep:4867/5000 train_loss:8.0922 train_time:882141ms step_avg:181.62ms\nstep:4868/5000 train_loss:8.1825 train_time:882323ms step_avg:181.62ms\nstep:4869/5000 train_loss:8.0866 train_time:882505ms step_avg:181.62ms\nstep:4870/5000 train_loss:8.1425 train_time:882686ms step_avg:181.62ms\nstep:4871/5000 train_loss:8.1283 train_time:882866ms step_avg:181.62ms\nstep:4872/5000 train_loss:8.0666 train_time:883046ms step_avg:181.62ms\nstep:4873/5000 train_loss:8.2240 train_time:883227ms step_avg:181.62ms\nstep:4874/5000 train_loss:8.0773 train_time:883409ms step_avg:181.62ms\nstep:4875/5000 train_loss:8.0214 train_time:883590ms step_avg:181.62ms\nstep:4876/5000 train_loss:8.1982 train_time:883772ms step_avg:181.62ms\nstep:4877/5000 train_loss:8.1298 train_time:883955ms step_avg:181.62ms\nstep:4878/5000 train_loss:8.1763 train_time:884138ms step_avg:181.62ms\nstep:4879/5000 train_loss:8.1792 train_time:884321ms step_avg:181.62ms\nstep:4880/5000 train_loss:8.1141 train_time:884503ms step_avg:181.62ms\nstep:4881/5000 train_loss:8.0395 train_time:884684ms step_avg:181.62ms\nstep:4882/5000 train_loss:8.1198 train_time:884864ms step_avg:181.62ms\nstep:4883/5000 train_loss:8.0994 train_time:885045ms step_avg:181.62ms\nstep:4884/5000 train_loss:8.1770 train_time:885226ms step_avg:181.62ms\nstep:4885/5000 train_loss:8.1269 train_time:885409ms step_avg:181.62ms\nstep:4886/5000 train_loss:8.1357 train_time:885589ms step_avg:181.62ms\nstep:4887/5000 train_loss:8.0652 train_time:885772ms step_avg:181.62ms\nstep:4888/5000 train_loss:8.1283 train_time:885955ms step_avg:181.62ms\nstep:4889/5000 train_loss:8.1653 train_time:886137ms step_avg:181.62ms\nstep:4890/5000 train_loss:8.1453 train_time:886319ms step_avg:181.62ms\nstep:4891/5000 train_loss:8.1100 train_time:886502ms step_avg:181.62ms\nstep:4892/5000 train_loss:8.2643 train_time:886682ms step_avg:181.62ms\nstep:4893/5000 train_loss:8.1322 train_time:886863ms step_avg:181.62ms\nstep:4894/5000 train_loss:8.1407 train_time:887044ms step_avg:181.62ms\nstep:4895/5000 train_loss:8.0857 train_time:887223ms step_avg:181.62ms\nstep:4896/5000 train_loss:8.2174 train_time:887406ms step_avg:181.62ms\nstep:4897/5000 train_loss:8.1174 train_time:887587ms step_avg:181.62ms\nstep:4898/5000 train_loss:8.2108 train_time:887767ms step_avg:181.62ms\nstep:4899/5000 train_loss:8.0460 train_time:887949ms step_avg:181.62ms\nstep:4900/5000 train_loss:8.0459 train_time:888130ms step_avg:181.62ms\nstep:4901/5000 train_loss:8.2557 train_time:888314ms step_avg:181.62ms\nstep:4902/5000 train_loss:7.9848 train_time:888496ms step_avg:181.62ms\nstep:4903/5000 train_loss:8.1764 train_time:888677ms step_avg:181.62ms\nstep:4904/5000 train_loss:8.2612 train_time:888859ms step_avg:181.62ms\nstep:4905/5000 train_loss:8.2652 train_time:889040ms step_avg:181.62ms\nstep:4906/5000 train_loss:8.1926 train_time:889222ms step_avg:181.62ms\nstep:4907/5000 train_loss:8.4037 train_time:889404ms step_avg:181.62ms\nstep:4908/5000 train_loss:8.1916 train_time:889586ms step_avg:181.62ms\nstep:4909/5000 train_loss:8.3914 train_time:889767ms step_avg:181.62ms\nstep:4910/5000 train_loss:8.0508 train_time:889948ms step_avg:181.62ms\nstep:4911/5000 train_loss:8.0599 train_time:890128ms step_avg:181.62ms\nstep:4912/5000 train_loss:8.0602 train_time:890310ms step_avg:181.62ms\nstep:4913/5000 train_loss:8.1195 train_time:890492ms step_avg:181.62ms\nstep:4914/5000 train_loss:8.0771 train_time:890675ms step_avg:181.62ms\nstep:4915/5000 train_loss:8.1581 train_time:890858ms step_avg:181.62ms\nstep:4916/5000 train_loss:8.2390 train_time:891039ms step_avg:181.62ms\nstep:4917/5000 train_loss:8.2083 train_time:891221ms step_avg:181.62ms\nstep:4918/5000 train_loss:7.9788 train_time:891403ms step_avg:181.62ms\nstep:4919/5000 train_loss:8.1313 train_time:891585ms step_avg:181.62ms\nstep:4920/5000 train_loss:8.0740 train_time:891765ms step_avg:181.62ms\nstep:4921/5000 train_loss:8.0995 train_time:891945ms step_avg:181.62ms\nstep:4922/5000 train_loss:8.1166 train_time:892127ms step_avg:181.62ms\nstep:4923/5000 train_loss:8.0848 train_time:892309ms step_avg:181.62ms\nstep:4924/5000 train_loss:8.0814 train_time:892491ms step_avg:181.62ms\nstep:4925/5000 train_loss:8.0555 train_time:892673ms step_avg:181.62ms\nstep:4926/5000 train_loss:8.0252 train_time:892855ms step_avg:181.62ms\nstep:4927/5000 train_loss:8.1342 train_time:893036ms step_avg:181.62ms\nstep:4928/5000 train_loss:8.1150 train_time:893219ms step_avg:181.62ms\nstep:4929/5000 train_loss:7.9892 train_time:893401ms step_avg:181.62ms\nstep:4930/5000 train_loss:8.1240 train_time:893582ms step_avg:181.62ms\nstep:4931/5000 train_loss:8.0475 train_time:893763ms step_avg:181.62ms\nstep:4932/5000 train_loss:8.0708 train_time:893943ms step_avg:181.62ms\nstep:4933/5000 train_loss:8.1263 train_time:894123ms step_avg:181.62ms\nstep:4934/5000 train_loss:8.1266 train_time:894304ms step_avg:181.62ms\nstep:4935/5000 train_loss:8.2276 train_time:894486ms step_avg:181.62ms\nstep:4936/5000 train_loss:8.2080 train_time:894667ms step_avg:181.62ms\nstep:4937/5000 train_loss:8.0333 train_time:894848ms step_avg:181.62ms\nstep:4938/5000 train_loss:8.1868 train_time:895028ms step_avg:181.62ms\nstep:4939/5000 train_loss:8.1778 train_time:895210ms step_avg:181.62ms\nstep:4940/5000 train_loss:8.0095 train_time:895393ms step_avg:181.62ms\nstep:4941/5000 train_loss:8.0731 train_time:895574ms step_avg:181.62ms\nstep:4942/5000 train_loss:8.0694 train_time:895757ms step_avg:181.62ms\nstep:4943/5000 train_loss:7.9693 train_time:895939ms step_avg:181.62ms\nstep:4944/5000 train_loss:8.1507 train_time:896121ms step_avg:181.62ms\nstep:4945/5000 train_loss:8.1387 train_time:896304ms step_avg:181.62ms\nstep:4946/5000 train_loss:8.1655 train_time:896484ms step_avg:181.62ms\nstep:4947/5000 train_loss:8.0365 train_time:896665ms step_avg:181.62ms\nstep:4948/5000 train_loss:8.0636 train_time:896846ms step_avg:181.62ms\nstep:4949/5000 train_loss:8.1289 train_time:897027ms step_avg:181.62ms\nstep:4950/5000 train_loss:8.1679 train_time:897208ms step_avg:181.62ms\nstep:4951/5000 train_loss:8.1136 train_time:897390ms step_avg:181.62ms\nstep:4952/5000 train_loss:8.1639 train_time:897573ms step_avg:181.62ms\nstep:4953/5000 train_loss:8.0133 train_time:897756ms step_avg:181.62ms\nstep:4954/5000 train_loss:8.2024 train_time:897939ms step_avg:181.62ms\nstep:4955/5000 train_loss:8.2180 train_time:898120ms step_avg:181.62ms\nstep:4956/5000 train_loss:8.0905 train_time:898301ms step_avg:181.62ms\nstep:4957/5000 train_loss:8.0662 train_time:898482ms step_avg:181.62ms\nstep:4958/5000 train_loss:8.0551 train_time:898662ms step_avg:181.62ms\nstep:4959/5000 train_loss:8.2288 train_time:898843ms step_avg:181.62ms\nstep:4960/5000 train_loss:8.0715 train_time:899024ms step_avg:181.62ms\nstep:4961/5000 train_loss:7.9317 train_time:899205ms step_avg:181.62ms\nstep:4962/5000 train_loss:8.2001 train_time:899386ms step_avg:181.62ms\nstep:4963/5000 train_loss:8.0911 train_time:899567ms step_avg:181.62ms\nstep:4964/5000 train_loss:8.0945 train_time:899749ms step_avg:181.62ms\nstep:4965/5000 train_loss:8.2197 train_time:899929ms step_avg:181.62ms\nstep:4966/5000 train_loss:8.1207 train_time:900113ms step_avg:181.62ms\nstep:4967/5000 train_loss:8.0578 train_time:900295ms step_avg:181.62ms\nstep:4968/5000 train_loss:8.1570 train_time:900477ms step_avg:181.62ms\nstep:4969/5000 train_loss:8.0751 train_time:900658ms step_avg:181.62ms\nstep:4970/5000 train_loss:8.0983 train_time:900839ms step_avg:181.62ms\nstep:4971/5000 train_loss:8.1312 train_time:901021ms step_avg:181.62ms\nstep:4972/5000 train_loss:7.9635 train_time:901203ms step_avg:181.62ms\nstep:4973/5000 train_loss:8.1998 train_time:901383ms step_avg:181.62ms\nstep:4974/5000 train_loss:8.0866 train_time:901563ms step_avg:181.62ms\nstep:4975/5000 train_loss:8.2247 train_time:901744ms step_avg:181.62ms\nstep:4976/5000 train_loss:8.2169 train_time:901924ms step_avg:181.62ms\nstep:4977/5000 train_loss:8.0555 train_time:902105ms step_avg:181.62ms\nstep:4978/5000 train_loss:7.9696 train_time:902287ms step_avg:181.62ms\nstep:4979/5000 train_loss:8.0783 train_time:902467ms step_avg:181.62ms\nstep:4980/5000 train_loss:8.1825 train_time:902648ms step_avg:181.62ms\nstep:4981/5000 train_loss:8.3799 train_time:902829ms step_avg:181.62ms\nstep:4982/5000 train_loss:8.1315 train_time:903012ms step_avg:181.62ms\nstep:4983/5000 train_loss:8.1063 train_time:903195ms step_avg:181.62ms\nstep:4984/5000 train_loss:8.0820 train_time:903378ms step_avg:181.62ms\nstep:4985/5000 train_loss:8.1431 train_time:903559ms step_avg:181.62ms\nstep:4986/5000 train_loss:8.1417 train_time:903741ms step_avg:181.62ms\nstep:4987/5000 train_loss:8.0763 train_time:903921ms step_avg:181.62ms\nstep:4988/5000 train_loss:8.0314 train_time:904105ms step_avg:181.62ms\nstep:4989/5000 train_loss:8.1477 train_time:904285ms step_avg:181.62ms\nstep:4990/5000 train_loss:8.1344 train_time:904466ms step_avg:181.62ms\nstep:4991/5000 train_loss:8.0424 train_time:904647ms step_avg:181.62ms\nstep:4992/5000 train_loss:8.2279 train_time:904828ms step_avg:181.62ms\nstep:4993/5000 train_loss:8.0212 train_time:905010ms step_avg:181.62ms\nstep:4994/5000 train_loss:8.0982 train_time:905192ms step_avg:181.62ms\nstep:4995/5000 train_loss:8.2451 train_time:905375ms step_avg:181.62ms\nstep:4996/5000 train_loss:8.3997 train_time:905556ms step_avg:181.62ms\nstep:4997/5000 train_loss:8.0318 train_time:905738ms step_avg:181.62ms\nstep:4998/5000 train_loss:8.1825 train_time:905921ms step_avg:181.62ms\nstep:4999/5000 train_loss:8.0537 train_time:906101ms step_avg:181.62ms\nstep:5000/5000 train_loss:8.0301 train_time:906282ms step_avg:181.62ms\npeak memory consumption: 37319 MiB\n(Current Step: 17, Remaining Steps: 33)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe training run completed successfully for 5000 iterations. The script generated `model.pt` and `model_config.pt` as required. The training loss decreased significantly during the initial phase and seemed to fluctuate around 8.0 - 8.2 towards the end, which is expected.\n\nLet's check if the output files were created.\n```bash\nls -l\n```",
      "thought": "DISCUSSION\nThe training run completed successfully for 5000 iterations. The script generated `model.pt` and `model_config.pt` as required. The training loss decreased significantly during the initial phase and seemed to fluctuate around 8.0 - 8.2 towards the end, which is expected.\n\nLet's check if the output files were created.\n",
      "action": "ls -l\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "total 633668\n-rw-r--r-- 1 agent mlgym     22640 Mar 26 03:21 baseline.py\ndrwxr-xr-x 1 agent mlgym        20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym      6550 Mar 21 18:52 evaluate.py*\ndrwxr-xr-x 1 agent mlgym       152 Mar 26 03:22 logs/\n-rw-r--r-- 1 agent mlgym 648835100 Mar 26 03:38 model.pt\n-rw-r--r-- 1 agent mlgym        98 Mar 26 03:38 model_config.pt\n(Current Step: 18, Remaining Steps: 32)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nYes, `model.pt` (648M) and `model_config.pt` (98 bytes) have been created. The model size seems reasonable for a GPT-2 small-like architecture (state dict contains weights, not the full model object).\n\nNow, let's validate this model to get a score.\n```bash\nvalidate\n```",
      "thought": "DISCUSSION\nYes, `model.pt` (648M) and `model_config.pt` (98 bytes) have been created. The model size seems reasonable for a GPT-2 small-like architecture (state dict contains weights, not the full model object).\n\nNow, let's validate this model to get a score.\n",
      "action": "validate\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "Failed to decode metrics from evaluation script. Output:\n---\nW0326 03:47:30.052000 4015 site-packages/torch/distributed/run.py:793] \nW0326 03:47:30.052000 4015 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:47:30.052000 4015 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:47:30.052000 4015 site-packages/torch/distributed/run.py:793] *****************************************\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/evaluate.py\", line 125, in <module>\n[rank0]:     val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/evaluate.py\", line 59, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\n[rank0]:[W326 03:47:32.784804127 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/evaluate.py\", line 125, in <module>\n[rank1]:     val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/evaluate.py\", line 59, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\nW0326 03:47:33.189000 4015 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 4082 closing signal SIGTERM\nE0326 03:47:33.353000 4015 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 4081) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\n/home/agent/workspace/evaluate.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-26_03:47:33\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 4081)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n---\nPlease make sure the evaluation script prints the metrics in json format.\n(Current Step: 19, Remaining Steps: 31)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe validation failed because the `evaluate.py` script is also using the incorrect data path (`./data/fineweb10B/fineweb_val_*.bin`). It should be `./data/fineweb108/fineweb_val_*.bin`. I need to edit `evaluate.py` to fix this path.\n```bash\nopen \"evaluate.py\"\n```",
      "thought": "DISCUSSION\nThe validation failed because the `evaluate.py` script is also using the incorrect data path (`./data/fineweb10B/fineweb_val_*.bin`). It should be `./data/fineweb108/fineweb_val_*.bin`. I need to edit `evaluate.py` to fix this path.\n",
      "action": "open \"evaluate.py\"\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/evaluate.py (168 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\"\"\"\n6:\n7:import glob\n8:import json\n9:import os\n10:import pickle\n11:import sys\n12:import time\n13:from dataclasses import dataclass\n14:\n15:import numpy as np\n16:import torch\n17:import torch.distributed as dist\n18:from baseline import Model, ModelConfig\n19:\n20:# -----------------------------------------------------------------------------\n21:# Our own simple Distributed Data Loader\n22:\n23:def _peek_data_shard(filename):\n24:    # only reads the header, returns header data\n25:    with open(filename, \"rb\") as f:\n26:        # first read the header, which is 256 int32 integers (4 bytes each)\n27:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n28:    if header[0] != 20240520:\n29:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n30:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n31:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n32:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n33:        exit(1)\n34:    assert header[1] == 1, \"unsupported version\"\n35:    ntok = header[2] # number of tokens (claimed)\n36:    return ntok # for now just return the number of tokens\n37:\n38:def _load_data_shard(filename):\n39:    with open(filename, \"rb\") as f:\n40:        # first read the header, which is 256 int32 integers (4 bytes each)\n41:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n42:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n43:        assert header[1] == 1, \"unsupported version\"\n44:        ntok = header[2] # number of tokens (claimed)\n45:        # the rest of it are tokens, stored as uint16\n46:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n47:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n48:    return tokens\n49:\n50:class DistributedDataLoader:\n51:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n52:        self.process_rank = process_rank\n53:        self.num_processes = num_processes\n54:        self.B = B\n55:        self.T = T\n56:\n57:        # glob files that match the pattern\n58:        self.files = sorted(glob.glob(filename_pattern))\n59:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n60:\n61:        # load and validate all data shards, count number of tokens in total\n62:        ntok_total = 0\n63:        for fname in self.files:\n64:            shard_ntok = _peek_data_shard(fname)\n65:            assert shard_ntok >= num_processes * B * T + 1\n66:            ntok_total += int(shard_ntok)\n67:        self.ntok_total = ntok_total\n68:\n69:        # kick things off\n70:        self.reset()\n71:\n72:    def reset(self):\n73:        self.current_shard = 0\n74:        self.current_position = self.process_rank * self.B * self.T\n75:        self.tokens = _load_data_shard(self.files[self.current_shard])\n76:\n77:    def advance(self): # advance to next data shard\n78:        self.current_shard = (self.current_shard + 1) % len(self.files)\n79:        self.current_position = self.process_rank * self.B * self.T\n80:        self.tokens = _load_data_shard(self.files[self.current_shard])\n81:\n82:    def next_batch(self):\n83:        B = self.B\n84:        T = self.T\n85:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n86:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n87:        x = (buf[:-1]).view(B, T) # inputs\n88:        y = (buf[1:]).view(B, T) # targets\n89:        # advance current position and load next shard if necessary\n90:        self.current_position += B * T * self.num_processes\n91:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n92:            self.advance()\n93:        return x.cuda(), y.cuda()\n94:\n95:\n96:@dataclass\n97:class Hyperparameters:\n98:    # data hyperparams\n99:    input_val_bin : str = './data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on\n100:    # optimization hyperparams\n101:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n102:    device_batch_size : int = 64 # batch size, in sequences, per device\n103:    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n104:    sequence_length : int = 1024 # sequence length, in tokens\n105:args = Hyperparameters()\n106:args.batch_size = args.device_batch_size * torch.cuda.device_count()\n107:\n108:# set up DDP (distributed data parallel). torchrun sets this env variable\n109:assert torch.cuda.is_available()\n110:dist.init_process_group(backend='nccl')\n111:ddp_rank = int(os.environ['RANK'])\n112:ddp_local_rank = int(os.environ['LOCAL_RANK'])\n113:ddp_world_size = int(os.environ['WORLD_SIZE'])\n114:device = f'cuda:{ddp_local_rank}'\n115:torch.cuda.set_device(device)\n116:\n117:master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n118:\n119:# convenience variables\n120:B, T = args.device_batch_size, args.sequence_length\n121:# calculate the number of steps to take in the val loop.\n122:assert args.val_tokens % (B * T * ddp_world_size) == 0\n123:val_steps = args.val_tokens // (B * T * ddp_world_size)\n124:\n125:val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n126:\n127:# Load the saved config and weights\n128:try:\n129:    import __main__\n130:    setattr(__main__, \"ModelConfig\", ModelConfig)\n131:    model_config = pickle.load(open(\"model_config.pt\", \"rb\"))\n132:    state_dict = torch.load('model.pt', weights_only=True)\n133:    new_state_dict = {}\n134:    for k, v in state_dict.items():\n135:        new_state_dict[k.replace(\"module._orig_mod.\", \"\")] = v\n136:    state_dict = new_state_dict\n137:except:\n138:    raise Exception(\"Model not found. Please run the training script first.\")\n139:\n140:\n141:# Reconstruct the model using the saved configuration\n142:model = Model(model_config)  # Use the saved config to initialize the model\n143:model.load_state_dict(state_dict)\n144:model = model.cuda()\n145:\n146:ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n147:\n148:# stop the clock\n149:t0 = time.time()\n150:# run validation batches\n151:model.eval()\n152:val_loader.reset()\n153:val_loss = 0.0\n154:torch.cuda.empty_cache()\n155:with torch.no_grad():\n156:    for _ in range(val_steps):\n157:        x_val, y_val = val_loader.next_batch()\n158:        with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason\n159:            _, loss = model(x_val, y_val, return_logits=False)\n160:            val_loss += loss.detach()\n161:            del loss\n162:dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n163:val_loss /= val_steps\n164:# log val loss to console and to logfile\n165:if master_process:\n166:    # Create and print the result dictionary\n167:    result = {\"val_loss\": val_loss.item()}\n168:    print(json.dumps(result))\n(Current Step: 20, Remaining Steps: 30)\n(Open file: /home/agent/workspace/evaluate.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the incorrect path is on line 99. Let's change `fineweb10B` to `fineweb108`.\n```bash\nedit 99:99\n    input_val_bin : str = './data/fineweb108/fineweb_val_*.bin' # input .bin to eval validation loss on\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the incorrect path is on line 99. Let's change `fineweb10B` to `fineweb108`.\n",
      "action": "edit 99:99\n    input_val_bin : str = './data/fineweb108/fineweb_val_*.bin' # input .bin to eval validation loss on\nend_of_edit\n",
      "agent": "primary_0"
    },
    {
      "role": "user",
      "content": "Error: You cannot edit read-only file /home/agent/workspace/evaluate.py\n(Current Step: 21, Remaining Steps: 29)\n(Open file: /home/agent/workspace/evaluate.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_0"
    },
    {
      "role": "assistant",
      "content": "Exit due to cost limit",
      "thought": "Exit due to cost limit",
      "action": "exit_cost",
      "agent": "primary_0"
    }
  ],
  "info": {
    "model_stats": {
      "total_cost": 4.62324,
      "task_cost": 4.62324,
      "tokens_sent": 1526888,
      "tokens_received": 3548,
      "api_calls": 22
    },
    "exit_status": "evaluation_format_error (exit_cost)",
    "submission": null,
    "score": [],
    "summarizer": {}
  }
}