{
  "environment": "mlgym_main",
  "trajectory": [
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, let's start by exploring the files in the current directory to understand the project structure and identify the baseline script or any relevant files.\n```bash\nls -al\n```",
      "thought": "DISCUSSION\nOkay, let's start by exploring the files in the current directory to understand the project structure and identify the baseline script or any relevant files.\n",
      "action": "ls -al\n",
      "execution_time": 0.30338684376329184,
      "observation": "total 32\ndrwxr-xr-x 1 agent mlgym    52 Mar 26 03:00 ./\ndrwxr-x--- 1 agent mlgym    66 Mar 26 03:00 ../\n-rw-r--r-- 1 agent mlgym 22293 Mar 21 18:52 baseline.py\ndrwxr-xr-x 1 agent mlgym    20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym  6550 Mar 21 18:52 evaluate.py*"
    },
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nWe have a `baseline.py` script, which likely contains the starting point for the model training. Let's examine it to understand the current approach. We also have an `evaluate.py` script, which is useful to understand the submission format, and a `data` directory.\n```bash\nopen \"baseline.py\"\n```",
      "thought": "DISCUSSION\nWe have a `baseline.py` script, which likely contains the starting point for the model training. Let's examine it to understand the current approach. We also have an `evaluate.py` script, which is useful to understand the submission format, and a `data` directory.\n",
      "action": "open \"baseline.py\"\n",
      "execution_time": 0.4188638059422374,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe baseline script trains a GPT-2 small model (12 layers, 6 heads, 768 embedding dimension) for only 500 iterations. This seems insufficient for the FineWeb dataset (2.4B tokens). The learning rate schedule is also unusual, effectively being a linear decay over 500 steps despite the `warmdown_iters` parameter being set to 1308.\n\nMy first step will be to significantly increase the number of training iterations and implement a more standard learning rate schedule: linear warmup followed by cosine decay.\n\nLet's increase `num_iterations` to 10,000 and set `warmup_iters` to 200. I will modify the `get_lr` function later to implement the cosine decay.\n\n```bash\nedit 357:359\n    num_iterations : int = 10000 # number of iterations to run\n    warmup_iters : int = 200\n    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe baseline script trains a GPT-2 small model (12 layers, 6 heads, 768 embedding dimension) for only 500 iterations. This seems insufficient for the FineWeb dataset (2.4B tokens). The learning rate schedule is also unusual, effectively being a linear decay over 500 steps despite the `warmdown_iters` parameter being set to 1308.\n\nMy first step will be to significantly increase the number of training iterations and implement a more standard learning rate schedule: linear warmup followed by cosine decay.\n\nLet's increase `num_iterations` to 10,000 and set `warmup_iters` to 200. I will modify the `get_lr` function later to implement the cosine decay.\n\n",
      "action": "edit 357:359\n    num_iterations : int = 10000 # number of iterations to run\n    warmup_iters : int = 200\n    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\nend_of_edit\n",
      "execution_time": 0.8378518018871546,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe changes to `num_iterations`, `warmup_iters`, and `warmdown_iters` seem correct. Now, let's modify the `get_lr` function to implement a cosine decay schedule after the linear warmup. We need to import the `math` module for `math.cos` and `math.pi`.\n\nFirst, import `math` at the top of the file.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nThe changes to `num_iterations`, `warmup_iters`, and `warmdown_iters` seem correct. Now, let's modify the `get_lr` function to implement a cosine decay schedule after the linear warmup. We need to import the `math` module for `math.cos` and `math.pi`.\n\nFirst, import `math` at the top of the file.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "execution_time": 0.8781819911673665,
      "observation": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAh, it seems the syntax checker flagged an issue, but it might be unrelated to the line I added. The error `SyntaxWarning: invalid escape sequence '\\s'` often occurs within string literals. However, the `import math` line itself doesn't have any strings or escape sequences. It's possible the checker picked up something elsewhere in the file, or it was a transient issue.\n\nLet's try inserting `import math` again at the same position, after the other standard library imports (after line 16).\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nAh, it seems the syntax checker flagged an issue, but it might be unrelated to the line I added. The error `SyntaxWarning: invalid escape sequence '\\s'` often occurs within string literals. However, the `import math` line itself doesn't have any strings or escape sequences. It's possible the checker picked up something elsewhere in the file, or it was a transient issue.\n\nLet's try inserting `import math` again at the same position, after the other standard library imports (after line 16).\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "execution_time": 0.9611371625214815,
      "observation": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe syntax warning persists even when trying to insert the import statement at a different line. It seems unrelated to the `import math` line itself and might be a spurious warning from the checker about some other part of the file, possibly within comments or docstrings. Since it's a `SyntaxWarning` and not a `SyntaxError`, it shouldn't prevent the script from running. I'll proceed with the necessary changes, ignoring the warning for now.\n\nFirst, let's add the `import math` statement. I'll place it with the other standard library imports.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nThe syntax warning persists even when trying to insert the import statement at a different line. It seems unrelated to the `import math` line itself and might be a spurious warning from the checker about some other part of the file, possibly within comments or docstrings. Since it's a `SyntaxWarning` and not a `SyntaxError`, it shouldn't prevent the script from running. I'll proceed with the necessary changes, ignoring the warning for now.\n\nFirst, let's add the `import math` statement. I'll place it with the other standard library imports.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "execution_time": 1.0045903455466032,
      "observation": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nIt seems the syntax checker is persistently blocking the insertion of `import math` due to an unrelated warning. Let's try to find the source of this `SyntaxWarning: invalid escape sequence '\\s'` within the file. Perhaps there's a literal `\\s` in a string or comment that's causing this.\n```bash\nsearch_file \"\\\\s\" \"baseline.py\"\n```",
      "thought": "DISCUSSION\nIt seems the syntax checker is persistently blocking the insertion of `import math` due to an unrelated warning. Let's try to find the source of this `SyntaxWarning: invalid escape sequence '\\s'` within the file. Perhaps there's a literal `\\s` in a string or comment that's causing this.\n",
      "action": "search_file \"\\\\s\" \"baseline.py\"\n",
      "execution_time": 0.8960570897907019,
      "observation": "Found 435 matches for \"\\s\" in /home/agent/workspace/baseline.py:\nLine 2:Copyright (c) Meta Platforms, Inc. and affiliates.\nLine 4:Adapted from https://github.com/KellerJordan/modded-nanogpt\nLine 6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\nLine 7:Run this file with the following command:\nLine 8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\nLine 11:import glob\nLine 12:import os\nLine 13:import pickle\nLine 14:import sys\nLine 15:import time\nLine 16:import uuid\nLine 17:from dataclasses import dataclass\nLine 19:import numpy as np\nLine 20:import torch\nLine 21:import torch._dynamo\nLine 22:import torch._inductor.config as config\nLine 23:import torch.distributed as dist\nLine 24:import torch.nn.functional as F\nLine 25:from torch import nn\nLine 26:from torch.nn.parallel import DistributedDataParallel as DDP\nLine 28:torch._dynamo.config.suppress_errors = True\nLine 30:# -----------------------------------------------------------------------------\nLine 31:# Muon optimizer\nLine 33:def zeropower_via_svd(G, steps=None):\nLine 34:    U, S, V = G.svd()\nLine 35:    return U @ V.T\nLine 38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\nLine 39:    \"\"\"\nLine 40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\nLine 41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\nLine 42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\nLine 43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\nLine 44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\nLine 45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\nLine 46:    performance at all relative to UV^T, where USV^T = G is the SVD.\nLine 47:    \"\"\"\nLine 48:    assert len(G.shape) == 2\nLine 49:    a, b, c = (3.4445, -4.7750,  2.0315)\nLine 50:    X = G.bfloat16()\nLine 51:    X /= (X.norm() + eps) # ensure top singular value <= 1\nLine 52:    if G.size(0) > G.size(1):\nLine 53:        X = X.T\nLine 54:    for _ in range(steps):\nLine 55:        A = X @ X.T\nLine 56:        B = A @ X\nLine 57:        X = a * X + b * B + c * A @ B\nLine 58:    if G.size(0) > G.size(1):\nLine 59:        X = X.T\nLine 60:    return X\nLine 62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\nLine 64:class Muon(torch.optim.Optimizer):\nLine 65:    \"\"\"\nLine 66:    Muon - MomentUm Orthogonalized by Newton-schulz\nLine 68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\nLine 69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\nLine 70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\nLine 71:    the advantage that it can be stably run in bfloat16 on the GPU.\nLine 73:    Some warnings:\nLine 74:    - This optimizer assumes that all parameters passed in are 2D.\nLine 75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\nLine 76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\nLine 77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\nLine 78:    - We believe it is unlikely to work well for training with small batch size.\nLine 79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\nLine 80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\nLine 82:    Arguments:\nLine 83:        lr: The learning rate used by the internal SGD.\nLine 84:        momentum: The momentum used by the internal SGD.\nLine 85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\nLine 86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\nLine 87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\nLine 88:    \"\"\"\nLine 89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\nLine 90:                 backend='newtonschulz5', backend_steps=5):\nLine 91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\nLine 92:        super().__init__(params, defaults)\nLine 94:    def step(self):\nLine 96:        for group in self.param_groups:\nLine 98:            lr = group['lr']\nLine 99:            momentum = group['momentum']\nLine 100:            zeropower_backend = zeropower_backends[group['backend']]\nLine 102:            # generate weight updates in distributed fashion\nLine 103:            total_params = sum(p.numel() for p in group['params'])\nLine 104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\nLine 105:            curr_idx = 0\nLine 106:            for i, p in enumerate(group['params']):\nLine 107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\nLine 108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\nLine 109:                    g = p.grad\nLine 110:                    assert g is not None\nLine 111:                    state = self.state[p]\nLine 112:                    if 'momentum_buffer' not in state:\nLine 113:                        state['momentum_buffer'] = torch.zeros_like(g)\nLine 114:                    buf = state['momentum_buffer']\nLine 115:                    buf.mul_(momentum).add_(g)\nLine 116:                    if group['nesterov']:\nLine 117:                        g = g.add(buf, alpha=momentum)\nLine 118:                    g = zeropower_backend(g, steps=group['backend_steps'])\nLine 119:                    g *= max(1, g.size(0)/g.size(1))**0.5\nLine 120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\nLine 121:                curr_idx += p.numel()\nLine 123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\nLine 124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\nLine 126:            # deserialize and apply updates\nLine 127:            curr_idx = 0\nLine 128:            for p in group['params']:\nLine 129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\nLine 130:                p.data.add_(g, alpha=-lr)\nLine 131:                curr_idx += p.numel()\nLine 133:# -----------------------------------------------------------------------------\nLine 134:# PyTorch nn.Module definitions for the GPT-2 model\nLine 136:class Rotary(torch.nn.Module):\nLine 138:    def __init__(self, dim, base=10000):\nLine 139:        super().__init__()\nLine 140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\nLine 141:        self.seq_len_cached = None\nLine 142:        self.cos_cached = None\nLine 143:        self.sin_cached = None\nLine 145:    def forward(self, x):\nLine 146:        seq_len = x.shape[1]\nLine 147:        if seq_len != self.seq_len_cached:\nLine 148:            self.seq_len_cached = seq_len\nLine 149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\nLine 150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\nLine 151:            self.cos_cached = freqs.cos().bfloat16()\nLine 152:            self.sin_cached = freqs.sin().bfloat16()\nLine 153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\nLine 155:def apply_rotary_emb(x, cos, sin):\nLine 156:    assert x.ndim == 4 # multihead attention\nLine 157:    d = x.shape[3]//2\nLine 158:    x1 = x[..., :d]\nLine 159:    x2 = x[..., d:]\nLine 160:    y1 = x1 * cos + x2 * sin\nLine 161:    y2 = x1 * (-sin) + x2 * cos\nLine 162:    return torch.cat([y1, y2], 3).type_as(x)\nLine 164:class CausalSelfAttention(nn.Module):\nLine 166:    def __init__(self, config):\nLine 167:        super().__init__()\nLine 168:        self.n_head = config.n_head\nLine 169:        self.n_embd = config.n_embd\nLine 170:        self.head_dim = self.n_embd // self.n_head\nLine 171:        assert self.n_embd % self.n_head == 0\nLine 172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 175:        # output projection\nLine 176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\nLine 178:        self.rotary = Rotary(self.head_dim)\nLine 180:    def forward(self, x):\nLine 181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\nLine 182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\nLine 183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\nLine 184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\nLine 185:        cos, sin = self.rotary(q)\nLine 186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\nLine 187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\nLine 188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\nLine 189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\nLine 190:        y = self.c_proj(y)\nLine 191:        return y\nLine 193:class MLP(nn.Module):\nLine 195:    def __init__(self, config):\nLine 196:        super().__init__()\nLine 197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\nLine 198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\nLine 199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\nLine 201:    def forward(self, x):\nLine 202:        x = self.c_fc(x)\nLine 203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\nLine 204:        x = self.c_proj(x)\nLine 205:        return x\nLine 207:class Block(nn.Module):\nLine 209:    def __init__(self, config):\nLine 210:        super().__init__()\nLine 211:        self.attn = CausalSelfAttention(config)\nLine 212:        self.mlp = MLP(config)\nLine 214:    def forward(self, x):\nLine 215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\nLine 216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\nLine 217:        return x\nLine 219:# -----------------------------------------------------------------------------\nLine 220:# The main GPT-2 model\nLine 223:class ModelConfig:\nLine 224:    vocab_size : int = 50304\nLine 225:    n_layer : int = 12\nLine 226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\nLine 227:    n_embd : int = 768\nLine 229:class Model(nn.Module):\nLine 231:    def __init__(self, config):\nLine 232:        super().__init__()\nLine 233:        self.config = config\nLine 235:        self.transformer = nn.ModuleDict(dict(\nLine 236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\nLine 237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\nLine 238:        ))\nLine 239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\nLine 240:        self.lm_head.weight.data.zero_() # @Grad62304977\nLine 242:    def forward(self, idx, targets=None, return_logits=True):\nLine 244:        # forward the GPT model itself\nLine 245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\nLine 246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\nLine 247:        for block in self.transformer.h:\nLine 248:            x = block(x)\nLine 249:        x = F.rms_norm(x, (x.size(-1),))\nLine 251:        if targets is not None:\nLine 252:            # if we are given some desired targets also calculate the loss\nLine 253:            logits = self.lm_head(x)\nLine 254:            logits = logits.float() # use tf32/fp32 for logits\nLine 255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\nLine 256:        else:\nLine 257:            # inference-time mini-optimization: only forward the lm_head on the very last position\nLine 258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\nLine 259:            logits = logits.float() # use tf32/fp32 for logits\nLine 260:            loss = None\nLine 262:        # there are performance reasons why not returning logits is prudent, if not needed\nLine 263:        if not return_logits:\nLine 264:            logits = None\nLine 266:        return logits, loss\nLine 267:    \nLine 268:    def get_config(self):\nLine 269:        return self.config\nLine 271:# -----------------------------------------------------------------------------\nLine 272:# Our own simple Distributed Data Loader\nLine 274:def _peek_data_shard(filename):\nLine 275:    # only reads the header, returns header data\nLine 276:    with open(filename, \"rb\") as f:\nLine 277:        # first read the header, which is 256 int32 integers (4 bytes each)\nLine 278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\nLine 279:    if header[0] != 20240520:\nLine 280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\nLine 281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\nLine 282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\nLine 283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\nLine 284:        exit(1)\nLine 285:    assert header[1] == 1, \"unsupported version\"\nLine 286:    ntok = header[2] # number of tokens (claimed)\nLine 287:    return ntok # for now just return the number of tokens\nLine 289:def _load_data_shard(filename):\nLine 290:    with open(filename, \"rb\") as f:\nLine 291:        # first read the header, which is 256 int32 integers (4 bytes each)\nLine 292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\nLine 293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\nLine 294:        assert header[1] == 1, \"unsupported version\"\nLine 295:        ntok = header[2] # number of tokens (claimed)\nLine 296:        # the rest of it are tokens, stored as uint16\nLine 297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\nLine 298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\nLine 299:    return tokens\nLine 301:class DistributedDataLoader:\nLine 302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\nLine 303:        self.process_rank = process_rank\nLine 304:        self.num_processes = num_processes\nLine 305:        self.B = B\nLine 306:        self.T = T\nLine 308:        # glob files that match the pattern\nLine 309:        self.files = sorted(glob.glob(filename_pattern))\nLine 310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\nLine 312:        # load and validate all data shards, count number of tokens in total\nLine 313:        ntok_total = 0\nLine 314:        for fname in self.files:\nLine 315:            shard_ntok = _peek_data_shard(fname)\nLine 316:            assert shard_ntok >= num_processes * B * T + 1\nLine 317:            ntok_total += int(shard_ntok)\nLine 318:        self.ntok_total = ntok_total\nLine 320:        # kick things off\nLine 321:        self.reset()\nLine 323:    def reset(self):\nLine 324:        self.current_shard = 0\nLine 325:        self.current_position = self.process_rank * self.B * self.T\nLine 326:        self.tokens = _load_data_shard(self.files[self.current_shard])\nLine 328:    def advance(self): # advance to next data shard\nLine 329:        self.current_shard = (self.current_shard + 1) % len(self.files)\nLine 330:        self.current_position = self.process_rank * self.B * self.T\nLine 331:        self.tokens = _load_data_shard(self.files[self.current_shard])\nLine 333:    def next_batch(self):\nLine 334:        B = self.B\nLine 335:        T = self.T\nLine 336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\nLine 337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\nLine 338:        x = (buf[:-1]).view(B, T) # inputs\nLine 339:        y = (buf[1:]).view(B, T) # targets\nLine 340:        # advance current position and load next shard if necessary\nLine 341:        self.current_position += B * T * self.num_processes\nLine 342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\nLine 343:            self.advance()\nLine 344:        return x.cuda(), y.cuda()\nLine 346:# -----------------------------------------------------------------------------\nLine 347:# int main\nLine 350:class Hyperparameters:\nLine 351:    # data hyperparams\nLine 352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\nLine 353:    # optimization hyperparams\nLine 354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\nLine 355:    device_batch_size : int = 64 # batch size, in sequences, per device\nLine 356:    sequence_length : int = 1024 # sequence length, in tokens\nLine 357:    num_iterations : int = 10000 # number of iterations to run\nLine 358:    warmup_iters : int = 200\nLine 359:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\nLine 360:    weight_decay : float = 0\nLine 361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\nLine 364:def main():\nLine 365:    with open(sys.argv[0]) as f:\nLine 366:        code = f.read() # read the code of this file ASAP, for logging\nLine 367:    args = Hyperparameters()\nLine 368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\nLine 370:    # set up DDP (distributed data parallel). torchrun sets this env variable\nLine 371:    assert torch.cuda.is_available()\nLine 372:    dist.init_process_group(backend='nccl')\nLine 373:    ddp_rank = int(os.environ['RANK'])\nLine 374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\nLine 375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\nLine 376:    device = f'cuda:{ddp_local_rank}'\nLine 377:    torch.cuda.set_device(device)\nLine 378:    print(f\"using device: {device}\")\nLine 379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\nLine 381:    # convenience variables\nLine 382:    B, T = args.device_batch_size, args.sequence_length\nLine 383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\nLine 384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\nLine 385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\nLine 387:    # load tokens\nLine 388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\nLine 389:    if master_process:\nLine 390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\nLine 391:    x, y = train_loader.next_batch()\nLine 393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\nLine 394:    # this originates from Karpathy's experiments.\nLine 395:    num_vocab = 50304\nLine 396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\nLine 397:    model = model.cuda()\nLine 398:    if hasattr(config, \"coordinate_descent_tuning\"):\nLine 399:        config.coordinate_descent_tuning = True # suggested by @Chillee\nLine 400:    model = torch.compile(model)\nLine 401:    # here we wrap model into DDP container\nLine 402:    model = DDP(model, device_ids=[ddp_local_rank])\nLine 403:    raw_model = model.module # always contains the \"raw\" unwrapped model\nLine 404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\nLine 406:    # init the optimizer(s)\nLine 407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\nLine 408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\nLine 409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\nLine 410:    optimizers = [optimizer1, optimizer2, optimizer3]\nLine 411:    # learning rate decay scheduler (linear warmup and warmdown)\nLine 412:    def get_lr(it):\nLine 413:        assert it <= args.num_iterations\nLine 414:        # 1) linear warmup for warmup_iters steps\nLine 415:        if it < args.warmup_iters:\nLine 416:            return (it+1) / args.warmup_iters\nLine 417:        # 2) constant lr for a while\nLine 418:        elif it < args.num_iterations - args.warmdown_iters:\nLine 419:            return 1.0\nLine 420:        # 3) linear warmdown\nLine 421:        else:\nLine 422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\nLine 423:            return decay_ratio\nLine 424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nLine 426:    # begin logging\nLine 427:    if master_process:\nLine 428:        run_id = str(uuid.uuid4())\nLine 429:        logdir = 'logs/%s/' % run_id\nLine 430:        os.makedirs(logdir, exist_ok=True)\nLine 431:        logfile = 'logs/%s.txt' % run_id\nLine 432:        # create the log file\nLine 433:        with open(logfile, \"w\") as f:\nLine 434:            # begin the log by printing this file (the Python code)\nLine 435:            f.write('='*100 + '\\n')\nLine 436:            f.write(code)\nLine 437:            f.write('='*100 + '\\n')\nLine 438:            # log information about the hardware/software environment this is running on\nLine 439:            # and print the full `nvidia-smi` to file\nLine 440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\nLine 441:            import subprocess\nLine 442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\nLine 443:            f.write(f'{result.stdout}\\n')\nLine 444:            f.write('='*100 + '\\n')\nLine 446:    training_time_ms = 0\nLine 447:    # start the clock\nLine 448:    torch.cuda.synchronize()\nLine 449:    t0 = time.time()\nLine 450:    # begin training\nLine 451:    train_loader.reset()\nLine 452:    for step in range(args.num_iterations + 1):\nLine 453:        last_step = (step == args.num_iterations)\nLine 454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\nLine 455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\nLine 456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\nLine 457:        if step == 10:\nLine 458:            training_time_ms = 0\nLine 459:            t0 = time.time()\nLine 460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\nLine 462:        if last_step:\nLine 463:            break\nLine 465:        # --------------- TRAINING SECTION BEGIN -----------------\nLine 466:        model.train()\nLine 467:        for i in range(1, train_accumulation_steps+1):\nLine 468:            # forward pass\nLine 469:            with ctx:\nLine 470:                _, loss = model(x, y, return_logits=False)\nLine 471:                train_loss = loss.detach()\nLine 472:            # advance the dataset for the next batch\nLine 473:            x, y = train_loader.next_batch()\nLine 474:            # backward pass\nLine 475:            if i < train_accumulation_steps:\nLine 476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\nLine 477:                    loss.backward()\nLine 478:            else:\nLine 479:                loss.backward() # just sync on the last step\nLine 480:        for p in model.parameters():\nLine 481:            p.grad /= train_accumulation_steps\nLine 482:        # step the optimizers and schedulers\nLine 483:        for opt, sched in zip(optimizers, schedulers):\nLine 484:            opt.step()\nLine 485:            sched.step()\nLine 486:        # null the gradients\nLine 487:        model.zero_grad(set_to_none=True)\nLine 488:        # --------------- TRAINING SECTION END -------------------\nLine 489:        # everything that follows now is just diagnostics, prints, logging, etc.\nLine 491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\nLine 492:        if master_process:\nLine 493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\nLine 494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\nLine 495:            with open(logfile, \"a\") as f:\nLine 496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\nLine 498:    torch.save(model.state_dict(), \"model.pt\")\nLine 499:    with open(\"model_config.pt\", \"wb\") as f:\nLine 500:        pickle.dump(raw_model.get_config(), f)\nLine 501:    f.close()\nLine 503:    if master_process:\nLine 504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\nLine 506:    # -------------------------------------------------------------------------\nLine 507:    # clean up nice\nLine 508:    dist.destroy_process_group()\nLine 510:if __name__ == \"__main__\":\nLine 511:    main()\nEnd of matches for \"\\s\" in /home/agent/workspace/baseline.py"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe `search_file` command seems to have interpreted `\\s` as a regex for whitespace, returning many irrelevant lines. Let's use `grep` to search for the literal sequence `\\s` which might be causing the `SyntaxWarning`.\n```bash\ngrep -n '\\\\s' baseline.py\n```",
      "thought": "DISCUSSION\nThe `search_file` command seems to have interpreted `\\s` as a regex for whitespace, returning many irrelevant lines. Let's use `grep` to search for the literal sequence `\\s` which might be causing the `SyntaxWarning`.\n",
      "action": "grep -n '\\\\s' baseline.py\n",
      "execution_time": 0.3008492160588503,
      "observation": "45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, `grep` found the problematic sequence `\\s` on line 45 within a comment. This is likely what triggers the `SyntaxWarning`. Since it's in a comment, it won't affect the execution. I'll proceed with modifying the learning rate schedule, ignoring the warning that the editor might raise.\n\nFirst, insert `import math` again.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nOkay, `grep` found the problematic sequence `\\s` on line 45 within a comment. This is likely what triggers the `SyntaxWarning`. Since it's in a comment, it won't affect the execution. I'll proceed with modifying the learning rate schedule, ignoring the warning that the editor might raise.\n\nFirst, insert `import math` again.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "execution_time": 0.8731093863025308,
      "observation": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe interface is still preventing the insertion due to the syntax warning, even though it's in a comment and harmless. Let's try fixing the source of the warning by editing line 45. I'll replace `\\sim` with `~` which is a common representation for \"distributed as\".\n```bash\nedit 45:45\n    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe interface is still preventing the insertion due to the syntax warning, even though it's in a comment and harmless. Let's try fixing the source of the warning by editing line 45. I'll replace `\\sim` with `~` which is a common representation for \"distributed as\".\n",
      "action": "edit 45:45\n    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\nend_of_edit\n",
      "execution_time": 0.773119393736124,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nGreat, the edit to fix the comment on line 45 worked. Now the syntax checker should allow us to insert the `import math` statement. I'll place it after the other standard library imports.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nGreat, the edit to fix the comment on line 45 worked. Now the syntax checker should allow us to insert the `import math` statement. I'll place it after the other standard library imports.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "execution_time": 0.7626609206199646,
      "observation": "[File: /home/agent/workspace/baseline.py (512 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n138:\n139:    def __init__(self, dim, base=10000):\n140:        super().__init__()\n141:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n142:        self.seq_len_cached = None\n143:        self.cos_cached = None\n144:        self.sin_cached = None\n145:\n146:    def forward(self, x):\n147:        seq_len = x.shape[1]\n148:        if seq_len != self.seq_len_cached:\n149:            self.seq_len_cached = seq_len\n150:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n151:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n152:            self.cos_cached = freqs.cos().bfloat16()\n153:            self.sin_cached = freqs.sin().bfloat16()\n154:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n155:\n156:def apply_rotary_emb(x, cos, sin):\n157:    assert x.ndim == 4 # multihead attention\n158:    d = x.shape[3]//2\n159:    x1 = x[..., :d]\n160:    x2 = x[..., d:]\n161:    y1 = x1 * cos + x2 * sin\n162:    y2 = x1 * (-sin) + x2 * cos\n163:    return torch.cat([y1, y2], 3).type_as(x)\n164:\n165:class CausalSelfAttention(nn.Module):\n166:\n167:    def __init__(self, config):\n168:        super().__init__()\n169:        self.n_head = config.n_head\n170:        self.n_embd = config.n_embd\n171:        self.head_dim = self.n_embd // self.n_head\n172:        assert self.n_embd % self.n_head == 0\n173:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n176:        # output projection\n177:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n178:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n179:        self.rotary = Rotary(self.head_dim)\n180:\n181:    def forward(self, x):\n182:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n183:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n184:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n185:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n186:        cos, sin = self.rotary(q)\n187:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n188:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n189:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n190:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n191:        y = self.c_proj(y)\n192:        return y\n193:\n194:class MLP(nn.Module):\n195:\n196:    def __init__(self, config):\n197:        super().__init__()\n198:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n199:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n200:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n201:\n202:    def forward(self, x):\n203:        x = self.c_fc(x)\n204:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n205:        x = self.c_proj(x)\n206:        return x\n207:\n208:class Block(nn.Module):\n209:\n210:    def __init__(self, config):\n211:        super().__init__()\n212:        self.attn = CausalSelfAttention(config)\n213:        self.mlp = MLP(config)\n214:\n215:    def forward(self, x):\n216:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n217:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n218:        return x\n219:\n220:# -----------------------------------------------------------------------------\n221:# The main GPT-2 model\n222:\n223:@dataclass\n224:class ModelConfig:\n225:    vocab_size : int = 50304\n226:    n_layer : int = 12\n227:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n228:    n_embd : int = 768\n229:\n230:class Model(nn.Module):\n231:\n232:    def __init__(self, config):\n233:        super().__init__()\n234:        self.config = config\n235:\n236:        self.transformer = nn.ModuleDict(dict(\n237:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n238:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n239:        ))\n240:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n241:        self.lm_head.weight.data.zero_() # @Grad62304977\n242:\n243:    def forward(self, idx, targets=None, return_logits=True):\n244:\n245:        # forward the GPT model itself\n246:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n247:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n248:        for block in self.transformer.h:\n249:            x = block(x)\n250:        x = F.rms_norm(x, (x.size(-1),))\n251:\n252:        if targets is not None:\n253:            # if we are given some desired targets also calculate the loss\n254:            logits = self.lm_head(x)\n255:            logits = logits.float() # use tf32/fp32 for logits\n256:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n257:        else:\n258:            # inference-time mini-optimization: only forward the lm_head on the very last position\n259:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n260:            logits = logits.float() # use tf32/fp32 for logits\n261:            loss = None\n262:\n263:        # there are performance reasons why not returning logits is prudent, if not needed\n264:        if not return_logits:\n265:            logits = None\n266:\n267:        return logits, loss\n268:    \n269:    def get_config(self):\n270:        return self.config\n271:\n272:# -----------------------------------------------------------------------------\n273:# Our own simple Distributed Data Loader\n274:\n275:def _peek_data_shard(filename):\n276:    # only reads the header, returns header data\n277:    with open(filename, \"rb\") as f:\n278:        # first read the header, which is 256 int32 integers (4 bytes each)\n279:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n280:    if header[0] != 20240520:\n281:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n282:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n283:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n284:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n285:        exit(1)\n286:    assert header[1] == 1, \"unsupported version\"\n287:    ntok = header[2] # number of tokens (claimed)\n288:    return ntok # for now just return the number of tokens\n289:\n290:def _load_data_shard(filename):\n291:    with open(filename, \"rb\") as f:\n292:        # first read the header, which is 256 int32 integers (4 bytes each)\n293:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n294:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n295:        assert header[1] == 1, \"unsupported version\"\n296:        ntok = header[2] # number of tokens (claimed)\n297:        # the rest of it are tokens, stored as uint16\n298:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n299:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n300:    return tokens\n301:\n302:class DistributedDataLoader:\n303:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n304:        self.process_rank = process_rank\n305:        self.num_processes = num_processes\n306:        self.B = B\n307:        self.T = T\n308:\n309:        # glob files that match the pattern\n310:        self.files = sorted(glob.glob(filename_pattern))\n311:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n312:\n313:        # load and validate all data shards, count number of tokens in total\n314:        ntok_total = 0\n315:        for fname in self.files:\n316:            shard_ntok = _peek_data_shard(fname)\n317:            assert shard_ntok >= num_processes * B * T + 1\n318:            ntok_total += int(shard_ntok)\n319:        self.ntok_total = ntok_total\n320:\n321:        # kick things off\n322:        self.reset()\n323:\n324:    def reset(self):\n325:        self.current_shard = 0\n326:        self.current_position = self.process_rank * self.B * self.T\n327:        self.tokens = _load_data_shard(self.files[self.current_shard])\n328:\n329:    def advance(self): # advance to next data shard\n330:        self.current_shard = (self.current_shard + 1) % len(self.files)\n331:        self.current_position = self.process_rank * self.B * self.T\n332:        self.tokens = _load_data_shard(self.files[self.current_shard])\n333:\n334:    def next_batch(self):\n335:        B = self.B\n336:        T = self.T\n337:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n338:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n339:        x = (buf[:-1]).view(B, T) # inputs\n340:        y = (buf[1:]).view(B, T) # targets\n341:        # advance current position and load next shard if necessary\n342:        self.current_position += B * T * self.num_processes\n343:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n344:            self.advance()\n345:        return x.cuda(), y.cuda()\n346:\n347:# -----------------------------------------------------------------------------\n348:# int main\n349:\n350:@dataclass\n351:class Hyperparameters:\n352:    # data hyperparams\n353:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n354:    # optimization hyperparams\n355:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n356:    device_batch_size : int = 64 # batch size, in sequences, per device\n357:    sequence_length : int = 1024 # sequence length, in tokens\n358:    num_iterations : int = 10000 # number of iterations to run\n359:    warmup_iters : int = 200\n360:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n361:    weight_decay : float = 0\n362:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n363:\n364:\n365:def main():\n366:    with open(sys.argv[0]) as f:\n367:        code = f.read() # read the code of this file ASAP, for logging\n368:    args = Hyperparameters()\n369:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n370:\n371:    # set up DDP (distributed data parallel). torchrun sets this env variable\n372:    assert torch.cuda.is_available()\n373:    dist.init_process_group(backend='nccl')\n374:    ddp_rank = int(os.environ['RANK'])\n375:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n376:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n377:    device = f'cuda:{ddp_local_rank}'\n378:    torch.cuda.set_device(device)\n379:    print(f\"using device: {device}\")\n380:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n381:\n382:    # convenience variables\n383:    B, T = args.device_batch_size, args.sequence_length\n384:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n385:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n386:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n387:\n388:    # load tokens\n389:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n390:    if master_process:\n391:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n392:    x, y = train_loader.next_batch()\n393:\n394:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n395:    # this originates from Karpathy's experiments.\n396:    num_vocab = 50304\n397:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n398:    model = model.cuda()\n399:    if hasattr(config, \"coordinate_descent_tuning\"):\n400:        config.coordinate_descent_tuning = True # suggested by @Chillee\n401:    model = torch.compile(model)\n402:    # here we wrap model into DDP container\n403:    model = DDP(model, device_ids=[ddp_local_rank])\n404:    raw_model = model.module # always contains the \"raw\" unwrapped model\n405:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n406:\n407:    # init the optimizer(s)\n408:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    optimizers = [optimizer1, optimizer2, optimizer3]\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        # 1) linear warmup for warmup_iters steps\n416:        if it < args.warmup_iters:\n417:            return (it+1) / args.warmup_iters\n418:        # 2) constant lr for a while\n419:        elif it < args.num_iterations - args.warmdown_iters:\n420:            return 1.0\n421:        # 3) linear warmdown\n422:        else:\n423:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n424:            return decay_ratio\n425:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n426:\n427:    # begin logging\n428:    if master_process:\n429:        run_id = str(uuid.uuid4())\n430:        logdir = 'logs/%s/' % run_id\n431:        os.makedirs(logdir, exist_ok=True)\n432:        logfile = 'logs/%s.txt' % run_id\n433:        # create the log file\n434:        with open(logfile, \"w\") as f:\n435:            # begin the log by printing this file (the Python code)\n436:            f.write('='*100 + '\\n')\n437:            f.write(code)\n438:            f.write('='*100 + '\\n')\n439:            # log information about the hardware/software environment this is running on\n440:            # and print the full `nvidia-smi` to file\n441:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n442:            import subprocess\n443:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n444:            f.write(f'{result.stdout}\\n')\n445:            f.write('='*100 + '\\n')\n446:\n447:    training_time_ms = 0\n448:    # start the clock\n449:    torch.cuda.synchronize()\n450:    t0 = time.time()\n451:    # begin training\n452:    train_loader.reset()\n453:    for step in range(args.num_iterations + 1):\n454:        last_step = (step == args.num_iterations)\n455:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n456:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n457:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n458:        if step == 10:\n459:            training_time_ms = 0\n460:            t0 = time.time()\n461:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n462:\n463:        if last_step:\n464:            break\n465:\n466:        # --------------- TRAINING SECTION BEGIN -----------------\n467:        model.train()\n468:        for i in range(1, train_accumulation_steps+1):\n469:            # forward pass\n470:            with ctx:\n471:                _, loss = model(x, y, return_logits=False)\n472:                train_loss = loss.detach()\n473:            # advance the dataset for the next batch\n474:            x, y = train_loader.next_batch()\n475:            # backward pass\n476:            if i < train_accumulation_steps:\n477:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n478:                    loss.backward()\n479:            else:\n480:                loss.backward() # just sync on the last step\n481:        for p in model.parameters():\n482:            p.grad /= train_accumulation_steps\n483:        # step the optimizers and schedulers\n484:        for opt, sched in zip(optimizers, schedulers):\n485:            opt.step()\n486:            sched.step()\n487:        # null the gradients\n488:        model.zero_grad(set_to_none=True)\n489:        # --------------- TRAINING SECTION END -------------------\n490:        # everything that follows now is just diagnostics, prints, logging, etc.\n491:\n492:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n493:        if master_process:\n494:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n495:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n496:            with open(logfile, \"a\") as f:\n497:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n498:\n499:    torch.save(model.state_dict(), \"model.pt\")\n500:    with open(\"model_config.pt\", \"wb\") as f:\n501:        pickle.dump(raw_model.get_config(), f)\n502:    f.close()\n503:\n504:    if master_process:\n505:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n506:\n507:    # -------------------------------------------------------------------------\n508:    # clean up nice\n509:    dist.destroy_process_group()\n510:\n511:if __name__ == \"__main__\":\n512:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nSuccess! The `import math` statement was inserted correctly after fixing the comment.\n\nNow, let's modify the `get_lr` function (lines 413-425) to implement the cosine decay schedule after the linear warmup. The current implementation has a linear warmup, then a constant phase, and finally a linear warmdown. We want linear warmup followed immediately by cosine decay down to a minimum learning rate (let's aim for 10% of the peak LR).\n```bash\nedit 413:425\n    def get_lr(it):\n        assert it <= args.num_iterations\n        lr = 0.0 # default value, can adjust min LR later if needed\n        \n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            lr = (it+1) / args.warmup_iters\n        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n        elif it < args.num_iterations:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n        else: \n             lr = 0.1 # Keep min lr at the very end\n\n        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n\n    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nend_of_edit\n```",
      "thought": "DISCUSSION\nSuccess! The `import math` statement was inserted correctly after fixing the comment.\n\nNow, let's modify the `get_lr` function (lines 413-425) to implement the cosine decay schedule after the linear warmup. The current implementation has a linear warmup, then a constant phase, and finally a linear warmdown. We want linear warmup followed immediately by cosine decay down to a minimum learning rate (let's aim for 10% of the peak LR).\n",
      "action": "edit 413:425\n    def get_lr(it):\n        assert it <= args.num_iterations\n        lr = 0.0 # default value, can adjust min LR later if needed\n        \n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            lr = (it+1) / args.warmup_iters\n        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n        elif it < args.num_iterations:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n        else: \n             lr = 0.1 # Keep min lr at the very end\n\n        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n\n    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nend_of_edit\n",
      "execution_time": 0.8751359563320875,
      "observation": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- E111 indentation is not a multiple of 4\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (520 lines total)]\n(406 more lines above)\n407:    # init the optimizer(s)\n408:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    optimizers = [optimizer1, optimizer2, optimizer3]\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        lr = 0.0 # default value, can adjust min LR later if needed\n416:        \n417:        # 1) linear warmup for warmup_iters steps\n418:        if it < args.warmup_iters:\n419:            lr = (it+1) / args.warmup_iters\n420:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n421:        elif it < args.num_iterations:\n422:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n423:            assert 0 <= decay_ratio <= 1\n424:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n425:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n426:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n427:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n428:        else: \n429:             lr = 0.1 # Keep min lr at the very end\n430:\n431:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n432:\n433:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n434:\n435:    # begin logging\n436:    if master_process:\n437:        run_id = str(uuid.uuid4())\n(83 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(407 more lines above)\n408:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    optimizers = [optimizer1, optimizer2, optimizer3]\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        # 1) linear warmup for warmup_iters steps\n416:        if it < args.warmup_iters:\n417:            return (it+1) / args.warmup_iters\n418:        # 2) constant lr for a while\n419:        elif it < args.num_iterations - args.warmdown_iters:\n420:            return 1.0\n421:        # 3) linear warmdown\n422:        else:\n423:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n424:            return decay_ratio\n425:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n426:\n427:    # begin logging\n428:    if master_process:\n429:        run_id = str(uuid.uuid4())\n430:        logdir = 'logs/%s/' % run_id\n(82 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAh, it seems I made an indentation error in the `else` block. The line `lr = 0.1` was indented with 5 spaces instead of 8. Let's fix that and try the edit again.\n```bash\nedit 413:425\n    def get_lr(it):\n        assert it <= args.num_iterations\n        lr = 0.0 # default value, can adjust min LR later if needed\n        \n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            lr = (it+1) / args.warmup_iters\n        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n        elif it < args.num_iterations:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n        else: \n            lr = 0.1 # Keep min lr at the very end\n\n        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n\n    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, it seems I made an indentation error in the `else` block. The line `lr = 0.1` was indented with 5 spaces instead of 8. Let's fix that and try the edit again.\n",
      "action": "edit 413:425\n    def get_lr(it):\n        assert it <= args.num_iterations\n        lr = 0.0 # default value, can adjust min LR later if needed\n        \n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            lr = (it+1) / args.warmup_iters\n        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n        elif it < args.num_iterations:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n        else: \n            lr = 0.1 # Keep min lr at the very end\n\n        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n\n    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nend_of_edit\n",
      "execution_time": 0.7831694921478629,
      "observation": "[File: /home/agent/workspace/baseline.py (520 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n138:\n139:    def __init__(self, dim, base=10000):\n140:        super().__init__()\n141:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n142:        self.seq_len_cached = None\n143:        self.cos_cached = None\n144:        self.sin_cached = None\n145:\n146:    def forward(self, x):\n147:        seq_len = x.shape[1]\n148:        if seq_len != self.seq_len_cached:\n149:            self.seq_len_cached = seq_len\n150:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n151:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n152:            self.cos_cached = freqs.cos().bfloat16()\n153:            self.sin_cached = freqs.sin().bfloat16()\n154:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n155:\n156:def apply_rotary_emb(x, cos, sin):\n157:    assert x.ndim == 4 # multihead attention\n158:    d = x.shape[3]//2\n159:    x1 = x[..., :d]\n160:    x2 = x[..., d:]\n161:    y1 = x1 * cos + x2 * sin\n162:    y2 = x1 * (-sin) + x2 * cos\n163:    return torch.cat([y1, y2], 3).type_as(x)\n164:\n165:class CausalSelfAttention(nn.Module):\n166:\n167:    def __init__(self, config):\n168:        super().__init__()\n169:        self.n_head = config.n_head\n170:        self.n_embd = config.n_embd\n171:        self.head_dim = self.n_embd // self.n_head\n172:        assert self.n_embd % self.n_head == 0\n173:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n176:        # output projection\n177:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n178:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n179:        self.rotary = Rotary(self.head_dim)\n180:\n181:    def forward(self, x):\n182:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n183:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n184:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n185:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n186:        cos, sin = self.rotary(q)\n187:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n188:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n189:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n190:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n191:        y = self.c_proj(y)\n192:        return y\n193:\n194:class MLP(nn.Module):\n195:\n196:    def __init__(self, config):\n197:        super().__init__()\n198:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n199:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n200:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n201:\n202:    def forward(self, x):\n203:        x = self.c_fc(x)\n204:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n205:        x = self.c_proj(x)\n206:        return x\n207:\n208:class Block(nn.Module):\n209:\n210:    def __init__(self, config):\n211:        super().__init__()\n212:        self.attn = CausalSelfAttention(config)\n213:        self.mlp = MLP(config)\n214:\n215:    def forward(self, x):\n216:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n217:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n218:        return x\n219:\n220:# -----------------------------------------------------------------------------\n221:# The main GPT-2 model\n222:\n223:@dataclass\n224:class ModelConfig:\n225:    vocab_size : int = 50304\n226:    n_layer : int = 12\n227:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n228:    n_embd : int = 768\n229:\n230:class Model(nn.Module):\n231:\n232:    def __init__(self, config):\n233:        super().__init__()\n234:        self.config = config\n235:\n236:        self.transformer = nn.ModuleDict(dict(\n237:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n238:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n239:        ))\n240:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n241:        self.lm_head.weight.data.zero_() # @Grad62304977\n242:\n243:    def forward(self, idx, targets=None, return_logits=True):\n244:\n245:        # forward the GPT model itself\n246:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n247:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n248:        for block in self.transformer.h:\n249:            x = block(x)\n250:        x = F.rms_norm(x, (x.size(-1),))\n251:\n252:        if targets is not None:\n253:            # if we are given some desired targets also calculate the loss\n254:            logits = self.lm_head(x)\n255:            logits = logits.float() # use tf32/fp32 for logits\n256:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n257:        else:\n258:            # inference-time mini-optimization: only forward the lm_head on the very last position\n259:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n260:            logits = logits.float() # use tf32/fp32 for logits\n261:            loss = None\n262:\n263:        # there are performance reasons why not returning logits is prudent, if not needed\n264:        if not return_logits:\n265:            logits = None\n266:\n267:        return logits, loss\n268:    \n269:    def get_config(self):\n270:        return self.config\n271:\n272:# -----------------------------------------------------------------------------\n273:# Our own simple Distributed Data Loader\n274:\n275:def _peek_data_shard(filename):\n276:    # only reads the header, returns header data\n277:    with open(filename, \"rb\") as f:\n278:        # first read the header, which is 256 int32 integers (4 bytes each)\n279:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n280:    if header[0] != 20240520:\n281:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n282:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n283:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n284:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n285:        exit(1)\n286:    assert header[1] == 1, \"unsupported version\"\n287:    ntok = header[2] # number of tokens (claimed)\n288:    return ntok # for now just return the number of tokens\n289:\n290:def _load_data_shard(filename):\n291:    with open(filename, \"rb\") as f:\n292:        # first read the header, which is 256 int32 integers (4 bytes each)\n293:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n294:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n295:        assert header[1] == 1, \"unsupported version\"\n296:        ntok = header[2] # number of tokens (claimed)\n297:        # the rest of it are tokens, stored as uint16\n298:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n299:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n300:    return tokens\n301:\n302:class DistributedDataLoader:\n303:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n304:        self.process_rank = process_rank\n305:        self.num_processes = num_processes\n306:        self.B = B\n307:        self.T = T\n308:\n309:        # glob files that match the pattern\n310:        self.files = sorted(glob.glob(filename_pattern))\n311:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n312:\n313:        # load and validate all data shards, count number of tokens in total\n314:        ntok_total = 0\n315:        for fname in self.files:\n316:            shard_ntok = _peek_data_shard(fname)\n317:            assert shard_ntok >= num_processes * B * T + 1\n318:            ntok_total += int(shard_ntok)\n319:        self.ntok_total = ntok_total\n320:\n321:        # kick things off\n322:        self.reset()\n323:\n324:    def reset(self):\n325:        self.current_shard = 0\n326:        self.current_position = self.process_rank * self.B * self.T\n327:        self.tokens = _load_data_shard(self.files[self.current_shard])\n328:\n329:    def advance(self): # advance to next data shard\n330:        self.current_shard = (self.current_shard + 1) % len(self.files)\n331:        self.current_position = self.process_rank * self.B * self.T\n332:        self.tokens = _load_data_shard(self.files[self.current_shard])\n333:\n334:    def next_batch(self):\n335:        B = self.B\n336:        T = self.T\n337:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n338:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n339:        x = (buf[:-1]).view(B, T) # inputs\n340:        y = (buf[1:]).view(B, T) # targets\n341:        # advance current position and load next shard if necessary\n342:        self.current_position += B * T * self.num_processes\n343:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n344:            self.advance()\n345:        return x.cuda(), y.cuda()\n346:\n347:# -----------------------------------------------------------------------------\n348:# int main\n349:\n350:@dataclass\n351:class Hyperparameters:\n352:    # data hyperparams\n353:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n354:    # optimization hyperparams\n355:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n356:    device_batch_size : int = 64 # batch size, in sequences, per device\n357:    sequence_length : int = 1024 # sequence length, in tokens\n358:    num_iterations : int = 10000 # number of iterations to run\n359:    warmup_iters : int = 200\n360:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n361:    weight_decay : float = 0\n362:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n363:\n364:\n365:def main():\n366:    with open(sys.argv[0]) as f:\n367:        code = f.read() # read the code of this file ASAP, for logging\n368:    args = Hyperparameters()\n369:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n370:\n371:    # set up DDP (distributed data parallel). torchrun sets this env variable\n372:    assert torch.cuda.is_available()\n373:    dist.init_process_group(backend='nccl')\n374:    ddp_rank = int(os.environ['RANK'])\n375:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n376:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n377:    device = f'cuda:{ddp_local_rank}'\n378:    torch.cuda.set_device(device)\n379:    print(f\"using device: {device}\")\n380:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n381:\n382:    # convenience variables\n383:    B, T = args.device_batch_size, args.sequence_length\n384:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n385:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n386:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n387:\n388:    # load tokens\n389:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n390:    if master_process:\n391:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n392:    x, y = train_loader.next_batch()\n393:\n394:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n395:    # this originates from Karpathy's experiments.\n396:    num_vocab = 50304\n397:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n398:    model = model.cuda()\n399:    if hasattr(config, \"coordinate_descent_tuning\"):\n400:        config.coordinate_descent_tuning = True # suggested by @Chillee\n401:    model = torch.compile(model)\n402:    # here we wrap model into DDP container\n403:    model = DDP(model, device_ids=[ddp_local_rank])\n404:    raw_model = model.module # always contains the \"raw\" unwrapped model\n405:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n406:\n407:    # init the optimizer(s)\n408:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    optimizers = [optimizer1, optimizer2, optimizer3]\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        lr = 0.0 # default value, can adjust min LR later if needed\n416:        \n417:        # 1) linear warmup for warmup_iters steps\n418:        if it < args.warmup_iters:\n419:            lr = (it+1) / args.warmup_iters\n420:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n421:        elif it < args.num_iterations:\n422:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n423:            assert 0 <= decay_ratio <= 1\n424:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n425:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n426:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n427:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n428:        else: \n429:            lr = 0.1 # Keep min lr at the very end\n430:\n431:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n432:\n433:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n434:\n435:    # begin logging\n436:    if master_process:\n437:        run_id = str(uuid.uuid4())\n438:        logdir = 'logs/%s/' % run_id\n439:        os.makedirs(logdir, exist_ok=True)\n440:        logfile = 'logs/%s.txt' % run_id\n441:        # create the log file\n442:        with open(logfile, \"w\") as f:\n443:            # begin the log by printing this file (the Python code)\n444:            f.write('='*100 + '\\n')\n445:            f.write(code)\n446:            f.write('='*100 + '\\n')\n447:            # log information about the hardware/software environment this is running on\n448:            # and print the full `nvidia-smi` to file\n449:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n450:            import subprocess\n451:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n452:            f.write(f'{result.stdout}\\n')\n453:            f.write('='*100 + '\\n')\n454:\n455:    training_time_ms = 0\n456:    # start the clock\n457:    torch.cuda.synchronize()\n458:    t0 = time.time()\n459:    # begin training\n460:    train_loader.reset()\n461:    for step in range(args.num_iterations + 1):\n462:        last_step = (step == args.num_iterations)\n463:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n464:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n465:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n466:        if step == 10:\n467:            training_time_ms = 0\n468:            t0 = time.time()\n469:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n470:\n471:        if last_step:\n472:            break\n473:\n474:        # --------------- TRAINING SECTION BEGIN -----------------\n475:        model.train()\n476:        for i in range(1, train_accumulation_steps+1):\n477:            # forward pass\n478:            with ctx:\n479:                _, loss = model(x, y, return_logits=False)\n480:                train_loss = loss.detach()\n481:            # advance the dataset for the next batch\n482:            x, y = train_loader.next_batch()\n483:            # backward pass\n484:            if i < train_accumulation_steps:\n485:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n486:                    loss.backward()\n487:            else:\n488:                loss.backward() # just sync on the last step\n489:        for p in model.parameters():\n490:            p.grad /= train_accumulation_steps\n491:        # step the optimizers and schedulers\n492:        for opt, sched in zip(optimizers, schedulers):\n493:            opt.step()\n494:            sched.step()\n495:        # null the gradients\n496:        model.zero_grad(set_to_none=True)\n497:        # --------------- TRAINING SECTION END -------------------\n498:        # everything that follows now is just diagnostics, prints, logging, etc.\n499:\n500:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n501:        if master_process:\n502:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n503:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n504:            with open(logfile, \"a\") as f:\n505:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n506:\n507:    torch.save(model.state_dict(), \"model.pt\")\n508:    with open(\"model_config.pt\", \"wb\") as f:\n509:        pickle.dump(raw_model.get_config(), f)\n510:    f.close()\n511:\n512:    if master_process:\n513:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n514:\n515:    # -------------------------------------------------------------------------\n516:    # clean up nice\n517:    dist.destroy_process_group()\n518:\n519:if __name__ == \"__main__\":\n520:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nExcellent, the indentation is fixed and the `get_lr` function now implements the desired linear warmup + cosine decay schedule.\n\nThe script uses separate optimizers for different parts of the model:\n- `optimizer1`: Adam for `wte` (embeddings) with LR 0.3\n- `optimizer2`: Adam for `lm_head` (output layer) with LR 0.002\n- `optimizer3`: Muon for transformer blocks (`h`) with LR 0.02\n\nThe `LambdaLR` scheduler applies the same relative factor (returned by `get_lr`) to the base learning rate of each optimizer. This means the embedding LR will warm up to 0.3 and decay to 0.03, the LM head LR will warm up to 0.002 and decay to 0.0002, and the transformer blocks LR will warm up to 0.02 and decay to 0.002. These LRs seem a bit arbitrary and the high LR for embeddings (0.3) is unusual.\n\nLet's try a more standard approach using a single AdamW optimizer for the entire model with a typical learning rate and weight decay. I'll remove the Muon optimizer and the separate Adam optimizers.\n\nFirst, remove the Muon optimizer definition and usage (lines 31-132 and the import on line 27).\n```bash\nedit 27:132\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ntorch._dynamo.config.suppress_errors = True\n\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n```",
      "thought": "DISCUSSION\nExcellent, the indentation is fixed and the `get_lr` function now implements the desired linear warmup + cosine decay schedule.\n\nThe script uses separate optimizers for different parts of the model:\n- `optimizer1`: Adam for `wte` (embeddings) with LR 0.3\n- `optimizer2`: Adam for `lm_head` (output layer) with LR 0.002\n- `optimizer3`: Muon for transformer blocks (`h`) with LR 0.02\n\nThe `LambdaLR` scheduler applies the same relative factor (returned by `get_lr`) to the base learning rate of each optimizer. This means the embedding LR will warm up to 0.3 and decay to 0.03, the LM head LR will warm up to 0.002 and decay to 0.0002, and the transformer blocks LR will warm up to 0.02 and decay to 0.002. These LRs seem a bit arbitrary and the high LR for embeddings (0.3) is unusual.\n\nLet's try a more standard approach using a single AdamW optimizer for the entire model with a typical learning rate and weight decay. I'll remove the Muon optimizer and the separate Adam optimizers.\n\nFirst, remove the Muon optimizer definition and usage (lines 31-132 and the import on line 27).\n",
      "action": "edit 27:132\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ntorch._dynamo.config.suppress_errors = True\n\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n",
      "execution_time": 0.9680211069062352,
      "observation": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'Muon'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (505 lines total)]\n(20 more lines above)\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n(384 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (520 lines total)]\n(21 more lines above)\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n(383 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nYou're right. I deleted the `Muon` class definition but didn't remove its usage in the `main` function where the optimizers are initialized. Let's first delete the Muon optimizer definition and related code (lines 31-132).\n```bash\nedit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n```",
      "thought": "DISCUSSION\nYou're right. I deleted the `Muon` class definition but didn't remove its usage in the `main` function where the optimizers are initialized. Let's first delete the Muon optimizer definition and related code (lines 31-132).\n",
      "action": "edit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n",
      "execution_time": 0.890966447070241,
      "observation": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'Muon'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (505 lines total)]\n(24 more lines above)\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n(384 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (520 lines total)]\n(25 more lines above)\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n(383 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nRight, I need to remove the usage of `Muon` as well. Let's perform the deletion of the `Muon` class and its related functions first (lines 31-132). Then, I'll adjust the optimizer initialization section.\n```bash\nedit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n```",
      "thought": "DISCUSSION\nRight, I need to remove the usage of `Muon` as well. Let's perform the deletion of the `Muon` class and its related functions first (lines 31-132). Then, I'll adjust the optimizer initialization section.\n",
      "action": "edit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n",
      "execution_time": 0.8788815904408693,
      "observation": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'Muon'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (505 lines total)]\n(24 more lines above)\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n(384 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (520 lines total)]\n(25 more lines above)\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n(383 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nMy apologies, the syntax checker is being very strict. It flags the error even if the definition is removed but the usage remains later in the file.\n\nLet's adjust the strategy. I will first edit the optimizer initialization section (lines 407-411, which will become 305-309 after the Muon deletion) to use a single AdamW optimizer, and *then* remove the Muon class definition.\n\nI will replace lines 407-411 with a single AdamW optimizer. A typical learning rate for GPT-style models is around 3e-4. I'll use AdamW with `lr=3e-4`, `betas=(0.9, 0.95)`, `weight_decay=0.1`, and `fused=True` (since the baseline used it).\n```bash\nedit 407:411\n    # init the optimizer\n    # Use AdamW for all parameters\n    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\nend_of_edit\n```",
      "thought": "DISCUSSION\nMy apologies, the syntax checker is being very strict. It flags the error even if the definition is removed but the usage remains later in the file.\n\nLet's adjust the strategy. I will first edit the optimizer initialization section (lines 407-411, which will become 305-309 after the Muon deletion) to use a single AdamW optimizer, and *then* remove the Muon class definition.\n\nI will replace lines 407-411 with a single AdamW optimizer. A typical learning rate for GPT-style models is around 3e-4. I'll use AdamW with `lr=3e-4`, `betas=(0.9, 0.95)`, `weight_decay=0.1`, and `fused=True` (since the baseline used it).\n",
      "action": "edit 407:411\n    # init the optimizer\n    # Use AdamW for all parameters\n    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\nend_of_edit\n",
      "execution_time": 0.8038778640329838,
      "observation": "[File: /home/agent/workspace/baseline.py (519 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n138:\n139:    def __init__(self, dim, base=10000):\n140:        super().__init__()\n141:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n142:        self.seq_len_cached = None\n143:        self.cos_cached = None\n144:        self.sin_cached = None\n145:\n146:    def forward(self, x):\n147:        seq_len = x.shape[1]\n148:        if seq_len != self.seq_len_cached:\n149:            self.seq_len_cached = seq_len\n150:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n151:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n152:            self.cos_cached = freqs.cos().bfloat16()\n153:            self.sin_cached = freqs.sin().bfloat16()\n154:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n155:\n156:def apply_rotary_emb(x, cos, sin):\n157:    assert x.ndim == 4 # multihead attention\n158:    d = x.shape[3]//2\n159:    x1 = x[..., :d]\n160:    x2 = x[..., d:]\n161:    y1 = x1 * cos + x2 * sin\n162:    y2 = x1 * (-sin) + x2 * cos\n163:    return torch.cat([y1, y2], 3).type_as(x)\n164:\n165:class CausalSelfAttention(nn.Module):\n166:\n167:    def __init__(self, config):\n168:        super().__init__()\n169:        self.n_head = config.n_head\n170:        self.n_embd = config.n_embd\n171:        self.head_dim = self.n_embd // self.n_head\n172:        assert self.n_embd % self.n_head == 0\n173:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n176:        # output projection\n177:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n178:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n179:        self.rotary = Rotary(self.head_dim)\n180:\n181:    def forward(self, x):\n182:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n183:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n184:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n185:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n186:        cos, sin = self.rotary(q)\n187:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n188:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n189:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n190:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n191:        y = self.c_proj(y)\n192:        return y\n193:\n194:class MLP(nn.Module):\n195:\n196:    def __init__(self, config):\n197:        super().__init__()\n198:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n199:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n200:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n201:\n202:    def forward(self, x):\n203:        x = self.c_fc(x)\n204:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n205:        x = self.c_proj(x)\n206:        return x\n207:\n208:class Block(nn.Module):\n209:\n210:    def __init__(self, config):\n211:        super().__init__()\n212:        self.attn = CausalSelfAttention(config)\n213:        self.mlp = MLP(config)\n214:\n215:    def forward(self, x):\n216:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n217:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n218:        return x\n219:\n220:# -----------------------------------------------------------------------------\n221:# The main GPT-2 model\n222:\n223:@dataclass\n224:class ModelConfig:\n225:    vocab_size : int = 50304\n226:    n_layer : int = 12\n227:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n228:    n_embd : int = 768\n229:\n230:class Model(nn.Module):\n231:\n232:    def __init__(self, config):\n233:        super().__init__()\n234:        self.config = config\n235:\n236:        self.transformer = nn.ModuleDict(dict(\n237:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n238:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n239:        ))\n240:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n241:        self.lm_head.weight.data.zero_() # @Grad62304977\n242:\n243:    def forward(self, idx, targets=None, return_logits=True):\n244:\n245:        # forward the GPT model itself\n246:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n247:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n248:        for block in self.transformer.h:\n249:            x = block(x)\n250:        x = F.rms_norm(x, (x.size(-1),))\n251:\n252:        if targets is not None:\n253:            # if we are given some desired targets also calculate the loss\n254:            logits = self.lm_head(x)\n255:            logits = logits.float() # use tf32/fp32 for logits\n256:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n257:        else:\n258:            # inference-time mini-optimization: only forward the lm_head on the very last position\n259:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n260:            logits = logits.float() # use tf32/fp32 for logits\n261:            loss = None\n262:\n263:        # there are performance reasons why not returning logits is prudent, if not needed\n264:        if not return_logits:\n265:            logits = None\n266:\n267:        return logits, loss\n268:    \n269:    def get_config(self):\n270:        return self.config\n271:\n272:# -----------------------------------------------------------------------------\n273:# Our own simple Distributed Data Loader\n274:\n275:def _peek_data_shard(filename):\n276:    # only reads the header, returns header data\n277:    with open(filename, \"rb\") as f:\n278:        # first read the header, which is 256 int32 integers (4 bytes each)\n279:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n280:    if header[0] != 20240520:\n281:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n282:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n283:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n284:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n285:        exit(1)\n286:    assert header[1] == 1, \"unsupported version\"\n287:    ntok = header[2] # number of tokens (claimed)\n288:    return ntok # for now just return the number of tokens\n289:\n290:def _load_data_shard(filename):\n291:    with open(filename, \"rb\") as f:\n292:        # first read the header, which is 256 int32 integers (4 bytes each)\n293:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n294:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n295:        assert header[1] == 1, \"unsupported version\"\n296:        ntok = header[2] # number of tokens (claimed)\n297:        # the rest of it are tokens, stored as uint16\n298:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n299:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n300:    return tokens\n301:\n302:class DistributedDataLoader:\n303:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n304:        self.process_rank = process_rank\n305:        self.num_processes = num_processes\n306:        self.B = B\n307:        self.T = T\n308:\n309:        # glob files that match the pattern\n310:        self.files = sorted(glob.glob(filename_pattern))\n311:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n312:\n313:        # load and validate all data shards, count number of tokens in total\n314:        ntok_total = 0\n315:        for fname in self.files:\n316:            shard_ntok = _peek_data_shard(fname)\n317:            assert shard_ntok >= num_processes * B * T + 1\n318:            ntok_total += int(shard_ntok)\n319:        self.ntok_total = ntok_total\n320:\n321:        # kick things off\n322:        self.reset()\n323:\n324:    def reset(self):\n325:        self.current_shard = 0\n326:        self.current_position = self.process_rank * self.B * self.T\n327:        self.tokens = _load_data_shard(self.files[self.current_shard])\n328:\n329:    def advance(self): # advance to next data shard\n330:        self.current_shard = (self.current_shard + 1) % len(self.files)\n331:        self.current_position = self.process_rank * self.B * self.T\n332:        self.tokens = _load_data_shard(self.files[self.current_shard])\n333:\n334:    def next_batch(self):\n335:        B = self.B\n336:        T = self.T\n337:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n338:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n339:        x = (buf[:-1]).view(B, T) # inputs\n340:        y = (buf[1:]).view(B, T) # targets\n341:        # advance current position and load next shard if necessary\n342:        self.current_position += B * T * self.num_processes\n343:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n344:            self.advance()\n345:        return x.cuda(), y.cuda()\n346:\n347:# -----------------------------------------------------------------------------\n348:# int main\n349:\n350:@dataclass\n351:class Hyperparameters:\n352:    # data hyperparams\n353:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n354:    # optimization hyperparams\n355:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n356:    device_batch_size : int = 64 # batch size, in sequences, per device\n357:    sequence_length : int = 1024 # sequence length, in tokens\n358:    num_iterations : int = 10000 # number of iterations to run\n359:    warmup_iters : int = 200\n360:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n361:    weight_decay : float = 0\n362:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n363:\n364:\n365:def main():\n366:    with open(sys.argv[0]) as f:\n367:        code = f.read() # read the code of this file ASAP, for logging\n368:    args = Hyperparameters()\n369:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n370:\n371:    # set up DDP (distributed data parallel). torchrun sets this env variable\n372:    assert torch.cuda.is_available()\n373:    dist.init_process_group(backend='nccl')\n374:    ddp_rank = int(os.environ['RANK'])\n375:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n376:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n377:    device = f'cuda:{ddp_local_rank}'\n378:    torch.cuda.set_device(device)\n379:    print(f\"using device: {device}\")\n380:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n381:\n382:    # convenience variables\n383:    B, T = args.device_batch_size, args.sequence_length\n384:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n385:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n386:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n387:\n388:    # load tokens\n389:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n390:    if master_process:\n391:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n392:    x, y = train_loader.next_batch()\n393:\n394:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n395:    # this originates from Karpathy's experiments.\n396:    num_vocab = 50304\n397:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n398:    model = model.cuda()\n399:    if hasattr(config, \"coordinate_descent_tuning\"):\n400:        config.coordinate_descent_tuning = True # suggested by @Chillee\n401:    model = torch.compile(model)\n402:    # here we wrap model into DDP container\n403:    model = DDP(model, device_ids=[ddp_local_rank])\n404:    raw_model = model.module # always contains the \"raw\" unwrapped model\n405:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n406:\n407:    # init the optimizer\n408:    # Use AdamW for all parameters\n409:    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n410:    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        lr = 0.0 # default value, can adjust min LR later if needed\n415:        \n416:        # 1) linear warmup for warmup_iters steps\n417:        if it < args.warmup_iters:\n418:            lr = (it+1) / args.warmup_iters\n419:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n420:        elif it < args.num_iterations:\n421:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n422:            assert 0 <= decay_ratio <= 1\n423:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n424:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n425:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n426:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n427:        else: \n428:            lr = 0.1 # Keep min lr at the very end\n429:\n430:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n431:\n432:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n433:\n434:    # begin logging\n435:    if master_process:\n436:        run_id = str(uuid.uuid4())\n437:        logdir = 'logs/%s/' % run_id\n438:        os.makedirs(logdir, exist_ok=True)\n439:        logfile = 'logs/%s.txt' % run_id\n440:        # create the log file\n441:        with open(logfile, \"w\") as f:\n442:            # begin the log by printing this file (the Python code)\n443:            f.write('='*100 + '\\n')\n444:            f.write(code)\n445:            f.write('='*100 + '\\n')\n446:            # log information about the hardware/software environment this is running on\n447:            # and print the full `nvidia-smi` to file\n448:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n449:            import subprocess\n450:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n451:            f.write(f'{result.stdout}\\n')\n452:            f.write('='*100 + '\\n')\n453:\n454:    training_time_ms = 0\n455:    # start the clock\n456:    torch.cuda.synchronize()\n457:    t0 = time.time()\n458:    # begin training\n459:    train_loader.reset()\n460:    for step in range(args.num_iterations + 1):\n461:        last_step = (step == args.num_iterations)\n462:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n463:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n464:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n465:        if step == 10:\n466:            training_time_ms = 0\n467:            t0 = time.time()\n468:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n469:\n470:        if last_step:\n471:            break\n472:\n473:        # --------------- TRAINING SECTION BEGIN -----------------\n474:        model.train()\n475:        for i in range(1, train_accumulation_steps+1):\n476:            # forward pass\n477:            with ctx:\n478:                _, loss = model(x, y, return_logits=False)\n479:                train_loss = loss.detach()\n480:            # advance the dataset for the next batch\n481:            x, y = train_loader.next_batch()\n482:            # backward pass\n483:            if i < train_accumulation_steps:\n484:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n485:                    loss.backward()\n486:            else:\n487:                loss.backward() # just sync on the last step\n488:        for p in model.parameters():\n489:            p.grad /= train_accumulation_steps\n490:        # step the optimizers and schedulers\n491:        for opt, sched in zip(optimizers, schedulers):\n492:            opt.step()\n493:            sched.step()\n494:        # null the gradients\n495:        model.zero_grad(set_to_none=True)\n496:        # --------------- TRAINING SECTION END -------------------\n497:        # everything that follows now is just diagnostics, prints, logging, etc.\n498:\n499:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n500:        if master_process:\n501:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n502:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n503:            with open(logfile, \"a\") as f:\n504:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n505:\n506:    torch.save(model.state_dict(), \"model.pt\")\n507:    with open(\"model_config.pt\", \"wb\") as f:\n508:        pickle.dump(raw_model.get_config(), f)\n509:    f.close()\n510:\n511:    if master_process:\n512:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n513:\n514:    # -------------------------------------------------------------------------\n515:    # clean up nice\n516:    dist.destroy_process_group()\n517:\n518:if __name__ == \"__main__\":\n519:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the optimizer initialization has been successfully changed to use a single AdamW optimizer with standard hyperparameters (LR=3e-4, WD=0.1).\n\nNow I can safely remove the unused `Muon` optimizer class definition and its helper functions (lines 31-132).\n```bash\nedit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the optimizer initialization has been successfully changed to use a single AdamW optimizer with standard hyperparameters (LR=3e-4, WD=0.1).\n\nNow I can safely remove the unused `Muon` optimizer class definition and its helper functions (lines 31-132).\n",
      "action": "edit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n",
      "execution_time": 0.7808182137086987,
      "observation": "[File: /home/agent/workspace/baseline.py (504 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n122:class Rotary(torch.nn.Module):\n123:\n124:    def __init__(self, dim, base=10000):\n125:        super().__init__()\n126:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n127:        self.seq_len_cached = None\n128:        self.cos_cached = None\n129:        self.sin_cached = None\n130:\n131:    def forward(self, x):\n132:        seq_len = x.shape[1]\n133:        if seq_len != self.seq_len_cached:\n134:            self.seq_len_cached = seq_len\n135:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n136:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n137:            self.cos_cached = freqs.cos().bfloat16()\n138:            self.sin_cached = freqs.sin().bfloat16()\n139:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n140:\n141:def apply_rotary_emb(x, cos, sin):\n142:    assert x.ndim == 4 # multihead attention\n143:    d = x.shape[3]//2\n144:    x1 = x[..., :d]\n145:    x2 = x[..., d:]\n146:    y1 = x1 * cos + x2 * sin\n147:    y2 = x1 * (-sin) + x2 * cos\n148:    return torch.cat([y1, y2], 3).type_as(x)\n149:\n150:class CausalSelfAttention(nn.Module):\n151:\n152:    def __init__(self, config):\n153:        super().__init__()\n154:        self.n_head = config.n_head\n155:        self.n_embd = config.n_embd\n156:        self.head_dim = self.n_embd // self.n_head\n157:        assert self.n_embd % self.n_head == 0\n158:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n159:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n160:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n161:        # output projection\n162:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n163:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n164:        self.rotary = Rotary(self.head_dim)\n165:\n166:    def forward(self, x):\n167:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n168:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n169:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n170:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n171:        cos, sin = self.rotary(q)\n172:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n173:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n174:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n175:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n176:        y = self.c_proj(y)\n177:        return y\n178:\n179:class MLP(nn.Module):\n180:\n181:    def __init__(self, config):\n182:        super().__init__()\n183:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n184:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n185:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n186:\n187:    def forward(self, x):\n188:        x = self.c_fc(x)\n189:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n190:        x = self.c_proj(x)\n191:        return x\n192:\n193:class Block(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.attn = CausalSelfAttention(config)\n198:        self.mlp = MLP(config)\n199:\n200:    def forward(self, x):\n201:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n202:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n203:        return x\n204:\n205:# -----------------------------------------------------------------------------\n206:# The main GPT-2 model\n207:\n208:@dataclass\n209:class ModelConfig:\n210:    vocab_size : int = 50304\n211:    n_layer : int = 12\n212:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n213:    n_embd : int = 768\n214:\n215:class Model(nn.Module):\n216:\n217:    def __init__(self, config):\n218:        super().__init__()\n219:        self.config = config\n220:\n221:        self.transformer = nn.ModuleDict(dict(\n222:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n223:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n224:        ))\n225:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n226:        self.lm_head.weight.data.zero_() # @Grad62304977\n227:\n228:    def forward(self, idx, targets=None, return_logits=True):\n229:\n230:        # forward the GPT model itself\n231:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n232:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n233:        for block in self.transformer.h:\n234:            x = block(x)\n235:        x = F.rms_norm(x, (x.size(-1),))\n236:\n237:        if targets is not None:\n238:            # if we are given some desired targets also calculate the loss\n239:            logits = self.lm_head(x)\n240:            logits = logits.float() # use tf32/fp32 for logits\n241:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n242:        else:\n243:            # inference-time mini-optimization: only forward the lm_head on the very last position\n244:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n245:            logits = logits.float() # use tf32/fp32 for logits\n246:            loss = None\n247:\n248:        # there are performance reasons why not returning logits is prudent, if not needed\n249:        if not return_logits:\n250:            logits = None\n251:\n252:        return logits, loss\n253:    \n254:    def get_config(self):\n255:        return self.config\n256:\n257:# -----------------------------------------------------------------------------\n258:# Our own simple Distributed Data Loader\n259:\n260:def _peek_data_shard(filename):\n261:    # only reads the header, returns header data\n262:    with open(filename, \"rb\") as f:\n263:        # first read the header, which is 256 int32 integers (4 bytes each)\n264:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n265:    if header[0] != 20240520:\n266:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n267:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n268:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n269:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n270:        exit(1)\n271:    assert header[1] == 1, \"unsupported version\"\n272:    ntok = header[2] # number of tokens (claimed)\n273:    return ntok # for now just return the number of tokens\n274:\n275:def _load_data_shard(filename):\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n280:        assert header[1] == 1, \"unsupported version\"\n281:        ntok = header[2] # number of tokens (claimed)\n282:        # the rest of it are tokens, stored as uint16\n283:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n284:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n285:    return tokens\n286:\n287:class DistributedDataLoader:\n288:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n289:        self.process_rank = process_rank\n290:        self.num_processes = num_processes\n291:        self.B = B\n292:        self.T = T\n293:\n294:        # glob files that match the pattern\n295:        self.files = sorted(glob.glob(filename_pattern))\n296:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n297:\n298:        # load and validate all data shards, count number of tokens in total\n299:        ntok_total = 0\n300:        for fname in self.files:\n301:            shard_ntok = _peek_data_shard(fname)\n302:            assert shard_ntok >= num_processes * B * T + 1\n303:            ntok_total += int(shard_ntok)\n304:        self.ntok_total = ntok_total\n305:\n306:        # kick things off\n307:        self.reset()\n308:\n309:    def reset(self):\n310:        self.current_shard = 0\n311:        self.current_position = self.process_rank * self.B * self.T\n312:        self.tokens = _load_data_shard(self.files[self.current_shard])\n313:\n314:    def advance(self): # advance to next data shard\n315:        self.current_shard = (self.current_shard + 1) % len(self.files)\n316:        self.current_position = self.process_rank * self.B * self.T\n317:        self.tokens = _load_data_shard(self.files[self.current_shard])\n318:\n319:    def next_batch(self):\n320:        B = self.B\n321:        T = self.T\n322:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n323:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n324:        x = (buf[:-1]).view(B, T) # inputs\n325:        y = (buf[1:]).view(B, T) # targets\n326:        # advance current position and load next shard if necessary\n327:        self.current_position += B * T * self.num_processes\n328:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n329:            self.advance()\n330:        return x.cuda(), y.cuda()\n331:\n332:# -----------------------------------------------------------------------------\n333:# int main\n334:\n335:@dataclass\n336:class Hyperparameters:\n337:    # data hyperparams\n338:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n339:    # optimization hyperparams\n340:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n341:    device_batch_size : int = 64 # batch size, in sequences, per device\n342:    sequence_length : int = 1024 # sequence length, in tokens\n343:    num_iterations : int = 10000 # number of iterations to run\n344:    warmup_iters : int = 200\n345:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n346:    weight_decay : float = 0\n347:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n348:\n349:\n350:def main():\n351:    with open(sys.argv[0]) as f:\n352:        code = f.read() # read the code of this file ASAP, for logging\n353:    args = Hyperparameters()\n354:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n355:\n356:    # set up DDP (distributed data parallel). torchrun sets this env variable\n357:    assert torch.cuda.is_available()\n358:    dist.init_process_group(backend='nccl')\n359:    ddp_rank = int(os.environ['RANK'])\n360:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n361:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n362:    device = f'cuda:{ddp_local_rank}'\n363:    torch.cuda.set_device(device)\n364:    print(f\"using device: {device}\")\n365:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n366:\n367:    # convenience variables\n368:    B, T = args.device_batch_size, args.sequence_length\n369:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n370:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n371:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n372:\n373:    # load tokens\n374:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n375:    if master_process:\n376:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n377:    x, y = train_loader.next_batch()\n378:\n379:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n380:    # this originates from Karpathy's experiments.\n381:    num_vocab = 50304\n382:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n383:    model = model.cuda()\n384:    if hasattr(config, \"coordinate_descent_tuning\"):\n385:        config.coordinate_descent_tuning = True # suggested by @Chillee\n386:    model = torch.compile(model)\n387:    # here we wrap model into DDP container\n388:    model = DDP(model, device_ids=[ddp_local_rank])\n389:    raw_model = model.module # always contains the \"raw\" unwrapped model\n390:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n391:\n392:    # init the optimizer\n393:    # Use AdamW for all parameters\n394:    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n395:    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\n396:    # learning rate decay scheduler (linear warmup and warmdown)\n397:    def get_lr(it):\n398:        assert it <= args.num_iterations\n399:        lr = 0.0 # default value, can adjust min LR later if needed\n400:        \n401:        # 1) linear warmup for warmup_iters steps\n402:        if it < args.warmup_iters:\n403:            lr = (it+1) / args.warmup_iters\n404:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n405:        elif it < args.num_iterations:\n406:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n407:            assert 0 <= decay_ratio <= 1\n408:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n409:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n410:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n411:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n412:        else: \n413:            lr = 0.1 # Keep min lr at the very end\n414:\n415:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n416:\n417:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n418:\n419:    # begin logging\n420:    if master_process:\n421:        run_id = str(uuid.uuid4())\n422:        logdir = 'logs/%s/' % run_id\n423:        os.makedirs(logdir, exist_ok=True)\n424:        logfile = 'logs/%s.txt' % run_id\n425:        # create the log file\n426:        with open(logfile, \"w\") as f:\n427:            # begin the log by printing this file (the Python code)\n428:            f.write('='*100 + '\\n')\n429:            f.write(code)\n430:            f.write('='*100 + '\\n')\n431:            # log information about the hardware/software environment this is running on\n432:            # and print the full `nvidia-smi` to file\n433:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n434:            import subprocess\n435:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n436:            f.write(f'{result.stdout}\\n')\n437:            f.write('='*100 + '\\n')\n438:\n439:    training_time_ms = 0\n440:    # start the clock\n441:    torch.cuda.synchronize()\n442:    t0 = time.time()\n443:    # begin training\n444:    train_loader.reset()\n445:    for step in range(args.num_iterations + 1):\n446:        last_step = (step == args.num_iterations)\n447:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n448:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n449:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n450:        if step == 10:\n451:            training_time_ms = 0\n452:            t0 = time.time()\n453:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n454:\n455:        if last_step:\n456:            break\n457:\n458:        # --------------- TRAINING SECTION BEGIN -----------------\n459:        model.train()\n460:        for i in range(1, train_accumulation_steps+1):\n461:            # forward pass\n462:            with ctx:\n463:                _, loss = model(x, y, return_logits=False)\n464:                train_loss = loss.detach()\n465:            # advance the dataset for the next batch\n466:            x, y = train_loader.next_batch()\n467:            # backward pass\n468:            if i < train_accumulation_steps:\n469:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n470:                    loss.backward()\n471:            else:\n472:                loss.backward() # just sync on the last step\n473:        for p in model.parameters():\n474:            p.grad /= train_accumulation_steps\n475:        # step the optimizers and schedulers\n476:        for opt, sched in zip(optimizers, schedulers):\n477:            opt.step()\n478:            sched.step()\n479:        # null the gradients\n480:        model.zero_grad(set_to_none=True)\n481:        # --------------- TRAINING SECTION END -------------------\n482:        # everything that follows now is just diagnostics, prints, logging, etc.\n483:\n484:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n485:        if master_process:\n486:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n487:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n488:            with open(logfile, \"a\") as f:\n489:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n490:\n491:    torch.save(model.state_dict(), \"model.pt\")\n492:    with open(\"model_config.pt\", \"wb\") as f:\n493:        pickle.dump(raw_model.get_config(), f)\n494:    f.close()\n495:\n496:    if master_process:\n497:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n498:\n499:    # -------------------------------------------------------------------------\n500:    # clean up nice\n501:    dist.destroy_process_group()\n502:\n503:if __name__ == \"__main__\":\n504:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nGreat, the `Muon` optimizer code has been removed. The script now uses a single AdamW optimizer with a reasonable learning rate (3e-4), weight decay (0.1), and a cosine learning rate schedule over 10,000 iterations.\n\nThe baseline script uses a device batch size of 64 and sequence length 1024. This seems reasonable. It also uses gradient accumulation to achieve a larger effective batch size (`args.batch_size = args.device_batch_size * torch.cuda.device_count()`). The accumulation steps are calculated automatically based on the number of GPUs.\n\nThe model is GPT-2 small (12 layers, 6 heads, 768 embedding dim). This is a good starting point.\n\nI'll run the training script now to see how it performs. The baseline only ran for 500 steps, so 10,000 steps with a proper LR schedule should yield much better results. The script expects to be run with `torchrun`. I need to check the number of available GPUs first. Let's check `nvidia-smi`.\n```bash\nnvidia-smi\n```",
      "thought": "DISCUSSION\nGreat, the `Muon` optimizer code has been removed. The script now uses a single AdamW optimizer with a reasonable learning rate (3e-4), weight decay (0.1), and a cosine learning rate schedule over 10,000 iterations.\n\nThe baseline script uses a device batch size of 64 and sequence length 1024. This seems reasonable. It also uses gradient accumulation to achieve a larger effective batch size (`args.batch_size = args.device_batch_size * torch.cuda.device_count()`). The accumulation steps are calculated automatically based on the number of GPUs.\n\nThe model is GPT-2 small (12 layers, 6 heads, 768 embedding dim). This is a good starting point.\n\nI'll run the training script now to see how it performs. The baseline only ran for 500 steps, so 10,000 steps with a proper LR schedule should yield much better results. The script expects to be run with `torchrun`. I need to check the number of available GPUs first. Let's check `nvidia-smi`.\n",
      "action": "nvidia-smi\n",
      "execution_time": 0.4529312951490283,
      "observation": "Wed Mar 26 03:17:56 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA H100                    On  | 00000000:CD:00.0 Off |                    0 |\n| N/A   30C    P0              65W / 500W |      3MiB / 97871MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA H100                    On  | 00000000:DD:00.0 Off |                    0 |\n| N/A   28C    P0              69W / 500W |      3MiB / 97871MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, we have 2 NVIDIA H100 GPUs available. I will now run the modified `baseline.py` script using `torchrun` with `nproc_per_node=2`. This will train the model for 10,000 iterations using AdamW and the cosine decay learning rate schedule. The script will save `model.pt` and `model_config.pt` upon completion.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nOkay, we have 2 NVIDIA H100 GPUs available. I will now run the modified `baseline.py` script using `torchrun` with `nproc_per_node=2`. This will train the model for 10,000 iterations using AdamW and the cosine decay learning rate schedule. The script will save `model.pt` and `model_config.pt` upon completion.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 4.388471122831106,
      "observation": "W0326 03:20:02.567000 1418 site-packages/torch/distributed/run.py:793] \nW0326 03:20:02.567000 1418 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:20:02.567000 1418 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:20:02.567000 1418 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 504, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 374, in main\n[rank0]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 296, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\n[rank0]:[W326 03:20:04.785838386 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 504, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 374, in main\n[rank1]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 296, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\nW0326 03:20:05.080000 1418 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1485 closing signal SIGTERM\nE0326 03:20:05.244000 1418 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 1484) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-26_03:20:05\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 1484)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe script failed because it couldn't find the training data files matching the pattern `./data/fineweb10B/fineweb_train_*.bin`.\n\nLet's check the `data` directory to see the correct path for the FineWeb dataset.\n```bash\nls -R data\n```",
      "thought": "DISCUSSION\nThe script failed because it couldn't find the training data files matching the pattern `./data/fineweb10B/fineweb_train_*.bin`.\n\nLet's check the `data` directory to see the correct path for the FineWeb dataset.\n",
      "action": "ls -R data\n",
      "execution_time": 0.30113491602241993,
      "observation": "data:\nfineweb108/\ndata/fineweb108:\nfineweb_train_000001.bin*\nfineweb_train_000002.bin*\nfineweb_train_000003.bin*\nfineweb_train_000004.bin*\nfineweb_train_000005.bin*\nfineweb_train_000006.bin*\nfineweb_train_000007.bin*\nfineweb_train_000008.bin*\nfineweb_train_000009.bin*\nfineweb_train_000010.bin*\nfineweb_train_000011.bin*\nfineweb_train_000012.bin*\nfineweb_train_000013.bin*\nfineweb_train_000014.bin*\nfineweb_train_000015.bin*\nfineweb_train_000016.bin*\nfineweb_train_000017.bin*\nfineweb_train_000018.bin*\nfineweb_val_000000.bin*"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAh, it seems the directory is named `fineweb108` instead of `fineweb10B`. Let's correct the `input_bin` path in the `Hyperparameters` dataclass (line 338).\n```bash\nedit 338:338\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, it seems the directory is named `fineweb108` instead of `fineweb10B`. Let's correct the `input_bin` path in the `Hyperparameters` dataclass (line 338).\n",
      "action": "edit 338:338\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "execution_time": 0.8025103406980634,
      "observation": "[File: /home/agent/workspace/baseline.py (504 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n122:class Rotary(torch.nn.Module):\n123:\n124:    def __init__(self, dim, base=10000):\n125:        super().__init__()\n126:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n127:        self.seq_len_cached = None\n128:        self.cos_cached = None\n129:        self.sin_cached = None\n130:\n131:    def forward(self, x):\n132:        seq_len = x.shape[1]\n133:        if seq_len != self.seq_len_cached:\n134:            self.seq_len_cached = seq_len\n135:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n136:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n137:            self.cos_cached = freqs.cos().bfloat16()\n138:            self.sin_cached = freqs.sin().bfloat16()\n139:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n140:\n141:def apply_rotary_emb(x, cos, sin):\n142:    assert x.ndim == 4 # multihead attention\n143:    d = x.shape[3]//2\n144:    x1 = x[..., :d]\n145:    x2 = x[..., d:]\n146:    y1 = x1 * cos + x2 * sin\n147:    y2 = x1 * (-sin) + x2 * cos\n148:    return torch.cat([y1, y2], 3).type_as(x)\n149:\n150:class CausalSelfAttention(nn.Module):\n151:\n152:    def __init__(self, config):\n153:        super().__init__()\n154:        self.n_head = config.n_head\n155:        self.n_embd = config.n_embd\n156:        self.head_dim = self.n_embd // self.n_head\n157:        assert self.n_embd % self.n_head == 0\n158:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n159:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n160:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n161:        # output projection\n162:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n163:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n164:        self.rotary = Rotary(self.head_dim)\n165:\n166:    def forward(self, x):\n167:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n168:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n169:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n170:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n171:        cos, sin = self.rotary(q)\n172:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n173:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n174:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n175:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n176:        y = self.c_proj(y)\n177:        return y\n178:\n179:class MLP(nn.Module):\n180:\n181:    def __init__(self, config):\n182:        super().__init__()\n183:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n184:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n185:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n186:\n187:    def forward(self, x):\n188:        x = self.c_fc(x)\n189:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n190:        x = self.c_proj(x)\n191:        return x\n192:\n193:class Block(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.attn = CausalSelfAttention(config)\n198:        self.mlp = MLP(config)\n199:\n200:    def forward(self, x):\n201:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n202:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n203:        return x\n204:\n205:# -----------------------------------------------------------------------------\n206:# The main GPT-2 model\n207:\n208:@dataclass\n209:class ModelConfig:\n210:    vocab_size : int = 50304\n211:    n_layer : int = 12\n212:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n213:    n_embd : int = 768\n214:\n215:class Model(nn.Module):\n216:\n217:    def __init__(self, config):\n218:        super().__init__()\n219:        self.config = config\n220:\n221:        self.transformer = nn.ModuleDict(dict(\n222:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n223:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n224:        ))\n225:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n226:        self.lm_head.weight.data.zero_() # @Grad62304977\n227:\n228:    def forward(self, idx, targets=None, return_logits=True):\n229:\n230:        # forward the GPT model itself\n231:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n232:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n233:        for block in self.transformer.h:\n234:            x = block(x)\n235:        x = F.rms_norm(x, (x.size(-1),))\n236:\n237:        if targets is not None:\n238:            # if we are given some desired targets also calculate the loss\n239:            logits = self.lm_head(x)\n240:            logits = logits.float() # use tf32/fp32 for logits\n241:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n242:        else:\n243:            # inference-time mini-optimization: only forward the lm_head on the very last position\n244:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n245:            logits = logits.float() # use tf32/fp32 for logits\n246:            loss = None\n247:\n248:        # there are performance reasons why not returning logits is prudent, if not needed\n249:        if not return_logits:\n250:            logits = None\n251:\n252:        return logits, loss\n253:    \n254:    def get_config(self):\n255:        return self.config\n256:\n257:# -----------------------------------------------------------------------------\n258:# Our own simple Distributed Data Loader\n259:\n260:def _peek_data_shard(filename):\n261:    # only reads the header, returns header data\n262:    with open(filename, \"rb\") as f:\n263:        # first read the header, which is 256 int32 integers (4 bytes each)\n264:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n265:    if header[0] != 20240520:\n266:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n267:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n268:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n269:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n270:        exit(1)\n271:    assert header[1] == 1, \"unsupported version\"\n272:    ntok = header[2] # number of tokens (claimed)\n273:    return ntok # for now just return the number of tokens\n274:\n275:def _load_data_shard(filename):\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n280:        assert header[1] == 1, \"unsupported version\"\n281:        ntok = header[2] # number of tokens (claimed)\n282:        # the rest of it are tokens, stored as uint16\n283:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n284:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n285:    return tokens\n286:\n287:class DistributedDataLoader:\n288:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n289:        self.process_rank = process_rank\n290:        self.num_processes = num_processes\n291:        self.B = B\n292:        self.T = T\n293:\n294:        # glob files that match the pattern\n295:        self.files = sorted(glob.glob(filename_pattern))\n296:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n297:\n298:        # load and validate all data shards, count number of tokens in total\n299:        ntok_total = 0\n300:        for fname in self.files:\n301:            shard_ntok = _peek_data_shard(fname)\n302:            assert shard_ntok >= num_processes * B * T + 1\n303:            ntok_total += int(shard_ntok)\n304:        self.ntok_total = ntok_total\n305:\n306:        # kick things off\n307:        self.reset()\n308:\n309:    def reset(self):\n310:        self.current_shard = 0\n311:        self.current_position = self.process_rank * self.B * self.T\n312:        self.tokens = _load_data_shard(self.files[self.current_shard])\n313:\n314:    def advance(self): # advance to next data shard\n315:        self.current_shard = (self.current_shard + 1) % len(self.files)\n316:        self.current_position = self.process_rank * self.B * self.T\n317:        self.tokens = _load_data_shard(self.files[self.current_shard])\n318:\n319:    def next_batch(self):\n320:        B = self.B\n321:        T = self.T\n322:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n323:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n324:        x = (buf[:-1]).view(B, T) # inputs\n325:        y = (buf[1:]).view(B, T) # targets\n326:        # advance current position and load next shard if necessary\n327:        self.current_position += B * T * self.num_processes\n328:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n329:            self.advance()\n330:        return x.cuda(), y.cuda()\n331:\n332:# -----------------------------------------------------------------------------\n333:# int main\n334:\n335:@dataclass\n336:class Hyperparameters:\n337:    # data hyperparams\n338:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n339:    # optimization hyperparams\n340:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n341:    device_batch_size : int = 64 # batch size, in sequences, per device\n342:    sequence_length : int = 1024 # sequence length, in tokens\n343:    num_iterations : int = 10000 # number of iterations to run\n344:    warmup_iters : int = 200\n345:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n346:    weight_decay : float = 0\n347:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n348:\n349:\n350:def main():\n351:    with open(sys.argv[0]) as f:\n352:        code = f.read() # read the code of this file ASAP, for logging\n353:    args = Hyperparameters()\n354:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n355:\n356:    # set up DDP (distributed data parallel). torchrun sets this env variable\n357:    assert torch.cuda.is_available()\n358:    dist.init_process_group(backend='nccl')\n359:    ddp_rank = int(os.environ['RANK'])\n360:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n361:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n362:    device = f'cuda:{ddp_local_rank}'\n363:    torch.cuda.set_device(device)\n364:    print(f\"using device: {device}\")\n365:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n366:\n367:    # convenience variables\n368:    B, T = args.device_batch_size, args.sequence_length\n369:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n370:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n371:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n372:\n373:    # load tokens\n374:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n375:    if master_process:\n376:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n377:    x, y = train_loader.next_batch()\n378:\n379:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n380:    # this originates from Karpathy's experiments.\n381:    num_vocab = 50304\n382:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n383:    model = model.cuda()\n384:    if hasattr(config, \"coordinate_descent_tuning\"):\n385:        config.coordinate_descent_tuning = True # suggested by @Chillee\n386:    model = torch.compile(model)\n387:    # here we wrap model into DDP container\n388:    model = DDP(model, device_ids=[ddp_local_rank])\n389:    raw_model = model.module # always contains the \"raw\" unwrapped model\n390:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n391:\n392:    # init the optimizer\n393:    # Use AdamW for all parameters\n394:    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n395:    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\n396:    # learning rate decay scheduler (linear warmup and warmdown)\n397:    def get_lr(it):\n398:        assert it <= args.num_iterations\n399:        lr = 0.0 # default value, can adjust min LR later if needed\n400:        \n401:        # 1) linear warmup for warmup_iters steps\n402:        if it < args.warmup_iters:\n403:            lr = (it+1) / args.warmup_iters\n404:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n405:        elif it < args.num_iterations:\n406:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n407:            assert 0 <= decay_ratio <= 1\n408:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n409:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n410:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n411:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n412:        else: \n413:            lr = 0.1 # Keep min lr at the very end\n414:\n415:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n416:\n417:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n418:\n419:    # begin logging\n420:    if master_process:\n421:        run_id = str(uuid.uuid4())\n422:        logdir = 'logs/%s/' % run_id\n423:        os.makedirs(logdir, exist_ok=True)\n424:        logfile = 'logs/%s.txt' % run_id\n425:        # create the log file\n426:        with open(logfile, \"w\") as f:\n427:            # begin the log by printing this file (the Python code)\n428:            f.write('='*100 + '\\n')\n429:            f.write(code)\n430:            f.write('='*100 + '\\n')\n431:            # log information about the hardware/software environment this is running on\n432:            # and print the full `nvidia-smi` to file\n433:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n434:            import subprocess\n435:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n436:            f.write(f'{result.stdout}\\n')\n437:            f.write('='*100 + '\\n')\n438:\n439:    training_time_ms = 0\n440:    # start the clock\n441:    torch.cuda.synchronize()\n442:    t0 = time.time()\n443:    # begin training\n444:    train_loader.reset()\n445:    for step in range(args.num_iterations + 1):\n446:        last_step = (step == args.num_iterations)\n447:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n448:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n449:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n450:        if step == 10:\n451:            training_time_ms = 0\n452:            t0 = time.time()\n453:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n454:\n455:        if last_step:\n456:            break\n457:\n458:        # --------------- TRAINING SECTION BEGIN -----------------\n459:        model.train()\n460:        for i in range(1, train_accumulation_steps+1):\n461:            # forward pass\n462:            with ctx:\n463:                _, loss = model(x, y, return_logits=False)\n464:                train_loss = loss.detach()\n465:            # advance the dataset for the next batch\n466:            x, y = train_loader.next_batch()\n467:            # backward pass\n468:            if i < train_accumulation_steps:\n469:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n470:                    loss.backward()\n471:            else:\n472:                loss.backward() # just sync on the last step\n473:        for p in model.parameters():\n474:            p.grad /= train_accumulation_steps\n475:        # step the optimizers and schedulers\n476:        for opt, sched in zip(optimizers, schedulers):\n477:            opt.step()\n478:            sched.step()\n479:        # null the gradients\n480:        model.zero_grad(set_to_none=True)\n481:        # --------------- TRAINING SECTION END -------------------\n482:        # everything that follows now is just diagnostics, prints, logging, etc.\n483:\n484:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n485:        if master_process:\n486:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n487:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n488:            with open(logfile, \"a\") as f:\n489:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n490:\n491:    torch.save(model.state_dict(), \"model.pt\")\n492:    with open(\"model_config.pt\", \"wb\") as f:\n493:        pickle.dump(raw_model.get_config(), f)\n494:    f.close()\n495:\n496:    if master_process:\n497:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n498:\n499:    # -------------------------------------------------------------------------\n500:    # clean up nice\n501:    dist.destroy_process_group()\n502:\n503:if __name__ == \"__main__\":\n504:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe data path has been corrected. Let's try running the training script again with `torchrun`.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThe data path has been corrected. Let's try running the training script again with `torchrun`.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 1784.1008732328191,
      "observation": "W0326 03:22:16.539000 1555 site-packages/torch/distributed/run.py:793] \nW0326 03:22:16.539000 1555 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:22:16.539000 1555 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:22:16.539000 1555 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\nusing device: cuda:1\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nstep:1/10000 train_loss:10.8258 train_time:57612ms step_avg:nanms\nstep:2/10000 train_loss:10.8255 train_time:69349ms step_avg:nanms\nstep:3/10000 train_loss:10.8253 train_time:69414ms step_avg:nanms\nstep:4/10000 train_loss:10.8248 train_time:69576ms step_avg:nanms\nstep:5/10000 train_loss:10.8237 train_time:69739ms step_avg:nanms\nstep:6/10000 train_loss:10.8220 train_time:69902ms step_avg:nanms\nstep:7/10000 train_loss:10.8195 train_time:70065ms step_avg:nanms\nstep:8/10000 train_loss:10.8165 train_time:70231ms step_avg:nanms\nstep:9/10000 train_loss:10.8117 train_time:70406ms step_avg:nanms\nstep:10/10000 train_loss:10.8054 train_time:70574ms step_avg:nanms\nstep:11/10000 train_loss:10.7964 train_time:65ms step_avg:nanms\nstep:12/10000 train_loss:10.7854 train_time:230ms step_avg:nanms\nstep:13/10000 train_loss:10.7715 train_time:394ms step_avg:131.47ms\nstep:14/10000 train_loss:10.7530 train_time:563ms step_avg:140.68ms\nstep:15/10000 train_loss:10.7352 train_time:735ms step_avg:146.92ms\nstep:16/10000 train_loss:10.7107 train_time:902ms step_avg:150.27ms\nstep:17/10000 train_loss:10.6921 train_time:1066ms step_avg:152.35ms\nstep:18/10000 train_loss:10.6619 train_time:1230ms step_avg:153.72ms\nstep:19/10000 train_loss:10.6325 train_time:1394ms step_avg:154.85ms\nstep:20/10000 train_loss:10.6056 train_time:1559ms step_avg:155.94ms\nstep:21/10000 train_loss:10.5739 train_time:1728ms step_avg:157.13ms\nstep:22/10000 train_loss:10.5373 train_time:1893ms step_avg:157.75ms\nstep:23/10000 train_loss:10.5061 train_time:2058ms step_avg:158.31ms\nstep:24/10000 train_loss:10.4714 train_time:2226ms step_avg:158.99ms\nstep:25/10000 train_loss:10.4389 train_time:2389ms step_avg:159.25ms\nstep:26/10000 train_loss:10.4242 train_time:2558ms step_avg:159.85ms\nstep:27/10000 train_loss:10.3629 train_time:2724ms step_avg:160.26ms\nstep:28/10000 train_loss:10.3289 train_time:2890ms step_avg:160.56ms\nstep:29/10000 train_loss:10.2807 train_time:3056ms step_avg:160.82ms\nstep:30/10000 train_loss:10.2565 train_time:3219ms step_avg:160.97ms\nstep:31/10000 train_loss:10.1978 train_time:3384ms step_avg:161.15ms\nstep:32/10000 train_loss:10.1327 train_time:3548ms step_avg:161.25ms\nstep:33/10000 train_loss:10.1191 train_time:3718ms step_avg:161.64ms\nstep:34/10000 train_loss:10.0625 train_time:3882ms step_avg:161.75ms\nstep:35/10000 train_loss:10.0230 train_time:4050ms step_avg:162.01ms\nstep:36/10000 train_loss:9.9878 train_time:4213ms step_avg:162.05ms\nstep:37/10000 train_loss:9.9189 train_time:4381ms step_avg:162.26ms\nstep:38/10000 train_loss:9.8888 train_time:4546ms step_avg:162.34ms\nstep:39/10000 train_loss:9.8309 train_time:4713ms step_avg:162.52ms\nstep:40/10000 train_loss:9.7969 train_time:4879ms step_avg:162.63ms\nstep:41/10000 train_loss:9.7565 train_time:5049ms step_avg:162.86ms\nstep:42/10000 train_loss:9.6892 train_time:5212ms step_avg:162.86ms\nstep:43/10000 train_loss:9.6401 train_time:5376ms step_avg:162.90ms\nstep:44/10000 train_loss:9.5886 train_time:5545ms step_avg:163.09ms\nstep:45/10000 train_loss:9.5121 train_time:5708ms step_avg:163.08ms\nstep:46/10000 train_loss:9.4664 train_time:5872ms step_avg:163.12ms\nstep:47/10000 train_loss:9.4413 train_time:6041ms step_avg:163.28ms\nstep:48/10000 train_loss:9.3869 train_time:6210ms step_avg:163.42ms\nstep:49/10000 train_loss:9.3355 train_time:6372ms step_avg:163.39ms\nstep:50/10000 train_loss:9.2839 train_time:6541ms step_avg:163.53ms\nstep:51/10000 train_loss:9.2958 train_time:6708ms step_avg:163.60ms\nstep:52/10000 train_loss:9.1764 train_time:6873ms step_avg:163.64ms\nstep:53/10000 train_loss:9.1613 train_time:7045ms step_avg:163.84ms\nstep:54/10000 train_loss:9.0863 train_time:7213ms step_avg:163.93ms\nstep:55/10000 train_loss:8.9653 train_time:7376ms step_avg:163.92ms\nstep:56/10000 train_loss:8.9825 train_time:7544ms step_avg:164.00ms\nstep:57/10000 train_loss:8.9233 train_time:7711ms step_avg:164.05ms\nstep:58/10000 train_loss:8.8872 train_time:7874ms step_avg:164.05ms\nstep:59/10000 train_loss:8.8324 train_time:8044ms step_avg:164.16ms\nstep:60/10000 train_loss:8.8212 train_time:8207ms step_avg:164.14ms\nstep:61/10000 train_loss:8.7849 train_time:8373ms step_avg:164.18ms\nstep:62/10000 train_loss:8.7181 train_time:8537ms step_avg:164.18ms\nstep:63/10000 train_loss:8.6493 train_time:8703ms step_avg:164.21ms\nstep:64/10000 train_loss:8.5927 train_time:8870ms step_avg:164.25ms\nstep:65/10000 train_loss:8.6563 train_time:9037ms step_avg:164.31ms\nstep:66/10000 train_loss:8.5959 train_time:9201ms step_avg:164.30ms\nstep:67/10000 train_loss:8.4156 train_time:9368ms step_avg:164.35ms\nstep:68/10000 train_loss:8.4062 train_time:9532ms step_avg:164.35ms\nstep:69/10000 train_loss:8.4304 train_time:9700ms step_avg:164.41ms\nstep:70/10000 train_loss:8.3786 train_time:9867ms step_avg:164.45ms\nstep:71/10000 train_loss:8.3654 train_time:10038ms step_avg:164.56ms\nstep:72/10000 train_loss:8.5040 train_time:10201ms step_avg:164.53ms\nstep:73/10000 train_loss:8.2692 train_time:10370ms step_avg:164.60ms\nstep:74/10000 train_loss:8.2040 train_time:10536ms step_avg:164.62ms\nstep:75/10000 train_loss:8.2607 train_time:10703ms step_avg:164.66ms\nstep:76/10000 train_loss:8.1474 train_time:10869ms step_avg:164.68ms\nstep:77/10000 train_loss:8.0876 train_time:11037ms step_avg:164.73ms\nstep:78/10000 train_loss:7.9232 train_time:11203ms step_avg:164.76ms\nstep:79/10000 train_loss:8.0679 train_time:11371ms step_avg:164.79ms\nstep:80/10000 train_loss:8.1221 train_time:11535ms step_avg:164.79ms\nstep:81/10000 train_loss:7.9723 train_time:11701ms step_avg:164.80ms\nstep:82/10000 train_loss:8.0687 train_time:11871ms step_avg:164.87ms\nstep:83/10000 train_loss:7.8928 train_time:12038ms step_avg:164.90ms\nstep:84/10000 train_loss:7.8787 train_time:12206ms step_avg:164.94ms\nstep:85/10000 train_loss:7.9864 train_time:12373ms step_avg:164.97ms\nstep:86/10000 train_loss:8.1385 train_time:12539ms step_avg:164.99ms\nstep:87/10000 train_loss:7.9937 train_time:12707ms step_avg:165.02ms\nstep:88/10000 train_loss:7.7912 train_time:12873ms step_avg:165.03ms\nstep:89/10000 train_loss:7.8286 train_time:13042ms step_avg:165.09ms\nstep:90/10000 train_loss:7.8380 train_time:13212ms step_avg:165.15ms\nstep:91/10000 train_loss:7.7943 train_time:13379ms step_avg:165.17ms\nstep:92/10000 train_loss:7.7340 train_time:13545ms step_avg:165.18ms\nstep:93/10000 train_loss:7.6614 train_time:13711ms step_avg:165.19ms\nstep:94/10000 train_loss:7.8675 train_time:13875ms step_avg:165.18ms\nstep:95/10000 train_loss:7.7414 train_time:14044ms step_avg:165.22ms\nstep:96/10000 train_loss:7.7115 train_time:14214ms step_avg:165.28ms\nstep:97/10000 train_loss:7.6117 train_time:14382ms step_avg:165.30ms\nstep:98/10000 train_loss:7.6542 train_time:14548ms step_avg:165.32ms\nstep:99/10000 train_loss:7.5525 train_time:14714ms step_avg:165.32ms\nstep:100/10000 train_loss:7.5125 train_time:14883ms step_avg:165.37ms\nstep:101/10000 train_loss:7.6417 train_time:15051ms step_avg:165.39ms\nstep:102/10000 train_loss:7.5764 train_time:15219ms step_avg:165.42ms\nstep:103/10000 train_loss:7.6352 train_time:15390ms step_avg:165.49ms\nstep:104/10000 train_loss:7.6687 train_time:15555ms step_avg:165.48ms\nstep:105/10000 train_loss:7.6261 train_time:15721ms step_avg:165.48ms\nstep:106/10000 train_loss:7.6709 train_time:15891ms step_avg:165.54ms\nstep:107/10000 train_loss:7.6467 train_time:16060ms step_avg:165.57ms\nstep:108/10000 train_loss:7.5979 train_time:16230ms step_avg:165.61ms\nstep:109/10000 train_loss:7.6933 train_time:16395ms step_avg:165.61ms\nstep:110/10000 train_loss:7.4468 train_time:16561ms step_avg:165.61ms\nstep:111/10000 train_loss:7.5137 train_time:16727ms step_avg:165.61ms\nstep:112/10000 train_loss:7.6115 train_time:16895ms step_avg:165.64ms\nstep:113/10000 train_loss:7.6128 train_time:17062ms step_avg:165.65ms\nstep:114/10000 train_loss:7.6078 train_time:17232ms step_avg:165.69ms\nstep:115/10000 train_loss:7.6011 train_time:17403ms step_avg:165.75ms\nstep:116/10000 train_loss:7.5246 train_time:17573ms step_avg:165.78ms\nstep:117/10000 train_loss:7.5299 train_time:17741ms step_avg:165.80ms\nstep:118/10000 train_loss:7.5272 train_time:17908ms step_avg:165.81ms\nstep:119/10000 train_loss:7.5730 train_time:18076ms step_avg:165.83ms\nstep:120/10000 train_loss:7.5873 train_time:18244ms step_avg:165.86ms\nstep:121/10000 train_loss:7.4671 train_time:18411ms step_avg:165.86ms\nstep:122/10000 train_loss:7.4531 train_time:18578ms step_avg:165.88ms\nstep:123/10000 train_loss:7.4181 train_time:18746ms step_avg:165.89ms\nstep:124/10000 train_loss:7.8958 train_time:18913ms step_avg:165.90ms\nstep:125/10000 train_loss:7.4670 train_time:19082ms step_avg:165.93ms\nstep:126/10000 train_loss:7.2848 train_time:19252ms step_avg:165.96ms\nstep:127/10000 train_loss:7.4183 train_time:19420ms step_avg:165.98ms\nstep:128/10000 train_loss:7.4048 train_time:19590ms step_avg:166.02ms\nstep:129/10000 train_loss:7.5265 train_time:19759ms step_avg:166.04ms\nstep:130/10000 train_loss:7.6298 train_time:19928ms step_avg:166.07ms\nstep:131/10000 train_loss:7.4047 train_time:20094ms step_avg:166.06ms\nstep:132/10000 train_loss:7.3360 train_time:20262ms step_avg:166.08ms\nstep:133/10000 train_loss:7.3964 train_time:20430ms step_avg:166.10ms\nstep:134/10000 train_loss:7.3842 train_time:20596ms step_avg:166.10ms\nstep:135/10000 train_loss:7.3975 train_time:20763ms step_avg:166.10ms\nstep:136/10000 train_loss:7.3252 train_time:20935ms step_avg:166.15ms\nstep:137/10000 train_loss:7.3305 train_time:21106ms step_avg:166.19ms\nstep:138/10000 train_loss:7.4776 train_time:21273ms step_avg:166.19ms\nstep:139/10000 train_loss:7.3155 train_time:21442ms step_avg:166.22ms\nstep:140/10000 train_loss:7.3974 train_time:21610ms step_avg:166.23ms\nstep:141/10000 train_loss:7.2573 train_time:21779ms step_avg:166.25ms\nstep:142/10000 train_loss:7.4407 train_time:21947ms step_avg:166.26ms\nstep:143/10000 train_loss:7.2208 train_time:22114ms step_avg:166.27ms\nstep:144/10000 train_loss:7.3711 train_time:22280ms step_avg:166.27ms\nstep:145/10000 train_loss:7.2706 train_time:22449ms step_avg:166.29ms\nstep:146/10000 train_loss:7.2713 train_time:22620ms step_avg:166.33ms\nstep:147/10000 train_loss:7.1704 train_time:22789ms step_avg:166.34ms\nstep:148/10000 train_loss:7.2885 train_time:22955ms step_avg:166.34ms\nstep:149/10000 train_loss:6.9866 train_time:23123ms step_avg:166.35ms\nstep:150/10000 train_loss:7.1613 train_time:23292ms step_avg:166.37ms\nstep:151/10000 train_loss:7.1938 train_time:23460ms step_avg:166.38ms\nstep:152/10000 train_loss:7.1506 train_time:23630ms step_avg:166.41ms\nstep:153/10000 train_loss:7.3162 train_time:23797ms step_avg:166.41ms\nstep:154/10000 train_loss:7.2149 train_time:23970ms step_avg:166.46ms\nstep:155/10000 train_loss:7.0380 train_time:24141ms step_avg:166.49ms\nstep:156/10000 train_loss:7.2263 train_time:24312ms step_avg:166.52ms\nstep:157/10000 train_loss:7.1835 train_time:24478ms step_avg:166.52ms\nstep:158/10000 train_loss:7.0823 train_time:24647ms step_avg:166.53ms\nstep:159/10000 train_loss:7.1364 train_time:24816ms step_avg:166.55ms\nstep:160/10000 train_loss:7.0387 train_time:24988ms step_avg:166.59ms\nstep:161/10000 train_loss:7.1819 train_time:25156ms step_avg:166.60ms\nstep:162/10000 train_loss:7.0827 train_time:25326ms step_avg:166.62ms\nstep:163/10000 train_loss:6.9844 train_time:25493ms step_avg:166.62ms\nstep:164/10000 train_loss:7.1264 train_time:25660ms step_avg:166.62ms\nstep:165/10000 train_loss:6.9666 train_time:25827ms step_avg:166.63ms\nstep:166/10000 train_loss:6.9060 train_time:25995ms step_avg:166.64ms\nstep:167/10000 train_loss:6.8705 train_time:26165ms step_avg:166.66ms\nstep:168/10000 train_loss:7.0584 train_time:26333ms step_avg:166.66ms\nstep:169/10000 train_loss:7.1540 train_time:26500ms step_avg:166.67ms\nstep:170/10000 train_loss:6.9736 train_time:26668ms step_avg:166.68ms\nstep:171/10000 train_loss:7.2091 train_time:26837ms step_avg:166.69ms\nstep:172/10000 train_loss:6.9989 train_time:27007ms step_avg:166.71ms\nstep:173/10000 train_loss:7.0192 train_time:27174ms step_avg:166.71ms\nstep:174/10000 train_loss:6.9326 train_time:27347ms step_avg:166.75ms\nstep:175/10000 train_loss:6.9110 train_time:27514ms step_avg:166.75ms\nstep:176/10000 train_loss:6.9408 train_time:27684ms step_avg:166.77ms\nstep:177/10000 train_loss:6.9093 train_time:27850ms step_avg:166.77ms\nstep:178/10000 train_loss:7.1229 train_time:28021ms step_avg:166.79ms\nstep:179/10000 train_loss:7.0903 train_time:28190ms step_avg:166.80ms\nstep:180/10000 train_loss:6.8180 train_time:28362ms step_avg:166.83ms\nstep:181/10000 train_loss:6.8361 train_time:28530ms step_avg:166.84ms\nstep:182/10000 train_loss:6.9095 train_time:28697ms step_avg:166.84ms\nstep:183/10000 train_loss:6.9174 train_time:28866ms step_avg:166.86ms\nstep:184/10000 train_loss:6.8582 train_time:29035ms step_avg:166.86ms\nstep:185/10000 train_loss:6.9522 train_time:29204ms step_avg:166.88ms\nstep:186/10000 train_loss:6.7615 train_time:29374ms step_avg:166.90ms\nstep:187/10000 train_loss:7.0200 train_time:29542ms step_avg:166.91ms\nstep:188/10000 train_loss:6.7594 train_time:29711ms step_avg:166.92ms\nstep:189/10000 train_loss:6.9278 train_time:29882ms step_avg:166.94ms\nstep:190/10000 train_loss:6.9543 train_time:30053ms step_avg:166.96ms\nstep:191/10000 train_loss:6.7982 train_time:30222ms step_avg:166.97ms\nstep:192/10000 train_loss:6.9477 train_time:30391ms step_avg:166.98ms\nstep:193/10000 train_loss:6.7775 train_time:30558ms step_avg:166.98ms\nstep:194/10000 train_loss:6.8080 train_time:30729ms step_avg:167.01ms\nstep:195/10000 train_loss:6.8384 train_time:30896ms step_avg:167.00ms\nstep:196/10000 train_loss:6.6252 train_time:31066ms step_avg:167.02ms\nstep:197/10000 train_loss:6.7824 train_time:31234ms step_avg:167.03ms\nstep:198/10000 train_loss:6.9396 train_time:31402ms step_avg:167.03ms\nstep:199/10000 train_loss:6.7779 train_time:31569ms step_avg:167.03ms\nstep:200/10000 train_loss:6.7832 train_time:31739ms step_avg:167.05ms\nstep:201/10000 train_loss:6.7363 train_time:31907ms step_avg:167.05ms\nstep:202/10000 train_loss:6.7864 train_time:32076ms step_avg:167.06ms\nstep:203/10000 train_loss:6.3636 train_time:32245ms step_avg:167.07ms\nstep:204/10000 train_loss:6.8664 train_time:32414ms step_avg:167.08ms\nstep:205/10000 train_loss:6.7396 train_time:32583ms step_avg:167.09ms\nstep:206/10000 train_loss:6.5998 train_time:32753ms step_avg:167.11ms\nstep:207/10000 train_loss:6.6561 train_time:32923ms step_avg:167.12ms\nstep:208/10000 train_loss:6.7902 train_time:33093ms step_avg:167.14ms\nstep:209/10000 train_loss:6.7489 train_time:33262ms step_avg:167.15ms\nstep:210/10000 train_loss:6.7164 train_time:33431ms step_avg:167.16ms\nstep:211/10000 train_loss:6.7350 train_time:33600ms step_avg:167.17ms\nstep:212/10000 train_loss:6.7127 train_time:33771ms step_avg:167.18ms\nstep:213/10000 train_loss:6.8698 train_time:33940ms step_avg:167.19ms\nstep:214/10000 train_loss:6.6396 train_time:34109ms step_avg:167.20ms\nstep:215/10000 train_loss:6.6423 train_time:34277ms step_avg:167.21ms\nstep:216/10000 train_loss:6.7213 train_time:34447ms step_avg:167.22ms\nstep:217/10000 train_loss:6.5093 train_time:34618ms step_avg:167.24ms\nstep:218/10000 train_loss:6.9019 train_time:34786ms step_avg:167.24ms\nstep:219/10000 train_loss:6.7298 train_time:34955ms step_avg:167.25ms\nstep:220/10000 train_loss:6.5775 train_time:35124ms step_avg:167.26ms\nstep:221/10000 train_loss:6.6136 train_time:35293ms step_avg:167.27ms\nstep:222/10000 train_loss:6.5941 train_time:35461ms step_avg:167.27ms\nstep:223/10000 train_loss:6.5562 train_time:35629ms step_avg:167.27ms\nstep:224/10000 train_loss:6.5533 train_time:35799ms step_avg:167.28ms\nstep:225/10000 train_loss:6.5901 train_time:35971ms step_avg:167.31ms\nstep:226/10000 train_loss:6.4625 train_time:36139ms step_avg:167.31ms\nstep:227/10000 train_loss:6.6837 train_time:36308ms step_avg:167.32ms\nstep:228/10000 train_loss:6.5618 train_time:36478ms step_avg:167.33ms\nstep:229/10000 train_loss:6.6134 train_time:36650ms step_avg:167.35ms\nstep:230/10000 train_loss:6.6338 train_time:36821ms step_avg:167.37ms\nstep:231/10000 train_loss:6.7040 train_time:36993ms step_avg:167.39ms\nstep:232/10000 train_loss:6.5538 train_time:37164ms step_avg:167.40ms\nstep:233/10000 train_loss:6.4314 train_time:37332ms step_avg:167.41ms\nstep:234/10000 train_loss:6.5103 train_time:37500ms step_avg:167.41ms\nstep:235/10000 train_loss:6.5722 train_time:37670ms step_avg:167.42ms\nstep:236/10000 train_loss:6.6165 train_time:37838ms step_avg:167.43ms\nstep:237/10000 train_loss:6.5593 train_time:38006ms step_avg:167.43ms\nstep:238/10000 train_loss:6.6316 train_time:38174ms step_avg:167.43ms\nstep:239/10000 train_loss:6.6563 train_time:38342ms step_avg:167.43ms\nstep:240/10000 train_loss:6.4777 train_time:38513ms step_avg:167.45ms\nstep:241/10000 train_loss:6.5924 train_time:38684ms step_avg:167.46ms\nstep:242/10000 train_loss:6.4184 train_time:38852ms step_avg:167.47ms\nstep:243/10000 train_loss:6.4077 train_time:39025ms step_avg:167.49ms\nstep:244/10000 train_loss:6.5265 train_time:39194ms step_avg:167.50ms\nstep:245/10000 train_loss:6.5537 train_time:39364ms step_avg:167.51ms\nstep:246/10000 train_loss:6.4864 train_time:39534ms step_avg:167.52ms\nstep:247/10000 train_loss:6.3764 train_time:39702ms step_avg:167.52ms\nstep:248/10000 train_loss:6.5378 train_time:39873ms step_avg:167.53ms\nstep:249/10000 train_loss:6.6541 train_time:40041ms step_avg:167.53ms\nstep:250/10000 train_loss:6.4147 train_time:40209ms step_avg:167.54ms\nstep:251/10000 train_loss:6.4098 train_time:40377ms step_avg:167.54ms\nstep:252/10000 train_loss:6.4505 train_time:40545ms step_avg:167.54ms\nstep:253/10000 train_loss:6.5016 train_time:40713ms step_avg:167.54ms\nstep:254/10000 train_loss:7.0553 train_time:40883ms step_avg:167.55ms\nstep:255/10000 train_loss:6.4180 train_time:41052ms step_avg:167.56ms\nstep:256/10000 train_loss:6.4031 train_time:41222ms step_avg:167.57ms\nstep:257/10000 train_loss:6.5660 train_time:41390ms step_avg:167.57ms\nstep:258/10000 train_loss:6.2501 train_time:41559ms step_avg:167.58ms\nstep:259/10000 train_loss:6.4642 train_time:41727ms step_avg:167.58ms\nstep:260/10000 train_loss:6.2063 train_time:41894ms step_avg:167.58ms\nstep:261/10000 train_loss:6.3993 train_time:42063ms step_avg:167.58ms\nstep:262/10000 train_loss:6.4151 train_time:42232ms step_avg:167.59ms\nstep:263/10000 train_loss:6.3821 train_time:42404ms step_avg:167.61ms\nstep:264/10000 train_loss:6.2481 train_time:42573ms step_avg:167.61ms\nstep:265/10000 train_loss:6.4949 train_time:42744ms step_avg:167.62ms\nstep:266/10000 train_loss:6.2907 train_time:42914ms step_avg:167.63ms\nstep:267/10000 train_loss:6.3345 train_time:43083ms step_avg:167.64ms\nstep:268/10000 train_loss:6.4682 train_time:43253ms step_avg:167.65ms\nstep:269/10000 train_loss:6.4501 train_time:43423ms step_avg:167.66ms\nstep:270/10000 train_loss:6.5942 train_time:43589ms step_avg:167.65ms\nstep:271/10000 train_loss:6.3225 train_time:43762ms step_avg:167.67ms\nstep:272/10000 train_loss:6.4279 train_time:43930ms step_avg:167.67ms\nstep:273/10000 train_loss:6.5661 train_time:44097ms step_avg:167.67ms\nstep:274/10000 train_loss:6.1756 train_time:44267ms step_avg:167.68ms\nstep:275/10000 train_loss:6.2454 train_time:44437ms step_avg:167.69ms\nstep:276/10000 train_loss:6.4630 train_time:44606ms step_avg:167.69ms\nstep:277/10000 train_loss:6.3457 train_time:44776ms step_avg:167.70ms\nstep:278/10000 train_loss:6.2340 train_time:44948ms step_avg:167.72ms\nstep:279/10000 train_loss:6.3273 train_time:45115ms step_avg:167.71ms\nstep:280/10000 train_loss:6.2647 train_time:45281ms step_avg:167.71ms\nstep:281/10000 train_loss:6.4302 train_time:45449ms step_avg:167.71ms\nstep:282/10000 train_loss:6.4545 train_time:45619ms step_avg:167.72ms\nstep:283/10000 train_loss:6.2753 train_time:45787ms step_avg:167.72ms\nstep:284/10000 train_loss:6.4237 train_time:45959ms step_avg:167.73ms\nstep:285/10000 train_loss:6.5528 train_time:46128ms step_avg:167.74ms\nstep:286/10000 train_loss:6.3171 train_time:46296ms step_avg:167.74ms\nstep:287/10000 train_loss:6.4594 train_time:46465ms step_avg:167.74ms\nstep:288/10000 train_loss:6.3288 train_time:46635ms step_avg:167.75ms\nstep:289/10000 train_loss:6.2447 train_time:46806ms step_avg:167.76ms\nstep:290/10000 train_loss:6.2156 train_time:46975ms step_avg:167.77ms\nstep:291/10000 train_loss:6.4156 train_time:47144ms step_avg:167.77ms\nstep:292/10000 train_loss:6.3627 train_time:47314ms step_avg:167.78ms\nstep:293/10000 train_loss:6.3688 train_time:47484ms step_avg:167.79ms\nstep:294/10000 train_loss:6.2978 train_time:47651ms step_avg:167.78ms\nstep:295/10000 train_loss:6.0946 train_time:47819ms step_avg:167.79ms\nstep:296/10000 train_loss:6.0550 train_time:47989ms step_avg:167.79ms\nstep:297/10000 train_loss:6.3389 train_time:48157ms step_avg:167.79ms\nstep:298/10000 train_loss:6.3086 train_time:48328ms step_avg:167.81ms\nstep:299/10000 train_loss:6.1746 train_time:48498ms step_avg:167.81ms\nstep:300/10000 train_loss:6.1585 train_time:48668ms step_avg:167.82ms\nstep:301/10000 train_loss:6.2522 train_time:48837ms step_avg:167.83ms\nstep:302/10000 train_loss:6.2909 train_time:49007ms step_avg:167.83ms\nstep:303/10000 train_loss:6.1135 train_time:49175ms step_avg:167.83ms\nstep:304/10000 train_loss:6.1370 train_time:49346ms step_avg:167.84ms\nstep:305/10000 train_loss:6.2381 train_time:49511ms step_avg:167.83ms\nstep:306/10000 train_loss:6.3186 train_time:49682ms step_avg:167.84ms\nstep:307/10000 train_loss:6.2914 train_time:49854ms step_avg:167.86ms\nstep:308/10000 train_loss:6.3201 train_time:50021ms step_avg:167.86ms\nstep:309/10000 train_loss:6.2369 train_time:50191ms step_avg:167.86ms\nstep:310/10000 train_loss:6.4667 train_time:50361ms step_avg:167.87ms\nstep:311/10000 train_loss:6.2652 train_time:50528ms step_avg:167.87ms\nstep:312/10000 train_loss:6.1732 train_time:50697ms step_avg:167.87ms\nstep:313/10000 train_loss:6.2785 train_time:50867ms step_avg:167.88ms\nstep:314/10000 train_loss:6.2922 train_time:51038ms step_avg:167.89ms\nstep:315/10000 train_loss:6.0349 train_time:51209ms step_avg:167.90ms\nstep:316/10000 train_loss:5.7398 train_time:51380ms step_avg:167.91ms\nstep:317/10000 train_loss:6.1947 train_time:51550ms step_avg:167.91ms\nstep:318/10000 train_loss:6.3358 train_time:51719ms step_avg:167.92ms\nstep:319/10000 train_loss:6.0446 train_time:51888ms step_avg:167.92ms\nstep:320/10000 train_loss:6.2206 train_time:52057ms step_avg:167.92ms\nstep:321/10000 train_loss:6.1847 train_time:52227ms step_avg:167.93ms\nstep:322/10000 train_loss:6.0544 train_time:52395ms step_avg:167.93ms\nstep:323/10000 train_loss:6.1647 train_time:52563ms step_avg:167.93ms\nstep:324/10000 train_loss:6.2400 train_time:52734ms step_avg:167.94ms\nstep:325/10000 train_loss:6.1633 train_time:52902ms step_avg:167.94ms\nstep:326/10000 train_loss:6.2394 train_time:53071ms step_avg:167.95ms\nstep:327/10000 train_loss:6.1611 train_time:53241ms step_avg:167.95ms\nstep:328/10000 train_loss:6.1283 train_time:53410ms step_avg:167.96ms\nstep:329/10000 train_loss:6.7143 train_time:53578ms step_avg:167.96ms\nstep:330/10000 train_loss:6.2189 train_time:53747ms step_avg:167.96ms\nstep:331/10000 train_loss:6.0045 train_time:53917ms step_avg:167.96ms\nstep:332/10000 train_loss:6.2174 train_time:54086ms step_avg:167.97ms\nstep:333/10000 train_loss:6.2008 train_time:54256ms step_avg:167.97ms\nstep:334/10000 train_loss:6.0835 train_time:54426ms step_avg:167.98ms\nstep:335/10000 train_loss:6.1168 train_time:54592ms step_avg:167.98ms\nstep:336/10000 train_loss:6.0728 train_time:54761ms step_avg:167.98ms\nstep:337/10000 train_loss:6.1879 train_time:54929ms step_avg:167.98ms\nstep:338/10000 train_loss:5.9547 train_time:55101ms step_avg:167.99ms\nstep:339/10000 train_loss:5.9767 train_time:55272ms step_avg:168.00ms\nstep:340/10000 train_loss:6.1660 train_time:55442ms step_avg:168.01ms\nstep:341/10000 train_loss:6.1366 train_time:55611ms step_avg:168.01ms\nstep:342/10000 train_loss:6.1683 train_time:55780ms step_avg:168.01ms\nstep:343/10000 train_loss:6.1332 train_time:55952ms step_avg:168.02ms\nstep:344/10000 train_loss:6.0854 train_time:56121ms step_avg:168.03ms\nstep:345/10000 train_loss:6.0865 train_time:56291ms step_avg:168.03ms\nstep:346/10000 train_loss:6.0734 train_time:56459ms step_avg:168.03ms\nstep:347/10000 train_loss:6.0629 train_time:56630ms step_avg:168.04ms\nstep:348/10000 train_loss:6.1418 train_time:56800ms step_avg:168.05ms\nstep:349/10000 train_loss:6.0668 train_time:56970ms step_avg:168.05ms\nstep:350/10000 train_loss:6.2377 train_time:57141ms step_avg:168.06ms\nstep:351/10000 train_loss:5.9990 train_time:57313ms step_avg:168.07ms\nstep:352/10000 train_loss:5.9836 train_time:57481ms step_avg:168.07ms\nstep:353/10000 train_loss:5.9444 train_time:57649ms step_avg:168.07ms\nstep:354/10000 train_loss:6.1505 train_time:57820ms step_avg:168.08ms\nstep:355/10000 train_loss:6.0826 train_time:57987ms step_avg:168.08ms\nstep:356/10000 train_loss:5.9999 train_time:58157ms step_avg:168.08ms\nstep:357/10000 train_loss:5.7083 train_time:58327ms step_avg:168.09ms\nstep:358/10000 train_loss:6.0712 train_time:58497ms step_avg:168.10ms\nstep:359/10000 train_loss:6.1163 train_time:58667ms step_avg:168.10ms\nstep:360/10000 train_loss:5.8396 train_time:58838ms step_avg:168.11ms\nstep:361/10000 train_loss:5.9396 train_time:59007ms step_avg:168.11ms\nstep:362/10000 train_loss:6.0295 train_time:59179ms step_avg:168.12ms\nstep:363/10000 train_loss:5.9961 train_time:59350ms step_avg:168.13ms\nstep:364/10000 train_loss:6.0594 train_time:59518ms step_avg:168.13ms\nstep:365/10000 train_loss:5.9944 train_time:59694ms step_avg:168.15ms\nstep:366/10000 train_loss:6.1500 train_time:59861ms step_avg:168.15ms\nstep:367/10000 train_loss:6.2499 train_time:60028ms step_avg:168.15ms\nstep:368/10000 train_loss:6.0093 train_time:60197ms step_avg:168.15ms\nstep:369/10000 train_loss:6.1727 train_time:60368ms step_avg:168.16ms\nstep:370/10000 train_loss:6.0071 train_time:60537ms step_avg:168.16ms\nstep:371/10000 train_loss:6.1119 train_time:60707ms step_avg:168.16ms\nstep:372/10000 train_loss:6.2749 train_time:60876ms step_avg:168.17ms\nstep:373/10000 train_loss:5.9539 train_time:61047ms step_avg:168.17ms\nstep:374/10000 train_loss:5.9882 train_time:61216ms step_avg:168.18ms\nstep:375/10000 train_loss:5.9906 train_time:61386ms step_avg:168.18ms\nstep:376/10000 train_loss:6.1329 train_time:61553ms step_avg:168.18ms\nstep:377/10000 train_loss:5.9149 train_time:61723ms step_avg:168.18ms\nstep:378/10000 train_loss:5.9847 train_time:61893ms step_avg:168.19ms\nstep:379/10000 train_loss:5.9994 train_time:62062ms step_avg:168.19ms\nstep:380/10000 train_loss:5.8787 train_time:62231ms step_avg:168.19ms\nstep:381/10000 train_loss:6.0874 train_time:62401ms step_avg:168.20ms\nstep:382/10000 train_loss:5.8573 train_time:62568ms step_avg:168.19ms\nstep:383/10000 train_loss:6.0960 train_time:62738ms step_avg:168.20ms\nstep:384/10000 train_loss:5.9519 train_time:62909ms step_avg:168.21ms\nstep:385/10000 train_loss:5.8671 train_time:63078ms step_avg:168.21ms\nstep:386/10000 train_loss:5.9691 train_time:63249ms step_avg:168.22ms\nstep:387/10000 train_loss:5.9201 train_time:63418ms step_avg:168.22ms\nstep:388/10000 train_loss:6.0496 train_time:63587ms step_avg:168.22ms\nstep:389/10000 train_loss:5.9124 train_time:63757ms step_avg:168.22ms\nstep:390/10000 train_loss:5.8503 train_time:63928ms step_avg:168.23ms\nstep:391/10000 train_loss:6.0069 train_time:64096ms step_avg:168.23ms\nstep:392/10000 train_loss:5.8254 train_time:64267ms step_avg:168.24ms\nstep:393/10000 train_loss:5.8690 train_time:64435ms step_avg:168.24ms\nstep:394/10000 train_loss:6.0146 train_time:64604ms step_avg:168.24ms\nstep:395/10000 train_loss:5.7569 train_time:64773ms step_avg:168.24ms\nstep:396/10000 train_loss:5.8113 train_time:64942ms step_avg:168.24ms\nstep:397/10000 train_loss:5.9616 train_time:65112ms step_avg:168.25ms\nstep:398/10000 train_loss:5.9704 train_time:65284ms step_avg:168.26ms\nstep:399/10000 train_loss:5.8422 train_time:65451ms step_avg:168.25ms\nstep:400/10000 train_loss:5.8290 train_time:65624ms step_avg:168.27ms\nstep:401/10000 train_loss:5.6635 train_time:65793ms step_avg:168.27ms\nstep:402/10000 train_loss:5.8608 train_time:65961ms step_avg:168.27ms\nstep:403/10000 train_loss:6.0188 train_time:66132ms step_avg:168.27ms\nstep:404/10000 train_loss:5.7308 train_time:66302ms step_avg:168.28ms\nstep:405/10000 train_loss:5.9749 train_time:66474ms step_avg:168.29ms\nstep:406/10000 train_loss:5.9690 train_time:66643ms step_avg:168.29ms\nstep:407/10000 train_loss:5.8996 train_time:66813ms step_avg:168.29ms\nstep:408/10000 train_loss:5.8932 train_time:66983ms step_avg:168.30ms\nstep:409/10000 train_loss:6.0240 train_time:67153ms step_avg:168.30ms\nstep:410/10000 train_loss:6.0200 train_time:67322ms step_avg:168.31ms\nstep:411/10000 train_loss:5.9889 train_time:67490ms step_avg:168.30ms\nstep:412/10000 train_loss:5.7353 train_time:67663ms step_avg:168.32ms\nstep:413/10000 train_loss:5.9537 train_time:67831ms step_avg:168.31ms\nstep:414/10000 train_loss:5.7979 train_time:67999ms step_avg:168.32ms\nstep:415/10000 train_loss:6.6224 train_time:68170ms step_avg:168.32ms\nstep:416/10000 train_loss:5.8921 train_time:68340ms step_avg:168.32ms\nstep:417/10000 train_loss:5.7117 train_time:68510ms step_avg:168.33ms\nstep:418/10000 train_loss:5.9057 train_time:68679ms step_avg:168.33ms\nstep:419/10000 train_loss:5.9195 train_time:68852ms step_avg:168.34ms\nstep:420/10000 train_loss:5.9168 train_time:69021ms step_avg:168.34ms\nstep:421/10000 train_loss:5.8039 train_time:69193ms step_avg:168.35ms\nstep:422/10000 train_loss:6.1257 train_time:69363ms step_avg:168.36ms\nstep:423/10000 train_loss:6.0179 train_time:69534ms step_avg:168.36ms\nstep:424/10000 train_loss:6.0658 train_time:69703ms step_avg:168.37ms\nstep:425/10000 train_loss:5.8734 train_time:69873ms step_avg:168.37ms\nstep:426/10000 train_loss:5.8806 train_time:70042ms step_avg:168.37ms\nstep:427/10000 train_loss:5.9937 train_time:70212ms step_avg:168.37ms\nstep:428/10000 train_loss:5.8146 train_time:70383ms step_avg:168.38ms\nstep:429/10000 train_loss:5.8995 train_time:70554ms step_avg:168.39ms\nstep:430/10000 train_loss:5.5830 train_time:70722ms step_avg:168.39ms\nstep:431/10000 train_loss:5.7216 train_time:70891ms step_avg:168.39ms\nstep:432/10000 train_loss:5.7096 train_time:71060ms step_avg:168.39ms\nstep:433/10000 train_loss:5.8726 train_time:71228ms step_avg:168.39ms\nstep:434/10000 train_loss:5.8160 train_time:71400ms step_avg:168.40ms\nstep:435/10000 train_loss:5.7456 train_time:71570ms step_avg:168.40ms\nstep:436/10000 train_loss:5.9738 train_time:71739ms step_avg:168.40ms\nstep:437/10000 train_loss:5.7680 train_time:71908ms step_avg:168.40ms\nstep:438/10000 train_loss:5.7969 train_time:72077ms step_avg:168.40ms\nstep:439/10000 train_loss:5.8245 train_time:72246ms step_avg:168.41ms\nstep:440/10000 train_loss:5.9180 train_time:72414ms step_avg:168.41ms\nstep:441/10000 train_loss:5.9313 train_time:72585ms step_avg:168.41ms\nstep:442/10000 train_loss:5.8142 train_time:72753ms step_avg:168.41ms\nstep:443/10000 train_loss:5.7846 train_time:72923ms step_avg:168.41ms\nstep:444/10000 train_loss:5.7922 train_time:73092ms step_avg:168.41ms\nstep:445/10000 train_loss:5.8353 train_time:73263ms step_avg:168.42ms\nstep:446/10000 train_loss:5.8736 train_time:73435ms step_avg:168.43ms\nstep:447/10000 train_loss:5.8787 train_time:73604ms step_avg:168.43ms\nstep:448/10000 train_loss:5.8598 train_time:73774ms step_avg:168.43ms\nstep:449/10000 train_loss:5.8219 train_time:73944ms step_avg:168.44ms\nstep:450/10000 train_loss:5.8114 train_time:74112ms step_avg:168.44ms\nstep:451/10000 train_loss:5.8633 train_time:74283ms step_avg:168.44ms\nstep:452/10000 train_loss:5.6028 train_time:74449ms step_avg:168.44ms\nstep:453/10000 train_loss:5.9861 train_time:74621ms step_avg:168.44ms\nstep:454/10000 train_loss:5.6267 train_time:74792ms step_avg:168.45ms\nstep:455/10000 train_loss:5.7640 train_time:74963ms step_avg:168.46ms\nstep:456/10000 train_loss:5.8356 train_time:75132ms step_avg:168.46ms\nstep:457/10000 train_loss:5.7340 train_time:75301ms step_avg:168.46ms\nstep:458/10000 train_loss:5.8329 train_time:75469ms step_avg:168.46ms\nstep:459/10000 train_loss:5.8104 train_time:75639ms step_avg:168.46ms\nstep:460/10000 train_loss:5.7125 train_time:75811ms step_avg:168.47ms\nstep:461/10000 train_loss:5.6769 train_time:75979ms step_avg:168.47ms\nstep:462/10000 train_loss:5.6421 train_time:76149ms step_avg:168.47ms\nstep:463/10000 train_loss:5.7867 train_time:76317ms step_avg:168.47ms\nstep:464/10000 train_loss:5.8841 train_time:76486ms step_avg:168.47ms\nstep:465/10000 train_loss:5.7568 train_time:76654ms step_avg:168.47ms\nstep:466/10000 train_loss:5.6584 train_time:76824ms step_avg:168.47ms\nstep:467/10000 train_loss:5.8135 train_time:76995ms step_avg:168.48ms\nstep:468/10000 train_loss:5.6648 train_time:77167ms step_avg:168.49ms\nstep:469/10000 train_loss:5.7536 train_time:77336ms step_avg:168.49ms\nstep:470/10000 train_loss:5.7580 train_time:77506ms step_avg:168.49ms\nstep:471/10000 train_loss:5.8162 train_time:77674ms step_avg:168.49ms\nstep:472/10000 train_loss:5.7383 train_time:77843ms step_avg:168.49ms\nstep:473/10000 train_loss:5.6244 train_time:78014ms step_avg:168.50ms\nstep:474/10000 train_loss:5.7888 train_time:78185ms step_avg:168.50ms\nstep:475/10000 train_loss:5.7726 train_time:78356ms step_avg:168.51ms\nstep:476/10000 train_loss:5.7431 train_time:78528ms step_avg:168.51ms\nstep:477/10000 train_loss:5.7809 train_time:78696ms step_avg:168.51ms\nstep:478/10000 train_loss:5.7249 train_time:78866ms step_avg:168.52ms\nstep:479/10000 train_loss:5.7002 train_time:79035ms step_avg:168.52ms\nstep:480/10000 train_loss:5.8093 train_time:79204ms step_avg:168.52ms\nstep:481/10000 train_loss:5.7128 train_time:79371ms step_avg:168.52ms\nstep:482/10000 train_loss:5.6035 train_time:79539ms step_avg:168.52ms\nstep:483/10000 train_loss:5.7124 train_time:79709ms step_avg:168.52ms\nstep:484/10000 train_loss:5.7491 train_time:79878ms step_avg:168.52ms\nstep:485/10000 train_loss:5.7834 train_time:80050ms step_avg:168.53ms\nstep:486/10000 train_loss:5.6553 train_time:80223ms step_avg:168.54ms\nstep:487/10000 train_loss:5.6531 train_time:80395ms step_avg:168.54ms\nstep:488/10000 train_loss:5.6403 train_time:80566ms step_avg:168.55ms\nstep:489/10000 train_loss:5.5055 train_time:80735ms step_avg:168.55ms\nstep:490/10000 train_loss:5.5463 train_time:80905ms step_avg:168.55ms\nstep:491/10000 train_loss:5.6823 train_time:81074ms step_avg:168.55ms\nstep:492/10000 train_loss:5.7418 train_time:81244ms step_avg:168.56ms\nstep:493/10000 train_loss:5.7172 train_time:81410ms step_avg:168.55ms\nstep:494/10000 train_loss:5.8754 train_time:81579ms step_avg:168.55ms\nstep:495/10000 train_loss:5.5767 train_time:81749ms step_avg:168.55ms\nstep:496/10000 train_loss:5.6240 train_time:81919ms step_avg:168.56ms\nstep:497/10000 train_loss:5.6687 train_time:82090ms step_avg:168.56ms\nstep:498/10000 train_loss:5.6624 train_time:82263ms step_avg:168.57ms\nstep:499/10000 train_loss:5.7073 train_time:82433ms step_avg:168.57ms\nstep:500/10000 train_loss:5.5026 train_time:82602ms step_avg:168.58ms\nstep:501/10000 train_loss:5.6244 train_time:82774ms step_avg:168.58ms\nstep:502/10000 train_loss:5.6814 train_time:82943ms step_avg:168.58ms\nstep:503/10000 train_loss:5.5779 train_time:83115ms step_avg:168.59ms\nstep:504/10000 train_loss:5.5184 train_time:83283ms step_avg:168.59ms\nstep:505/10000 train_loss:5.6213 train_time:83451ms step_avg:168.59ms\nstep:506/10000 train_loss:5.7473 train_time:83619ms step_avg:168.59ms\nstep:507/10000 train_loss:5.7748 train_time:83799ms step_avg:168.61ms\nstep:508/10000 train_loss:5.6787 train_time:83961ms step_avg:168.60ms\nstep:509/10000 train_loss:5.8309 train_time:84130ms step_avg:168.60ms\nstep:510/10000 train_loss:5.5827 train_time:84303ms step_avg:168.61ms\nstep:511/10000 train_loss:5.5582 train_time:84474ms step_avg:168.61ms\nstep:512/10000 train_loss:5.7031 train_time:84643ms step_avg:168.61ms\nstep:513/10000 train_loss:5.6260 train_time:84811ms step_avg:168.61ms\nstep:514/10000 train_loss:5.8299 train_time:84980ms step_avg:168.61ms\nstep:515/10000 train_loss:5.6465 train_time:85155ms step_avg:168.62ms\nstep:516/10000 train_loss:5.6665 train_time:85319ms step_avg:168.62ms\nstep:517/10000 train_loss:5.5641 train_time:85490ms step_avg:168.62ms\nstep:518/10000 train_loss:5.6473 train_time:85658ms step_avg:168.62ms\nstep:519/10000 train_loss:5.4211 train_time:85830ms step_avg:168.62ms\nstep:520/10000 train_loss:5.7178 train_time:85999ms step_avg:168.63ms\nstep:521/10000 train_loss:5.4951 train_time:86171ms step_avg:168.63ms\nstep:522/10000 train_loss:5.6786 train_time:86341ms step_avg:168.64ms\nstep:523/10000 train_loss:5.5376 train_time:86513ms step_avg:168.64ms\nstep:524/10000 train_loss:5.8174 train_time:86681ms step_avg:168.64ms\nstep:525/10000 train_loss:5.9170 train_time:86851ms step_avg:168.64ms\nstep:526/10000 train_loss:5.6518 train_time:87022ms step_avg:168.65ms\nstep:527/10000 train_loss:5.5852 train_time:87195ms step_avg:168.66ms\nstep:528/10000 train_loss:5.5707 train_time:87363ms step_avg:168.66ms\nstep:529/10000 train_loss:5.5839 train_time:87534ms step_avg:168.66ms\nstep:530/10000 train_loss:5.6408 train_time:87705ms step_avg:168.66ms\nstep:531/10000 train_loss:5.6133 train_time:87873ms step_avg:168.66ms\nstep:532/10000 train_loss:5.5432 train_time:88045ms step_avg:168.67ms\nstep:533/10000 train_loss:5.5975 train_time:88218ms step_avg:168.68ms\nstep:534/10000 train_loss:5.5975 train_time:88388ms step_avg:168.68ms\nstep:535/10000 train_loss:5.5591 train_time:88556ms step_avg:168.68ms\nstep:536/10000 train_loss:5.5954 train_time:88726ms step_avg:168.68ms\nstep:537/10000 train_loss:5.7121 train_time:88895ms step_avg:168.68ms\nstep:538/10000 train_loss:5.6121 train_time:89066ms step_avg:168.69ms\nstep:539/10000 train_loss:5.6645 train_time:89236ms step_avg:168.69ms\nstep:540/10000 train_loss:5.5205 train_time:89407ms step_avg:168.69ms\nstep:541/10000 train_loss:5.4689 train_time:89575ms step_avg:168.69ms\nstep:542/10000 train_loss:5.4312 train_time:89747ms step_avg:168.70ms\nstep:543/10000 train_loss:5.6623 train_time:89915ms step_avg:168.70ms\nstep:544/10000 train_loss:5.4650 train_time:90088ms step_avg:168.70ms\nstep:545/10000 train_loss:5.5871 train_time:90259ms step_avg:168.71ms\nstep:546/10000 train_loss:5.5684 train_time:90430ms step_avg:168.71ms\nstep:547/10000 train_loss:5.4392 train_time:90598ms step_avg:168.71ms\nstep:548/10000 train_loss:5.5947 train_time:90768ms step_avg:168.71ms\nstep:549/10000 train_loss:5.4936 train_time:90935ms step_avg:168.71ms\nstep:550/10000 train_loss:5.5728 train_time:91105ms step_avg:168.71ms\nstep:551/10000 train_loss:5.5630 train_time:91275ms step_avg:168.71ms\nstep:552/10000 train_loss:5.5427 train_time:91445ms step_avg:168.72ms\nstep:553/10000 train_loss:5.4987 train_time:91613ms step_avg:168.72ms\nstep:554/10000 train_loss:5.5724 train_time:91783ms step_avg:168.72ms\nstep:555/10000 train_loss:5.4945 train_time:91954ms step_avg:168.72ms\nstep:556/10000 train_loss:5.5585 train_time:92126ms step_avg:168.73ms\nstep:557/10000 train_loss:5.5753 train_time:92293ms step_avg:168.73ms\nstep:558/10000 train_loss:5.5551 train_time:92463ms step_avg:168.73ms\nstep:559/10000 train_loss:5.4948 train_time:92633ms step_avg:168.73ms\nstep:560/10000 train_loss:5.4445 train_time:92803ms step_avg:168.73ms\nstep:561/10000 train_loss:5.6815 train_time:92971ms step_avg:168.73ms\nstep:562/10000 train_loss:5.5002 train_time:93140ms step_avg:168.73ms\nstep:563/10000 train_loss:5.3782 train_time:93310ms step_avg:168.73ms\nstep:564/10000 train_loss:5.4529 train_time:93478ms step_avg:168.73ms\nstep:565/10000 train_loss:5.6250 train_time:93646ms step_avg:168.73ms\nstep:566/10000 train_loss:5.5901 train_time:93814ms step_avg:168.73ms\nstep:567/10000 train_loss:5.7417 train_time:93983ms step_avg:168.73ms\nstep:568/10000 train_loss:5.4535 train_time:94153ms step_avg:168.73ms\nstep:569/10000 train_loss:5.6220 train_time:94324ms step_avg:168.74ms\nstep:570/10000 train_loss:5.3687 train_time:94492ms step_avg:168.74ms\nstep:571/10000 train_loss:5.5041 train_time:94662ms step_avg:168.74ms\nstep:572/10000 train_loss:5.6707 train_time:94832ms step_avg:168.74ms\nstep:573/10000 train_loss:5.5966 train_time:95002ms step_avg:168.74ms\nstep:574/10000 train_loss:5.5297 train_time:95172ms step_avg:168.75ms\nstep:575/10000 train_loss:5.4333 train_time:95343ms step_avg:168.75ms\nstep:576/10000 train_loss:6.0124 train_time:95512ms step_avg:168.75ms\nstep:577/10000 train_loss:5.4609 train_time:95683ms step_avg:168.75ms\nstep:578/10000 train_loss:5.4908 train_time:95850ms step_avg:168.75ms\nstep:579/10000 train_loss:5.5374 train_time:96019ms step_avg:168.75ms\nstep:580/10000 train_loss:5.4768 train_time:96190ms step_avg:168.76ms\nstep:581/10000 train_loss:5.5181 train_time:96360ms step_avg:168.76ms\nstep:582/10000 train_loss:5.4387 train_time:96530ms step_avg:168.76ms\nstep:583/10000 train_loss:5.4974 train_time:96698ms step_avg:168.76ms\nstep:584/10000 train_loss:5.5373 train_time:96871ms step_avg:168.76ms\nstep:585/10000 train_loss:5.4015 train_time:97043ms step_avg:168.77ms\nstep:586/10000 train_loss:5.5107 train_time:97212ms step_avg:168.77ms\nstep:587/10000 train_loss:5.4044 train_time:97382ms step_avg:168.77ms\nstep:588/10000 train_loss:5.5049 train_time:97553ms step_avg:168.78ms\nstep:589/10000 train_loss:5.4243 train_time:97722ms step_avg:168.78ms\nstep:590/10000 train_loss:5.4743 train_time:97891ms step_avg:168.78ms\nstep:591/10000 train_loss:5.4420 train_time:98062ms step_avg:168.78ms\nstep:592/10000 train_loss:5.2500 train_time:98233ms step_avg:168.79ms\nstep:593/10000 train_loss:5.5040 train_time:98404ms step_avg:168.79ms\nstep:594/10000 train_loss:5.5988 train_time:98573ms step_avg:168.79ms\nstep:595/10000 train_loss:5.3571 train_time:98743ms step_avg:168.79ms\nstep:596/10000 train_loss:5.7034 train_time:98917ms step_avg:168.80ms\nstep:597/10000 train_loss:5.5748 train_time:99085ms step_avg:168.80ms\nstep:598/10000 train_loss:5.6126 train_time:99255ms step_avg:168.80ms\nstep:599/10000 train_loss:5.6596 train_time:99425ms step_avg:168.80ms\nstep:600/10000 train_loss:5.6240 train_time:99593ms step_avg:168.80ms\nstep:601/10000 train_loss:5.6410 train_time:99764ms step_avg:168.81ms\nstep:602/10000 train_loss:5.5023 train_time:99932ms step_avg:168.80ms\nstep:603/10000 train_loss:5.6140 train_time:100103ms step_avg:168.81ms\nstep:604/10000 train_loss:5.4376 train_time:100273ms step_avg:168.81ms\nstep:605/10000 train_loss:5.3847 train_time:100442ms step_avg:168.81ms\nstep:606/10000 train_loss:5.4780 train_time:100611ms step_avg:168.81ms\nstep:607/10000 train_loss:5.4421 train_time:100779ms step_avg:168.81ms\nstep:608/10000 train_loss:5.5558 train_time:100952ms step_avg:168.82ms\nstep:609/10000 train_loss:5.3931 train_time:101121ms step_avg:168.82ms\nstep:610/10000 train_loss:5.5074 train_time:101290ms step_avg:168.82ms\nstep:611/10000 train_loss:5.5052 train_time:101460ms step_avg:168.82ms\nstep:612/10000 train_loss:5.4347 train_time:101630ms step_avg:168.82ms\nstep:613/10000 train_loss:5.2267 train_time:101800ms step_avg:168.82ms\nstep:614/10000 train_loss:5.4639 train_time:101974ms step_avg:168.83ms\nstep:615/10000 train_loss:5.4417 train_time:102143ms step_avg:168.83ms\nstep:616/10000 train_loss:5.3626 train_time:102313ms step_avg:168.83ms\nstep:617/10000 train_loss:5.4925 train_time:102481ms step_avg:168.83ms\nstep:618/10000 train_loss:5.4724 train_time:102651ms step_avg:168.83ms\nstep:619/10000 train_loss:5.5443 train_time:102821ms step_avg:168.84ms\nstep:620/10000 train_loss:5.3885 train_time:102991ms step_avg:168.84ms\nstep:621/10000 train_loss:5.5303 train_time:103161ms step_avg:168.84ms\nstep:622/10000 train_loss:5.4463 train_time:103330ms step_avg:168.84ms\nstep:623/10000 train_loss:5.3885 train_time:103499ms step_avg:168.84ms\nstep:624/10000 train_loss:5.4970 train_time:103670ms step_avg:168.84ms\nstep:625/10000 train_loss:5.5354 train_time:103840ms step_avg:168.85ms\nstep:626/10000 train_loss:5.5840 train_time:104011ms step_avg:168.85ms\nstep:627/10000 train_loss:5.3736 train_time:104182ms step_avg:168.85ms\nstep:628/10000 train_loss:5.4704 train_time:104351ms step_avg:168.85ms\nstep:629/10000 train_loss:5.4665 train_time:104519ms step_avg:168.85ms\nstep:630/10000 train_loss:5.3916 train_time:104687ms step_avg:168.85ms\nstep:631/10000 train_loss:5.3479 train_time:104855ms step_avg:168.85ms\nstep:632/10000 train_loss:5.4839 train_time:105029ms step_avg:168.86ms\nstep:633/10000 train_loss:5.3465 train_time:105196ms step_avg:168.85ms\nstep:634/10000 train_loss:5.4267 train_time:105368ms step_avg:168.86ms\nstep:635/10000 train_loss:5.5531 train_time:105537ms step_avg:168.86ms\nstep:636/10000 train_loss:5.3602 train_time:105707ms step_avg:168.86ms\nstep:637/10000 train_loss:5.3330 train_time:105875ms step_avg:168.86ms\nstep:638/10000 train_loss:5.3164 train_time:106046ms step_avg:168.86ms\nstep:639/10000 train_loss:5.5807 train_time:106216ms step_avg:168.87ms\nstep:640/10000 train_loss:5.3176 train_time:106388ms step_avg:168.87ms\nstep:641/10000 train_loss:5.3729 train_time:106555ms step_avg:168.87ms\nstep:642/10000 train_loss:5.4047 train_time:106726ms step_avg:168.87ms\nstep:643/10000 train_loss:5.4255 train_time:106894ms step_avg:168.87ms\nstep:644/10000 train_loss:5.3851 train_time:107065ms step_avg:168.87ms\nstep:645/10000 train_loss:5.2832 train_time:107238ms step_avg:168.88ms\nstep:646/10000 train_loss:5.4644 train_time:107411ms step_avg:168.88ms\nstep:647/10000 train_loss:5.4592 train_time:107580ms step_avg:168.89ms\nstep:648/10000 train_loss:5.4223 train_time:107748ms step_avg:168.88ms\nstep:649/10000 train_loss:5.3668 train_time:107917ms step_avg:168.88ms\nstep:650/10000 train_loss:5.4722 train_time:108088ms step_avg:168.89ms\nstep:651/10000 train_loss:5.5436 train_time:108259ms step_avg:168.89ms\nstep:652/10000 train_loss:5.3767 train_time:108432ms step_avg:168.90ms\nstep:653/10000 train_loss:5.3026 train_time:108601ms step_avg:168.90ms\nstep:654/10000 train_loss:5.2816 train_time:108772ms step_avg:168.90ms\nstep:655/10000 train_loss:5.4831 train_time:108943ms step_avg:168.90ms\nstep:656/10000 train_loss:5.2773 train_time:109113ms step_avg:168.91ms\nstep:657/10000 train_loss:5.2683 train_time:109285ms step_avg:168.91ms\nstep:658/10000 train_loss:5.3938 train_time:109455ms step_avg:168.91ms\nstep:659/10000 train_loss:5.3918 train_time:109627ms step_avg:168.92ms\nstep:660/10000 train_loss:5.3700 train_time:109794ms step_avg:168.91ms\nstep:661/10000 train_loss:5.1974 train_time:109963ms step_avg:168.91ms\nstep:662/10000 train_loss:5.5697 train_time:110130ms step_avg:168.91ms\nstep:663/10000 train_loss:5.3408 train_time:110301ms step_avg:168.91ms\nstep:664/10000 train_loss:5.4617 train_time:110470ms step_avg:168.91ms\nstep:665/10000 train_loss:5.4520 train_time:110639ms step_avg:168.91ms\nstep:666/10000 train_loss:5.3378 train_time:110807ms step_avg:168.91ms\nstep:667/10000 train_loss:5.3258 train_time:110977ms step_avg:168.91ms\nstep:668/10000 train_loss:5.3304 train_time:111149ms step_avg:168.92ms\nstep:669/10000 train_loss:5.1373 train_time:111319ms step_avg:168.92ms\nstep:670/10000 train_loss:5.4062 train_time:111488ms step_avg:168.92ms\nstep:671/10000 train_loss:5.4728 train_time:111656ms step_avg:168.92ms\nstep:672/10000 train_loss:5.3408 train_time:111827ms step_avg:168.92ms\nstep:673/10000 train_loss:5.2764 train_time:111996ms step_avg:168.92ms\nstep:674/10000 train_loss:5.2946 train_time:112166ms step_avg:168.92ms\nstep:675/10000 train_loss:5.2558 train_time:112334ms step_avg:168.92ms\nstep:676/10000 train_loss:5.3722 train_time:112506ms step_avg:168.93ms\nstep:677/10000 train_loss:5.3400 train_time:112674ms step_avg:168.93ms\nstep:678/10000 train_loss:5.2846 train_time:112844ms step_avg:168.93ms\nstep:679/10000 train_loss:5.4515 train_time:113015ms step_avg:168.93ms\nstep:680/10000 train_loss:5.3377 train_time:113185ms step_avg:168.93ms\nstep:681/10000 train_loss:5.3306 train_time:113355ms step_avg:168.93ms\nstep:682/10000 train_loss:5.3468 train_time:113526ms step_avg:168.94ms\nstep:683/10000 train_loss:5.4722 train_time:113695ms step_avg:168.94ms\nstep:684/10000 train_loss:4.7200 train_time:113862ms step_avg:168.94ms\nstep:685/10000 train_loss:5.3116 train_time:114031ms step_avg:168.93ms\nstep:686/10000 train_loss:5.3185 train_time:114198ms step_avg:168.93ms\nstep:687/10000 train_loss:5.5094 train_time:114369ms step_avg:168.94ms\nstep:688/10000 train_loss:5.2820 train_time:114537ms step_avg:168.93ms\nstep:689/10000 train_loss:5.5789 train_time:114707ms step_avg:168.94ms\nstep:690/10000 train_loss:5.3896 train_time:114877ms step_avg:168.94ms\nstep:691/10000 train_loss:5.4325 train_time:115049ms step_avg:168.94ms\nstep:692/10000 train_loss:5.2399 train_time:115216ms step_avg:168.94ms\nstep:693/10000 train_loss:5.2328 train_time:115387ms step_avg:168.94ms\nstep:694/10000 train_loss:5.4962 train_time:115556ms step_avg:168.94ms\nstep:695/10000 train_loss:5.2469 train_time:115724ms step_avg:168.94ms\nstep:696/10000 train_loss:5.4769 train_time:115894ms step_avg:168.94ms\nstep:697/10000 train_loss:5.3863 train_time:116066ms step_avg:168.95ms\nstep:698/10000 train_loss:5.4042 train_time:116234ms step_avg:168.94ms\nstep:699/10000 train_loss:5.2283 train_time:116404ms step_avg:168.95ms\nstep:700/10000 train_loss:5.4408 train_time:116574ms step_avg:168.95ms\nstep:701/10000 train_loss:5.5293 train_time:116743ms step_avg:168.95ms\nstep:702/10000 train_loss:5.2404 train_time:116910ms step_avg:168.94ms\nstep:703/10000 train_loss:5.3751 train_time:117080ms step_avg:168.95ms\nstep:704/10000 train_loss:5.3335 train_time:117249ms step_avg:168.95ms\nstep:705/10000 train_loss:5.3170 train_time:117417ms step_avg:168.95ms\nstep:706/10000 train_loss:5.1896 train_time:117584ms step_avg:168.94ms\nstep:707/10000 train_loss:5.1883 train_time:117752ms step_avg:168.94ms\nstep:708/10000 train_loss:5.5061 train_time:117924ms step_avg:168.95ms\nstep:709/10000 train_loss:5.3385 train_time:118093ms step_avg:168.95ms\nstep:710/10000 train_loss:5.2191 train_time:118263ms step_avg:168.95ms\nstep:711/10000 train_loss:5.1312 train_time:118432ms step_avg:168.95ms\nstep:712/10000 train_loss:5.2629 train_time:118603ms step_avg:168.95ms\nstep:713/10000 train_loss:5.4893 train_time:118770ms step_avg:168.95ms\nstep:714/10000 train_loss:5.3076 train_time:118943ms step_avg:168.95ms\nstep:715/10000 train_loss:5.4189 train_time:119114ms step_avg:168.96ms\nstep:716/10000 train_loss:5.2660 train_time:119283ms step_avg:168.96ms\nstep:717/10000 train_loss:5.2568 train_time:119450ms step_avg:168.95ms\nstep:718/10000 train_loss:5.2819 train_time:119622ms step_avg:168.96ms\nstep:719/10000 train_loss:5.3849 train_time:119793ms step_avg:168.96ms\nstep:720/10000 train_loss:5.3410 train_time:119962ms step_avg:168.96ms\nstep:721/10000 train_loss:5.3395 train_time:120132ms step_avg:168.96ms\nstep:722/10000 train_loss:5.3748 train_time:120301ms step_avg:168.96ms\nstep:723/10000 train_loss:5.3237 train_time:120472ms step_avg:168.97ms\nstep:724/10000 train_loss:5.2893 train_time:120641ms step_avg:168.97ms\nstep:725/10000 train_loss:5.1689 train_time:120809ms step_avg:168.96ms\nstep:726/10000 train_loss:5.2693 train_time:120978ms step_avg:168.96ms\nstep:727/10000 train_loss:5.2756 train_time:121150ms step_avg:168.97ms\nstep:728/10000 train_loss:5.1635 train_time:121318ms step_avg:168.97ms\nstep:729/10000 train_loss:5.3962 train_time:121485ms step_avg:168.96ms\nstep:730/10000 train_loss:5.2182 train_time:121655ms step_avg:168.97ms\nstep:731/10000 train_loss:5.1559 train_time:121825ms step_avg:168.97ms\nstep:732/10000 train_loss:5.3421 train_time:121995ms step_avg:168.97ms\nstep:733/10000 train_loss:5.2987 train_time:122165ms step_avg:168.97ms\nstep:734/10000 train_loss:5.1950 train_time:122334ms step_avg:168.97ms\nstep:735/10000 train_loss:5.2276 train_time:122505ms step_avg:168.97ms\nstep:736/10000 train_loss:5.4648 train_time:122675ms step_avg:168.97ms\nstep:737/10000 train_loss:5.2058 train_time:122842ms step_avg:168.97ms\nstep:738/10000 train_loss:5.3538 train_time:123012ms step_avg:168.97ms\nstep:739/10000 train_loss:5.1029 train_time:123181ms step_avg:168.97ms\nstep:740/10000 train_loss:5.2187 train_time:123350ms step_avg:168.97ms\nstep:741/10000 train_loss:5.2446 train_time:123519ms step_avg:168.97ms\nstep:742/10000 train_loss:5.2013 train_time:123687ms step_avg:168.97ms\nstep:743/10000 train_loss:5.2209 train_time:123856ms step_avg:168.97ms\nstep:744/10000 train_loss:5.3029 train_time:124028ms step_avg:168.98ms\nstep:745/10000 train_loss:5.1760 train_time:124200ms step_avg:168.98ms\nstep:746/10000 train_loss:5.5111 train_time:124371ms step_avg:168.98ms\nstep:747/10000 train_loss:5.2712 train_time:124541ms step_avg:168.98ms\nstep:748/10000 train_loss:5.2867 train_time:124715ms step_avg:168.99ms\nstep:749/10000 train_loss:5.4496 train_time:124887ms step_avg:168.99ms\nstep:750/10000 train_loss:5.2822 train_time:125056ms step_avg:168.99ms\nstep:751/10000 train_loss:5.1462 train_time:125226ms step_avg:169.00ms\nstep:752/10000 train_loss:5.3401 train_time:125395ms step_avg:169.00ms\nstep:753/10000 train_loss:5.2929 train_time:125565ms step_avg:169.00ms\nstep:754/10000 train_loss:5.1975 train_time:125733ms step_avg:169.00ms\nstep:755/10000 train_loss:5.3272 train_time:125903ms step_avg:169.00ms\nstep:756/10000 train_loss:4.8729 train_time:126075ms step_avg:169.00ms\nstep:757/10000 train_loss:4.8017 train_time:126247ms step_avg:169.00ms\nstep:758/10000 train_loss:5.3832 train_time:126417ms step_avg:169.01ms\nstep:759/10000 train_loss:5.2573 train_time:126587ms step_avg:169.01ms\nstep:760/10000 train_loss:5.1639 train_time:126758ms step_avg:169.01ms\nstep:761/10000 train_loss:5.0971 train_time:126928ms step_avg:169.01ms\nstep:762/10000 train_loss:5.3545 train_time:127296ms step_avg:169.28ms\nstep:763/10000 train_loss:5.1918 train_time:127460ms step_avg:169.27ms\nstep:764/10000 train_loss:5.1845 train_time:127627ms step_avg:169.27ms\nstep:765/10000 train_loss:5.1093 train_time:127795ms step_avg:169.26ms\nstep:766/10000 train_loss:5.2758 train_time:127963ms step_avg:169.26ms\nstep:767/10000 train_loss:5.2358 train_time:128131ms step_avg:169.26ms\nstep:768/10000 train_loss:5.1448 train_time:128309ms step_avg:169.27ms\nstep:769/10000 train_loss:5.3103 train_time:128480ms step_avg:169.28ms\nstep:770/10000 train_loss:5.2320 train_time:128651ms step_avg:169.28ms\nstep:771/10000 train_loss:5.3080 train_time:128823ms step_avg:169.28ms\nstep:772/10000 train_loss:5.2955 train_time:128991ms step_avg:169.28ms\nstep:773/10000 train_loss:5.1009 train_time:129165ms step_avg:169.29ms\nstep:774/10000 train_loss:5.2476 train_time:129337ms step_avg:169.29ms\nstep:775/10000 train_loss:5.2766 train_time:129507ms step_avg:169.29ms\nstep:776/10000 train_loss:5.2521 train_time:129675ms step_avg:169.29ms\nstep:777/10000 train_loss:5.2980 train_time:129845ms step_avg:169.29ms\nstep:778/10000 train_loss:5.2316 train_time:130013ms step_avg:169.29ms\nstep:779/10000 train_loss:5.3473 train_time:130183ms step_avg:169.29ms\nstep:780/10000 train_loss:5.4507 train_time:130354ms step_avg:169.29ms\nstep:781/10000 train_loss:5.2188 train_time:130525ms step_avg:169.29ms\nstep:782/10000 train_loss:5.2182 train_time:130692ms step_avg:169.29ms\nstep:783/10000 train_loss:5.1863 train_time:130864ms step_avg:169.29ms\nstep:784/10000 train_loss:5.3123 train_time:131032ms step_avg:169.29ms\nstep:785/10000 train_loss:5.1482 train_time:131201ms step_avg:169.29ms\nstep:786/10000 train_loss:5.2944 train_time:131374ms step_avg:169.30ms\nstep:787/10000 train_loss:5.2814 train_time:131546ms step_avg:169.30ms\nstep:788/10000 train_loss:5.1614 train_time:131717ms step_avg:169.30ms\nstep:789/10000 train_loss:5.1300 train_time:131887ms step_avg:169.30ms\nstep:790/10000 train_loss:5.1122 train_time:132057ms step_avg:169.30ms\nstep:791/10000 train_loss:5.3073 train_time:132227ms step_avg:169.31ms\nstep:792/10000 train_loss:5.1741 train_time:132394ms step_avg:169.30ms\nstep:793/10000 train_loss:5.1452 train_time:132566ms step_avg:169.31ms\nstep:794/10000 train_loss:5.3208 train_time:132738ms step_avg:169.31ms\nstep:795/10000 train_loss:5.1667 train_time:132907ms step_avg:169.31ms\nstep:796/10000 train_loss:5.0944 train_time:133075ms step_avg:169.31ms\nstep:797/10000 train_loss:5.1333 train_time:133246ms step_avg:169.31ms\nstep:798/10000 train_loss:5.2043 train_time:133414ms step_avg:169.31ms\nstep:799/10000 train_loss:5.3287 train_time:133586ms step_avg:169.31ms\nstep:800/10000 train_loss:5.1826 train_time:133755ms step_avg:169.31ms\nstep:801/10000 train_loss:5.1537 train_time:133927ms step_avg:169.31ms\nstep:802/10000 train_loss:5.9462 train_time:134096ms step_avg:169.31ms\nstep:803/10000 train_loss:5.3191 train_time:134266ms step_avg:169.31ms\nstep:804/10000 train_loss:5.1382 train_time:134433ms step_avg:169.31ms\nstep:805/10000 train_loss:5.2985 train_time:134604ms step_avg:169.31ms\nstep:806/10000 train_loss:5.4176 train_time:134773ms step_avg:169.31ms\nstep:807/10000 train_loss:5.1533 train_time:134942ms step_avg:169.31ms\nstep:808/10000 train_loss:5.2968 train_time:135110ms step_avg:169.31ms\nstep:809/10000 train_loss:5.3321 train_time:135279ms step_avg:169.31ms\nstep:810/10000 train_loss:5.1975 train_time:135452ms step_avg:169.31ms\nstep:811/10000 train_loss:5.1278 train_time:135624ms step_avg:169.32ms\nstep:812/10000 train_loss:5.2253 train_time:135797ms step_avg:169.32ms\nstep:813/10000 train_loss:5.3501 train_time:135967ms step_avg:169.32ms\nstep:814/10000 train_loss:5.1349 train_time:136137ms step_avg:169.32ms\nstep:815/10000 train_loss:5.2436 train_time:136305ms step_avg:169.32ms\nstep:816/10000 train_loss:5.1712 train_time:136474ms step_avg:169.32ms\nstep:817/10000 train_loss:5.1379 train_time:136645ms step_avg:169.32ms\nstep:818/10000 train_loss:5.1622 train_time:136814ms step_avg:169.32ms\nstep:819/10000 train_loss:5.2080 train_time:136986ms step_avg:169.33ms\nstep:820/10000 train_loss:5.3409 train_time:137157ms step_avg:169.33ms\nstep:821/10000 train_loss:5.1406 train_time:137328ms step_avg:169.33ms\nstep:822/10000 train_loss:5.1278 train_time:137496ms step_avg:169.33ms\nstep:823/10000 train_loss:5.1719 train_time:137669ms step_avg:169.33ms\nstep:824/10000 train_loss:5.0204 train_time:137838ms step_avg:169.33ms\nstep:825/10000 train_loss:5.1259 train_time:138010ms step_avg:169.34ms\nstep:826/10000 train_loss:5.2402 train_time:138177ms step_avg:169.33ms\nstep:827/10000 train_loss:5.2486 train_time:138346ms step_avg:169.33ms\nstep:828/10000 train_loss:5.1732 train_time:138516ms step_avg:169.34ms\nstep:829/10000 train_loss:5.1201 train_time:138686ms step_avg:169.34ms\nstep:830/10000 train_loss:5.0645 train_time:138855ms step_avg:169.34ms\nstep:831/10000 train_loss:5.1290 train_time:139026ms step_avg:169.34ms\nstep:832/10000 train_loss:5.1288 train_time:139198ms step_avg:169.34ms\nstep:833/10000 train_loss:5.2336 train_time:139367ms step_avg:169.34ms\nstep:834/10000 train_loss:5.1690 train_time:139534ms step_avg:169.34ms\nstep:835/10000 train_loss:4.9498 train_time:139704ms step_avg:169.34ms\nstep:836/10000 train_loss:5.2832 train_time:139872ms step_avg:169.34ms\nstep:837/10000 train_loss:5.1102 train_time:140043ms step_avg:169.34ms\nstep:838/10000 train_loss:5.0765 train_time:140216ms step_avg:169.34ms\nstep:839/10000 train_loss:5.1600 train_time:140388ms step_avg:169.35ms\nstep:840/10000 train_loss:5.1601 train_time:140558ms step_avg:169.35ms\nstep:841/10000 train_loss:5.2759 train_time:140728ms step_avg:169.35ms\nstep:842/10000 train_loss:5.1412 train_time:140898ms step_avg:169.35ms\nstep:843/10000 train_loss:5.2679 train_time:141068ms step_avg:169.35ms\nstep:844/10000 train_loss:5.0980 train_time:141237ms step_avg:169.35ms\nstep:845/10000 train_loss:5.1717 train_time:141408ms step_avg:169.35ms\nstep:846/10000 train_loss:5.1414 train_time:141576ms step_avg:169.35ms\nstep:847/10000 train_loss:5.1801 train_time:141745ms step_avg:169.35ms\nstep:848/10000 train_loss:5.3026 train_time:141915ms step_avg:169.35ms\nstep:849/10000 train_loss:5.1328 train_time:142087ms step_avg:169.35ms\nstep:850/10000 train_loss:5.0952 train_time:142256ms step_avg:169.35ms\nstep:851/10000 train_loss:5.3897 train_time:142425ms step_avg:169.35ms\nstep:852/10000 train_loss:5.0750 train_time:142593ms step_avg:169.35ms\nstep:853/10000 train_loss:5.1819 train_time:142763ms step_avg:169.35ms\nstep:854/10000 train_loss:5.1407 train_time:142933ms step_avg:169.35ms\nstep:855/10000 train_loss:5.2567 train_time:143102ms step_avg:169.35ms\nstep:856/10000 train_loss:5.1700 train_time:143272ms step_avg:169.35ms\nstep:857/10000 train_loss:5.0233 train_time:143440ms step_avg:169.35ms\nstep:858/10000 train_loss:5.1640 train_time:143607ms step_avg:169.35ms\nstep:859/10000 train_loss:5.3034 train_time:143777ms step_avg:169.35ms\nstep:860/10000 train_loss:5.0323 train_time:143948ms step_avg:169.35ms\nstep:861/10000 train_loss:5.3582 train_time:144117ms step_avg:169.35ms\nstep:862/10000 train_loss:5.0791 train_time:144287ms step_avg:169.35ms\nstep:863/10000 train_loss:5.1538 train_time:144456ms step_avg:169.35ms\nstep:864/10000 train_loss:5.1553 train_time:144630ms step_avg:169.36ms\nstep:865/10000 train_loss:5.0315 train_time:144799ms step_avg:169.36ms\nstep:866/10000 train_loss:5.1589 train_time:144971ms step_avg:169.36ms\nstep:867/10000 train_loss:5.0066 train_time:145141ms step_avg:169.36ms\nstep:868/10000 train_loss:5.1241 train_time:145309ms step_avg:169.36ms\nstep:869/10000 train_loss:5.0579 train_time:145475ms step_avg:169.35ms\nstep:870/10000 train_loss:5.0979 train_time:145646ms step_avg:169.36ms\nstep:871/10000 train_loss:5.1285 train_time:145815ms step_avg:169.36ms\nstep:872/10000 train_loss:5.0634 train_time:145986ms step_avg:169.36ms\nstep:873/10000 train_loss:5.0301 train_time:146157ms step_avg:169.36ms\nstep:874/10000 train_loss:4.9303 train_time:146328ms step_avg:169.36ms\nstep:875/10000 train_loss:5.1528 train_time:146496ms step_avg:169.36ms\nstep:876/10000 train_loss:5.0891 train_time:146666ms step_avg:169.36ms\nstep:877/10000 train_loss:5.1790 train_time:146836ms step_avg:169.36ms\nstep:878/10000 train_loss:5.0855 train_time:147004ms step_avg:169.36ms\nstep:879/10000 train_loss:4.9605 train_time:147172ms step_avg:169.36ms\nstep:880/10000 train_loss:5.1034 train_time:147342ms step_avg:169.36ms\nstep:881/10000 train_loss:5.0420 train_time:147513ms step_avg:169.36ms\nstep:882/10000 train_loss:5.1601 train_time:147684ms step_avg:169.36ms\nstep:883/10000 train_loss:5.0357 train_time:147852ms step_avg:169.36ms\nstep:884/10000 train_loss:5.1591 train_time:148022ms step_avg:169.36ms\nstep:885/10000 train_loss:5.0301 train_time:148191ms step_avg:169.36ms\nstep:886/10000 train_loss:5.1495 train_time:148361ms step_avg:169.36ms\nstep:887/10000 train_loss:5.0404 train_time:148531ms step_avg:169.36ms\nstep:888/10000 train_loss:5.1569 train_time:148700ms step_avg:169.36ms\nstep:889/10000 train_loss:5.1198 train_time:148868ms step_avg:169.36ms\nstep:890/10000 train_loss:5.1014 train_time:149038ms step_avg:169.36ms\nstep:891/10000 train_loss:5.0553 train_time:149209ms step_avg:169.36ms\nstep:892/10000 train_loss:5.1374 train_time:149378ms step_avg:169.36ms\nstep:893/10000 train_loss:5.0978 train_time:149547ms step_avg:169.36ms\nstep:894/10000 train_loss:5.0758 train_time:149715ms step_avg:169.36ms\nstep:895/10000 train_loss:5.2243 train_time:149886ms step_avg:169.36ms\nstep:896/10000 train_loss:5.2165 train_time:150056ms step_avg:169.36ms\nstep:897/10000 train_loss:4.9904 train_time:150228ms step_avg:169.37ms\nstep:898/10000 train_loss:5.1130 train_time:150396ms step_avg:169.36ms\nstep:899/10000 train_loss:5.2085 train_time:150565ms step_avg:169.36ms\nstep:900/10000 train_loss:4.9271 train_time:150735ms step_avg:169.36ms\nstep:901/10000 train_loss:5.1135 train_time:150904ms step_avg:169.37ms\nstep:902/10000 train_loss:5.0069 train_time:151073ms step_avg:169.36ms\nstep:903/10000 train_loss:5.1496 train_time:151244ms step_avg:169.37ms\nstep:904/10000 train_loss:5.0014 train_time:151415ms step_avg:169.37ms\nstep:905/10000 train_loss:5.1576 train_time:151585ms step_avg:169.37ms\nstep:906/10000 train_loss:5.0033 train_time:151756ms step_avg:169.37ms\nstep:907/10000 train_loss:5.0505 train_time:151927ms step_avg:169.37ms\nstep:908/10000 train_loss:4.9720 train_time:152096ms step_avg:169.37ms\nstep:909/10000 train_loss:5.1321 train_time:152268ms step_avg:169.37ms\nstep:910/10000 train_loss:5.0156 train_time:152437ms step_avg:169.37ms\nstep:911/10000 train_loss:5.1888 train_time:152606ms step_avg:169.37ms\nstep:912/10000 train_loss:5.1299 train_time:152776ms step_avg:169.37ms\nstep:913/10000 train_loss:5.1334 train_time:152948ms step_avg:169.38ms\nstep:914/10000 train_loss:5.1671 train_time:153117ms step_avg:169.38ms\nstep:915/10000 train_loss:5.0908 train_time:153288ms step_avg:169.38ms\nstep:916/10000 train_loss:5.0120 train_time:153458ms step_avg:169.38ms\nstep:917/10000 train_loss:5.0488 train_time:153630ms step_avg:169.38ms\nstep:918/10000 train_loss:5.0861 train_time:153798ms step_avg:169.38ms\nstep:919/10000 train_loss:5.2366 train_time:153971ms step_avg:169.39ms\nstep:920/10000 train_loss:5.1376 train_time:154140ms step_avg:169.38ms\nstep:921/10000 train_loss:5.0736 train_time:154308ms step_avg:169.38ms\nstep:922/10000 train_loss:5.0265 train_time:154477ms step_avg:169.38ms\nstep:923/10000 train_loss:4.9150 train_time:154651ms step_avg:169.39ms\nstep:924/10000 train_loss:5.0268 train_time:154819ms step_avg:169.39ms\nstep:925/10000 train_loss:4.9137 train_time:154990ms step_avg:169.39ms\nstep:926/10000 train_loss:5.1610 train_time:155159ms step_avg:169.39ms\nstep:927/10000 train_loss:5.0086 train_time:155329ms step_avg:169.39ms\nstep:928/10000 train_loss:4.9264 train_time:155500ms step_avg:169.39ms\nstep:929/10000 train_loss:5.1612 train_time:155670ms step_avg:169.39ms\nstep:930/10000 train_loss:4.9880 train_time:155840ms step_avg:169.39ms\nstep:931/10000 train_loss:5.0247 train_time:156010ms step_avg:169.39ms\nstep:932/10000 train_loss:5.1720 train_time:156180ms step_avg:169.39ms\nstep:933/10000 train_loss:5.2064 train_time:156349ms step_avg:169.39ms\nstep:934/10000 train_loss:5.0313 train_time:156517ms step_avg:169.39ms\nstep:935/10000 train_loss:5.0228 train_time:156688ms step_avg:169.39ms\nstep:936/10000 train_loss:5.0056 train_time:156857ms step_avg:169.39ms\nstep:937/10000 train_loss:5.1100 train_time:157026ms step_avg:169.39ms\nstep:938/10000 train_loss:5.2014 train_time:157198ms step_avg:169.39ms\nstep:939/10000 train_loss:5.2293 train_time:157370ms step_avg:169.40ms\nstep:940/10000 train_loss:4.8799 train_time:157539ms step_avg:169.40ms\nstep:941/10000 train_loss:5.2242 train_time:157709ms step_avg:169.40ms\nstep:942/10000 train_loss:5.0744 train_time:157880ms step_avg:169.40ms\nstep:943/10000 train_loss:4.9517 train_time:158052ms step_avg:169.40ms\nstep:944/10000 train_loss:5.2105 train_time:158221ms step_avg:169.40ms\nstep:945/10000 train_loss:4.9004 train_time:158390ms step_avg:169.40ms\nstep:946/10000 train_loss:5.2048 train_time:158559ms step_avg:169.40ms\nstep:947/10000 train_loss:5.1691 train_time:158728ms step_avg:169.40ms\nstep:948/10000 train_loss:5.0457 train_time:158896ms step_avg:169.40ms\nstep:949/10000 train_loss:4.8743 train_time:159069ms step_avg:169.40ms\nstep:950/10000 train_loss:5.1139 train_time:159238ms step_avg:169.40ms\nstep:951/10000 train_loss:5.1021 train_time:159408ms step_avg:169.40ms\nstep:952/10000 train_loss:4.9946 train_time:159576ms step_avg:169.40ms\nstep:953/10000 train_loss:5.2050 train_time:159748ms step_avg:169.40ms\nstep:954/10000 train_loss:5.0417 train_time:159917ms step_avg:169.40ms\nstep:955/10000 train_loss:5.0790 train_time:160096ms step_avg:169.41ms\nstep:956/10000 train_loss:5.1173 train_time:160259ms step_avg:169.41ms\nstep:957/10000 train_loss:4.9361 train_time:160433ms step_avg:169.41ms\nstep:958/10000 train_loss:5.0681 train_time:160604ms step_avg:169.41ms\nstep:959/10000 train_loss:4.9374 train_time:160773ms step_avg:169.41ms\nstep:960/10000 train_loss:5.1219 train_time:160942ms step_avg:169.41ms\nstep:961/10000 train_loss:4.9244 train_time:161114ms step_avg:169.42ms\nstep:962/10000 train_loss:5.3488 train_time:161286ms step_avg:169.42ms\nstep:963/10000 train_loss:5.0568 train_time:161456ms step_avg:169.42ms\nstep:964/10000 train_loss:5.0036 train_time:161628ms step_avg:169.42ms\nstep:965/10000 train_loss:5.1369 train_time:161796ms step_avg:169.42ms\nstep:966/10000 train_loss:5.0326 train_time:161969ms step_avg:169.42ms\nstep:967/10000 train_loss:5.1817 train_time:162136ms step_avg:169.42ms\nstep:968/10000 train_loss:5.1960 train_time:162308ms step_avg:169.42ms\nstep:969/10000 train_loss:5.1407 train_time:162478ms step_avg:169.42ms\nstep:970/10000 train_loss:5.3664 train_time:162651ms step_avg:169.43ms\nstep:971/10000 train_loss:5.1285 train_time:162820ms step_avg:169.43ms\nstep:972/10000 train_loss:5.0005 train_time:162990ms step_avg:169.43ms\nstep:973/10000 train_loss:4.9387 train_time:163160ms step_avg:169.43ms\nstep:974/10000 train_loss:4.9593 train_time:163329ms step_avg:169.43ms\nstep:975/10000 train_loss:4.9165 train_time:163498ms step_avg:169.43ms\nstep:976/10000 train_loss:4.9854 train_time:163667ms step_avg:169.43ms\nstep:977/10000 train_loss:5.0907 train_time:163838ms step_avg:169.43ms\nstep:978/10000 train_loss:5.0210 train_time:164008ms step_avg:169.43ms\nstep:979/10000 train_loss:5.1557 train_time:164177ms step_avg:169.43ms\nstep:980/10000 train_loss:5.0663 train_time:164348ms step_avg:169.43ms\nstep:981/10000 train_loss:5.0350 train_time:164517ms step_avg:169.43ms\nstep:982/10000 train_loss:5.0451 train_time:164687ms step_avg:169.43ms\nstep:983/10000 train_loss:5.0241 train_time:164857ms step_avg:169.43ms\nstep:984/10000 train_loss:5.0222 train_time:165027ms step_avg:169.43ms\nstep:985/10000 train_loss:4.9482 train_time:165195ms step_avg:169.43ms\nstep:986/10000 train_loss:5.0063 train_time:165367ms step_avg:169.43ms\nstep:987/10000 train_loss:5.1553 train_time:165535ms step_avg:169.43ms\nstep:988/10000 train_loss:4.9966 train_time:165706ms step_avg:169.43ms\nstep:989/10000 train_loss:4.8284 train_time:165871ms step_avg:169.43ms\nstep:990/10000 train_loss:5.0441 train_time:166043ms step_avg:169.43ms\nstep:991/10000 train_loss:5.0005 train_time:166215ms step_avg:169.43ms\nstep:992/10000 train_loss:5.2010 train_time:166385ms step_avg:169.44ms\nstep:993/10000 train_loss:4.9589 train_time:166553ms step_avg:169.43ms\nstep:994/10000 train_loss:4.8862 train_time:166724ms step_avg:169.43ms\nstep:995/10000 train_loss:5.1256 train_time:166892ms step_avg:169.43ms\nstep:996/10000 train_loss:4.8980 train_time:167062ms step_avg:169.43ms\nstep:997/10000 train_loss:5.0622 train_time:167232ms step_avg:169.43ms\nstep:998/10000 train_loss:5.0764 train_time:167403ms step_avg:169.44ms\nstep:999/10000 train_loss:4.9887 train_time:167570ms step_avg:169.43ms\nstep:1000/10000 train_loss:4.9189 train_time:167739ms step_avg:169.43ms\nstep:1001/10000 train_loss:4.9847 train_time:167910ms step_avg:169.43ms\nstep:1002/10000 train_loss:4.9960 train_time:168079ms step_avg:169.43ms\nstep:1003/10000 train_loss:4.7720 train_time:168250ms step_avg:169.44ms\nstep:1004/10000 train_loss:5.0513 train_time:168419ms step_avg:169.44ms\nstep:1005/10000 train_loss:5.0813 train_time:168589ms step_avg:169.44ms\nstep:1006/10000 train_loss:4.8513 train_time:168757ms step_avg:169.43ms\nstep:1007/10000 train_loss:5.2369 train_time:168930ms step_avg:169.44ms\nstep:1008/10000 train_loss:5.0413 train_time:169100ms step_avg:169.44ms\nstep:1009/10000 train_loss:4.9549 train_time:169271ms step_avg:169.44ms\nstep:1010/10000 train_loss:4.8487 train_time:169439ms step_avg:169.44ms\nstep:1011/10000 train_loss:5.0158 train_time:169608ms step_avg:169.44ms\nstep:1012/10000 train_loss:4.9436 train_time:169776ms step_avg:169.44ms\nstep:1013/10000 train_loss:4.8885 train_time:169944ms step_avg:169.44ms\nstep:1014/10000 train_loss:4.9518 train_time:170114ms step_avg:169.44ms\nstep:1015/10000 train_loss:4.9389 train_time:170287ms step_avg:169.44ms\nstep:1016/10000 train_loss:4.9627 train_time:170458ms step_avg:169.44ms\nstep:1017/10000 train_loss:4.9118 train_time:170628ms step_avg:169.44ms\nstep:1018/10000 train_loss:4.9496 train_time:170798ms step_avg:169.44ms\nstep:1019/10000 train_loss:4.8360 train_time:170969ms step_avg:169.44ms\nstep:1020/10000 train_loss:5.0626 train_time:171138ms step_avg:169.44ms\nstep:1021/10000 train_loss:5.1128 train_time:171307ms step_avg:169.44ms\nstep:1022/10000 train_loss:4.8637 train_time:171479ms step_avg:169.45ms\nstep:1023/10000 train_loss:5.0430 train_time:171651ms step_avg:169.45ms\nstep:1024/10000 train_loss:5.0817 train_time:171820ms step_avg:169.45ms\nstep:1025/10000 train_loss:4.9700 train_time:171989ms step_avg:169.45ms\nstep:1026/10000 train_loss:4.9575 train_time:172165ms step_avg:169.45ms\nstep:1027/10000 train_loss:4.8538 train_time:172333ms step_avg:169.45ms\nstep:1028/10000 train_loss:5.0053 train_time:172502ms step_avg:169.45ms\nstep:1029/10000 train_loss:4.9967 train_time:172673ms step_avg:169.45ms\nstep:1030/10000 train_loss:5.1087 train_time:172846ms step_avg:169.46ms\nstep:1031/10000 train_loss:4.9400 train_time:173014ms step_avg:169.46ms\nstep:1032/10000 train_loss:4.9192 train_time:173185ms step_avg:169.46ms\nstep:1033/10000 train_loss:5.2236 train_time:173354ms step_avg:169.46ms\nstep:1034/10000 train_loss:4.8530 train_time:173523ms step_avg:169.46ms\nstep:1035/10000 train_loss:4.8984 train_time:173688ms step_avg:169.45ms\nstep:1036/10000 train_loss:4.9369 train_time:173861ms step_avg:169.45ms\nstep:1037/10000 train_loss:4.8802 train_time:174035ms step_avg:169.46ms\nstep:1038/10000 train_loss:4.9813 train_time:174205ms step_avg:169.46ms\nstep:1039/10000 train_loss:4.9015 train_time:174377ms step_avg:169.46ms\nstep:1040/10000 train_loss:4.9921 train_time:174546ms step_avg:169.46ms\nstep:1041/10000 train_loss:5.0397 train_time:174714ms step_avg:169.46ms\nstep:1042/10000 train_loss:4.8945 train_time:174882ms step_avg:169.46ms\nstep:1043/10000 train_loss:4.9788 train_time:175055ms step_avg:169.46ms\nstep:1044/10000 train_loss:5.0184 train_time:175225ms step_avg:169.46ms\nstep:1045/10000 train_loss:4.7827 train_time:175392ms step_avg:169.46ms\nstep:1046/10000 train_loss:4.9059 train_time:175560ms step_avg:169.46ms\nstep:1047/10000 train_loss:4.7668 train_time:175729ms step_avg:169.46ms\nstep:1048/10000 train_loss:4.9345 train_time:175897ms step_avg:169.46ms\nstep:1049/10000 train_loss:5.0715 train_time:176067ms step_avg:169.46ms\nstep:1050/10000 train_loss:5.0106 train_time:176239ms step_avg:169.46ms\nstep:1051/10000 train_loss:5.0115 train_time:176410ms step_avg:169.46ms\nstep:1052/10000 train_loss:4.8998 train_time:176579ms step_avg:169.46ms\nstep:1053/10000 train_loss:4.9746 train_time:176750ms step_avg:169.46ms\nstep:1054/10000 train_loss:5.0191 train_time:176921ms step_avg:169.46ms\nstep:1055/10000 train_loss:4.9320 train_time:177092ms step_avg:169.47ms\nstep:1056/10000 train_loss:4.9189 train_time:177263ms step_avg:169.47ms\nstep:1057/10000 train_loss:5.0128 train_time:177433ms step_avg:169.47ms\nstep:1058/10000 train_loss:5.3824 train_time:177601ms step_avg:169.47ms\nstep:1059/10000 train_loss:4.8808 train_time:177770ms step_avg:169.47ms\nstep:1060/10000 train_loss:4.7914 train_time:177938ms step_avg:169.46ms\nstep:1061/10000 train_loss:4.7072 train_time:178108ms step_avg:169.47ms\nstep:1062/10000 train_loss:4.9742 train_time:178279ms step_avg:169.47ms\nstep:1063/10000 train_loss:5.0582 train_time:178452ms step_avg:169.47ms\nstep:1064/10000 train_loss:4.8372 train_time:178622ms step_avg:169.47ms\nstep:1065/10000 train_loss:4.8569 train_time:178791ms step_avg:169.47ms\nstep:1066/10000 train_loss:5.0787 train_time:178962ms step_avg:169.47ms\nstep:1067/10000 train_loss:5.0410 train_time:179131ms step_avg:169.47ms\nstep:1068/10000 train_loss:4.8921 train_time:179303ms step_avg:169.47ms\nstep:1069/10000 train_loss:4.7792 train_time:179474ms step_avg:169.47ms\nstep:1070/10000 train_loss:4.8705 train_time:179646ms step_avg:169.48ms\nstep:1071/10000 train_loss:4.9908 train_time:179813ms step_avg:169.47ms\nstep:1072/10000 train_loss:4.8100 train_time:179984ms step_avg:169.48ms\nstep:1073/10000 train_loss:4.9943 train_time:180153ms step_avg:169.48ms\nstep:1074/10000 train_loss:4.8320 train_time:180325ms step_avg:169.48ms\nstep:1075/10000 train_loss:4.7911 train_time:180497ms step_avg:169.48ms\nstep:1076/10000 train_loss:4.8380 train_time:180669ms step_avg:169.48ms\nstep:1077/10000 train_loss:4.9064 train_time:180838ms step_avg:169.48ms\nstep:1078/10000 train_loss:4.9790 train_time:181005ms step_avg:169.48ms\nstep:1079/10000 train_loss:4.8491 train_time:181175ms step_avg:169.48ms\nstep:1080/10000 train_loss:5.0164 train_time:181345ms step_avg:169.48ms\nstep:1081/10000 train_loss:4.7564 train_time:181513ms step_avg:169.48ms\nstep:1082/10000 train_loss:5.0284 train_time:181683ms step_avg:169.48ms\nstep:1083/10000 train_loss:4.9282 train_time:181853ms step_avg:169.48ms\nstep:1084/10000 train_loss:4.9617 train_time:182021ms step_avg:169.48ms\nstep:1085/10000 train_loss:5.0038 train_time:182189ms step_avg:169.48ms\nstep:1086/10000 train_loss:4.9626 train_time:182358ms step_avg:169.48ms\nstep:1087/10000 train_loss:4.9472 train_time:182530ms step_avg:169.48ms\nstep:1088/10000 train_loss:4.7985 train_time:182700ms step_avg:169.48ms\nstep:1089/10000 train_loss:4.8590 train_time:182872ms step_avg:169.48ms\nstep:1090/10000 train_loss:4.8246 train_time:183042ms step_avg:169.48ms\nstep:1091/10000 train_loss:4.9190 train_time:183213ms step_avg:169.48ms\nstep:1092/10000 train_loss:4.8379 train_time:183385ms step_avg:169.49ms\nstep:1093/10000 train_loss:4.9130 train_time:183553ms step_avg:169.49ms\nstep:1094/10000 train_loss:5.0685 train_time:183721ms step_avg:169.48ms\nstep:1095/10000 train_loss:4.8561 train_time:183890ms step_avg:169.48ms\nstep:1096/10000 train_loss:4.9858 train_time:184059ms step_avg:169.48ms\nstep:1097/10000 train_loss:4.9988 train_time:184233ms step_avg:169.49ms\nstep:1098/10000 train_loss:4.8195 train_time:184403ms step_avg:169.49ms\nstep:1099/10000 train_loss:4.7557 train_time:184573ms step_avg:169.49ms\nstep:1100/10000 train_loss:4.9968 train_time:184745ms step_avg:169.49ms\nstep:1101/10000 train_loss:4.8236 train_time:184914ms step_avg:169.49ms\nstep:1102/10000 train_loss:4.9705 train_time:185084ms step_avg:169.49ms\nstep:1103/10000 train_loss:4.9439 train_time:185253ms step_avg:169.49ms\nstep:1104/10000 train_loss:5.1841 train_time:185424ms step_avg:169.49ms\nstep:1105/10000 train_loss:4.8877 train_time:185592ms step_avg:169.49ms\nstep:1106/10000 train_loss:4.9754 train_time:185762ms step_avg:169.49ms\nstep:1107/10000 train_loss:4.8785 train_time:185932ms step_avg:169.49ms\nstep:1108/10000 train_loss:4.9421 train_time:186101ms step_avg:169.49ms\nstep:1109/10000 train_loss:4.8971 train_time:186270ms step_avg:169.49ms\nstep:1110/10000 train_loss:4.7827 train_time:186442ms step_avg:169.49ms\nstep:1111/10000 train_loss:4.8779 train_time:186614ms step_avg:169.50ms\nstep:1112/10000 train_loss:5.0769 train_time:186787ms step_avg:169.50ms\nstep:1113/10000 train_loss:4.8158 train_time:186955ms step_avg:169.50ms\nstep:1114/10000 train_loss:5.0933 train_time:187125ms step_avg:169.50ms\nstep:1115/10000 train_loss:4.8417 train_time:187295ms step_avg:169.50ms\nstep:1116/10000 train_loss:4.9083 train_time:187466ms step_avg:169.50ms\nstep:1117/10000 train_loss:4.7382 train_time:187636ms step_avg:169.50ms\nstep:1118/10000 train_loss:4.9157 train_time:187809ms step_avg:169.50ms\nstep:1119/10000 train_loss:4.9412 train_time:187977ms step_avg:169.50ms\nstep:1120/10000 train_loss:4.8848 train_time:188147ms step_avg:169.50ms\nstep:1121/10000 train_loss:4.9969 train_time:188316ms step_avg:169.50ms\nstep:1122/10000 train_loss:4.8652 train_time:188487ms step_avg:169.50ms\nstep:1123/10000 train_loss:4.7951 train_time:188658ms step_avg:169.50ms\nstep:1124/10000 train_loss:4.8885 train_time:188831ms step_avg:169.51ms\nstep:1125/10000 train_loss:4.8428 train_time:189001ms step_avg:169.51ms\nstep:1126/10000 train_loss:4.8897 train_time:189174ms step_avg:169.51ms\nstep:1127/10000 train_loss:4.9319 train_time:189342ms step_avg:169.51ms\nstep:1128/10000 train_loss:5.0175 train_time:189513ms step_avg:169.51ms\nstep:1129/10000 train_loss:4.9271 train_time:189684ms step_avg:169.51ms\nstep:1130/10000 train_loss:4.9943 train_time:189852ms step_avg:169.51ms\nstep:1131/10000 train_loss:4.8480 train_time:190022ms step_avg:169.51ms\nstep:1132/10000 train_loss:4.8283 train_time:190193ms step_avg:169.51ms\nstep:1133/10000 train_loss:4.8205 train_time:190366ms step_avg:169.52ms\nstep:1134/10000 train_loss:4.9024 train_time:190533ms step_avg:169.51ms\nstep:1135/10000 train_loss:4.8880 train_time:190702ms step_avg:169.51ms\nstep:1136/10000 train_loss:4.9861 train_time:190872ms step_avg:169.51ms\nstep:1137/10000 train_loss:4.7534 train_time:191042ms step_avg:169.51ms\nstep:1138/10000 train_loss:4.8083 train_time:191213ms step_avg:169.51ms\nstep:1139/10000 train_loss:4.9892 train_time:191386ms step_avg:169.52ms\nstep:1140/10000 train_loss:4.9578 train_time:191556ms step_avg:169.52ms\nstep:1141/10000 train_loss:4.9430 train_time:191725ms step_avg:169.52ms\nstep:1142/10000 train_loss:4.9289 train_time:191896ms step_avg:169.52ms\nstep:1143/10000 train_loss:4.8999 train_time:192067ms step_avg:169.52ms\nstep:1144/10000 train_loss:4.9949 train_time:192243ms step_avg:169.53ms\nstep:1145/10000 train_loss:4.8924 train_time:192409ms step_avg:169.52ms\nstep:1146/10000 train_loss:4.8789 train_time:192578ms step_avg:169.52ms\nstep:1147/10000 train_loss:4.8635 train_time:192749ms step_avg:169.52ms\nstep:1148/10000 train_loss:4.8705 train_time:192920ms step_avg:169.53ms\nstep:1149/10000 train_loss:4.8686 train_time:193095ms step_avg:169.53ms\nstep:1150/10000 train_loss:4.9057 train_time:193264ms step_avg:169.53ms\nstep:1151/10000 train_loss:4.8055 train_time:193433ms step_avg:169.53ms\nstep:1152/10000 train_loss:4.9139 train_time:193603ms step_avg:169.53ms\nstep:1153/10000 train_loss:4.8794 train_time:193774ms step_avg:169.53ms\nstep:1154/10000 train_loss:5.0054 train_time:193942ms step_avg:169.53ms\nstep:1155/10000 train_loss:4.8994 train_time:194113ms step_avg:169.53ms\nstep:1156/10000 train_loss:4.7602 train_time:194282ms step_avg:169.53ms\nstep:1157/10000 train_loss:4.6711 train_time:194450ms step_avg:169.53ms\nstep:1158/10000 train_loss:4.9816 train_time:194621ms step_avg:169.53ms\nstep:1159/10000 train_loss:4.7691 train_time:194793ms step_avg:169.53ms\nstep:1160/10000 train_loss:4.7756 train_time:194963ms step_avg:169.53ms\nstep:1161/10000 train_loss:4.9854 train_time:195136ms step_avg:169.54ms\nstep:1162/10000 train_loss:4.7957 train_time:195305ms step_avg:169.54ms\nstep:1163/10000 train_loss:4.8211 train_time:195474ms step_avg:169.53ms\nstep:1164/10000 train_loss:4.9197 train_time:195644ms step_avg:169.54ms\nstep:1165/10000 train_loss:4.8605 train_time:195812ms step_avg:169.53ms\nstep:1166/10000 train_loss:4.7604 train_time:195982ms step_avg:169.53ms\nstep:1167/10000 train_loss:4.8565 train_time:196153ms step_avg:169.54ms\nstep:1168/10000 train_loss:4.7968 train_time:196321ms step_avg:169.53ms\nstep:1169/10000 train_loss:4.8934 train_time:196487ms step_avg:169.53ms\nstep:1170/10000 train_loss:4.9528 train_time:196657ms step_avg:169.53ms\nstep:1171/10000 train_loss:4.8640 train_time:196827ms step_avg:169.53ms\nstep:1172/10000 train_loss:4.8445 train_time:196994ms step_avg:169.53ms\nstep:1173/10000 train_loss:4.8983 train_time:197171ms step_avg:169.54ms\nstep:1174/10000 train_loss:4.7533 train_time:197339ms step_avg:169.54ms\nstep:1175/10000 train_loss:4.7395 train_time:197507ms step_avg:169.53ms\nstep:1176/10000 train_loss:4.8352 train_time:197677ms step_avg:169.53ms\nstep:1177/10000 train_loss:5.0993 train_time:197847ms step_avg:169.54ms\nstep:1178/10000 train_loss:4.9577 train_time:198015ms step_avg:169.53ms\nstep:1179/10000 train_loss:5.0645 train_time:198184ms step_avg:169.53ms\nstep:1180/10000 train_loss:4.7307 train_time:198352ms step_avg:169.53ms\nstep:1181/10000 train_loss:4.6045 train_time:198518ms step_avg:169.53ms\nstep:1182/10000 train_loss:4.9615 train_time:198692ms step_avg:169.53ms\nstep:1183/10000 train_loss:4.9824 train_time:198861ms step_avg:169.53ms\nstep:1184/10000 train_loss:4.6979 train_time:199029ms step_avg:169.53ms\nstep:1185/10000 train_loss:4.8482 train_time:199203ms step_avg:169.53ms\nstep:1186/10000 train_loss:4.7754 train_time:199376ms step_avg:169.54ms\nstep:1187/10000 train_loss:4.9333 train_time:199541ms step_avg:169.53ms\nstep:1188/10000 train_loss:4.7310 train_time:199712ms step_avg:169.54ms\nstep:1189/10000 train_loss:4.9724 train_time:199880ms step_avg:169.53ms\nstep:1190/10000 train_loss:4.9481 train_time:200049ms step_avg:169.53ms\nstep:1191/10000 train_loss:4.8324 train_time:200216ms step_avg:169.53ms\nstep:1192/10000 train_loss:4.8371 train_time:200386ms step_avg:169.53ms\nstep:1193/10000 train_loss:4.9755 train_time:200553ms step_avg:169.53ms\nstep:1194/10000 train_loss:4.7974 train_time:200727ms step_avg:169.53ms\nstep:1195/10000 train_loss:4.7961 train_time:200895ms step_avg:169.53ms\nstep:1196/10000 train_loss:4.7393 train_time:201064ms step_avg:169.53ms\nstep:1197/10000 train_loss:4.7950 train_time:201236ms step_avg:169.53ms\nstep:1198/10000 train_loss:5.0480 train_time:201402ms step_avg:169.53ms\nstep:1199/10000 train_loss:4.7043 train_time:201572ms step_avg:169.53ms\nstep:1200/10000 train_loss:4.9177 train_time:201743ms step_avg:169.53ms\nstep:1201/10000 train_loss:4.8782 train_time:201917ms step_avg:169.54ms\nstep:1202/10000 train_loss:4.7860 train_time:202088ms step_avg:169.54ms\nstep:1203/10000 train_loss:4.8065 train_time:202259ms step_avg:169.54ms\nstep:1204/10000 train_loss:4.8693 train_time:202429ms step_avg:169.54ms\nstep:1205/10000 train_loss:4.7751 train_time:202597ms step_avg:169.54ms\nstep:1206/10000 train_loss:4.8292 train_time:202765ms step_avg:169.54ms\nstep:1207/10000 train_loss:4.8778 train_time:202938ms step_avg:169.54ms\nstep:1208/10000 train_loss:4.7790 train_time:203109ms step_avg:169.54ms\nstep:1209/10000 train_loss:4.8622 train_time:203280ms step_avg:169.54ms\nstep:1210/10000 train_loss:4.8128 train_time:203455ms step_avg:169.55ms\nstep:1211/10000 train_loss:4.8320 train_time:203620ms step_avg:169.54ms\nstep:1212/10000 train_loss:4.8559 train_time:203794ms step_avg:169.55ms\nstep:1213/10000 train_loss:4.8025 train_time:203963ms step_avg:169.55ms\nstep:1214/10000 train_loss:4.8526 train_time:204133ms step_avg:169.55ms\nstep:1215/10000 train_loss:4.7650 train_time:204302ms step_avg:169.55ms\nstep:1216/10000 train_loss:4.8354 train_time:204472ms step_avg:169.55ms\nstep:1217/10000 train_loss:4.8305 train_time:204639ms step_avg:169.54ms\nstep:1218/10000 train_loss:4.8067 train_time:204808ms step_avg:169.54ms\nstep:1219/10000 train_loss:4.8236 train_time:204976ms step_avg:169.54ms\nstep:1220/10000 train_loss:5.2780 train_time:205142ms step_avg:169.54ms\nstep:1221/10000 train_loss:4.6627 train_time:205311ms step_avg:169.54ms\nstep:1222/10000 train_loss:4.9054 train_time:205483ms step_avg:169.54ms\nstep:1223/10000 train_loss:4.5781 train_time:205649ms step_avg:169.54ms\nstep:1224/10000 train_loss:4.7919 train_time:205820ms step_avg:169.54ms\nstep:1225/10000 train_loss:4.7589 train_time:205992ms step_avg:169.54ms\nstep:1226/10000 train_loss:4.8459 train_time:206159ms step_avg:169.54ms\nstep:1227/10000 train_loss:4.9148 train_time:206327ms step_avg:169.54ms\nstep:1228/10000 train_loss:4.7045 train_time:206494ms step_avg:169.54ms\nstep:1229/10000 train_loss:4.8627 train_time:206664ms step_avg:169.54ms\nstep:1230/10000 train_loss:4.7879 train_time:206835ms step_avg:169.54ms\nstep:1231/10000 train_loss:4.8147 train_time:207006ms step_avg:169.54ms\nstep:1232/10000 train_loss:4.9136 train_time:207174ms step_avg:169.54ms\nstep:1233/10000 train_loss:4.6974 train_time:207344ms step_avg:169.54ms\nstep:1234/10000 train_loss:4.9443 train_time:207514ms step_avg:169.54ms\nstep:1235/10000 train_loss:4.6965 train_time:207685ms step_avg:169.54ms\nstep:1236/10000 train_loss:4.6740 train_time:207855ms step_avg:169.54ms\nstep:1237/10000 train_loss:4.7058 train_time:208023ms step_avg:169.54ms\nstep:1238/10000 train_loss:4.7730 train_time:208193ms step_avg:169.54ms\nstep:1239/10000 train_loss:4.9925 train_time:208363ms step_avg:169.54ms\nstep:1240/10000 train_loss:4.9150 train_time:208530ms step_avg:169.54ms\nstep:1241/10000 train_loss:4.8440 train_time:208701ms step_avg:169.54ms\nstep:1242/10000 train_loss:4.8372 train_time:208872ms step_avg:169.54ms\nstep:1243/10000 train_loss:4.9063 train_time:209043ms step_avg:169.54ms\nstep:1244/10000 train_loss:4.8556 train_time:209213ms step_avg:169.54ms\nstep:1245/10000 train_loss:4.8787 train_time:209383ms step_avg:169.54ms\nstep:1246/10000 train_loss:4.8154 train_time:209554ms step_avg:169.54ms\nstep:1247/10000 train_loss:4.9096 train_time:209727ms step_avg:169.54ms\nstep:1248/10000 train_loss:4.7618 train_time:209896ms step_avg:169.54ms\nstep:1249/10000 train_loss:4.8342 train_time:210067ms step_avg:169.55ms\nstep:1250/10000 train_loss:4.7039 train_time:210236ms step_avg:169.54ms\nstep:1251/10000 train_loss:4.7804 train_time:210405ms step_avg:169.54ms\nstep:1252/10000 train_loss:4.9343 train_time:210573ms step_avg:169.54ms\nstep:1253/10000 train_loss:4.8787 train_time:210743ms step_avg:169.54ms\nstep:1254/10000 train_loss:4.7392 train_time:210912ms step_avg:169.54ms\nstep:1255/10000 train_loss:4.9159 train_time:211084ms step_avg:169.55ms\nstep:1256/10000 train_loss:5.0108 train_time:211254ms step_avg:169.55ms\nstep:1257/10000 train_loss:4.7426 train_time:211425ms step_avg:169.55ms\nstep:1258/10000 train_loss:4.8918 train_time:211594ms step_avg:169.55ms\nstep:1259/10000 train_loss:4.8189 train_time:211766ms step_avg:169.55ms\nstep:1260/10000 train_loss:4.8354 train_time:211934ms step_avg:169.55ms\nstep:1261/10000 train_loss:4.8605 train_time:212105ms step_avg:169.55ms\nstep:1262/10000 train_loss:4.5264 train_time:212276ms step_avg:169.55ms\nstep:1263/10000 train_loss:4.8622 train_time:212445ms step_avg:169.55ms\nstep:1264/10000 train_loss:4.7417 train_time:212612ms step_avg:169.55ms\nstep:1265/10000 train_loss:4.8402 train_time:212783ms step_avg:169.55ms\nstep:1266/10000 train_loss:4.8048 train_time:212953ms step_avg:169.55ms\nstep:1267/10000 train_loss:4.7651 train_time:213123ms step_avg:169.55ms\nstep:1268/10000 train_loss:4.7575 train_time:213293ms step_avg:169.55ms\nstep:1269/10000 train_loss:4.6843 train_time:213462ms step_avg:169.55ms\nstep:1270/10000 train_loss:4.7836 train_time:213632ms step_avg:169.55ms\nstep:1271/10000 train_loss:4.7431 train_time:213801ms step_avg:169.55ms\nstep:1272/10000 train_loss:4.7287 train_time:213973ms step_avg:169.55ms\nstep:1273/10000 train_loss:4.6096 train_time:214144ms step_avg:169.55ms\nstep:1274/10000 train_loss:4.6913 train_time:214317ms step_avg:169.55ms\nstep:1275/10000 train_loss:4.7204 train_time:214489ms step_avg:169.56ms\nstep:1276/10000 train_loss:4.6860 train_time:214656ms step_avg:169.55ms\nstep:1277/10000 train_loss:4.9245 train_time:214824ms step_avg:169.55ms\nstep:1278/10000 train_loss:4.7532 train_time:214994ms step_avg:169.55ms\nstep:1279/10000 train_loss:4.8026 train_time:215164ms step_avg:169.55ms\nstep:1280/10000 train_loss:4.7571 train_time:215338ms step_avg:169.56ms\nstep:1281/10000 train_loss:4.8428 train_time:215508ms step_avg:169.56ms\nstep:1282/10000 train_loss:4.7090 train_time:215675ms step_avg:169.56ms\nstep:1283/10000 train_loss:4.7855 train_time:215846ms step_avg:169.56ms\nstep:1284/10000 train_loss:4.7928 train_time:216014ms step_avg:169.56ms\nstep:1285/10000 train_loss:4.6464 train_time:216184ms step_avg:169.56ms\nstep:1286/10000 train_loss:4.8023 train_time:216352ms step_avg:169.55ms\nstep:1287/10000 train_loss:4.8719 train_time:216522ms step_avg:169.56ms\nstep:1288/10000 train_loss:4.7442 train_time:216692ms step_avg:169.56ms\nstep:1289/10000 train_loss:4.8028 train_time:216863ms step_avg:169.56ms\nstep:1290/10000 train_loss:4.6782 train_time:217032ms step_avg:169.56ms\nstep:1291/10000 train_loss:4.7536 train_time:217201ms step_avg:169.56ms\nstep:1292/10000 train_loss:4.8366 train_time:217375ms step_avg:169.56ms\nstep:1293/10000 train_loss:4.8257 train_time:217544ms step_avg:169.56ms\nstep:1294/10000 train_loss:4.7758 train_time:217713ms step_avg:169.56ms\nstep:1295/10000 train_loss:4.8259 train_time:217883ms step_avg:169.56ms\nstep:1296/10000 train_loss:4.8788 train_time:218052ms step_avg:169.56ms\nstep:1297/10000 train_loss:4.7119 train_time:218224ms step_avg:169.56ms\nstep:1298/10000 train_loss:4.7546 train_time:218395ms step_avg:169.56ms\nstep:1299/10000 train_loss:4.7458 train_time:218565ms step_avg:169.56ms\nstep:1300/10000 train_loss:4.9105 train_time:218733ms step_avg:169.56ms\nstep:1301/10000 train_loss:4.8603 train_time:218903ms step_avg:169.56ms\nstep:1302/10000 train_loss:4.7113 train_time:219073ms step_avg:169.56ms\nstep:1303/10000 train_loss:4.8773 train_time:219242ms step_avg:169.56ms\nstep:1304/10000 train_loss:4.7688 train_time:219412ms step_avg:169.56ms\nstep:1305/10000 train_loss:4.8103 train_time:219585ms step_avg:169.56ms\nstep:1306/10000 train_loss:4.7482 train_time:219755ms step_avg:169.56ms\nstep:1307/10000 train_loss:4.8599 train_time:219926ms step_avg:169.56ms\nstep:1308/10000 train_loss:5.2270 train_time:220093ms step_avg:169.56ms\nstep:1309/10000 train_loss:4.6514 train_time:220262ms step_avg:169.56ms\nstep:1310/10000 train_loss:4.8830 train_time:220433ms step_avg:169.56ms\nstep:1311/10000 train_loss:4.9247 train_time:220605ms step_avg:169.57ms\nstep:1312/10000 train_loss:4.9208 train_time:220777ms step_avg:169.57ms\nstep:1313/10000 train_loss:4.8394 train_time:220946ms step_avg:169.57ms\nstep:1314/10000 train_loss:4.8447 train_time:221114ms step_avg:169.57ms\nstep:1315/10000 train_loss:4.7727 train_time:221287ms step_avg:169.57ms\nstep:1316/10000 train_loss:4.7081 train_time:221455ms step_avg:169.57ms\nstep:1317/10000 train_loss:4.8079 train_time:221625ms step_avg:169.57ms\nstep:1318/10000 train_loss:4.7749 train_time:221793ms step_avg:169.57ms\nstep:1319/10000 train_loss:4.9713 train_time:221963ms step_avg:169.57ms\nstep:1320/10000 train_loss:4.7073 train_time:222130ms step_avg:169.57ms\nstep:1321/10000 train_loss:4.8411 train_time:222300ms step_avg:169.57ms\nstep:1322/10000 train_loss:4.7295 train_time:222470ms step_avg:169.57ms\nstep:1323/10000 train_loss:4.6972 train_time:222639ms step_avg:169.57ms\nstep:1324/10000 train_loss:4.8006 train_time:222811ms step_avg:169.57ms\nstep:1325/10000 train_loss:4.6897 train_time:222979ms step_avg:169.57ms\nstep:1326/10000 train_loss:5.2643 train_time:223149ms step_avg:169.57ms\nstep:1327/10000 train_loss:4.7026 train_time:223316ms step_avg:169.56ms\nstep:1328/10000 train_loss:4.7233 train_time:223486ms step_avg:169.56ms\nstep:1329/10000 train_loss:4.6550 train_time:223656ms step_avg:169.57ms\nstep:1330/10000 train_loss:4.6815 train_time:223830ms step_avg:169.57ms\nstep:1331/10000 train_loss:4.6634 train_time:223999ms step_avg:169.57ms\nstep:1332/10000 train_loss:4.7314 train_time:224173ms step_avg:169.57ms\nstep:1333/10000 train_loss:4.6533 train_time:224340ms step_avg:169.57ms\nstep:1334/10000 train_loss:4.6510 train_time:224509ms step_avg:169.57ms\nstep:1335/10000 train_loss:4.6328 train_time:224678ms step_avg:169.57ms\nstep:1336/10000 train_loss:4.6660 train_time:224847ms step_avg:169.57ms\nstep:1337/10000 train_loss:4.6485 train_time:225016ms step_avg:169.57ms\nstep:1338/10000 train_loss:4.7463 train_time:225188ms step_avg:169.57ms\nstep:1339/10000 train_loss:4.7829 train_time:225356ms step_avg:169.57ms\nstep:1340/10000 train_loss:4.8679 train_time:225527ms step_avg:169.57ms\nstep:1341/10000 train_loss:4.7423 train_time:225695ms step_avg:169.57ms\nstep:1342/10000 train_loss:4.7831 train_time:225865ms step_avg:169.57ms\nstep:1343/10000 train_loss:4.6903 train_time:226035ms step_avg:169.57ms\nstep:1344/10000 train_loss:4.7945 train_time:226206ms step_avg:169.57ms\nstep:1345/10000 train_loss:4.6288 train_time:226375ms step_avg:169.57ms\nstep:1346/10000 train_loss:4.7387 train_time:226546ms step_avg:169.57ms\nstep:1347/10000 train_loss:4.6233 train_time:226715ms step_avg:169.57ms\nstep:1348/10000 train_loss:5.3180 train_time:226885ms step_avg:169.57ms\nstep:1349/10000 train_loss:5.8792 train_time:227053ms step_avg:169.57ms\nstep:1350/10000 train_loss:4.7455 train_time:227221ms step_avg:169.57ms\nstep:1351/10000 train_loss:4.8317 train_time:227390ms step_avg:169.57ms\nstep:1352/10000 train_loss:4.7682 train_time:227562ms step_avg:169.57ms\nstep:1353/10000 train_loss:4.7510 train_time:227733ms step_avg:169.57ms\nstep:1354/10000 train_loss:4.7252 train_time:227901ms step_avg:169.57ms\nstep:1355/10000 train_loss:4.8319 train_time:228069ms step_avg:169.57ms\nstep:1356/10000 train_loss:4.7661 train_time:228239ms step_avg:169.57ms\nstep:1357/10000 train_loss:4.6711 train_time:228411ms step_avg:169.57ms\nstep:1358/10000 train_loss:4.6690 train_time:228580ms step_avg:169.57ms\nstep:1359/10000 train_loss:4.7129 train_time:228750ms step_avg:169.57ms\nstep:1360/10000 train_loss:4.7103 train_time:228921ms step_avg:169.57ms\nstep:1361/10000 train_loss:4.7181 train_time:229093ms step_avg:169.57ms\nstep:1362/10000 train_loss:4.6237 train_time:229262ms step_avg:169.57ms\nstep:1363/10000 train_loss:4.6522 train_time:229433ms step_avg:169.57ms\nstep:1364/10000 train_loss:4.6597 train_time:229605ms step_avg:169.58ms\nstep:1365/10000 train_loss:4.7688 train_time:229773ms step_avg:169.57ms\nstep:1366/10000 train_loss:4.6770 train_time:229942ms step_avg:169.57ms\nstep:1367/10000 train_loss:4.6882 train_time:230112ms step_avg:169.57ms\nstep:1368/10000 train_loss:4.6405 train_time:230280ms step_avg:169.57ms\nstep:1369/10000 train_loss:4.7106 train_time:230452ms step_avg:169.57ms\nstep:1370/10000 train_loss:4.6523 train_time:230623ms step_avg:169.58ms\nstep:1371/10000 train_loss:4.7208 train_time:230794ms step_avg:169.58ms\nstep:1372/10000 train_loss:4.7035 train_time:230967ms step_avg:169.58ms\nstep:1373/10000 train_loss:4.6693 train_time:231136ms step_avg:169.58ms\nstep:1374/10000 train_loss:4.8117 train_time:231305ms step_avg:169.58ms\nstep:1375/10000 train_loss:4.7277 train_time:231475ms step_avg:169.58ms\nstep:1376/10000 train_loss:4.7914 train_time:231645ms step_avg:169.58ms\nstep:1377/10000 train_loss:4.4632 train_time:231813ms step_avg:169.58ms\nstep:1378/10000 train_loss:4.6739 train_time:231981ms step_avg:169.58ms\nstep:1379/10000 train_loss:4.8167 train_time:232150ms step_avg:169.58ms\nstep:1380/10000 train_loss:4.6911 train_time:232321ms step_avg:169.58ms\nstep:1381/10000 train_loss:4.6591 train_time:232494ms step_avg:169.58ms\nstep:1382/10000 train_loss:4.6502 train_time:232665ms step_avg:169.58ms\nstep:1383/10000 train_loss:4.6177 train_time:232835ms step_avg:169.58ms\nstep:1384/10000 train_loss:4.6387 train_time:233004ms step_avg:169.58ms\nstep:1385/10000 train_loss:4.6110 train_time:233174ms step_avg:169.58ms\nstep:1386/10000 train_loss:4.7342 train_time:233341ms step_avg:169.58ms\nstep:1387/10000 train_loss:4.6354 train_time:233511ms step_avg:169.58ms\nstep:1388/10000 train_loss:4.6880 train_time:233680ms step_avg:169.58ms\nstep:1389/10000 train_loss:4.7161 train_time:233855ms step_avg:169.58ms\nstep:1390/10000 train_loss:4.6147 train_time:234022ms step_avg:169.58ms\nstep:1391/10000 train_loss:4.6598 train_time:234193ms step_avg:169.58ms\nstep:1392/10000 train_loss:4.6787 train_time:234362ms step_avg:169.58ms\nstep:1393/10000 train_loss:4.7519 train_time:234532ms step_avg:169.58ms\nstep:1394/10000 train_loss:4.6937 train_time:234702ms step_avg:169.58ms\nstep:1395/10000 train_loss:4.6082 train_time:234870ms step_avg:169.58ms\nstep:1396/10000 train_loss:4.5920 train_time:235040ms step_avg:169.58ms\nstep:1397/10000 train_loss:4.4444 train_time:235211ms step_avg:169.58ms\nstep:1398/10000 train_loss:4.6927 train_time:235380ms step_avg:169.58ms\nstep:1399/10000 train_loss:4.5851 train_time:235547ms step_avg:169.58ms\nstep:1400/10000 train_loss:4.2642 train_time:235716ms step_avg:169.58ms\nstep:1401/10000 train_loss:4.6536 train_time:235885ms step_avg:169.58ms\nstep:1402/10000 train_loss:4.8914 train_time:236054ms step_avg:169.58ms\nstep:1403/10000 train_loss:4.6192 train_time:236226ms step_avg:169.58ms\nstep:1404/10000 train_loss:4.5677 train_time:236394ms step_avg:169.58ms\nstep:1405/10000 train_loss:4.6952 train_time:236563ms step_avg:169.58ms\nstep:1406/10000 train_loss:4.6029 train_time:236731ms step_avg:169.58ms\nstep:1407/10000 train_loss:4.7068 train_time:236902ms step_avg:169.58ms\nstep:1408/10000 train_loss:4.9577 train_time:237069ms step_avg:169.58ms\nstep:1409/10000 train_loss:4.6151 train_time:237238ms step_avg:169.58ms\nstep:1410/10000 train_loss:4.6390 train_time:237407ms step_avg:169.58ms\nstep:1411/10000 train_loss:4.6447 train_time:237577ms step_avg:169.58ms\nstep:1412/10000 train_loss:4.4993 train_time:237747ms step_avg:169.58ms\nstep:1413/10000 train_loss:4.7058 train_time:237916ms step_avg:169.58ms\nstep:1414/10000 train_loss:4.6599 train_time:238086ms step_avg:169.58ms\nstep:1415/10000 train_loss:4.6811 train_time:238255ms step_avg:169.58ms\nstep:1416/10000 train_loss:4.7028 train_time:238425ms step_avg:169.58ms\nstep:1417/10000 train_loss:4.5728 train_time:238595ms step_avg:169.58ms\nstep:1418/10000 train_loss:4.5370 train_time:238767ms step_avg:169.58ms\nstep:1419/10000 train_loss:4.5710 train_time:238936ms step_avg:169.58ms\nstep:1420/10000 train_loss:4.6319 train_time:239106ms step_avg:169.58ms\nstep:1421/10000 train_loss:4.7012 train_time:239275ms step_avg:169.58ms\nstep:1422/10000 train_loss:4.7141 train_time:239444ms step_avg:169.58ms\nstep:1423/10000 train_loss:4.7063 train_time:239610ms step_avg:169.58ms\nstep:1424/10000 train_loss:4.7143 train_time:239779ms step_avg:169.57ms\nstep:1425/10000 train_loss:4.5908 train_time:239949ms step_avg:169.58ms\nstep:1426/10000 train_loss:4.5999 train_time:240120ms step_avg:169.58ms\nstep:1427/10000 train_loss:4.4930 train_time:240288ms step_avg:169.58ms\nstep:1428/10000 train_loss:4.7139 train_time:240459ms step_avg:169.58ms\nstep:1429/10000 train_loss:4.6309 train_time:240630ms step_avg:169.58ms\nstep:1430/10000 train_loss:4.6952 train_time:240799ms step_avg:169.58ms\nstep:1431/10000 train_loss:4.8662 train_time:240968ms step_avg:169.58ms\nstep:1432/10000 train_loss:4.6384 train_time:241138ms step_avg:169.58ms\nstep:1433/10000 train_loss:4.5174 train_time:241307ms step_avg:169.58ms\nstep:1434/10000 train_loss:4.7091 train_time:241475ms step_avg:169.58ms\nstep:1435/10000 train_loss:4.7736 train_time:241647ms step_avg:169.58ms\nstep:1436/10000 train_loss:4.8355 train_time:241816ms step_avg:169.58ms\nstep:1437/10000 train_loss:4.6394 train_time:241984ms step_avg:169.58ms\nstep:1438/10000 train_loss:4.6033 train_time:242154ms step_avg:169.58ms\nstep:1439/10000 train_loss:4.8370 train_time:242325ms step_avg:169.58ms\nstep:1440/10000 train_loss:4.4178 train_time:242495ms step_avg:169.58ms\nstep:1441/10000 train_loss:4.6110 train_time:242665ms step_avg:169.58ms\nstep:1442/10000 train_loss:4.8948 train_time:242836ms step_avg:169.58ms\nstep:1443/10000 train_loss:4.7485 train_time:243006ms step_avg:169.58ms\nstep:1444/10000 train_loss:4.8545 train_time:243175ms step_avg:169.58ms\nstep:1445/10000 train_loss:4.6825 train_time:243346ms step_avg:169.58ms\nstep:1446/10000 train_loss:4.8172 train_time:243515ms step_avg:169.58ms\nstep:1447/10000 train_loss:4.4715 train_time:243686ms step_avg:169.58ms\nstep:1448/10000 train_loss:4.7673 train_time:243854ms step_avg:169.58ms\nstep:1449/10000 train_loss:4.7071 train_time:244024ms step_avg:169.58ms\nstep:1450/10000 train_loss:4.6882 train_time:244197ms step_avg:169.58ms\nstep:1451/10000 train_loss:4.7204 train_time:244366ms step_avg:169.58ms\nstep:1452/10000 train_loss:4.6243 train_time:244536ms step_avg:169.58ms\nstep:1453/10000 train_loss:4.6401 train_time:244704ms step_avg:169.58ms\nstep:1454/10000 train_loss:4.5897 train_time:244869ms step_avg:169.58ms\nstep:1455/10000 train_loss:4.6278 train_time:245039ms step_avg:169.58ms\nstep:1456/10000 train_loss:4.5698 train_time:245206ms step_avg:169.58ms\nstep:1457/10000 train_loss:5.0218 train_time:245375ms step_avg:169.57ms\nstep:1458/10000 train_loss:4.7468 train_time:245546ms step_avg:169.58ms\nstep:1459/10000 train_loss:4.6418 train_time:245715ms step_avg:169.58ms\nstep:1460/10000 train_loss:4.7065 train_time:245884ms step_avg:169.57ms\nstep:1461/10000 train_loss:4.6722 train_time:246054ms step_avg:169.58ms\nstep:1462/10000 train_loss:4.6550 train_time:246228ms step_avg:169.58ms\nstep:1463/10000 train_loss:4.7540 train_time:246395ms step_avg:169.58ms\nstep:1464/10000 train_loss:4.6360 train_time:246566ms step_avg:169.58ms\nstep:1465/10000 train_loss:4.5801 train_time:246735ms step_avg:169.58ms\nstep:1466/10000 train_loss:4.6523 train_time:246905ms step_avg:169.58ms\nstep:1467/10000 train_loss:4.6593 train_time:247075ms step_avg:169.58ms\nstep:1468/10000 train_loss:4.6402 train_time:247244ms step_avg:169.58ms\nstep:1469/10000 train_loss:4.6023 train_time:247412ms step_avg:169.58ms\nstep:1470/10000 train_loss:4.6229 train_time:247584ms step_avg:169.58ms\nstep:1471/10000 train_loss:4.6196 train_time:247753ms step_avg:169.58ms\nstep:1472/10000 train_loss:4.6211 train_time:247921ms step_avg:169.58ms\nstep:1473/10000 train_loss:4.6081 train_time:248089ms step_avg:169.58ms\nstep:1474/10000 train_loss:4.7017 train_time:248257ms step_avg:169.57ms\nstep:1475/10000 train_loss:4.6279 train_time:248428ms step_avg:169.58ms\nstep:1476/10000 train_loss:4.5056 train_time:248597ms step_avg:169.58ms\nstep:1477/10000 train_loss:4.6099 train_time:248772ms step_avg:169.58ms\nstep:1478/10000 train_loss:4.6053 train_time:248944ms step_avg:169.58ms\nstep:1479/10000 train_loss:4.5836 train_time:249116ms step_avg:169.58ms\nstep:1480/10000 train_loss:4.6579 train_time:249284ms step_avg:169.58ms\nstep:1481/10000 train_loss:4.6212 train_time:249454ms step_avg:169.58ms\nstep:1482/10000 train_loss:4.4671 train_time:249624ms step_avg:169.58ms\nstep:1483/10000 train_loss:4.7078 train_time:249792ms step_avg:169.58ms\nstep:1484/10000 train_loss:4.6271 train_time:249963ms step_avg:169.58ms\nstep:1485/10000 train_loss:4.7203 train_time:250134ms step_avg:169.58ms\nstep:1486/10000 train_loss:4.5969 train_time:250304ms step_avg:169.58ms\nstep:1487/10000 train_loss:4.6117 train_time:250474ms step_avg:169.58ms\nstep:1488/10000 train_loss:4.4755 train_time:250645ms step_avg:169.58ms\nstep:1489/10000 train_loss:4.5866 train_time:250814ms step_avg:169.58ms\nstep:1490/10000 train_loss:4.6004 train_time:250982ms step_avg:169.58ms\nstep:1491/10000 train_loss:4.7399 train_time:251151ms step_avg:169.58ms\nstep:1492/10000 train_loss:4.6620 train_time:251321ms step_avg:169.58ms\nstep:1493/10000 train_loss:4.6320 train_time:251490ms step_avg:169.58ms\nstep:1494/10000 train_loss:4.6363 train_time:251659ms step_avg:169.58ms\nstep:1495/10000 train_loss:4.5298 train_time:251828ms step_avg:169.58ms\nstep:1496/10000 train_loss:4.5946 train_time:251999ms step_avg:169.58ms\nstep:1497/10000 train_loss:4.7047 train_time:252172ms step_avg:169.58ms\nstep:1498/10000 train_loss:4.6326 train_time:252342ms step_avg:169.58ms\nstep:1499/10000 train_loss:4.5102 train_time:252510ms step_avg:169.58ms\nstep:1500/10000 train_loss:4.5965 train_time:252681ms step_avg:169.58ms\nstep:1501/10000 train_loss:4.5876 train_time:252848ms step_avg:169.58ms\nstep:1502/10000 train_loss:4.6270 train_time:253018ms step_avg:169.58ms\nstep:1503/10000 train_loss:4.5930 train_time:253187ms step_avg:169.58ms\nstep:1504/10000 train_loss:4.6453 train_time:253354ms step_avg:169.58ms\nstep:1505/10000 train_loss:4.6771 train_time:253524ms step_avg:169.58ms\nstep:1506/10000 train_loss:4.5204 train_time:253695ms step_avg:169.58ms\nstep:1507/10000 train_loss:4.6545 train_time:253865ms step_avg:169.58ms\nstep:1508/10000 train_loss:4.5427 train_time:254034ms step_avg:169.58ms\nstep:1509/10000 train_loss:4.5195 train_time:254205ms step_avg:169.58ms\nstep:1510/10000 train_loss:3.9579 train_time:254375ms step_avg:169.58ms\nstep:1511/10000 train_loss:4.5996 train_time:254548ms step_avg:169.59ms\nstep:1512/10000 train_loss:4.6160 train_time:254716ms step_avg:169.58ms\nstep:1513/10000 train_loss:4.7648 train_time:254885ms step_avg:169.58ms\nstep:1514/10000 train_loss:4.4521 train_time:255055ms step_avg:169.58ms\nstep:1515/10000 train_loss:4.4684 train_time:255226ms step_avg:169.59ms\nstep:1516/10000 train_loss:4.7030 train_time:255394ms step_avg:169.58ms\nstep:1517/10000 train_loss:4.6876 train_time:255563ms step_avg:169.58ms\nstep:1518/10000 train_loss:4.5060 train_time:255730ms step_avg:169.58ms\nstep:1519/10000 train_loss:4.7870 train_time:255898ms step_avg:169.58ms\nstep:1520/10000 train_loss:4.7200 train_time:256069ms step_avg:169.58ms\nstep:1521/10000 train_loss:4.5585 train_time:256236ms step_avg:169.58ms\nstep:1522/10000 train_loss:4.6024 train_time:256407ms step_avg:169.58ms\nstep:1523/10000 train_loss:4.6669 train_time:256576ms step_avg:169.58ms\nstep:1524/10000 train_loss:4.5086 train_time:256954ms step_avg:169.72ms\nstep:1525/10000 train_loss:4.5433 train_time:257118ms step_avg:169.71ms\nstep:1526/10000 train_loss:4.6630 train_time:257285ms step_avg:169.71ms\nstep:1527/10000 train_loss:4.6587 train_time:257453ms step_avg:169.71ms\nstep:1528/10000 train_loss:4.5918 train_time:257622ms step_avg:169.71ms\nstep:1529/10000 train_loss:4.8672 train_time:257791ms step_avg:169.71ms\nstep:1530/10000 train_loss:4.6638 train_time:257973ms step_avg:169.72ms\nstep:1531/10000 train_loss:4.6017 train_time:258144ms step_avg:169.72ms\nstep:1532/10000 train_loss:4.5362 train_time:258313ms step_avg:169.72ms\nstep:1533/10000 train_loss:4.5996 train_time:258483ms step_avg:169.72ms\nstep:1534/10000 train_loss:4.5180 train_time:258652ms step_avg:169.72ms\nstep:1535/10000 train_loss:4.4752 train_time:258824ms step_avg:169.72ms\nstep:1536/10000 train_loss:4.6302 train_time:258997ms step_avg:169.72ms\nstep:1537/10000 train_loss:4.6800 train_time:259169ms step_avg:169.72ms\nstep:1538/10000 train_loss:4.6320 train_time:259341ms step_avg:169.73ms\nstep:1539/10000 train_loss:4.5796 train_time:259510ms step_avg:169.73ms\nstep:1540/10000 train_loss:4.4543 train_time:259679ms step_avg:169.72ms\nstep:1541/10000 train_loss:4.6633 train_time:259850ms step_avg:169.73ms\nstep:1542/10000 train_loss:4.5506 train_time:260023ms step_avg:169.73ms\nstep:1543/10000 train_loss:4.6278 train_time:260193ms step_avg:169.73ms\nstep:1544/10000 train_loss:4.6364 train_time:260364ms step_avg:169.73ms\nstep:1545/10000 train_loss:4.7046 train_time:260533ms step_avg:169.73ms\nstep:1546/10000 train_loss:4.6722 train_time:260702ms step_avg:169.73ms\nstep:1547/10000 train_loss:4.5730 train_time:260873ms step_avg:169.73ms\nstep:1548/10000 train_loss:4.5251 train_time:261047ms step_avg:169.73ms\nstep:1549/10000 train_loss:4.6363 train_time:261216ms step_avg:169.73ms\nstep:1550/10000 train_loss:4.8878 train_time:261385ms step_avg:169.73ms\nstep:1551/10000 train_loss:4.5875 train_time:261554ms step_avg:169.73ms\nstep:1552/10000 train_loss:4.6746 train_time:261723ms step_avg:169.73ms\nstep:1553/10000 train_loss:4.6930 train_time:261894ms step_avg:169.73ms\nstep:1554/10000 train_loss:4.5733 train_time:262066ms step_avg:169.73ms\nstep:1555/10000 train_loss:4.4865 train_time:262237ms step_avg:169.73ms\nstep:1556/10000 train_loss:4.5807 train_time:262407ms step_avg:169.73ms\nstep:1557/10000 train_loss:4.5833 train_time:262575ms step_avg:169.73ms\nstep:1558/10000 train_loss:4.5006 train_time:262747ms step_avg:169.73ms\nstep:1559/10000 train_loss:4.6122 train_time:262917ms step_avg:169.73ms\nstep:1560/10000 train_loss:4.5963 train_time:263092ms step_avg:169.74ms\nstep:1561/10000 train_loss:4.4420 train_time:263263ms step_avg:169.74ms\nstep:1562/10000 train_loss:4.6515 train_time:263433ms step_avg:169.74ms\nstep:1563/10000 train_loss:4.6418 train_time:263601ms step_avg:169.74ms\nstep:1564/10000 train_loss:4.5154 train_time:263771ms step_avg:169.74ms\nstep:1565/10000 train_loss:4.5930 train_time:263942ms step_avg:169.74ms\nstep:1566/10000 train_loss:4.5708 train_time:264115ms step_avg:169.74ms\nstep:1567/10000 train_loss:4.5888 train_time:264285ms step_avg:169.74ms\nstep:1568/10000 train_loss:4.6247 train_time:264454ms step_avg:169.74ms\nstep:1569/10000 train_loss:4.5716 train_time:264623ms step_avg:169.74ms\nstep:1570/10000 train_loss:4.6562 train_time:264791ms step_avg:169.74ms\nstep:1571/10000 train_loss:4.7214 train_time:264964ms step_avg:169.74ms\nstep:1572/10000 train_loss:4.5264 train_time:265137ms step_avg:169.74ms\nstep:1573/10000 train_loss:4.6028 train_time:265306ms step_avg:169.74ms\nstep:1574/10000 train_loss:4.4846 train_time:265478ms step_avg:169.74ms\nstep:1575/10000 train_loss:4.6364 train_time:265649ms step_avg:169.74ms\nstep:1576/10000 train_loss:4.5788 train_time:265818ms step_avg:169.74ms\nstep:1577/10000 train_loss:4.5981 train_time:265989ms step_avg:169.74ms\nstep:1578/10000 train_loss:4.6112 train_time:266159ms step_avg:169.74ms\nstep:1579/10000 train_loss:4.6683 train_time:266330ms step_avg:169.74ms\nstep:1580/10000 train_loss:4.5383 train_time:266499ms step_avg:169.74ms\nstep:1581/10000 train_loss:4.6294 train_time:266669ms step_avg:169.74ms\nstep:1582/10000 train_loss:4.4138 train_time:266839ms step_avg:169.74ms\nstep:1583/10000 train_loss:4.5703 train_time:267012ms step_avg:169.75ms\nstep:1584/10000 train_loss:4.5703 train_time:267182ms step_avg:169.75ms\nstep:1585/10000 train_loss:4.5297 train_time:267351ms step_avg:169.75ms\nstep:1586/10000 train_loss:4.5513 train_time:267522ms step_avg:169.75ms\nstep:1587/10000 train_loss:4.4139 train_time:267694ms step_avg:169.75ms\nstep:1588/10000 train_loss:4.3509 train_time:267863ms step_avg:169.75ms\nstep:1589/10000 train_loss:4.4521 train_time:268032ms step_avg:169.75ms\nstep:1590/10000 train_loss:4.6918 train_time:268202ms step_avg:169.75ms\nstep:1591/10000 train_loss:4.8250 train_time:268372ms step_avg:169.75ms\nstep:1592/10000 train_loss:4.5484 train_time:268542ms step_avg:169.75ms\nstep:1593/10000 train_loss:4.7082 train_time:268713ms step_avg:169.75ms\nstep:1594/10000 train_loss:4.6186 train_time:268884ms step_avg:169.75ms\nstep:1595/10000 train_loss:4.5113 train_time:269055ms step_avg:169.75ms\nstep:1596/10000 train_loss:4.5462 train_time:269226ms step_avg:169.75ms\nstep:1597/10000 train_loss:4.6265 train_time:269396ms step_avg:169.75ms\nstep:1598/10000 train_loss:4.5468 train_time:269565ms step_avg:169.75ms\nstep:1599/10000 train_loss:4.4878 train_time:269733ms step_avg:169.75ms\nstep:1600/10000 train_loss:4.5130 train_time:269904ms step_avg:169.75ms\nstep:1601/10000 train_loss:4.5244 train_time:270074ms step_avg:169.75ms\nstep:1602/10000 train_loss:4.6377 train_time:270245ms step_avg:169.75ms\nstep:1603/10000 train_loss:4.5771 train_time:270416ms step_avg:169.75ms\nstep:1604/10000 train_loss:4.6064 train_time:270587ms step_avg:169.75ms\nstep:1605/10000 train_loss:4.5435 train_time:270757ms step_avg:169.75ms\nstep:1606/10000 train_loss:4.6291 train_time:270928ms step_avg:169.75ms\nstep:1607/10000 train_loss:4.6653 train_time:271096ms step_avg:169.75ms\nstep:1608/10000 train_loss:4.5086 train_time:271266ms step_avg:169.75ms\nstep:1609/10000 train_loss:4.5224 train_time:271437ms step_avg:169.75ms\nstep:1610/10000 train_loss:4.6009 train_time:271610ms step_avg:169.76ms\nstep:1611/10000 train_loss:4.5769 train_time:271777ms step_avg:169.75ms\nstep:1612/10000 train_loss:4.7748 train_time:271947ms step_avg:169.75ms\nstep:1613/10000 train_loss:4.5457 train_time:272118ms step_avg:169.76ms\nstep:1614/10000 train_loss:4.6977 train_time:272286ms step_avg:169.75ms\nstep:1615/10000 train_loss:4.4711 train_time:272455ms step_avg:169.75ms\nstep:1616/10000 train_loss:4.6795 train_time:272626ms step_avg:169.75ms\nstep:1617/10000 train_loss:4.5735 train_time:272794ms step_avg:169.75ms\nstep:1618/10000 train_loss:4.5089 train_time:272963ms step_avg:169.75ms\nstep:1619/10000 train_loss:4.5381 train_time:273134ms step_avg:169.75ms\nstep:1620/10000 train_loss:4.6128 train_time:273305ms step_avg:169.75ms\nstep:1621/10000 train_loss:4.5779 train_time:273473ms step_avg:169.75ms\nstep:1622/10000 train_loss:4.5282 train_time:273645ms step_avg:169.76ms\nstep:1623/10000 train_loss:4.5431 train_time:273814ms step_avg:169.75ms\nstep:1624/10000 train_loss:4.6040 train_time:273985ms step_avg:169.75ms\nstep:1625/10000 train_loss:4.9630 train_time:274156ms step_avg:169.76ms\nstep:1626/10000 train_loss:4.8012 train_time:274328ms step_avg:169.76ms\nstep:1627/10000 train_loss:4.5006 train_time:274498ms step_avg:169.76ms\nstep:1628/10000 train_loss:4.5318 train_time:274673ms step_avg:169.76ms\nstep:1629/10000 train_loss:4.6121 train_time:274843ms step_avg:169.76ms\nstep:1630/10000 train_loss:4.5607 train_time:275011ms step_avg:169.76ms\nstep:1631/10000 train_loss:4.5767 train_time:275181ms step_avg:169.76ms\nstep:1632/10000 train_loss:4.4523 train_time:275351ms step_avg:169.76ms\nstep:1633/10000 train_loss:4.7497 train_time:275520ms step_avg:169.76ms\nstep:1634/10000 train_loss:4.5684 train_time:275694ms step_avg:169.76ms\nstep:1635/10000 train_loss:4.5692 train_time:275865ms step_avg:169.76ms\nstep:1636/10000 train_loss:4.5857 train_time:276034ms step_avg:169.76ms\nstep:1637/10000 train_loss:4.7440 train_time:276203ms step_avg:169.76ms\nstep:1638/10000 train_loss:4.5946 train_time:276370ms step_avg:169.76ms\nstep:1639/10000 train_loss:4.5165 train_time:276542ms step_avg:169.76ms\nstep:1640/10000 train_loss:4.4820 train_time:276714ms step_avg:169.76ms\nstep:1641/10000 train_loss:4.4852 train_time:276886ms step_avg:169.76ms\nstep:1642/10000 train_loss:4.4913 train_time:277055ms step_avg:169.76ms\nstep:1643/10000 train_loss:4.5723 train_time:277224ms step_avg:169.76ms\nstep:1644/10000 train_loss:4.5411 train_time:277392ms step_avg:169.76ms\nstep:1645/10000 train_loss:4.5145 train_time:277562ms step_avg:169.76ms\nstep:1646/10000 train_loss:4.5140 train_time:277731ms step_avg:169.76ms\nstep:1647/10000 train_loss:4.5088 train_time:277902ms step_avg:169.76ms\nstep:1648/10000 train_loss:4.5121 train_time:278070ms step_avg:169.76ms\nstep:1649/10000 train_loss:4.6107 train_time:278242ms step_avg:169.76ms\nstep:1650/10000 train_loss:4.9031 train_time:278411ms step_avg:169.76ms\nstep:1651/10000 train_loss:4.5572 train_time:278581ms step_avg:169.76ms\nstep:1652/10000 train_loss:4.5020 train_time:278749ms step_avg:169.76ms\nstep:1653/10000 train_loss:4.6673 train_time:278918ms step_avg:169.76ms\nstep:1654/10000 train_loss:4.3862 train_time:279091ms step_avg:169.76ms\nstep:1655/10000 train_loss:4.4755 train_time:279261ms step_avg:169.76ms\nstep:1656/10000 train_loss:4.5754 train_time:279433ms step_avg:169.76ms\nstep:1657/10000 train_loss:4.6078 train_time:279604ms step_avg:169.77ms\nstep:1658/10000 train_loss:4.7238 train_time:279774ms step_avg:169.77ms\nstep:1659/10000 train_loss:4.6405 train_time:279944ms step_avg:169.77ms\nstep:1660/10000 train_loss:4.4946 train_time:280116ms step_avg:169.77ms\nstep:1661/10000 train_loss:4.5018 train_time:280285ms step_avg:169.77ms\nstep:1662/10000 train_loss:4.4881 train_time:280453ms step_avg:169.77ms\nstep:1663/10000 train_loss:4.6393 train_time:280624ms step_avg:169.77ms\nstep:1664/10000 train_loss:4.4940 train_time:280793ms step_avg:169.77ms\nstep:1665/10000 train_loss:4.7017 train_time:280964ms step_avg:169.77ms\nstep:1666/10000 train_loss:4.5123 train_time:281134ms step_avg:169.77ms\nstep:1667/10000 train_loss:4.2404 train_time:281305ms step_avg:169.77ms\nstep:1668/10000 train_loss:4.6149 train_time:281474ms step_avg:169.77ms\nstep:1669/10000 train_loss:4.5792 train_time:281644ms step_avg:169.77ms\nstep:1670/10000 train_loss:4.6543 train_time:281813ms step_avg:169.77ms\nstep:1671/10000 train_loss:4.5167 train_time:281982ms step_avg:169.77ms\nstep:1672/10000 train_loss:4.3616 train_time:282153ms step_avg:169.77ms\nstep:1673/10000 train_loss:4.4552 train_time:282326ms step_avg:169.77ms\nstep:1674/10000 train_loss:4.4081 train_time:282493ms step_avg:169.77ms\nstep:1675/10000 train_loss:4.6004 train_time:282665ms step_avg:169.77ms\nstep:1676/10000 train_loss:4.6691 train_time:282839ms step_avg:169.77ms\nstep:1677/10000 train_loss:4.3095 train_time:283009ms step_avg:169.77ms\nstep:1678/10000 train_loss:4.5138 train_time:283174ms step_avg:169.77ms\nstep:1679/10000 train_loss:4.4697 train_time:283345ms step_avg:169.77ms\nstep:1680/10000 train_loss:4.6935 train_time:283514ms step_avg:169.77ms\nstep:1681/10000 train_loss:4.3893 train_time:283686ms step_avg:169.77ms\nstep:1682/10000 train_loss:4.4949 train_time:283854ms step_avg:169.77ms\nstep:1683/10000 train_loss:4.5481 train_time:284023ms step_avg:169.77ms\nstep:1684/10000 train_loss:4.5195 train_time:284193ms step_avg:169.77ms\nstep:1685/10000 train_loss:4.5644 train_time:284365ms step_avg:169.77ms\nstep:1686/10000 train_loss:4.3692 train_time:284537ms step_avg:169.77ms\nstep:1687/10000 train_loss:4.4558 train_time:284709ms step_avg:169.77ms\nstep:1688/10000 train_loss:4.6055 train_time:284879ms step_avg:169.77ms\nstep:1689/10000 train_loss:4.4844 train_time:285048ms step_avg:169.77ms\nstep:1690/10000 train_loss:4.4561 train_time:285219ms step_avg:169.77ms\nstep:1691/10000 train_loss:4.4346 train_time:285389ms step_avg:169.77ms\nstep:1692/10000 train_loss:4.3584 train_time:285558ms step_avg:169.77ms\nstep:1693/10000 train_loss:4.4794 train_time:285730ms step_avg:169.77ms\nstep:1694/10000 train_loss:4.5735 train_time:285898ms step_avg:169.77ms\nstep:1695/10000 train_loss:4.5058 train_time:286071ms step_avg:169.77ms\nstep:1696/10000 train_loss:4.4454 train_time:286241ms step_avg:169.78ms\nstep:1697/10000 train_loss:4.5398 train_time:286412ms step_avg:169.78ms\nstep:1698/10000 train_loss:4.4014 train_time:286582ms step_avg:169.78ms\nstep:1699/10000 train_loss:4.5761 train_time:286751ms step_avg:169.78ms\nstep:1700/10000 train_loss:4.3839 train_time:286920ms step_avg:169.78ms\nstep:1701/10000 train_loss:4.5544 train_time:287091ms step_avg:169.78ms\nstep:1702/10000 train_loss:4.5463 train_time:287260ms step_avg:169.78ms\nstep:1703/10000 train_loss:4.4311 train_time:287428ms step_avg:169.77ms\nstep:1704/10000 train_loss:4.5891 train_time:287596ms step_avg:169.77ms\nstep:1705/10000 train_loss:4.4613 train_time:287768ms step_avg:169.77ms\nstep:1706/10000 train_loss:4.4392 train_time:287943ms step_avg:169.78ms\nstep:1707/10000 train_loss:4.5846 train_time:288105ms step_avg:169.77ms\nstep:1708/10000 train_loss:4.5503 train_time:288273ms step_avg:169.77ms\nstep:1709/10000 train_loss:4.4072 train_time:288443ms step_avg:169.77ms\nstep:1710/10000 train_loss:4.5965 train_time:288612ms step_avg:169.77ms\nstep:1711/10000 train_loss:4.5361 train_time:288784ms step_avg:169.77ms\nstep:1712/10000 train_loss:4.4440 train_time:288956ms step_avg:169.77ms\nstep:1713/10000 train_loss:4.4759 train_time:289128ms step_avg:169.78ms\nstep:1714/10000 train_loss:4.5581 train_time:289300ms step_avg:169.78ms\nstep:1715/10000 train_loss:4.4797 train_time:289471ms step_avg:169.78ms\nstep:1716/10000 train_loss:4.5465 train_time:289643ms step_avg:169.78ms\nstep:1717/10000 train_loss:4.4167 train_time:289812ms step_avg:169.78ms\nstep:1718/10000 train_loss:4.4668 train_time:289984ms step_avg:169.78ms\nstep:1719/10000 train_loss:4.3790 train_time:290153ms step_avg:169.78ms\nstep:1720/10000 train_loss:4.4291 train_time:290322ms step_avg:169.78ms\nstep:1721/10000 train_loss:4.4470 train_time:290489ms step_avg:169.78ms\nstep:1722/10000 train_loss:4.4529 train_time:290660ms step_avg:169.78ms\nstep:1723/10000 train_loss:4.6372 train_time:290829ms step_avg:169.78ms\nstep:1724/10000 train_loss:4.3577 train_time:290997ms step_avg:169.78ms\nstep:1725/10000 train_loss:4.4149 train_time:291167ms step_avg:169.78ms\nstep:1726/10000 train_loss:4.4249 train_time:291333ms step_avg:169.77ms\nstep:1727/10000 train_loss:4.7181 train_time:291503ms step_avg:169.77ms\nstep:1728/10000 train_loss:4.5647 train_time:291671ms step_avg:169.77ms\nstep:1729/10000 train_loss:4.6974 train_time:291841ms step_avg:169.77ms\nstep:1730/10000 train_loss:4.4803 train_time:292013ms step_avg:169.77ms\nstep:1731/10000 train_loss:4.4817 train_time:292183ms step_avg:169.78ms\nstep:1732/10000 train_loss:4.5944 train_time:292353ms step_avg:169.78ms\nstep:1733/10000 train_loss:4.3637 train_time:292522ms step_avg:169.78ms\nstep:1734/10000 train_loss:4.5570 train_time:292692ms step_avg:169.77ms\nstep:1735/10000 train_loss:4.4784 train_time:292862ms step_avg:169.77ms\nstep:1736/10000 train_loss:4.5508 train_time:293032ms step_avg:169.78ms\nstep:1737/10000 train_loss:4.5428 train_time:293201ms step_avg:169.77ms\nstep:1738/10000 train_loss:4.4961 train_time:293369ms step_avg:169.77ms\nstep:1739/10000 train_loss:4.5281 train_time:293537ms step_avg:169.77ms\nstep:1740/10000 train_loss:4.4482 train_time:293711ms step_avg:169.77ms\nstep:1741/10000 train_loss:4.5219 train_time:293878ms step_avg:169.77ms\nstep:1742/10000 train_loss:4.5196 train_time:294049ms step_avg:169.77ms\nstep:1743/10000 train_loss:4.5855 train_time:294221ms step_avg:169.78ms\nstep:1744/10000 train_loss:4.5408 train_time:294391ms step_avg:169.78ms\nstep:1745/10000 train_loss:4.4550 train_time:294560ms step_avg:169.78ms\nstep:1746/10000 train_loss:4.5400 train_time:294730ms step_avg:169.78ms\nstep:1747/10000 train_loss:4.5930 train_time:294900ms step_avg:169.78ms\nstep:1748/10000 train_loss:4.5858 train_time:295077ms step_avg:169.78ms\nstep:1749/10000 train_loss:4.5217 train_time:295238ms step_avg:169.77ms\nstep:1750/10000 train_loss:4.4302 train_time:295409ms step_avg:169.78ms\nstep:1751/10000 train_loss:4.8375 train_time:295581ms step_avg:169.78ms\nstep:1752/10000 train_loss:4.5110 train_time:295754ms step_avg:169.78ms\nstep:1753/10000 train_loss:4.5743 train_time:295923ms step_avg:169.78ms\nstep:1754/10000 train_loss:4.4367 train_time:296091ms step_avg:169.78ms\nstep:1755/10000 train_loss:4.4967 train_time:296262ms step_avg:169.78ms\nstep:1756/10000 train_loss:4.5497 train_time:296433ms step_avg:169.78ms\nstep:1757/10000 train_loss:4.5232 train_time:296603ms step_avg:169.78ms\nstep:1758/10000 train_loss:4.3820 train_time:296770ms step_avg:169.78ms\nstep:1759/10000 train_loss:4.5488 train_time:296939ms step_avg:169.78ms\nstep:1760/10000 train_loss:4.4140 train_time:297110ms step_avg:169.78ms\nstep:1761/10000 train_loss:4.3854 train_time:297280ms step_avg:169.78ms\nstep:1762/10000 train_loss:4.5755 train_time:297452ms step_avg:169.78ms\nstep:1763/10000 train_loss:4.5139 train_time:297622ms step_avg:169.78ms\nstep:1764/10000 train_loss:4.4627 train_time:297792ms step_avg:169.78ms\nstep:1765/10000 train_loss:4.2270 train_time:297964ms step_avg:169.78ms\nstep:1766/10000 train_loss:4.4943 train_time:298136ms step_avg:169.78ms\nstep:1767/10000 train_loss:4.3991 train_time:298307ms step_avg:169.78ms\nstep:1768/10000 train_loss:4.4807 train_time:298476ms step_avg:169.78ms\nstep:1769/10000 train_loss:4.4310 train_time:298645ms step_avg:169.78ms\nstep:1770/10000 train_loss:4.4681 train_time:298814ms step_avg:169.78ms\nstep:1771/10000 train_loss:4.4178 train_time:298986ms step_avg:169.78ms\nstep:1772/10000 train_loss:4.3356 train_time:299155ms step_avg:169.78ms\nstep:1773/10000 train_loss:4.5321 train_time:299325ms step_avg:169.78ms\nstep:1774/10000 train_loss:4.3502 train_time:299493ms step_avg:169.78ms\nstep:1775/10000 train_loss:4.4119 train_time:299663ms step_avg:169.78ms\nstep:1776/10000 train_loss:4.4985 train_time:299832ms step_avg:169.78ms\nstep:1777/10000 train_loss:4.4509 train_time:300003ms step_avg:169.78ms\nstep:1778/10000 train_loss:4.6210 train_time:300173ms step_avg:169.78ms\nstep:1779/10000 train_loss:4.5299 train_time:300343ms step_avg:169.78ms\nstep:1780/10000 train_loss:4.6771 train_time:300510ms step_avg:169.78ms\nstep:1781/10000 train_loss:4.4504 train_time:300678ms step_avg:169.78ms\nstep:1782/10000 train_loss:4.5232 train_time:300849ms step_avg:169.78ms\nstep:1783/10000 train_loss:4.6237 train_time:301020ms step_avg:169.78ms\nstep:1784/10000 train_loss:4.3655 train_time:301191ms step_avg:169.78ms\nstep:1785/10000 train_loss:4.4494 train_time:301361ms step_avg:169.78ms\nstep:1786/10000 train_loss:4.5286 train_time:301533ms step_avg:169.78ms\nstep:1787/10000 train_loss:4.4787 train_time:301704ms step_avg:169.78ms\nstep:1788/10000 train_loss:4.5268 train_time:301872ms step_avg:169.78ms\nstep:1789/10000 train_loss:4.5624 train_time:302045ms step_avg:169.78ms\nstep:1790/10000 train_loss:4.4505 train_time:302213ms step_avg:169.78ms\nstep:1791/10000 train_loss:4.3820 train_time:302382ms step_avg:169.78ms\nstep:1792/10000 train_loss:4.5253 train_time:302550ms step_avg:169.78ms\nstep:1793/10000 train_loss:4.5589 train_time:302721ms step_avg:169.78ms\nstep:1794/10000 train_loss:4.5253 train_time:302889ms step_avg:169.78ms\nstep:1795/10000 train_loss:4.7333 train_time:303059ms step_avg:169.78ms\nstep:1796/10000 train_loss:4.3891 train_time:303232ms step_avg:169.78ms\nstep:1797/10000 train_loss:4.5453 train_time:303400ms step_avg:169.78ms\nstep:1798/10000 train_loss:4.3711 train_time:303571ms step_avg:169.78ms\nstep:1799/10000 train_loss:4.5468 train_time:303741ms step_avg:169.78ms\nstep:1800/10000 train_loss:4.4025 train_time:303910ms step_avg:169.78ms\nstep:1801/10000 train_loss:4.6047 train_time:304080ms step_avg:169.78ms\nstep:1802/10000 train_loss:4.4107 train_time:304249ms step_avg:169.78ms\nstep:1803/10000 train_loss:4.5545 train_time:304420ms step_avg:169.78ms\nstep:1804/10000 train_loss:4.4677 train_time:304593ms step_avg:169.78ms\nstep:1805/10000 train_loss:4.4664 train_time:304763ms step_avg:169.78ms\nstep:1806/10000 train_loss:4.3080 train_time:304933ms step_avg:169.78ms\nstep:1807/10000 train_loss:4.5134 train_time:305103ms step_avg:169.78ms\nstep:1808/10000 train_loss:4.7719 train_time:305271ms step_avg:169.78ms\nstep:1809/10000 train_loss:4.5430 train_time:305439ms step_avg:169.78ms\nstep:1810/10000 train_loss:4.4114 train_time:305608ms step_avg:169.78ms\nstep:1811/10000 train_loss:4.4570 train_time:305779ms step_avg:169.78ms\nstep:1812/10000 train_loss:4.6453 train_time:305948ms step_avg:169.78ms\nstep:1813/10000 train_loss:4.5200 train_time:306117ms step_avg:169.78ms\nstep:1814/10000 train_loss:4.3631 train_time:306288ms step_avg:169.78ms\nstep:1815/10000 train_loss:4.4661 train_time:306457ms step_avg:169.78ms\nstep:1816/10000 train_loss:4.4242 train_time:306627ms step_avg:169.78ms\nstep:1817/10000 train_loss:4.6914 train_time:306797ms step_avg:169.78ms\nstep:1818/10000 train_loss:4.3829 train_time:306969ms step_avg:169.78ms\nstep:1819/10000 train_loss:4.2473 train_time:307142ms step_avg:169.79ms\nstep:1820/10000 train_loss:4.3963 train_time:307313ms step_avg:169.79ms\nstep:1821/10000 train_loss:4.5820 train_time:307483ms step_avg:169.79ms\nstep:1822/10000 train_loss:4.5658 train_time:307659ms step_avg:169.79ms\nstep:1823/10000 train_loss:4.3448 train_time:307864ms step_avg:169.81ms\nstep:1824/10000 train_loss:4.5259 train_time:308016ms step_avg:169.80ms\nstep:1825/10000 train_loss:4.5516 train_time:308186ms step_avg:169.80ms\nstep:1826/10000 train_loss:4.4204 train_time:308355ms step_avg:169.80ms\nstep:1827/10000 train_loss:4.4334 train_time:308524ms step_avg:169.80ms\nstep:1828/10000 train_loss:4.5001 train_time:308693ms step_avg:169.80ms\nstep:1829/10000 train_loss:4.4884 train_time:308862ms step_avg:169.80ms\nstep:1830/10000 train_loss:4.5076 train_time:309031ms step_avg:169.80ms\nstep:1831/10000 train_loss:4.3647 train_time:309201ms step_avg:169.80ms\nstep:1832/10000 train_loss:4.4989 train_time:309373ms step_avg:169.80ms\nstep:1833/10000 train_loss:4.2314 train_time:309540ms step_avg:169.80ms\nstep:1834/10000 train_loss:4.5249 train_time:309710ms step_avg:169.80ms\nstep:1835/10000 train_loss:4.4084 train_time:309881ms step_avg:169.80ms\nstep:1836/10000 train_loss:4.5428 train_time:310052ms step_avg:169.80ms\nstep:1837/10000 train_loss:4.4380 train_time:310222ms step_avg:169.80ms\nstep:1838/10000 train_loss:4.3259 train_time:310391ms step_avg:169.80ms\nstep:1839/10000 train_loss:4.3369 train_time:310560ms step_avg:169.80ms\nstep:1840/10000 train_loss:4.4300 train_time:310735ms step_avg:169.80ms\nstep:1841/10000 train_loss:4.4321 train_time:310906ms step_avg:169.80ms\nstep:1842/10000 train_loss:4.4872 train_time:311073ms step_avg:169.80ms\nstep:1843/10000 train_loss:4.5010 train_time:311242ms step_avg:169.80ms\nstep:1844/10000 train_loss:4.4904 train_time:311412ms step_avg:169.80ms\nstep:1845/10000 train_loss:4.4246 train_time:311580ms step_avg:169.80ms\nstep:1846/10000 train_loss:4.4259 train_time:311750ms step_avg:169.80ms\nstep:1847/10000 train_loss:4.5353 train_time:311922ms step_avg:169.80ms\nstep:1848/10000 train_loss:4.4169 train_time:312090ms step_avg:169.80ms\nstep:1849/10000 train_loss:4.4164 train_time:312259ms step_avg:169.80ms\nstep:1850/10000 train_loss:4.3732 train_time:312430ms step_avg:169.80ms\nstep:1851/10000 train_loss:4.4275 train_time:312602ms step_avg:169.80ms\nstep:1852/10000 train_loss:4.2579 train_time:312772ms step_avg:169.80ms\nstep:1853/10000 train_loss:4.3109 train_time:312943ms step_avg:169.80ms\nstep:1854/10000 train_loss:4.4907 train_time:313111ms step_avg:169.80ms\nstep:1855/10000 train_loss:4.3871 train_time:313282ms step_avg:169.80ms\nstep:1856/10000 train_loss:4.4287 train_time:313450ms step_avg:169.80ms\nstep:1857/10000 train_loss:4.5027 train_time:313620ms step_avg:169.80ms\nstep:1858/10000 train_loss:4.4048 train_time:313789ms step_avg:169.80ms\nstep:1859/10000 train_loss:4.5310 train_time:313958ms step_avg:169.80ms\nstep:1860/10000 train_loss:4.5246 train_time:314128ms step_avg:169.80ms\nstep:1861/10000 train_loss:4.4927 train_time:314297ms step_avg:169.80ms\nstep:1862/10000 train_loss:4.4017 train_time:314466ms step_avg:169.80ms\nstep:1863/10000 train_loss:4.4346 train_time:314634ms step_avg:169.80ms\nstep:1864/10000 train_loss:4.3889 train_time:314804ms step_avg:169.80ms\nstep:1865/10000 train_loss:4.4934 train_time:314973ms step_avg:169.80ms\nstep:1866/10000 train_loss:4.5335 train_time:315147ms step_avg:169.80ms\nstep:1867/10000 train_loss:4.3961 train_time:315317ms step_avg:169.80ms\nstep:1868/10000 train_loss:4.4187 train_time:315489ms step_avg:169.80ms\nstep:1869/10000 train_loss:4.5560 train_time:315657ms step_avg:169.80ms\nstep:1870/10000 train_loss:4.5358 train_time:315824ms step_avg:169.80ms\nstep:1871/10000 train_loss:4.3213 train_time:315993ms step_avg:169.80ms\nstep:1872/10000 train_loss:4.3806 train_time:316164ms step_avg:169.80ms\nstep:1873/10000 train_loss:4.4029 train_time:316336ms step_avg:169.80ms\nstep:1874/10000 train_loss:4.4952 train_time:316509ms step_avg:169.80ms\nstep:1875/10000 train_loss:4.3275 train_time:316677ms step_avg:169.80ms\nstep:1876/10000 train_loss:4.5879 train_time:316846ms step_avg:169.80ms\nstep:1877/10000 train_loss:4.4575 train_time:317015ms step_avg:169.80ms\nstep:1878/10000 train_loss:4.4069 train_time:317187ms step_avg:169.80ms\nstep:1879/10000 train_loss:4.4240 train_time:317356ms step_avg:169.80ms\nstep:1880/10000 train_loss:4.4650 train_time:317529ms step_avg:169.80ms\nstep:1881/10000 train_loss:4.4661 train_time:317700ms step_avg:169.80ms\nstep:1882/10000 train_loss:4.4913 train_time:317869ms step_avg:169.80ms\nstep:1883/10000 train_loss:4.5733 train_time:318038ms step_avg:169.80ms\nstep:1884/10000 train_loss:4.2785 train_time:318207ms step_avg:169.80ms\nstep:1885/10000 train_loss:4.2412 train_time:318375ms step_avg:169.80ms\nstep:1886/10000 train_loss:4.5204 train_time:318545ms step_avg:169.80ms\nstep:1887/10000 train_loss:4.6858 train_time:318713ms step_avg:169.80ms\nstep:1888/10000 train_loss:4.4988 train_time:318883ms step_avg:169.80ms\nstep:1889/10000 train_loss:4.4322 train_time:319053ms step_avg:169.80ms\nstep:1890/10000 train_loss:4.4483 train_time:319223ms step_avg:169.80ms\nstep:1891/10000 train_loss:4.3984 train_time:319392ms step_avg:169.80ms\nstep:1892/10000 train_loss:4.3879 train_time:319562ms step_avg:169.80ms\nstep:1893/10000 train_loss:4.4672 train_time:319731ms step_avg:169.80ms\nstep:1894/10000 train_loss:4.4109 train_time:319900ms step_avg:169.80ms\nstep:1895/10000 train_loss:4.4733 train_time:320070ms step_avg:169.80ms\nstep:1896/10000 train_loss:4.5067 train_time:320239ms step_avg:169.80ms\nstep:1897/10000 train_loss:4.3780 train_time:320410ms step_avg:169.80ms\nstep:1898/10000 train_loss:4.2889 train_time:320578ms step_avg:169.80ms\nstep:1899/10000 train_loss:4.5152 train_time:320746ms step_avg:169.80ms\nstep:1900/10000 train_loss:4.5072 train_time:320914ms step_avg:169.80ms\nstep:1901/10000 train_loss:4.4450 train_time:321084ms step_avg:169.80ms\nstep:1902/10000 train_loss:4.7274 train_time:321252ms step_avg:169.79ms\nstep:1903/10000 train_loss:4.3467 train_time:321422ms step_avg:169.79ms\nstep:1904/10000 train_loss:4.6822 train_time:321591ms step_avg:169.79ms\nstep:1905/10000 train_loss:4.5096 train_time:321761ms step_avg:169.79ms\nstep:1906/10000 train_loss:4.4712 train_time:321931ms step_avg:169.79ms\nstep:1907/10000 train_loss:4.6405 train_time:322100ms step_avg:169.79ms\nstep:1908/10000 train_loss:4.4909 train_time:322269ms step_avg:169.79ms\nstep:1909/10000 train_loss:4.1845 train_time:322439ms step_avg:169.79ms\nstep:1910/10000 train_loss:4.2932 train_time:322610ms step_avg:169.79ms\nstep:1911/10000 train_loss:4.3714 train_time:322780ms step_avg:169.79ms\nstep:1912/10000 train_loss:4.5078 train_time:322951ms step_avg:169.80ms\nstep:1913/10000 train_loss:4.3901 train_time:323121ms step_avg:169.80ms\nstep:1914/10000 train_loss:4.4574 train_time:323290ms step_avg:169.80ms\nstep:1915/10000 train_loss:4.3891 train_time:323462ms step_avg:169.80ms\nstep:1916/10000 train_loss:4.4143 train_time:323633ms step_avg:169.80ms\nstep:1917/10000 train_loss:4.4299 train_time:323805ms step_avg:169.80ms\nstep:1918/10000 train_loss:4.4421 train_time:323976ms step_avg:169.80ms\nstep:1919/10000 train_loss:4.3667 train_time:324147ms step_avg:169.80ms\nstep:1920/10000 train_loss:4.3192 train_time:324315ms step_avg:169.80ms\nstep:1921/10000 train_loss:4.5361 train_time:324486ms step_avg:169.80ms\nstep:1922/10000 train_loss:4.5830 train_time:324655ms step_avg:169.80ms\nstep:1923/10000 train_loss:4.4014 train_time:324825ms step_avg:169.80ms\nstep:1924/10000 train_loss:4.4296 train_time:324995ms step_avg:169.80ms\nstep:1925/10000 train_loss:4.5701 train_time:325165ms step_avg:169.80ms\nstep:1926/10000 train_loss:4.4137 train_time:325335ms step_avg:169.80ms\nstep:1927/10000 train_loss:4.3337 train_time:325504ms step_avg:169.80ms\nstep:1928/10000 train_loss:4.5519 train_time:325671ms step_avg:169.80ms\nstep:1929/10000 train_loss:4.3566 train_time:325842ms step_avg:169.80ms\nstep:1930/10000 train_loss:4.2152 train_time:326013ms step_avg:169.80ms\nstep:1931/10000 train_loss:4.3327 train_time:326183ms step_avg:169.80ms\nstep:1932/10000 train_loss:4.4085 train_time:326354ms step_avg:169.80ms\nstep:1933/10000 train_loss:4.2704 train_time:326523ms step_avg:169.80ms\nstep:1934/10000 train_loss:4.4735 train_time:326694ms step_avg:169.80ms\nstep:1935/10000 train_loss:4.6105 train_time:326865ms step_avg:169.80ms\nstep:1936/10000 train_loss:4.4812 train_time:327034ms step_avg:169.80ms\nstep:1937/10000 train_loss:4.3882 train_time:327205ms step_avg:169.80ms\nstep:1938/10000 train_loss:4.3508 train_time:327374ms step_avg:169.80ms\nstep:1939/10000 train_loss:4.4597 train_time:327544ms step_avg:169.80ms\nstep:1940/10000 train_loss:4.4332 train_time:327711ms step_avg:169.80ms\nstep:1941/10000 train_loss:4.4832 train_time:327882ms step_avg:169.80ms\nstep:1942/10000 train_loss:4.3843 train_time:328052ms step_avg:169.80ms\nstep:1943/10000 train_loss:4.3418 train_time:328221ms step_avg:169.80ms\nstep:1944/10000 train_loss:4.2850 train_time:328390ms step_avg:169.80ms\nstep:1945/10000 train_loss:4.4770 train_time:328560ms step_avg:169.80ms\nstep:1946/10000 train_loss:4.3185 train_time:328727ms step_avg:169.80ms\nstep:1947/10000 train_loss:4.4320 train_time:328894ms step_avg:169.80ms\nstep:1948/10000 train_loss:4.4356 train_time:329067ms step_avg:169.80ms\nstep:1949/10000 train_loss:4.3764 train_time:329237ms step_avg:169.80ms\nstep:1950/10000 train_loss:4.3285 train_time:329409ms step_avg:169.80ms\nstep:1951/10000 train_loss:4.3670 train_time:329578ms step_avg:169.80ms\nstep:1952/10000 train_loss:4.4444 train_time:329751ms step_avg:169.80ms\nstep:1953/10000 train_loss:4.3394 train_time:329921ms step_avg:169.80ms\nstep:1954/10000 train_loss:4.5496 train_time:330094ms step_avg:169.80ms\nstep:1955/10000 train_loss:4.3769 train_time:330264ms step_avg:169.80ms\nstep:1956/10000 train_loss:4.3955 train_time:330433ms step_avg:169.80ms\nstep:1957/10000 train_loss:4.3462 train_time:330603ms step_avg:169.80ms\nstep:1958/10000 train_loss:4.3990 train_time:330770ms step_avg:169.80ms\nstep:1959/10000 train_loss:4.4402 train_time:330939ms step_avg:169.80ms\nstep:1960/10000 train_loss:4.5852 train_time:331110ms step_avg:169.80ms\nstep:1961/10000 train_loss:4.3515 train_time:331278ms step_avg:169.80ms\nstep:1962/10000 train_loss:4.3026 train_time:331449ms step_avg:169.80ms\nstep:1963/10000 train_loss:4.2515 train_time:331620ms step_avg:169.80ms\nstep:1964/10000 train_loss:4.3854 train_time:331795ms step_avg:169.80ms\nstep:1965/10000 train_loss:4.4018 train_time:331965ms step_avg:169.80ms\nstep:1966/10000 train_loss:4.3061 train_time:332134ms step_avg:169.80ms\nstep:1967/10000 train_loss:4.6359 train_time:332305ms step_avg:169.80ms\nstep:1968/10000 train_loss:4.3362 train_time:332477ms step_avg:169.80ms\nstep:1969/10000 train_loss:4.2999 train_time:332648ms step_avg:169.80ms\nstep:1970/10000 train_loss:4.4243 train_time:332817ms step_avg:169.80ms\nstep:1971/10000 train_loss:4.4691 train_time:332990ms step_avg:169.81ms\nstep:1972/10000 train_loss:4.4742 train_time:333160ms step_avg:169.81ms\nstep:1973/10000 train_loss:4.5359 train_time:333330ms step_avg:169.81ms\nstep:1974/10000 train_loss:4.2536 train_time:333499ms step_avg:169.81ms\nstep:1975/10000 train_loss:4.3303 train_time:333671ms step_avg:169.81ms\nstep:1976/10000 train_loss:4.2411 train_time:333840ms step_avg:169.81ms\nstep:1977/10000 train_loss:4.4849 train_time:334010ms step_avg:169.81ms\nstep:1978/10000 train_loss:4.4011 train_time:334183ms step_avg:169.81ms\nstep:1979/10000 train_loss:4.3487 train_time:334352ms step_avg:169.81ms\nstep:1980/10000 train_loss:4.4215 train_time:334521ms step_avg:169.81ms\nstep:1981/10000 train_loss:4.4106 train_time:334693ms step_avg:169.81ms\nstep:1982/10000 train_loss:4.3038 train_time:334863ms step_avg:169.81ms\nstep:1983/10000 train_loss:4.4291 train_time:335031ms step_avg:169.81ms\nstep:1984/10000 train_loss:4.5829 train_time:335201ms step_avg:169.81ms\nstep:1985/10000 train_loss:4.4733 train_time:335371ms step_avg:169.81ms\nstep:1986/10000 train_loss:4.2927 train_time:335539ms step_avg:169.81ms\nstep:1987/10000 train_loss:4.4158 train_time:335709ms step_avg:169.81ms\nstep:1988/10000 train_loss:4.3961 train_time:335879ms step_avg:169.81ms\nstep:1989/10000 train_loss:4.3881 train_time:336050ms step_avg:169.81ms\nstep:1990/10000 train_loss:4.4168 train_time:336221ms step_avg:169.81ms\nstep:1991/10000 train_loss:4.4033 train_time:336392ms step_avg:169.81ms\nstep:1992/10000 train_loss:4.3490 train_time:336558ms step_avg:169.81ms\nstep:1993/10000 train_loss:4.8295 train_time:336729ms step_avg:169.81ms\nstep:1994/10000 train_loss:4.5014 train_time:336899ms step_avg:169.81ms\nstep:1995/10000 train_loss:4.3498 train_time:337072ms step_avg:169.81ms\nstep:1996/10000 train_loss:4.3566 train_time:337242ms step_avg:169.81ms\nstep:1997/10000 train_loss:4.3048 train_time:337414ms step_avg:169.81ms\nstep:1998/10000 train_loss:4.5545 train_time:337583ms step_avg:169.81ms\nstep:1999/10000 train_loss:4.5023 train_time:337753ms step_avg:169.81ms\nstep:2000/10000 train_loss:4.4739 train_time:337925ms step_avg:169.81ms\nstep:2001/10000 train_loss:4.4478 train_time:338095ms step_avg:169.81ms\nstep:2002/10000 train_loss:4.4377 train_time:338264ms step_avg:169.81ms\nstep:2003/10000 train_loss:4.4417 train_time:338433ms step_avg:169.81ms\nstep:2004/10000 train_loss:4.4117 train_time:338603ms step_avg:169.81ms\nstep:2005/10000 train_loss:4.3904 train_time:338771ms step_avg:169.81ms\nstep:2006/10000 train_loss:4.4411 train_time:338942ms step_avg:169.81ms\nstep:2007/10000 train_loss:4.3447 train_time:339113ms step_avg:169.81ms\nstep:2008/10000 train_loss:4.2840 train_time:339284ms step_avg:169.81ms\nstep:2009/10000 train_loss:5.0017 train_time:339453ms step_avg:169.81ms\nstep:2010/10000 train_loss:4.4096 train_time:339624ms step_avg:169.81ms\nstep:2011/10000 train_loss:4.4278 train_time:339795ms step_avg:169.81ms\nstep:2012/10000 train_loss:4.3878 train_time:339966ms step_avg:169.81ms\nstep:2013/10000 train_loss:4.4025 train_time:340136ms step_avg:169.81ms\nstep:2014/10000 train_loss:4.4302 train_time:340308ms step_avg:169.81ms\nstep:2015/10000 train_loss:4.2872 train_time:340477ms step_avg:169.81ms\nstep:2016/10000 train_loss:4.2901 train_time:340648ms step_avg:169.81ms\nstep:2017/10000 train_loss:4.2884 train_time:340817ms step_avg:169.81ms\nstep:2018/10000 train_loss:4.4241 train_time:340986ms step_avg:169.81ms\nstep:2019/10000 train_loss:4.4779 train_time:341154ms step_avg:169.81ms\nstep:2020/10000 train_loss:4.6716 train_time:341322ms step_avg:169.81ms\nstep:2021/10000 train_loss:4.2195 train_time:341490ms step_avg:169.81ms\nstep:2022/10000 train_loss:4.5255 train_time:341660ms step_avg:169.81ms\nstep:2023/10000 train_loss:4.2217 train_time:341831ms step_avg:169.81ms\nstep:2024/10000 train_loss:4.3835 train_time:342002ms step_avg:169.81ms\nstep:2025/10000 train_loss:4.3459 train_time:342175ms step_avg:169.81ms\nstep:2026/10000 train_loss:4.2724 train_time:342347ms step_avg:169.82ms\nstep:2027/10000 train_loss:4.3261 train_time:342515ms step_avg:169.81ms\nstep:2028/10000 train_loss:4.4279 train_time:342686ms step_avg:169.81ms\nstep:2029/10000 train_loss:4.2105 train_time:342856ms step_avg:169.81ms\nstep:2030/10000 train_loss:4.4241 train_time:343027ms step_avg:169.82ms\nstep:2031/10000 train_loss:4.4319 train_time:343196ms step_avg:169.81ms\nstep:2032/10000 train_loss:4.6037 train_time:343365ms step_avg:169.81ms\nstep:2033/10000 train_loss:4.5118 train_time:343536ms step_avg:169.82ms\nstep:2034/10000 train_loss:4.5070 train_time:343707ms step_avg:169.82ms\nstep:2035/10000 train_loss:4.3087 train_time:343875ms step_avg:169.81ms\nstep:2036/10000 train_loss:4.3011 train_time:344046ms step_avg:169.82ms\nstep:2037/10000 train_loss:4.3967 train_time:344216ms step_avg:169.82ms\nstep:2038/10000 train_loss:4.4469 train_time:344387ms step_avg:169.82ms\nstep:2039/10000 train_loss:4.5045 train_time:344557ms step_avg:169.82ms\nstep:2040/10000 train_loss:4.4138 train_time:344728ms step_avg:169.82ms\nstep:2041/10000 train_loss:4.3766 train_time:344897ms step_avg:169.82ms\nstep:2042/10000 train_loss:4.2664 train_time:345070ms step_avg:169.82ms\nstep:2043/10000 train_loss:4.3489 train_time:345238ms step_avg:169.82ms\nstep:2044/10000 train_loss:4.4189 train_time:345406ms step_avg:169.82ms\nstep:2045/10000 train_loss:4.3656 train_time:345574ms step_avg:169.82ms\nstep:2046/10000 train_loss:4.4547 train_time:345746ms step_avg:169.82ms\nstep:2047/10000 train_loss:4.3759 train_time:345914ms step_avg:169.82ms\nstep:2048/10000 train_loss:4.2454 train_time:346085ms step_avg:169.82ms\nstep:2049/10000 train_loss:4.3210 train_time:346254ms step_avg:169.82ms\nstep:2050/10000 train_loss:4.3789 train_time:346424ms step_avg:169.82ms\nstep:2051/10000 train_loss:4.3642 train_time:346594ms step_avg:169.82ms\nstep:2052/10000 train_loss:4.2379 train_time:346767ms step_avg:169.82ms\nstep:2053/10000 train_loss:4.4404 train_time:346937ms step_avg:169.82ms\nstep:2054/10000 train_loss:4.3697 train_time:347110ms step_avg:169.82ms\nstep:2055/10000 train_loss:4.3366 train_time:347279ms step_avg:169.82ms\nstep:2056/10000 train_loss:4.4595 train_time:347448ms step_avg:169.82ms\nstep:2057/10000 train_loss:4.3380 train_time:347618ms step_avg:169.82ms\nstep:2058/10000 train_loss:4.4296 train_time:347791ms step_avg:169.82ms\nstep:2059/10000 train_loss:4.3308 train_time:347962ms step_avg:169.82ms\nstep:2060/10000 train_loss:4.6292 train_time:348132ms step_avg:169.82ms\nstep:2061/10000 train_loss:4.5019 train_time:348302ms step_avg:169.82ms\nstep:2062/10000 train_loss:4.4592 train_time:348473ms step_avg:169.82ms\nstep:2063/10000 train_loss:4.3891 train_time:348643ms step_avg:169.82ms\nstep:2064/10000 train_loss:4.4422 train_time:348813ms step_avg:169.82ms\nstep:2065/10000 train_loss:4.4442 train_time:348985ms step_avg:169.82ms\nstep:2066/10000 train_loss:4.7870 train_time:349154ms step_avg:169.82ms\nstep:2067/10000 train_loss:4.5374 train_time:349325ms step_avg:169.82ms\nstep:2068/10000 train_loss:4.2816 train_time:349494ms step_avg:169.82ms\nstep:2069/10000 train_loss:4.3991 train_time:349665ms step_avg:169.82ms\nstep:2070/10000 train_loss:4.3641 train_time:349833ms step_avg:169.82ms\nstep:2071/10000 train_loss:4.3881 train_time:350002ms step_avg:169.82ms\nstep:2072/10000 train_loss:4.4700 train_time:350170ms step_avg:169.82ms\nstep:2073/10000 train_loss:4.4394 train_time:350341ms step_avg:169.82ms\nstep:2074/10000 train_loss:4.4805 train_time:350512ms step_avg:169.82ms\nstep:2075/10000 train_loss:4.2363 train_time:350680ms step_avg:169.82ms\nstep:2076/10000 train_loss:4.2121 train_time:350852ms step_avg:169.82ms\nstep:2077/10000 train_loss:4.4971 train_time:351022ms step_avg:169.82ms\nstep:2078/10000 train_loss:4.4221 train_time:351191ms step_avg:169.82ms\nstep:2079/10000 train_loss:4.3213 train_time:351363ms step_avg:169.82ms\nstep:2080/10000 train_loss:4.4411 train_time:351533ms step_avg:169.82ms\nstep:2081/10000 train_loss:4.3606 train_time:351703ms step_avg:169.82ms\nstep:2082/10000 train_loss:4.3899 train_time:351871ms step_avg:169.82ms\nstep:2083/10000 train_loss:4.3429 train_time:352041ms step_avg:169.82ms\nstep:2084/10000 train_loss:4.3167 train_time:352209ms step_avg:169.82ms\nstep:2085/10000 train_loss:4.3527 train_time:352376ms step_avg:169.82ms\nstep:2086/10000 train_loss:4.2769 train_time:352545ms step_avg:169.82ms\nstep:2087/10000 train_loss:4.4060 train_time:352714ms step_avg:169.82ms\nstep:2088/10000 train_loss:4.1992 train_time:352882ms step_avg:169.82ms\nstep:2089/10000 train_loss:4.4074 train_time:353053ms step_avg:169.82ms\nstep:2090/10000 train_loss:4.3113 train_time:353224ms step_avg:169.82ms\nstep:2091/10000 train_loss:4.3879 train_time:353396ms step_avg:169.82ms\nstep:2092/10000 train_loss:4.4571 train_time:353572ms step_avg:169.82ms\nstep:2093/10000 train_loss:4.4918 train_time:353738ms step_avg:169.82ms\nstep:2094/10000 train_loss:4.9065 train_time:353911ms step_avg:169.82ms\nstep:2095/10000 train_loss:4.2554 train_time:354079ms step_avg:169.82ms\nstep:2096/10000 train_loss:4.2551 train_time:354250ms step_avg:169.82ms\nstep:2097/10000 train_loss:4.1848 train_time:354417ms step_avg:169.82ms\nstep:2098/10000 train_loss:4.4245 train_time:354590ms step_avg:169.82ms\nstep:2099/10000 train_loss:4.3350 train_time:354758ms step_avg:169.82ms\nstep:2100/10000 train_loss:4.3234 train_time:354929ms step_avg:169.82ms\nstep:2101/10000 train_loss:4.3424 train_time:355098ms step_avg:169.82ms\nstep:2102/10000 train_loss:4.3728 train_time:355270ms step_avg:169.82ms\nstep:2103/10000 train_loss:4.3251 train_time:355440ms step_avg:169.82ms\nstep:2104/10000 train_loss:4.3545 train_time:355611ms step_avg:169.82ms\nstep:2105/10000 train_loss:4.1608 train_time:355780ms step_avg:169.82ms\nstep:2106/10000 train_loss:4.3608 train_time:355951ms step_avg:169.82ms\nstep:2107/10000 train_loss:4.5217 train_time:356120ms step_avg:169.82ms\nstep:2108/10000 train_loss:4.5921 train_time:356290ms step_avg:169.82ms\nstep:2109/10000 train_loss:4.2593 train_time:356459ms step_avg:169.82ms\nstep:2110/10000 train_loss:4.2995 train_time:356629ms step_avg:169.82ms\nstep:2111/10000 train_loss:4.3587 train_time:356801ms step_avg:169.82ms\nstep:2112/10000 train_loss:4.3149 train_time:356971ms step_avg:169.82ms\nstep:2113/10000 train_loss:4.3815 train_time:357142ms step_avg:169.82ms\nstep:2114/10000 train_loss:4.2894 train_time:357312ms step_avg:169.83ms\nstep:2115/10000 train_loss:4.4580 train_time:357483ms step_avg:169.83ms\nstep:2116/10000 train_loss:4.2950 train_time:357650ms step_avg:169.82ms\nstep:2117/10000 train_loss:4.3154 train_time:357819ms step_avg:169.82ms\nstep:2118/10000 train_loss:4.4604 train_time:357987ms step_avg:169.82ms\nstep:2119/10000 train_loss:4.4650 train_time:358156ms step_avg:169.82ms\nstep:2120/10000 train_loss:4.3393 train_time:358325ms step_avg:169.82ms\nstep:2121/10000 train_loss:4.3758 train_time:358494ms step_avg:169.82ms\nstep:2122/10000 train_loss:4.2990 train_time:358663ms step_avg:169.82ms\nstep:2123/10000 train_loss:4.3587 train_time:358832ms step_avg:169.82ms\nstep:2124/10000 train_loss:4.4525 train_time:359003ms step_avg:169.82ms\nstep:2125/10000 train_loss:4.2587 train_time:359170ms step_avg:169.82ms\nstep:2126/10000 train_loss:4.5671 train_time:359341ms step_avg:169.82ms\nstep:2127/10000 train_loss:4.2331 train_time:359511ms step_avg:169.82ms\nstep:2128/10000 train_loss:4.3538 train_time:359683ms step_avg:169.82ms\nstep:2129/10000 train_loss:4.1594 train_time:359851ms step_avg:169.82ms\nstep:2130/10000 train_loss:4.3684 train_time:360024ms step_avg:169.82ms\nstep:2131/10000 train_loss:4.4676 train_time:360193ms step_avg:169.82ms\nstep:2132/10000 train_loss:4.3717 train_time:360365ms step_avg:169.82ms\nstep:2133/10000 train_loss:4.1512 train_time:360534ms step_avg:169.82ms\nstep:2134/10000 train_loss:4.3449 train_time:360702ms step_avg:169.82ms\nstep:2135/10000 train_loss:4.4205 train_time:360873ms step_avg:169.82ms\nstep:2136/10000 train_loss:4.4552 train_time:361045ms step_avg:169.82ms\nstep:2137/10000 train_loss:4.3628 train_time:361218ms step_avg:169.83ms\nstep:2138/10000 train_loss:4.3454 train_time:361382ms step_avg:169.82ms\nstep:2139/10000 train_loss:4.4078 train_time:361549ms step_avg:169.82ms\nstep:2140/10000 train_loss:4.4307 train_time:361720ms step_avg:169.82ms\nstep:2141/10000 train_loss:4.3641 train_time:361891ms step_avg:169.82ms\nstep:2142/10000 train_loss:4.2909 train_time:362063ms step_avg:169.82ms\nstep:2143/10000 train_loss:4.3806 train_time:362232ms step_avg:169.82ms\nstep:2144/10000 train_loss:4.3524 train_time:362405ms step_avg:169.82ms\nstep:2145/10000 train_loss:4.6370 train_time:362576ms step_avg:169.82ms\nstep:2146/10000 train_loss:4.3893 train_time:362745ms step_avg:169.82ms\nstep:2147/10000 train_loss:4.2291 train_time:362914ms step_avg:169.82ms\nstep:2148/10000 train_loss:4.3690 train_time:363085ms step_avg:169.82ms\nstep:2149/10000 train_loss:4.2100 train_time:363255ms step_avg:169.82ms\nstep:2150/10000 train_loss:4.3794 train_time:363427ms step_avg:169.83ms\nstep:2151/10000 train_loss:4.2000 train_time:363597ms step_avg:169.83ms\nstep:2152/10000 train_loss:4.2593 train_time:363768ms step_avg:169.83ms\nstep:2153/10000 train_loss:4.4306 train_time:363938ms step_avg:169.83ms\nstep:2154/10000 train_loss:4.3122 train_time:364111ms step_avg:169.83ms\nstep:2155/10000 train_loss:4.2171 train_time:364279ms step_avg:169.83ms\nstep:2156/10000 train_loss:4.2249 train_time:364451ms step_avg:169.83ms\nstep:2157/10000 train_loss:4.4110 train_time:364622ms step_avg:169.83ms\nstep:2158/10000 train_loss:4.7756 train_time:364789ms step_avg:169.83ms\nstep:2159/10000 train_loss:4.2718 train_time:364960ms step_avg:169.83ms\nstep:2160/10000 train_loss:4.3038 train_time:365135ms step_avg:169.83ms\nstep:2161/10000 train_loss:4.4047 train_time:365304ms step_avg:169.83ms\nstep:2162/10000 train_loss:4.3500 train_time:365472ms step_avg:169.83ms\nstep:2163/10000 train_loss:4.4648 train_time:365642ms step_avg:169.83ms\nstep:2164/10000 train_loss:4.3168 train_time:365811ms step_avg:169.83ms\nstep:2165/10000 train_loss:4.0744 train_time:365981ms step_avg:169.83ms\nstep:2166/10000 train_loss:4.4300 train_time:366152ms step_avg:169.83ms\nstep:2167/10000 train_loss:4.8516 train_time:366323ms step_avg:169.83ms\nstep:2168/10000 train_loss:4.2854 train_time:366491ms step_avg:169.83ms\nstep:2169/10000 train_loss:4.3582 train_time:366662ms step_avg:169.83ms\nstep:2170/10000 train_loss:4.2990 train_time:366829ms step_avg:169.83ms\nstep:2171/10000 train_loss:4.2240 train_time:367000ms step_avg:169.83ms\nstep:2172/10000 train_loss:4.3774 train_time:367169ms step_avg:169.83ms\nstep:2173/10000 train_loss:4.4981 train_time:367339ms step_avg:169.83ms\nstep:2174/10000 train_loss:4.2909 train_time:367510ms step_avg:169.83ms\nstep:2175/10000 train_loss:4.4389 train_time:367680ms step_avg:169.83ms\nstep:2176/10000 train_loss:4.3520 train_time:367848ms step_avg:169.83ms\nstep:2177/10000 train_loss:4.4037 train_time:368016ms step_avg:169.83ms\nstep:2178/10000 train_loss:4.3926 train_time:368185ms step_avg:169.83ms\nstep:2179/10000 train_loss:4.2796 train_time:368353ms step_avg:169.83ms\nstep:2180/10000 train_loss:4.4017 train_time:368523ms step_avg:169.83ms\nstep:2181/10000 train_loss:4.5076 train_time:368692ms step_avg:169.83ms\nstep:2182/10000 train_loss:4.2789 train_time:368862ms step_avg:169.83ms\nstep:2183/10000 train_loss:4.2094 train_time:369032ms step_avg:169.83ms\nstep:2184/10000 train_loss:4.3954 train_time:369202ms step_avg:169.83ms\nstep:2185/10000 train_loss:4.3421 train_time:369373ms step_avg:169.83ms\nstep:2186/10000 train_loss:4.3312 train_time:369545ms step_avg:169.83ms\nstep:2187/10000 train_loss:4.2695 train_time:369713ms step_avg:169.83ms\nstep:2188/10000 train_loss:4.2179 train_time:369881ms step_avg:169.83ms\nstep:2189/10000 train_loss:4.3330 train_time:370052ms step_avg:169.83ms\nstep:2190/10000 train_loss:4.3238 train_time:370223ms step_avg:169.83ms\nstep:2191/10000 train_loss:4.3981 train_time:370393ms step_avg:169.83ms\nstep:2192/10000 train_loss:4.4864 train_time:370565ms step_avg:169.83ms\nstep:2193/10000 train_loss:4.2509 train_time:370734ms step_avg:169.83ms\nstep:2194/10000 train_loss:4.3190 train_time:370905ms step_avg:169.83ms\nstep:2195/10000 train_loss:4.1726 train_time:371073ms step_avg:169.83ms\nstep:2196/10000 train_loss:4.0254 train_time:371241ms step_avg:169.83ms\nstep:2197/10000 train_loss:4.3254 train_time:371411ms step_avg:169.83ms\nstep:2198/10000 train_loss:4.3714 train_time:371581ms step_avg:169.83ms\nstep:2199/10000 train_loss:4.2126 train_time:371750ms step_avg:169.83ms\nstep:2200/10000 train_loss:4.4300 train_time:371920ms step_avg:169.83ms\nstep:2201/10000 train_loss:4.1899 train_time:372091ms step_avg:169.83ms\nstep:2202/10000 train_loss:4.2420 train_time:372259ms step_avg:169.83ms\nstep:2203/10000 train_loss:4.2735 train_time:372430ms step_avg:169.83ms\nstep:2204/10000 train_loss:4.3540 train_time:372599ms step_avg:169.83ms\nstep:2205/10000 train_loss:4.3545 train_time:372770ms step_avg:169.83ms\nstep:2206/10000 train_loss:4.2873 train_time:372940ms step_avg:169.83ms\nstep:2207/10000 train_loss:4.2406 train_time:373110ms step_avg:169.83ms\nstep:2208/10000 train_loss:4.2668 train_time:373286ms step_avg:169.83ms\nstep:2209/10000 train_loss:4.2793 train_time:373457ms step_avg:169.83ms\nstep:2210/10000 train_loss:4.5954 train_time:373629ms step_avg:169.83ms\nstep:2211/10000 train_loss:4.3263 train_time:373797ms step_avg:169.83ms\nstep:2212/10000 train_loss:4.4654 train_time:373965ms step_avg:169.83ms\nstep:2213/10000 train_loss:4.3989 train_time:374132ms step_avg:169.83ms\nstep:2214/10000 train_loss:4.3948 train_time:374307ms step_avg:169.83ms\nstep:2215/10000 train_loss:4.3648 train_time:374475ms step_avg:169.83ms\nstep:2216/10000 train_loss:4.3424 train_time:374644ms step_avg:169.83ms\nstep:2217/10000 train_loss:4.2093 train_time:374815ms step_avg:169.83ms\nstep:2218/10000 train_loss:4.3733 train_time:374986ms step_avg:169.83ms\nstep:2219/10000 train_loss:4.3027 train_time:375153ms step_avg:169.83ms\nstep:2220/10000 train_loss:4.3030 train_time:375325ms step_avg:169.83ms\nstep:2221/10000 train_loss:4.1615 train_time:375496ms step_avg:169.83ms\nstep:2222/10000 train_loss:4.3949 train_time:375668ms step_avg:169.83ms\nstep:2223/10000 train_loss:4.3820 train_time:375838ms step_avg:169.83ms\nstep:2224/10000 train_loss:4.4072 train_time:376012ms step_avg:169.83ms\nstep:2225/10000 train_loss:4.2483 train_time:376181ms step_avg:169.83ms\nstep:2226/10000 train_loss:4.3403 train_time:376351ms step_avg:169.83ms\nstep:2227/10000 train_loss:4.3409 train_time:376521ms step_avg:169.83ms\nstep:2228/10000 train_loss:4.3584 train_time:376691ms step_avg:169.83ms\nstep:2229/10000 train_loss:4.2202 train_time:376864ms step_avg:169.84ms\nstep:2230/10000 train_loss:4.0846 train_time:377037ms step_avg:169.84ms\nstep:2231/10000 train_loss:4.4580 train_time:377204ms step_avg:169.84ms\nstep:2232/10000 train_loss:4.2515 train_time:377375ms step_avg:169.84ms\nstep:2233/10000 train_loss:4.1956 train_time:377544ms step_avg:169.84ms\nstep:2234/10000 train_loss:4.2543 train_time:377714ms step_avg:169.84ms\nstep:2235/10000 train_loss:4.2791 train_time:377888ms step_avg:169.84ms\nstep:2236/10000 train_loss:4.3558 train_time:378057ms step_avg:169.84ms\nstep:2237/10000 train_loss:4.4430 train_time:378229ms step_avg:169.84ms\nstep:2238/10000 train_loss:4.3175 train_time:378397ms step_avg:169.84ms\nstep:2239/10000 train_loss:4.2762 train_time:378565ms step_avg:169.84ms\nstep:2240/10000 train_loss:4.2354 train_time:378733ms step_avg:169.84ms\nstep:2241/10000 train_loss:4.4419 train_time:378906ms step_avg:169.84ms\nstep:2242/10000 train_loss:4.3534 train_time:379073ms step_avg:169.84ms\nstep:2243/10000 train_loss:4.5069 train_time:379244ms step_avg:169.84ms\nstep:2244/10000 train_loss:4.3210 train_time:379413ms step_avg:169.84ms\nstep:2245/10000 train_loss:4.2004 train_time:379584ms step_avg:169.84ms\nstep:2246/10000 train_loss:4.2438 train_time:379754ms step_avg:169.84ms\nstep:2247/10000 train_loss:4.3216 train_time:379928ms step_avg:169.84ms\nstep:2248/10000 train_loss:4.3013 train_time:380096ms step_avg:169.84ms\nstep:2249/10000 train_loss:4.2990 train_time:380266ms step_avg:169.84ms\nstep:2250/10000 train_loss:4.1958 train_time:380435ms step_avg:169.84ms\nstep:2251/10000 train_loss:4.2101 train_time:380606ms step_avg:169.84ms\nstep:2252/10000 train_loss:4.1548 train_time:380775ms step_avg:169.84ms\nstep:2253/10000 train_loss:4.3885 train_time:380948ms step_avg:169.84ms\nstep:2254/10000 train_loss:4.3936 train_time:381116ms step_avg:169.84ms\nstep:2255/10000 train_loss:4.2569 train_time:381285ms step_avg:169.84ms\nstep:2256/10000 train_loss:4.3657 train_time:381453ms step_avg:169.84ms\nstep:2257/10000 train_loss:4.2267 train_time:381621ms step_avg:169.84ms\nstep:2258/10000 train_loss:4.2344 train_time:381789ms step_avg:169.84ms\nstep:2259/10000 train_loss:4.3678 train_time:381962ms step_avg:169.84ms\nstep:2260/10000 train_loss:4.2913 train_time:382135ms step_avg:169.84ms\nstep:2261/10000 train_loss:4.4011 train_time:382307ms step_avg:169.84ms\nstep:2262/10000 train_loss:4.3328 train_time:382477ms step_avg:169.84ms\nstep:2263/10000 train_loss:4.2227 train_time:382648ms step_avg:169.84ms\nstep:2264/10000 train_loss:4.2648 train_time:382817ms step_avg:169.84ms\nstep:2265/10000 train_loss:4.4578 train_time:382989ms step_avg:169.84ms\nstep:2266/10000 train_loss:4.3138 train_time:383157ms step_avg:169.84ms\nstep:2267/10000 train_loss:4.2451 train_time:383325ms step_avg:169.84ms\nstep:2268/10000 train_loss:4.3808 train_time:383492ms step_avg:169.84ms\nstep:2269/10000 train_loss:4.2673 train_time:383661ms step_avg:169.84ms\nstep:2270/10000 train_loss:4.2432 train_time:383831ms step_avg:169.84ms\nstep:2271/10000 train_loss:4.2227 train_time:384001ms step_avg:169.84ms\nstep:2272/10000 train_loss:4.3521 train_time:384174ms step_avg:169.84ms\nstep:2273/10000 train_loss:4.3360 train_time:384344ms step_avg:169.84ms\nstep:2274/10000 train_loss:4.3235 train_time:384513ms step_avg:169.84ms\nstep:2275/10000 train_loss:4.3286 train_time:384682ms step_avg:169.84ms\nstep:2276/10000 train_loss:4.5667 train_time:384851ms step_avg:169.84ms\nstep:2277/10000 train_loss:4.3209 train_time:385021ms step_avg:169.84ms\nstep:2278/10000 train_loss:4.3103 train_time:385192ms step_avg:169.84ms\nstep:2279/10000 train_loss:4.4212 train_time:385362ms step_avg:169.84ms\nstep:2280/10000 train_loss:4.2913 train_time:385533ms step_avg:169.84ms\nstep:2281/10000 train_loss:4.1540 train_time:385704ms step_avg:169.84ms\nstep:2282/10000 train_loss:4.3286 train_time:385871ms step_avg:169.84ms\nstep:2283/10000 train_loss:4.3546 train_time:386040ms step_avg:169.84ms\nstep:2284/10000 train_loss:4.3175 train_time:386213ms step_avg:169.84ms\nstep:2285/10000 train_loss:4.2919 train_time:386382ms step_avg:169.84ms\nstep:2286/10000 train_loss:4.2203 train_time:386777ms step_avg:169.94ms\nstep:2287/10000 train_loss:4.3105 train_time:386941ms step_avg:169.93ms\nstep:2288/10000 train_loss:4.4431 train_time:387109ms step_avg:169.93ms\nstep:2289/10000 train_loss:4.2328 train_time:387274ms step_avg:169.93ms\nstep:2290/10000 train_loss:4.1578 train_time:387446ms step_avg:169.93ms\nstep:2291/10000 train_loss:4.3520 train_time:387616ms step_avg:169.93ms\nstep:2292/10000 train_loss:4.3128 train_time:387797ms step_avg:169.94ms\nstep:2293/10000 train_loss:4.2730 train_time:387973ms step_avg:169.94ms\nstep:2294/10000 train_loss:4.2251 train_time:388142ms step_avg:169.94ms\nstep:2295/10000 train_loss:4.2930 train_time:388309ms step_avg:169.94ms\nstep:2296/10000 train_loss:4.3254 train_time:388476ms step_avg:169.94ms\nstep:2297/10000 train_loss:4.4217 train_time:388645ms step_avg:169.94ms\nstep:2298/10000 train_loss:4.6448 train_time:388817ms step_avg:169.94ms\nstep:2299/10000 train_loss:4.3694 train_time:388988ms step_avg:169.94ms\nstep:2300/10000 train_loss:4.4129 train_time:389158ms step_avg:169.94ms\nstep:2301/10000 train_loss:4.2709 train_time:389325ms step_avg:169.94ms\nstep:2302/10000 train_loss:4.2174 train_time:389493ms step_avg:169.94ms\nstep:2303/10000 train_loss:4.2465 train_time:389663ms step_avg:169.94ms\nstep:2304/10000 train_loss:4.3891 train_time:389831ms step_avg:169.94ms\nstep:2305/10000 train_loss:4.3538 train_time:390002ms step_avg:169.94ms\nstep:2306/10000 train_loss:4.2954 train_time:390171ms step_avg:169.93ms\nstep:2307/10000 train_loss:4.3744 train_time:390341ms step_avg:169.94ms\nstep:2308/10000 train_loss:4.3841 train_time:390510ms step_avg:169.93ms\nstep:2309/10000 train_loss:4.2502 train_time:390680ms step_avg:169.93ms\nstep:2310/10000 train_loss:4.4548 train_time:390849ms step_avg:169.93ms\nstep:2311/10000 train_loss:4.2973 train_time:391020ms step_avg:169.93ms\nstep:2312/10000 train_loss:4.3401 train_time:391192ms step_avg:169.94ms\nstep:2313/10000 train_loss:4.4129 train_time:391364ms step_avg:169.94ms\nstep:2314/10000 train_loss:4.2241 train_time:391532ms step_avg:169.94ms\nstep:2315/10000 train_loss:4.1813 train_time:391701ms step_avg:169.94ms\nstep:2316/10000 train_loss:4.2981 train_time:391871ms step_avg:169.94ms\nstep:2317/10000 train_loss:4.3862 train_time:392041ms step_avg:169.94ms\nstep:2318/10000 train_loss:4.2360 train_time:392212ms step_avg:169.94ms\nstep:2319/10000 train_loss:4.5193 train_time:392382ms step_avg:169.94ms\nstep:2320/10000 train_loss:4.2776 train_time:392550ms step_avg:169.94ms\nstep:2321/10000 train_loss:4.4000 train_time:392719ms step_avg:169.93ms\nstep:2322/10000 train_loss:4.3824 train_time:392887ms step_avg:169.93ms\nstep:2323/10000 train_loss:4.2274 train_time:393055ms step_avg:169.93ms\nstep:2324/10000 train_loss:4.2398 train_time:393226ms step_avg:169.93ms\nstep:2325/10000 train_loss:4.3318 train_time:393395ms step_avg:169.93ms\nstep:2326/10000 train_loss:4.3591 train_time:393565ms step_avg:169.93ms\nstep:2327/10000 train_loss:4.2304 train_time:393735ms step_avg:169.93ms\nstep:2328/10000 train_loss:4.2571 train_time:393906ms step_avg:169.93ms\nstep:2329/10000 train_loss:4.2002 train_time:394075ms step_avg:169.93ms\nstep:2330/10000 train_loss:4.3302 train_time:394244ms step_avg:169.93ms\nstep:2331/10000 train_loss:4.4001 train_time:394412ms step_avg:169.93ms\nstep:2332/10000 train_loss:4.4972 train_time:394582ms step_avg:169.93ms\nstep:2333/10000 train_loss:4.2931 train_time:394750ms step_avg:169.93ms\nstep:2334/10000 train_loss:4.4871 train_time:394920ms step_avg:169.93ms\nstep:2335/10000 train_loss:4.1851 train_time:395089ms step_avg:169.93ms\nstep:2336/10000 train_loss:4.2484 train_time:395259ms step_avg:169.93ms\nstep:2337/10000 train_loss:4.2539 train_time:395428ms step_avg:169.93ms\nstep:2338/10000 train_loss:4.2427 train_time:395597ms step_avg:169.93ms\nstep:2339/10000 train_loss:4.3508 train_time:395769ms step_avg:169.93ms\nstep:2340/10000 train_loss:4.2111 train_time:395941ms step_avg:169.93ms\nstep:2341/10000 train_loss:4.2770 train_time:396112ms step_avg:169.93ms\nstep:2342/10000 train_loss:4.3764 train_time:396280ms step_avg:169.93ms\nstep:2343/10000 train_loss:4.1897 train_time:396448ms step_avg:169.93ms\nstep:2344/10000 train_loss:4.3830 train_time:396620ms step_avg:169.93ms\nstep:2345/10000 train_loss:4.3401 train_time:396789ms step_avg:169.93ms\nstep:2346/10000 train_loss:4.3115 train_time:396961ms step_avg:169.93ms\nstep:2347/10000 train_loss:4.2675 train_time:397135ms step_avg:169.93ms\nstep:2348/10000 train_loss:4.1878 train_time:397306ms step_avg:169.93ms\nstep:2349/10000 train_loss:4.2600 train_time:397479ms step_avg:169.94ms\nstep:2350/10000 train_loss:4.6610 train_time:397644ms step_avg:169.93ms\nstep:2351/10000 train_loss:4.2035 train_time:397812ms step_avg:169.93ms\nstep:2352/10000 train_loss:4.2964 train_time:397982ms step_avg:169.93ms\nstep:2353/10000 train_loss:4.3278 train_time:398152ms step_avg:169.93ms\nstep:2354/10000 train_loss:4.1343 train_time:398323ms step_avg:169.93ms\nstep:2355/10000 train_loss:4.2584 train_time:398492ms step_avg:169.93ms\nstep:2356/10000 train_loss:4.3159 train_time:398664ms step_avg:169.93ms\nstep:2357/10000 train_loss:4.2728 train_time:398834ms step_avg:169.93ms\nstep:2358/10000 train_loss:4.5048 train_time:399004ms step_avg:169.93ms\nstep:2359/10000 train_loss:4.2837 train_time:399172ms step_avg:169.93ms\nstep:2360/10000 train_loss:4.6426 train_time:399341ms step_avg:169.93ms\nstep:2361/10000 train_loss:4.3513 train_time:399508ms step_avg:169.93ms\nstep:2362/10000 train_loss:4.3393 train_time:399677ms step_avg:169.93ms\nstep:2363/10000 train_loss:4.4656 train_time:399852ms step_avg:169.93ms\nstep:2364/10000 train_loss:4.4035 train_time:400023ms step_avg:169.93ms\nstep:2365/10000 train_loss:4.2860 train_time:400194ms step_avg:169.93ms\nstep:2366/10000 train_loss:4.1489 train_time:400366ms step_avg:169.93ms\nstep:2367/10000 train_loss:4.1800 train_time:400536ms step_avg:169.93ms\nstep:2368/10000 train_loss:4.1536 train_time:400706ms step_avg:169.93ms\nstep:2369/10000 train_loss:4.4662 train_time:400875ms step_avg:169.93ms\nstep:2370/10000 train_loss:4.2238 train_time:401045ms step_avg:169.93ms\nstep:2371/10000 train_loss:4.4029 train_time:401214ms step_avg:169.93ms\nstep:2372/10000 train_loss:4.1660 train_time:401384ms step_avg:169.93ms\nstep:2373/10000 train_loss:4.2803 train_time:401553ms step_avg:169.93ms\nstep:2374/10000 train_loss:4.3754 train_time:401725ms step_avg:169.93ms\nstep:2375/10000 train_loss:4.3852 train_time:401894ms step_avg:169.93ms\nstep:2376/10000 train_loss:4.1900 train_time:402066ms step_avg:169.93ms\nstep:2377/10000 train_loss:4.1946 train_time:402236ms step_avg:169.93ms\nstep:2378/10000 train_loss:4.2560 train_time:402405ms step_avg:169.93ms\nstep:2379/10000 train_loss:4.3227 train_time:402575ms step_avg:169.93ms\nstep:2380/10000 train_loss:4.2436 train_time:402744ms step_avg:169.93ms\nstep:2381/10000 train_loss:4.2163 train_time:402915ms step_avg:169.93ms\nstep:2382/10000 train_loss:4.2717 train_time:403087ms step_avg:169.94ms\nstep:2383/10000 train_loss:4.6480 train_time:403256ms step_avg:169.94ms\nstep:2384/10000 train_loss:4.5442 train_time:403425ms step_avg:169.93ms\nstep:2385/10000 train_loss:4.2974 train_time:403596ms step_avg:169.94ms\nstep:2386/10000 train_loss:4.2923 train_time:403765ms step_avg:169.93ms\nstep:2387/10000 train_loss:4.1621 train_time:403934ms step_avg:169.93ms\nstep:2388/10000 train_loss:4.3004 train_time:404103ms step_avg:169.93ms\nstep:2389/10000 train_loss:4.3755 train_time:404273ms step_avg:169.93ms\nstep:2390/10000 train_loss:4.3362 train_time:404442ms step_avg:169.93ms\nstep:2391/10000 train_loss:4.2860 train_time:404611ms step_avg:169.93ms\nstep:2392/10000 train_loss:4.2280 train_time:404782ms step_avg:169.93ms\nstep:2393/10000 train_loss:4.2815 train_time:404952ms step_avg:169.93ms\nstep:2394/10000 train_loss:4.3521 train_time:405122ms step_avg:169.93ms\nstep:2395/10000 train_loss:4.3127 train_time:405294ms step_avg:169.93ms\nstep:2396/10000 train_loss:4.2796 train_time:405465ms step_avg:169.93ms\nstep:2397/10000 train_loss:4.2979 train_time:405636ms step_avg:169.94ms\nstep:2398/10000 train_loss:4.4215 train_time:405806ms step_avg:169.94ms\nstep:2399/10000 train_loss:4.2677 train_time:405974ms step_avg:169.93ms\nstep:2400/10000 train_loss:4.1060 train_time:406145ms step_avg:169.94ms\nstep:2401/10000 train_loss:4.2696 train_time:406315ms step_avg:169.94ms\nstep:2402/10000 train_loss:4.2746 train_time:406486ms step_avg:169.94ms\nstep:2403/10000 train_loss:4.2141 train_time:406654ms step_avg:169.93ms\nstep:2404/10000 train_loss:4.2333 train_time:406826ms step_avg:169.94ms\nstep:2405/10000 train_loss:4.3344 train_time:406994ms step_avg:169.93ms\nstep:2406/10000 train_loss:4.1881 train_time:407163ms step_avg:169.93ms\nstep:2407/10000 train_loss:4.1712 train_time:407332ms step_avg:169.93ms\nstep:2408/10000 train_loss:4.2327 train_time:407502ms step_avg:169.93ms\nstep:2409/10000 train_loss:4.1790 train_time:407673ms step_avg:169.93ms\nstep:2410/10000 train_loss:4.3572 train_time:407845ms step_avg:169.94ms\nstep:2411/10000 train_loss:4.3032 train_time:408014ms step_avg:169.94ms\nstep:2412/10000 train_loss:4.2579 train_time:408187ms step_avg:169.94ms\nstep:2413/10000 train_loss:4.1750 train_time:408355ms step_avg:169.94ms\nstep:2414/10000 train_loss:4.1847 train_time:408524ms step_avg:169.93ms\nstep:2415/10000 train_loss:4.3159 train_time:408693ms step_avg:169.93ms\nstep:2416/10000 train_loss:4.3919 train_time:408866ms step_avg:169.94ms\nstep:2417/10000 train_loss:4.2914 train_time:409033ms step_avg:169.93ms\nstep:2418/10000 train_loss:4.2971 train_time:409202ms step_avg:169.93ms\nstep:2419/10000 train_loss:4.3906 train_time:409370ms step_avg:169.93ms\nstep:2420/10000 train_loss:4.3069 train_time:409540ms step_avg:169.93ms\nstep:2421/10000 train_loss:4.2991 train_time:409708ms step_avg:169.93ms\nstep:2422/10000 train_loss:4.5661 train_time:409878ms step_avg:169.93ms\nstep:2423/10000 train_loss:4.3950 train_time:410052ms step_avg:169.93ms\nstep:2424/10000 train_loss:4.2459 train_time:410222ms step_avg:169.93ms\nstep:2425/10000 train_loss:4.1554 train_time:410390ms step_avg:169.93ms\nstep:2426/10000 train_loss:4.3383 train_time:410559ms step_avg:169.93ms\nstep:2427/10000 train_loss:4.1067 train_time:410732ms step_avg:169.93ms\nstep:2428/10000 train_loss:4.1569 train_time:410903ms step_avg:169.94ms\nstep:2429/10000 train_loss:4.2335 train_time:411073ms step_avg:169.94ms\nstep:2430/10000 train_loss:4.3616 train_time:411245ms step_avg:169.94ms\nstep:2431/10000 train_loss:4.2750 train_time:411415ms step_avg:169.94ms\nstep:2432/10000 train_loss:4.3957 train_time:411585ms step_avg:169.94ms\nstep:2433/10000 train_loss:4.1995 train_time:411755ms step_avg:169.94ms\nstep:2434/10000 train_loss:4.2953 train_time:411925ms step_avg:169.94ms\nstep:2435/10000 train_loss:4.2677 train_time:412094ms step_avg:169.94ms\nstep:2436/10000 train_loss:4.2592 train_time:412265ms step_avg:169.94ms\nstep:2437/10000 train_loss:4.2559 train_time:412432ms step_avg:169.93ms\nstep:2438/10000 train_loss:4.1923 train_time:412602ms step_avg:169.93ms\nstep:2439/10000 train_loss:4.2145 train_time:412770ms step_avg:169.93ms\nstep:2440/10000 train_loss:4.2549 train_time:412941ms step_avg:169.93ms\nstep:2441/10000 train_loss:4.2966 train_time:413109ms step_avg:169.93ms\nstep:2442/10000 train_loss:4.2748 train_time:413280ms step_avg:169.93ms\nstep:2443/10000 train_loss:4.2762 train_time:413453ms step_avg:169.94ms\nstep:2444/10000 train_loss:4.3513 train_time:413624ms step_avg:169.94ms\nstep:2445/10000 train_loss:4.2001 train_time:413792ms step_avg:169.94ms\nstep:2446/10000 train_loss:4.2114 train_time:413962ms step_avg:169.94ms\nstep:2447/10000 train_loss:4.2723 train_time:414133ms step_avg:169.94ms\nstep:2448/10000 train_loss:4.2600 train_time:414303ms step_avg:169.94ms\nstep:2449/10000 train_loss:4.3253 train_time:414472ms step_avg:169.94ms\nstep:2450/10000 train_loss:4.3905 train_time:414641ms step_avg:169.93ms\nstep:2451/10000 train_loss:4.1701 train_time:414811ms step_avg:169.94ms\nstep:2452/10000 train_loss:4.2225 train_time:414980ms step_avg:169.93ms\nstep:2453/10000 train_loss:3.9823 train_time:415148ms step_avg:169.93ms\nstep:2454/10000 train_loss:4.1877 train_time:415318ms step_avg:169.93ms\nstep:2455/10000 train_loss:4.2369 train_time:415492ms step_avg:169.94ms\nstep:2456/10000 train_loss:4.3919 train_time:415660ms step_avg:169.93ms\nstep:2457/10000 train_loss:4.1484 train_time:415829ms step_avg:169.93ms\nstep:2458/10000 train_loss:4.2786 train_time:415998ms step_avg:169.93ms\nstep:2459/10000 train_loss:4.3670 train_time:416171ms step_avg:169.93ms\nstep:2460/10000 train_loss:4.3682 train_time:416340ms step_avg:169.93ms\nstep:2461/10000 train_loss:4.2839 train_time:416510ms step_avg:169.93ms\nstep:2462/10000 train_loss:4.2214 train_time:416679ms step_avg:169.93ms\nstep:2463/10000 train_loss:4.3938 train_time:416851ms step_avg:169.94ms\nstep:2464/10000 train_loss:4.3265 train_time:417019ms step_avg:169.93ms\nstep:2465/10000 train_loss:4.2371 train_time:417189ms step_avg:169.93ms\nstep:2466/10000 train_loss:4.2805 train_time:417361ms step_avg:169.94ms\nstep:2467/10000 train_loss:4.2029 train_time:417535ms step_avg:169.94ms\nstep:2468/10000 train_loss:4.2010 train_time:417703ms step_avg:169.94ms\nstep:2469/10000 train_loss:4.3495 train_time:417873ms step_avg:169.94ms\nstep:2470/10000 train_loss:4.3164 train_time:418044ms step_avg:169.94ms\nstep:2471/10000 train_loss:4.2053 train_time:418212ms step_avg:169.94ms\nstep:2472/10000 train_loss:4.1933 train_time:418382ms step_avg:169.94ms\nstep:2473/10000 train_loss:4.3579 train_time:418551ms step_avg:169.94ms\nstep:2474/10000 train_loss:4.1101 train_time:418721ms step_avg:169.94ms\nstep:2475/10000 train_loss:4.2675 train_time:418891ms step_avg:169.94ms\nstep:2476/10000 train_loss:4.2753 train_time:419060ms step_avg:169.94ms\nstep:2477/10000 train_loss:4.3306 train_time:419227ms step_avg:169.93ms\nstep:2478/10000 train_loss:4.3913 train_time:419397ms step_avg:169.93ms\nstep:2479/10000 train_loss:4.2612 train_time:419568ms step_avg:169.93ms\nstep:2480/10000 train_loss:4.2076 train_time:419739ms step_avg:169.93ms\nstep:2481/10000 train_loss:4.2320 train_time:419910ms step_avg:169.94ms\nstep:2482/10000 train_loss:4.3038 train_time:420082ms step_avg:169.94ms\nstep:2483/10000 train_loss:4.2073 train_time:420253ms step_avg:169.94ms\nstep:2484/10000 train_loss:4.2262 train_time:420424ms step_avg:169.94ms\nstep:2485/10000 train_loss:4.2363 train_time:420594ms step_avg:169.94ms\nstep:2486/10000 train_loss:4.2864 train_time:420765ms step_avg:169.94ms\nstep:2487/10000 train_loss:4.2863 train_time:420935ms step_avg:169.94ms\nstep:2488/10000 train_loss:4.4788 train_time:421106ms step_avg:169.94ms\nstep:2489/10000 train_loss:4.4545 train_time:421278ms step_avg:169.94ms\nstep:2490/10000 train_loss:4.1280 train_time:421450ms step_avg:169.94ms\nstep:2491/10000 train_loss:4.2910 train_time:421619ms step_avg:169.94ms\nstep:2492/10000 train_loss:4.2216 train_time:421789ms step_avg:169.94ms\nstep:2493/10000 train_loss:4.3454 train_time:421958ms step_avg:169.94ms\nstep:2494/10000 train_loss:4.2644 train_time:422126ms step_avg:169.94ms\nstep:2495/10000 train_loss:4.3168 train_time:422296ms step_avg:169.94ms\nstep:2496/10000 train_loss:4.2666 train_time:422468ms step_avg:169.94ms\nstep:2497/10000 train_loss:4.2950 train_time:422635ms step_avg:169.94ms\nstep:2498/10000 train_loss:4.3387 train_time:422806ms step_avg:169.94ms\nstep:2499/10000 train_loss:4.0915 train_time:422974ms step_avg:169.94ms\nstep:2500/10000 train_loss:4.3026 train_time:423147ms step_avg:169.94ms\nstep:2501/10000 train_loss:4.3200 train_time:423314ms step_avg:169.94ms\nstep:2502/10000 train_loss:4.2449 train_time:423483ms step_avg:169.94ms\nstep:2503/10000 train_loss:4.2657 train_time:423651ms step_avg:169.94ms\nstep:2504/10000 train_loss:4.3735 train_time:423822ms step_avg:169.94ms\nstep:2505/10000 train_loss:4.2028 train_time:423994ms step_avg:169.94ms\nstep:2506/10000 train_loss:4.2347 train_time:424164ms step_avg:169.94ms\nstep:2507/10000 train_loss:4.3010 train_time:424333ms step_avg:169.94ms\nstep:2508/10000 train_loss:4.3624 train_time:424504ms step_avg:169.94ms\nstep:2509/10000 train_loss:4.2185 train_time:424672ms step_avg:169.94ms\nstep:2510/10000 train_loss:4.1418 train_time:424843ms step_avg:169.94ms\nstep:2511/10000 train_loss:4.2654 train_time:425012ms step_avg:169.94ms\nstep:2512/10000 train_loss:4.3389 train_time:425184ms step_avg:169.94ms\nstep:2513/10000 train_loss:4.4055 train_time:425355ms step_avg:169.94ms\nstep:2514/10000 train_loss:4.3044 train_time:425528ms step_avg:169.94ms\nstep:2515/10000 train_loss:4.1624 train_time:425696ms step_avg:169.94ms\nstep:2516/10000 train_loss:4.3892 train_time:425864ms step_avg:169.94ms\nstep:2517/10000 train_loss:4.2158 train_time:426032ms step_avg:169.94ms\nstep:2518/10000 train_loss:4.1399 train_time:426202ms step_avg:169.94ms\nstep:2519/10000 train_loss:4.2324 train_time:426372ms step_avg:169.94ms\nstep:2520/10000 train_loss:4.1860 train_time:426542ms step_avg:169.94ms\nstep:2521/10000 train_loss:4.2710 train_time:426711ms step_avg:169.94ms\nstep:2522/10000 train_loss:4.3122 train_time:426882ms step_avg:169.94ms\nstep:2523/10000 train_loss:4.3431 train_time:427049ms step_avg:169.94ms\nstep:2524/10000 train_loss:4.3261 train_time:427221ms step_avg:169.94ms\nstep:2525/10000 train_loss:4.2042 train_time:427389ms step_avg:169.94ms\nstep:2526/10000 train_loss:4.1439 train_time:427560ms step_avg:169.94ms\nstep:2527/10000 train_loss:4.2367 train_time:427730ms step_avg:169.94ms\nstep:2528/10000 train_loss:4.3806 train_time:427900ms step_avg:169.94ms\nstep:2529/10000 train_loss:4.2426 train_time:428073ms step_avg:169.94ms\nstep:2530/10000 train_loss:4.2627 train_time:428243ms step_avg:169.94ms\nstep:2531/10000 train_loss:4.2016 train_time:428412ms step_avg:169.94ms\nstep:2532/10000 train_loss:4.2760 train_time:428582ms step_avg:169.94ms\nstep:2533/10000 train_loss:4.1410 train_time:428753ms step_avg:169.94ms\nstep:2534/10000 train_loss:4.4027 train_time:428922ms step_avg:169.94ms\nstep:2535/10000 train_loss:4.3642 train_time:429093ms step_avg:169.94ms\nstep:2536/10000 train_loss:4.1700 train_time:429263ms step_avg:169.94ms\nstep:2537/10000 train_loss:4.4154 train_time:429431ms step_avg:169.94ms\nstep:2538/10000 train_loss:4.2455 train_time:429600ms step_avg:169.94ms\nstep:2539/10000 train_loss:4.3176 train_time:429771ms step_avg:169.94ms\nstep:2540/10000 train_loss:4.2675 train_time:429939ms step_avg:169.94ms\nstep:2541/10000 train_loss:4.2091 train_time:430107ms step_avg:169.94ms\nstep:2542/10000 train_loss:4.1486 train_time:430276ms step_avg:169.94ms\nstep:2543/10000 train_loss:4.0813 train_time:430445ms step_avg:169.93ms\nstep:2544/10000 train_loss:4.5302 train_time:430613ms step_avg:169.93ms\nstep:2545/10000 train_loss:4.2451 train_time:430786ms step_avg:169.94ms\nstep:2546/10000 train_loss:4.2267 train_time:430953ms step_avg:169.93ms\nstep:2547/10000 train_loss:4.3101 train_time:431123ms step_avg:169.93ms\nstep:2548/10000 train_loss:4.1359 train_time:431292ms step_avg:169.93ms\nstep:2549/10000 train_loss:4.1930 train_time:431462ms step_avg:169.93ms\nstep:2550/10000 train_loss:4.1509 train_time:431633ms step_avg:169.93ms\nstep:2551/10000 train_loss:4.3662 train_time:431805ms step_avg:169.94ms\nstep:2552/10000 train_loss:3.9879 train_time:431974ms step_avg:169.93ms\nstep:2553/10000 train_loss:4.2774 train_time:432146ms step_avg:169.94ms\nstep:2554/10000 train_loss:4.1897 train_time:432313ms step_avg:169.93ms\nstep:2555/10000 train_loss:4.1859 train_time:432482ms step_avg:169.93ms\nstep:2556/10000 train_loss:4.1918 train_time:432649ms step_avg:169.93ms\nstep:2557/10000 train_loss:4.2422 train_time:432826ms step_avg:169.94ms\nstep:2558/10000 train_loss:4.2619 train_time:432991ms step_avg:169.93ms\nstep:2559/10000 train_loss:4.3364 train_time:433163ms step_avg:169.93ms\nstep:2560/10000 train_loss:4.1959 train_time:433333ms step_avg:169.93ms\nstep:2561/10000 train_loss:4.0992 train_time:433509ms step_avg:169.94ms\nstep:2562/10000 train_loss:4.1589 train_time:433673ms step_avg:169.93ms\nstep:2563/10000 train_loss:4.3317 train_time:433842ms step_avg:169.93ms\nstep:2564/10000 train_loss:4.2109 train_time:434012ms step_avg:169.93ms\nstep:2565/10000 train_loss:4.3103 train_time:434182ms step_avg:169.93ms\nstep:2566/10000 train_loss:4.1822 train_time:434352ms step_avg:169.93ms\nstep:2567/10000 train_loss:4.2545 train_time:434522ms step_avg:169.93ms\nstep:2568/10000 train_loss:4.2043 train_time:434694ms step_avg:169.93ms\nstep:2569/10000 train_loss:4.2804 train_time:434862ms step_avg:169.93ms\nstep:2570/10000 train_loss:4.3488 train_time:435031ms step_avg:169.93ms\nstep:2571/10000 train_loss:4.2193 train_time:435200ms step_avg:169.93ms\nstep:2572/10000 train_loss:4.2257 train_time:435370ms step_avg:169.93ms\nstep:2573/10000 train_loss:4.0569 train_time:435540ms step_avg:169.93ms\nstep:2574/10000 train_loss:4.0318 train_time:435711ms step_avg:169.93ms\nstep:2575/10000 train_loss:4.2858 train_time:435882ms step_avg:169.93ms\nstep:2576/10000 train_loss:4.2724 train_time:436055ms step_avg:169.94ms\nstep:2577/10000 train_loss:4.2282 train_time:436225ms step_avg:169.94ms\nstep:2578/10000 train_loss:4.2014 train_time:436393ms step_avg:169.94ms\nstep:2579/10000 train_loss:4.3866 train_time:436562ms step_avg:169.93ms\nstep:2580/10000 train_loss:4.1740 train_time:436733ms step_avg:169.93ms\nstep:2581/10000 train_loss:4.3537 train_time:436903ms step_avg:169.93ms\nstep:2582/10000 train_loss:4.1735 train_time:437073ms step_avg:169.94ms\nstep:2583/10000 train_loss:4.3227 train_time:437245ms step_avg:169.94ms\nstep:2584/10000 train_loss:4.3861 train_time:437415ms step_avg:169.94ms\nstep:2585/10000 train_loss:4.1772 train_time:437583ms step_avg:169.94ms\nstep:2586/10000 train_loss:4.3281 train_time:437752ms step_avg:169.93ms\nstep:2587/10000 train_loss:4.1937 train_time:437921ms step_avg:169.93ms\nstep:2588/10000 train_loss:4.2770 train_time:438091ms step_avg:169.93ms\nstep:2589/10000 train_loss:4.2474 train_time:438264ms step_avg:169.94ms\nstep:2590/10000 train_loss:4.2385 train_time:438434ms step_avg:169.94ms\nstep:2591/10000 train_loss:4.1834 train_time:438602ms step_avg:169.93ms\nstep:2592/10000 train_loss:4.2860 train_time:438771ms step_avg:169.93ms\nstep:2593/10000 train_loss:4.2222 train_time:438941ms step_avg:169.93ms\nstep:2594/10000 train_loss:4.4876 train_time:439110ms step_avg:169.93ms\nstep:2595/10000 train_loss:4.5321 train_time:439282ms step_avg:169.93ms\nstep:2596/10000 train_loss:4.2718 train_time:439455ms step_avg:169.94ms\nstep:2597/10000 train_loss:4.3476 train_time:439624ms step_avg:169.94ms\nstep:2598/10000 train_loss:4.2614 train_time:439795ms step_avg:169.94ms\nstep:2599/10000 train_loss:4.2424 train_time:439968ms step_avg:169.94ms\nstep:2600/10000 train_loss:4.3729 train_time:440137ms step_avg:169.94ms\nstep:2601/10000 train_loss:4.2718 train_time:440307ms step_avg:169.94ms\nstep:2602/10000 train_loss:4.1315 train_time:440476ms step_avg:169.94ms\nstep:2603/10000 train_loss:4.3042 train_time:440646ms step_avg:169.94ms\nstep:2604/10000 train_loss:4.2350 train_time:440817ms step_avg:169.94ms\nstep:2605/10000 train_loss:4.3546 train_time:440988ms step_avg:169.94ms\nstep:2606/10000 train_loss:4.1298 train_time:441158ms step_avg:169.94ms\nstep:2607/10000 train_loss:4.2398 train_time:441326ms step_avg:169.94ms\nstep:2608/10000 train_loss:4.3459 train_time:441496ms step_avg:169.94ms\nstep:2609/10000 train_loss:4.0922 train_time:441669ms step_avg:169.94ms\nstep:2610/10000 train_loss:4.2556 train_time:441837ms step_avg:169.94ms\nstep:2611/10000 train_loss:4.1999 train_time:442006ms step_avg:169.94ms\nstep:2612/10000 train_loss:4.2046 train_time:442175ms step_avg:169.94ms\nstep:2613/10000 train_loss:4.1594 train_time:442348ms step_avg:169.94ms\nstep:2614/10000 train_loss:4.2127 train_time:442516ms step_avg:169.94ms\nstep:2615/10000 train_loss:4.2107 train_time:442688ms step_avg:169.94ms\nstep:2616/10000 train_loss:4.2726 train_time:442859ms step_avg:169.94ms\nstep:2617/10000 train_loss:4.1995 train_time:443032ms step_avg:169.94ms\nstep:2618/10000 train_loss:4.3624 train_time:443201ms step_avg:169.94ms\nstep:2619/10000 train_loss:4.1740 train_time:443372ms step_avg:169.94ms\nstep:2620/10000 train_loss:4.0414 train_time:443543ms step_avg:169.94ms\nstep:2621/10000 train_loss:4.3266 train_time:443712ms step_avg:169.94ms\nstep:2622/10000 train_loss:4.3956 train_time:443881ms step_avg:169.94ms\nstep:2623/10000 train_loss:4.3813 train_time:444050ms step_avg:169.94ms\nstep:2624/10000 train_loss:4.1770 train_time:444221ms step_avg:169.94ms\nstep:2625/10000 train_loss:4.0785 train_time:444393ms step_avg:169.94ms\nstep:2626/10000 train_loss:4.2787 train_time:444564ms step_avg:169.94ms\nstep:2627/10000 train_loss:4.2689 train_time:444734ms step_avg:169.94ms\nstep:2628/10000 train_loss:4.1799 train_time:444904ms step_avg:169.94ms\nstep:2629/10000 train_loss:4.2888 train_time:445074ms step_avg:169.94ms\nstep:2630/10000 train_loss:4.2217 train_time:445244ms step_avg:169.94ms\nstep:2631/10000 train_loss:4.2525 train_time:445413ms step_avg:169.94ms\nstep:2632/10000 train_loss:4.1305 train_time:445581ms step_avg:169.94ms\nstep:2633/10000 train_loss:3.9789 train_time:445751ms step_avg:169.94ms\nstep:2634/10000 train_loss:4.0907 train_time:445922ms step_avg:169.94ms\nstep:2635/10000 train_loss:4.2841 train_time:446092ms step_avg:169.94ms\nstep:2636/10000 train_loss:4.3064 train_time:446262ms step_avg:169.94ms\nstep:2637/10000 train_loss:4.1759 train_time:446434ms step_avg:169.94ms\nstep:2638/10000 train_loss:4.2523 train_time:446604ms step_avg:169.94ms\nstep:2639/10000 train_loss:4.2136 train_time:446773ms step_avg:169.94ms\nstep:2640/10000 train_loss:4.2046 train_time:446944ms step_avg:169.94ms\nstep:2641/10000 train_loss:4.1606 train_time:447115ms step_avg:169.94ms\nstep:2642/10000 train_loss:4.3484 train_time:447285ms step_avg:169.94ms\nstep:2643/10000 train_loss:4.2984 train_time:447454ms step_avg:169.94ms\nstep:2644/10000 train_loss:4.2636 train_time:447625ms step_avg:169.94ms\nstep:2645/10000 train_loss:4.2428 train_time:447793ms step_avg:169.94ms\nstep:2646/10000 train_loss:4.2761 train_time:447964ms step_avg:169.94ms\nstep:2647/10000 train_loss:4.2571 train_time:448135ms step_avg:169.94ms\nstep:2648/10000 train_loss:4.3424 train_time:448308ms step_avg:169.94ms\nstep:2649/10000 train_loss:4.3491 train_time:448477ms step_avg:169.94ms\nstep:2650/10000 train_loss:4.1221 train_time:448646ms step_avg:169.94ms\nstep:2651/10000 train_loss:4.3200 train_time:448814ms step_avg:169.94ms\nstep:2652/10000 train_loss:4.2696 train_time:448983ms step_avg:169.94ms\nstep:2653/10000 train_loss:4.1889 train_time:449153ms step_avg:169.94ms\nstep:2654/10000 train_loss:4.2552 train_time:449324ms step_avg:169.94ms\nstep:2655/10000 train_loss:4.2145 train_time:449492ms step_avg:169.94ms\nstep:2656/10000 train_loss:4.1505 train_time:449663ms step_avg:169.94ms\nstep:2657/10000 train_loss:4.1780 train_time:449833ms step_avg:169.94ms\nstep:2658/10000 train_loss:4.2076 train_time:450003ms step_avg:169.94ms\nstep:2659/10000 train_loss:4.1638 train_time:450171ms step_avg:169.94ms\nstep:2660/10000 train_loss:4.2312 train_time:450344ms step_avg:169.94ms\nstep:2661/10000 train_loss:4.1807 train_time:450515ms step_avg:169.94ms\nstep:2662/10000 train_loss:4.1463 train_time:450684ms step_avg:169.94ms\nstep:2663/10000 train_loss:4.2460 train_time:450853ms step_avg:169.94ms\nstep:2664/10000 train_loss:4.1015 train_time:451023ms step_avg:169.94ms\nstep:2665/10000 train_loss:4.3865 train_time:451193ms step_avg:169.94ms\nstep:2666/10000 train_loss:4.2678 train_time:451364ms step_avg:169.94ms\nstep:2667/10000 train_loss:4.1175 train_time:451531ms step_avg:169.94ms\nstep:2668/10000 train_loss:4.3880 train_time:451701ms step_avg:169.94ms\nstep:2669/10000 train_loss:4.1313 train_time:451872ms step_avg:169.94ms\nstep:2670/10000 train_loss:4.3267 train_time:452044ms step_avg:169.94ms\nstep:2671/10000 train_loss:4.2696 train_time:452211ms step_avg:169.94ms\nstep:2672/10000 train_loss:4.2418 train_time:452385ms step_avg:169.94ms\nstep:2673/10000 train_loss:4.2399 train_time:452551ms step_avg:169.94ms\nstep:2674/10000 train_loss:4.3277 train_time:452720ms step_avg:169.94ms\nstep:2675/10000 train_loss:4.3141 train_time:452891ms step_avg:169.94ms\nstep:2676/10000 train_loss:4.2099 train_time:453063ms step_avg:169.94ms\nstep:2677/10000 train_loss:4.3691 train_time:453229ms step_avg:169.94ms\nstep:2678/10000 train_loss:4.1482 train_time:453399ms step_avg:169.94ms\nstep:2679/10000 train_loss:4.2384 train_time:453570ms step_avg:169.94ms\nstep:2680/10000 train_loss:4.0816 train_time:453738ms step_avg:169.94ms\nstep:2681/10000 train_loss:4.1987 train_time:453909ms step_avg:169.94ms\nstep:2682/10000 train_loss:4.2688 train_time:454078ms step_avg:169.94ms\nstep:2683/10000 train_loss:4.1925 train_time:454248ms step_avg:169.94ms\nstep:2684/10000 train_loss:4.1891 train_time:454417ms step_avg:169.94ms\nstep:2685/10000 train_loss:4.1817 train_time:454587ms step_avg:169.94ms\nstep:2686/10000 train_loss:4.2233 train_time:454757ms step_avg:169.94ms\nstep:2687/10000 train_loss:4.4315 train_time:454928ms step_avg:169.94ms\nstep:2688/10000 train_loss:4.1486 train_time:455098ms step_avg:169.94ms\nstep:2689/10000 train_loss:4.1724 train_time:455268ms step_avg:169.94ms\nstep:2690/10000 train_loss:4.0479 train_time:455438ms step_avg:169.94ms\nstep:2691/10000 train_loss:4.3167 train_time:455610ms step_avg:169.94ms\nstep:2692/10000 train_loss:4.1873 train_time:455780ms step_avg:169.94ms\nstep:2693/10000 train_loss:4.2162 train_time:455951ms step_avg:169.94ms\nstep:2694/10000 train_loss:4.4848 train_time:456120ms step_avg:169.94ms\nstep:2695/10000 train_loss:4.1661 train_time:456290ms step_avg:169.94ms\nstep:2696/10000 train_loss:4.4414 train_time:456461ms step_avg:169.94ms\nstep:2697/10000 train_loss:4.1304 train_time:456632ms step_avg:169.94ms\nstep:2698/10000 train_loss:4.3310 train_time:456803ms step_avg:169.94ms\nstep:2699/10000 train_loss:4.1827 train_time:456974ms step_avg:169.94ms\nstep:2700/10000 train_loss:4.2641 train_time:457145ms step_avg:169.94ms\nstep:2701/10000 train_loss:4.1812 train_time:457320ms step_avg:169.94ms\nstep:2702/10000 train_loss:4.2432 train_time:457485ms step_avg:169.94ms\nstep:2703/10000 train_loss:4.1027 train_time:457652ms step_avg:169.94ms\nstep:2704/10000 train_loss:4.2929 train_time:457821ms step_avg:169.94ms\nstep:2705/10000 train_loss:4.1820 train_time:457991ms step_avg:169.94ms\nstep:2706/10000 train_loss:4.2579 train_time:458161ms step_avg:169.94ms\nstep:2707/10000 train_loss:4.2082 train_time:458333ms step_avg:169.94ms\nstep:2708/10000 train_loss:4.0764 train_time:458503ms step_avg:169.94ms\nstep:2709/10000 train_loss:4.2252 train_time:458676ms step_avg:169.94ms\nstep:2710/10000 train_loss:4.2079 train_time:458846ms step_avg:169.94ms\nstep:2711/10000 train_loss:4.2179 train_time:459014ms step_avg:169.94ms\nstep:2712/10000 train_loss:4.1765 train_time:459184ms step_avg:169.94ms\nstep:2713/10000 train_loss:4.1445 train_time:459354ms step_avg:169.94ms\nstep:2714/10000 train_loss:4.0924 train_time:459525ms step_avg:169.94ms\nstep:2715/10000 train_loss:4.2419 train_time:459693ms step_avg:169.94ms\nstep:2716/10000 train_loss:4.1290 train_time:459864ms step_avg:169.94ms\nstep:2717/10000 train_loss:4.0202 train_time:460033ms step_avg:169.94ms\nstep:2718/10000 train_loss:4.2329 train_time:460204ms step_avg:169.94ms\nstep:2719/10000 train_loss:4.2748 train_time:460373ms step_avg:169.94ms\nstep:2720/10000 train_loss:4.2806 train_time:460544ms step_avg:169.94ms\nstep:2721/10000 train_loss:4.0527 train_time:460712ms step_avg:169.94ms\nstep:2722/10000 train_loss:4.2639 train_time:460881ms step_avg:169.94ms\nstep:2723/10000 train_loss:4.1344 train_time:461050ms step_avg:169.94ms\nstep:2724/10000 train_loss:4.1866 train_time:461221ms step_avg:169.94ms\nstep:2725/10000 train_loss:4.1930 train_time:461395ms step_avg:169.94ms\nstep:2726/10000 train_loss:4.3708 train_time:461562ms step_avg:169.94ms\nstep:2727/10000 train_loss:4.2640 train_time:461733ms step_avg:169.94ms\nstep:2728/10000 train_loss:4.2130 train_time:461904ms step_avg:169.94ms\nstep:2729/10000 train_loss:4.1456 train_time:462073ms step_avg:169.94ms\nstep:2730/10000 train_loss:4.1595 train_time:462243ms step_avg:169.94ms\nstep:2731/10000 train_loss:4.1693 train_time:462412ms step_avg:169.94ms\nstep:2732/10000 train_loss:4.2927 train_time:462581ms step_avg:169.94ms\nstep:2733/10000 train_loss:4.3329 train_time:462751ms step_avg:169.94ms\nstep:2734/10000 train_loss:4.2286 train_time:462922ms step_avg:169.94ms\nstep:2735/10000 train_loss:4.1670 train_time:463090ms step_avg:169.94ms\nstep:2736/10000 train_loss:4.3451 train_time:463260ms step_avg:169.94ms\nstep:2737/10000 train_loss:4.7891 train_time:463427ms step_avg:169.94ms\nstep:2738/10000 train_loss:4.2360 train_time:463596ms step_avg:169.94ms\nstep:2739/10000 train_loss:4.0417 train_time:463768ms step_avg:169.94ms\nstep:2740/10000 train_loss:4.2314 train_time:463941ms step_avg:169.94ms\nstep:2741/10000 train_loss:4.4174 train_time:464110ms step_avg:169.94ms\nstep:2742/10000 train_loss:4.1699 train_time:464278ms step_avg:169.94ms\nstep:2743/10000 train_loss:4.2804 train_time:464448ms step_avg:169.94ms\nstep:2744/10000 train_loss:4.3175 train_time:464618ms step_avg:169.94ms\nstep:2745/10000 train_loss:4.2279 train_time:464788ms step_avg:169.94ms\nstep:2746/10000 train_loss:4.3040 train_time:464957ms step_avg:169.94ms\nstep:2747/10000 train_loss:4.1919 train_time:465127ms step_avg:169.94ms\nstep:2748/10000 train_loss:4.2467 train_time:465298ms step_avg:169.94ms\nstep:2749/10000 train_loss:4.1645 train_time:465469ms step_avg:169.94ms\nstep:2750/10000 train_loss:4.1995 train_time:465641ms step_avg:169.94ms\nstep:2751/10000 train_loss:4.3332 train_time:465811ms step_avg:169.94ms\nstep:2752/10000 train_loss:4.2717 train_time:465982ms step_avg:169.94ms\nstep:2753/10000 train_loss:4.1307 train_time:466153ms step_avg:169.94ms\nstep:2754/10000 train_loss:4.1201 train_time:466324ms step_avg:169.94ms\nstep:2755/10000 train_loss:4.1573 train_time:466494ms step_avg:169.94ms\nstep:2756/10000 train_loss:3.9883 train_time:466666ms step_avg:169.94ms\nstep:2757/10000 train_loss:4.1298 train_time:466838ms step_avg:169.94ms\nstep:2758/10000 train_loss:4.2538 train_time:467009ms step_avg:169.94ms\nstep:2759/10000 train_loss:4.3756 train_time:467178ms step_avg:169.94ms\nstep:2760/10000 train_loss:4.0461 train_time:467348ms step_avg:169.94ms\nstep:2761/10000 train_loss:4.1443 train_time:467517ms step_avg:169.94ms\nstep:2762/10000 train_loss:4.0733 train_time:467686ms step_avg:169.94ms\nstep:2763/10000 train_loss:4.1751 train_time:467854ms step_avg:169.94ms\nstep:2764/10000 train_loss:4.1646 train_time:468024ms step_avg:169.94ms\nstep:2765/10000 train_loss:4.2595 train_time:468194ms step_avg:169.94ms\nstep:2766/10000 train_loss:4.2645 train_time:468365ms step_avg:169.94ms\nstep:2767/10000 train_loss:4.1621 train_time:468534ms step_avg:169.94ms\nstep:2768/10000 train_loss:4.0684 train_time:468706ms step_avg:169.94ms\nstep:2769/10000 train_loss:4.2189 train_time:468875ms step_avg:169.94ms\nstep:2770/10000 train_loss:4.2754 train_time:469047ms step_avg:169.94ms\nstep:2771/10000 train_loss:4.1743 train_time:469215ms step_avg:169.94ms\nstep:2772/10000 train_loss:4.2741 train_time:469386ms step_avg:169.94ms\nstep:2773/10000 train_loss:4.1710 train_time:469556ms step_avg:169.94ms\nstep:2774/10000 train_loss:4.0459 train_time:469727ms step_avg:169.94ms\nstep:2775/10000 train_loss:4.1798 train_time:469896ms step_avg:169.94ms\nstep:2776/10000 train_loss:4.2753 train_time:470067ms step_avg:169.94ms\nstep:2777/10000 train_loss:4.0273 train_time:470238ms step_avg:169.94ms\nstep:2778/10000 train_loss:4.2526 train_time:470412ms step_avg:169.95ms\nstep:2779/10000 train_loss:4.2364 train_time:470583ms step_avg:169.95ms\nstep:2780/10000 train_loss:4.1495 train_time:470755ms step_avg:169.95ms\nstep:2781/10000 train_loss:4.1153 train_time:470927ms step_avg:169.95ms\nstep:2782/10000 train_loss:4.2250 train_time:471096ms step_avg:169.95ms\nstep:2783/10000 train_loss:4.2610 train_time:471264ms step_avg:169.95ms\nstep:2784/10000 train_loss:4.1373 train_time:471432ms step_avg:169.95ms\nstep:2785/10000 train_loss:4.1127 train_time:471604ms step_avg:169.95ms\nstep:2786/10000 train_loss:3.9447 train_time:471772ms step_avg:169.95ms\nstep:2787/10000 train_loss:4.1692 train_time:471944ms step_avg:169.95ms\nstep:2788/10000 train_loss:4.4380 train_time:472114ms step_avg:169.95ms\nstep:2789/10000 train_loss:4.2289 train_time:472284ms step_avg:169.95ms\nstep:2790/10000 train_loss:4.3240 train_time:472452ms step_avg:169.95ms\nstep:2791/10000 train_loss:4.2474 train_time:472624ms step_avg:169.95ms\nstep:2792/10000 train_loss:4.2255 train_time:472795ms step_avg:169.95ms\nstep:2793/10000 train_loss:4.0710 train_time:472972ms step_avg:169.95ms\nstep:2794/10000 train_loss:4.1985 train_time:473136ms step_avg:169.95ms\nstep:2795/10000 train_loss:4.1274 train_time:473306ms step_avg:169.95ms\nstep:2796/10000 train_loss:4.2140 train_time:473475ms step_avg:169.95ms\nstep:2797/10000 train_loss:4.0552 train_time:473646ms step_avg:169.95ms\nstep:2798/10000 train_loss:4.1744 train_time:473816ms step_avg:169.95ms\nstep:2799/10000 train_loss:4.1455 train_time:473989ms step_avg:169.95ms\nstep:2800/10000 train_loss:4.4036 train_time:474158ms step_avg:169.95ms\nstep:2801/10000 train_loss:4.1718 train_time:474328ms step_avg:169.95ms\nstep:2802/10000 train_loss:4.1296 train_time:474496ms step_avg:169.95ms\nstep:2803/10000 train_loss:4.4681 train_time:474665ms step_avg:169.95ms\nstep:2804/10000 train_loss:4.1663 train_time:474833ms step_avg:169.95ms\nstep:2805/10000 train_loss:4.0487 train_time:475006ms step_avg:169.95ms\nstep:2806/10000 train_loss:4.1546 train_time:475176ms step_avg:169.95ms\nstep:2807/10000 train_loss:4.0534 train_time:475351ms step_avg:169.95ms\nstep:2808/10000 train_loss:4.0928 train_time:475517ms step_avg:169.95ms\nstep:2809/10000 train_loss:4.2829 train_time:475684ms step_avg:169.95ms\nstep:2810/10000 train_loss:3.7395 train_time:475855ms step_avg:169.95ms\nstep:2811/10000 train_loss:4.2714 train_time:476026ms step_avg:169.95ms\nstep:2812/10000 train_loss:4.1172 train_time:476195ms step_avg:169.95ms\nstep:2813/10000 train_loss:4.1740 train_time:476368ms step_avg:169.95ms\nstep:2814/10000 train_loss:4.1803 train_time:476537ms step_avg:169.95ms\nstep:2815/10000 train_loss:4.1672 train_time:476707ms step_avg:169.95ms\nstep:2816/10000 train_loss:4.0491 train_time:476878ms step_avg:169.95ms\nstep:2817/10000 train_loss:4.1570 train_time:477048ms step_avg:169.95ms\nstep:2818/10000 train_loss:4.0524 train_time:477215ms step_avg:169.95ms\nstep:2819/10000 train_loss:4.1964 train_time:477384ms step_avg:169.95ms\nstep:2820/10000 train_loss:4.1484 train_time:477556ms step_avg:169.95ms\nstep:2821/10000 train_loss:4.3007 train_time:477725ms step_avg:169.95ms\nstep:2822/10000 train_loss:4.2848 train_time:477895ms step_avg:169.95ms\nstep:2823/10000 train_loss:4.2702 train_time:478067ms step_avg:169.95ms\nstep:2824/10000 train_loss:4.1699 train_time:478238ms step_avg:169.95ms\nstep:2825/10000 train_loss:4.2502 train_time:478409ms step_avg:169.95ms\nstep:2826/10000 train_loss:4.1300 train_time:478581ms step_avg:169.95ms\nstep:2827/10000 train_loss:4.1733 train_time:478752ms step_avg:169.95ms\nstep:2828/10000 train_loss:4.1826 train_time:478923ms step_avg:169.95ms\nstep:2829/10000 train_loss:4.0584 train_time:479092ms step_avg:169.95ms\nstep:2830/10000 train_loss:4.2210 train_time:479264ms step_avg:169.95ms\nstep:2831/10000 train_loss:4.1719 train_time:479431ms step_avg:169.95ms\nstep:2832/10000 train_loss:4.2393 train_time:479600ms step_avg:169.95ms\nstep:2833/10000 train_loss:4.1665 train_time:479767ms step_avg:169.95ms\nstep:2834/10000 train_loss:4.1796 train_time:479938ms step_avg:169.95ms\nstep:2835/10000 train_loss:4.5858 train_time:480109ms step_avg:169.95ms\nstep:2836/10000 train_loss:4.1767 train_time:480280ms step_avg:169.95ms\nstep:2837/10000 train_loss:4.1033 train_time:480447ms step_avg:169.95ms\nstep:2838/10000 train_loss:4.2578 train_time:480618ms step_avg:169.95ms\nstep:2839/10000 train_loss:4.2171 train_time:480788ms step_avg:169.95ms\nstep:2840/10000 train_loss:4.1498 train_time:480957ms step_avg:169.95ms\nstep:2841/10000 train_loss:4.2330 train_time:481127ms step_avg:169.95ms\nstep:2842/10000 train_loss:4.2416 train_time:481298ms step_avg:169.95ms\nstep:2843/10000 train_loss:4.1450 train_time:481466ms step_avg:169.95ms\nstep:2844/10000 train_loss:4.1584 train_time:481635ms step_avg:169.95ms\nstep:2845/10000 train_loss:4.2303 train_time:481804ms step_avg:169.95ms\nstep:2846/10000 train_loss:4.1881 train_time:481974ms step_avg:169.95ms\nstep:2847/10000 train_loss:4.3247 train_time:482145ms step_avg:169.95ms\nstep:2848/10000 train_loss:4.1940 train_time:482313ms step_avg:169.95ms\nstep:2849/10000 train_loss:4.1788 train_time:482485ms step_avg:169.95ms\nstep:2850/10000 train_loss:4.1366 train_time:482656ms step_avg:169.95ms\nstep:2851/10000 train_loss:4.2414 train_time:482828ms step_avg:169.95ms\nstep:2852/10000 train_loss:4.2291 train_time:482997ms step_avg:169.95ms\nstep:2853/10000 train_loss:4.1726 train_time:483167ms step_avg:169.95ms\nstep:2854/10000 train_loss:4.1477 train_time:483337ms step_avg:169.95ms\nstep:2855/10000 train_loss:4.1044 train_time:483509ms step_avg:169.95ms\nstep:2856/10000 train_loss:4.2940 train_time:483679ms step_avg:169.95ms\nstep:2857/10000 train_loss:4.2021 train_time:483848ms step_avg:169.95ms\nstep:2858/10000 train_loss:4.1935 train_time:484016ms step_avg:169.95ms\nstep:2859/10000 train_loss:4.3416 train_time:484186ms step_avg:169.95ms\nstep:2860/10000 train_loss:4.1559 train_time:484353ms step_avg:169.95ms\nstep:2861/10000 train_loss:4.1302 train_time:484524ms step_avg:169.95ms\nstep:2862/10000 train_loss:4.0208 train_time:484694ms step_avg:169.95ms\nstep:2863/10000 train_loss:4.0999 train_time:484864ms step_avg:169.95ms\nstep:2864/10000 train_loss:4.1696 train_time:485032ms step_avg:169.95ms\nstep:2865/10000 train_loss:4.1295 train_time:485204ms step_avg:169.95ms\nstep:2866/10000 train_loss:4.4254 train_time:485371ms step_avg:169.95ms\nstep:2867/10000 train_loss:4.3747 train_time:485543ms step_avg:169.95ms\nstep:2868/10000 train_loss:4.1873 train_time:485712ms step_avg:169.95ms\nstep:2869/10000 train_loss:3.9908 train_time:485882ms step_avg:169.95ms\nstep:2870/10000 train_loss:4.3020 train_time:486051ms step_avg:169.95ms\nstep:2871/10000 train_loss:4.1480 train_time:486222ms step_avg:169.95ms\nstep:2872/10000 train_loss:4.3256 train_time:486392ms step_avg:169.95ms\nstep:2873/10000 train_loss:4.1587 train_time:486563ms step_avg:169.95ms\nstep:2874/10000 train_loss:4.2940 train_time:486732ms step_avg:169.95ms\nstep:2875/10000 train_loss:4.1933 train_time:486901ms step_avg:169.95ms\nstep:2876/10000 train_loss:4.1840 train_time:487071ms step_avg:169.95ms\nstep:2877/10000 train_loss:4.2272 train_time:487242ms step_avg:169.95ms\nstep:2878/10000 train_loss:4.2637 train_time:487411ms step_avg:169.95ms\nstep:2879/10000 train_loss:4.2020 train_time:487581ms step_avg:169.95ms\nstep:2880/10000 train_loss:4.2322 train_time:487752ms step_avg:169.95ms\nstep:2881/10000 train_loss:4.0517 train_time:487923ms step_avg:169.95ms\nstep:2882/10000 train_loss:4.2137 train_time:488092ms step_avg:169.95ms\nstep:2883/10000 train_loss:4.0562 train_time:488263ms step_avg:169.95ms\nstep:2884/10000 train_loss:4.3443 train_time:488431ms step_avg:169.95ms\nstep:2885/10000 train_loss:4.3329 train_time:488601ms step_avg:169.95ms\nstep:2886/10000 train_loss:4.0809 train_time:488770ms step_avg:169.95ms\nstep:2887/10000 train_loss:4.1537 train_time:488943ms step_avg:169.95ms\nstep:2888/10000 train_loss:4.0389 train_time:489113ms step_avg:169.95ms\nstep:2889/10000 train_loss:4.2565 train_time:489285ms step_avg:169.95ms\nstep:2890/10000 train_loss:4.1921 train_time:489456ms step_avg:169.95ms\nstep:2891/10000 train_loss:4.3684 train_time:489626ms step_avg:169.95ms\nstep:2892/10000 train_loss:4.3168 train_time:489796ms step_avg:169.95ms\nstep:2893/10000 train_loss:4.1502 train_time:489968ms step_avg:169.95ms\nstep:2894/10000 train_loss:4.1684 train_time:490137ms step_avg:169.95ms\nstep:2895/10000 train_loss:4.3216 train_time:490311ms step_avg:169.95ms\nstep:2896/10000 train_loss:4.3569 train_time:490480ms step_avg:169.95ms\nstep:2897/10000 train_loss:4.2121 train_time:490652ms step_avg:169.95ms\nstep:2898/10000 train_loss:4.1939 train_time:490822ms step_avg:169.95ms\nstep:2899/10000 train_loss:3.9728 train_time:490992ms step_avg:169.95ms\nstep:2900/10000 train_loss:4.1499 train_time:491161ms step_avg:169.95ms\nstep:2901/10000 train_loss:4.1431 train_time:491335ms step_avg:169.95ms\nstep:2902/10000 train_loss:4.1883 train_time:491504ms step_avg:169.95ms\nstep:2903/10000 train_loss:4.2164 train_time:491673ms step_avg:169.95ms\nstep:2904/10000 train_loss:4.2286 train_time:491848ms step_avg:169.95ms\nstep:2905/10000 train_loss:4.1130 train_time:492012ms step_avg:169.95ms\nstep:2906/10000 train_loss:4.0716 train_time:492183ms step_avg:169.95ms\nstep:2907/10000 train_loss:4.0006 train_time:492354ms step_avg:169.95ms\nstep:2908/10000 train_loss:4.1446 train_time:492525ms step_avg:169.95ms\nstep:2909/10000 train_loss:4.0771 train_time:492692ms step_avg:169.95ms\nstep:2910/10000 train_loss:4.2333 train_time:492864ms step_avg:169.95ms\nstep:2911/10000 train_loss:4.2286 train_time:493035ms step_avg:169.95ms\nstep:2912/10000 train_loss:4.1064 train_time:493205ms step_avg:169.95ms\nstep:2913/10000 train_loss:4.2146 train_time:493373ms step_avg:169.95ms\nstep:2914/10000 train_loss:4.1653 train_time:493545ms step_avg:169.95ms\nstep:2915/10000 train_loss:4.1593 train_time:493712ms step_avg:169.95ms\nstep:2916/10000 train_loss:4.3014 train_time:493885ms step_avg:169.95ms\nstep:2917/10000 train_loss:4.1644 train_time:494057ms step_avg:169.95ms\nstep:2918/10000 train_loss:3.9664 train_time:494225ms step_avg:169.95ms\nstep:2919/10000 train_loss:4.0138 train_time:494393ms step_avg:169.95ms\nstep:2920/10000 train_loss:4.2919 train_time:494563ms step_avg:169.95ms\nstep:2921/10000 train_loss:4.1957 train_time:494731ms step_avg:169.95ms\nstep:2922/10000 train_loss:4.1087 train_time:494902ms step_avg:169.95ms\nstep:2923/10000 train_loss:4.2200 train_time:495070ms step_avg:169.95ms\nstep:2924/10000 train_loss:4.2950 train_time:495244ms step_avg:169.95ms\nstep:2925/10000 train_loss:4.1978 train_time:495413ms step_avg:169.95ms\nstep:2926/10000 train_loss:4.2346 train_time:495585ms step_avg:169.95ms\nstep:2927/10000 train_loss:4.2402 train_time:495753ms step_avg:169.95ms\nstep:2928/10000 train_loss:4.1407 train_time:495925ms step_avg:169.95ms\nstep:2929/10000 train_loss:4.2209 train_time:496095ms step_avg:169.95ms\nstep:2930/10000 train_loss:4.2103 train_time:496267ms step_avg:169.95ms\nstep:2931/10000 train_loss:4.2235 train_time:496434ms step_avg:169.95ms\nstep:2932/10000 train_loss:4.1817 train_time:496606ms step_avg:169.95ms\nstep:2933/10000 train_loss:4.1157 train_time:496774ms step_avg:169.95ms\nstep:2934/10000 train_loss:4.2870 train_time:496946ms step_avg:169.95ms\nstep:2935/10000 train_loss:4.3379 train_time:497113ms step_avg:169.95ms\nstep:2936/10000 train_loss:4.4098 train_time:497282ms step_avg:169.95ms\nstep:2937/10000 train_loss:4.0996 train_time:497451ms step_avg:169.95ms\nstep:2938/10000 train_loss:4.1161 train_time:497621ms step_avg:169.95ms\nstep:2939/10000 train_loss:4.2302 train_time:497789ms step_avg:169.95ms\nstep:2940/10000 train_loss:4.1298 train_time:497958ms step_avg:169.95ms\nstep:2941/10000 train_loss:4.2347 train_time:498131ms step_avg:169.95ms\nstep:2942/10000 train_loss:4.2134 train_time:498299ms step_avg:169.95ms\nstep:2943/10000 train_loss:4.1617 train_time:498468ms step_avg:169.95ms\nstep:2944/10000 train_loss:4.2309 train_time:498639ms step_avg:169.95ms\nstep:2945/10000 train_loss:4.2290 train_time:498812ms step_avg:169.95ms\nstep:2946/10000 train_loss:4.2042 train_time:498982ms step_avg:169.95ms\nstep:2947/10000 train_loss:4.1570 train_time:499152ms step_avg:169.95ms\nstep:2948/10000 train_loss:4.3319 train_time:499321ms step_avg:169.95ms\nstep:2949/10000 train_loss:4.0581 train_time:499492ms step_avg:169.95ms\nstep:2950/10000 train_loss:4.2620 train_time:499666ms step_avg:169.95ms\nstep:2951/10000 train_loss:4.0575 train_time:499834ms step_avg:169.95ms\nstep:2952/10000 train_loss:4.2207 train_time:500004ms step_avg:169.95ms\nstep:2953/10000 train_loss:4.1685 train_time:500173ms step_avg:169.95ms\nstep:2954/10000 train_loss:4.1863 train_time:500346ms step_avg:169.95ms\nstep:2955/10000 train_loss:4.2494 train_time:500512ms step_avg:169.95ms\nstep:2956/10000 train_loss:4.1897 train_time:500682ms step_avg:169.95ms\nstep:2957/10000 train_loss:4.1224 train_time:500854ms step_avg:169.95ms\nstep:2958/10000 train_loss:4.2016 train_time:501026ms step_avg:169.95ms\nstep:2959/10000 train_loss:4.1327 train_time:501197ms step_avg:169.96ms\nstep:2960/10000 train_loss:4.0803 train_time:501368ms step_avg:169.96ms\nstep:2961/10000 train_loss:4.1919 train_time:501538ms step_avg:169.96ms\nstep:2962/10000 train_loss:4.2261 train_time:501708ms step_avg:169.96ms\nstep:2963/10000 train_loss:4.0959 train_time:501877ms step_avg:169.95ms\nstep:2964/10000 train_loss:4.7103 train_time:502047ms step_avg:169.95ms\nstep:2965/10000 train_loss:4.2479 train_time:502217ms step_avg:169.96ms\nstep:2966/10000 train_loss:4.1987 train_time:502389ms step_avg:169.96ms\nstep:2967/10000 train_loss:4.2136 train_time:502558ms step_avg:169.96ms\nstep:2968/10000 train_loss:4.0968 train_time:502729ms step_avg:169.96ms\nstep:2969/10000 train_loss:4.2164 train_time:502899ms step_avg:169.96ms\nstep:2970/10000 train_loss:4.2229 train_time:503067ms step_avg:169.96ms\nstep:2971/10000 train_loss:4.2543 train_time:503238ms step_avg:169.96ms\nstep:2972/10000 train_loss:4.1858 train_time:503410ms step_avg:169.96ms\nstep:2973/10000 train_loss:4.3536 train_time:503576ms step_avg:169.95ms\nstep:2974/10000 train_loss:4.1000 train_time:503747ms step_avg:169.96ms\nstep:2975/10000 train_loss:4.1782 train_time:503917ms step_avg:169.96ms\nstep:2976/10000 train_loss:4.1493 train_time:504089ms step_avg:169.96ms\nstep:2977/10000 train_loss:4.2256 train_time:504260ms step_avg:169.96ms\nstep:2978/10000 train_loss:4.3426 train_time:504431ms step_avg:169.96ms\nstep:2979/10000 train_loss:4.1353 train_time:504600ms step_avg:169.96ms\nstep:2980/10000 train_loss:4.2062 train_time:504768ms step_avg:169.96ms\nstep:2981/10000 train_loss:4.0939 train_time:504938ms step_avg:169.96ms\nstep:2982/10000 train_loss:4.1253 train_time:505108ms step_avg:169.96ms\nstep:2983/10000 train_loss:3.9333 train_time:505281ms step_avg:169.96ms\nstep:2984/10000 train_loss:4.2412 train_time:505453ms step_avg:169.96ms\nstep:2985/10000 train_loss:4.1432 train_time:505623ms step_avg:169.96ms\nstep:2986/10000 train_loss:4.1578 train_time:505791ms step_avg:169.96ms\nstep:2987/10000 train_loss:4.0600 train_time:505961ms step_avg:169.96ms\nstep:2988/10000 train_loss:4.1823 train_time:506128ms step_avg:169.96ms\nstep:2989/10000 train_loss:4.2119 train_time:506301ms step_avg:169.96ms\nstep:2990/10000 train_loss:4.2770 train_time:506472ms step_avg:169.96ms\nstep:2991/10000 train_loss:4.1194 train_time:506640ms step_avg:169.96ms\nstep:2992/10000 train_loss:4.2060 train_time:506808ms step_avg:169.96ms\nstep:2993/10000 train_loss:4.1952 train_time:506977ms step_avg:169.96ms\nstep:2994/10000 train_loss:4.1447 train_time:507148ms step_avg:169.96ms\nstep:2995/10000 train_loss:4.1715 train_time:507317ms step_avg:169.96ms\nstep:2996/10000 train_loss:4.1242 train_time:507487ms step_avg:169.96ms\nstep:2997/10000 train_loss:4.2619 train_time:507657ms step_avg:169.96ms\nstep:2998/10000 train_loss:4.2248 train_time:507829ms step_avg:169.96ms\nstep:2999/10000 train_loss:4.3066 train_time:507998ms step_avg:169.96ms\nstep:3000/10000 train_loss:4.1474 train_time:508170ms step_avg:169.96ms\nstep:3001/10000 train_loss:4.0627 train_time:508341ms step_avg:169.96ms\nstep:3002/10000 train_loss:4.1077 train_time:508512ms step_avg:169.96ms\nstep:3003/10000 train_loss:4.2185 train_time:508683ms step_avg:169.96ms\nstep:3004/10000 train_loss:4.1890 train_time:508851ms step_avg:169.96ms\nstep:3005/10000 train_loss:4.1356 train_time:509020ms step_avg:169.96ms\nstep:3006/10000 train_loss:4.2351 train_time:509195ms step_avg:169.96ms\nstep:3007/10000 train_loss:4.0765 train_time:509358ms step_avg:169.96ms\nstep:3008/10000 train_loss:4.1483 train_time:509528ms step_avg:169.96ms\nstep:3009/10000 train_loss:4.2429 train_time:509698ms step_avg:169.96ms\nstep:3010/10000 train_loss:4.0794 train_time:509868ms step_avg:169.96ms\nstep:3011/10000 train_loss:4.1099 train_time:510035ms step_avg:169.96ms\nstep:3012/10000 train_loss:4.1583 train_time:510208ms step_avg:169.96ms\nstep:3013/10000 train_loss:4.1382 train_time:510378ms step_avg:169.96ms\nstep:3014/10000 train_loss:4.1398 train_time:510550ms step_avg:169.96ms\nstep:3015/10000 train_loss:4.1974 train_time:510722ms step_avg:169.96ms\nstep:3016/10000 train_loss:4.2029 train_time:510893ms step_avg:169.96ms\nstep:3017/10000 train_loss:4.1146 train_time:511063ms step_avg:169.96ms\nstep:3018/10000 train_loss:4.1072 train_time:511233ms step_avg:169.96ms\nstep:3019/10000 train_loss:4.2789 train_time:511405ms step_avg:169.96ms\nstep:3020/10000 train_loss:4.1508 train_time:511574ms step_avg:169.96ms\nstep:3021/10000 train_loss:4.3203 train_time:511745ms step_avg:169.96ms\nstep:3022/10000 train_loss:4.1604 train_time:511915ms step_avg:169.96ms\nstep:3023/10000 train_loss:4.0123 train_time:512085ms step_avg:169.96ms\nstep:3024/10000 train_loss:4.2403 train_time:512254ms step_avg:169.96ms\nstep:3025/10000 train_loss:4.2177 train_time:512426ms step_avg:169.96ms\nstep:3026/10000 train_loss:4.0670 train_time:512596ms step_avg:169.96ms\nstep:3027/10000 train_loss:4.2445 train_time:512778ms step_avg:169.96ms\nstep:3028/10000 train_loss:4.0872 train_time:512940ms step_avg:169.96ms\nstep:3029/10000 train_loss:4.0685 train_time:513109ms step_avg:169.96ms\nstep:3030/10000 train_loss:4.3351 train_time:513278ms step_avg:169.96ms\nstep:3031/10000 train_loss:4.2876 train_time:513446ms step_avg:169.96ms\nstep:3032/10000 train_loss:4.2846 train_time:513614ms step_avg:169.96ms\nstep:3033/10000 train_loss:4.1059 train_time:513787ms step_avg:169.96ms\nstep:3034/10000 train_loss:4.4191 train_time:513956ms step_avg:169.96ms\nstep:3035/10000 train_loss:3.8838 train_time:514126ms step_avg:169.96ms\nstep:3036/10000 train_loss:4.2128 train_time:514294ms step_avg:169.96ms\nstep:3037/10000 train_loss:4.1619 train_time:514467ms step_avg:169.96ms\nstep:3038/10000 train_loss:4.1612 train_time:514634ms step_avg:169.96ms\nstep:3039/10000 train_loss:4.1249 train_time:514808ms step_avg:169.96ms\nstep:3040/10000 train_loss:4.1310 train_time:514979ms step_avg:169.96ms\nstep:3041/10000 train_loss:4.1601 train_time:515152ms step_avg:169.96ms\nstep:3042/10000 train_loss:4.1031 train_time:515324ms step_avg:169.96ms\nstep:3043/10000 train_loss:4.2015 train_time:515494ms step_avg:169.96ms\nstep:3044/10000 train_loss:4.2359 train_time:515664ms step_avg:169.96ms\nstep:3045/10000 train_loss:4.1337 train_time:515833ms step_avg:169.96ms\nstep:3046/10000 train_loss:4.2790 train_time:516004ms step_avg:169.96ms\nstep:3047/10000 train_loss:4.2118 train_time:516175ms step_avg:169.96ms\nstep:3048/10000 train_loss:4.1833 train_time:516561ms step_avg:170.03ms\nstep:3049/10000 train_loss:4.1665 train_time:516724ms step_avg:170.03ms\nstep:3050/10000 train_loss:3.9856 train_time:516892ms step_avg:170.03ms\nstep:3051/10000 train_loss:4.1281 train_time:517059ms step_avg:170.03ms\nstep:3052/10000 train_loss:4.0703 train_time:517228ms step_avg:170.03ms\nstep:3053/10000 train_loss:4.0943 train_time:517395ms step_avg:170.03ms\nstep:3054/10000 train_loss:4.1498 train_time:517574ms step_avg:170.03ms\nstep:3055/10000 train_loss:4.2564 train_time:517746ms step_avg:170.03ms\nstep:3056/10000 train_loss:3.9682 train_time:517914ms step_avg:170.03ms\nstep:3057/10000 train_loss:4.1487 train_time:518081ms step_avg:170.03ms\nstep:3058/10000 train_loss:4.2497 train_time:518248ms step_avg:170.03ms\nstep:3059/10000 train_loss:4.1375 train_time:518419ms step_avg:170.03ms\nstep:3060/10000 train_loss:4.1372 train_time:518593ms step_avg:170.03ms\nstep:3061/10000 train_loss:4.1983 train_time:518763ms step_avg:170.03ms\nstep:3062/10000 train_loss:3.9143 train_time:518932ms step_avg:170.03ms\nstep:3063/10000 train_loss:4.1741 train_time:519106ms step_avg:170.03ms\nstep:3064/10000 train_loss:4.1427 train_time:519269ms step_avg:170.03ms\nstep:3065/10000 train_loss:4.1236 train_time:519443ms step_avg:170.03ms\nstep:3066/10000 train_loss:4.3161 train_time:519614ms step_avg:170.03ms\nstep:3067/10000 train_loss:4.1501 train_time:519785ms step_avg:170.03ms\nstep:3068/10000 train_loss:5.1184 train_time:519953ms step_avg:170.03ms\nstep:3069/10000 train_loss:4.0961 train_time:520122ms step_avg:170.03ms\nstep:3070/10000 train_loss:4.2181 train_time:520292ms step_avg:170.03ms\nstep:3071/10000 train_loss:4.1706 train_time:520464ms step_avg:170.03ms\nstep:3072/10000 train_loss:4.1079 train_time:520636ms step_avg:170.03ms\nstep:3073/10000 train_loss:4.1612 train_time:520810ms step_avg:170.03ms\nstep:3074/10000 train_loss:4.1358 train_time:520981ms step_avg:170.03ms\nstep:3075/10000 train_loss:4.3629 train_time:521151ms step_avg:170.03ms\nstep:3076/10000 train_loss:4.1989 train_time:521322ms step_avg:170.03ms\nstep:3077/10000 train_loss:4.2008 train_time:521491ms step_avg:170.03ms\nstep:3078/10000 train_loss:4.1552 train_time:521662ms step_avg:170.03ms\nstep:3079/10000 train_loss:4.2236 train_time:521830ms step_avg:170.03ms\nstep:3080/10000 train_loss:4.2102 train_time:522000ms step_avg:170.03ms\nstep:3081/10000 train_loss:4.0786 train_time:522169ms step_avg:170.03ms\nstep:3082/10000 train_loss:4.1852 train_time:522341ms step_avg:170.03ms\nstep:3083/10000 train_loss:4.1537 train_time:522515ms step_avg:170.03ms\nstep:3084/10000 train_loss:4.7357 train_time:522685ms step_avg:170.03ms\nstep:3085/10000 train_loss:4.0300 train_time:522853ms step_avg:170.03ms\nstep:3086/10000 train_loss:4.2544 train_time:523024ms step_avg:170.03ms\nstep:3087/10000 train_loss:3.9526 train_time:523191ms step_avg:170.03ms\nstep:3088/10000 train_loss:4.1917 train_time:523362ms step_avg:170.03ms\nstep:3089/10000 train_loss:4.2738 train_time:523531ms step_avg:170.03ms\nstep:3090/10000 train_loss:4.4542 train_time:523701ms step_avg:170.03ms\nstep:3091/10000 train_loss:4.1834 train_time:523871ms step_avg:170.03ms\nstep:3092/10000 train_loss:4.1544 train_time:524043ms step_avg:170.03ms\nstep:3093/10000 train_loss:4.1874 train_time:524212ms step_avg:170.03ms\nstep:3094/10000 train_loss:4.0200 train_time:524382ms step_avg:170.03ms\nstep:3095/10000 train_loss:4.0385 train_time:524553ms step_avg:170.03ms\nstep:3096/10000 train_loss:4.1964 train_time:524724ms step_avg:170.03ms\nstep:3097/10000 train_loss:4.1522 train_time:524893ms step_avg:170.03ms\nstep:3098/10000 train_loss:4.2575 train_time:525064ms step_avg:170.03ms\nstep:3099/10000 train_loss:4.1016 train_time:525234ms step_avg:170.03ms\nstep:3100/10000 train_loss:4.3032 train_time:525406ms step_avg:170.03ms\nstep:3101/10000 train_loss:4.0129 train_time:525576ms step_avg:170.03ms\nstep:3102/10000 train_loss:4.2037 train_time:525750ms step_avg:170.04ms\nstep:3103/10000 train_loss:4.4940 train_time:525923ms step_avg:170.04ms\nstep:3104/10000 train_loss:4.1154 train_time:526093ms step_avg:170.04ms\nstep:3105/10000 train_loss:3.9536 train_time:526263ms step_avg:170.04ms\nstep:3106/10000 train_loss:4.1646 train_time:526431ms step_avg:170.04ms\nstep:3107/10000 train_loss:4.0091 train_time:526602ms step_avg:170.04ms\nstep:3108/10000 train_loss:4.1881 train_time:526768ms step_avg:170.03ms\nstep:3109/10000 train_loss:4.1756 train_time:526942ms step_avg:170.04ms\nstep:3110/10000 train_loss:4.0229 train_time:527115ms step_avg:170.04ms\nstep:3111/10000 train_loss:4.1354 train_time:527287ms step_avg:170.04ms\nstep:3112/10000 train_loss:4.2012 train_time:527455ms step_avg:170.04ms\nstep:3113/10000 train_loss:4.1028 train_time:527623ms step_avg:170.04ms\nstep:3114/10000 train_loss:3.9892 train_time:527795ms step_avg:170.04ms\nstep:3115/10000 train_loss:4.2248 train_time:527967ms step_avg:170.04ms\nstep:3116/10000 train_loss:4.1593 train_time:528137ms step_avg:170.04ms\nstep:3117/10000 train_loss:4.0288 train_time:528309ms step_avg:170.04ms\nstep:3118/10000 train_loss:4.0489 train_time:528480ms step_avg:170.04ms\nstep:3119/10000 train_loss:4.2141 train_time:528648ms step_avg:170.04ms\nstep:3120/10000 train_loss:4.1356 train_time:528818ms step_avg:170.04ms\nstep:3121/10000 train_loss:4.2403 train_time:528988ms step_avg:170.04ms\nstep:3122/10000 train_loss:4.1124 train_time:529157ms step_avg:170.04ms\nstep:3123/10000 train_loss:4.1370 train_time:529330ms step_avg:170.04ms\nstep:3124/10000 train_loss:3.9991 train_time:529500ms step_avg:170.04ms\nstep:3125/10000 train_loss:4.1778 train_time:529669ms step_avg:170.04ms\nstep:3126/10000 train_loss:4.1674 train_time:529839ms step_avg:170.04ms\nstep:3127/10000 train_loss:4.1219 train_time:530008ms step_avg:170.04ms\nstep:3128/10000 train_loss:4.1449 train_time:530178ms step_avg:170.04ms\nstep:3129/10000 train_loss:4.1584 train_time:530349ms step_avg:170.04ms\nstep:3130/10000 train_loss:4.2296 train_time:530521ms step_avg:170.04ms\nstep:3131/10000 train_loss:3.9605 train_time:530689ms step_avg:170.04ms\nstep:3132/10000 train_loss:4.1488 train_time:530860ms step_avg:170.04ms\nstep:3133/10000 train_loss:4.0844 train_time:531027ms step_avg:170.04ms\nstep:3134/10000 train_loss:4.0952 train_time:531199ms step_avg:170.04ms\nstep:3135/10000 train_loss:4.0909 train_time:531372ms step_avg:170.04ms\nstep:3136/10000 train_loss:4.1245 train_time:531541ms step_avg:170.04ms\nstep:3137/10000 train_loss:4.2066 train_time:531712ms step_avg:170.04ms\nstep:3138/10000 train_loss:4.1618 train_time:531883ms step_avg:170.04ms\nstep:3139/10000 train_loss:4.0930 train_time:532053ms step_avg:170.04ms\nstep:3140/10000 train_loss:4.0887 train_time:532229ms step_avg:170.04ms\nstep:3141/10000 train_loss:4.0613 train_time:532398ms step_avg:170.04ms\nstep:3142/10000 train_loss:4.1038 train_time:532567ms step_avg:170.04ms\nstep:3143/10000 train_loss:4.0279 train_time:532729ms step_avg:170.04ms\nstep:3144/10000 train_loss:4.1980 train_time:532899ms step_avg:170.04ms\nstep:3145/10000 train_loss:4.1583 train_time:533067ms step_avg:170.04ms\nstep:3146/10000 train_loss:4.1281 train_time:533238ms step_avg:170.04ms\nstep:3147/10000 train_loss:4.1362 train_time:533411ms step_avg:170.04ms\nstep:3148/10000 train_loss:4.0765 train_time:533582ms step_avg:170.04ms\nstep:3149/10000 train_loss:3.9560 train_time:533752ms step_avg:170.04ms\nstep:3150/10000 train_loss:4.0267 train_time:533924ms step_avg:170.04ms\nstep:3151/10000 train_loss:4.1929 train_time:534091ms step_avg:170.04ms\nstep:3152/10000 train_loss:4.1475 train_time:534260ms step_avg:170.04ms\nstep:3153/10000 train_loss:4.2401 train_time:534430ms step_avg:170.04ms\nstep:3154/10000 train_loss:4.2969 train_time:534602ms step_avg:170.04ms\nstep:3155/10000 train_loss:4.1031 train_time:534774ms step_avg:170.04ms\nstep:3156/10000 train_loss:4.1441 train_time:534944ms step_avg:170.04ms\nstep:3157/10000 train_loss:4.1662 train_time:535114ms step_avg:170.04ms\nstep:3158/10000 train_loss:4.0760 train_time:535284ms step_avg:170.04ms\nstep:3159/10000 train_loss:4.0658 train_time:535455ms step_avg:170.04ms\nstep:3160/10000 train_loss:4.0974 train_time:535625ms step_avg:170.04ms\nstep:3161/10000 train_loss:4.1165 train_time:535796ms step_avg:170.04ms\nstep:3162/10000 train_loss:4.1465 train_time:535968ms step_avg:170.04ms\nstep:3163/10000 train_loss:4.1334 train_time:536137ms step_avg:170.04ms\nstep:3164/10000 train_loss:4.1153 train_time:536305ms step_avg:170.04ms\nstep:3165/10000 train_loss:4.5076 train_time:536473ms step_avg:170.04ms\nstep:3166/10000 train_loss:4.1907 train_time:536644ms step_avg:170.04ms\nstep:3167/10000 train_loss:4.0517 train_time:536813ms step_avg:170.04ms\nstep:3168/10000 train_loss:4.1764 train_time:536984ms step_avg:170.04ms\nstep:3169/10000 train_loss:4.1855 train_time:537154ms step_avg:170.04ms\nstep:3170/10000 train_loss:4.3103 train_time:537326ms step_avg:170.04ms\nstep:3171/10000 train_loss:4.3391 train_time:537496ms step_avg:170.04ms\nstep:3172/10000 train_loss:4.1938 train_time:537666ms step_avg:170.04ms\nstep:3173/10000 train_loss:4.0746 train_time:537836ms step_avg:170.04ms\nstep:3174/10000 train_loss:4.3324 train_time:538007ms step_avg:170.04ms\nstep:3175/10000 train_loss:4.3142 train_time:538176ms step_avg:170.04ms\nstep:3176/10000 train_loss:4.1797 train_time:538347ms step_avg:170.04ms\nstep:3177/10000 train_loss:4.2489 train_time:538516ms step_avg:170.04ms\nstep:3178/10000 train_loss:4.0943 train_time:538688ms step_avg:170.04ms\nstep:3179/10000 train_loss:4.0229 train_time:538858ms step_avg:170.04ms\nstep:3180/10000 train_loss:4.1199 train_time:539032ms step_avg:170.04ms\nstep:3181/10000 train_loss:4.1542 train_time:539201ms step_avg:170.04ms\nstep:3182/10000 train_loss:4.1623 train_time:539368ms step_avg:170.04ms\nstep:3183/10000 train_loss:4.1093 train_time:539538ms step_avg:170.04ms\nstep:3184/10000 train_loss:4.2051 train_time:539706ms step_avg:170.04ms\nstep:3185/10000 train_loss:5.1851 train_time:539876ms step_avg:170.04ms\nstep:3186/10000 train_loss:4.2362 train_time:540047ms step_avg:170.04ms\nstep:3187/10000 train_loss:4.1359 train_time:540217ms step_avg:170.04ms\nstep:3188/10000 train_loss:4.1818 train_time:540393ms step_avg:170.04ms\nstep:3189/10000 train_loss:4.1261 train_time:540556ms step_avg:170.04ms\nstep:3190/10000 train_loss:4.3436 train_time:540727ms step_avg:170.04ms\nstep:3191/10000 train_loss:4.5343 train_time:540895ms step_avg:170.04ms\nstep:3192/10000 train_loss:3.9678 train_time:541068ms step_avg:170.04ms\nstep:3193/10000 train_loss:4.1226 train_time:541235ms step_avg:170.04ms\nstep:3194/10000 train_loss:4.0915 train_time:541405ms step_avg:170.04ms\nstep:3195/10000 train_loss:4.2701 train_time:541573ms step_avg:170.04ms\nstep:3196/10000 train_loss:4.0390 train_time:541743ms step_avg:170.04ms\nstep:3197/10000 train_loss:4.1336 train_time:541914ms step_avg:170.04ms\nstep:3198/10000 train_loss:4.2302 train_time:542086ms step_avg:170.04ms\nstep:3199/10000 train_loss:4.2695 train_time:542255ms step_avg:170.04ms\nstep:3200/10000 train_loss:4.7347 train_time:542429ms step_avg:170.04ms\nstep:3201/10000 train_loss:4.0760 train_time:542599ms step_avg:170.04ms\nstep:3202/10000 train_loss:4.1413 train_time:542768ms step_avg:170.04ms\nstep:3203/10000 train_loss:4.2484 train_time:542937ms step_avg:170.04ms\nstep:3204/10000 train_loss:4.2673 train_time:543110ms step_avg:170.04ms\nstep:3205/10000 train_loss:4.1457 train_time:543281ms step_avg:170.04ms\nstep:3206/10000 train_loss:4.1413 train_time:543451ms step_avg:170.04ms\nstep:3207/10000 train_loss:4.0583 train_time:543621ms step_avg:170.04ms\nstep:3208/10000 train_loss:4.1470 train_time:543790ms step_avg:170.04ms\nstep:3209/10000 train_loss:4.2143 train_time:543958ms step_avg:170.04ms\nstep:3210/10000 train_loss:4.2382 train_time:544129ms step_avg:170.04ms\nstep:3211/10000 train_loss:4.3617 train_time:544297ms step_avg:170.04ms\nstep:3212/10000 train_loss:4.1836 train_time:544466ms step_avg:170.04ms\nstep:3213/10000 train_loss:4.3171 train_time:544636ms step_avg:170.04ms\nstep:3214/10000 train_loss:4.0726 train_time:544809ms step_avg:170.04ms\nstep:3215/10000 train_loss:4.1534 train_time:544979ms step_avg:170.04ms\nstep:3216/10000 train_loss:4.1647 train_time:545150ms step_avg:170.04ms\nstep:3217/10000 train_loss:4.0897 train_time:545319ms step_avg:170.04ms\nstep:3218/10000 train_loss:4.1272 train_time:545485ms step_avg:170.04ms\nstep:3219/10000 train_loss:4.1220 train_time:545656ms step_avg:170.04ms\nstep:3220/10000 train_loss:4.0888 train_time:545827ms step_avg:170.04ms\nstep:3221/10000 train_loss:4.0875 train_time:545997ms step_avg:170.04ms\nstep:3222/10000 train_loss:4.2116 train_time:546170ms step_avg:170.04ms\nstep:3223/10000 train_loss:4.1057 train_time:546337ms step_avg:170.04ms\nstep:3224/10000 train_loss:4.0189 train_time:546508ms step_avg:170.04ms\nstep:3225/10000 train_loss:4.0983 train_time:546680ms step_avg:170.04ms\nstep:3226/10000 train_loss:4.1077 train_time:546852ms step_avg:170.04ms\nstep:3227/10000 train_loss:4.2407 train_time:547025ms step_avg:170.04ms\nstep:3228/10000 train_loss:4.1347 train_time:547196ms step_avg:170.04ms\nstep:3229/10000 train_loss:4.2106 train_time:547366ms step_avg:170.04ms\nstep:3230/10000 train_loss:4.0957 train_time:547535ms step_avg:170.04ms\nstep:3231/10000 train_loss:4.0439 train_time:547703ms step_avg:170.04ms\nstep:3232/10000 train_loss:4.1770 train_time:547872ms step_avg:170.04ms\nstep:3233/10000 train_loss:4.2310 train_time:548044ms step_avg:170.04ms\nstep:3234/10000 train_loss:4.2226 train_time:548212ms step_avg:170.04ms\nstep:3235/10000 train_loss:4.0327 train_time:548385ms step_avg:170.04ms\nstep:3236/10000 train_loss:4.3975 train_time:548552ms step_avg:170.04ms\nstep:3237/10000 train_loss:4.1266 train_time:548721ms step_avg:170.04ms\nstep:3238/10000 train_loss:4.1344 train_time:548891ms step_avg:170.04ms\nstep:3239/10000 train_loss:4.1623 train_time:549064ms step_avg:170.04ms\nstep:3240/10000 train_loss:4.2407 train_time:549240ms step_avg:170.04ms\nstep:3241/10000 train_loss:4.2164 train_time:549404ms step_avg:170.04ms\nstep:3242/10000 train_loss:4.0355 train_time:549572ms step_avg:170.04ms\nstep:3243/10000 train_loss:4.5801 train_time:549742ms step_avg:170.04ms\nstep:3244/10000 train_loss:4.1017 train_time:549911ms step_avg:170.04ms\nstep:3245/10000 train_loss:4.1951 train_time:550084ms step_avg:170.04ms\nstep:3246/10000 train_loss:4.1343 train_time:550254ms step_avg:170.04ms\nstep:3247/10000 train_loss:4.0721 train_time:550427ms step_avg:170.04ms\nstep:3248/10000 train_loss:4.1680 train_time:550592ms step_avg:170.04ms\nstep:3249/10000 train_loss:4.1760 train_time:550763ms step_avg:170.04ms\nstep:3250/10000 train_loss:4.1811 train_time:550931ms step_avg:170.04ms\nstep:3251/10000 train_loss:4.1768 train_time:551102ms step_avg:170.04ms\nstep:3252/10000 train_loss:4.1446 train_time:551271ms step_avg:170.04ms\nstep:3253/10000 train_loss:4.3475 train_time:551443ms step_avg:170.04ms\nstep:3254/10000 train_loss:4.1407 train_time:551611ms step_avg:170.04ms\nstep:3255/10000 train_loss:4.1953 train_time:551781ms step_avg:170.04ms\nstep:3256/10000 train_loss:4.1649 train_time:551949ms step_avg:170.04ms\nstep:3257/10000 train_loss:4.1271 train_time:552120ms step_avg:170.04ms\nstep:3258/10000 train_loss:4.2040 train_time:552287ms step_avg:170.04ms\nstep:3259/10000 train_loss:4.1324 train_time:552456ms step_avg:170.04ms\nstep:3260/10000 train_loss:3.9956 train_time:552627ms step_avg:170.04ms\nstep:3261/10000 train_loss:4.3617 train_time:552797ms step_avg:170.04ms\nstep:3262/10000 train_loss:4.1556 train_time:552968ms step_avg:170.04ms\nstep:3263/10000 train_loss:4.3411 train_time:553137ms step_avg:170.04ms\nstep:3264/10000 train_loss:4.3378 train_time:553307ms step_avg:170.04ms\nstep:3265/10000 train_loss:4.1056 train_time:553476ms step_avg:170.04ms\nstep:3266/10000 train_loss:4.1315 train_time:553648ms step_avg:170.04ms\nstep:3267/10000 train_loss:4.0850 train_time:553816ms step_avg:170.04ms\nstep:3268/10000 train_loss:4.0987 train_time:553987ms step_avg:170.04ms\nstep:3269/10000 train_loss:4.1998 train_time:554155ms step_avg:170.04ms\nstep:3270/10000 train_loss:4.1869 train_time:554325ms step_avg:170.04ms\nstep:3271/10000 train_loss:4.0804 train_time:554493ms step_avg:170.04ms\nstep:3272/10000 train_loss:4.0857 train_time:554663ms step_avg:170.04ms\nstep:3273/10000 train_loss:4.0831 train_time:554831ms step_avg:170.04ms\nstep:3274/10000 train_loss:4.2898 train_time:555002ms step_avg:170.04ms\nstep:3275/10000 train_loss:4.0834 train_time:555172ms step_avg:170.04ms\nstep:3276/10000 train_loss:4.0820 train_time:555343ms step_avg:170.04ms\nstep:3277/10000 train_loss:4.1411 train_time:555513ms step_avg:170.04ms\nstep:3278/10000 train_loss:4.1131 train_time:555681ms step_avg:170.04ms\nstep:3279/10000 train_loss:4.1098 train_time:555851ms step_avg:170.04ms\nstep:3280/10000 train_loss:4.1377 train_time:556021ms step_avg:170.04ms\nstep:3281/10000 train_loss:4.0758 train_time:556190ms step_avg:170.04ms\nstep:3282/10000 train_loss:4.2973 train_time:556360ms step_avg:170.04ms\nstep:3283/10000 train_loss:4.1449 train_time:556530ms step_avg:170.04ms\nstep:3284/10000 train_loss:3.9580 train_time:556700ms step_avg:170.04ms\nstep:3285/10000 train_loss:4.0296 train_time:556874ms step_avg:170.04ms\nstep:3286/10000 train_loss:4.0770 train_time:557046ms step_avg:170.04ms\nstep:3287/10000 train_loss:4.1678 train_time:557212ms step_avg:170.04ms\nstep:3288/10000 train_loss:4.1139 train_time:557383ms step_avg:170.04ms\nstep:3289/10000 train_loss:4.0476 train_time:557554ms step_avg:170.04ms\nstep:3290/10000 train_loss:4.0970 train_time:557726ms step_avg:170.04ms\nstep:3291/10000 train_loss:4.1274 train_time:557896ms step_avg:170.04ms\nstep:3292/10000 train_loss:4.2117 train_time:558067ms step_avg:170.04ms\nstep:3293/10000 train_loss:4.0257 train_time:558237ms step_avg:170.04ms\nstep:3294/10000 train_loss:4.1888 train_time:558407ms step_avg:170.04ms\nstep:3295/10000 train_loss:4.1057 train_time:558575ms step_avg:170.04ms\nstep:3296/10000 train_loss:3.9229 train_time:558745ms step_avg:170.04ms\nstep:3297/10000 train_loss:4.0664 train_time:558916ms step_avg:170.04ms\nstep:3298/10000 train_loss:4.1384 train_time:559088ms step_avg:170.04ms\nstep:3299/10000 train_loss:4.0896 train_time:559256ms step_avg:170.04ms\nstep:3300/10000 train_loss:4.1333 train_time:559426ms step_avg:170.04ms\nstep:3301/10000 train_loss:4.0703 train_time:559595ms step_avg:170.04ms\nstep:3302/10000 train_loss:4.0650 train_time:559766ms step_avg:170.04ms\nstep:3303/10000 train_loss:4.1424 train_time:559937ms step_avg:170.04ms\nstep:3304/10000 train_loss:4.2670 train_time:560107ms step_avg:170.04ms\nstep:3305/10000 train_loss:4.1029 train_time:560276ms step_avg:170.04ms\nstep:3306/10000 train_loss:4.1524 train_time:560448ms step_avg:170.04ms\nstep:3307/10000 train_loss:4.0922 train_time:560615ms step_avg:170.04ms\nstep:3308/10000 train_loss:4.0316 train_time:560785ms step_avg:170.04ms\nstep:3309/10000 train_loss:3.9556 train_time:560954ms step_avg:170.04ms\nstep:3310/10000 train_loss:4.0769 train_time:561125ms step_avg:170.04ms\nstep:3311/10000 train_loss:4.1901 train_time:561293ms step_avg:170.04ms\nstep:3312/10000 train_loss:4.0840 train_time:561463ms step_avg:170.04ms\nstep:3313/10000 train_loss:4.0828 train_time:561632ms step_avg:170.04ms\nstep:3314/10000 train_loss:4.2852 train_time:561802ms step_avg:170.04ms\nstep:3315/10000 train_loss:4.0849 train_time:561970ms step_avg:170.04ms\nstep:3316/10000 train_loss:4.1098 train_time:562143ms step_avg:170.04ms\nstep:3317/10000 train_loss:4.0892 train_time:562314ms step_avg:170.04ms\nstep:3318/10000 train_loss:4.1767 train_time:562486ms step_avg:170.04ms\nstep:3319/10000 train_loss:4.1288 train_time:562656ms step_avg:170.04ms\nstep:3320/10000 train_loss:4.1742 train_time:562827ms step_avg:170.04ms\nstep:3321/10000 train_loss:4.0264 train_time:562997ms step_avg:170.04ms\nstep:3322/10000 train_loss:4.2732 train_time:563167ms step_avg:170.04ms\nstep:3323/10000 train_loss:4.0859 train_time:563336ms step_avg:170.04ms\nstep:3324/10000 train_loss:3.9936 train_time:563509ms step_avg:170.04ms\nstep:3325/10000 train_loss:4.1455 train_time:563678ms step_avg:170.04ms\nstep:3326/10000 train_loss:4.0768 train_time:563847ms step_avg:170.04ms\nstep:3327/10000 train_loss:4.1692 train_time:564017ms step_avg:170.04ms\nstep:3328/10000 train_loss:4.1912 train_time:564195ms step_avg:170.04ms\nstep:3329/10000 train_loss:4.1171 train_time:564360ms step_avg:170.04ms\nstep:3330/10000 train_loss:4.1378 train_time:564531ms step_avg:170.04ms\nstep:3331/10000 train_loss:4.3745 train_time:564702ms step_avg:170.04ms\nstep:3332/10000 train_loss:4.1676 train_time:564870ms step_avg:170.04ms\nstep:3333/10000 train_loss:4.0774 train_time:565040ms step_avg:170.04ms\nstep:3334/10000 train_loss:4.0781 train_time:565211ms step_avg:170.04ms\nstep:3335/10000 train_loss:4.1650 train_time:565382ms step_avg:170.04ms\nstep:3336/10000 train_loss:4.1606 train_time:565551ms step_avg:170.04ms\nstep:3337/10000 train_loss:4.1228 train_time:565721ms step_avg:170.04ms\nstep:3338/10000 train_loss:4.1852 train_time:565890ms step_avg:170.04ms\nstep:3339/10000 train_loss:4.2529 train_time:566063ms step_avg:170.04ms\nstep:3340/10000 train_loss:4.0386 train_time:566231ms step_avg:170.04ms\nstep:3341/10000 train_loss:4.2348 train_time:566401ms step_avg:170.04ms\nstep:3342/10000 train_loss:4.0802 train_time:566571ms step_avg:170.04ms\nstep:3343/10000 train_loss:4.0999 train_time:566742ms step_avg:170.04ms\nstep:3344/10000 train_loss:4.2652 train_time:566912ms step_avg:170.04ms\nstep:3345/10000 train_loss:4.0289 train_time:567082ms step_avg:170.04ms\nstep:3346/10000 train_loss:4.0235 train_time:567251ms step_avg:170.04ms\nstep:3347/10000 train_loss:4.0335 train_time:567422ms step_avg:170.04ms\nstep:3348/10000 train_loss:4.0046 train_time:567591ms step_avg:170.04ms\nstep:3349/10000 train_loss:4.1548 train_time:567759ms step_avg:170.04ms\nstep:3350/10000 train_loss:4.2937 train_time:567930ms step_avg:170.04ms\nstep:3351/10000 train_loss:4.0535 train_time:568099ms step_avg:170.04ms\nstep:3352/10000 train_loss:3.9526 train_time:568268ms step_avg:170.04ms\nstep:3353/10000 train_loss:3.9151 train_time:568439ms step_avg:170.04ms\nstep:3354/10000 train_loss:3.9997 train_time:568609ms step_avg:170.04ms\nstep:3355/10000 train_loss:4.1436 train_time:568780ms step_avg:170.04ms\nstep:3356/10000 train_loss:4.1617 train_time:568949ms step_avg:170.04ms\nstep:3357/10000 train_loss:4.0994 train_time:569119ms step_avg:170.04ms\nstep:3358/10000 train_loss:4.1371 train_time:569290ms step_avg:170.04ms\nstep:3359/10000 train_loss:4.0826 train_time:569459ms step_avg:170.04ms\nstep:3360/10000 train_loss:4.1024 train_time:569629ms step_avg:170.04ms\nstep:3361/10000 train_loss:4.0403 train_time:569797ms step_avg:170.04ms\nstep:3362/10000 train_loss:4.0032 train_time:569966ms step_avg:170.04ms\nstep:3363/10000 train_loss:4.1382 train_time:570135ms step_avg:170.04ms\nstep:3364/10000 train_loss:4.1507 train_time:570304ms step_avg:170.04ms\nstep:3365/10000 train_loss:4.1517 train_time:570477ms step_avg:170.04ms\nstep:3366/10000 train_loss:4.0398 train_time:570650ms step_avg:170.04ms\nstep:3367/10000 train_loss:4.2112 train_time:570821ms step_avg:170.04ms\nstep:3368/10000 train_loss:4.0817 train_time:570989ms step_avg:170.04ms\nstep:3369/10000 train_loss:4.0755 train_time:571160ms step_avg:170.04ms\nstep:3370/10000 train_loss:4.1166 train_time:571329ms step_avg:170.04ms\nstep:3371/10000 train_loss:4.1274 train_time:571498ms step_avg:170.04ms\nstep:3372/10000 train_loss:4.0602 train_time:571673ms step_avg:170.04ms\nstep:3373/10000 train_loss:4.1710 train_time:571837ms step_avg:170.04ms\nstep:3374/10000 train_loss:4.1168 train_time:572009ms step_avg:170.04ms\nstep:3375/10000 train_loss:3.9640 train_time:572178ms step_avg:170.04ms\nstep:3376/10000 train_loss:4.0590 train_time:572347ms step_avg:170.04ms\nstep:3377/10000 train_loss:4.0933 train_time:572516ms step_avg:170.04ms\nstep:3378/10000 train_loss:4.1293 train_time:572687ms step_avg:170.04ms\nstep:3379/10000 train_loss:4.2268 train_time:572857ms step_avg:170.04ms\nstep:3380/10000 train_loss:4.1202 train_time:573030ms step_avg:170.04ms\nstep:3381/10000 train_loss:3.8799 train_time:573201ms step_avg:170.04ms\nstep:3382/10000 train_loss:3.9809 train_time:573370ms step_avg:170.04ms\nstep:3383/10000 train_loss:4.1249 train_time:573537ms step_avg:170.04ms\nstep:3384/10000 train_loss:4.0998 train_time:573708ms step_avg:170.04ms\nstep:3385/10000 train_loss:4.1896 train_time:573877ms step_avg:170.04ms\nstep:3386/10000 train_loss:4.2187 train_time:574047ms step_avg:170.04ms\nstep:3387/10000 train_loss:4.0660 train_time:574217ms step_avg:170.04ms\nstep:3388/10000 train_loss:4.1412 train_time:574391ms step_avg:170.04ms\nstep:3389/10000 train_loss:4.1371 train_time:574562ms step_avg:170.04ms\nstep:3390/10000 train_loss:4.0841 train_time:574730ms step_avg:170.04ms\nstep:3391/10000 train_loss:4.1412 train_time:574898ms step_avg:170.04ms\nstep:3392/10000 train_loss:4.0947 train_time:575069ms step_avg:170.04ms\nstep:3393/10000 train_loss:4.0679 train_time:575240ms step_avg:170.04ms\nstep:3394/10000 train_loss:4.0529 train_time:575410ms step_avg:170.04ms\nstep:3395/10000 train_loss:4.1736 train_time:575579ms step_avg:170.04ms\nstep:3396/10000 train_loss:4.0025 train_time:575751ms step_avg:170.04ms\nstep:3397/10000 train_loss:4.1799 train_time:575920ms step_avg:170.04ms\nstep:3398/10000 train_loss:4.1666 train_time:576088ms step_avg:170.04ms\nstep:3399/10000 train_loss:4.2150 train_time:576256ms step_avg:170.04ms\nstep:3400/10000 train_loss:4.1657 train_time:576426ms step_avg:170.04ms\nstep:3401/10000 train_loss:4.1312 train_time:576595ms step_avg:170.04ms\nstep:3402/10000 train_loss:4.0450 train_time:576764ms step_avg:170.04ms\nstep:3403/10000 train_loss:4.1484 train_time:576934ms step_avg:170.04ms\nstep:3404/10000 train_loss:4.0309 train_time:577108ms step_avg:170.04ms\nstep:3405/10000 train_loss:4.1476 train_time:577279ms step_avg:170.04ms\nstep:3406/10000 train_loss:4.0116 train_time:577451ms step_avg:170.04ms\nstep:3407/10000 train_loss:4.4087 train_time:577621ms step_avg:170.04ms\nstep:3408/10000 train_loss:4.1566 train_time:577792ms step_avg:170.04ms\nstep:3409/10000 train_loss:4.2282 train_time:577963ms step_avg:170.04ms\nstep:3410/10000 train_loss:4.3254 train_time:578131ms step_avg:170.04ms\nstep:3411/10000 train_loss:4.1218 train_time:578301ms step_avg:170.04ms\nstep:3412/10000 train_loss:3.8999 train_time:578471ms step_avg:170.04ms\nstep:3413/10000 train_loss:4.2713 train_time:578641ms step_avg:170.04ms\nstep:3414/10000 train_loss:4.0312 train_time:578815ms step_avg:170.04ms\nstep:3415/10000 train_loss:4.1133 train_time:578983ms step_avg:170.04ms\nstep:3416/10000 train_loss:4.2247 train_time:579147ms step_avg:170.04ms\nstep:3417/10000 train_loss:4.0412 train_time:579318ms step_avg:170.04ms\nstep:3418/10000 train_loss:4.1242 train_time:579490ms step_avg:170.04ms\nstep:3419/10000 train_loss:4.1066 train_time:579660ms step_avg:170.04ms\nstep:3420/10000 train_loss:4.0900 train_time:579832ms step_avg:170.04ms\nstep:3421/10000 train_loss:4.1371 train_time:580003ms step_avg:170.04ms\nstep:3422/10000 train_loss:4.2117 train_time:580177ms step_avg:170.04ms\nstep:3423/10000 train_loss:4.1021 train_time:580344ms step_avg:170.04ms\nstep:3424/10000 train_loss:3.8724 train_time:580513ms step_avg:170.04ms\nstep:3425/10000 train_loss:4.1345 train_time:580684ms step_avg:170.04ms\nstep:3426/10000 train_loss:4.1264 train_time:580853ms step_avg:170.04ms\nstep:3427/10000 train_loss:4.0401 train_time:581024ms step_avg:170.04ms\nstep:3428/10000 train_loss:4.1776 train_time:581192ms step_avg:170.04ms\nstep:3429/10000 train_loss:4.1623 train_time:581362ms step_avg:170.04ms\nstep:3430/10000 train_loss:4.0966 train_time:581531ms step_avg:170.04ms\nstep:3431/10000 train_loss:4.1093 train_time:581701ms step_avg:170.04ms\nstep:3432/10000 train_loss:4.2434 train_time:581868ms step_avg:170.04ms\nstep:3433/10000 train_loss:4.1096 train_time:582040ms step_avg:170.04ms\nstep:3434/10000 train_loss:4.1330 train_time:582211ms step_avg:170.04ms\nstep:3435/10000 train_loss:4.1374 train_time:582379ms step_avg:170.04ms\nstep:3436/10000 train_loss:3.9776 train_time:582549ms step_avg:170.04ms\nstep:3437/10000 train_loss:4.1095 train_time:582719ms step_avg:170.04ms\nstep:3438/10000 train_loss:4.0929 train_time:582889ms step_avg:170.04ms\nstep:3439/10000 train_loss:3.9324 train_time:583060ms step_avg:170.04ms\nstep:3440/10000 train_loss:4.0904 train_time:583233ms step_avg:170.04ms\nstep:3441/10000 train_loss:4.4200 train_time:583401ms step_avg:170.04ms\nstep:3442/10000 train_loss:4.1687 train_time:583571ms step_avg:170.04ms\nstep:3443/10000 train_loss:3.9846 train_time:583741ms step_avg:170.04ms\nstep:3444/10000 train_loss:4.2181 train_time:583909ms step_avg:170.04ms\nstep:3445/10000 train_loss:4.2402 train_time:584080ms step_avg:170.04ms\nstep:3446/10000 train_loss:4.1602 train_time:584249ms step_avg:170.04ms\nstep:3447/10000 train_loss:4.1255 train_time:584419ms step_avg:170.04ms\nstep:3448/10000 train_loss:3.9548 train_time:584588ms step_avg:170.04ms\nstep:3449/10000 train_loss:4.1410 train_time:584760ms step_avg:170.04ms\nstep:3450/10000 train_loss:4.2137 train_time:584931ms step_avg:170.04ms\nstep:3451/10000 train_loss:3.9564 train_time:585101ms step_avg:170.04ms\nstep:3452/10000 train_loss:4.0567 train_time:585268ms step_avg:170.04ms\nstep:3453/10000 train_loss:4.0599 train_time:585439ms step_avg:170.04ms\nstep:3454/10000 train_loss:4.1144 train_time:585610ms step_avg:170.04ms\nstep:3455/10000 train_loss:4.0195 train_time:585781ms step_avg:170.04ms\nstep:3456/10000 train_loss:4.3468 train_time:585952ms step_avg:170.04ms\nstep:3457/10000 train_loss:4.0984 train_time:586123ms step_avg:170.04ms\nstep:3458/10000 train_loss:4.1233 train_time:586292ms step_avg:170.04ms\nstep:3459/10000 train_loss:4.0712 train_time:586461ms step_avg:170.04ms\nstep:3460/10000 train_loss:4.3354 train_time:586633ms step_avg:170.04ms\nstep:3461/10000 train_loss:4.0742 train_time:586804ms step_avg:170.04ms\nstep:3462/10000 train_loss:3.9919 train_time:586976ms step_avg:170.04ms\nstep:3463/10000 train_loss:4.2390 train_time:587146ms step_avg:170.04ms\nstep:3464/10000 train_loss:4.1258 train_time:587313ms step_avg:170.04ms\nstep:3465/10000 train_loss:4.2503 train_time:587483ms step_avg:170.04ms\nstep:3466/10000 train_loss:4.1456 train_time:587650ms step_avg:170.04ms\nstep:3467/10000 train_loss:3.9979 train_time:587818ms step_avg:170.04ms\nstep:3468/10000 train_loss:4.0718 train_time:587988ms step_avg:170.04ms\nstep:3469/10000 train_loss:4.4088 train_time:588159ms step_avg:170.04ms\nstep:3470/10000 train_loss:4.4385 train_time:588331ms step_avg:170.04ms\nstep:3471/10000 train_loss:4.0523 train_time:588502ms step_avg:170.04ms\nstep:3472/10000 train_loss:4.3107 train_time:588670ms step_avg:170.04ms\nstep:3473/10000 train_loss:4.0584 train_time:588841ms step_avg:170.04ms\nstep:3474/10000 train_loss:3.9970 train_time:589011ms step_avg:170.04ms\nstep:3475/10000 train_loss:4.2150 train_time:589178ms step_avg:170.04ms\nstep:3476/10000 train_loss:4.0163 train_time:589352ms step_avg:170.04ms\nstep:3477/10000 train_loss:4.0915 train_time:589523ms step_avg:170.04ms\nstep:3478/10000 train_loss:4.1738 train_time:589691ms step_avg:170.04ms\nstep:3479/10000 train_loss:4.0729 train_time:589861ms step_avg:170.04ms\nstep:3480/10000 train_loss:3.9383 train_time:590033ms step_avg:170.04ms\nstep:3481/10000 train_loss:4.0851 train_time:590201ms step_avg:170.04ms\nstep:3482/10000 train_loss:4.1568 train_time:590370ms step_avg:170.04ms\nstep:3483/10000 train_loss:4.1819 train_time:590541ms step_avg:170.04ms\nstep:3484/10000 train_loss:4.1676 train_time:590710ms step_avg:170.04ms\nstep:3485/10000 train_loss:4.1875 train_time:590878ms step_avg:170.04ms\nstep:3486/10000 train_loss:3.9895 train_time:591047ms step_avg:170.04ms\nstep:3487/10000 train_loss:4.2097 train_time:591216ms step_avg:170.04ms\nstep:3488/10000 train_loss:4.1094 train_time:591387ms step_avg:170.04ms\nstep:3489/10000 train_loss:4.0186 train_time:591558ms step_avg:170.04ms\nstep:3490/10000 train_loss:4.0029 train_time:591733ms step_avg:170.04ms\nstep:3491/10000 train_loss:4.0461 train_time:591907ms step_avg:170.04ms\nstep:3492/10000 train_loss:4.0384 train_time:592075ms step_avg:170.04ms\nstep:3493/10000 train_loss:4.2090 train_time:592245ms step_avg:170.04ms\nstep:3494/10000 train_loss:4.2116 train_time:592416ms step_avg:170.04ms\nstep:3495/10000 train_loss:4.1083 train_time:592586ms step_avg:170.04ms\nstep:3496/10000 train_loss:4.1220 train_time:592753ms step_avg:170.04ms\nstep:3497/10000 train_loss:4.1721 train_time:592926ms step_avg:170.04ms\nstep:3498/10000 train_loss:3.9988 train_time:593093ms step_avg:170.04ms\nstep:3499/10000 train_loss:4.1179 train_time:593263ms step_avg:170.04ms\nstep:3500/10000 train_loss:4.0496 train_time:593431ms step_avg:170.04ms\nstep:3501/10000 train_loss:4.1559 train_time:593600ms step_avg:170.04ms\nstep:3502/10000 train_loss:3.6136 train_time:593772ms step_avg:170.04ms\nstep:3503/10000 train_loss:4.0858 train_time:593944ms step_avg:170.04ms\nstep:3504/10000 train_loss:4.1741 train_time:594111ms step_avg:170.04ms\nstep:3505/10000 train_loss:4.1300 train_time:594285ms step_avg:170.04ms\nstep:3506/10000 train_loss:4.1775 train_time:594450ms step_avg:170.04ms\nstep:3507/10000 train_loss:4.1170 train_time:594622ms step_avg:170.04ms\nstep:3508/10000 train_loss:3.9254 train_time:594795ms step_avg:170.04ms\nstep:3509/10000 train_loss:4.1133 train_time:594964ms step_avg:170.04ms\nstep:3510/10000 train_loss:4.0267 train_time:595137ms step_avg:170.04ms\nstep:3511/10000 train_loss:3.8939 train_time:595309ms step_avg:170.04ms\nstep:3512/10000 train_loss:4.2075 train_time:595477ms step_avg:170.04ms\nstep:3513/10000 train_loss:4.3869 train_time:595648ms step_avg:170.04ms\nstep:3514/10000 train_loss:4.1909 train_time:595817ms step_avg:170.04ms\nstep:3515/10000 train_loss:4.0713 train_time:595986ms step_avg:170.04ms\nstep:3516/10000 train_loss:4.0511 train_time:596157ms step_avg:170.04ms\nstep:3517/10000 train_loss:4.0341 train_time:596329ms step_avg:170.04ms\nstep:3518/10000 train_loss:4.0612 train_time:596498ms step_avg:170.04ms\nstep:3519/10000 train_loss:4.1352 train_time:596666ms step_avg:170.04ms\nstep:3520/10000 train_loss:4.4060 train_time:596835ms step_avg:170.04ms\nstep:3521/10000 train_loss:4.1315 train_time:597008ms step_avg:170.04ms\nstep:3522/10000 train_loss:4.2063 train_time:597178ms step_avg:170.04ms\nstep:3523/10000 train_loss:3.9329 train_time:597347ms step_avg:170.04ms\nstep:3524/10000 train_loss:4.1455 train_time:597520ms step_avg:170.04ms\nstep:3525/10000 train_loss:4.0454 train_time:597691ms step_avg:170.04ms\nstep:3526/10000 train_loss:3.8850 train_time:597860ms step_avg:170.04ms\nstep:3527/10000 train_loss:4.0247 train_time:598029ms step_avg:170.04ms\nstep:3528/10000 train_loss:3.9494 train_time:598197ms step_avg:170.04ms\nstep:3529/10000 train_loss:3.9999 train_time:598368ms step_avg:170.04ms\nstep:3530/10000 train_loss:4.0596 train_time:598540ms step_avg:170.04ms\nstep:3531/10000 train_loss:4.1830 train_time:598714ms step_avg:170.04ms\nstep:3532/10000 train_loss:4.2539 train_time:598884ms step_avg:170.04ms\nstep:3533/10000 train_loss:4.0530 train_time:599053ms step_avg:170.04ms\nstep:3534/10000 train_loss:4.2482 train_time:599224ms step_avg:170.04ms\nstep:3535/10000 train_loss:4.0924 train_time:599392ms step_avg:170.04ms\nstep:3536/10000 train_loss:3.9732 train_time:599563ms step_avg:170.04ms\nstep:3537/10000 train_loss:4.1144 train_time:599734ms step_avg:170.04ms\nstep:3538/10000 train_loss:4.4026 train_time:599903ms step_avg:170.04ms\nstep:3539/10000 train_loss:3.9886 train_time:600072ms step_avg:170.04ms\nstep:3540/10000 train_loss:4.2028 train_time:600243ms step_avg:170.04ms\nstep:3541/10000 train_loss:3.9902 train_time:600410ms step_avg:170.04ms\nstep:3542/10000 train_loss:4.1374 train_time:600588ms step_avg:170.04ms\nstep:3543/10000 train_loss:4.0603 train_time:600749ms step_avg:170.04ms\nstep:3544/10000 train_loss:4.0695 train_time:600919ms step_avg:170.04ms\nstep:3545/10000 train_loss:3.9720 train_time:601089ms step_avg:170.04ms\nstep:3546/10000 train_loss:4.0287 train_time:601259ms step_avg:170.04ms\nstep:3547/10000 train_loss:4.0005 train_time:601430ms step_avg:170.04ms\nstep:3548/10000 train_loss:4.2371 train_time:601599ms step_avg:170.04ms\nstep:3549/10000 train_loss:4.0657 train_time:601768ms step_avg:170.04ms\nstep:3550/10000 train_loss:4.1568 train_time:601940ms step_avg:170.04ms\nstep:3551/10000 train_loss:4.1039 train_time:602112ms step_avg:170.04ms\nstep:3552/10000 train_loss:4.1113 train_time:602284ms step_avg:170.04ms\nstep:3553/10000 train_loss:3.9898 train_time:602454ms step_avg:170.04ms\nstep:3554/10000 train_loss:4.6007 train_time:602623ms step_avg:170.04ms\nstep:3555/10000 train_loss:4.2596 train_time:602793ms step_avg:170.04ms\nstep:3556/10000 train_loss:4.1700 train_time:602962ms step_avg:170.04ms\nstep:3557/10000 train_loss:3.9758 train_time:603132ms step_avg:170.04ms\nstep:3558/10000 train_loss:4.3534 train_time:603304ms step_avg:170.04ms\nstep:3559/10000 train_loss:4.0613 train_time:603475ms step_avg:170.04ms\nstep:3560/10000 train_loss:4.1276 train_time:603647ms step_avg:170.04ms\nstep:3561/10000 train_loss:4.1562 train_time:603816ms step_avg:170.04ms\nstep:3562/10000 train_loss:4.0823 train_time:603985ms step_avg:170.04ms\nstep:3563/10000 train_loss:3.9463 train_time:604154ms step_avg:170.04ms\nstep:3564/10000 train_loss:3.9507 train_time:604324ms step_avg:170.04ms\nstep:3565/10000 train_loss:3.9763 train_time:604492ms step_avg:170.04ms\nstep:3566/10000 train_loss:4.0905 train_time:604665ms step_avg:170.04ms\nstep:3567/10000 train_loss:3.8551 train_time:604828ms step_avg:170.04ms\nstep:3568/10000 train_loss:4.1218 train_time:605000ms step_avg:170.04ms\nstep:3569/10000 train_loss:4.0114 train_time:605169ms step_avg:170.04ms\nstep:3570/10000 train_loss:3.9101 train_time:605340ms step_avg:170.04ms\nstep:3571/10000 train_loss:3.9387 train_time:605510ms step_avg:170.04ms\nstep:3572/10000 train_loss:4.0154 train_time:605679ms step_avg:170.04ms\nstep:3573/10000 train_loss:4.1769 train_time:605846ms step_avg:170.04ms\nstep:3574/10000 train_loss:4.2258 train_time:606017ms step_avg:170.04ms\nstep:3575/10000 train_loss:4.2536 train_time:606189ms step_avg:170.04ms\nstep:3576/10000 train_loss:4.0869 train_time:606359ms step_avg:170.04ms\nstep:3577/10000 train_loss:3.9174 train_time:606533ms step_avg:170.04ms\nstep:3578/10000 train_loss:3.9876 train_time:606704ms step_avg:170.04ms\nstep:3579/10000 train_loss:4.0550 train_time:606875ms step_avg:170.04ms\nstep:3580/10000 train_loss:3.9126 train_time:607047ms step_avg:170.04ms\nstep:3581/10000 train_loss:3.9879 train_time:607216ms step_avg:170.04ms\nstep:3582/10000 train_loss:4.3209 train_time:607384ms step_avg:170.04ms\nstep:3583/10000 train_loss:4.0929 train_time:607554ms step_avg:170.04ms\nstep:3584/10000 train_loss:3.9038 train_time:607725ms step_avg:170.04ms\nstep:3585/10000 train_loss:4.2171 train_time:607893ms step_avg:170.04ms\nstep:3586/10000 train_loss:4.2546 train_time:608064ms step_avg:170.04ms\nstep:3587/10000 train_loss:4.0213 train_time:608232ms step_avg:170.04ms\nstep:3588/10000 train_loss:4.0074 train_time:608405ms step_avg:170.04ms\nstep:3589/10000 train_loss:3.9776 train_time:608575ms step_avg:170.04ms\nstep:3590/10000 train_loss:4.0591 train_time:608747ms step_avg:170.04ms\nstep:3591/10000 train_loss:4.0502 train_time:608920ms step_avg:170.04ms\nstep:3592/10000 train_loss:4.1895 train_time:609090ms step_avg:170.04ms\nstep:3593/10000 train_loss:4.0990 train_time:609259ms step_avg:170.04ms\nstep:3594/10000 train_loss:4.0720 train_time:609427ms step_avg:170.04ms\nstep:3595/10000 train_loss:4.1014 train_time:609596ms step_avg:170.04ms\nstep:3596/10000 train_loss:4.0759 train_time:609763ms step_avg:170.04ms\nstep:3597/10000 train_loss:4.1081 train_time:609933ms step_avg:170.04ms\nstep:3598/10000 train_loss:4.1316 train_time:610104ms step_avg:170.04ms\nstep:3599/10000 train_loss:4.0436 train_time:610274ms step_avg:170.04ms\nstep:3600/10000 train_loss:4.0891 train_time:610446ms step_avg:170.04ms\nstep:3601/10000 train_loss:4.6287 train_time:610617ms step_avg:170.04ms\nstep:3602/10000 train_loss:4.0310 train_time:610788ms step_avg:170.04ms\nstep:3603/10000 train_loss:4.0227 train_time:610956ms step_avg:170.04ms\nstep:3604/10000 train_loss:4.2835 train_time:611124ms step_avg:170.04ms\nstep:3605/10000 train_loss:4.0068 train_time:611294ms step_avg:170.04ms\nstep:3606/10000 train_loss:3.9700 train_time:611467ms step_avg:170.04ms\nstep:3607/10000 train_loss:4.0618 train_time:611637ms step_avg:170.04ms\nstep:3608/10000 train_loss:4.0838 train_time:611809ms step_avg:170.04ms\nstep:3609/10000 train_loss:4.0482 train_time:611980ms step_avg:170.04ms\nstep:3610/10000 train_loss:4.1910 train_time:612148ms step_avg:170.04ms\nstep:3611/10000 train_loss:4.0334 train_time:612318ms step_avg:170.04ms\nstep:3612/10000 train_loss:3.9916 train_time:612487ms step_avg:170.04ms\nstep:3613/10000 train_loss:4.1024 train_time:612657ms step_avg:170.04ms\nstep:3614/10000 train_loss:4.1970 train_time:612831ms step_avg:170.04ms\nstep:3615/10000 train_loss:4.4674 train_time:613002ms step_avg:170.04ms\nstep:3616/10000 train_loss:4.2372 train_time:613171ms step_avg:170.04ms\nstep:3617/10000 train_loss:4.0625 train_time:613342ms step_avg:170.04ms\nstep:3618/10000 train_loss:4.0735 train_time:613511ms step_avg:170.04ms\nstep:3619/10000 train_loss:4.0410 train_time:613683ms step_avg:170.04ms\nstep:3620/10000 train_loss:4.1686 train_time:613852ms step_avg:170.04ms\nstep:3621/10000 train_loss:4.0654 train_time:614025ms step_avg:170.04ms\nstep:3622/10000 train_loss:4.0545 train_time:614192ms step_avg:170.04ms\nstep:3623/10000 train_loss:4.1095 train_time:614363ms step_avg:170.04ms\nstep:3624/10000 train_loss:4.0709 train_time:614532ms step_avg:170.04ms\nstep:3625/10000 train_loss:4.0293 train_time:614704ms step_avg:170.04ms\nstep:3626/10000 train_loss:4.0539 train_time:614873ms step_avg:170.04ms\nstep:3627/10000 train_loss:3.9408 train_time:615042ms step_avg:170.04ms\nstep:3628/10000 train_loss:4.0183 train_time:615210ms step_avg:170.04ms\nstep:3629/10000 train_loss:4.0304 train_time:615381ms step_avg:170.04ms\nstep:3630/10000 train_loss:4.2724 train_time:615552ms step_avg:170.04ms\nstep:3631/10000 train_loss:4.0661 train_time:615722ms step_avg:170.04ms\nstep:3632/10000 train_loss:4.3139 train_time:615889ms step_avg:170.04ms\nstep:3633/10000 train_loss:4.0890 train_time:616059ms step_avg:170.04ms\nstep:3634/10000 train_loss:4.0662 train_time:616229ms step_avg:170.04ms\nstep:3635/10000 train_loss:4.1677 train_time:616398ms step_avg:170.04ms\nstep:3636/10000 train_loss:4.0296 train_time:616567ms step_avg:170.04ms\nstep:3637/10000 train_loss:4.0087 train_time:616738ms step_avg:170.04ms\nstep:3638/10000 train_loss:4.0870 train_time:616911ms step_avg:170.04ms\nstep:3639/10000 train_loss:4.0695 train_time:617079ms step_avg:170.04ms\nstep:3640/10000 train_loss:4.1668 train_time:617248ms step_avg:170.04ms\nstep:3641/10000 train_loss:4.0759 train_time:617417ms step_avg:170.04ms\nstep:3642/10000 train_loss:3.9980 train_time:617590ms step_avg:170.04ms\nstep:3643/10000 train_loss:4.1629 train_time:617760ms step_avg:170.04ms\nstep:3644/10000 train_loss:4.4041 train_time:617928ms step_avg:170.04ms\nstep:3645/10000 train_loss:4.1684 train_time:618099ms step_avg:170.04ms\nstep:3646/10000 train_loss:4.1058 train_time:618269ms step_avg:170.04ms\nstep:3647/10000 train_loss:4.0387 train_time:618437ms step_avg:170.04ms\nstep:3648/10000 train_loss:3.9245 train_time:618607ms step_avg:170.04ms\nstep:3649/10000 train_loss:4.0447 train_time:618778ms step_avg:170.04ms\nstep:3650/10000 train_loss:4.1435 train_time:618947ms step_avg:170.04ms\nstep:3651/10000 train_loss:3.9990 train_time:619116ms step_avg:170.04ms\nstep:3652/10000 train_loss:4.1966 train_time:619287ms step_avg:170.04ms\nstep:3653/10000 train_loss:4.0704 train_time:619456ms step_avg:170.04ms\nstep:3654/10000 train_loss:4.1508 train_time:619629ms step_avg:170.04ms\nstep:3655/10000 train_loss:3.9925 train_time:619797ms step_avg:170.04ms\nstep:3656/10000 train_loss:4.1162 train_time:619966ms step_avg:170.04ms\nstep:3657/10000 train_loss:4.1034 train_time:620134ms step_avg:170.04ms\nstep:3658/10000 train_loss:4.0646 train_time:620303ms step_avg:170.04ms\nstep:3659/10000 train_loss:4.1105 train_time:620471ms step_avg:170.04ms\nstep:3660/10000 train_loss:4.0963 train_time:620645ms step_avg:170.04ms\nstep:3661/10000 train_loss:3.9091 train_time:620815ms step_avg:170.04ms\nstep:3662/10000 train_loss:4.2806 train_time:620982ms step_avg:170.04ms\nstep:3663/10000 train_loss:4.1020 train_time:621151ms step_avg:170.04ms\nstep:3664/10000 train_loss:4.2740 train_time:621322ms step_avg:170.04ms\nstep:3665/10000 train_loss:4.0908 train_time:621491ms step_avg:170.04ms\nstep:3666/10000 train_loss:4.1155 train_time:621662ms step_avg:170.04ms\nstep:3667/10000 train_loss:3.9944 train_time:621832ms step_avg:170.04ms\nstep:3668/10000 train_loss:4.0167 train_time:622003ms step_avg:170.04ms\nstep:3669/10000 train_loss:4.2827 train_time:622172ms step_avg:170.04ms\nstep:3670/10000 train_loss:4.1711 train_time:622343ms step_avg:170.04ms\nstep:3671/10000 train_loss:4.1412 train_time:622512ms step_avg:170.04ms\nstep:3672/10000 train_loss:3.9962 train_time:622683ms step_avg:170.04ms\nstep:3673/10000 train_loss:4.5297 train_time:622851ms step_avg:170.04ms\nstep:3674/10000 train_loss:4.1690 train_time:623021ms step_avg:170.04ms\nstep:3675/10000 train_loss:4.0023 train_time:623190ms step_avg:170.04ms\nstep:3676/10000 train_loss:4.0710 train_time:623361ms step_avg:170.04ms\nstep:3677/10000 train_loss:4.0715 train_time:623532ms step_avg:170.04ms\nstep:3678/10000 train_loss:5.2969 train_time:623703ms step_avg:170.04ms\nstep:3679/10000 train_loss:4.0265 train_time:623873ms step_avg:170.04ms\nstep:3680/10000 train_loss:3.9967 train_time:624043ms step_avg:170.04ms\nstep:3681/10000 train_loss:3.7511 train_time:624210ms step_avg:170.04ms\nstep:3682/10000 train_loss:4.1156 train_time:624380ms step_avg:170.04ms\nstep:3683/10000 train_loss:3.8859 train_time:624548ms step_avg:170.04ms\nstep:3684/10000 train_loss:4.0525 train_time:624719ms step_avg:170.04ms\nstep:3685/10000 train_loss:4.1648 train_time:624890ms step_avg:170.04ms\nstep:3686/10000 train_loss:4.1047 train_time:625062ms step_avg:170.04ms\nstep:3687/10000 train_loss:4.1313 train_time:625230ms step_avg:170.04ms\nstep:3688/10000 train_loss:4.0089 train_time:625402ms step_avg:170.04ms\nstep:3689/10000 train_loss:4.0592 train_time:625569ms step_avg:170.04ms\nstep:3690/10000 train_loss:4.0639 train_time:625739ms step_avg:170.04ms\nstep:3691/10000 train_loss:3.9728 train_time:625908ms step_avg:170.04ms\nstep:3692/10000 train_loss:4.1183 train_time:626080ms step_avg:170.04ms\nstep:3693/10000 train_loss:4.0621 train_time:626250ms step_avg:170.04ms\nstep:3694/10000 train_loss:4.1420 train_time:626420ms step_avg:170.04ms\nstep:3695/10000 train_loss:4.0346 train_time:626588ms step_avg:170.04ms\nstep:3696/10000 train_loss:4.0933 train_time:626757ms step_avg:170.04ms\nstep:3697/10000 train_loss:4.0434 train_time:626929ms step_avg:170.04ms\nstep:3698/10000 train_loss:4.1291 train_time:627099ms step_avg:170.04ms\nstep:3699/10000 train_loss:3.9836 train_time:627274ms step_avg:170.04ms\nstep:3700/10000 train_loss:3.9819 train_time:627442ms step_avg:170.04ms\nstep:3701/10000 train_loss:4.0642 train_time:627613ms step_avg:170.04ms\nstep:3702/10000 train_loss:4.2081 train_time:627780ms step_avg:170.04ms\nstep:3703/10000 train_loss:3.9581 train_time:627953ms step_avg:170.04ms\nstep:3704/10000 train_loss:4.0833 train_time:628117ms step_avg:170.04ms\nstep:3705/10000 train_loss:4.0117 train_time:628287ms step_avg:170.04ms\nstep:3706/10000 train_loss:4.2024 train_time:628455ms step_avg:170.04ms\nstep:3707/10000 train_loss:4.0528 train_time:628626ms step_avg:170.04ms\nstep:3708/10000 train_loss:3.9940 train_time:628790ms step_avg:170.04ms\nstep:3709/10000 train_loss:3.8797 train_time:628962ms step_avg:170.04ms\nstep:3710/10000 train_loss:4.0800 train_time:629130ms step_avg:170.04ms\nstep:3711/10000 train_loss:3.9661 train_time:629298ms step_avg:170.03ms\nstep:3712/10000 train_loss:4.1790 train_time:629466ms step_avg:170.03ms\nstep:3713/10000 train_loss:4.2026 train_time:629636ms step_avg:170.03ms\nstep:3714/10000 train_loss:4.0868 train_time:629806ms step_avg:170.03ms\nstep:3715/10000 train_loss:3.9094 train_time:629975ms step_avg:170.03ms\nstep:3716/10000 train_loss:4.0568 train_time:630146ms step_avg:170.03ms\nstep:3717/10000 train_loss:3.9809 train_time:630315ms step_avg:170.03ms\nstep:3718/10000 train_loss:4.2598 train_time:630486ms step_avg:170.03ms\nstep:3719/10000 train_loss:4.1124 train_time:630654ms step_avg:170.03ms\nstep:3720/10000 train_loss:3.8962 train_time:630823ms step_avg:170.03ms\nstep:3721/10000 train_loss:4.0956 train_time:630993ms step_avg:170.03ms\nstep:3722/10000 train_loss:4.1391 train_time:631164ms step_avg:170.03ms\nstep:3723/10000 train_loss:4.1603 train_time:631333ms step_avg:170.03ms\nstep:3724/10000 train_loss:4.2456 train_time:631503ms step_avg:170.03ms\nstep:3725/10000 train_loss:4.2668 train_time:631674ms step_avg:170.03ms\nstep:3726/10000 train_loss:3.9144 train_time:631845ms step_avg:170.03ms\nstep:3727/10000 train_loss:4.1612 train_time:632013ms step_avg:170.03ms\nstep:3728/10000 train_loss:3.9812 train_time:632183ms step_avg:170.03ms\nstep:3729/10000 train_loss:4.0559 train_time:632353ms step_avg:170.03ms\nstep:3730/10000 train_loss:4.0361 train_time:632526ms step_avg:170.03ms\nstep:3731/10000 train_loss:4.1253 train_time:632694ms step_avg:170.03ms\nstep:3732/10000 train_loss:3.9549 train_time:632866ms step_avg:170.03ms\nstep:3733/10000 train_loss:4.0564 train_time:633036ms step_avg:170.03ms\nstep:3734/10000 train_loss:4.2238 train_time:633210ms step_avg:170.03ms\nstep:3735/10000 train_loss:4.3319 train_time:633373ms step_avg:170.03ms\nstep:3736/10000 train_loss:4.1046 train_time:633542ms step_avg:170.03ms\nstep:3737/10000 train_loss:4.0550 train_time:633711ms step_avg:170.03ms\nstep:3738/10000 train_loss:3.9840 train_time:633882ms step_avg:170.03ms\nstep:3739/10000 train_loss:3.9840 train_time:634050ms step_avg:170.03ms\nstep:3740/10000 train_loss:4.1150 train_time:634217ms step_avg:170.03ms\nstep:3741/10000 train_loss:4.0211 train_time:634387ms step_avg:170.03ms\nstep:3742/10000 train_loss:4.0882 train_time:634556ms step_avg:170.03ms\nstep:3743/10000 train_loss:4.2315 train_time:634727ms step_avg:170.03ms\nstep:3744/10000 train_loss:3.9481 train_time:634896ms step_avg:170.03ms\nstep:3745/10000 train_loss:4.2176 train_time:635068ms step_avg:170.03ms\nstep:3746/10000 train_loss:4.0108 train_time:635236ms step_avg:170.03ms\nstep:3747/10000 train_loss:3.9553 train_time:635408ms step_avg:170.03ms\nstep:3748/10000 train_loss:3.9232 train_time:635577ms step_avg:170.03ms\nstep:3749/10000 train_loss:4.2991 train_time:635751ms step_avg:170.03ms\nstep:3750/10000 train_loss:4.0575 train_time:635921ms step_avg:170.03ms\nstep:3751/10000 train_loss:4.3067 train_time:636090ms step_avg:170.03ms\nstep:3752/10000 train_loss:4.0431 train_time:636258ms step_avg:170.03ms\nstep:3753/10000 train_loss:4.0047 train_time:636428ms step_avg:170.03ms\nstep:3754/10000 train_loss:4.1499 train_time:636598ms step_avg:170.03ms\nstep:3755/10000 train_loss:4.1487 train_time:636772ms step_avg:170.03ms\nstep:3756/10000 train_loss:3.8692 train_time:636942ms step_avg:170.03ms\nstep:3757/10000 train_loss:4.0351 train_time:637115ms step_avg:170.03ms\nstep:3758/10000 train_loss:4.0615 train_time:637282ms step_avg:170.03ms\nstep:3759/10000 train_loss:4.0323 train_time:637452ms step_avg:170.03ms\nstep:3760/10000 train_loss:4.1117 train_time:637623ms step_avg:170.03ms\nstep:3761/10000 train_loss:4.1062 train_time:637794ms step_avg:170.03ms\nstep:3762/10000 train_loss:4.2440 train_time:637965ms step_avg:170.03ms\nstep:3763/10000 train_loss:4.0000 train_time:638135ms step_avg:170.03ms\nstep:3764/10000 train_loss:3.9662 train_time:638307ms step_avg:170.03ms\nstep:3765/10000 train_loss:4.1162 train_time:638475ms step_avg:170.03ms\nstep:3766/10000 train_loss:4.0392 train_time:638647ms step_avg:170.03ms\nstep:3767/10000 train_loss:3.9717 train_time:638820ms step_avg:170.03ms\nstep:3768/10000 train_loss:3.9338 train_time:638991ms step_avg:170.03ms\nstep:3769/10000 train_loss:4.0454 train_time:639162ms step_avg:170.04ms\nstep:3770/10000 train_loss:4.0975 train_time:639330ms step_avg:170.03ms\nstep:3771/10000 train_loss:4.6136 train_time:639502ms step_avg:170.04ms\nstep:3772/10000 train_loss:4.0840 train_time:639671ms step_avg:170.03ms\nstep:3773/10000 train_loss:4.0838 train_time:639841ms step_avg:170.03ms\nstep:3774/10000 train_loss:3.9133 train_time:640013ms step_avg:170.04ms\nstep:3775/10000 train_loss:3.9094 train_time:640185ms step_avg:170.04ms\nstep:3776/10000 train_loss:3.9785 train_time:640352ms step_avg:170.03ms\nstep:3777/10000 train_loss:4.2433 train_time:640521ms step_avg:170.03ms\nstep:3778/10000 train_loss:4.2148 train_time:640692ms step_avg:170.03ms\nstep:3779/10000 train_loss:4.3129 train_time:640863ms step_avg:170.04ms\nstep:3780/10000 train_loss:4.0164 train_time:641032ms step_avg:170.03ms\nstep:3781/10000 train_loss:4.0699 train_time:641201ms step_avg:170.03ms\nstep:3782/10000 train_loss:3.9483 train_time:641370ms step_avg:170.03ms\nstep:3783/10000 train_loss:4.0196 train_time:641540ms step_avg:170.03ms\nstep:3784/10000 train_loss:4.2740 train_time:641711ms step_avg:170.03ms\nstep:3785/10000 train_loss:4.1331 train_time:641884ms step_avg:170.04ms\nstep:3786/10000 train_loss:3.9703 train_time:642053ms step_avg:170.04ms\nstep:3787/10000 train_loss:4.2287 train_time:642222ms step_avg:170.04ms\nstep:3788/10000 train_loss:4.0652 train_time:642392ms step_avg:170.03ms\nstep:3789/10000 train_loss:3.9973 train_time:642561ms step_avg:170.03ms\nstep:3790/10000 train_loss:5.5217 train_time:642730ms step_avg:170.03ms\nstep:3791/10000 train_loss:3.9876 train_time:642902ms step_avg:170.03ms\nstep:3792/10000 train_loss:4.1422 train_time:643070ms step_avg:170.03ms\nstep:3793/10000 train_loss:4.1244 train_time:643242ms step_avg:170.03ms\nstep:3794/10000 train_loss:4.1231 train_time:643411ms step_avg:170.03ms\nstep:3795/10000 train_loss:4.3671 train_time:643580ms step_avg:170.03ms\nstep:3796/10000 train_loss:4.3020 train_time:643752ms step_avg:170.03ms\nstep:3797/10000 train_loss:4.1096 train_time:643925ms step_avg:170.04ms\nstep:3798/10000 train_loss:4.0338 train_time:644093ms step_avg:170.04ms\nstep:3799/10000 train_loss:4.0639 train_time:644263ms step_avg:170.04ms\nstep:3800/10000 train_loss:3.9096 train_time:644432ms step_avg:170.03ms\nstep:3801/10000 train_loss:4.0919 train_time:644603ms step_avg:170.04ms\nstep:3802/10000 train_loss:3.9422 train_time:644774ms step_avg:170.04ms\nstep:3803/10000 train_loss:4.0073 train_time:644946ms step_avg:170.04ms\nstep:3804/10000 train_loss:3.9946 train_time:645116ms step_avg:170.04ms\nstep:3805/10000 train_loss:3.9749 train_time:645288ms step_avg:170.04ms\nstep:3806/10000 train_loss:3.9996 train_time:645459ms step_avg:170.04ms\nstep:3807/10000 train_loss:3.9233 train_time:645634ms step_avg:170.04ms\nstep:3808/10000 train_loss:4.2759 train_time:645806ms step_avg:170.04ms\nstep:3809/10000 train_loss:4.0514 train_time:645974ms step_avg:170.04ms\nstep:3810/10000 train_loss:4.2150 train_time:646366ms step_avg:170.10ms\nstep:3811/10000 train_loss:4.0896 train_time:646531ms step_avg:170.09ms\nstep:3812/10000 train_loss:3.9508 train_time:646700ms step_avg:170.09ms\nstep:3813/10000 train_loss:4.0174 train_time:646869ms step_avg:170.09ms\nstep:3814/10000 train_loss:4.1234 train_time:647038ms step_avg:170.09ms\nstep:3815/10000 train_loss:4.0570 train_time:647206ms step_avg:170.09ms\nstep:3816/10000 train_loss:4.0238 train_time:647394ms step_avg:170.10ms\nstep:3817/10000 train_loss:4.0518 train_time:647565ms step_avg:170.10ms\nstep:3818/10000 train_loss:3.8127 train_time:647735ms step_avg:170.10ms\nstep:3819/10000 train_loss:3.9948 train_time:647904ms step_avg:170.10ms\nstep:3820/10000 train_loss:4.0730 train_time:648066ms step_avg:170.10ms\nstep:3821/10000 train_loss:4.0198 train_time:648242ms step_avg:170.10ms\nstep:3822/10000 train_loss:4.0079 train_time:648417ms step_avg:170.10ms\nstep:3823/10000 train_loss:3.9747 train_time:648589ms step_avg:170.10ms\nstep:3824/10000 train_loss:3.9448 train_time:648757ms step_avg:170.10ms\nstep:3825/10000 train_loss:4.1118 train_time:648921ms step_avg:170.10ms\nstep:3826/10000 train_loss:3.9866 train_time:649088ms step_avg:170.10ms\nstep:3827/10000 train_loss:4.0152 train_time:649260ms step_avg:170.10ms\nstep:3828/10000 train_loss:4.0245 train_time:649431ms step_avg:170.10ms\nstep:3829/10000 train_loss:4.0043 train_time:649603ms step_avg:170.10ms\nstep:3830/10000 train_loss:4.0099 train_time:649770ms step_avg:170.10ms\nstep:3831/10000 train_loss:4.0935 train_time:649941ms step_avg:170.10ms\nstep:3832/10000 train_loss:3.9594 train_time:650110ms step_avg:170.10ms\nstep:3833/10000 train_loss:4.5736 train_time:650279ms step_avg:170.10ms\nstep:3834/10000 train_loss:4.0059 train_time:650449ms step_avg:170.10ms\nstep:3835/10000 train_loss:3.9770 train_time:650618ms step_avg:170.10ms\nstep:3836/10000 train_loss:4.0415 train_time:650787ms step_avg:170.10ms\nstep:3837/10000 train_loss:4.0942 train_time:650957ms step_avg:170.10ms\nstep:3838/10000 train_loss:4.1595 train_time:651128ms step_avg:170.10ms\nstep:3839/10000 train_loss:4.0465 train_time:651299ms step_avg:170.10ms\nstep:3840/10000 train_loss:4.0191 train_time:651471ms step_avg:170.10ms\nstep:3841/10000 train_loss:4.2782 train_time:651642ms step_avg:170.10ms\nstep:3842/10000 train_loss:4.2192 train_time:651809ms step_avg:170.10ms\nstep:3843/10000 train_loss:3.9363 train_time:651977ms step_avg:170.10ms\nstep:3844/10000 train_loss:4.0384 train_time:652145ms step_avg:170.10ms\nstep:3845/10000 train_loss:4.2876 train_time:652317ms step_avg:170.10ms\nstep:3846/10000 train_loss:4.1595 train_time:652488ms step_avg:170.10ms\nstep:3847/10000 train_loss:4.0833 train_time:652660ms step_avg:170.10ms\nstep:3848/10000 train_loss:3.9073 train_time:652829ms step_avg:170.10ms\nstep:3849/10000 train_loss:4.2030 train_time:652999ms step_avg:170.10ms\nstep:3850/10000 train_loss:4.0312 train_time:653170ms step_avg:170.10ms\nstep:3851/10000 train_loss:4.0690 train_time:653340ms step_avg:170.10ms\nstep:3852/10000 train_loss:4.1991 train_time:653509ms step_avg:170.10ms\nstep:3853/10000 train_loss:4.0476 train_time:653679ms step_avg:170.10ms\nstep:3854/10000 train_loss:3.9100 train_time:653847ms step_avg:170.10ms\nstep:3855/10000 train_loss:4.0176 train_time:654017ms step_avg:170.10ms\nstep:3856/10000 train_loss:4.1422 train_time:654186ms step_avg:170.10ms\nstep:3857/10000 train_loss:4.0688 train_time:654354ms step_avg:170.09ms\nstep:3858/10000 train_loss:4.0117 train_time:654524ms step_avg:170.09ms\nstep:3859/10000 train_loss:3.9726 train_time:654693ms step_avg:170.09ms\nstep:3860/10000 train_loss:3.9899 train_time:654864ms step_avg:170.09ms\nstep:3861/10000 train_loss:4.0664 train_time:655035ms step_avg:170.09ms\nstep:3862/10000 train_loss:4.1142 train_time:655207ms step_avg:170.10ms\nstep:3863/10000 train_loss:4.1759 train_time:655378ms step_avg:170.10ms\nstep:3864/10000 train_loss:3.9742 train_time:655547ms step_avg:170.10ms\nstep:3865/10000 train_loss:3.9114 train_time:655715ms step_avg:170.09ms\nstep:3866/10000 train_loss:4.1625 train_time:655886ms step_avg:170.09ms\nstep:3867/10000 train_loss:4.0142 train_time:656053ms step_avg:170.09ms\nstep:3868/10000 train_loss:4.0209 train_time:656224ms step_avg:170.09ms\nstep:3869/10000 train_loss:4.0596 train_time:656391ms step_avg:170.09ms\nstep:3870/10000 train_loss:4.0321 train_time:656563ms step_avg:170.09ms\nstep:3871/10000 train_loss:4.0203 train_time:656735ms step_avg:170.09ms\nstep:3872/10000 train_loss:4.2173 train_time:656907ms step_avg:170.09ms\nstep:3873/10000 train_loss:4.0936 train_time:657076ms step_avg:170.09ms\nstep:3874/10000 train_loss:4.1034 train_time:657248ms step_avg:170.10ms\nstep:3875/10000 train_loss:3.9463 train_time:657417ms step_avg:170.09ms\nstep:3876/10000 train_loss:4.0752 train_time:657590ms step_avg:170.10ms\nstep:3877/10000 train_loss:4.0722 train_time:657762ms step_avg:170.10ms\nstep:3878/10000 train_loss:4.0752 train_time:657930ms step_avg:170.10ms\nstep:3879/10000 train_loss:4.1136 train_time:658101ms step_avg:170.10ms\nstep:3880/10000 train_loss:4.0727 train_time:658270ms step_avg:170.10ms\nstep:3881/10000 train_loss:3.9724 train_time:658442ms step_avg:170.10ms\nstep:3882/10000 train_loss:4.1012 train_time:658611ms step_avg:170.10ms\nstep:3883/10000 train_loss:3.9749 train_time:658781ms step_avg:170.10ms\nstep:3884/10000 train_loss:4.0837 train_time:658948ms step_avg:170.10ms\nstep:3885/10000 train_loss:4.0421 train_time:659117ms step_avg:170.09ms\nstep:3886/10000 train_loss:3.9791 train_time:659285ms step_avg:170.09ms\nstep:3887/10000 train_loss:3.9415 train_time:659456ms step_avg:170.09ms\nstep:3888/10000 train_loss:3.9298 train_time:659627ms step_avg:170.09ms\nstep:3889/10000 train_loss:4.1846 train_time:659797ms step_avg:170.09ms\nstep:3890/10000 train_loss:3.8876 train_time:659969ms step_avg:170.10ms\nstep:3891/10000 train_loss:4.0817 train_time:660140ms step_avg:170.10ms\nstep:3892/10000 train_loss:4.0738 train_time:660307ms step_avg:170.09ms\nstep:3893/10000 train_loss:4.0437 train_time:660476ms step_avg:170.09ms\nstep:3894/10000 train_loss:4.0704 train_time:660649ms step_avg:170.09ms\nstep:3895/10000 train_loss:3.9498 train_time:660818ms step_avg:170.09ms\nstep:3896/10000 train_loss:4.0367 train_time:660989ms step_avg:170.09ms\nstep:3897/10000 train_loss:3.9779 train_time:661160ms step_avg:170.10ms\nstep:3898/10000 train_loss:3.9078 train_time:661330ms step_avg:170.10ms\nstep:3899/10000 train_loss:3.8986 train_time:661500ms step_avg:170.10ms\nstep:3900/10000 train_loss:4.0837 train_time:661669ms step_avg:170.09ms\nstep:3901/10000 train_loss:4.0113 train_time:661840ms step_avg:170.10ms\nstep:3902/10000 train_loss:4.0438 train_time:662009ms step_avg:170.09ms\nstep:3903/10000 train_loss:4.1194 train_time:662179ms step_avg:170.09ms\nstep:3904/10000 train_loss:4.1724 train_time:662349ms step_avg:170.09ms\nstep:3905/10000 train_loss:3.9322 train_time:662520ms step_avg:170.10ms\nstep:3906/10000 train_loss:4.1105 train_time:662689ms step_avg:170.09ms\nstep:3907/10000 train_loss:4.0127 train_time:662860ms step_avg:170.09ms\nstep:3908/10000 train_loss:4.0156 train_time:663029ms step_avg:170.09ms\nstep:3909/10000 train_loss:4.0162 train_time:663202ms step_avg:170.10ms\nstep:3910/10000 train_loss:3.9939 train_time:663372ms step_avg:170.10ms\nstep:3911/10000 train_loss:4.3507 train_time:663542ms step_avg:170.10ms\nstep:3912/10000 train_loss:4.2168 train_time:663712ms step_avg:170.10ms\nstep:3913/10000 train_loss:3.9980 train_time:663881ms step_avg:170.10ms\nstep:3914/10000 train_loss:4.0279 train_time:664052ms step_avg:170.10ms\nstep:3915/10000 train_loss:4.1910 train_time:664222ms step_avg:170.10ms\nstep:3916/10000 train_loss:4.1054 train_time:664391ms step_avg:170.09ms\nstep:3917/10000 train_loss:3.9655 train_time:664563ms step_avg:170.10ms\nstep:3918/10000 train_loss:4.0934 train_time:664730ms step_avg:170.09ms\nstep:3919/10000 train_loss:4.0847 train_time:664900ms step_avg:170.09ms\nstep:3920/10000 train_loss:3.9551 train_time:665067ms step_avg:170.09ms\nstep:3921/10000 train_loss:4.1124 train_time:665238ms step_avg:170.09ms\nstep:3922/10000 train_loss:3.9089 train_time:665407ms step_avg:170.09ms\nstep:3923/10000 train_loss:3.9589 train_time:665577ms step_avg:170.09ms\nstep:3924/10000 train_loss:4.2417 train_time:665746ms step_avg:170.09ms\nstep:3925/10000 train_loss:4.0859 train_time:665919ms step_avg:170.09ms\nstep:3926/10000 train_loss:4.0244 train_time:666088ms step_avg:170.09ms\nstep:3927/10000 train_loss:4.0022 train_time:666257ms step_avg:170.09ms\nstep:3928/10000 train_loss:3.9552 train_time:666426ms step_avg:170.09ms\nstep:3929/10000 train_loss:4.1579 train_time:666595ms step_avg:170.09ms\nstep:3930/10000 train_loss:3.9874 train_time:666767ms step_avg:170.09ms\nstep:3931/10000 train_loss:4.0748 train_time:666935ms step_avg:170.09ms\nstep:3932/10000 train_loss:4.1143 train_time:667107ms step_avg:170.09ms\nstep:3933/10000 train_loss:4.0315 train_time:667275ms step_avg:170.09ms\nstep:3934/10000 train_loss:4.0177 train_time:667448ms step_avg:170.09ms\nstep:3935/10000 train_loss:4.0296 train_time:667616ms step_avg:170.09ms\nstep:3936/10000 train_loss:4.1077 train_time:667787ms step_avg:170.09ms\nstep:3937/10000 train_loss:4.0544 train_time:667955ms step_avg:170.09ms\nstep:3938/10000 train_loss:4.1365 train_time:668127ms step_avg:170.09ms\nstep:3939/10000 train_loss:3.8887 train_time:668300ms step_avg:170.09ms\nstep:3940/10000 train_loss:4.1417 train_time:668471ms step_avg:170.09ms\nstep:3941/10000 train_loss:4.0296 train_time:668640ms step_avg:170.09ms\nstep:3942/10000 train_loss:4.0110 train_time:668809ms step_avg:170.09ms\nstep:3943/10000 train_loss:3.9875 train_time:668982ms step_avg:170.09ms\nstep:3944/10000 train_loss:4.0483 train_time:669150ms step_avg:170.09ms\nstep:3945/10000 train_loss:4.1276 train_time:669319ms step_avg:170.09ms\nstep:3946/10000 train_loss:4.0106 train_time:669490ms step_avg:170.09ms\nstep:3947/10000 train_loss:3.9237 train_time:669665ms step_avg:170.10ms\nstep:3948/10000 train_loss:4.1335 train_time:669835ms step_avg:170.10ms\nstep:3949/10000 train_loss:4.0831 train_time:670005ms step_avg:170.10ms\nstep:3950/10000 train_loss:4.0262 train_time:670174ms step_avg:170.09ms\nstep:3951/10000 train_loss:4.4697 train_time:670346ms step_avg:170.10ms\nstep:3952/10000 train_loss:3.9506 train_time:670518ms step_avg:170.10ms\nstep:3953/10000 train_loss:4.1206 train_time:670688ms step_avg:170.10ms\nstep:3954/10000 train_loss:4.1297 train_time:670863ms step_avg:170.10ms\nstep:3955/10000 train_loss:3.9353 train_time:671032ms step_avg:170.10ms\nstep:3956/10000 train_loss:4.0258 train_time:671196ms step_avg:170.10ms\nstep:3957/10000 train_loss:4.1828 train_time:671371ms step_avg:170.10ms\nstep:3958/10000 train_loss:3.8994 train_time:671534ms step_avg:170.09ms\nstep:3959/10000 train_loss:4.1176 train_time:671702ms step_avg:170.09ms\nstep:3960/10000 train_loss:4.0380 train_time:671873ms step_avg:170.09ms\nstep:3961/10000 train_loss:4.0675 train_time:672045ms step_avg:170.10ms\nstep:3962/10000 train_loss:3.9361 train_time:672214ms step_avg:170.09ms\nstep:3963/10000 train_loss:3.8562 train_time:672385ms step_avg:170.09ms\nstep:3964/10000 train_loss:3.9749 train_time:672554ms step_avg:170.09ms\nstep:3965/10000 train_loss:3.9974 train_time:672724ms step_avg:170.09ms\nstep:3966/10000 train_loss:4.0946 train_time:672895ms step_avg:170.09ms\nstep:3967/10000 train_loss:3.9588 train_time:673067ms step_avg:170.10ms\nstep:3968/10000 train_loss:4.1347 train_time:673236ms step_avg:170.09ms\nstep:3969/10000 train_loss:3.9689 train_time:673404ms step_avg:170.09ms\nstep:3970/10000 train_loss:4.0220 train_time:673573ms step_avg:170.09ms\nstep:3971/10000 train_loss:4.0276 train_time:673742ms step_avg:170.09ms\nstep:3972/10000 train_loss:4.0170 train_time:673913ms step_avg:170.09ms\nstep:3973/10000 train_loss:4.0600 train_time:674086ms step_avg:170.09ms\nstep:3974/10000 train_loss:4.0456 train_time:674257ms step_avg:170.10ms\nstep:3975/10000 train_loss:3.9442 train_time:674429ms step_avg:170.10ms\nstep:3976/10000 train_loss:3.9687 train_time:674600ms step_avg:170.10ms\nstep:3977/10000 train_loss:4.0930 train_time:674769ms step_avg:170.10ms\nstep:3978/10000 train_loss:4.0015 train_time:674941ms step_avg:170.10ms\nstep:3979/10000 train_loss:4.0608 train_time:675111ms step_avg:170.10ms\nstep:3980/10000 train_loss:4.0680 train_time:675283ms step_avg:170.10ms\nstep:3981/10000 train_loss:3.9465 train_time:675454ms step_avg:170.10ms\nstep:3982/10000 train_loss:3.9562 train_time:675625ms step_avg:170.10ms\nstep:3983/10000 train_loss:4.3071 train_time:675793ms step_avg:170.10ms\nstep:3984/10000 train_loss:4.1226 train_time:675966ms step_avg:170.10ms\nstep:3985/10000 train_loss:4.1544 train_time:676135ms step_avg:170.10ms\nstep:3986/10000 train_loss:3.9972 train_time:676305ms step_avg:170.10ms\nstep:3987/10000 train_loss:4.1012 train_time:676473ms step_avg:170.10ms\nstep:3988/10000 train_loss:4.0806 train_time:676643ms step_avg:170.10ms\nstep:3989/10000 train_loss:3.9851 train_time:676813ms step_avg:170.10ms\nstep:3990/10000 train_loss:3.9991 train_time:676983ms step_avg:170.10ms\nstep:3991/10000 train_loss:3.9667 train_time:677151ms step_avg:170.10ms\nstep:3992/10000 train_loss:3.9732 train_time:677325ms step_avg:170.10ms\nstep:3993/10000 train_loss:3.8074 train_time:677492ms step_avg:170.10ms\nstep:3994/10000 train_loss:4.1340 train_time:677662ms step_avg:170.10ms\nstep:3995/10000 train_loss:4.0482 train_time:677832ms step_avg:170.10ms\nstep:3996/10000 train_loss:4.3692 train_time:678001ms step_avg:170.10ms\nstep:3997/10000 train_loss:3.9515 train_time:678170ms step_avg:170.10ms\nstep:3998/10000 train_loss:3.9485 train_time:678342ms step_avg:170.10ms\nstep:3999/10000 train_loss:4.0568 train_time:678512ms step_avg:170.10ms\nstep:4000/10000 train_loss:3.9910 train_time:678682ms step_avg:170.10ms\nstep:4001/10000 train_loss:4.0869 train_time:678852ms step_avg:170.10ms\nstep:4002/10000 train_loss:4.1765 train_time:679025ms step_avg:170.10ms\nstep:4003/10000 train_loss:4.0777 train_time:679193ms step_avg:170.10ms\nstep:4004/10000 train_loss:4.1001 train_time:679364ms step_avg:170.10ms\nstep:4005/10000 train_loss:4.0674 train_time:679535ms step_avg:170.10ms\nstep:4006/10000 train_loss:4.1055 train_time:679705ms step_avg:170.10ms\nstep:4007/10000 train_loss:4.1603 train_time:679874ms step_avg:170.10ms\nstep:4008/10000 train_loss:3.9833 train_time:680044ms step_avg:170.10ms\nstep:4009/10000 train_loss:4.2333 train_time:680214ms step_avg:170.10ms\nstep:4010/10000 train_loss:4.0804 train_time:680386ms step_avg:170.10ms\nstep:4011/10000 train_loss:4.1163 train_time:680557ms step_avg:170.10ms\nstep:4012/10000 train_loss:4.0591 train_time:680730ms step_avg:170.10ms\nstep:4013/10000 train_loss:4.0212 train_time:680899ms step_avg:170.10ms\nstep:4014/10000 train_loss:4.0565 train_time:681069ms step_avg:170.10ms\nstep:4015/10000 train_loss:4.0285 train_time:681237ms step_avg:170.10ms\nstep:4016/10000 train_loss:3.9280 train_time:681408ms step_avg:170.10ms\nstep:4017/10000 train_loss:4.0945 train_time:681579ms step_avg:170.10ms\nstep:4018/10000 train_loss:4.0347 train_time:681750ms step_avg:170.10ms\nstep:4019/10000 train_loss:4.0781 train_time:681921ms step_avg:170.10ms\nstep:4020/10000 train_loss:4.1058 train_time:682089ms step_avg:170.10ms\nstep:4021/10000 train_loss:3.9787 train_time:682258ms step_avg:170.10ms\nstep:4022/10000 train_loss:4.0380 train_time:682428ms step_avg:170.10ms\nstep:4023/10000 train_loss:4.2624 train_time:682600ms step_avg:170.10ms\nstep:4024/10000 train_loss:4.1609 train_time:682769ms step_avg:170.10ms\nstep:4025/10000 train_loss:4.0283 train_time:682941ms step_avg:170.10ms\nstep:4026/10000 train_loss:4.0002 train_time:683108ms step_avg:170.10ms\nstep:4027/10000 train_loss:4.2146 train_time:683276ms step_avg:170.10ms\nstep:4028/10000 train_loss:3.9287 train_time:683447ms step_avg:170.10ms\nstep:4029/10000 train_loss:3.9408 train_time:683615ms step_avg:170.10ms\nstep:4030/10000 train_loss:3.9970 train_time:683786ms step_avg:170.10ms\nstep:4031/10000 train_loss:4.1011 train_time:683955ms step_avg:170.10ms\nstep:4032/10000 train_loss:4.0187 train_time:684129ms step_avg:170.10ms\nstep:4033/10000 train_loss:3.9704 train_time:684298ms step_avg:170.10ms\nstep:4034/10000 train_loss:4.0005 train_time:684471ms step_avg:170.10ms\nstep:4035/10000 train_loss:3.9801 train_time:684640ms step_avg:170.10ms\nstep:4036/10000 train_loss:3.9863 train_time:684811ms step_avg:170.10ms\nstep:4037/10000 train_loss:4.2656 train_time:684979ms step_avg:170.10ms\nstep:4038/10000 train_loss:4.0028 train_time:685148ms step_avg:170.10ms\nstep:4039/10000 train_loss:4.0292 train_time:685317ms step_avg:170.10ms\nstep:4040/10000 train_loss:4.1065 train_time:685485ms step_avg:170.10ms\nstep:4041/10000 train_loss:3.9804 train_time:685656ms step_avg:170.10ms\nstep:4042/10000 train_loss:4.2892 train_time:685829ms step_avg:170.10ms\nstep:4043/10000 train_loss:4.0943 train_time:685999ms step_avg:170.10ms\nstep:4044/10000 train_loss:4.1895 train_time:686168ms step_avg:170.10ms\nstep:4045/10000 train_loss:4.0682 train_time:686339ms step_avg:170.10ms\nstep:4046/10000 train_loss:4.1830 train_time:686511ms step_avg:170.10ms\nstep:4047/10000 train_loss:4.1280 train_time:686677ms step_avg:170.10ms\nstep:4048/10000 train_loss:4.0914 train_time:686846ms step_avg:170.10ms\nstep:4049/10000 train_loss:4.1576 train_time:687014ms step_avg:170.10ms\nstep:4050/10000 train_loss:3.9502 train_time:687182ms step_avg:170.09ms\nstep:4051/10000 train_loss:4.0846 train_time:687354ms step_avg:170.09ms\nstep:4052/10000 train_loss:4.0467 train_time:687525ms step_avg:170.10ms\nstep:4053/10000 train_loss:4.0407 train_time:687694ms step_avg:170.09ms\nstep:4054/10000 train_loss:4.0035 train_time:687866ms step_avg:170.10ms\nstep:4055/10000 train_loss:4.3722 train_time:688036ms step_avg:170.10ms\nstep:4056/10000 train_loss:3.9175 train_time:688204ms step_avg:170.10ms\nstep:4057/10000 train_loss:4.1167 train_time:688373ms step_avg:170.09ms\nstep:4058/10000 train_loss:3.9900 train_time:688542ms step_avg:170.09ms\nstep:4059/10000 train_loss:4.0237 train_time:688712ms step_avg:170.09ms\nstep:4060/10000 train_loss:4.0625 train_time:688883ms step_avg:170.09ms\nstep:4061/10000 train_loss:4.0901 train_time:689052ms step_avg:170.09ms\nstep:4062/10000 train_loss:4.0584 train_time:689223ms step_avg:170.09ms\nstep:4063/10000 train_loss:4.1109 train_time:689391ms step_avg:170.09ms\nstep:4064/10000 train_loss:4.1866 train_time:689560ms step_avg:170.09ms\nstep:4065/10000 train_loss:3.9280 train_time:689731ms step_avg:170.09ms\nstep:4066/10000 train_loss:4.1297 train_time:689900ms step_avg:170.09ms\nstep:4067/10000 train_loss:3.9741 train_time:690070ms step_avg:170.09ms\nstep:4068/10000 train_loss:3.8725 train_time:690238ms step_avg:170.09ms\nstep:4069/10000 train_loss:4.2649 train_time:690406ms step_avg:170.09ms\nstep:4070/10000 train_loss:3.9577 train_time:690575ms step_avg:170.09ms\nstep:4071/10000 train_loss:3.9995 train_time:690747ms step_avg:170.09ms\nstep:4072/10000 train_loss:3.9945 train_time:690915ms step_avg:170.09ms\nstep:4073/10000 train_loss:4.0240 train_time:691086ms step_avg:170.09ms\nstep:4074/10000 train_loss:4.0156 train_time:691254ms step_avg:170.09ms\nstep:4075/10000 train_loss:4.0412 train_time:691423ms step_avg:170.09ms\nstep:4076/10000 train_loss:4.0236 train_time:691591ms step_avg:170.09ms\nstep:4077/10000 train_loss:4.0035 train_time:691763ms step_avg:170.09ms\nstep:4078/10000 train_loss:3.9811 train_time:691933ms step_avg:170.09ms\nstep:4079/10000 train_loss:4.0170 train_time:692104ms step_avg:170.09ms\nstep:4080/10000 train_loss:3.9202 train_time:692274ms step_avg:170.09ms\nstep:4081/10000 train_loss:3.9297 train_time:692444ms step_avg:170.09ms\nstep:4082/10000 train_loss:4.1532 train_time:692615ms step_avg:170.09ms\nstep:4083/10000 train_loss:4.0267 train_time:692786ms step_avg:170.09ms\nstep:4084/10000 train_loss:4.0778 train_time:692954ms step_avg:170.09ms\nstep:4085/10000 train_loss:4.3721 train_time:693123ms step_avg:170.09ms\nstep:4086/10000 train_loss:3.9229 train_time:693292ms step_avg:170.09ms\nstep:4087/10000 train_loss:4.0562 train_time:693464ms step_avg:170.09ms\nstep:4088/10000 train_loss:4.0105 train_time:693634ms step_avg:170.09ms\nstep:4089/10000 train_loss:4.0024 train_time:693808ms step_avg:170.09ms\nstep:4090/10000 train_loss:4.2730 train_time:693979ms step_avg:170.09ms\nstep:4091/10000 train_loss:4.0447 train_time:694150ms step_avg:170.09ms\nstep:4092/10000 train_loss:3.9227 train_time:694315ms step_avg:170.09ms\nstep:4093/10000 train_loss:3.9860 train_time:694484ms step_avg:170.09ms\nstep:4094/10000 train_loss:4.1378 train_time:694656ms step_avg:170.09ms\nstep:4095/10000 train_loss:4.0861 train_time:694823ms step_avg:170.09ms\nstep:4096/10000 train_loss:4.0203 train_time:694995ms step_avg:170.09ms\nstep:4097/10000 train_loss:4.0158 train_time:695166ms step_avg:170.09ms\nstep:4098/10000 train_loss:3.8728 train_time:695331ms step_avg:170.09ms\nstep:4099/10000 train_loss:4.1039 train_time:695502ms step_avg:170.09ms\nstep:4100/10000 train_loss:4.0544 train_time:695672ms step_avg:170.09ms\nstep:4101/10000 train_loss:4.1231 train_time:695844ms step_avg:170.09ms\nstep:4102/10000 train_loss:4.0364 train_time:696018ms step_avg:170.09ms\nstep:4103/10000 train_loss:4.0339 train_time:696186ms step_avg:170.09ms\nstep:4104/10000 train_loss:4.0141 train_time:696355ms step_avg:170.09ms\nstep:4105/10000 train_loss:4.0347 train_time:696526ms step_avg:170.09ms\nstep:4106/10000 train_loss:3.9884 train_time:696696ms step_avg:170.09ms\nstep:4107/10000 train_loss:4.0887 train_time:696866ms step_avg:170.09ms\nstep:4108/10000 train_loss:4.0650 train_time:697037ms step_avg:170.09ms\nstep:4109/10000 train_loss:3.9470 train_time:697206ms step_avg:170.09ms\nstep:4110/10000 train_loss:4.0332 train_time:697374ms step_avg:170.09ms\nstep:4111/10000 train_loss:4.1584 train_time:697543ms step_avg:170.09ms\nstep:4112/10000 train_loss:4.2007 train_time:697712ms step_avg:170.09ms\nstep:4113/10000 train_loss:3.9311 train_time:697884ms step_avg:170.09ms\nstep:4114/10000 train_loss:3.9945 train_time:698055ms step_avg:170.09ms\nstep:4115/10000 train_loss:4.0727 train_time:698226ms step_avg:170.09ms\nstep:4116/10000 train_loss:3.8699 train_time:698394ms step_avg:170.09ms\nstep:4117/10000 train_loss:4.1939 train_time:698564ms step_avg:170.09ms\nstep:4118/10000 train_loss:3.9169 train_time:698732ms step_avg:170.09ms\nstep:4119/10000 train_loss:4.0632 train_time:698906ms step_avg:170.09ms\nstep:4120/10000 train_loss:3.9020 train_time:699075ms step_avg:170.09ms\nstep:4121/10000 train_loss:4.0250 train_time:699249ms step_avg:170.09ms\nstep:4122/10000 train_loss:4.0106 train_time:699413ms step_avg:170.09ms\nstep:4123/10000 train_loss:3.9095 train_time:699581ms step_avg:170.09ms\nstep:4124/10000 train_loss:3.9138 train_time:699750ms step_avg:170.09ms\nstep:4125/10000 train_loss:4.0817 train_time:699922ms step_avg:170.09ms\nstep:4126/10000 train_loss:4.4302 train_time:700105ms step_avg:170.09ms\nstep:4127/10000 train_loss:3.9318 train_time:700263ms step_avg:170.09ms\nstep:4128/10000 train_loss:4.0170 train_time:700430ms step_avg:170.09ms\nstep:4129/10000 train_loss:3.9521 train_time:700600ms step_avg:170.09ms\nstep:4130/10000 train_loss:3.9612 train_time:700768ms step_avg:170.09ms\nstep:4131/10000 train_loss:4.0469 train_time:700939ms step_avg:170.09ms\nstep:4132/10000 train_loss:4.0463 train_time:701111ms step_avg:170.09ms\nstep:4133/10000 train_loss:3.8334 train_time:701282ms step_avg:170.09ms\nstep:4134/10000 train_loss:3.9507 train_time:701451ms step_avg:170.09ms\nstep:4135/10000 train_loss:4.0141 train_time:701619ms step_avg:170.09ms\nstep:4136/10000 train_loss:4.2037 train_time:701788ms step_avg:170.09ms\nstep:4137/10000 train_loss:4.0480 train_time:701959ms step_avg:170.09ms\nstep:4138/10000 train_loss:4.0389 train_time:702131ms step_avg:170.09ms\nstep:4139/10000 train_loss:4.1022 train_time:702305ms step_avg:170.09ms\nstep:4140/10000 train_loss:4.0341 train_time:702475ms step_avg:170.09ms\nstep:4141/10000 train_loss:3.9930 train_time:702644ms step_avg:170.09ms\nstep:4142/10000 train_loss:4.0496 train_time:702813ms step_avg:170.09ms\nstep:4143/10000 train_loss:4.1117 train_time:702985ms step_avg:170.09ms\nstep:4144/10000 train_loss:4.0089 train_time:703157ms step_avg:170.09ms\nstep:4145/10000 train_loss:4.0578 train_time:703324ms step_avg:170.09ms\nstep:4146/10000 train_loss:3.8944 train_time:703492ms step_avg:170.09ms\nstep:4147/10000 train_loss:4.1865 train_time:703663ms step_avg:170.09ms\nstep:4148/10000 train_loss:3.9547 train_time:703833ms step_avg:170.09ms\nstep:4149/10000 train_loss:3.9895 train_time:704006ms step_avg:170.09ms\nstep:4150/10000 train_loss:3.8714 train_time:704174ms step_avg:170.09ms\nstep:4151/10000 train_loss:4.0321 train_time:704345ms step_avg:170.09ms\nstep:4152/10000 train_loss:4.0581 train_time:704514ms step_avg:170.09ms\nstep:4153/10000 train_loss:3.9639 train_time:704684ms step_avg:170.09ms\nstep:4154/10000 train_loss:4.0928 train_time:704852ms step_avg:170.09ms\nstep:4155/10000 train_loss:4.1646 train_time:705022ms step_avg:170.09ms\nstep:4156/10000 train_loss:4.3474 train_time:705189ms step_avg:170.09ms\nstep:4157/10000 train_loss:3.9777 train_time:705361ms step_avg:170.09ms\nstep:4158/10000 train_loss:3.8789 train_time:705530ms step_avg:170.09ms\nstep:4159/10000 train_loss:4.1014 train_time:705699ms step_avg:170.09ms\nstep:4160/10000 train_loss:4.1518 train_time:705870ms step_avg:170.09ms\nstep:4161/10000 train_loss:4.0328 train_time:706042ms step_avg:170.09ms\nstep:4162/10000 train_loss:3.9287 train_time:706212ms step_avg:170.09ms\nstep:4163/10000 train_loss:3.9838 train_time:706382ms step_avg:170.09ms\nstep:4164/10000 train_loss:4.0471 train_time:706549ms step_avg:170.09ms\nstep:4165/10000 train_loss:3.9317 train_time:706718ms step_avg:170.09ms\nstep:4166/10000 train_loss:4.0293 train_time:706887ms step_avg:170.09ms\nstep:4167/10000 train_loss:4.0418 train_time:707058ms step_avg:170.09ms\nstep:4168/10000 train_loss:3.9621 train_time:707226ms step_avg:170.09ms\nstep:4169/10000 train_loss:4.0384 train_time:707397ms step_avg:170.09ms\nstep:4170/10000 train_loss:3.9148 train_time:707568ms step_avg:170.09ms\nstep:4171/10000 train_loss:4.0957 train_time:707739ms step_avg:170.09ms\nstep:4172/10000 train_loss:4.0046 train_time:707908ms step_avg:170.09ms\nstep:4173/10000 train_loss:4.2093 train_time:708078ms step_avg:170.09ms\nstep:4174/10000 train_loss:3.9502 train_time:708248ms step_avg:170.09ms\nstep:4175/10000 train_loss:3.9603 train_time:708417ms step_avg:170.09ms\nstep:4176/10000 train_loss:4.0647 train_time:708585ms step_avg:170.09ms\nstep:4177/10000 train_loss:4.0297 train_time:708757ms step_avg:170.09ms\nstep:4178/10000 train_loss:4.0558 train_time:708930ms step_avg:170.09ms\nstep:4179/10000 train_loss:4.2149 train_time:709101ms step_avg:170.09ms\nstep:4180/10000 train_loss:3.9786 train_time:709269ms step_avg:170.09ms\nstep:4181/10000 train_loss:3.9897 train_time:709439ms step_avg:170.09ms\nstep:4182/10000 train_loss:4.1888 train_time:709607ms step_avg:170.09ms\nstep:4183/10000 train_loss:4.1242 train_time:709777ms step_avg:170.09ms\nstep:4184/10000 train_loss:3.9851 train_time:709949ms step_avg:170.09ms\nstep:4185/10000 train_loss:3.9653 train_time:710117ms step_avg:170.09ms\nstep:4186/10000 train_loss:3.8560 train_time:710285ms step_avg:170.09ms\nstep:4187/10000 train_loss:3.9698 train_time:710455ms step_avg:170.09ms\nstep:4188/10000 train_loss:4.0886 train_time:710628ms step_avg:170.09ms\nstep:4189/10000 train_loss:4.0548 train_time:710796ms step_avg:170.09ms\nstep:4190/10000 train_loss:4.2199 train_time:710970ms step_avg:170.09ms\nstep:4191/10000 train_loss:3.9995 train_time:711141ms step_avg:170.09ms\nstep:4192/10000 train_loss:3.9953 train_time:711311ms step_avg:170.09ms\nstep:4193/10000 train_loss:4.0029 train_time:711479ms step_avg:170.09ms\nstep:4194/10000 train_loss:3.9153 train_time:711647ms step_avg:170.09ms\nstep:4195/10000 train_loss:4.0061 train_time:711818ms step_avg:170.09ms\nstep:4196/10000 train_loss:4.2290 train_time:711988ms step_avg:170.09ms\nstep:4197/10000 train_loss:3.9916 train_time:712159ms step_avg:170.09ms\nstep:4198/10000 train_loss:4.4807 train_time:712330ms step_avg:170.09ms\nstep:4199/10000 train_loss:4.0239 train_time:712500ms step_avg:170.09ms\nstep:4200/10000 train_loss:4.0409 train_time:712669ms step_avg:170.09ms\nstep:4201/10000 train_loss:3.8572 train_time:712840ms step_avg:170.09ms\nstep:4202/10000 train_loss:4.0563 train_time:713009ms step_avg:170.09ms\nstep:4203/10000 train_loss:4.0453 train_time:713177ms step_avg:170.09ms\nstep:4204/10000 train_loss:3.9823 train_time:713347ms step_avg:170.09ms\nstep:4205/10000 train_loss:4.0640 train_time:713516ms step_avg:170.09ms\nstep:4206/10000 train_loss:3.9600 train_time:713688ms step_avg:170.09ms\nstep:4207/10000 train_loss:3.9805 train_time:713857ms step_avg:170.09ms\nstep:4208/10000 train_loss:3.9307 train_time:714025ms step_avg:170.09ms\nstep:4209/10000 train_loss:4.0111 train_time:714195ms step_avg:170.09ms\nstep:4210/10000 train_loss:3.9332 train_time:714363ms step_avg:170.09ms\nstep:4211/10000 train_loss:4.0039 train_time:714531ms step_avg:170.09ms\nstep:4212/10000 train_loss:4.0580 train_time:714703ms step_avg:170.09ms\nstep:4213/10000 train_loss:4.0327 train_time:714870ms step_avg:170.09ms\nstep:4214/10000 train_loss:4.0420 train_time:715042ms step_avg:170.09ms\nstep:4215/10000 train_loss:4.0787 train_time:715212ms step_avg:170.09ms\nstep:4216/10000 train_loss:3.9450 train_time:715383ms step_avg:170.09ms\nstep:4217/10000 train_loss:3.9936 train_time:715553ms step_avg:170.09ms\nstep:4218/10000 train_loss:3.9922 train_time:715723ms step_avg:170.09ms\nstep:4219/10000 train_loss:4.0787 train_time:715896ms step_avg:170.09ms\nstep:4220/10000 train_loss:4.2681 train_time:716069ms step_avg:170.09ms\nstep:4221/10000 train_loss:4.0563 train_time:716233ms step_avg:170.09ms\nstep:4222/10000 train_loss:4.0051 train_time:716403ms step_avg:170.09ms\nstep:4223/10000 train_loss:3.9335 train_time:716572ms step_avg:170.09ms\nstep:4224/10000 train_loss:4.0776 train_time:716742ms step_avg:170.09ms\nstep:4225/10000 train_loss:4.0997 train_time:716911ms step_avg:170.09ms\nstep:4226/10000 train_loss:3.9495 train_time:717084ms step_avg:170.09ms\nstep:4227/10000 train_loss:3.9121 train_time:717252ms step_avg:170.09ms\nstep:4228/10000 train_loss:3.9532 train_time:717421ms step_avg:170.09ms\nstep:4229/10000 train_loss:3.9284 train_time:717590ms step_avg:170.09ms\nstep:4230/10000 train_loss:3.9814 train_time:717760ms step_avg:170.09ms\nstep:4231/10000 train_loss:4.0111 train_time:717931ms step_avg:170.09ms\nstep:4232/10000 train_loss:4.0526 train_time:718104ms step_avg:170.09ms\nstep:4233/10000 train_loss:3.9171 train_time:718275ms step_avg:170.09ms\nstep:4234/10000 train_loss:4.0628 train_time:718446ms step_avg:170.09ms\nstep:4235/10000 train_loss:3.9085 train_time:718614ms step_avg:170.09ms\nstep:4236/10000 train_loss:4.1129 train_time:718784ms step_avg:170.09ms\nstep:4237/10000 train_loss:3.9958 train_time:718955ms step_avg:170.09ms\nstep:4238/10000 train_loss:4.1848 train_time:719125ms step_avg:170.09ms\nstep:4239/10000 train_loss:3.9588 train_time:719295ms step_avg:170.09ms\nstep:4240/10000 train_loss:3.8469 train_time:719466ms step_avg:170.09ms\nstep:4241/10000 train_loss:3.9494 train_time:719634ms step_avg:170.09ms\nstep:4242/10000 train_loss:3.9852 train_time:719805ms step_avg:170.09ms\nstep:4243/10000 train_loss:3.9969 train_time:719976ms step_avg:170.09ms\nstep:4244/10000 train_loss:4.0086 train_time:720147ms step_avg:170.09ms\nstep:4245/10000 train_loss:4.0500 train_time:720316ms step_avg:170.09ms\nstep:4246/10000 train_loss:4.2003 train_time:720487ms step_avg:170.09ms\nstep:4247/10000 train_loss:4.0431 train_time:720656ms step_avg:170.09ms\nstep:4248/10000 train_loss:4.0006 train_time:720825ms step_avg:170.09ms\nstep:4249/10000 train_loss:3.9913 train_time:720995ms step_avg:170.09ms\nstep:4250/10000 train_loss:3.9107 train_time:721166ms step_avg:170.09ms\nstep:4251/10000 train_loss:3.8262 train_time:721336ms step_avg:170.09ms\nstep:4252/10000 train_loss:3.9676 train_time:721508ms step_avg:170.09ms\nstep:4253/10000 train_loss:4.1510 train_time:721677ms step_avg:170.09ms\nstep:4254/10000 train_loss:3.9537 train_time:721849ms step_avg:170.09ms\nstep:4255/10000 train_loss:4.0835 train_time:722020ms step_avg:170.09ms\nstep:4256/10000 train_loss:3.9381 train_time:722189ms step_avg:170.09ms\nstep:4257/10000 train_loss:3.9579 train_time:722360ms step_avg:170.09ms\nstep:4258/10000 train_loss:4.1279 train_time:722530ms step_avg:170.09ms\nstep:4259/10000 train_loss:4.0095 train_time:722699ms step_avg:170.09ms\nstep:4260/10000 train_loss:4.0531 train_time:722869ms step_avg:170.09ms\nstep:4261/10000 train_loss:3.9454 train_time:723042ms step_avg:170.09ms\nstep:4262/10000 train_loss:4.0319 train_time:723208ms step_avg:170.09ms\nstep:4263/10000 train_loss:3.9916 train_time:723378ms step_avg:170.09ms\nstep:4264/10000 train_loss:3.9617 train_time:723547ms step_avg:170.09ms\nstep:4265/10000 train_loss:4.0769 train_time:723716ms step_avg:170.09ms\nstep:4266/10000 train_loss:4.1632 train_time:723889ms step_avg:170.09ms\nstep:4267/10000 train_loss:4.1141 train_time:724059ms step_avg:170.09ms\nstep:4268/10000 train_loss:3.9121 train_time:724230ms step_avg:170.09ms\nstep:4269/10000 train_loss:4.0687 train_time:724403ms step_avg:170.09ms\nstep:4270/10000 train_loss:3.6935 train_time:724571ms step_avg:170.09ms\nstep:4271/10000 train_loss:4.1336 train_time:724742ms step_avg:170.09ms\nstep:4272/10000 train_loss:3.9199 train_time:724912ms step_avg:170.09ms\nstep:4273/10000 train_loss:4.0856 train_time:725084ms step_avg:170.09ms\nstep:4274/10000 train_loss:3.9192 train_time:725252ms step_avg:170.09ms\nstep:4275/10000 train_loss:3.9845 train_time:725424ms step_avg:170.09ms\nstep:4276/10000 train_loss:3.8747 train_time:725592ms step_avg:170.09ms\nstep:4277/10000 train_loss:4.0513 train_time:725762ms step_avg:170.09ms\nstep:4278/10000 train_loss:4.0230 train_time:725930ms step_avg:170.09ms\nstep:4279/10000 train_loss:3.9572 train_time:726103ms step_avg:170.09ms\nstep:4280/10000 train_loss:3.9534 train_time:726270ms step_avg:170.09ms\nstep:4281/10000 train_loss:3.9438 train_time:726440ms step_avg:170.09ms\nstep:4282/10000 train_loss:4.0168 train_time:726607ms step_avg:170.09ms\nstep:4283/10000 train_loss:4.2053 train_time:726776ms step_avg:170.09ms\nstep:4284/10000 train_loss:3.8722 train_time:726947ms step_avg:170.09ms\nstep:4285/10000 train_loss:4.0785 train_time:727116ms step_avg:170.09ms\nstep:4286/10000 train_loss:3.9658 train_time:727289ms step_avg:170.09ms\nstep:4287/10000 train_loss:4.0842 train_time:727460ms step_avg:170.09ms\nstep:4288/10000 train_loss:4.0657 train_time:727631ms step_avg:170.09ms\nstep:4289/10000 train_loss:4.1368 train_time:727802ms step_avg:170.09ms\nstep:4290/10000 train_loss:3.8982 train_time:727971ms step_avg:170.09ms\nstep:4291/10000 train_loss:3.9618 train_time:728143ms step_avg:170.09ms\nstep:4292/10000 train_loss:4.0379 train_time:728313ms step_avg:170.09ms\nstep:4293/10000 train_loss:3.9416 train_time:728485ms step_avg:170.09ms\nstep:4294/10000 train_loss:3.9632 train_time:728653ms step_avg:170.09ms\nstep:4295/10000 train_loss:3.9575 train_time:728826ms step_avg:170.09ms\nstep:4296/10000 train_loss:4.0039 train_time:728996ms step_avg:170.09ms\nstep:4297/10000 train_loss:3.9611 train_time:729165ms step_avg:170.09ms\nstep:4298/10000 train_loss:4.0392 train_time:729335ms step_avg:170.09ms\nstep:4299/10000 train_loss:4.0908 train_time:729504ms step_avg:170.09ms\nstep:4300/10000 train_loss:4.0565 train_time:729672ms step_avg:170.09ms\nstep:4301/10000 train_loss:3.7835 train_time:729842ms step_avg:170.09ms\nstep:4302/10000 train_loss:3.9098 train_time:730012ms step_avg:170.09ms\nstep:4303/10000 train_loss:3.9025 train_time:730184ms step_avg:170.09ms\nstep:4304/10000 train_loss:4.0344 train_time:730352ms step_avg:170.09ms\nstep:4305/10000 train_loss:4.1375 train_time:730523ms step_avg:170.09ms\nstep:4306/10000 train_loss:4.1205 train_time:730692ms step_avg:170.09ms\nstep:4307/10000 train_loss:3.9108 train_time:730862ms step_avg:170.09ms\nstep:4308/10000 train_loss:3.9417 train_time:731030ms step_avg:170.09ms\nstep:4309/10000 train_loss:4.0651 train_time:731199ms step_avg:170.09ms\nstep:4310/10000 train_loss:4.0209 train_time:731368ms step_avg:170.09ms\nstep:4311/10000 train_loss:4.0404 train_time:731536ms step_avg:170.09ms\nstep:4312/10000 train_loss:4.3161 train_time:731706ms step_avg:170.08ms\nstep:4313/10000 train_loss:4.1145 train_time:731875ms step_avg:170.08ms\nstep:4314/10000 train_loss:3.9916 train_time:732048ms step_avg:170.09ms\nstep:4315/10000 train_loss:4.0138 train_time:732216ms step_avg:170.08ms\nstep:4316/10000 train_loss:4.0734 train_time:732385ms step_avg:170.08ms\nstep:4317/10000 train_loss:3.8847 train_time:732552ms step_avg:170.08ms\nstep:4318/10000 train_loss:3.9825 train_time:732722ms step_avg:170.08ms\nstep:4319/10000 train_loss:4.0148 train_time:732891ms step_avg:170.08ms\nstep:4320/10000 train_loss:3.7671 train_time:733063ms step_avg:170.08ms\nstep:4321/10000 train_loss:3.9501 train_time:733230ms step_avg:170.08ms\nstep:4322/10000 train_loss:4.0576 train_time:733401ms step_avg:170.08ms\nstep:4323/10000 train_loss:4.0293 train_time:733573ms step_avg:170.08ms\nstep:4324/10000 train_loss:4.0713 train_time:733745ms step_avg:170.08ms\nstep:4325/10000 train_loss:4.0003 train_time:733914ms step_avg:170.08ms\nstep:4326/10000 train_loss:4.0640 train_time:734085ms step_avg:170.08ms\nstep:4327/10000 train_loss:3.9653 train_time:734253ms step_avg:170.08ms\nstep:4328/10000 train_loss:3.9962 train_time:734424ms step_avg:170.08ms\nstep:4329/10000 train_loss:4.0221 train_time:734593ms step_avg:170.08ms\nstep:4330/10000 train_loss:4.0181 train_time:734767ms step_avg:170.08ms\nstep:4331/10000 train_loss:3.9194 train_time:734937ms step_avg:170.08ms\nstep:4332/10000 train_loss:4.0771 train_time:735107ms step_avg:170.09ms\nstep:4333/10000 train_loss:3.9028 train_time:735275ms step_avg:170.08ms\nstep:4334/10000 train_loss:3.9768 train_time:735445ms step_avg:170.08ms\nstep:4335/10000 train_loss:3.9367 train_time:735614ms step_avg:170.08ms\nstep:4336/10000 train_loss:4.1667 train_time:735784ms step_avg:170.08ms\nstep:4337/10000 train_loss:3.8587 train_time:735953ms step_avg:170.08ms\nstep:4338/10000 train_loss:3.7761 train_time:736122ms step_avg:170.08ms\nstep:4339/10000 train_loss:3.9240 train_time:736291ms step_avg:170.08ms\nstep:4340/10000 train_loss:4.0511 train_time:736459ms step_avg:170.08ms\nstep:4341/10000 train_loss:3.9579 train_time:736626ms step_avg:170.08ms\nstep:4342/10000 train_loss:3.9233 train_time:736799ms step_avg:170.08ms\nstep:4343/10000 train_loss:3.8962 train_time:736971ms step_avg:170.08ms\nstep:4344/10000 train_loss:4.0563 train_time:737141ms step_avg:170.08ms\nstep:4345/10000 train_loss:4.0516 train_time:737311ms step_avg:170.08ms\nstep:4346/10000 train_loss:4.0537 train_time:737482ms step_avg:170.08ms\nstep:4347/10000 train_loss:3.9434 train_time:737650ms step_avg:170.08ms\nstep:4348/10000 train_loss:3.9903 train_time:737820ms step_avg:170.08ms\nstep:4349/10000 train_loss:4.0243 train_time:737991ms step_avg:170.08ms\nstep:4350/10000 train_loss:3.8968 train_time:738162ms step_avg:170.08ms\nstep:4351/10000 train_loss:3.9936 train_time:738330ms step_avg:170.08ms\nstep:4352/10000 train_loss:4.1925 train_time:738501ms step_avg:170.08ms\nstep:4353/10000 train_loss:4.0597 train_time:738669ms step_avg:170.08ms\nstep:4354/10000 train_loss:3.8617 train_time:738840ms step_avg:170.08ms\nstep:4355/10000 train_loss:4.0353 train_time:739010ms step_avg:170.08ms\nstep:4356/10000 train_loss:4.1081 train_time:739182ms step_avg:170.08ms\nstep:4357/10000 train_loss:4.0402 train_time:739351ms step_avg:170.08ms\nstep:4358/10000 train_loss:3.9967 train_time:739522ms step_avg:170.08ms\nstep:4359/10000 train_loss:3.8426 train_time:739691ms step_avg:170.08ms\nstep:4360/10000 train_loss:3.8912 train_time:739863ms step_avg:170.08ms\nstep:4361/10000 train_loss:3.9266 train_time:740031ms step_avg:170.08ms\nstep:4362/10000 train_loss:4.0398 train_time:740201ms step_avg:170.08ms\nstep:4363/10000 train_loss:3.9266 train_time:740370ms step_avg:170.08ms\nstep:4364/10000 train_loss:3.9393 train_time:740541ms step_avg:170.08ms\nstep:4365/10000 train_loss:3.9603 train_time:740709ms step_avg:170.08ms\nstep:4366/10000 train_loss:4.0512 train_time:740880ms step_avg:170.08ms\nstep:4367/10000 train_loss:3.9443 train_time:741052ms step_avg:170.08ms\nstep:4368/10000 train_loss:4.0507 train_time:741223ms step_avg:170.08ms\nstep:4369/10000 train_loss:4.0055 train_time:741393ms step_avg:170.08ms\nstep:4370/10000 train_loss:4.0232 train_time:741564ms step_avg:170.08ms\nstep:4371/10000 train_loss:3.9239 train_time:741734ms step_avg:170.08ms\nstep:4372/10000 train_loss:3.8484 train_time:741907ms step_avg:170.08ms\nstep:4373/10000 train_loss:3.9496 train_time:742075ms step_avg:170.08ms\nstep:4374/10000 train_loss:3.9131 train_time:742244ms step_avg:170.08ms\nstep:4375/10000 train_loss:3.9847 train_time:742413ms step_avg:170.08ms\nstep:4376/10000 train_loss:4.0554 train_time:742586ms step_avg:170.08ms\nstep:4377/10000 train_loss:3.9298 train_time:742754ms step_avg:170.08ms\nstep:4378/10000 train_loss:3.9067 train_time:742924ms step_avg:170.08ms\nstep:4379/10000 train_loss:3.9636 train_time:743094ms step_avg:170.08ms\nstep:4380/10000 train_loss:4.1669 train_time:743268ms step_avg:170.08ms\nstep:4381/10000 train_loss:4.0105 train_time:743435ms step_avg:170.08ms\nstep:4382/10000 train_loss:4.1506 train_time:743608ms step_avg:170.08ms\nstep:4383/10000 train_loss:3.9679 train_time:743780ms step_avg:170.08ms\nstep:4384/10000 train_loss:4.0158 train_time:743950ms step_avg:170.08ms\nstep:4385/10000 train_loss:4.1463 train_time:744121ms step_avg:170.08ms\nstep:4386/10000 train_loss:3.9421 train_time:744290ms step_avg:170.08ms\nstep:4387/10000 train_loss:3.9194 train_time:744459ms step_avg:170.08ms\nstep:4388/10000 train_loss:3.9768 train_time:744631ms step_avg:170.08ms\nstep:4389/10000 train_loss:4.0553 train_time:744801ms step_avg:170.08ms\nstep:4390/10000 train_loss:3.9349 train_time:744969ms step_avg:170.08ms\nstep:4391/10000 train_loss:4.0229 train_time:745140ms step_avg:170.08ms\nstep:4392/10000 train_loss:3.9983 train_time:745310ms step_avg:170.08ms\nstep:4393/10000 train_loss:4.0658 train_time:745480ms step_avg:170.08ms\nstep:4394/10000 train_loss:3.9614 train_time:745648ms step_avg:170.08ms\nstep:4395/10000 train_loss:3.9091 train_time:745816ms step_avg:170.08ms\nstep:4396/10000 train_loss:4.0456 train_time:745988ms step_avg:170.08ms\nstep:4397/10000 train_loss:4.0050 train_time:746158ms step_avg:170.08ms\nstep:4398/10000 train_loss:4.0154 train_time:746325ms step_avg:170.08ms\nstep:4399/10000 train_loss:4.0307 train_time:746495ms step_avg:170.08ms\nstep:4400/10000 train_loss:4.0966 train_time:746667ms step_avg:170.08ms\nstep:4401/10000 train_loss:3.8986 train_time:746837ms step_avg:170.08ms\nstep:4402/10000 train_loss:4.0920 train_time:747005ms step_avg:170.08ms\nstep:4403/10000 train_loss:3.9536 train_time:747182ms step_avg:170.08ms\nstep:4404/10000 train_loss:4.0531 train_time:747347ms step_avg:170.08ms\nstep:4405/10000 train_loss:3.9644 train_time:747516ms step_avg:170.08ms\nstep:4406/10000 train_loss:3.9484 train_time:747685ms step_avg:170.08ms\nstep:4407/10000 train_loss:3.8926 train_time:747854ms step_avg:170.08ms\nstep:4408/10000 train_loss:4.0113 train_time:748030ms step_avg:170.08ms\nstep:4409/10000 train_loss:4.0244 train_time:748196ms step_avg:170.08ms\nstep:4410/10000 train_loss:3.9979 train_time:748366ms step_avg:170.08ms\nstep:4411/10000 train_loss:3.9260 train_time:748536ms step_avg:170.08ms\nstep:4412/10000 train_loss:3.8564 train_time:748708ms step_avg:170.08ms\nstep:4413/10000 train_loss:4.0273 train_time:748879ms step_avg:170.08ms\nstep:4414/10000 train_loss:3.9405 train_time:749050ms step_avg:170.08ms\nstep:4415/10000 train_loss:4.1363 train_time:749220ms step_avg:170.08ms\nstep:4416/10000 train_loss:3.9569 train_time:749388ms step_avg:170.08ms\nstep:4417/10000 train_loss:4.0689 train_time:749558ms step_avg:170.08ms\nstep:4418/10000 train_loss:3.8868 train_time:749731ms step_avg:170.08ms\nstep:4419/10000 train_loss:4.2731 train_time:749901ms step_avg:170.08ms\nstep:4420/10000 train_loss:3.9927 train_time:750072ms step_avg:170.08ms\nstep:4421/10000 train_loss:4.0532 train_time:750244ms step_avg:170.08ms\nstep:4422/10000 train_loss:4.0200 train_time:750414ms step_avg:170.08ms\nstep:4423/10000 train_loss:4.0071 train_time:750586ms step_avg:170.09ms\nstep:4424/10000 train_loss:3.8855 train_time:750755ms step_avg:170.09ms\nstep:4425/10000 train_loss:4.2299 train_time:750926ms step_avg:170.09ms\nstep:4426/10000 train_loss:4.1140 train_time:751097ms step_avg:170.09ms\nstep:4427/10000 train_loss:4.2558 train_time:751267ms step_avg:170.09ms\nstep:4428/10000 train_loss:4.6151 train_time:751436ms step_avg:170.09ms\nstep:4429/10000 train_loss:4.0116 train_time:751610ms step_avg:170.09ms\nstep:4430/10000 train_loss:3.9426 train_time:751780ms step_avg:170.09ms\nstep:4431/10000 train_loss:4.0659 train_time:751948ms step_avg:170.09ms\nstep:4432/10000 train_loss:3.7898 train_time:752120ms step_avg:170.09ms\nstep:4433/10000 train_loss:4.0538 train_time:752291ms step_avg:170.09ms\nstep:4434/10000 train_loss:4.1817 train_time:752462ms step_avg:170.09ms\nstep:4435/10000 train_loss:4.0743 train_time:752631ms step_avg:170.09ms\nstep:4436/10000 train_loss:4.1118 train_time:752800ms step_avg:170.09ms\nstep:4437/10000 train_loss:4.0688 train_time:752969ms step_avg:170.09ms\nstep:4438/10000 train_loss:3.8892 train_time:753139ms step_avg:170.09ms\nstep:4439/10000 train_loss:4.1977 train_time:753306ms step_avg:170.08ms\nstep:4440/10000 train_loss:3.9238 train_time:753477ms step_avg:170.09ms\nstep:4441/10000 train_loss:3.9392 train_time:753646ms step_avg:170.08ms\nstep:4442/10000 train_loss:4.0209 train_time:753814ms step_avg:170.08ms\nstep:4443/10000 train_loss:4.1160 train_time:753984ms step_avg:170.08ms\nstep:4444/10000 train_loss:4.0376 train_time:754154ms step_avg:170.08ms\nstep:4445/10000 train_loss:3.9386 train_time:754328ms step_avg:170.09ms\nstep:4446/10000 train_loss:4.0423 train_time:754493ms step_avg:170.08ms\nstep:4447/10000 train_loss:3.8822 train_time:754664ms step_avg:170.08ms\nstep:4448/10000 train_loss:4.0016 train_time:754832ms step_avg:170.08ms\nstep:4449/10000 train_loss:3.9487 train_time:755004ms step_avg:170.08ms\nstep:4450/10000 train_loss:4.0108 train_time:755173ms step_avg:170.08ms\nstep:4451/10000 train_loss:3.9435 train_time:755345ms step_avg:170.08ms\nstep:4452/10000 train_loss:4.0229 train_time:755516ms step_avg:170.08ms\nstep:4453/10000 train_loss:3.8144 train_time:755687ms step_avg:170.08ms\nstep:4454/10000 train_loss:3.9876 train_time:755857ms step_avg:170.08ms\nstep:4455/10000 train_loss:3.8584 train_time:756028ms step_avg:170.09ms\nstep:4456/10000 train_loss:4.1177 train_time:756199ms step_avg:170.09ms\nstep:4457/10000 train_loss:4.0771 train_time:756369ms step_avg:170.09ms\nstep:4458/10000 train_loss:3.8885 train_time:756543ms step_avg:170.09ms\nstep:4459/10000 train_loss:4.0034 train_time:756714ms step_avg:170.09ms\nstep:4460/10000 train_loss:3.9835 train_time:756884ms step_avg:170.09ms\nstep:4461/10000 train_loss:3.9511 train_time:757054ms step_avg:170.09ms\nstep:4462/10000 train_loss:4.0841 train_time:757225ms step_avg:170.09ms\nstep:4463/10000 train_loss:4.0983 train_time:757393ms step_avg:170.09ms\nstep:4464/10000 train_loss:3.9122 train_time:757565ms step_avg:170.09ms\nstep:4465/10000 train_loss:4.0631 train_time:757734ms step_avg:170.09ms\nstep:4466/10000 train_loss:4.0717 train_time:757906ms step_avg:170.09ms\nstep:4467/10000 train_loss:4.2154 train_time:758073ms step_avg:170.09ms\nstep:4468/10000 train_loss:3.8454 train_time:758244ms step_avg:170.09ms\nstep:4469/10000 train_loss:3.9970 train_time:758417ms step_avg:170.09ms\nstep:4470/10000 train_loss:3.9005 train_time:758580ms step_avg:170.09ms\nstep:4471/10000 train_loss:3.9144 train_time:758750ms step_avg:170.09ms\nstep:4472/10000 train_loss:3.9923 train_time:758927ms step_avg:170.09ms\nstep:4473/10000 train_loss:4.0849 train_time:759091ms step_avg:170.09ms\nstep:4474/10000 train_loss:4.1561 train_time:759260ms step_avg:170.09ms\nstep:4475/10000 train_loss:3.8625 train_time:759430ms step_avg:170.09ms\nstep:4476/10000 train_loss:4.1396 train_time:759601ms step_avg:170.09ms\nstep:4477/10000 train_loss:3.9617 train_time:759771ms step_avg:170.09ms\nstep:4478/10000 train_loss:3.9855 train_time:759941ms step_avg:170.09ms\nstep:4479/10000 train_loss:4.0951 train_time:760111ms step_avg:170.09ms\nstep:4480/10000 train_loss:4.1676 train_time:760282ms step_avg:170.09ms\nstep:4481/10000 train_loss:4.0020 train_time:760451ms step_avg:170.09ms\nstep:4482/10000 train_loss:3.9822 train_time:760623ms step_avg:170.09ms\nstep:4483/10000 train_loss:3.9739 train_time:760793ms step_avg:170.09ms\nstep:4484/10000 train_loss:4.0168 train_time:760962ms step_avg:170.09ms\nstep:4485/10000 train_loss:4.0737 train_time:761130ms step_avg:170.08ms\nstep:4486/10000 train_loss:4.0516 train_time:761298ms step_avg:170.08ms\nstep:4487/10000 train_loss:3.9499 train_time:761466ms step_avg:170.08ms\nstep:4488/10000 train_loss:4.0450 train_time:761636ms step_avg:170.08ms\nstep:4489/10000 train_loss:3.9141 train_time:761807ms step_avg:170.08ms\nstep:4490/10000 train_loss:4.2340 train_time:761977ms step_avg:170.08ms\nstep:4491/10000 train_loss:4.0787 train_time:762149ms step_avg:170.08ms\nstep:4492/10000 train_loss:3.9490 train_time:762320ms step_avg:170.08ms\nstep:4493/10000 train_loss:4.0616 train_time:762490ms step_avg:170.08ms\nstep:4494/10000 train_loss:3.9499 train_time:762658ms step_avg:170.08ms\nstep:4495/10000 train_loss:3.9438 train_time:762829ms step_avg:170.08ms\nstep:4496/10000 train_loss:3.9974 train_time:762999ms step_avg:170.08ms\nstep:4497/10000 train_loss:4.0126 train_time:763169ms step_avg:170.08ms\nstep:4498/10000 train_loss:3.9526 train_time:763342ms step_avg:170.09ms\nstep:4499/10000 train_loss:3.9811 train_time:763512ms step_avg:170.09ms\nstep:4500/10000 train_loss:4.1314 train_time:763683ms step_avg:170.09ms\nstep:4501/10000 train_loss:3.9135 train_time:763853ms step_avg:170.09ms\nstep:4502/10000 train_loss:3.9579 train_time:764023ms step_avg:170.09ms\nstep:4503/10000 train_loss:4.0295 train_time:764190ms step_avg:170.08ms\nstep:4504/10000 train_loss:3.9259 train_time:764360ms step_avg:170.08ms\nstep:4505/10000 train_loss:4.0684 train_time:764531ms step_avg:170.08ms\nstep:4506/10000 train_loss:4.0136 train_time:764701ms step_avg:170.08ms\nstep:4507/10000 train_loss:3.9485 train_time:764872ms step_avg:170.08ms\nstep:4508/10000 train_loss:3.8126 train_time:765044ms step_avg:170.09ms\nstep:4509/10000 train_loss:4.0419 train_time:765211ms step_avg:170.08ms\nstep:4510/10000 train_loss:4.1042 train_time:765381ms step_avg:170.08ms\nstep:4511/10000 train_loss:4.1878 train_time:765551ms step_avg:170.08ms\nstep:4512/10000 train_loss:4.0423 train_time:765721ms step_avg:170.08ms\nstep:4513/10000 train_loss:3.9041 train_time:765892ms step_avg:170.08ms\nstep:4514/10000 train_loss:3.8817 train_time:766070ms step_avg:170.09ms\nstep:4515/10000 train_loss:3.9694 train_time:766239ms step_avg:170.09ms\nstep:4516/10000 train_loss:4.2537 train_time:766411ms step_avg:170.09ms\nstep:4517/10000 train_loss:3.9739 train_time:766580ms step_avg:170.09ms\nstep:4518/10000 train_loss:3.9318 train_time:766741ms step_avg:170.08ms\nstep:4519/10000 train_loss:4.1523 train_time:766911ms step_avg:170.08ms\nstep:4520/10000 train_loss:3.7900 train_time:767082ms step_avg:170.08ms\nstep:4521/10000 train_loss:3.8699 train_time:767252ms step_avg:170.08ms\nstep:4522/10000 train_loss:4.0384 train_time:767425ms step_avg:170.09ms\nstep:4523/10000 train_loss:3.9563 train_time:767592ms step_avg:170.08ms\nstep:4524/10000 train_loss:4.1038 train_time:767761ms step_avg:170.08ms\nstep:4525/10000 train_loss:4.0386 train_time:767932ms step_avg:170.08ms\nstep:4526/10000 train_loss:3.9030 train_time:768102ms step_avg:170.08ms\nstep:4527/10000 train_loss:4.0071 train_time:768271ms step_avg:170.08ms\nstep:4528/10000 train_loss:3.9535 train_time:768442ms step_avg:170.08ms\nstep:4529/10000 train_loss:3.9602 train_time:768609ms step_avg:170.08ms\nstep:4530/10000 train_loss:4.0524 train_time:768778ms step_avg:170.08ms\nstep:4531/10000 train_loss:3.8362 train_time:768947ms step_avg:170.08ms\nstep:4532/10000 train_loss:3.9539 train_time:769115ms step_avg:170.08ms\nstep:4533/10000 train_loss:4.1296 train_time:769287ms step_avg:170.08ms\nstep:4534/10000 train_loss:3.9752 train_time:769456ms step_avg:170.08ms\nstep:4535/10000 train_loss:3.8470 train_time:769630ms step_avg:170.08ms\nstep:4536/10000 train_loss:3.9300 train_time:769799ms step_avg:170.08ms\nstep:4537/10000 train_loss:4.0573 train_time:769968ms step_avg:170.08ms\nstep:4538/10000 train_loss:3.9923 train_time:770137ms step_avg:170.08ms\nstep:4539/10000 train_loss:3.9750 train_time:770309ms step_avg:170.08ms\nstep:4540/10000 train_loss:4.1063 train_time:770480ms step_avg:170.08ms\nstep:4541/10000 train_loss:3.9563 train_time:770650ms step_avg:170.08ms\nstep:4542/10000 train_loss:3.9278 train_time:770823ms step_avg:170.08ms\nstep:4543/10000 train_loss:3.8714 train_time:770993ms step_avg:170.08ms\nstep:4544/10000 train_loss:4.0246 train_time:771163ms step_avg:170.08ms\nstep:4545/10000 train_loss:4.0842 train_time:771331ms step_avg:170.08ms\nstep:4546/10000 train_loss:4.1937 train_time:771501ms step_avg:170.08ms\nstep:4547/10000 train_loss:3.9292 train_time:771671ms step_avg:170.08ms\nstep:4548/10000 train_loss:4.0585 train_time:771844ms step_avg:170.08ms\nstep:4549/10000 train_loss:4.0829 train_time:772013ms step_avg:170.08ms\nstep:4550/10000 train_loss:4.0801 train_time:772184ms step_avg:170.08ms\nstep:4551/10000 train_loss:4.0320 train_time:772357ms step_avg:170.09ms\nstep:4552/10000 train_loss:4.0808 train_time:772527ms step_avg:170.09ms\nstep:4553/10000 train_loss:3.9059 train_time:772699ms step_avg:170.09ms\nstep:4554/10000 train_loss:3.9080 train_time:772872ms step_avg:170.09ms\nstep:4555/10000 train_loss:3.8843 train_time:773043ms step_avg:170.09ms\nstep:4556/10000 train_loss:4.0126 train_time:773213ms step_avg:170.09ms\nstep:4557/10000 train_loss:4.0346 train_time:773384ms step_avg:170.09ms\nstep:4558/10000 train_loss:4.0432 train_time:773551ms step_avg:170.09ms\nstep:4559/10000 train_loss:3.9806 train_time:773720ms step_avg:170.09ms\nstep:4560/10000 train_loss:3.9188 train_time:773888ms step_avg:170.09ms\nstep:4561/10000 train_loss:3.8673 train_time:774059ms step_avg:170.09ms\nstep:4562/10000 train_loss:3.9525 train_time:774227ms step_avg:170.09ms\nstep:4563/10000 train_loss:3.9522 train_time:774400ms step_avg:170.09ms\nstep:4564/10000 train_loss:4.1973 train_time:774567ms step_avg:170.09ms\nstep:4565/10000 train_loss:3.9007 train_time:774740ms step_avg:170.09ms\nstep:4566/10000 train_loss:3.9805 train_time:774910ms step_avg:170.09ms\nstep:4567/10000 train_loss:4.0320 train_time:775080ms step_avg:170.09ms\nstep:4568/10000 train_loss:4.0163 train_time:775249ms step_avg:170.09ms\nstep:4569/10000 train_loss:3.8663 train_time:775419ms step_avg:170.09ms\nstep:4570/10000 train_loss:4.0220 train_time:775590ms step_avg:170.09ms\nstep:4571/10000 train_loss:3.8998 train_time:775763ms step_avg:170.09ms\nstep:4572/10000 train_loss:4.1243 train_time:776150ms step_avg:170.13ms\nstep:4573/10000 train_loss:3.9433 train_time:776314ms step_avg:170.13ms\nstep:4574/10000 train_loss:4.0746 train_time:776482ms step_avg:170.13ms\nstep:4575/10000 train_loss:3.9404 train_time:776650ms step_avg:170.13ms\nstep:4576/10000 train_loss:4.1272 train_time:776820ms step_avg:170.13ms\nstep:4577/10000 train_loss:4.0952 train_time:776988ms step_avg:170.13ms\nstep:4578/10000 train_loss:4.0213 train_time:777168ms step_avg:170.13ms\nstep:4579/10000 train_loss:3.9783 train_time:777338ms step_avg:170.13ms\nstep:4580/10000 train_loss:3.8417 train_time:777507ms step_avg:170.13ms\nstep:4581/10000 train_loss:4.0928 train_time:777677ms step_avg:170.13ms\nstep:4582/10000 train_loss:3.9420 train_time:777843ms step_avg:170.13ms\nstep:4583/10000 train_loss:3.9795 train_time:778012ms step_avg:170.13ms\nstep:4584/10000 train_loss:4.0898 train_time:778187ms step_avg:170.13ms\nstep:4585/10000 train_loss:3.9382 train_time:778358ms step_avg:170.13ms\nstep:4586/10000 train_loss:4.0270 train_time:778528ms step_avg:170.13ms\nstep:4587/10000 train_loss:3.8940 train_time:778696ms step_avg:170.13ms\nstep:4588/10000 train_loss:3.8998 train_time:778867ms step_avg:170.13ms\nstep:4589/10000 train_loss:4.0355 train_time:779036ms step_avg:170.13ms\nstep:4590/10000 train_loss:4.1546 train_time:779204ms step_avg:170.13ms\nstep:4591/10000 train_loss:3.9386 train_time:779375ms step_avg:170.13ms\nstep:4592/10000 train_loss:3.9436 train_time:779546ms step_avg:170.13ms\nstep:4593/10000 train_loss:4.0128 train_time:779715ms step_avg:170.13ms\nstep:4594/10000 train_loss:4.0813 train_time:779885ms step_avg:170.13ms\nstep:4595/10000 train_loss:4.2991 train_time:780054ms step_avg:170.13ms\nstep:4596/10000 train_loss:4.0238 train_time:780224ms step_avg:170.13ms\nstep:4597/10000 train_loss:3.9732 train_time:780392ms step_avg:170.13ms\nstep:4598/10000 train_loss:4.0150 train_time:780561ms step_avg:170.13ms\nstep:4599/10000 train_loss:4.1129 train_time:780731ms step_avg:170.13ms\nstep:4600/10000 train_loss:4.0789 train_time:780903ms step_avg:170.13ms\nstep:4601/10000 train_loss:3.9384 train_time:781073ms step_avg:170.13ms\nstep:4602/10000 train_loss:4.0576 train_time:781244ms step_avg:170.13ms\nstep:4603/10000 train_loss:4.1049 train_time:781413ms step_avg:170.13ms\nstep:4604/10000 train_loss:4.0394 train_time:781585ms step_avg:170.13ms\nstep:4605/10000 train_loss:3.9836 train_time:781756ms step_avg:170.13ms\nstep:4606/10000 train_loss:3.9158 train_time:781927ms step_avg:170.13ms\nstep:4607/10000 train_loss:3.9359 train_time:782097ms step_avg:170.13ms\nstep:4608/10000 train_loss:3.9519 train_time:782267ms step_avg:170.13ms\nstep:4609/10000 train_loss:3.8964 train_time:782437ms step_avg:170.13ms\nstep:4610/10000 train_loss:3.9035 train_time:782606ms step_avg:170.13ms\nstep:4611/10000 train_loss:4.0591 train_time:782775ms step_avg:170.13ms\nstep:4612/10000 train_loss:3.8939 train_time:782954ms step_avg:170.13ms\nstep:4613/10000 train_loss:4.0067 train_time:783114ms step_avg:170.13ms\nstep:4614/10000 train_loss:3.9016 train_time:783283ms step_avg:170.13ms\nstep:4615/10000 train_loss:3.9541 train_time:783453ms step_avg:170.13ms\nstep:4616/10000 train_loss:4.1622 train_time:783623ms step_avg:170.13ms\nstep:4617/10000 train_loss:3.7892 train_time:783794ms step_avg:170.13ms\nstep:4618/10000 train_loss:4.0883 train_time:783966ms step_avg:170.13ms\nstep:4619/10000 train_loss:4.1155 train_time:784135ms step_avg:170.13ms\nstep:4620/10000 train_loss:4.1561 train_time:784305ms step_avg:170.13ms\nstep:4621/10000 train_loss:4.2105 train_time:784473ms step_avg:170.13ms\nstep:4622/10000 train_loss:4.0233 train_time:784644ms step_avg:170.13ms\nstep:4623/10000 train_loss:3.8587 train_time:784812ms step_avg:170.13ms\nstep:4624/10000 train_loss:3.8423 train_time:784985ms step_avg:170.13ms\nstep:4625/10000 train_loss:4.0591 train_time:785156ms step_avg:170.13ms\nstep:4626/10000 train_loss:4.0562 train_time:785326ms step_avg:170.13ms\nstep:4627/10000 train_loss:3.8797 train_time:785494ms step_avg:170.13ms\nstep:4628/10000 train_loss:3.8574 train_time:785666ms step_avg:170.13ms\nstep:4629/10000 train_loss:4.0755 train_time:785836ms step_avg:170.13ms\nstep:4630/10000 train_loss:3.9457 train_time:786008ms step_avg:170.13ms\nstep:4631/10000 train_loss:3.9780 train_time:786176ms step_avg:170.13ms\nstep:4632/10000 train_loss:3.9986 train_time:786348ms step_avg:170.13ms\nstep:4633/10000 train_loss:3.9704 train_time:786535ms step_avg:170.14ms\nstep:4634/10000 train_loss:4.0838 train_time:786693ms step_avg:170.13ms\nstep:4635/10000 train_loss:4.0474 train_time:786861ms step_avg:170.13ms\nstep:4636/10000 train_loss:4.0717 train_time:787031ms step_avg:170.13ms\nstep:4637/10000 train_loss:4.0835 train_time:787203ms step_avg:170.13ms\nstep:4638/10000 train_loss:4.0551 train_time:787371ms step_avg:170.13ms\nstep:4639/10000 train_loss:3.8913 train_time:787543ms step_avg:170.13ms\nstep:4640/10000 train_loss:3.7928 train_time:787712ms step_avg:170.13ms\nstep:4641/10000 train_loss:4.0637 train_time:787883ms step_avg:170.13ms\nstep:4642/10000 train_loss:3.9107 train_time:788054ms step_avg:170.13ms\nstep:4643/10000 train_loss:4.3296 train_time:788229ms step_avg:170.13ms\nstep:4644/10000 train_loss:3.8638 train_time:788398ms step_avg:170.13ms\nstep:4645/10000 train_loss:4.0386 train_time:788568ms step_avg:170.13ms\nstep:4646/10000 train_loss:3.8765 train_time:788739ms step_avg:170.13ms\nstep:4647/10000 train_loss:4.0617 train_time:788909ms step_avg:170.13ms\nstep:4648/10000 train_loss:3.8619 train_time:789078ms step_avg:170.13ms\nstep:4649/10000 train_loss:3.9014 train_time:789249ms step_avg:170.13ms\nstep:4650/10000 train_loss:3.8803 train_time:789422ms step_avg:170.13ms\nstep:4651/10000 train_loss:3.9203 train_time:789590ms step_avg:170.13ms\nstep:4652/10000 train_loss:4.0438 train_time:789762ms step_avg:170.13ms\nstep:4653/10000 train_loss:4.0044 train_time:789931ms step_avg:170.13ms\nstep:4654/10000 train_loss:3.7024 train_time:790100ms step_avg:170.13ms\nstep:4655/10000 train_loss:3.8985 train_time:790276ms step_avg:170.13ms\nstep:4656/10000 train_loss:3.8377 train_time:790444ms step_avg:170.13ms\nstep:4657/10000 train_loss:3.8595 train_time:790611ms step_avg:170.13ms\nstep:4658/10000 train_loss:4.0189 train_time:790782ms step_avg:170.13ms\nstep:4659/10000 train_loss:3.9981 train_time:790951ms step_avg:170.13ms\nstep:4660/10000 train_loss:3.9839 train_time:791119ms step_avg:170.13ms\nstep:4661/10000 train_loss:4.0462 train_time:791286ms step_avg:170.13ms\nstep:4662/10000 train_loss:3.9800 train_time:791456ms step_avg:170.13ms\nstep:4663/10000 train_loss:3.8329 train_time:791627ms step_avg:170.13ms\nstep:4664/10000 train_loss:4.0362 train_time:791798ms step_avg:170.13ms\nstep:4665/10000 train_loss:4.0115 train_time:791966ms step_avg:170.13ms\nstep:4666/10000 train_loss:3.8447 train_time:792135ms step_avg:170.13ms\nstep:4667/10000 train_loss:4.1399 train_time:792304ms step_avg:170.13ms\nstep:4668/10000 train_loss:3.9899 train_time:792472ms step_avg:170.13ms\nstep:4669/10000 train_loss:3.9086 train_time:792645ms step_avg:170.13ms\nstep:4670/10000 train_loss:3.8588 train_time:792814ms step_avg:170.13ms\nstep:4671/10000 train_loss:4.0285 train_time:792985ms step_avg:170.13ms\nstep:4672/10000 train_loss:3.9432 train_time:793153ms step_avg:170.13ms\nstep:4673/10000 train_loss:3.9962 train_time:793323ms step_avg:170.13ms\nstep:4674/10000 train_loss:3.9283 train_time:793491ms step_avg:170.13ms\nstep:4675/10000 train_loss:3.9839 train_time:793665ms step_avg:170.13ms\nstep:4676/10000 train_loss:3.9491 train_time:793836ms step_avg:170.13ms\nstep:4677/10000 train_loss:4.0521 train_time:794008ms step_avg:170.13ms\nstep:4678/10000 train_loss:3.9153 train_time:794180ms step_avg:170.13ms\nstep:4679/10000 train_loss:3.9389 train_time:794351ms step_avg:170.13ms\nstep:4680/10000 train_loss:4.1339 train_time:794519ms step_avg:170.13ms\nstep:4681/10000 train_loss:3.9716 train_time:794689ms step_avg:170.13ms\nstep:4682/10000 train_loss:4.2391 train_time:794859ms step_avg:170.13ms\nstep:4683/10000 train_loss:3.9799 train_time:795029ms step_avg:170.13ms\nstep:4684/10000 train_loss:3.9411 train_time:795199ms step_avg:170.13ms\nstep:4685/10000 train_loss:4.0128 train_time:795369ms step_avg:170.13ms\nstep:4686/10000 train_loss:3.9490 train_time:795544ms step_avg:170.13ms\nstep:4687/10000 train_loss:3.9213 train_time:795705ms step_avg:170.13ms\nstep:4688/10000 train_loss:4.0358 train_time:795874ms step_avg:170.13ms\nstep:4689/10000 train_loss:3.8994 train_time:796043ms step_avg:170.13ms\nstep:4690/10000 train_loss:3.9624 train_time:796212ms step_avg:170.13ms\nstep:4691/10000 train_loss:3.9618 train_time:796382ms step_avg:170.13ms\nstep:4692/10000 train_loss:4.0052 train_time:796552ms step_avg:170.13ms\nstep:4693/10000 train_loss:3.9951 train_time:796723ms step_avg:170.13ms\nstep:4694/10000 train_loss:3.9075 train_time:796892ms step_avg:170.13ms\nstep:4695/10000 train_loss:3.8461 train_time:797061ms step_avg:170.13ms\nstep:4696/10000 train_loss:3.8848 train_time:797230ms step_avg:170.13ms\nstep:4697/10000 train_loss:3.8067 train_time:797400ms step_avg:170.13ms\nstep:4698/10000 train_loss:3.9442 train_time:797568ms step_avg:170.13ms\nstep:4699/10000 train_loss:4.1501 train_time:797737ms step_avg:170.13ms\nstep:4700/10000 train_loss:3.9796 train_time:797909ms step_avg:170.13ms\nstep:4701/10000 train_loss:3.9994 train_time:798087ms step_avg:170.13ms\nstep:4702/10000 train_loss:4.1044 train_time:798248ms step_avg:170.13ms\nstep:4703/10000 train_loss:3.9860 train_time:798419ms step_avg:170.13ms\nstep:4704/10000 train_loss:4.3148 train_time:798589ms step_avg:170.13ms\nstep:4705/10000 train_loss:4.0794 train_time:798758ms step_avg:170.13ms\nstep:4706/10000 train_loss:3.9408 train_time:798928ms step_avg:170.13ms\nstep:4707/10000 train_loss:3.9074 train_time:799098ms step_avg:170.13ms\nstep:4708/10000 train_loss:3.9334 train_time:799269ms step_avg:170.13ms\nstep:4709/10000 train_loss:3.9775 train_time:799437ms step_avg:170.13ms\nstep:4710/10000 train_loss:3.9736 train_time:799607ms step_avg:170.13ms\nstep:4711/10000 train_loss:3.9475 train_time:799776ms step_avg:170.13ms\nstep:4712/10000 train_loss:3.8872 train_time:799948ms step_avg:170.13ms\nstep:4713/10000 train_loss:3.9288 train_time:800117ms step_avg:170.13ms\nstep:4714/10000 train_loss:4.0188 train_time:800288ms step_avg:170.13ms\nstep:4715/10000 train_loss:4.0695 train_time:800459ms step_avg:170.13ms\nstep:4716/10000 train_loss:3.8844 train_time:800629ms step_avg:170.13ms\nstep:4717/10000 train_loss:4.0167 train_time:800799ms step_avg:170.13ms\nstep:4718/10000 train_loss:3.9907 train_time:800970ms step_avg:170.13ms\nstep:4719/10000 train_loss:4.0220 train_time:801141ms step_avg:170.13ms\nstep:4720/10000 train_loss:3.9910 train_time:801310ms step_avg:170.13ms\nstep:4721/10000 train_loss:3.9427 train_time:801482ms step_avg:170.13ms\nstep:4722/10000 train_loss:4.0323 train_time:801652ms step_avg:170.13ms\nstep:4723/10000 train_loss:3.9136 train_time:801823ms step_avg:170.13ms\nstep:4724/10000 train_loss:4.0314 train_time:801991ms step_avg:170.13ms\nstep:4725/10000 train_loss:3.8939 train_time:802163ms step_avg:170.13ms\nstep:4726/10000 train_loss:3.9682 train_time:802331ms step_avg:170.13ms\nstep:4727/10000 train_loss:4.0880 train_time:802501ms step_avg:170.13ms\nstep:4728/10000 train_loss:3.9578 train_time:802670ms step_avg:170.13ms\nstep:4729/10000 train_loss:3.9232 train_time:802840ms step_avg:170.13ms\nstep:4730/10000 train_loss:3.9883 train_time:803009ms step_avg:170.13ms\nstep:4731/10000 train_loss:3.9882 train_time:803178ms step_avg:170.13ms\nstep:4732/10000 train_loss:3.8250 train_time:803348ms step_avg:170.13ms\nstep:4733/10000 train_loss:3.9237 train_time:803519ms step_avg:170.13ms\nstep:4734/10000 train_loss:3.9069 train_time:803688ms step_avg:170.13ms\nstep:4735/10000 train_loss:4.0006 train_time:803859ms step_avg:170.13ms\nstep:4736/10000 train_loss:3.9569 train_time:804032ms step_avg:170.13ms\nstep:4737/10000 train_loss:3.8849 train_time:804201ms step_avg:170.13ms\nstep:4738/10000 train_loss:3.9677 train_time:804373ms step_avg:170.13ms\nstep:4739/10000 train_loss:4.0578 train_time:804543ms step_avg:170.13ms\nstep:4740/10000 train_loss:4.0916 train_time:804713ms step_avg:170.13ms\nstep:4741/10000 train_loss:4.0133 train_time:804885ms step_avg:170.13ms\nstep:4742/10000 train_loss:3.8943 train_time:805053ms step_avg:170.13ms\nstep:4743/10000 train_loss:3.9461 train_time:805222ms step_avg:170.13ms\nstep:4744/10000 train_loss:4.2156 train_time:805390ms step_avg:170.13ms\nstep:4745/10000 train_loss:3.8872 train_time:805559ms step_avg:170.13ms\nstep:4746/10000 train_loss:4.0511 train_time:805730ms step_avg:170.13ms\nstep:4747/10000 train_loss:3.9030 train_time:805900ms step_avg:170.13ms\nstep:4748/10000 train_loss:4.0696 train_time:806072ms step_avg:170.13ms\nstep:4749/10000 train_loss:3.8283 train_time:806245ms step_avg:170.13ms\nstep:4750/10000 train_loss:4.1130 train_time:806414ms step_avg:170.13ms\nstep:4751/10000 train_loss:4.0893 train_time:806584ms step_avg:170.13ms\nstep:4752/10000 train_loss:3.9394 train_time:806756ms step_avg:170.13ms\nstep:4753/10000 train_loss:3.8965 train_time:806927ms step_avg:170.13ms\nstep:4754/10000 train_loss:3.8898 train_time:807097ms step_avg:170.13ms\nstep:4755/10000 train_loss:3.9812 train_time:807265ms step_avg:170.13ms\nstep:4756/10000 train_loss:3.8182 train_time:807434ms step_avg:170.13ms\nstep:4757/10000 train_loss:3.9518 train_time:807603ms step_avg:170.13ms\nstep:4758/10000 train_loss:3.9613 train_time:807772ms step_avg:170.13ms\nstep:4759/10000 train_loss:4.0074 train_time:807943ms step_avg:170.13ms\nstep:4760/10000 train_loss:3.9127 train_time:808112ms step_avg:170.13ms\nstep:4761/10000 train_loss:3.9290 train_time:808281ms step_avg:170.13ms\nstep:4762/10000 train_loss:3.9974 train_time:808450ms step_avg:170.13ms\nstep:4763/10000 train_loss:4.1934 train_time:808619ms step_avg:170.13ms\nstep:4764/10000 train_loss:3.9598 train_time:808789ms step_avg:170.13ms\nstep:4765/10000 train_loss:3.8369 train_time:808961ms step_avg:170.13ms\nstep:4766/10000 train_loss:3.8371 train_time:809133ms step_avg:170.13ms\nstep:4767/10000 train_loss:4.1526 train_time:809303ms step_avg:170.13ms\nstep:4768/10000 train_loss:4.0023 train_time:809471ms step_avg:170.13ms\nstep:4769/10000 train_loss:3.9976 train_time:809641ms step_avg:170.13ms\nstep:4770/10000 train_loss:4.0150 train_time:809810ms step_avg:170.13ms\nstep:4771/10000 train_loss:3.8420 train_time:809981ms step_avg:170.13ms\nstep:4772/10000 train_loss:3.9762 train_time:810150ms step_avg:170.13ms\nstep:4773/10000 train_loss:4.1226 train_time:810321ms step_avg:170.13ms\nstep:4774/10000 train_loss:4.1365 train_time:810490ms step_avg:170.13ms\nstep:4775/10000 train_loss:4.1421 train_time:810660ms step_avg:170.13ms\nstep:4776/10000 train_loss:4.0686 train_time:810830ms step_avg:170.13ms\nstep:4777/10000 train_loss:3.9531 train_time:811001ms step_avg:170.13ms\nstep:4778/10000 train_loss:3.9572 train_time:811171ms step_avg:170.13ms\nstep:4779/10000 train_loss:3.9228 train_time:811344ms step_avg:170.13ms\nstep:4780/10000 train_loss:3.8879 train_time:811513ms step_avg:170.13ms\nstep:4781/10000 train_loss:4.4609 train_time:811683ms step_avg:170.13ms\nstep:4782/10000 train_loss:3.8434 train_time:811855ms step_avg:170.13ms\nstep:4783/10000 train_loss:3.9131 train_time:812022ms step_avg:170.13ms\nstep:4784/10000 train_loss:3.9222 train_time:812194ms step_avg:170.13ms\nstep:4785/10000 train_loss:3.9342 train_time:812363ms step_avg:170.13ms\nstep:4786/10000 train_loss:4.1508 train_time:812530ms step_avg:170.13ms\nstep:4787/10000 train_loss:3.9010 train_time:812704ms step_avg:170.13ms\nstep:4788/10000 train_loss:3.9064 train_time:812869ms step_avg:170.13ms\nstep:4789/10000 train_loss:3.8041 train_time:813039ms step_avg:170.13ms\nstep:4790/10000 train_loss:3.9259 train_time:813211ms step_avg:170.13ms\nstep:4791/10000 train_loss:3.9491 train_time:813381ms step_avg:170.13ms\nstep:4792/10000 train_loss:3.9639 train_time:813551ms step_avg:170.13ms\nstep:4793/10000 train_loss:3.7680 train_time:813722ms step_avg:170.13ms\nstep:4794/10000 train_loss:3.9277 train_time:813889ms step_avg:170.13ms\nstep:4795/10000 train_loss:4.0630 train_time:814062ms step_avg:170.13ms\nstep:4796/10000 train_loss:3.9622 train_time:814234ms step_avg:170.13ms\nstep:4797/10000 train_loss:3.9730 train_time:814404ms step_avg:170.13ms\nstep:4798/10000 train_loss:3.8400 train_time:814572ms step_avg:170.13ms\nstep:4799/10000 train_loss:3.8584 train_time:814749ms step_avg:170.13ms\nstep:4800/10000 train_loss:3.9058 train_time:814912ms step_avg:170.13ms\nstep:4801/10000 train_loss:3.8085 train_time:815082ms step_avg:170.13ms\nstep:4802/10000 train_loss:3.8162 train_time:815252ms step_avg:170.13ms\nstep:4803/10000 train_loss:3.8815 train_time:815424ms step_avg:170.13ms\nstep:4804/10000 train_loss:4.3142 train_time:815595ms step_avg:170.13ms\nstep:4805/10000 train_loss:3.9559 train_time:815766ms step_avg:170.13ms\nstep:4806/10000 train_loss:3.9005 train_time:815937ms step_avg:170.13ms\nstep:4807/10000 train_loss:4.0070 train_time:816108ms step_avg:170.13ms\nstep:4808/10000 train_loss:3.8777 train_time:816279ms step_avg:170.13ms\nstep:4809/10000 train_loss:3.9072 train_time:816450ms step_avg:170.13ms\nstep:4810/10000 train_loss:3.9617 train_time:816619ms step_avg:170.13ms\nstep:4811/10000 train_loss:3.8736 train_time:816792ms step_avg:170.13ms\nstep:4812/10000 train_loss:3.9561 train_time:816962ms step_avg:170.13ms\nstep:4813/10000 train_loss:3.8542 train_time:817128ms step_avg:170.13ms\nstep:4814/10000 train_loss:4.0686 train_time:817298ms step_avg:170.13ms\nstep:4815/10000 train_loss:3.9810 train_time:817467ms step_avg:170.13ms\nstep:4816/10000 train_loss:3.9386 train_time:817638ms step_avg:170.13ms\nstep:4817/10000 train_loss:3.7711 train_time:817809ms step_avg:170.13ms\nstep:4818/10000 train_loss:4.0365 train_time:817977ms step_avg:170.13ms\nstep:4819/10000 train_loss:3.8715 train_time:818146ms step_avg:170.13ms\nstep:4820/10000 train_loss:4.0455 train_time:818315ms step_avg:170.13ms\nstep:4821/10000 train_loss:3.9177 train_time:818483ms step_avg:170.13ms\nstep:4822/10000 train_loss:3.9872 train_time:818650ms step_avg:170.13ms\nstep:4823/10000 train_loss:3.8828 train_time:818825ms step_avg:170.13ms\nstep:4824/10000 train_loss:4.0321 train_time:818996ms step_avg:170.13ms\nstep:4825/10000 train_loss:4.0486 train_time:819165ms step_avg:170.13ms\nstep:4826/10000 train_loss:3.9314 train_time:819334ms step_avg:170.13ms\nstep:4827/10000 train_loss:3.8507 train_time:819506ms step_avg:170.13ms\nstep:4828/10000 train_loss:3.9880 train_time:819676ms step_avg:170.13ms\nstep:4829/10000 train_loss:4.1280 train_time:819844ms step_avg:170.13ms\nstep:4830/10000 train_loss:3.9635 train_time:820013ms step_avg:170.13ms\nstep:4831/10000 train_loss:4.0443 train_time:820185ms step_avg:170.13ms\nstep:4832/10000 train_loss:3.9082 train_time:820355ms step_avg:170.13ms\nstep:4833/10000 train_loss:4.0903 train_time:820526ms step_avg:170.13ms\nstep:4834/10000 train_loss:4.0121 train_time:820696ms step_avg:170.13ms\nstep:4835/10000 train_loss:3.8744 train_time:820865ms step_avg:170.13ms\nstep:4836/10000 train_loss:3.9449 train_time:821037ms step_avg:170.13ms\nstep:4837/10000 train_loss:3.7994 train_time:821203ms step_avg:170.13ms\nstep:4838/10000 train_loss:4.0749 train_time:821372ms step_avg:170.13ms\nstep:4839/10000 train_loss:4.1342 train_time:821545ms step_avg:170.13ms\nstep:4840/10000 train_loss:3.8890 train_time:821712ms step_avg:170.13ms\nstep:4841/10000 train_loss:3.9060 train_time:821882ms step_avg:170.13ms\nstep:4842/10000 train_loss:3.9484 train_time:822050ms step_avg:170.13ms\nstep:4843/10000 train_loss:3.8965 train_time:822220ms step_avg:170.13ms\nstep:4844/10000 train_loss:3.9754 train_time:822389ms step_avg:170.13ms\nstep:4845/10000 train_loss:3.8912 train_time:822560ms step_avg:170.13ms\nstep:4846/10000 train_loss:3.7977 train_time:822729ms step_avg:170.13ms\nstep:4847/10000 train_loss:3.8799 train_time:822901ms step_avg:170.13ms\nstep:4848/10000 train_loss:4.2127 train_time:823070ms step_avg:170.13ms\nstep:4849/10000 train_loss:3.8735 train_time:823242ms step_avg:170.13ms\nstep:4850/10000 train_loss:3.8582 train_time:823409ms step_avg:170.13ms\nstep:4851/10000 train_loss:3.9974 train_time:823579ms step_avg:170.13ms\nstep:4852/10000 train_loss:3.8775 train_time:823749ms step_avg:170.13ms\nstep:4853/10000 train_loss:3.9748 train_time:823922ms step_avg:170.13ms\nstep:4854/10000 train_loss:3.9560 train_time:824091ms step_avg:170.13ms\nstep:4855/10000 train_loss:3.9436 train_time:824262ms step_avg:170.13ms\nstep:4856/10000 train_loss:4.1391 train_time:824432ms step_avg:170.13ms\nstep:4857/10000 train_loss:3.8582 train_time:824601ms step_avg:170.13ms\nstep:4858/10000 train_loss:3.9753 train_time:824772ms step_avg:170.13ms\nstep:4859/10000 train_loss:3.8421 train_time:824945ms step_avg:170.13ms\nstep:4860/10000 train_loss:3.9872 train_time:825113ms step_avg:170.13ms\nstep:4861/10000 train_loss:3.9153 train_time:825286ms step_avg:170.13ms\nstep:4862/10000 train_loss:4.0086 train_time:825460ms step_avg:170.13ms\nstep:4863/10000 train_loss:4.0291 train_time:825628ms step_avg:170.13ms\nstep:4864/10000 train_loss:4.0046 train_time:825795ms step_avg:170.13ms\nstep:4865/10000 train_loss:3.9548 train_time:825967ms step_avg:170.13ms\nstep:4866/10000 train_loss:3.9516 train_time:826139ms step_avg:170.13ms\nstep:4867/10000 train_loss:3.9908 train_time:826306ms step_avg:170.13ms\nstep:4868/10000 train_loss:3.9810 train_time:826478ms step_avg:170.13ms\nstep:4869/10000 train_loss:3.9115 train_time:826646ms step_avg:170.13ms\nstep:4870/10000 train_loss:3.9638 train_time:826817ms step_avg:170.13ms\nstep:4871/10000 train_loss:3.8929 train_time:826986ms step_avg:170.13ms\nstep:4872/10000 train_loss:3.9788 train_time:827157ms step_avg:170.13ms\nstep:4873/10000 train_loss:4.1259 train_time:827329ms step_avg:170.13ms\nstep:4874/10000 train_loss:3.8972 train_time:827501ms step_avg:170.13ms\nstep:4875/10000 train_loss:3.7800 train_time:827672ms step_avg:170.13ms\nstep:4876/10000 train_loss:4.0200 train_time:827841ms step_avg:170.13ms\nstep:4877/10000 train_loss:3.9285 train_time:828010ms step_avg:170.13ms\nstep:4878/10000 train_loss:3.9348 train_time:828180ms step_avg:170.13ms\nstep:4879/10000 train_loss:3.9970 train_time:828351ms step_avg:170.13ms\nstep:4880/10000 train_loss:3.9416 train_time:828522ms step_avg:170.13ms\nstep:4881/10000 train_loss:3.8491 train_time:828696ms step_avg:170.13ms\nstep:4882/10000 train_loss:3.9944 train_time:828863ms step_avg:170.13ms\nstep:4883/10000 train_loss:3.9769 train_time:829035ms step_avg:170.13ms\nstep:4884/10000 train_loss:3.9920 train_time:829206ms step_avg:170.13ms\nstep:4885/10000 train_loss:3.9222 train_time:829376ms step_avg:170.13ms\nstep:4886/10000 train_loss:3.8270 train_time:829546ms step_avg:170.13ms\nstep:4887/10000 train_loss:3.9982 train_time:829714ms step_avg:170.13ms\nstep:4888/10000 train_loss:3.9823 train_time:829884ms step_avg:170.13ms\nstep:4889/10000 train_loss:4.0422 train_time:830052ms step_avg:170.13ms\nstep:4890/10000 train_loss:4.0410 train_time:830221ms step_avg:170.13ms\nstep:4891/10000 train_loss:3.8827 train_time:830389ms step_avg:170.13ms\nstep:4892/10000 train_loss:4.0600 train_time:830560ms step_avg:170.13ms\nstep:4893/10000 train_loss:3.9743 train_time:830729ms step_avg:170.13ms\nstep:4894/10000 train_loss:4.0793 train_time:830900ms step_avg:170.13ms\nstep:4895/10000 train_loss:3.8815 train_time:831070ms step_avg:170.13ms\nstep:4896/10000 train_loss:4.0039 train_time:831239ms step_avg:170.13ms\nstep:4897/10000 train_loss:3.9555 train_time:831406ms step_avg:170.13ms\nstep:4898/10000 train_loss:4.0279 train_time:831579ms step_avg:170.13ms\nstep:4899/10000 train_loss:4.0559 train_time:831748ms step_avg:170.13ms\nstep:4900/10000 train_loss:3.9558 train_time:831920ms step_avg:170.13ms\nstep:4901/10000 train_loss:4.1436 train_time:832092ms step_avg:170.13ms\nstep:4902/10000 train_loss:3.8377 train_time:832262ms step_avg:170.13ms\nstep:4903/10000 train_loss:3.9736 train_time:832431ms step_avg:170.13ms\nstep:4904/10000 train_loss:3.8902 train_time:832603ms step_avg:170.13ms\nstep:4905/10000 train_loss:4.1826 train_time:832775ms step_avg:170.13ms\nstep:4906/10000 train_loss:3.8664 train_time:832944ms step_avg:170.13ms\nstep:4907/10000 train_loss:4.1249 train_time:833114ms step_avg:170.13ms\nstep:4908/10000 train_loss:4.0070 train_time:833281ms step_avg:170.13ms\nstep:4909/10000 train_loss:3.9970 train_time:833450ms step_avg:170.13ms\nstep:4910/10000 train_loss:3.9360 train_time:833621ms step_avg:170.13ms\nstep:4911/10000 train_loss:3.9692 train_time:833790ms step_avg:170.13ms\nstep:4912/10000 train_loss:3.7978 train_time:833963ms step_avg:170.13ms\nstep:4913/10000 train_loss:3.9341 train_time:834131ms step_avg:170.13ms\nstep:4914/10000 train_loss:3.9082 train_time:834302ms step_avg:170.13ms\nstep:4915/10000 train_loss:4.0006 train_time:834470ms step_avg:170.13ms\nstep:4916/10000 train_loss:4.1550 train_time:834640ms step_avg:170.13ms\nstep:4917/10000 train_loss:4.1537 train_time:834808ms step_avg:170.13ms\nstep:4918/10000 train_loss:3.8355 train_time:834979ms step_avg:170.13ms\nstep:4919/10000 train_loss:3.9263 train_time:835150ms step_avg:170.13ms\nstep:4920/10000 train_loss:3.8982 train_time:835321ms step_avg:170.13ms\nstep:4921/10000 train_loss:3.9250 train_time:835491ms step_avg:170.13ms\nstep:4922/10000 train_loss:4.0185 train_time:835660ms step_avg:170.13ms\nstep:4923/10000 train_loss:4.0261 train_time:835827ms step_avg:170.13ms\nstep:4924/10000 train_loss:3.9039 train_time:835996ms step_avg:170.13ms\nstep:4925/10000 train_loss:3.7903 train_time:836163ms step_avg:170.12ms\nstep:4926/10000 train_loss:3.9291 train_time:836333ms step_avg:170.12ms\nstep:4927/10000 train_loss:3.9480 train_time:836503ms step_avg:170.12ms\nstep:4928/10000 train_loss:4.0763 train_time:836672ms step_avg:170.12ms\nstep:4929/10000 train_loss:3.7306 train_time:836843ms step_avg:170.12ms\nstep:4930/10000 train_loss:4.0018 train_time:837012ms step_avg:170.12ms\nstep:4931/10000 train_loss:3.7877 train_time:837183ms step_avg:170.12ms\nstep:4932/10000 train_loss:3.8012 train_time:837352ms step_avg:170.12ms\nstep:4933/10000 train_loss:3.9680 train_time:837524ms step_avg:170.12ms\nstep:4934/10000 train_loss:4.1561 train_time:837691ms step_avg:170.12ms\nstep:4935/10000 train_loss:3.9904 train_time:837864ms step_avg:170.12ms\nstep:4936/10000 train_loss:3.9219 train_time:838034ms step_avg:170.12ms\nstep:4937/10000 train_loss:3.8645 train_time:838203ms step_avg:170.12ms\nstep:4938/10000 train_loss:4.0540 train_time:838375ms step_avg:170.12ms\nstep:4939/10000 train_loss:4.1134 train_time:838547ms step_avg:170.13ms\nstep:4940/10000 train_loss:3.9296 train_time:838716ms step_avg:170.13ms\nstep:4941/10000 train_loss:3.9170 train_time:838888ms step_avg:170.13ms\nstep:4942/10000 train_loss:3.9791 train_time:839061ms step_avg:170.13ms\nstep:4943/10000 train_loss:3.7126 train_time:839230ms step_avg:170.13ms\nstep:4944/10000 train_loss:3.9891 train_time:839404ms step_avg:170.13ms\nstep:4945/10000 train_loss:3.8506 train_time:839572ms step_avg:170.13ms\nstep:4946/10000 train_loss:4.1404 train_time:839741ms step_avg:170.13ms\nstep:4947/10000 train_loss:3.8502 train_time:839912ms step_avg:170.13ms\nstep:4948/10000 train_loss:3.9052 train_time:840082ms step_avg:170.13ms\nstep:4949/10000 train_loss:3.8189 train_time:840249ms step_avg:170.13ms\nstep:4950/10000 train_loss:3.9264 train_time:840421ms step_avg:170.13ms\nstep:4951/10000 train_loss:3.9535 train_time:840588ms step_avg:170.13ms\nstep:4952/10000 train_loss:3.9195 train_time:840759ms step_avg:170.13ms\nstep:4953/10000 train_loss:3.9571 train_time:840930ms step_avg:170.13ms\nstep:4954/10000 train_loss:3.8761 train_time:841100ms step_avg:170.13ms\nstep:4955/10000 train_loss:4.0231 train_time:841269ms step_avg:170.13ms\nstep:4956/10000 train_loss:4.1398 train_time:841440ms step_avg:170.13ms\nstep:4957/10000 train_loss:3.9565 train_time:841610ms step_avg:170.13ms\nstep:4958/10000 train_loss:3.8108 train_time:841783ms step_avg:170.13ms\nstep:4959/10000 train_loss:4.0978 train_time:841954ms step_avg:170.13ms\nstep:4960/10000 train_loss:3.8949 train_time:842122ms step_avg:170.13ms\nstep:4961/10000 train_loss:3.7935 train_time:842290ms step_avg:170.13ms\nstep:4962/10000 train_loss:3.9392 train_time:842463ms step_avg:170.13ms\nstep:4963/10000 train_loss:4.0552 train_time:842632ms step_avg:170.13ms\nstep:4964/10000 train_loss:3.7838 train_time:842803ms step_avg:170.13ms\nstep:4965/10000 train_loss:3.8748 train_time:842971ms step_avg:170.13ms\nstep:4966/10000 train_loss:3.8932 train_time:843141ms step_avg:170.13ms\nstep:4967/10000 train_loss:4.0240 train_time:843310ms step_avg:170.13ms\nstep:4968/10000 train_loss:4.1117 train_time:843477ms step_avg:170.12ms\nstep:4969/10000 train_loss:3.9676 train_time:843647ms step_avg:170.12ms\nstep:4970/10000 train_loss:3.9476 train_time:843819ms step_avg:170.12ms\nstep:4971/10000 train_loss:3.9450 train_time:843989ms step_avg:170.12ms\nstep:4972/10000 train_loss:3.7483 train_time:844159ms step_avg:170.12ms\nstep:4973/10000 train_loss:3.8755 train_time:844327ms step_avg:170.12ms\nstep:4974/10000 train_loss:3.9880 train_time:844496ms step_avg:170.12ms\nstep:4975/10000 train_loss:3.9591 train_time:844666ms step_avg:170.12ms\nstep:4976/10000 train_loss:3.9918 train_time:844836ms step_avg:170.12ms\nstep:4977/10000 train_loss:3.9762 train_time:845008ms step_avg:170.12ms\nstep:4978/10000 train_loss:3.8617 train_time:845178ms step_avg:170.12ms\nstep:4979/10000 train_loss:3.8663 train_time:845347ms step_avg:170.12ms\nstep:4980/10000 train_loss:4.2130 train_time:845517ms step_avg:170.12ms\nstep:4981/10000 train_loss:4.1539 train_time:845687ms step_avg:170.12ms\nstep:4982/10000 train_loss:3.9540 train_time:845856ms step_avg:170.12ms\nstep:4983/10000 train_loss:4.0561 train_time:846024ms step_avg:170.12ms\nstep:4984/10000 train_loss:3.8623 train_time:846194ms step_avg:170.12ms\nstep:4985/10000 train_loss:3.9147 train_time:846367ms step_avg:170.12ms\nstep:4986/10000 train_loss:3.9709 train_time:846536ms step_avg:170.12ms\nstep:4987/10000 train_loss:4.0678 train_time:846706ms step_avg:170.12ms\nstep:4988/10000 train_loss:3.9373 train_time:846876ms step_avg:170.12ms\nstep:4989/10000 train_loss:3.9071 train_time:847046ms step_avg:170.12ms\nstep:4990/10000 train_loss:3.9740 train_time:847215ms step_avg:170.12ms\nstep:4991/10000 train_loss:3.9676 train_time:847384ms step_avg:170.12ms\nstep:4992/10000 train_loss:4.0554 train_time:847552ms step_avg:170.12ms\nstep:4993/10000 train_loss:3.9707 train_time:847724ms step_avg:170.12ms\nstep:4994/10000 train_loss:3.8122 train_time:847895ms step_avg:170.12ms\nstep:4995/10000 train_loss:3.9843 train_time:848075ms step_avg:170.13ms\nstep:4996/10000 train_loss:3.8875 train_time:848238ms step_avg:170.12ms\nstep:4997/10000 train_loss:3.9503 train_time:848410ms step_avg:170.12ms\nstep:4998/10000 train_loss:3.9690 train_time:848581ms step_avg:170.12ms\nstep:4999/10000 train_loss:3.9535 train_time:848749ms step_avg:170.12ms\nstep:5000/10000 train_loss:3.8503 train_time:848918ms step_avg:170.12ms\nstep:5001/10000 train_loss:3.9578 train_time:849090ms step_avg:170.12ms\nstep:5002/10000 train_loss:4.0167 train_time:849260ms step_avg:170.12ms\nstep:5003/10000 train_loss:3.9525 train_time:849429ms step_avg:170.12ms\nstep:5004/10000 train_loss:3.8751 train_time:849598ms step_avg:170.12ms\nstep:5005/10000 train_loss:3.9445 train_time:849766ms step_avg:170.12ms\nstep:5006/10000 train_loss:3.9734 train_time:849937ms step_avg:170.12ms\nstep:5007/10000 train_loss:3.9851 train_time:850109ms step_avg:170.12ms\nstep:5008/10000 train_loss:3.7408 train_time:850281ms step_avg:170.12ms\nstep:5009/10000 train_loss:3.9063 train_time:850446ms step_avg:170.12ms\nstep:5010/10000 train_loss:3.8652 train_time:850615ms step_avg:170.12ms\nstep:5011/10000 train_loss:3.8184 train_time:850786ms step_avg:170.12ms\nstep:5012/10000 train_loss:4.0107 train_time:850955ms step_avg:170.12ms\nstep:5013/10000 train_loss:3.8647 train_time:851127ms step_avg:170.12ms\nstep:5014/10000 train_loss:4.0828 train_time:851297ms step_avg:170.12ms\nstep:5015/10000 train_loss:3.8466 train_time:851465ms step_avg:170.12ms\nstep:5016/10000 train_loss:4.0918 train_time:851633ms step_avg:170.12ms\nstep:5017/10000 train_loss:3.8418 train_time:851803ms step_avg:170.12ms\nstep:5018/10000 train_loss:3.9718 train_time:851972ms step_avg:170.12ms\nstep:5019/10000 train_loss:3.9443 train_time:852142ms step_avg:170.12ms\nstep:5020/10000 train_loss:3.8649 train_time:852311ms step_avg:170.12ms\nstep:5021/10000 train_loss:3.9206 train_time:852480ms step_avg:170.12ms\nstep:5022/10000 train_loss:3.8606 train_time:852647ms step_avg:170.12ms\nstep:5023/10000 train_loss:4.0111 train_time:852817ms step_avg:170.12ms\nstep:5024/10000 train_loss:4.0396 train_time:852986ms step_avg:170.12ms\nstep:5025/10000 train_loss:3.9641 train_time:853156ms step_avg:170.12ms\nstep:5026/10000 train_loss:4.0147 train_time:853324ms step_avg:170.12ms\nstep:5027/10000 train_loss:3.9413 train_time:853493ms step_avg:170.12ms\nstep:5028/10000 train_loss:3.7863 train_time:853665ms step_avg:170.12ms\nstep:5029/10000 train_loss:4.0522 train_time:853835ms step_avg:170.12ms\nstep:5030/10000 train_loss:4.0755 train_time:854003ms step_avg:170.12ms\nstep:5031/10000 train_loss:3.9864 train_time:854173ms step_avg:170.12ms\nstep:5032/10000 train_loss:3.9797 train_time:854345ms step_avg:170.12ms\nstep:5033/10000 train_loss:3.8270 train_time:854514ms step_avg:170.12ms\nstep:5034/10000 train_loss:3.8923 train_time:854684ms step_avg:170.12ms\nstep:5035/10000 train_loss:3.9853 train_time:854854ms step_avg:170.12ms\nstep:5036/10000 train_loss:4.0922 train_time:855026ms step_avg:170.12ms\nstep:5037/10000 train_loss:3.9335 train_time:855194ms step_avg:170.12ms\nstep:5038/10000 train_loss:3.8898 train_time:855366ms step_avg:170.12ms\nstep:5039/10000 train_loss:3.8604 train_time:855535ms step_avg:170.12ms\nstep:5040/10000 train_loss:3.8993 train_time:855709ms step_avg:170.12ms\nstep:5041/10000 train_loss:4.0819 train_time:855881ms step_avg:170.12ms\nstep:5042/10000 train_loss:3.9298 train_time:856056ms step_avg:170.12ms\nstep:5043/10000 train_loss:4.7526 train_time:856220ms step_avg:170.12ms\nstep:5044/10000 train_loss:3.9625 train_time:856390ms step_avg:170.12ms\nstep:5045/10000 train_loss:3.9681 train_time:856561ms step_avg:170.12ms\nstep:5046/10000 train_loss:3.9330 train_time:856727ms step_avg:170.12ms\nstep:5047/10000 train_loss:3.9526 train_time:856899ms step_avg:170.12ms\nstep:5048/10000 train_loss:4.0927 train_time:857069ms step_avg:170.12ms\nstep:5049/10000 train_loss:3.9218 train_time:857240ms step_avg:170.12ms\nstep:5050/10000 train_loss:3.7723 train_time:857410ms step_avg:170.12ms\nstep:5051/10000 train_loss:4.0473 train_time:857581ms step_avg:170.12ms\nstep:5052/10000 train_loss:3.8004 train_time:857748ms step_avg:170.12ms\nstep:5053/10000 train_loss:3.9620 train_time:857916ms step_avg:170.12ms\nstep:5054/10000 train_loss:3.8525 train_time:858084ms step_avg:170.12ms\nstep:5055/10000 train_loss:3.9431 train_time:858254ms step_avg:170.12ms\nstep:5056/10000 train_loss:4.0133 train_time:858425ms step_avg:170.12ms\nstep:5057/10000 train_loss:4.0557 train_time:858593ms step_avg:170.12ms\nstep:5058/10000 train_loss:3.9764 train_time:858765ms step_avg:170.12ms\nstep:5059/10000 train_loss:4.1230 train_time:858933ms step_avg:170.12ms\nstep:5060/10000 train_loss:3.9247 train_time:859103ms step_avg:170.12ms\nstep:5061/10000 train_loss:3.9848 train_time:859273ms step_avg:170.12ms\nstep:5062/10000 train_loss:3.7726 train_time:859443ms step_avg:170.12ms\nstep:5063/10000 train_loss:3.9808 train_time:859612ms step_avg:170.12ms\nstep:5064/10000 train_loss:3.9351 train_time:859782ms step_avg:170.12ms\nstep:5065/10000 train_loss:4.1095 train_time:859951ms step_avg:170.12ms\nstep:5066/10000 train_loss:4.0177 train_time:860122ms step_avg:170.12ms\nstep:5067/10000 train_loss:3.9197 train_time:860293ms step_avg:170.12ms\nstep:5068/10000 train_loss:3.8576 train_time:860465ms step_avg:170.12ms\nstep:5069/10000 train_loss:3.8806 train_time:860633ms step_avg:170.12ms\nstep:5070/10000 train_loss:3.9939 train_time:860804ms step_avg:170.12ms\nstep:5071/10000 train_loss:4.0539 train_time:860973ms step_avg:170.12ms\nstep:5072/10000 train_loss:3.8681 train_time:861144ms step_avg:170.12ms\nstep:5073/10000 train_loss:3.8096 train_time:861313ms step_avg:170.12ms\nstep:5074/10000 train_loss:3.9980 train_time:861485ms step_avg:170.12ms\nstep:5075/10000 train_loss:3.8316 train_time:861656ms step_avg:170.12ms\nstep:5076/10000 train_loss:3.9748 train_time:861827ms step_avg:170.12ms\nstep:5077/10000 train_loss:4.0120 train_time:861997ms step_avg:170.12ms\nstep:5078/10000 train_loss:3.9366 train_time:862166ms step_avg:170.12ms\nstep:5079/10000 train_loss:3.7440 train_time:862337ms step_avg:170.12ms\nstep:5080/10000 train_loss:3.8600 train_time:862506ms step_avg:170.12ms\nstep:5081/10000 train_loss:3.9548 train_time:862676ms step_avg:170.12ms\nstep:5082/10000 train_loss:3.9798 train_time:862848ms step_avg:170.12ms\nstep:5083/10000 train_loss:3.8731 train_time:863018ms step_avg:170.12ms\nstep:5084/10000 train_loss:3.9502 train_time:863188ms step_avg:170.12ms\nstep:5085/10000 train_loss:3.8540 train_time:863359ms step_avg:170.12ms\nstep:5086/10000 train_loss:3.8386 train_time:863529ms step_avg:170.12ms\nstep:5087/10000 train_loss:3.9898 train_time:863699ms step_avg:170.12ms\nstep:5088/10000 train_loss:3.9425 train_time:863870ms step_avg:170.12ms\nstep:5089/10000 train_loss:3.8726 train_time:864041ms step_avg:170.12ms\nstep:5090/10000 train_loss:3.6864 train_time:864209ms step_avg:170.12ms\nstep:5091/10000 train_loss:3.9220 train_time:864379ms step_avg:170.12ms\nstep:5092/10000 train_loss:3.9584 train_time:864550ms step_avg:170.12ms\nstep:5093/10000 train_loss:4.1172 train_time:864722ms step_avg:170.12ms\nstep:5094/10000 train_loss:3.9473 train_time:864892ms step_avg:170.12ms\nstep:5095/10000 train_loss:3.9012 train_time:865064ms step_avg:170.12ms\nstep:5096/10000 train_loss:3.8712 train_time:865232ms step_avg:170.12ms\nstep:5097/10000 train_loss:3.8982 train_time:865400ms step_avg:170.12ms\nstep:5098/10000 train_loss:3.9176 train_time:865569ms step_avg:170.12ms\nstep:5099/10000 train_loss:3.9652 train_time:865739ms step_avg:170.12ms\nstep:5100/10000 train_loss:4.0059 train_time:865907ms step_avg:170.12ms\nstep:5101/10000 train_loss:3.9405 train_time:866076ms step_avg:170.12ms\nstep:5102/10000 train_loss:3.9891 train_time:866245ms step_avg:170.12ms\nstep:5103/10000 train_loss:4.0725 train_time:866415ms step_avg:170.12ms\nstep:5104/10000 train_loss:3.9077 train_time:866588ms step_avg:170.12ms\nstep:5105/10000 train_loss:3.9688 train_time:866758ms step_avg:170.12ms\nstep:5106/10000 train_loss:3.9016 train_time:866924ms step_avg:170.12ms\nstep:5107/10000 train_loss:3.9115 train_time:867095ms step_avg:170.12ms\nstep:5108/10000 train_loss:3.9917 train_time:867267ms step_avg:170.12ms\nstep:5109/10000 train_loss:3.9720 train_time:867437ms step_avg:170.12ms\nstep:5110/10000 train_loss:4.0118 train_time:867605ms step_avg:170.12ms\nstep:5111/10000 train_loss:3.8098 train_time:867776ms step_avg:170.12ms\nstep:5112/10000 train_loss:3.9550 train_time:867949ms step_avg:170.12ms\nstep:5113/10000 train_loss:3.8401 train_time:868121ms step_avg:170.12ms\nstep:5114/10000 train_loss:4.0382 train_time:868290ms step_avg:170.12ms\nstep:5115/10000 train_loss:3.9926 train_time:868463ms step_avg:170.12ms\nstep:5116/10000 train_loss:3.8810 train_time:868632ms step_avg:170.12ms\nstep:5117/10000 train_loss:4.0304 train_time:868802ms step_avg:170.12ms\nstep:5118/10000 train_loss:3.9958 train_time:868970ms step_avg:170.12ms\nstep:5119/10000 train_loss:3.9076 train_time:869140ms step_avg:170.12ms\nstep:5120/10000 train_loss:3.8187 train_time:869308ms step_avg:170.12ms\nstep:5121/10000 train_loss:3.8695 train_time:869477ms step_avg:170.12ms\nstep:5122/10000 train_loss:3.9820 train_time:869646ms step_avg:170.12ms\nstep:5123/10000 train_loss:3.9815 train_time:869818ms step_avg:170.12ms\nstep:5124/10000 train_loss:3.9671 train_time:869989ms step_avg:170.12ms\nstep:5125/10000 train_loss:3.9178 train_time:870161ms step_avg:170.12ms\nstep:5126/10000 train_loss:3.8012 train_time:870331ms step_avg:170.12ms\nstep:5127/10000 train_loss:3.7498 train_time:870500ms step_avg:170.12ms\nstep:5128/10000 train_loss:3.8724 train_time:870670ms step_avg:170.12ms\nstep:5129/10000 train_loss:4.0551 train_time:870842ms step_avg:170.12ms\nstep:5130/10000 train_loss:3.9496 train_time:871012ms step_avg:170.12ms\nstep:5131/10000 train_loss:3.9680 train_time:871186ms step_avg:170.12ms\nstep:5132/10000 train_loss:4.0650 train_time:871357ms step_avg:170.12ms\nstep:5133/10000 train_loss:3.9404 train_time:871524ms step_avg:170.12ms\nstep:5134/10000 train_loss:4.0103 train_time:871692ms step_avg:170.12ms\nstep:5135/10000 train_loss:3.9000 train_time:871862ms step_avg:170.12ms\nstep:5136/10000 train_loss:3.9083 train_time:872032ms step_avg:170.12ms\nstep:5137/10000 train_loss:4.0268 train_time:872202ms step_avg:170.12ms\nstep:5138/10000 train_loss:4.0255 train_time:872372ms step_avg:170.12ms\nstep:5139/10000 train_loss:3.8761 train_time:872543ms step_avg:170.12ms\nstep:5140/10000 train_loss:4.0086 train_time:872713ms step_avg:170.12ms\nstep:5141/10000 train_loss:3.9222 train_time:872884ms step_avg:170.12ms\nstep:5142/10000 train_loss:3.9231 train_time:873052ms step_avg:170.12ms\nstep:5143/10000 train_loss:3.8244 train_time:873221ms step_avg:170.12ms\nstep:5144/10000 train_loss:3.8728 train_time:873389ms step_avg:170.12ms\nstep:5145/10000 train_loss:3.9209 train_time:873559ms step_avg:170.12ms\nstep:5146/10000 train_loss:3.9660 train_time:873727ms step_avg:170.12ms\nstep:5147/10000 train_loss:3.8092 train_time:873896ms step_avg:170.12ms\nstep:5148/10000 train_loss:3.9728 train_time:874067ms step_avg:170.12ms\nstep:5149/10000 train_loss:3.8301 train_time:874237ms step_avg:170.12ms\nstep:5150/10000 train_loss:3.9259 train_time:874409ms step_avg:170.12ms\nstep:5151/10000 train_loss:4.0196 train_time:874580ms step_avg:170.12ms\nstep:5152/10000 train_loss:3.9905 train_time:874748ms step_avg:170.12ms\nstep:5153/10000 train_loss:4.0049 train_time:874918ms step_avg:170.12ms\nstep:5154/10000 train_loss:3.9161 train_time:875086ms step_avg:170.12ms\nstep:5155/10000 train_loss:3.8966 train_time:875257ms step_avg:170.12ms\nstep:5156/10000 train_loss:3.9247 train_time:875428ms step_avg:170.12ms\nstep:5157/10000 train_loss:4.1058 train_time:875598ms step_avg:170.12ms\nstep:5158/10000 train_loss:4.0125 train_time:875769ms step_avg:170.12ms\nstep:5159/10000 train_loss:4.1831 train_time:875941ms step_avg:170.12ms\nstep:5160/10000 train_loss:3.9469 train_time:876111ms step_avg:170.12ms\nstep:5161/10000 train_loss:3.8480 train_time:876281ms step_avg:170.12ms\nstep:5162/10000 train_loss:3.8898 train_time:876451ms step_avg:170.12ms\nstep:5163/10000 train_loss:3.9251 train_time:876621ms step_avg:170.12ms\nstep:5164/10000 train_loss:4.0831 train_time:876791ms step_avg:170.12ms\nstep:5165/10000 train_loss:3.8239 train_time:876964ms step_avg:170.12ms\nstep:5166/10000 train_loss:3.9506 train_time:877132ms step_avg:170.12ms\nstep:5167/10000 train_loss:3.7430 train_time:877303ms step_avg:170.12ms\nstep:5168/10000 train_loss:3.9258 train_time:877471ms step_avg:170.12ms\nstep:5169/10000 train_loss:4.0054 train_time:877639ms step_avg:170.12ms\nstep:5170/10000 train_loss:4.0225 train_time:877807ms step_avg:170.12ms\nstep:5171/10000 train_loss:3.9627 train_time:877977ms step_avg:170.12ms\nstep:5172/10000 train_loss:3.6938 train_time:878144ms step_avg:170.12ms\nstep:5173/10000 train_loss:3.7897 train_time:878313ms step_avg:170.12ms\nstep:5174/10000 train_loss:3.9501 train_time:878483ms step_avg:170.12ms\nstep:5175/10000 train_loss:3.8690 train_time:878650ms step_avg:170.12ms\nstep:5176/10000 train_loss:3.9309 train_time:878821ms step_avg:170.12ms\nstep:5177/10000 train_loss:4.0176 train_time:878992ms step_avg:170.12ms\nstep:5178/10000 train_loss:4.0415 train_time:879161ms step_avg:170.12ms\nstep:5179/10000 train_loss:3.9031 train_time:879331ms step_avg:170.12ms\nstep:5180/10000 train_loss:3.9475 train_time:879505ms step_avg:170.12ms\nstep:5181/10000 train_loss:4.1210 train_time:879673ms step_avg:170.12ms\nstep:5182/10000 train_loss:4.0711 train_time:879845ms step_avg:170.12ms\nstep:5183/10000 train_loss:3.9048 train_time:880016ms step_avg:170.12ms\nstep:5184/10000 train_loss:3.9823 train_time:880192ms step_avg:170.12ms\nstep:5185/10000 train_loss:4.0832 train_time:880358ms step_avg:170.12ms\nstep:5186/10000 train_loss:4.0442 train_time:880529ms step_avg:170.12ms\nstep:5187/10000 train_loss:3.9127 train_time:880699ms step_avg:170.12ms\nstep:5188/10000 train_loss:4.0023 train_time:880870ms step_avg:170.12ms\nstep:5189/10000 train_loss:3.8759 train_time:881041ms step_avg:170.12ms\nstep:5190/10000 train_loss:3.8575 train_time:881211ms step_avg:170.12ms\nstep:5191/10000 train_loss:4.0473 train_time:881383ms step_avg:170.12ms\nstep:5192/10000 train_loss:3.9727 train_time:881552ms step_avg:170.12ms\nstep:5193/10000 train_loss:4.0134 train_time:881721ms step_avg:170.12ms\nstep:5194/10000 train_loss:3.9268 train_time:881891ms step_avg:170.12ms\nstep:5195/10000 train_loss:4.0571 train_time:882060ms step_avg:170.12ms\nstep:5196/10000 train_loss:4.0310 train_time:882227ms step_avg:170.12ms\nstep:5197/10000 train_loss:3.9215 train_time:882398ms step_avg:170.12ms\nstep:5198/10000 train_loss:3.8157 train_time:882569ms step_avg:170.12ms\nstep:5199/10000 train_loss:4.1112 train_time:882739ms step_avg:170.12ms\nstep:5200/10000 train_loss:3.9345 train_time:882909ms step_avg:170.12ms\nstep:5201/10000 train_loss:3.8306 train_time:883081ms step_avg:170.12ms\nstep:5202/10000 train_loss:3.9158 train_time:883250ms step_avg:170.12ms\nstep:5203/10000 train_loss:3.8820 train_time:883421ms step_avg:170.12ms\nstep:5204/10000 train_loss:4.0588 train_time:883589ms step_avg:170.12ms\nstep:5205/10000 train_loss:4.0250 train_time:883759ms step_avg:170.12ms\nstep:5206/10000 train_loss:3.9452 train_time:883931ms step_avg:170.12ms\nstep:5207/10000 train_loss:3.8826 train_time:884101ms step_avg:170.12ms\nstep:5208/10000 train_loss:3.9815 train_time:884271ms step_avg:170.12ms\nstep:5209/10000 train_loss:3.8267 train_time:884441ms step_avg:170.12ms\nstep:5210/10000 train_loss:3.9818 train_time:884609ms step_avg:170.12ms\nstep:5211/10000 train_loss:3.8633 train_time:884779ms step_avg:170.12ms\nstep:5212/10000 train_loss:3.9727 train_time:884951ms step_avg:170.12ms\nstep:5213/10000 train_loss:4.0525 train_time:885123ms step_avg:170.12ms\nstep:5214/10000 train_loss:3.9932 train_time:885294ms step_avg:170.12ms\nstep:5215/10000 train_loss:4.0894 train_time:885468ms step_avg:170.12ms\nstep:5216/10000 train_loss:3.8970 train_time:885638ms step_avg:170.12ms\nstep:5217/10000 train_loss:3.8729 train_time:885808ms step_avg:170.12ms\nstep:5218/10000 train_loss:4.1016 train_time:885977ms step_avg:170.12ms\nstep:5219/10000 train_loss:3.9307 train_time:886146ms step_avg:170.12ms\nstep:5220/10000 train_loss:4.0346 train_time:886315ms step_avg:170.12ms\nstep:5221/10000 train_loss:3.9181 train_time:886483ms step_avg:170.12ms\nstep:5222/10000 train_loss:3.9140 train_time:886651ms step_avg:170.12ms\nstep:5223/10000 train_loss:3.8999 train_time:886824ms step_avg:170.12ms\nstep:5224/10000 train_loss:4.0824 train_time:886993ms step_avg:170.12ms\nstep:5225/10000 train_loss:3.9658 train_time:887165ms step_avg:170.12ms\nstep:5226/10000 train_loss:4.1213 train_time:887336ms step_avg:170.12ms\nstep:5227/10000 train_loss:3.9249 train_time:887505ms step_avg:170.12ms\nstep:5228/10000 train_loss:3.8709 train_time:887673ms step_avg:170.12ms\nstep:5229/10000 train_loss:3.8996 train_time:887842ms step_avg:170.12ms\nstep:5230/10000 train_loss:3.8611 train_time:888012ms step_avg:170.12ms\nstep:5231/10000 train_loss:3.8945 train_time:888184ms step_avg:170.12ms\nstep:5232/10000 train_loss:3.8736 train_time:888351ms step_avg:170.12ms\nstep:5233/10000 train_loss:3.9474 train_time:888522ms step_avg:170.12ms\nstep:5234/10000 train_loss:4.0034 train_time:888692ms step_avg:170.12ms\nstep:5235/10000 train_loss:3.9256 train_time:888862ms step_avg:170.12ms\nstep:5236/10000 train_loss:3.8701 train_time:889032ms step_avg:170.12ms\nstep:5237/10000 train_loss:3.9470 train_time:889205ms step_avg:170.12ms\nstep:5238/10000 train_loss:4.0194 train_time:889373ms step_avg:170.12ms\nstep:5239/10000 train_loss:3.8377 train_time:889544ms step_avg:170.12ms\nstep:5240/10000 train_loss:3.8940 train_time:889713ms step_avg:170.12ms\nstep:5241/10000 train_loss:3.8879 train_time:889885ms step_avg:170.12ms\nstep:5242/10000 train_loss:3.9139 train_time:890049ms step_avg:170.12ms\nstep:5243/10000 train_loss:3.9579 train_time:890219ms step_avg:170.12ms\nstep:5244/10000 train_loss:3.8508 train_time:890388ms step_avg:170.12ms\nstep:5245/10000 train_loss:3.9215 train_time:890560ms step_avg:170.12ms\nstep:5246/10000 train_loss:3.8260 train_time:890731ms step_avg:170.12ms\nstep:5247/10000 train_loss:4.0032 train_time:890901ms step_avg:170.12ms\nstep:5248/10000 train_loss:4.0237 train_time:891069ms step_avg:170.12ms\nstep:5249/10000 train_loss:4.2050 train_time:891240ms step_avg:170.12ms\nstep:5250/10000 train_loss:3.8881 train_time:891408ms step_avg:170.12ms\nstep:5251/10000 train_loss:3.7805 train_time:891578ms step_avg:170.12ms\nstep:5252/10000 train_loss:3.9382 train_time:891749ms step_avg:170.12ms\nstep:5253/10000 train_loss:3.9291 train_time:891920ms step_avg:170.12ms\nstep:5254/10000 train_loss:3.8403 train_time:892089ms step_avg:170.12ms\nstep:5255/10000 train_loss:4.0185 train_time:892257ms step_avg:170.12ms\nstep:5256/10000 train_loss:3.9939 train_time:892428ms step_avg:170.12ms\nstep:5257/10000 train_loss:3.8585 train_time:892598ms step_avg:170.12ms\nstep:5258/10000 train_loss:3.9822 train_time:892770ms step_avg:170.12ms\nstep:5259/10000 train_loss:3.9584 train_time:892941ms step_avg:170.12ms\nstep:5260/10000 train_loss:3.9802 train_time:893110ms step_avg:170.12ms\nstep:5261/10000 train_loss:3.8887 train_time:893280ms step_avg:170.12ms\nstep:5262/10000 train_loss:3.9190 train_time:893449ms step_avg:170.12ms\nstep:5263/10000 train_loss:3.9019 train_time:893620ms step_avg:170.12ms\nstep:5264/10000 train_loss:3.9807 train_time:893788ms step_avg:170.12ms\nstep:5265/10000 train_loss:3.7987 train_time:893960ms step_avg:170.12ms\nstep:5266/10000 train_loss:3.6538 train_time:894132ms step_avg:170.12ms\nstep:5267/10000 train_loss:3.8325 train_time:894301ms step_avg:170.12ms\nstep:5268/10000 train_loss:3.8211 train_time:894469ms step_avg:170.12ms\nstep:5269/10000 train_loss:3.8387 train_time:894641ms step_avg:170.12ms\nstep:5270/10000 train_loss:3.8720 train_time:894810ms step_avg:170.12ms\nstep:5271/10000 train_loss:3.9727 train_time:894983ms step_avg:170.12ms\nstep:5272/10000 train_loss:4.0650 train_time:895151ms step_avg:170.12ms\nstep:5273/10000 train_loss:3.8332 train_time:895321ms step_avg:170.12ms\nstep:5274/10000 train_loss:3.9416 train_time:895490ms step_avg:170.12ms\nstep:5275/10000 train_loss:4.0301 train_time:895662ms step_avg:170.12ms\nstep:5276/10000 train_loss:4.0005 train_time:895830ms step_avg:170.12ms\nstep:5277/10000 train_loss:4.0310 train_time:896001ms step_avg:170.12ms\nstep:5278/10000 train_loss:3.9645 train_time:896170ms step_avg:170.12ms\nstep:5279/10000 train_loss:3.8028 train_time:896340ms step_avg:170.12ms\nstep:5280/10000 train_loss:3.9224 train_time:896510ms step_avg:170.12ms\nstep:5281/10000 train_loss:3.9980 train_time:896680ms step_avg:170.12ms\nstep:5282/10000 train_loss:3.8193 train_time:896849ms step_avg:170.12ms\nstep:5283/10000 train_loss:3.9094 train_time:897020ms step_avg:170.12ms\nstep:5284/10000 train_loss:4.0103 train_time:897187ms step_avg:170.12ms\nstep:5285/10000 train_loss:3.8537 train_time:897357ms step_avg:170.12ms\nstep:5286/10000 train_loss:3.9760 train_time:897526ms step_avg:170.11ms\nstep:5287/10000 train_loss:3.8287 train_time:897695ms step_avg:170.11ms\nstep:5288/10000 train_loss:3.9724 train_time:897865ms step_avg:170.11ms\nstep:5289/10000 train_loss:3.9879 train_time:898035ms step_avg:170.11ms\nstep:5290/10000 train_loss:4.0229 train_time:898205ms step_avg:170.11ms\nstep:5291/10000 train_loss:3.9194 train_time:898375ms step_avg:170.11ms\nstep:5292/10000 train_loss:3.8894 train_time:898548ms step_avg:170.12ms\nstep:5293/10000 train_loss:4.0615 train_time:898717ms step_avg:170.11ms\nstep:5294/10000 train_loss:3.9151 train_time:898890ms step_avg:170.12ms\nstep:5295/10000 train_loss:3.6779 train_time:899062ms step_avg:170.12ms\nstep:5296/10000 train_loss:4.1109 train_time:899232ms step_avg:170.12ms\nstep:5297/10000 train_loss:4.0179 train_time:899401ms step_avg:170.12ms\nstep:5298/10000 train_loss:3.8973 train_time:899569ms step_avg:170.12ms\nstep:5299/10000 train_loss:3.9668 train_time:899740ms step_avg:170.12ms\nstep:5300/10000 train_loss:4.0054 train_time:899911ms step_avg:170.12ms\nstep:5301/10000 train_loss:3.7229 train_time:900083ms step_avg:170.12ms\nstep:5302/10000 train_loss:3.9296 train_time:900251ms step_avg:170.12ms\nstep:5303/10000 train_loss:3.8185 train_time:900420ms step_avg:170.12ms\nstep:5304/10000 train_loss:4.0216 train_time:900596ms step_avg:170.12ms\nstep:5305/10000 train_loss:3.8984 train_time:900757ms step_avg:170.11ms\nstep:5306/10000 train_loss:4.0083 train_time:900928ms step_avg:170.11ms\nstep:5307/10000 train_loss:3.9099 train_time:901098ms step_avg:170.11ms\nstep:5308/10000 train_loss:4.0689 train_time:901267ms step_avg:170.11ms\nstep:5309/10000 train_loss:3.9987 train_time:901438ms step_avg:170.11ms\nstep:5310/10000 train_loss:4.0661 train_time:901607ms step_avg:170.11ms\nstep:5311/10000 train_loss:3.8532 train_time:901778ms step_avg:170.11ms\nstep:5312/10000 train_loss:3.8380 train_time:901949ms step_avg:170.11ms\nstep:5313/10000 train_loss:3.8613 train_time:902118ms step_avg:170.11ms\nstep:5314/10000 train_loss:3.8540 train_time:902288ms step_avg:170.11ms\nstep:5315/10000 train_loss:3.9654 train_time:902458ms step_avg:170.11ms\nstep:5316/10000 train_loss:3.8835 train_time:902625ms step_avg:170.11ms\nstep:5317/10000 train_loss:3.8828 train_time:902794ms step_avg:170.11ms\nstep:5318/10000 train_loss:3.7519 train_time:902965ms step_avg:170.11ms\nstep:5319/10000 train_loss:3.9658 train_time:903138ms step_avg:170.11ms\nstep:5320/10000 train_loss:3.9699 train_time:903307ms step_avg:170.11ms\nstep:5321/10000 train_loss:3.9105 train_time:903474ms step_avg:170.11ms\nstep:5322/10000 train_loss:4.0264 train_time:903642ms step_avg:170.11ms\nstep:5323/10000 train_loss:3.9823 train_time:903811ms step_avg:170.11ms\nstep:5324/10000 train_loss:2.8577 train_time:903983ms step_avg:170.11ms\nstep:5325/10000 train_loss:3.9077 train_time:904153ms step_avg:170.11ms\nstep:5326/10000 train_loss:3.9311 train_time:904325ms step_avg:170.11ms\nstep:5327/10000 train_loss:3.8559 train_time:904494ms step_avg:170.11ms\nstep:5328/10000 train_loss:3.9374 train_time:904665ms step_avg:170.11ms\nstep:5329/10000 train_loss:4.2154 train_time:904835ms step_avg:170.11ms\nstep:5330/10000 train_loss:4.0182 train_time:905006ms step_avg:170.11ms\nstep:5331/10000 train_loss:3.9152 train_time:905178ms step_avg:170.11ms\nstep:5332/10000 train_loss:3.9212 train_time:905351ms step_avg:170.11ms\nstep:5333/10000 train_loss:3.7166 train_time:905523ms step_avg:170.12ms\nstep:5334/10000 train_loss:3.8656 train_time:905906ms step_avg:170.16ms\nstep:5335/10000 train_loss:3.9019 train_time:906071ms step_avg:170.15ms\nstep:5336/10000 train_loss:3.9484 train_time:906238ms step_avg:170.15ms\nstep:5337/10000 train_loss:3.9672 train_time:906407ms step_avg:170.15ms\nstep:5338/10000 train_loss:3.8871 train_time:906576ms step_avg:170.15ms\nstep:5339/10000 train_loss:3.8859 train_time:906746ms step_avg:170.15ms\nstep:5340/10000 train_loss:4.3065 train_time:906924ms step_avg:170.15ms\nstep:5341/10000 train_loss:4.0446 train_time:907091ms step_avg:170.15ms\nstep:5342/10000 train_loss:3.8618 train_time:907260ms step_avg:170.15ms\nstep:5343/10000 train_loss:3.9010 train_time:907429ms step_avg:170.15ms\nstep:5344/10000 train_loss:4.0073 train_time:907597ms step_avg:170.15ms\nstep:5345/10000 train_loss:4.0100 train_time:907766ms step_avg:170.15ms\nstep:5346/10000 train_loss:3.9662 train_time:907936ms step_avg:170.15ms\nstep:5347/10000 train_loss:3.8773 train_time:908109ms step_avg:170.15ms\nstep:5348/10000 train_loss:3.9162 train_time:908279ms step_avg:170.15ms\nstep:5349/10000 train_loss:3.8918 train_time:908447ms step_avg:170.15ms\nstep:5350/10000 train_loss:3.8948 train_time:908616ms step_avg:170.15ms\nstep:5351/10000 train_loss:3.9428 train_time:908790ms step_avg:170.15ms\nstep:5352/10000 train_loss:3.9046 train_time:908962ms step_avg:170.15ms\nstep:5353/10000 train_loss:4.3959 train_time:909131ms step_avg:170.15ms\nstep:5354/10000 train_loss:3.9719 train_time:909305ms step_avg:170.15ms\nstep:5355/10000 train_loss:3.9532 train_time:909473ms step_avg:170.15ms\nstep:5356/10000 train_loss:3.8814 train_time:909643ms step_avg:170.15ms\nstep:5357/10000 train_loss:3.9621 train_time:909812ms step_avg:170.15ms\nstep:5358/10000 train_loss:3.8784 train_time:909986ms step_avg:170.15ms\nstep:5359/10000 train_loss:3.9055 train_time:910163ms step_avg:170.16ms\nstep:5360/10000 train_loss:4.1346 train_time:910327ms step_avg:170.15ms\nstep:5361/10000 train_loss:3.8756 train_time:910498ms step_avg:170.15ms\nstep:5362/10000 train_loss:3.8005 train_time:910670ms step_avg:170.16ms\nstep:5363/10000 train_loss:4.0990 train_time:910842ms step_avg:170.16ms\nstep:5364/10000 train_loss:4.0371 train_time:911011ms step_avg:170.16ms\nstep:5365/10000 train_loss:3.9094 train_time:911182ms step_avg:170.16ms\nstep:5366/10000 train_loss:3.8583 train_time:911350ms step_avg:170.16ms\nstep:5367/10000 train_loss:3.7846 train_time:911521ms step_avg:170.16ms\nstep:5368/10000 train_loss:3.9020 train_time:911688ms step_avg:170.15ms\nstep:5369/10000 train_loss:3.8773 train_time:911859ms step_avg:170.15ms\nstep:5370/10000 train_loss:3.9711 train_time:912030ms step_avg:170.15ms\nstep:5371/10000 train_loss:3.5189 train_time:912201ms step_avg:170.15ms\nstep:5372/10000 train_loss:3.8702 train_time:912369ms step_avg:170.15ms\nstep:5373/10000 train_loss:3.7839 train_time:912538ms step_avg:170.15ms\nstep:5374/10000 train_loss:3.9209 train_time:912708ms step_avg:170.15ms\nstep:5375/10000 train_loss:3.9375 train_time:912877ms step_avg:170.15ms\nstep:5376/10000 train_loss:4.1815 train_time:913047ms step_avg:170.15ms\nstep:5377/10000 train_loss:3.8606 train_time:913220ms step_avg:170.15ms\nstep:5378/10000 train_loss:4.0563 train_time:913391ms step_avg:170.15ms\nstep:5379/10000 train_loss:3.9071 train_time:913559ms step_avg:170.15ms\nstep:5380/10000 train_loss:3.9523 train_time:913730ms step_avg:170.15ms\nstep:5381/10000 train_loss:4.3596 train_time:913898ms step_avg:170.15ms\nstep:5382/10000 train_loss:3.9236 train_time:914069ms step_avg:170.15ms\nstep:5383/10000 train_loss:3.8842 train_time:914239ms step_avg:170.15ms\nstep:5384/10000 train_loss:3.9477 train_time:914410ms step_avg:170.15ms\nstep:5385/10000 train_loss:3.9863 train_time:914581ms step_avg:170.15ms\nstep:5386/10000 train_loss:3.8945 train_time:914750ms step_avg:170.15ms\nstep:5387/10000 train_loss:3.9391 train_time:914921ms step_avg:170.15ms\nstep:5388/10000 train_loss:4.0034 train_time:915090ms step_avg:170.15ms\nstep:5389/10000 train_loss:3.9043 train_time:915262ms step_avg:170.15ms\nstep:5390/10000 train_loss:3.8915 train_time:915432ms step_avg:170.15ms\nstep:5391/10000 train_loss:3.8736 train_time:915603ms step_avg:170.15ms\nstep:5392/10000 train_loss:3.9514 train_time:915771ms step_avg:170.15ms\nstep:5393/10000 train_loss:3.7888 train_time:915941ms step_avg:170.15ms\nstep:5394/10000 train_loss:3.8455 train_time:916111ms step_avg:170.15ms\nstep:5395/10000 train_loss:4.0187 train_time:916284ms step_avg:170.15ms\nstep:5396/10000 train_loss:3.8674 train_time:916454ms step_avg:170.15ms\nstep:5397/10000 train_loss:3.9114 train_time:916626ms step_avg:170.16ms\nstep:5398/10000 train_loss:3.8169 train_time:916799ms step_avg:170.16ms\nstep:5399/10000 train_loss:4.0010 train_time:916971ms step_avg:170.16ms\nstep:5400/10000 train_loss:3.7671 train_time:917142ms step_avg:170.16ms\nstep:5401/10000 train_loss:3.8463 train_time:917310ms step_avg:170.16ms\nstep:5402/10000 train_loss:4.0975 train_time:917481ms step_avg:170.16ms\nstep:5403/10000 train_loss:3.9463 train_time:917650ms step_avg:170.16ms\nstep:5404/10000 train_loss:3.8841 train_time:917818ms step_avg:170.16ms\nstep:5405/10000 train_loss:3.9567 train_time:917988ms step_avg:170.16ms\nstep:5406/10000 train_loss:3.8209 train_time:918159ms step_avg:170.16ms\nstep:5407/10000 train_loss:3.9275 train_time:918328ms step_avg:170.16ms\nstep:5408/10000 train_loss:3.8474 train_time:918499ms step_avg:170.16ms\nstep:5409/10000 train_loss:3.8228 train_time:918670ms step_avg:170.16ms\nstep:5410/10000 train_loss:3.9235 train_time:918841ms step_avg:170.16ms\nstep:5411/10000 train_loss:3.8559 train_time:919011ms step_avg:170.16ms\nstep:5412/10000 train_loss:3.9709 train_time:919182ms step_avg:170.16ms\nstep:5413/10000 train_loss:3.8326 train_time:919353ms step_avg:170.16ms\nstep:5414/10000 train_loss:4.0192 train_time:919523ms step_avg:170.16ms\nstep:5415/10000 train_loss:3.9784 train_time:919691ms step_avg:170.16ms\nstep:5416/10000 train_loss:3.8127 train_time:919860ms step_avg:170.16ms\nstep:5417/10000 train_loss:4.0415 train_time:920030ms step_avg:170.16ms\nstep:5418/10000 train_loss:3.9572 train_time:920203ms step_avg:170.16ms\nstep:5419/10000 train_loss:3.9962 train_time:920369ms step_avg:170.16ms\nstep:5420/10000 train_loss:3.8723 train_time:920539ms step_avg:170.16ms\nstep:5421/10000 train_loss:3.7856 train_time:920709ms step_avg:170.16ms\nstep:5422/10000 train_loss:3.8784 train_time:920881ms step_avg:170.16ms\nstep:5423/10000 train_loss:3.9062 train_time:921050ms step_avg:170.16ms\nstep:5424/10000 train_loss:4.0011 train_time:921219ms step_avg:170.16ms\nstep:5425/10000 train_loss:4.0276 train_time:921390ms step_avg:170.16ms\nstep:5426/10000 train_loss:3.8763 train_time:921559ms step_avg:170.15ms\nstep:5427/10000 train_loss:3.8375 train_time:921731ms step_avg:170.16ms\nstep:5428/10000 train_loss:3.8211 train_time:921903ms step_avg:170.16ms\nstep:5429/10000 train_loss:3.8283 train_time:922075ms step_avg:170.16ms\nstep:5430/10000 train_loss:3.9876 train_time:922245ms step_avg:170.16ms\nstep:5431/10000 train_loss:3.8832 train_time:922414ms step_avg:170.16ms\nstep:5432/10000 train_loss:3.7533 train_time:922582ms step_avg:170.16ms\nstep:5433/10000 train_loss:3.8678 train_time:922749ms step_avg:170.15ms\nstep:5434/10000 train_loss:3.8692 train_time:922923ms step_avg:170.16ms\nstep:5435/10000 train_loss:3.8910 train_time:923092ms step_avg:170.16ms\nstep:5436/10000 train_loss:4.1013 train_time:923266ms step_avg:170.16ms\nstep:5437/10000 train_loss:3.9820 train_time:923434ms step_avg:170.16ms\nstep:5438/10000 train_loss:3.9467 train_time:923601ms step_avg:170.16ms\nstep:5439/10000 train_loss:3.8587 train_time:923772ms step_avg:170.16ms\nstep:5440/10000 train_loss:4.0405 train_time:923943ms step_avg:170.16ms\nstep:5441/10000 train_loss:4.0946 train_time:924112ms step_avg:170.16ms\nstep:5442/10000 train_loss:4.0667 train_time:924286ms step_avg:170.16ms\nstep:5443/10000 train_loss:3.8879 train_time:924456ms step_avg:170.16ms\nstep:5444/10000 train_loss:3.7557 train_time:924626ms step_avg:170.16ms\nstep:5445/10000 train_loss:3.9445 train_time:924794ms step_avg:170.16ms\nstep:5446/10000 train_loss:4.0809 train_time:924968ms step_avg:170.16ms\nstep:5447/10000 train_loss:4.0072 train_time:925139ms step_avg:170.16ms\nstep:5448/10000 train_loss:4.0467 train_time:925309ms step_avg:170.16ms\nstep:5449/10000 train_loss:3.8844 train_time:925480ms step_avg:170.16ms\nstep:5450/10000 train_loss:3.8396 train_time:925649ms step_avg:170.16ms\nstep:5451/10000 train_loss:3.9160 train_time:925819ms step_avg:170.16ms\nstep:5452/10000 train_loss:3.9076 train_time:925986ms step_avg:170.16ms\nstep:5453/10000 train_loss:3.9815 train_time:926158ms step_avg:170.16ms\nstep:5454/10000 train_loss:3.8639 train_time:926329ms step_avg:170.16ms\nstep:5455/10000 train_loss:3.7198 train_time:926499ms step_avg:170.16ms\nstep:5456/10000 train_loss:3.8177 train_time:926668ms step_avg:170.16ms\nstep:5457/10000 train_loss:3.9020 train_time:926839ms step_avg:170.16ms\nstep:5458/10000 train_loss:3.8745 train_time:927010ms step_avg:170.16ms\nstep:5459/10000 train_loss:3.9277 train_time:927178ms step_avg:170.16ms\nstep:5460/10000 train_loss:3.9457 train_time:927351ms step_avg:170.16ms\nstep:5461/10000 train_loss:3.9529 train_time:927522ms step_avg:170.16ms\nstep:5462/10000 train_loss:3.8099 train_time:927690ms step_avg:170.16ms\nstep:5463/10000 train_loss:3.9717 train_time:927859ms step_avg:170.16ms\nstep:5464/10000 train_loss:3.8376 train_time:928031ms step_avg:170.16ms\nstep:5465/10000 train_loss:4.0161 train_time:928200ms step_avg:170.16ms\nstep:5466/10000 train_loss:3.7610 train_time:928370ms step_avg:170.16ms\nstep:5467/10000 train_loss:3.7793 train_time:928541ms step_avg:170.16ms\nstep:5468/10000 train_loss:3.8936 train_time:928710ms step_avg:170.16ms\nstep:5469/10000 train_loss:3.9821 train_time:928881ms step_avg:170.16ms\nstep:5470/10000 train_loss:3.7877 train_time:929050ms step_avg:170.16ms\nstep:5471/10000 train_loss:3.9149 train_time:929219ms step_avg:170.16ms\nstep:5472/10000 train_loss:3.8993 train_time:929388ms step_avg:170.16ms\nstep:5473/10000 train_loss:3.9114 train_time:929560ms step_avg:170.16ms\nstep:5474/10000 train_loss:3.8708 train_time:929732ms step_avg:170.16ms\nstep:5475/10000 train_loss:3.8350 train_time:929902ms step_avg:170.16ms\nstep:5476/10000 train_loss:3.9932 train_time:930072ms step_avg:170.16ms\nstep:5477/10000 train_loss:3.8846 train_time:930242ms step_avg:170.16ms\nstep:5478/10000 train_loss:3.9287 train_time:930410ms step_avg:170.16ms\nstep:5479/10000 train_loss:3.8921 train_time:930581ms step_avg:170.16ms\nstep:5480/10000 train_loss:3.9800 train_time:930753ms step_avg:170.16ms\nstep:5481/10000 train_loss:4.0758 train_time:930923ms step_avg:170.16ms\nstep:5482/10000 train_loss:3.9068 train_time:931094ms step_avg:170.16ms\nstep:5483/10000 train_loss:3.9331 train_time:931263ms step_avg:170.16ms\nstep:5484/10000 train_loss:3.9488 train_time:931433ms step_avg:170.16ms\nstep:5485/10000 train_loss:3.8960 train_time:931602ms step_avg:170.16ms\nstep:5486/10000 train_loss:4.0891 train_time:931771ms step_avg:170.16ms\nstep:5487/10000 train_loss:3.9191 train_time:931943ms step_avg:170.16ms\nstep:5488/10000 train_loss:3.7953 train_time:932113ms step_avg:170.16ms\nstep:5489/10000 train_loss:3.8052 train_time:932285ms step_avg:170.16ms\nstep:5490/10000 train_loss:3.8995 train_time:932452ms step_avg:170.16ms\nstep:5491/10000 train_loss:3.8444 train_time:932623ms step_avg:170.16ms\nstep:5492/10000 train_loss:4.1572 train_time:932793ms step_avg:170.16ms\nstep:5493/10000 train_loss:3.8557 train_time:932964ms step_avg:170.16ms\nstep:5494/10000 train_loss:3.7763 train_time:933133ms step_avg:170.16ms\nstep:5495/10000 train_loss:4.4172 train_time:933306ms step_avg:170.16ms\nstep:5496/10000 train_loss:3.9250 train_time:933474ms step_avg:170.16ms\nstep:5497/10000 train_loss:3.8776 train_time:933642ms step_avg:170.16ms\nstep:5498/10000 train_loss:3.7824 train_time:933812ms step_avg:170.16ms\nstep:5499/10000 train_loss:3.8688 train_time:933987ms step_avg:170.16ms\nstep:5500/10000 train_loss:3.9424 train_time:934158ms step_avg:170.16ms\nstep:5501/10000 train_loss:3.8478 train_time:934327ms step_avg:170.16ms\nstep:5502/10000 train_loss:4.0128 train_time:934496ms step_avg:170.16ms\nstep:5503/10000 train_loss:3.9989 train_time:934667ms step_avg:170.16ms\nstep:5504/10000 train_loss:3.9331 train_time:934836ms step_avg:170.16ms\nstep:5505/10000 train_loss:3.8095 train_time:935005ms step_avg:170.16ms\nstep:5506/10000 train_loss:3.9151 train_time:935176ms step_avg:170.16ms\nstep:5507/10000 train_loss:3.9117 train_time:935345ms step_avg:170.16ms\nstep:5508/10000 train_loss:3.7646 train_time:935513ms step_avg:170.16ms\nstep:5509/10000 train_loss:3.8757 train_time:935682ms step_avg:170.15ms\nstep:5510/10000 train_loss:3.9896 train_time:935851ms step_avg:170.15ms\nstep:5511/10000 train_loss:3.8682 train_time:936021ms step_avg:170.15ms\nstep:5512/10000 train_loss:4.1466 train_time:936192ms step_avg:170.15ms\nstep:5513/10000 train_loss:3.9757 train_time:936367ms step_avg:170.16ms\nstep:5514/10000 train_loss:3.8174 train_time:936535ms step_avg:170.16ms\nstep:5515/10000 train_loss:3.9110 train_time:936704ms step_avg:170.16ms\nstep:5516/10000 train_loss:3.9152 train_time:936873ms step_avg:170.15ms\nstep:5517/10000 train_loss:3.8910 train_time:937044ms step_avg:170.16ms\nstep:5518/10000 train_loss:3.8972 train_time:937215ms step_avg:170.16ms\nstep:5519/10000 train_loss:4.1203 train_time:937386ms step_avg:170.16ms\nstep:5520/10000 train_loss:4.0477 train_time:937557ms step_avg:170.16ms\nstep:5521/10000 train_loss:3.9229 train_time:937729ms step_avg:170.16ms\nstep:5522/10000 train_loss:3.8505 train_time:937901ms step_avg:170.16ms\nstep:5523/10000 train_loss:3.8494 train_time:938068ms step_avg:170.16ms\nstep:5524/10000 train_loss:4.1026 train_time:938238ms step_avg:170.16ms\nstep:5525/10000 train_loss:3.7852 train_time:938408ms step_avg:170.16ms\nstep:5526/10000 train_loss:3.8774 train_time:938578ms step_avg:170.16ms\nstep:5527/10000 train_loss:3.8818 train_time:938747ms step_avg:170.16ms\nstep:5528/10000 train_loss:3.7349 train_time:938922ms step_avg:170.16ms\nstep:5529/10000 train_loss:3.8308 train_time:939088ms step_avg:170.16ms\nstep:5530/10000 train_loss:4.0741 train_time:939258ms step_avg:170.16ms\nstep:5531/10000 train_loss:3.7298 train_time:939432ms step_avg:170.16ms\nstep:5532/10000 train_loss:3.9053 train_time:939602ms step_avg:170.16ms\nstep:5533/10000 train_loss:3.8198 train_time:939769ms step_avg:170.16ms\nstep:5534/10000 train_loss:3.8153 train_time:939941ms step_avg:170.16ms\nstep:5535/10000 train_loss:3.6920 train_time:940113ms step_avg:170.16ms\nstep:5536/10000 train_loss:4.3519 train_time:940283ms step_avg:170.16ms\nstep:5537/10000 train_loss:3.8934 train_time:940451ms step_avg:170.16ms\nstep:5538/10000 train_loss:3.8316 train_time:940621ms step_avg:170.16ms\nstep:5539/10000 train_loss:3.9174 train_time:940789ms step_avg:170.16ms\nstep:5540/10000 train_loss:3.8332 train_time:940961ms step_avg:170.16ms\nstep:5541/10000 train_loss:3.9691 train_time:941132ms step_avg:170.16ms\nstep:5542/10000 train_loss:3.9112 train_time:941301ms step_avg:170.16ms\nstep:5543/10000 train_loss:3.8178 train_time:941469ms step_avg:170.16ms\nstep:5544/10000 train_loss:3.9733 train_time:941641ms step_avg:170.16ms\nstep:5545/10000 train_loss:3.6343 train_time:941807ms step_avg:170.15ms\nstep:5546/10000 train_loss:3.8773 train_time:941978ms step_avg:170.15ms\nstep:5547/10000 train_loss:4.1573 train_time:942146ms step_avg:170.15ms\nstep:5548/10000 train_loss:4.0391 train_time:942316ms step_avg:170.15ms\nstep:5549/10000 train_loss:3.8224 train_time:942491ms step_avg:170.16ms\nstep:5550/10000 train_loss:3.8508 train_time:942663ms step_avg:170.16ms\nstep:5551/10000 train_loss:3.8757 train_time:942832ms step_avg:170.16ms\nstep:5552/10000 train_loss:3.9420 train_time:942998ms step_avg:170.15ms\nstep:5553/10000 train_loss:3.9051 train_time:943169ms step_avg:170.15ms\nstep:5554/10000 train_loss:3.8003 train_time:943336ms step_avg:170.15ms\nstep:5555/10000 train_loss:3.7572 train_time:943507ms step_avg:170.15ms\nstep:5556/10000 train_loss:3.9687 train_time:943676ms step_avg:170.15ms\nstep:5557/10000 train_loss:3.9771 train_time:943852ms step_avg:170.16ms\nstep:5558/10000 train_loss:3.9376 train_time:944018ms step_avg:170.15ms\nstep:5559/10000 train_loss:3.9742 train_time:944191ms step_avg:170.16ms\nstep:5560/10000 train_loss:3.7747 train_time:944362ms step_avg:170.16ms\nstep:5561/10000 train_loss:3.9487 train_time:944531ms step_avg:170.16ms\nstep:5562/10000 train_loss:3.9675 train_time:944702ms step_avg:170.16ms\nstep:5563/10000 train_loss:3.8535 train_time:944870ms step_avg:170.15ms\nstep:5564/10000 train_loss:3.9185 train_time:945042ms step_avg:170.16ms\nstep:5565/10000 train_loss:3.8138 train_time:945210ms step_avg:170.15ms\nstep:5566/10000 train_loss:3.9943 train_time:945380ms step_avg:170.15ms\nstep:5567/10000 train_loss:3.9246 train_time:945549ms step_avg:170.15ms\nstep:5568/10000 train_loss:3.8807 train_time:945721ms step_avg:170.15ms\nstep:5569/10000 train_loss:3.8205 train_time:945891ms step_avg:170.15ms\nstep:5570/10000 train_loss:4.0649 train_time:946063ms step_avg:170.16ms\nstep:5571/10000 train_loss:3.9857 train_time:946231ms step_avg:170.15ms\nstep:5572/10000 train_loss:4.1686 train_time:946401ms step_avg:170.15ms\nstep:5573/10000 train_loss:3.8750 train_time:946570ms step_avg:170.15ms\nstep:5574/10000 train_loss:3.9503 train_time:946739ms step_avg:170.15ms\nstep:5575/10000 train_loss:3.8252 train_time:946910ms step_avg:170.15ms\nstep:5576/10000 train_loss:3.8733 train_time:947083ms step_avg:170.15ms\nstep:5577/10000 train_loss:4.0234 train_time:947256ms step_avg:170.16ms\nstep:5578/10000 train_loss:3.9779 train_time:947422ms step_avg:170.15ms\nstep:5579/10000 train_loss:3.8812 train_time:947591ms step_avg:170.15ms\nstep:5580/10000 train_loss:3.8628 train_time:947762ms step_avg:170.15ms\nstep:5581/10000 train_loss:3.8519 train_time:947931ms step_avg:170.15ms\nstep:5582/10000 train_loss:3.9430 train_time:948100ms step_avg:170.15ms\nstep:5583/10000 train_loss:3.9590 train_time:948272ms step_avg:170.15ms\nstep:5584/10000 train_loss:3.8565 train_time:948442ms step_avg:170.15ms\nstep:5585/10000 train_loss:3.9513 train_time:948611ms step_avg:170.15ms\nstep:5586/10000 train_loss:3.7403 train_time:948782ms step_avg:170.15ms\nstep:5587/10000 train_loss:3.8891 train_time:948949ms step_avg:170.15ms\nstep:5588/10000 train_loss:4.1159 train_time:949121ms step_avg:170.15ms\nstep:5589/10000 train_loss:4.0565 train_time:949291ms step_avg:170.15ms\nstep:5590/10000 train_loss:4.0101 train_time:949461ms step_avg:170.15ms\nstep:5591/10000 train_loss:3.8623 train_time:949629ms step_avg:170.15ms\nstep:5592/10000 train_loss:4.0156 train_time:949800ms step_avg:170.15ms\nstep:5593/10000 train_loss:3.9370 train_time:949970ms step_avg:170.15ms\nstep:5594/10000 train_loss:3.8897 train_time:950141ms step_avg:170.15ms\nstep:5595/10000 train_loss:4.0277 train_time:950309ms step_avg:170.15ms\nstep:5596/10000 train_loss:3.9997 train_time:950479ms step_avg:170.15ms\nstep:5597/10000 train_loss:3.8961 train_time:950650ms step_avg:170.15ms\nstep:5598/10000 train_loss:3.8036 train_time:950821ms step_avg:170.15ms\nstep:5599/10000 train_loss:3.8984 train_time:950989ms step_avg:170.15ms\nstep:5600/10000 train_loss:3.8573 train_time:951162ms step_avg:170.15ms\nstep:5601/10000 train_loss:3.9794 train_time:951332ms step_avg:170.15ms\nstep:5602/10000 train_loss:3.7353 train_time:951504ms step_avg:170.15ms\nstep:5603/10000 train_loss:3.9729 train_time:951671ms step_avg:170.15ms\nstep:5604/10000 train_loss:3.8632 train_time:951841ms step_avg:170.15ms\nstep:5605/10000 train_loss:4.0023 train_time:952014ms step_avg:170.15ms\nstep:5606/10000 train_loss:3.7312 train_time:952186ms step_avg:170.15ms\nstep:5607/10000 train_loss:3.9300 train_time:952354ms step_avg:170.15ms\nstep:5608/10000 train_loss:3.9294 train_time:952525ms step_avg:170.15ms\nstep:5609/10000 train_loss:4.0442 train_time:952694ms step_avg:170.15ms\nstep:5610/10000 train_loss:4.0894 train_time:952866ms step_avg:170.15ms\nstep:5611/10000 train_loss:3.6836 train_time:953035ms step_avg:170.15ms\nstep:5612/10000 train_loss:3.8795 train_time:953207ms step_avg:170.15ms\nstep:5613/10000 train_loss:3.9267 train_time:953378ms step_avg:170.16ms\nstep:5614/10000 train_loss:4.1189 train_time:953550ms step_avg:170.16ms\nstep:5615/10000 train_loss:3.8974 train_time:953721ms step_avg:170.16ms\nstep:5616/10000 train_loss:3.8935 train_time:953891ms step_avg:170.16ms\nstep:5617/10000 train_loss:3.7215 train_time:954062ms step_avg:170.16ms\nstep:5618/10000 train_loss:3.9702 train_time:954229ms step_avg:170.16ms\nstep:5619/10000 train_loss:3.9644 train_time:954400ms step_avg:170.16ms\nstep:5620/10000 train_loss:3.8801 train_time:954569ms step_avg:170.15ms\nstep:5621/10000 train_loss:3.8731 train_time:954741ms step_avg:170.16ms\nstep:5622/10000 train_loss:3.8772 train_time:954910ms step_avg:170.15ms\nstep:5623/10000 train_loss:3.8990 train_time:955083ms step_avg:170.16ms\nstep:5624/10000 train_loss:4.0959 train_time:955252ms step_avg:170.16ms\nstep:5625/10000 train_loss:3.9372 train_time:955424ms step_avg:170.16ms\nstep:5626/10000 train_loss:3.9694 train_time:955595ms step_avg:170.16ms\nstep:5627/10000 train_loss:3.9250 train_time:955768ms step_avg:170.16ms\nstep:5628/10000 train_loss:3.8376 train_time:955936ms step_avg:170.16ms\nstep:5629/10000 train_loss:3.8659 train_time:956106ms step_avg:170.16ms\nstep:5630/10000 train_loss:3.8391 train_time:956274ms step_avg:170.16ms\nstep:5631/10000 train_loss:3.8577 train_time:956443ms step_avg:170.16ms\nstep:5632/10000 train_loss:3.9074 train_time:956613ms step_avg:170.16ms\nstep:5633/10000 train_loss:3.9123 train_time:956787ms step_avg:170.16ms\nstep:5634/10000 train_loss:3.7117 train_time:956957ms step_avg:170.16ms\nstep:5635/10000 train_loss:3.9345 train_time:957126ms step_avg:170.16ms\nstep:5636/10000 train_loss:3.9127 train_time:957296ms step_avg:170.16ms\nstep:5637/10000 train_loss:4.0896 train_time:957466ms step_avg:170.16ms\nstep:5638/10000 train_loss:3.8473 train_time:957637ms step_avg:170.16ms\nstep:5639/10000 train_loss:3.8288 train_time:957812ms step_avg:170.16ms\nstep:5640/10000 train_loss:3.7826 train_time:957984ms step_avg:170.16ms\nstep:5641/10000 train_loss:3.9978 train_time:958153ms step_avg:170.16ms\nstep:5642/10000 train_loss:3.9307 train_time:958325ms step_avg:170.16ms\nstep:5643/10000 train_loss:3.8865 train_time:958492ms step_avg:170.16ms\nstep:5644/10000 train_loss:3.8580 train_time:958660ms step_avg:170.16ms\nstep:5645/10000 train_loss:3.9806 train_time:958828ms step_avg:170.16ms\nstep:5646/10000 train_loss:4.2388 train_time:959000ms step_avg:170.16ms\nstep:5647/10000 train_loss:3.8782 train_time:959169ms step_avg:170.16ms\nstep:5648/10000 train_loss:3.8492 train_time:959340ms step_avg:170.16ms\nstep:5649/10000 train_loss:3.9227 train_time:959511ms step_avg:170.16ms\nstep:5650/10000 train_loss:4.0446 train_time:959683ms step_avg:170.16ms\nstep:5651/10000 train_loss:3.9015 train_time:959851ms step_avg:170.16ms\nstep:5652/10000 train_loss:3.8483 train_time:960022ms step_avg:170.16ms\nstep:5653/10000 train_loss:3.9274 train_time:960192ms step_avg:170.16ms\nstep:5654/10000 train_loss:4.0569 train_time:960362ms step_avg:170.16ms\nstep:5655/10000 train_loss:3.8834 train_time:960530ms step_avg:170.16ms\nstep:5656/10000 train_loss:3.9629 train_time:960701ms step_avg:170.16ms\nstep:5657/10000 train_loss:3.9847 train_time:960870ms step_avg:170.16ms\nstep:5658/10000 train_loss:4.0503 train_time:961040ms step_avg:170.16ms\nstep:5659/10000 train_loss:3.8736 train_time:961210ms step_avg:170.16ms\nstep:5660/10000 train_loss:3.8565 train_time:961381ms step_avg:170.16ms\nstep:5661/10000 train_loss:3.9217 train_time:961551ms step_avg:170.16ms\nstep:5662/10000 train_loss:3.8524 train_time:961722ms step_avg:170.16ms\nstep:5663/10000 train_loss:3.9789 train_time:961889ms step_avg:170.16ms\nstep:5664/10000 train_loss:3.9372 train_time:962060ms step_avg:170.16ms\nstep:5665/10000 train_loss:3.8999 train_time:962231ms step_avg:170.16ms\nstep:5666/10000 train_loss:3.9307 train_time:962403ms step_avg:170.16ms\nstep:5667/10000 train_loss:3.9550 train_time:962572ms step_avg:170.16ms\nstep:5668/10000 train_loss:3.9383 train_time:962746ms step_avg:170.16ms\nstep:5669/10000 train_loss:3.7566 train_time:962915ms step_avg:170.16ms\nstep:5670/10000 train_loss:3.7158 train_time:963084ms step_avg:170.16ms\nstep:5671/10000 train_loss:3.8878 train_time:963253ms step_avg:170.16ms\nstep:5672/10000 train_loss:4.0004 train_time:963422ms step_avg:170.16ms\nstep:5673/10000 train_loss:3.9353 train_time:963591ms step_avg:170.16ms\nstep:5674/10000 train_loss:3.3646 train_time:963764ms step_avg:170.16ms\nstep:5675/10000 train_loss:3.9278 train_time:963931ms step_avg:170.16ms\nstep:5676/10000 train_loss:3.8053 train_time:964101ms step_avg:170.16ms\nstep:5677/10000 train_loss:3.9722 train_time:964272ms step_avg:170.16ms\nstep:5678/10000 train_loss:3.9293 train_time:964440ms step_avg:170.16ms\nstep:5679/10000 train_loss:3.9102 train_time:964610ms step_avg:170.16ms\nstep:5680/10000 train_loss:3.8665 train_time:964784ms step_avg:170.16ms\nstep:5681/10000 train_loss:4.1041 train_time:964954ms step_avg:170.16ms\nstep:5682/10000 train_loss:3.9803 train_time:965124ms step_avg:170.16ms\nstep:5683/10000 train_loss:3.9898 train_time:965291ms step_avg:170.16ms\nstep:5684/10000 train_loss:3.9542 train_time:965461ms step_avg:170.16ms\nstep:5685/10000 train_loss:3.9833 train_time:965631ms step_avg:170.16ms\nstep:5686/10000 train_loss:3.8827 train_time:965804ms step_avg:170.16ms\nstep:5687/10000 train_loss:3.8191 train_time:965975ms step_avg:170.16ms\nstep:5688/10000 train_loss:4.0236 train_time:966148ms step_avg:170.16ms\nstep:5689/10000 train_loss:4.1257 train_time:966319ms step_avg:170.16ms\nstep:5690/10000 train_loss:3.9475 train_time:966486ms step_avg:170.16ms\nstep:5691/10000 train_loss:3.9494 train_time:966657ms step_avg:170.16ms\nstep:5692/10000 train_loss:3.9669 train_time:966829ms step_avg:170.16ms\nstep:5693/10000 train_loss:3.8322 train_time:967002ms step_avg:170.16ms\nstep:5694/10000 train_loss:3.9443 train_time:967171ms step_avg:170.16ms\nstep:5695/10000 train_loss:3.9057 train_time:967342ms step_avg:170.16ms\nstep:5696/10000 train_loss:3.9293 train_time:967509ms step_avg:170.16ms\nstep:5697/10000 train_loss:3.8613 train_time:967680ms step_avg:170.16ms\nstep:5698/10000 train_loss:3.7891 train_time:967850ms step_avg:170.16ms\nstep:5699/10000 train_loss:3.8968 train_time:968023ms step_avg:170.16ms\nstep:5700/10000 train_loss:3.9499 train_time:968192ms step_avg:170.16ms\nstep:5701/10000 train_loss:3.9431 train_time:968364ms step_avg:170.16ms\nstep:5702/10000 train_loss:3.8075 train_time:968532ms step_avg:170.16ms\nstep:5703/10000 train_loss:4.1484 train_time:968703ms step_avg:170.16ms\nstep:5704/10000 train_loss:3.9376 train_time:968871ms step_avg:170.16ms\nstep:5705/10000 train_loss:3.9183 train_time:969042ms step_avg:170.16ms\nstep:5706/10000 train_loss:4.0309 train_time:969210ms step_avg:170.16ms\nstep:5707/10000 train_loss:3.8038 train_time:969383ms step_avg:170.16ms\nstep:5708/10000 train_loss:3.9081 train_time:969553ms step_avg:170.16ms\nstep:5709/10000 train_loss:3.8183 train_time:969723ms step_avg:170.16ms\nstep:5710/10000 train_loss:3.7488 train_time:969893ms step_avg:170.16ms\nstep:5711/10000 train_loss:4.0817 train_time:970064ms step_avg:170.16ms\nstep:5712/10000 train_loss:3.7747 train_time:970232ms step_avg:170.16ms\nstep:5713/10000 train_loss:3.9680 train_time:970404ms step_avg:170.16ms\nstep:5714/10000 train_loss:3.9419 train_time:970574ms step_avg:170.16ms\nstep:5715/10000 train_loss:3.8750 train_time:970744ms step_avg:170.16ms\nstep:5716/10000 train_loss:3.9130 train_time:970914ms step_avg:170.16ms\nstep:5717/10000 train_loss:4.0422 train_time:971085ms step_avg:170.16ms\nstep:5718/10000 train_loss:3.9944 train_time:971255ms step_avg:170.16ms\nstep:5719/10000 train_loss:3.8067 train_time:971427ms step_avg:170.16ms\nstep:5720/10000 train_loss:3.8121 train_time:971597ms step_avg:170.16ms\nstep:5721/10000 train_loss:3.8805 train_time:971766ms step_avg:170.16ms\nstep:5722/10000 train_loss:3.8992 train_time:971935ms step_avg:170.16ms\nstep:5723/10000 train_loss:3.9334 train_time:972105ms step_avg:170.16ms\nstep:5724/10000 train_loss:3.9459 train_time:972273ms step_avg:170.16ms\nstep:5725/10000 train_loss:4.0194 train_time:972445ms step_avg:170.16ms\nstep:5726/10000 train_loss:3.8942 train_time:972614ms step_avg:170.16ms\nstep:5727/10000 train_loss:3.8173 train_time:972784ms step_avg:170.16ms\nstep:5728/10000 train_loss:3.9267 train_time:972951ms step_avg:170.16ms\nstep:5729/10000 train_loss:3.9209 train_time:973122ms step_avg:170.16ms\nstep:5730/10000 train_loss:3.8267 train_time:973291ms step_avg:170.16ms\nstep:5731/10000 train_loss:3.8909 train_time:973462ms step_avg:170.16ms\nstep:5732/10000 train_loss:3.8431 train_time:973634ms step_avg:170.16ms\nstep:5733/10000 train_loss:3.8388 train_time:973807ms step_avg:170.16ms\nstep:5734/10000 train_loss:3.9509 train_time:973978ms step_avg:170.16ms\nstep:5735/10000 train_loss:3.8362 train_time:974148ms step_avg:170.16ms\nstep:5736/10000 train_loss:3.8785 train_time:974318ms step_avg:170.16ms\nstep:5737/10000 train_loss:3.8547 train_time:974485ms step_avg:170.16ms\nstep:5738/10000 train_loss:3.8526 train_time:974654ms step_avg:170.16ms\nstep:5739/10000 train_loss:3.9072 train_time:974826ms step_avg:170.16ms\nstep:5740/10000 train_loss:3.9425 train_time:974993ms step_avg:170.16ms\nstep:5741/10000 train_loss:3.8659 train_time:975163ms step_avg:170.16ms\nstep:5742/10000 train_loss:4.3390 train_time:975333ms step_avg:170.16ms\nstep:5743/10000 train_loss:3.8463 train_time:975504ms step_avg:170.16ms\nstep:5744/10000 train_loss:3.7665 train_time:975673ms step_avg:170.16ms\nstep:5745/10000 train_loss:3.9585 train_time:975846ms step_avg:170.16ms\nstep:5746/10000 train_loss:3.7312 train_time:976016ms step_avg:170.16ms\nstep:5747/10000 train_loss:3.8609 train_time:976184ms step_avg:170.16ms\nstep:5748/10000 train_loss:3.9093 train_time:976353ms step_avg:170.16ms\nstep:5749/10000 train_loss:3.8742 train_time:976525ms step_avg:170.16ms\nstep:5750/10000 train_loss:3.8364 train_time:976693ms step_avg:170.16ms\nstep:5751/10000 train_loss:3.7618 train_time:976865ms step_avg:170.16ms\nstep:5752/10000 train_loss:3.7654 train_time:977035ms step_avg:170.16ms\nstep:5753/10000 train_loss:3.8697 train_time:977208ms step_avg:170.16ms\nstep:5754/10000 train_loss:3.8476 train_time:977377ms step_avg:170.16ms\nstep:5755/10000 train_loss:3.6969 train_time:977544ms step_avg:170.16ms\nstep:5756/10000 train_loss:3.7885 train_time:977716ms step_avg:170.16ms\nstep:5757/10000 train_loss:3.7221 train_time:977890ms step_avg:170.16ms\nstep:5758/10000 train_loss:3.8679 train_time:978059ms step_avg:170.16ms\nstep:5759/10000 train_loss:3.8070 train_time:978228ms step_avg:170.16ms\nstep:5760/10000 train_loss:3.9931 train_time:978398ms step_avg:170.16ms\nstep:5761/10000 train_loss:3.9303 train_time:978565ms step_avg:170.16ms\nstep:5762/10000 train_loss:3.8141 train_time:978734ms step_avg:170.16ms\nstep:5763/10000 train_loss:3.9226 train_time:978908ms step_avg:170.16ms\nstep:5764/10000 train_loss:3.9331 train_time:979080ms step_avg:170.16ms\nstep:5765/10000 train_loss:3.9093 train_time:979249ms step_avg:170.16ms\nstep:5766/10000 train_loss:3.9653 train_time:979421ms step_avg:170.16ms\nstep:5767/10000 train_loss:3.8982 train_time:979592ms step_avg:170.16ms\nstep:5768/10000 train_loss:3.8984 train_time:979763ms step_avg:170.16ms\nstep:5769/10000 train_loss:3.9726 train_time:979933ms step_avg:170.16ms\nstep:5770/10000 train_loss:3.8169 train_time:980106ms step_avg:170.16ms\nstep:5771/10000 train_loss:3.8467 train_time:980274ms step_avg:170.16ms\nstep:5772/10000 train_loss:3.7509 train_time:980444ms step_avg:170.16ms\nstep:5773/10000 train_loss:3.7644 train_time:980612ms step_avg:170.16ms\nstep:5774/10000 train_loss:3.9539 train_time:980784ms step_avg:170.16ms\nstep:5775/10000 train_loss:3.9726 train_time:980952ms step_avg:170.16ms\nstep:5776/10000 train_loss:3.9384 train_time:981124ms step_avg:170.16ms\nstep:5777/10000 train_loss:3.8042 train_time:981294ms step_avg:170.16ms\nstep:5778/10000 train_loss:3.8857 train_time:981467ms step_avg:170.16ms\nstep:5779/10000 train_loss:3.8655 train_time:981635ms step_avg:170.16ms\nstep:5780/10000 train_loss:3.9994 train_time:981807ms step_avg:170.16ms\nstep:5781/10000 train_loss:4.0182 train_time:981977ms step_avg:170.16ms\nstep:5782/10000 train_loss:3.8655 train_time:982149ms step_avg:170.16ms\nstep:5783/10000 train_loss:3.7516 train_time:982319ms step_avg:170.16ms\nstep:5784/10000 train_loss:4.0217 train_time:982490ms step_avg:170.16ms\nstep:5785/10000 train_loss:3.9577 train_time:982659ms step_avg:170.16ms\nstep:5786/10000 train_loss:4.0718 train_time:982828ms step_avg:170.16ms\nstep:5787/10000 train_loss:3.9539 train_time:982998ms step_avg:170.16ms\nstep:5788/10000 train_loss:4.0343 train_time:983167ms step_avg:170.16ms\nstep:5789/10000 train_loss:3.8317 train_time:983338ms step_avg:170.16ms\nstep:5790/10000 train_loss:3.7919 train_time:983510ms step_avg:170.16ms\nstep:5791/10000 train_loss:3.9532 train_time:983681ms step_avg:170.16ms\nstep:5792/10000 train_loss:3.9129 train_time:983851ms step_avg:170.16ms\nstep:5793/10000 train_loss:3.9265 train_time:984021ms step_avg:170.16ms\nstep:5794/10000 train_loss:3.6913 train_time:984189ms step_avg:170.16ms\nstep:5795/10000 train_loss:3.9376 train_time:984367ms step_avg:170.16ms\nstep:5796/10000 train_loss:3.8074 train_time:984535ms step_avg:170.16ms\nstep:5797/10000 train_loss:3.9077 train_time:984709ms step_avg:170.16ms\nstep:5798/10000 train_loss:4.0375 train_time:984876ms step_avg:170.16ms\nstep:5799/10000 train_loss:3.9318 train_time:985047ms step_avg:170.16ms\nstep:5800/10000 train_loss:3.8493 train_time:985215ms step_avg:170.16ms\nstep:5801/10000 train_loss:3.9243 train_time:985387ms step_avg:170.16ms\nstep:5802/10000 train_loss:3.8951 train_time:985556ms step_avg:170.16ms\nstep:5803/10000 train_loss:3.6954 train_time:985727ms step_avg:170.16ms\nstep:5804/10000 train_loss:3.8875 train_time:985897ms step_avg:170.16ms\nstep:5805/10000 train_loss:3.9439 train_time:986069ms step_avg:170.16ms\nstep:5806/10000 train_loss:3.8765 train_time:986240ms step_avg:170.16ms\nstep:5807/10000 train_loss:3.9680 train_time:986409ms step_avg:170.16ms\nstep:5808/10000 train_loss:3.9947 train_time:986578ms step_avg:170.16ms\nstep:5809/10000 train_loss:3.8815 train_time:986745ms step_avg:170.16ms\nstep:5810/10000 train_loss:3.7887 train_time:986915ms step_avg:170.16ms\nstep:5811/10000 train_loss:3.6958 train_time:987085ms step_avg:170.16ms\nstep:5812/10000 train_loss:4.0097 train_time:987254ms step_avg:170.16ms\nstep:5813/10000 train_loss:3.9045 train_time:987425ms step_avg:170.16ms\nstep:5814/10000 train_loss:3.7310 train_time:987594ms step_avg:170.16ms\nstep:5815/10000 train_loss:4.0278 train_time:987767ms step_avg:170.16ms\nstep:5816/10000 train_loss:3.8208 train_time:987937ms step_avg:170.16ms\nstep:5817/10000 train_loss:3.9654 train_time:988108ms step_avg:170.16ms\nstep:5818/10000 train_loss:3.8627 train_time:988279ms step_avg:170.16ms\nstep:5819/10000 train_loss:3.7084 train_time:988451ms step_avg:170.16ms\nstep:5820/10000 train_loss:3.9154 train_time:988621ms step_avg:170.16ms\nstep:5821/10000 train_loss:3.8646 train_time:988792ms step_avg:170.16ms\nstep:5822/10000 train_loss:3.8585 train_time:988966ms step_avg:170.16ms\nstep:5823/10000 train_loss:3.9647 train_time:989136ms step_avg:170.16ms\nstep:5824/10000 train_loss:3.8194 train_time:989308ms step_avg:170.16ms\nstep:5825/10000 train_loss:3.8907 train_time:989476ms step_avg:170.16ms\nstep:5826/10000 train_loss:3.7447 train_time:989643ms step_avg:170.16ms\nstep:5827/10000 train_loss:3.8725 train_time:989813ms step_avg:170.16ms\nstep:5828/10000 train_loss:3.8632 train_time:989986ms step_avg:170.16ms\nstep:5829/10000 train_loss:3.9997 train_time:990156ms step_avg:170.16ms\nstep:5830/10000 train_loss:3.9734 train_time:990330ms step_avg:170.16ms\nstep:5831/10000 train_loss:3.9292 train_time:990502ms step_avg:170.16ms\nstep:5832/10000 train_loss:3.9168 train_time:990671ms step_avg:170.16ms\nstep:5833/10000 train_loss:3.9901 train_time:990841ms step_avg:170.16ms\nstep:5834/10000 train_loss:3.9742 train_time:991012ms step_avg:170.16ms\nstep:5835/10000 train_loss:3.7912 train_time:991182ms step_avg:170.16ms\nstep:5836/10000 train_loss:3.8186 train_time:991352ms step_avg:170.16ms\nstep:5837/10000 train_loss:3.8311 train_time:991524ms step_avg:170.16ms\nstep:5838/10000 train_loss:3.8383 train_time:991694ms step_avg:170.16ms\nstep:5839/10000 train_loss:3.9572 train_time:991865ms step_avg:170.16ms\nstep:5840/10000 train_loss:3.7513 train_time:992034ms step_avg:170.16ms\nstep:5841/10000 train_loss:3.8289 train_time:992205ms step_avg:170.16ms\nstep:5842/10000 train_loss:3.9117 train_time:992374ms step_avg:170.16ms\nstep:5843/10000 train_loss:3.9990 train_time:992546ms step_avg:170.16ms\nstep:5844/10000 train_loss:3.9643 train_time:992714ms step_avg:170.16ms\nstep:5845/10000 train_loss:4.3010 train_time:992888ms step_avg:170.16ms\nstep:5846/10000 train_loss:3.8360 train_time:993064ms step_avg:170.16ms\nstep:5847/10000 train_loss:3.8601 train_time:993225ms step_avg:170.16ms\nstep:5848/10000 train_loss:3.8727 train_time:993396ms step_avg:170.16ms\nstep:5849/10000 train_loss:3.8247 train_time:993569ms step_avg:170.16ms\nstep:5850/10000 train_loss:3.9016 train_time:993737ms step_avg:170.16ms\nstep:5851/10000 train_loss:3.9016 train_time:993908ms step_avg:170.16ms\nstep:5852/10000 train_loss:3.9041 train_time:994076ms step_avg:170.16ms\nstep:5853/10000 train_loss:3.7822 train_time:994246ms step_avg:170.16ms\nstep:5854/10000 train_loss:3.8502 train_time:994417ms step_avg:170.16ms\nstep:5855/10000 train_loss:3.8220 train_time:994590ms step_avg:170.16ms\nstep:5856/10000 train_loss:4.0451 train_time:994760ms step_avg:170.16ms\nstep:5857/10000 train_loss:3.8276 train_time:994937ms step_avg:170.16ms\nstep:5858/10000 train_loss:3.8429 train_time:995099ms step_avg:170.16ms\nstep:5859/10000 train_loss:3.9185 train_time:995269ms step_avg:170.16ms\nstep:5860/10000 train_loss:3.8620 train_time:995440ms step_avg:170.16ms\nstep:5861/10000 train_loss:3.7810 train_time:995610ms step_avg:170.16ms\nstep:5862/10000 train_loss:3.7827 train_time:995783ms step_avg:170.16ms\nstep:5863/10000 train_loss:3.7857 train_time:995954ms step_avg:170.16ms\nstep:5864/10000 train_loss:4.0052 train_time:996125ms step_avg:170.16ms\nstep:5865/10000 train_loss:3.8728 train_time:996294ms step_avg:170.16ms\nstep:5866/10000 train_loss:3.9728 train_time:996464ms step_avg:170.16ms\nstep:5867/10000 train_loss:4.0091 train_time:996635ms step_avg:170.16ms\nstep:5868/10000 train_loss:3.9117 train_time:996806ms step_avg:170.16ms\nstep:5869/10000 train_loss:3.9258 train_time:996976ms step_avg:170.16ms\nstep:5870/10000 train_loss:3.9404 train_time:997148ms step_avg:170.16ms\nstep:5871/10000 train_loss:3.8841 train_time:997318ms step_avg:170.16ms\nstep:5872/10000 train_loss:3.9219 train_time:997487ms step_avg:170.16ms\nstep:5873/10000 train_loss:3.9952 train_time:997658ms step_avg:170.16ms\nstep:5874/10000 train_loss:3.8310 train_time:997829ms step_avg:170.16ms\nstep:5875/10000 train_loss:3.8701 train_time:997999ms step_avg:170.16ms\nstep:5876/10000 train_loss:3.8524 train_time:998171ms step_avg:170.16ms\nstep:5877/10000 train_loss:3.9092 train_time:998341ms step_avg:170.16ms\nstep:5878/10000 train_loss:3.8580 train_time:998510ms step_avg:170.16ms\nstep:5879/10000 train_loss:3.9349 train_time:998680ms step_avg:170.16ms\nstep:5880/10000 train_loss:4.0266 train_time:998851ms step_avg:170.16ms\nstep:5881/10000 train_loss:3.8383 train_time:999021ms step_avg:170.16ms\nstep:5882/10000 train_loss:3.8992 train_time:999189ms step_avg:170.16ms\nstep:5883/10000 train_loss:3.8259 train_time:999361ms step_avg:170.16ms\nstep:5884/10000 train_loss:3.8691 train_time:999531ms step_avg:170.16ms\nstep:5885/10000 train_loss:3.9309 train_time:999701ms step_avg:170.16ms\nstep:5886/10000 train_loss:3.7585 train_time:999870ms step_avg:170.16ms\nstep:5887/10000 train_loss:3.8742 train_time:1000043ms step_avg:170.16ms\nstep:5888/10000 train_loss:3.8233 train_time:1000213ms step_avg:170.16ms\nstep:5889/10000 train_loss:3.8300 train_time:1000384ms step_avg:170.16ms\nstep:5890/10000 train_loss:3.7736 train_time:1000552ms step_avg:170.16ms\nstep:5891/10000 train_loss:3.9063 train_time:1000721ms step_avg:170.16ms\nstep:5892/10000 train_loss:3.8891 train_time:1000890ms step_avg:170.16ms\nstep:5893/10000 train_loss:3.8458 train_time:1001061ms step_avg:170.16ms\nstep:5894/10000 train_loss:3.9424 train_time:1001230ms step_avg:170.16ms\nstep:5895/10000 train_loss:3.6573 train_time:1001400ms step_avg:170.16ms\nstep:5896/10000 train_loss:3.8189 train_time:1001571ms step_avg:170.16ms\nstep:5897/10000 train_loss:3.9070 train_time:1001742ms step_avg:170.16ms\nstep:5898/10000 train_loss:3.8530 train_time:1001913ms step_avg:170.16ms\nstep:5899/10000 train_loss:3.8545 train_time:1002086ms step_avg:170.16ms\nstep:5900/10000 train_loss:3.9494 train_time:1002255ms step_avg:170.16ms\nstep:5901/10000 train_loss:3.8514 train_time:1002424ms step_avg:170.16ms\nstep:5902/10000 train_loss:3.9769 train_time:1002592ms step_avg:170.16ms\nstep:5903/10000 train_loss:3.9417 train_time:1002764ms step_avg:170.16ms\nstep:5904/10000 train_loss:3.9861 train_time:1002934ms step_avg:170.16ms\nstep:5905/10000 train_loss:3.9048 train_time:1003106ms step_avg:170.16ms\nstep:5906/10000 train_loss:3.8108 train_time:1003276ms step_avg:170.16ms\nstep:5907/10000 train_loss:3.8355 train_time:1003448ms step_avg:170.16ms\nstep:5908/10000 train_loss:3.8988 train_time:1003617ms step_avg:170.16ms\nstep:5909/10000 train_loss:3.8417 train_time:1003787ms step_avg:170.16ms\nstep:5910/10000 train_loss:3.9033 train_time:1003958ms step_avg:170.16ms\nstep:5911/10000 train_loss:3.8582 train_time:1004131ms step_avg:170.16ms\nstep:5912/10000 train_loss:3.7009 train_time:1004301ms step_avg:170.16ms\nstep:5913/10000 train_loss:3.7028 train_time:1004471ms step_avg:170.16ms\nstep:5914/10000 train_loss:4.0240 train_time:1004642ms step_avg:170.16ms\nstep:5915/10000 train_loss:3.8440 train_time:1004810ms step_avg:170.16ms\nstep:5916/10000 train_loss:3.8195 train_time:1004980ms step_avg:170.16ms\nstep:5917/10000 train_loss:3.7937 train_time:1005149ms step_avg:170.16ms\nstep:5918/10000 train_loss:3.8218 train_time:1005319ms step_avg:170.16ms\nstep:5919/10000 train_loss:4.0035 train_time:1005489ms step_avg:170.16ms\nstep:5920/10000 train_loss:3.7946 train_time:1005661ms step_avg:170.16ms\nstep:5921/10000 train_loss:3.6508 train_time:1005829ms step_avg:170.16ms\nstep:5922/10000 train_loss:3.7977 train_time:1005998ms step_avg:170.16ms\nstep:5923/10000 train_loss:3.7754 train_time:1006171ms step_avg:170.16ms\nstep:5924/10000 train_loss:3.9133 train_time:1006343ms step_avg:170.16ms\nstep:5925/10000 train_loss:3.8926 train_time:1006513ms step_avg:170.16ms\nstep:5926/10000 train_loss:3.8522 train_time:1006684ms step_avg:170.16ms\nstep:5927/10000 train_loss:3.9617 train_time:1006853ms step_avg:170.16ms\nstep:5928/10000 train_loss:3.9719 train_time:1007025ms step_avg:170.16ms\nstep:5929/10000 train_loss:3.8184 train_time:1007194ms step_avg:170.16ms\nstep:5930/10000 train_loss:3.8487 train_time:1007366ms step_avg:170.16ms\nstep:5931/10000 train_loss:3.7910 train_time:1007536ms step_avg:170.16ms\nstep:5932/10000 train_loss:3.8460 train_time:1007710ms step_avg:170.16ms\nstep:5933/10000 train_loss:3.8648 train_time:1007879ms step_avg:170.16ms\nstep:5934/10000 train_loss:4.0240 train_time:1008048ms step_avg:170.16ms\nstep:5935/10000 train_loss:3.9680 train_time:1008220ms step_avg:170.16ms\nstep:5936/10000 train_loss:4.0172 train_time:1008390ms step_avg:170.16ms\nstep:5937/10000 train_loss:3.8511 train_time:1008563ms step_avg:170.16ms\nstep:5938/10000 train_loss:3.7391 train_time:1008735ms step_avg:170.16ms\nstep:5939/10000 train_loss:3.9033 train_time:1008905ms step_avg:170.16ms\nstep:5940/10000 train_loss:3.9598 train_time:1009073ms step_avg:170.16ms\nstep:5941/10000 train_loss:3.8098 train_time:1009245ms step_avg:170.16ms\nstep:5942/10000 train_loss:3.8763 train_time:1009413ms step_avg:170.16ms\nstep:5943/10000 train_loss:3.8895 train_time:1009585ms step_avg:170.16ms\nstep:5944/10000 train_loss:3.8557 train_time:1009754ms step_avg:170.16ms\nstep:5945/10000 train_loss:3.9409 train_time:1009922ms step_avg:170.16ms\nstep:5946/10000 train_loss:3.9596 train_time:1010091ms step_avg:170.16ms\nstep:5947/10000 train_loss:3.8009 train_time:1010261ms step_avg:170.16ms\nstep:5948/10000 train_loss:3.8424 train_time:1010431ms step_avg:170.16ms\nstep:5949/10000 train_loss:3.9661 train_time:1010604ms step_avg:170.16ms\nstep:5950/10000 train_loss:3.8614 train_time:1010777ms step_avg:170.16ms\nstep:5951/10000 train_loss:3.9108 train_time:1010947ms step_avg:170.16ms\nstep:5952/10000 train_loss:3.8140 train_time:1011118ms step_avg:170.16ms\nstep:5953/10000 train_loss:3.9029 train_time:1011286ms step_avg:170.16ms\nstep:5954/10000 train_loss:3.7530 train_time:1011455ms step_avg:170.16ms\nstep:5955/10000 train_loss:3.9693 train_time:1011627ms step_avg:170.16ms\nstep:5956/10000 train_loss:3.8194 train_time:1011800ms step_avg:170.16ms\nstep:5957/10000 train_loss:3.8751 train_time:1011970ms step_avg:170.16ms\nstep:5958/10000 train_loss:3.8767 train_time:1012139ms step_avg:170.16ms\nstep:5959/10000 train_loss:3.9238 train_time:1012309ms step_avg:170.16ms\nstep:5960/10000 train_loss:3.8691 train_time:1012478ms step_avg:170.16ms\nstep:5961/10000 train_loss:4.0417 train_time:1012649ms step_avg:170.16ms\nstep:5962/10000 train_loss:3.6831 train_time:1012820ms step_avg:170.16ms\nstep:5963/10000 train_loss:3.9078 train_time:1012988ms step_avg:170.16ms\nstep:5964/10000 train_loss:3.7811 train_time:1013160ms step_avg:170.16ms\nstep:5965/10000 train_loss:3.9353 train_time:1013329ms step_avg:170.16ms\nstep:5966/10000 train_loss:3.6067 train_time:1013497ms step_avg:170.16ms\nstep:5967/10000 train_loss:4.2999 train_time:1013669ms step_avg:170.16ms\nstep:5968/10000 train_loss:3.9198 train_time:1013842ms step_avg:170.16ms\nstep:5969/10000 train_loss:3.9008 train_time:1014009ms step_avg:170.16ms\nstep:5970/10000 train_loss:3.7206 train_time:1014179ms step_avg:170.16ms\nstep:5971/10000 train_loss:4.0421 train_time:1014348ms step_avg:170.16ms\nstep:5972/10000 train_loss:3.8383 train_time:1014517ms step_avg:170.16ms\nstep:5973/10000 train_loss:3.8090 train_time:1014685ms step_avg:170.16ms\nstep:5974/10000 train_loss:4.0739 train_time:1014857ms step_avg:170.16ms\nstep:5975/10000 train_loss:4.0708 train_time:1015027ms step_avg:170.16ms\nstep:5976/10000 train_loss:3.7824 train_time:1015198ms step_avg:170.16ms\nstep:5977/10000 train_loss:3.9829 train_time:1015367ms step_avg:170.16ms\nstep:5978/10000 train_loss:3.9343 train_time:1015539ms step_avg:170.16ms\nstep:5979/10000 train_loss:3.9266 train_time:1015708ms step_avg:170.16ms\nstep:5980/10000 train_loss:3.8164 train_time:1015879ms step_avg:170.16ms\nstep:5981/10000 train_loss:3.7176 train_time:1016050ms step_avg:170.16ms\nstep:5982/10000 train_loss:3.8406 train_time:1016219ms step_avg:170.16ms\nstep:5983/10000 train_loss:3.8983 train_time:1016389ms step_avg:170.16ms\nstep:5984/10000 train_loss:4.0162 train_time:1016561ms step_avg:170.16ms\nstep:5985/10000 train_loss:3.8580 train_time:1016729ms step_avg:170.16ms\nstep:5986/10000 train_loss:4.0560 train_time:1016901ms step_avg:170.16ms\nstep:5987/10000 train_loss:3.8714 train_time:1017070ms step_avg:170.16ms\nstep:5988/10000 train_loss:3.8767 train_time:1017242ms step_avg:170.16ms\nstep:5989/10000 train_loss:3.7781 train_time:1017412ms step_avg:170.16ms\nstep:5990/10000 train_loss:3.9075 train_time:1017584ms step_avg:170.16ms\nstep:5991/10000 train_loss:3.8773 train_time:1017755ms step_avg:170.16ms\nstep:5992/10000 train_loss:3.6257 train_time:1017931ms step_avg:170.17ms\nstep:5993/10000 train_loss:3.9827 train_time:1018100ms step_avg:170.17ms\nstep:5994/10000 train_loss:3.8662 train_time:1018267ms step_avg:170.16ms\nstep:5995/10000 train_loss:3.8070 train_time:1018437ms step_avg:170.16ms\nstep:5996/10000 train_loss:3.9150 train_time:1018605ms step_avg:170.16ms\nstep:5997/10000 train_loss:3.8116 train_time:1018774ms step_avg:170.16ms\nstep:5998/10000 train_loss:3.8295 train_time:1018944ms step_avg:170.16ms\nstep:5999/10000 train_loss:3.9348 train_time:1019114ms step_avg:170.16ms\nstep:6000/10000 train_loss:3.8773 train_time:1019286ms step_avg:170.16ms\nstep:6001/10000 train_loss:4.0139 train_time:1019455ms step_avg:170.16ms\nstep:6002/10000 train_loss:3.9178 train_time:1019623ms step_avg:170.16ms\nstep:6003/10000 train_loss:3.8895 train_time:1019793ms step_avg:170.16ms\nstep:6004/10000 train_loss:3.9161 train_time:1019962ms step_avg:170.16ms\nstep:6005/10000 train_loss:3.7127 train_time:1020131ms step_avg:170.16ms\nstep:6006/10000 train_loss:3.8742 train_time:1020303ms step_avg:170.16ms\nstep:6007/10000 train_loss:3.8244 train_time:1020473ms step_avg:170.16ms\nstep:6008/10000 train_loss:3.9037 train_time:1020642ms step_avg:170.16ms\nstep:6009/10000 train_loss:3.7914 train_time:1020812ms step_avg:170.16ms\nstep:6010/10000 train_loss:3.8456 train_time:1020985ms step_avg:170.16ms\nstep:6011/10000 train_loss:3.8640 train_time:1021153ms step_avg:170.16ms\nstep:6012/10000 train_loss:3.8313 train_time:1021328ms step_avg:170.16ms\nstep:6013/10000 train_loss:4.0105 train_time:1021497ms step_avg:170.16ms\nstep:6014/10000 train_loss:3.8011 train_time:1021666ms step_avg:170.16ms\nstep:6015/10000 train_loss:3.9307 train_time:1021834ms step_avg:170.16ms\nstep:6016/10000 train_loss:3.6611 train_time:1022005ms step_avg:170.16ms\nstep:6017/10000 train_loss:3.5415 train_time:1022176ms step_avg:170.16ms\nstep:6018/10000 train_loss:3.7774 train_time:1022346ms step_avg:170.16ms\nstep:6019/10000 train_loss:4.1310 train_time:1022518ms step_avg:170.16ms\nstep:6020/10000 train_loss:3.9242 train_time:1022689ms step_avg:170.16ms\nstep:6021/10000 train_loss:3.7719 train_time:1022860ms step_avg:170.16ms\nstep:6022/10000 train_loss:3.8709 train_time:1023031ms step_avg:170.16ms\nstep:6023/10000 train_loss:3.9025 train_time:1023204ms step_avg:170.17ms\nstep:6024/10000 train_loss:3.8174 train_time:1023375ms step_avg:170.17ms\nstep:6025/10000 train_loss:3.8846 train_time:1023546ms step_avg:170.17ms\nstep:6026/10000 train_loss:3.9144 train_time:1023714ms step_avg:170.17ms\nstep:6027/10000 train_loss:3.5454 train_time:1023881ms step_avg:170.16ms\nstep:6028/10000 train_loss:3.8314 train_time:1024051ms step_avg:170.16ms\nstep:6029/10000 train_loss:3.8269 train_time:1024224ms step_avg:170.17ms\nstep:6030/10000 train_loss:3.8708 train_time:1024395ms step_avg:170.17ms\nstep:6031/10000 train_loss:3.7540 train_time:1024569ms step_avg:170.17ms\nstep:6032/10000 train_loss:3.7935 train_time:1024739ms step_avg:170.17ms\nstep:6033/10000 train_loss:3.8409 train_time:1024907ms step_avg:170.17ms\nstep:6034/10000 train_loss:3.7958 train_time:1025078ms step_avg:170.17ms\nstep:6035/10000 train_loss:3.7678 train_time:1025247ms step_avg:170.17ms\nstep:6036/10000 train_loss:3.8725 train_time:1025418ms step_avg:170.17ms\nstep:6037/10000 train_loss:4.0468 train_time:1025587ms step_avg:170.17ms\nstep:6038/10000 train_loss:3.7914 train_time:1025758ms step_avg:170.17ms\nstep:6039/10000 train_loss:3.8078 train_time:1025925ms step_avg:170.16ms\nstep:6040/10000 train_loss:3.8101 train_time:1026099ms step_avg:170.17ms\nstep:6041/10000 train_loss:3.8023 train_time:1026268ms step_avg:170.17ms\nstep:6042/10000 train_loss:3.8296 train_time:1026436ms step_avg:170.17ms\nstep:6043/10000 train_loss:3.7851 train_time:1026606ms step_avg:170.17ms\nstep:6044/10000 train_loss:4.0975 train_time:1026779ms step_avg:170.17ms\nstep:6045/10000 train_loss:3.9434 train_time:1026950ms step_avg:170.17ms\nstep:6046/10000 train_loss:3.8469 train_time:1027124ms step_avg:170.17ms\nstep:6047/10000 train_loss:3.7915 train_time:1027292ms step_avg:170.17ms\nstep:6048/10000 train_loss:3.8324 train_time:1027463ms step_avg:170.17ms\nstep:6049/10000 train_loss:3.9892 train_time:1027630ms step_avg:170.17ms\nstep:6050/10000 train_loss:3.8955 train_time:1027799ms step_avg:170.17ms\nstep:6051/10000 train_loss:3.7928 train_time:1027970ms step_avg:170.17ms\nstep:6052/10000 train_loss:3.8522 train_time:1028143ms step_avg:170.17ms\nstep:6053/10000 train_loss:3.8192 train_time:1028310ms step_avg:170.17ms\nstep:6054/10000 train_loss:3.7881 train_time:1028483ms step_avg:170.17ms\nstep:6055/10000 train_loss:3.9480 train_time:1028650ms step_avg:170.17ms\nstep:6056/10000 train_loss:3.9478 train_time:1028821ms step_avg:170.17ms\nstep:6057/10000 train_loss:3.8390 train_time:1028993ms step_avg:170.17ms\nstep:6058/10000 train_loss:3.9390 train_time:1029161ms step_avg:170.17ms\nstep:6059/10000 train_loss:3.9070 train_time:1029330ms step_avg:170.17ms\nstep:6060/10000 train_loss:3.9909 train_time:1029500ms step_avg:170.17ms\nstep:6061/10000 train_loss:3.9500 train_time:1029670ms step_avg:170.17ms\nstep:6062/10000 train_loss:3.8219 train_time:1029844ms step_avg:170.17ms\nstep:6063/10000 train_loss:3.8110 train_time:1030014ms step_avg:170.17ms\nstep:6064/10000 train_loss:3.8579 train_time:1030185ms step_avg:170.17ms\nstep:6065/10000 train_loss:3.8716 train_time:1030354ms step_avg:170.17ms\nstep:6066/10000 train_loss:3.8657 train_time:1030526ms step_avg:170.17ms\nstep:6067/10000 train_loss:3.9025 train_time:1030699ms step_avg:170.17ms\nstep:6068/10000 train_loss:3.6716 train_time:1030865ms step_avg:170.17ms\nstep:6069/10000 train_loss:3.9577 train_time:1031036ms step_avg:170.17ms\nstep:6070/10000 train_loss:3.8531 train_time:1031208ms step_avg:170.17ms\nstep:6071/10000 train_loss:3.9490 train_time:1031376ms step_avg:170.17ms\nstep:6072/10000 train_loss:3.8087 train_time:1031545ms step_avg:170.17ms\nstep:6073/10000 train_loss:3.9840 train_time:1031716ms step_avg:170.17ms\nstep:6074/10000 train_loss:3.8247 train_time:1031888ms step_avg:170.17ms\nstep:6075/10000 train_loss:3.8410 train_time:1032058ms step_avg:170.17ms\nstep:6076/10000 train_loss:3.8298 train_time:1032230ms step_avg:170.17ms\nstep:6077/10000 train_loss:4.0383 train_time:1032401ms step_avg:170.17ms\nstep:6078/10000 train_loss:3.9143 train_time:1032570ms step_avg:170.17ms\nstep:6079/10000 train_loss:3.8633 train_time:1032743ms step_avg:170.17ms\nstep:6080/10000 train_loss:3.9289 train_time:1032914ms step_avg:170.17ms\nstep:6081/10000 train_loss:3.8897 train_time:1033087ms step_avg:170.17ms\nstep:6082/10000 train_loss:3.7829 train_time:1033256ms step_avg:170.17ms\nstep:6083/10000 train_loss:3.9038 train_time:1033426ms step_avg:170.17ms\nstep:6084/10000 train_loss:3.8153 train_time:1033596ms step_avg:170.17ms\nstep:6085/10000 train_loss:3.8708 train_time:1033769ms step_avg:170.17ms\nstep:6086/10000 train_loss:3.9510 train_time:1033940ms step_avg:170.17ms\nstep:6087/10000 train_loss:3.8178 train_time:1034108ms step_avg:170.17ms\nstep:6088/10000 train_loss:4.0654 train_time:1034277ms step_avg:170.17ms\nstep:6089/10000 train_loss:3.9979 train_time:1034445ms step_avg:170.17ms\nstep:6090/10000 train_loss:3.9555 train_time:1034615ms step_avg:170.17ms\nstep:6091/10000 train_loss:3.9669 train_time:1034786ms step_avg:170.17ms\nstep:6092/10000 train_loss:3.7811 train_time:1034957ms step_avg:170.17ms\nstep:6093/10000 train_loss:3.8159 train_time:1035129ms step_avg:170.17ms\nstep:6094/10000 train_loss:3.8835 train_time:1035300ms step_avg:170.17ms\nstep:6095/10000 train_loss:3.8499 train_time:1035470ms step_avg:170.17ms\nstep:6096/10000 train_loss:3.8423 train_time:1035855ms step_avg:170.20ms\nstep:6097/10000 train_loss:3.7806 train_time:1036019ms step_avg:170.20ms\nstep:6098/10000 train_loss:3.8393 train_time:1036186ms step_avg:170.20ms\nstep:6099/10000 train_loss:3.9011 train_time:1036354ms step_avg:170.20ms\nstep:6100/10000 train_loss:3.8594 train_time:1036521ms step_avg:170.20ms\nstep:6101/10000 train_loss:3.8477 train_time:1036689ms step_avg:170.20ms\nstep:6102/10000 train_loss:3.9536 train_time:1036870ms step_avg:170.20ms\nstep:6103/10000 train_loss:3.7658 train_time:1037042ms step_avg:170.20ms\nstep:6104/10000 train_loss:3.8695 train_time:1037211ms step_avg:170.20ms\nstep:6105/10000 train_loss:3.8414 train_time:1037379ms step_avg:170.20ms\nstep:6106/10000 train_loss:3.8228 train_time:1037550ms step_avg:170.20ms\nstep:6107/10000 train_loss:3.7477 train_time:1037723ms step_avg:170.20ms\nstep:6108/10000 train_loss:3.9421 train_time:1037896ms step_avg:170.20ms\nstep:6109/10000 train_loss:3.9858 train_time:1038066ms step_avg:170.20ms\nstep:6110/10000 train_loss:3.8947 train_time:1038237ms step_avg:170.20ms\nstep:6111/10000 train_loss:3.9359 train_time:1038406ms step_avg:170.20ms\nstep:6112/10000 train_loss:3.8211 train_time:1038574ms step_avg:170.20ms\nstep:6113/10000 train_loss:3.8773 train_time:1038747ms step_avg:170.20ms\nstep:6114/10000 train_loss:3.8954 train_time:1038921ms step_avg:170.20ms\nstep:6115/10000 train_loss:3.8978 train_time:1039090ms step_avg:170.20ms\nstep:6116/10000 train_loss:3.8254 train_time:1039259ms step_avg:170.20ms\nstep:6117/10000 train_loss:3.8871 train_time:1039427ms step_avg:170.20ms\nstep:6118/10000 train_loss:3.8632 train_time:1039597ms step_avg:170.20ms\nstep:6119/10000 train_loss:3.9392 train_time:1039766ms step_avg:170.20ms\nstep:6120/10000 train_loss:3.8289 train_time:1039937ms step_avg:170.20ms\nstep:6121/10000 train_loss:3.8801 train_time:1040110ms step_avg:170.20ms\nstep:6122/10000 train_loss:3.8570 train_time:1040278ms step_avg:170.20ms\nstep:6123/10000 train_loss:3.8334 train_time:1040447ms step_avg:170.20ms\nstep:6124/10000 train_loss:3.8592 train_time:1040617ms step_avg:170.20ms\nstep:6125/10000 train_loss:3.8762 train_time:1040787ms step_avg:170.20ms\nstep:6126/10000 train_loss:3.9970 train_time:1040958ms step_avg:170.20ms\nstep:6127/10000 train_loss:4.0066 train_time:1041135ms step_avg:170.20ms\nstep:6128/10000 train_loss:3.8342 train_time:1041303ms step_avg:170.20ms\nstep:6129/10000 train_loss:3.8237 train_time:1041471ms step_avg:170.20ms\nstep:6130/10000 train_loss:3.9862 train_time:1041641ms step_avg:170.20ms\nstep:6131/10000 train_loss:3.9325 train_time:1041812ms step_avg:170.20ms\nstep:6132/10000 train_loss:3.8255 train_time:1041985ms step_avg:170.20ms\nstep:6133/10000 train_loss:3.8600 train_time:1042152ms step_avg:170.20ms\nstep:6134/10000 train_loss:3.8115 train_time:1042323ms step_avg:170.20ms\nstep:6135/10000 train_loss:3.8303 train_time:1042492ms step_avg:170.20ms\nstep:6136/10000 train_loss:4.0055 train_time:1042663ms step_avg:170.20ms\nstep:6137/10000 train_loss:3.8638 train_time:1042832ms step_avg:170.20ms\nstep:6138/10000 train_loss:3.7978 train_time:1043001ms step_avg:170.20ms\nstep:6139/10000 train_loss:3.9859 train_time:1043171ms step_avg:170.20ms\nstep:6140/10000 train_loss:3.8402 train_time:1043343ms step_avg:170.20ms\nstep:6141/10000 train_loss:3.9312 train_time:1043512ms step_avg:170.20ms\nstep:6142/10000 train_loss:3.9446 train_time:1043685ms step_avg:170.20ms\nstep:6143/10000 train_loss:3.8116 train_time:1043855ms step_avg:170.20ms\nstep:6144/10000 train_loss:3.9115 train_time:1044027ms step_avg:170.20ms\nstep:6145/10000 train_loss:3.8140 train_time:1044198ms step_avg:170.20ms\nstep:6146/10000 train_loss:3.8896 train_time:1044369ms step_avg:170.20ms\nstep:6147/10000 train_loss:3.8111 train_time:1044541ms step_avg:170.20ms\nstep:6148/10000 train_loss:3.7934 train_time:1044711ms step_avg:170.20ms\nstep:6149/10000 train_loss:3.9233 train_time:1044882ms step_avg:170.20ms\nstep:6150/10000 train_loss:3.8797 train_time:1045052ms step_avg:170.20ms\nstep:6151/10000 train_loss:3.8594 train_time:1045224ms step_avg:170.20ms\nstep:6152/10000 train_loss:3.7269 train_time:1045392ms step_avg:170.20ms\nstep:6153/10000 train_loss:3.7799 train_time:1045562ms step_avg:170.20ms\nstep:6154/10000 train_loss:3.8541 train_time:1045729ms step_avg:170.20ms\nstep:6155/10000 train_loss:3.8541 train_time:1045899ms step_avg:170.20ms\nstep:6156/10000 train_loss:3.6400 train_time:1046069ms step_avg:170.20ms\nstep:6157/10000 train_loss:4.0881 train_time:1046240ms step_avg:170.20ms\nstep:6158/10000 train_loss:3.7866 train_time:1046410ms step_avg:170.20ms\nstep:6159/10000 train_loss:3.8945 train_time:1046581ms step_avg:170.20ms\nstep:6160/10000 train_loss:3.7897 train_time:1046749ms step_avg:170.20ms\nstep:6161/10000 train_loss:3.7398 train_time:1046918ms step_avg:170.20ms\nstep:6162/10000 train_loss:3.8791 train_time:1047086ms step_avg:170.20ms\nstep:6163/10000 train_loss:3.9163 train_time:1047257ms step_avg:170.20ms\nstep:6164/10000 train_loss:3.7859 train_time:1047428ms step_avg:170.20ms\nstep:6165/10000 train_loss:3.8902 train_time:1047597ms step_avg:170.20ms\nstep:6166/10000 train_loss:4.0347 train_time:1047766ms step_avg:170.20ms\nstep:6167/10000 train_loss:3.8839 train_time:1047935ms step_avg:170.20ms\nstep:6168/10000 train_loss:3.6998 train_time:1048104ms step_avg:170.20ms\nstep:6169/10000 train_loss:3.8487 train_time:1048272ms step_avg:170.20ms\nstep:6170/10000 train_loss:3.9527 train_time:1048444ms step_avg:170.20ms\nstep:6171/10000 train_loss:4.2616 train_time:1048613ms step_avg:170.20ms\nstep:6172/10000 train_loss:4.0043 train_time:1048784ms step_avg:170.20ms\nstep:6173/10000 train_loss:3.7447 train_time:1048952ms step_avg:170.20ms\nstep:6174/10000 train_loss:3.8691 train_time:1049127ms step_avg:170.20ms\nstep:6175/10000 train_loss:3.8184 train_time:1049291ms step_avg:170.20ms\nstep:6176/10000 train_loss:3.7783 train_time:1049463ms step_avg:170.20ms\nstep:6177/10000 train_loss:3.8251 train_time:1049631ms step_avg:170.20ms\nstep:6178/10000 train_loss:3.8420 train_time:1049800ms step_avg:170.20ms\nstep:6179/10000 train_loss:3.9614 train_time:1049972ms step_avg:170.20ms\nstep:6180/10000 train_loss:3.6421 train_time:1050142ms step_avg:170.20ms\nstep:6181/10000 train_loss:4.0010 train_time:1050310ms step_avg:170.20ms\nstep:6182/10000 train_loss:3.8044 train_time:1050480ms step_avg:170.20ms\nstep:6183/10000 train_loss:3.9239 train_time:1050651ms step_avg:170.20ms\nstep:6184/10000 train_loss:3.8505 train_time:1050820ms step_avg:170.20ms\nstep:6185/10000 train_loss:3.7477 train_time:1050990ms step_avg:170.20ms\nstep:6186/10000 train_loss:3.8028 train_time:1051163ms step_avg:170.20ms\nstep:6187/10000 train_loss:3.9446 train_time:1051332ms step_avg:170.20ms\nstep:6188/10000 train_loss:3.8736 train_time:1051501ms step_avg:170.20ms\nstep:6189/10000 train_loss:3.8540 train_time:1051671ms step_avg:170.20ms\nstep:6190/10000 train_loss:3.8970 train_time:1051842ms step_avg:170.20ms\nstep:6191/10000 train_loss:3.9330 train_time:1052013ms step_avg:170.20ms\nstep:6192/10000 train_loss:3.6444 train_time:1052189ms step_avg:170.20ms\nstep:6193/10000 train_loss:3.8438 train_time:1052360ms step_avg:170.20ms\nstep:6194/10000 train_loss:3.8572 train_time:1052528ms step_avg:170.20ms\nstep:6195/10000 train_loss:3.9265 train_time:1052696ms step_avg:170.20ms\nstep:6196/10000 train_loss:4.0593 train_time:1052869ms step_avg:170.20ms\nstep:6197/10000 train_loss:4.0832 train_time:1053041ms step_avg:170.20ms\nstep:6198/10000 train_loss:3.8140 train_time:1053211ms step_avg:170.20ms\nstep:6199/10000 train_loss:3.8117 train_time:1053383ms step_avg:170.20ms\nstep:6200/10000 train_loss:3.9777 train_time:1053551ms step_avg:170.20ms\nstep:6201/10000 train_loss:3.8078 train_time:1053722ms step_avg:170.20ms\nstep:6202/10000 train_loss:3.7921 train_time:1053894ms step_avg:170.20ms\nstep:6203/10000 train_loss:3.8960 train_time:1054070ms step_avg:170.20ms\nstep:6204/10000 train_loss:3.8974 train_time:1054237ms step_avg:170.20ms\nstep:6205/10000 train_loss:3.9555 train_time:1054410ms step_avg:170.20ms\nstep:6206/10000 train_loss:3.8081 train_time:1054579ms step_avg:170.20ms\nstep:6207/10000 train_loss:3.9676 train_time:1054748ms step_avg:170.20ms\nstep:6208/10000 train_loss:3.9907 train_time:1054919ms step_avg:170.20ms\nstep:6209/10000 train_loss:3.9000 train_time:1055093ms step_avg:170.20ms\nstep:6210/10000 train_loss:3.9354 train_time:1055264ms step_avg:170.20ms\nstep:6211/10000 train_loss:3.8912 train_time:1055430ms step_avg:170.20ms\nstep:6212/10000 train_loss:3.9175 train_time:1055601ms step_avg:170.20ms\nstep:6213/10000 train_loss:3.9411 train_time:1055771ms step_avg:170.20ms\nstep:6214/10000 train_loss:3.8452 train_time:1055944ms step_avg:170.20ms\nstep:6215/10000 train_loss:3.9681 train_time:1056114ms step_avg:170.20ms\nstep:6216/10000 train_loss:3.8141 train_time:1056288ms step_avg:170.20ms\nstep:6217/10000 train_loss:3.9721 train_time:1056457ms step_avg:170.20ms\nstep:6218/10000 train_loss:3.7999 train_time:1056623ms step_avg:170.20ms\nstep:6219/10000 train_loss:3.9230 train_time:1056793ms step_avg:170.20ms\nstep:6220/10000 train_loss:3.9966 train_time:1056968ms step_avg:170.20ms\nstep:6221/10000 train_loss:3.7876 train_time:1057137ms step_avg:170.20ms\nstep:6222/10000 train_loss:3.7849 train_time:1057310ms step_avg:170.20ms\nstep:6223/10000 train_loss:3.9276 train_time:1057481ms step_avg:170.20ms\nstep:6224/10000 train_loss:3.9288 train_time:1057651ms step_avg:170.20ms\nstep:6225/10000 train_loss:3.8985 train_time:1057821ms step_avg:170.20ms\nstep:6226/10000 train_loss:3.7660 train_time:1057991ms step_avg:170.20ms\nstep:6227/10000 train_loss:3.8308 train_time:1058161ms step_avg:170.20ms\nstep:6228/10000 train_loss:3.8434 train_time:1058330ms step_avg:170.20ms\nstep:6229/10000 train_loss:3.8634 train_time:1058500ms step_avg:170.20ms\nstep:6230/10000 train_loss:3.8973 train_time:1058668ms step_avg:170.20ms\nstep:6231/10000 train_loss:3.8266 train_time:1058840ms step_avg:170.20ms\nstep:6232/10000 train_loss:3.7852 train_time:1059009ms step_avg:170.20ms\nstep:6233/10000 train_loss:3.9048 train_time:1059181ms step_avg:170.20ms\nstep:6234/10000 train_loss:3.9097 train_time:1059351ms step_avg:170.20ms\nstep:6235/10000 train_loss:3.8309 train_time:1059524ms step_avg:170.20ms\nstep:6236/10000 train_loss:3.8536 train_time:1059694ms step_avg:170.20ms\nstep:6237/10000 train_loss:3.9620 train_time:1059865ms step_avg:170.20ms\nstep:6238/10000 train_loss:3.8509 train_time:1060034ms step_avg:170.20ms\nstep:6239/10000 train_loss:3.9705 train_time:1060201ms step_avg:170.20ms\nstep:6240/10000 train_loss:3.8098 train_time:1060370ms step_avg:170.20ms\nstep:6241/10000 train_loss:3.8802 train_time:1060541ms step_avg:170.20ms\nstep:6242/10000 train_loss:3.9154 train_time:1060709ms step_avg:170.20ms\nstep:6243/10000 train_loss:3.9389 train_time:1060882ms step_avg:170.20ms\nstep:6244/10000 train_loss:3.8750 train_time:1061050ms step_avg:170.20ms\nstep:6245/10000 train_loss:3.6661 train_time:1061220ms step_avg:170.20ms\nstep:6246/10000 train_loss:3.8407 train_time:1061389ms step_avg:170.20ms\nstep:6247/10000 train_loss:3.8583 train_time:1061561ms step_avg:170.20ms\nstep:6248/10000 train_loss:3.8300 train_time:1061730ms step_avg:170.20ms\nstep:6249/10000 train_loss:3.7022 train_time:1061904ms step_avg:170.20ms\nstep:6250/10000 train_loss:3.7901 train_time:1062069ms step_avg:170.20ms\nstep:6251/10000 train_loss:3.8660 train_time:1062239ms step_avg:170.20ms\nstep:6252/10000 train_loss:3.9436 train_time:1062409ms step_avg:170.20ms\nstep:6253/10000 train_loss:4.0503 train_time:1062580ms step_avg:170.20ms\nstep:6254/10000 train_loss:3.9755 train_time:1062751ms step_avg:170.20ms\nstep:6255/10000 train_loss:3.8783 train_time:1062920ms step_avg:170.20ms\nstep:6256/10000 train_loss:3.8148 train_time:1063089ms step_avg:170.20ms\nstep:6257/10000 train_loss:3.7951 train_time:1063261ms step_avg:170.20ms\nstep:6258/10000 train_loss:3.8306 train_time:1063432ms step_avg:170.20ms\nstep:6259/10000 train_loss:3.8573 train_time:1063599ms step_avg:170.20ms\nstep:6260/10000 train_loss:3.9260 train_time:1063769ms step_avg:170.20ms\nstep:6261/10000 train_loss:3.8547 train_time:1063941ms step_avg:170.20ms\nstep:6262/10000 train_loss:3.7771 train_time:1064109ms step_avg:170.20ms\nstep:6263/10000 train_loss:4.0076 train_time:1064278ms step_avg:170.20ms\nstep:6264/10000 train_loss:3.7795 train_time:1064446ms step_avg:170.20ms\nstep:6265/10000 train_loss:3.8744 train_time:1064617ms step_avg:170.20ms\nstep:6266/10000 train_loss:3.9378 train_time:1064784ms step_avg:170.20ms\nstep:6267/10000 train_loss:3.7447 train_time:1064954ms step_avg:170.20ms\nstep:6268/10000 train_loss:3.9062 train_time:1065126ms step_avg:170.20ms\nstep:6269/10000 train_loss:3.8912 train_time:1065295ms step_avg:170.20ms\nstep:6270/10000 train_loss:3.7330 train_time:1065467ms step_avg:170.20ms\nstep:6271/10000 train_loss:3.8763 train_time:1065637ms step_avg:170.20ms\nstep:6272/10000 train_loss:3.7051 train_time:1065809ms step_avg:170.20ms\nstep:6273/10000 train_loss:3.8883 train_time:1065981ms step_avg:170.20ms\nstep:6274/10000 train_loss:3.8387 train_time:1066151ms step_avg:170.20ms\nstep:6275/10000 train_loss:3.8584 train_time:1066320ms step_avg:170.20ms\nstep:6276/10000 train_loss:3.7860 train_time:1066489ms step_avg:170.20ms\nstep:6277/10000 train_loss:3.7931 train_time:1066659ms step_avg:170.20ms\nstep:6278/10000 train_loss:3.9228 train_time:1066828ms step_avg:170.20ms\nstep:6279/10000 train_loss:3.8631 train_time:1066999ms step_avg:170.20ms\nstep:6280/10000 train_loss:3.7572 train_time:1067166ms step_avg:170.20ms\nstep:6281/10000 train_loss:3.7617 train_time:1067336ms step_avg:170.20ms\nstep:6282/10000 train_loss:3.8355 train_time:1067508ms step_avg:170.20ms\nstep:6283/10000 train_loss:3.8295 train_time:1067676ms step_avg:170.20ms\nstep:6284/10000 train_loss:3.7647 train_time:1067847ms step_avg:170.20ms\nstep:6285/10000 train_loss:3.8373 train_time:1068018ms step_avg:170.20ms\nstep:6286/10000 train_loss:3.8089 train_time:1068186ms step_avg:170.20ms\nstep:6287/10000 train_loss:3.8552 train_time:1068358ms step_avg:170.20ms\nstep:6288/10000 train_loss:3.7864 train_time:1068529ms step_avg:170.20ms\nstep:6289/10000 train_loss:3.9176 train_time:1068699ms step_avg:170.20ms\nstep:6290/10000 train_loss:3.9149 train_time:1068870ms step_avg:170.20ms\nstep:6291/10000 train_loss:3.8350 train_time:1069039ms step_avg:170.20ms\nstep:6292/10000 train_loss:3.8360 train_time:1069208ms step_avg:170.20ms\nstep:6293/10000 train_loss:3.8275 train_time:1069376ms step_avg:170.20ms\nstep:6294/10000 train_loss:3.9352 train_time:1069547ms step_avg:170.20ms\nstep:6295/10000 train_loss:3.6949 train_time:1069718ms step_avg:170.20ms\nstep:6296/10000 train_loss:3.6639 train_time:1069888ms step_avg:170.20ms\nstep:6297/10000 train_loss:3.8971 train_time:1070062ms step_avg:170.20ms\nstep:6298/10000 train_loss:3.7397 train_time:1070231ms step_avg:170.20ms\nstep:6299/10000 train_loss:3.8030 train_time:1070403ms step_avg:170.20ms\nstep:6300/10000 train_loss:3.8454 train_time:1070570ms step_avg:170.20ms\nstep:6301/10000 train_loss:3.8337 train_time:1070740ms step_avg:170.20ms\nstep:6302/10000 train_loss:3.7699 train_time:1070909ms step_avg:170.20ms\nstep:6303/10000 train_loss:3.7444 train_time:1071081ms step_avg:170.20ms\nstep:6304/10000 train_loss:3.8544 train_time:1071251ms step_avg:170.20ms\nstep:6305/10000 train_loss:3.8305 train_time:1071421ms step_avg:170.20ms\nstep:6306/10000 train_loss:3.8851 train_time:1071589ms step_avg:170.20ms\nstep:6307/10000 train_loss:3.8208 train_time:1071761ms step_avg:170.20ms\nstep:6308/10000 train_loss:3.8315 train_time:1071930ms step_avg:170.20ms\nstep:6309/10000 train_loss:3.9060 train_time:1072103ms step_avg:170.20ms\nstep:6310/10000 train_loss:3.7599 train_time:1072274ms step_avg:170.20ms\nstep:6311/10000 train_loss:3.8510 train_time:1072448ms step_avg:170.20ms\nstep:6312/10000 train_loss:3.8497 train_time:1072620ms step_avg:170.20ms\nstep:6313/10000 train_loss:3.9828 train_time:1072793ms step_avg:170.20ms\nstep:6314/10000 train_loss:3.8914 train_time:1072958ms step_avg:170.20ms\nstep:6315/10000 train_loss:3.8128 train_time:1073127ms step_avg:170.20ms\nstep:6316/10000 train_loss:3.8236 train_time:1073304ms step_avg:170.20ms\nstep:6317/10000 train_loss:3.8452 train_time:1073468ms step_avg:170.20ms\nstep:6318/10000 train_loss:3.8085 train_time:1073641ms step_avg:170.20ms\nstep:6319/10000 train_loss:3.8935 train_time:1073808ms step_avg:170.20ms\nstep:6320/10000 train_loss:3.8168 train_time:1073980ms step_avg:170.20ms\nstep:6321/10000 train_loss:4.1874 train_time:1074148ms step_avg:170.20ms\nstep:6322/10000 train_loss:3.8689 train_time:1074319ms step_avg:170.20ms\nstep:6323/10000 train_loss:3.7816 train_time:1074490ms step_avg:170.20ms\nstep:6324/10000 train_loss:3.9970 train_time:1074661ms step_avg:170.20ms\nstep:6325/10000 train_loss:3.8964 train_time:1074831ms step_avg:170.20ms\nstep:6326/10000 train_loss:3.8114 train_time:1075003ms step_avg:170.20ms\nstep:6327/10000 train_loss:3.7027 train_time:1075174ms step_avg:170.20ms\nstep:6328/10000 train_loss:3.8756 train_time:1075346ms step_avg:170.20ms\nstep:6329/10000 train_loss:4.0582 train_time:1075516ms step_avg:170.20ms\nstep:6330/10000 train_loss:3.7934 train_time:1075689ms step_avg:170.20ms\nstep:6331/10000 train_loss:3.8260 train_time:1075859ms step_avg:170.20ms\nstep:6332/10000 train_loss:3.9249 train_time:1076029ms step_avg:170.20ms\nstep:6333/10000 train_loss:3.8627 train_time:1076202ms step_avg:170.20ms\nstep:6334/10000 train_loss:3.7789 train_time:1076370ms step_avg:170.20ms\nstep:6335/10000 train_loss:3.7375 train_time:1076542ms step_avg:170.20ms\nstep:6336/10000 train_loss:3.7052 train_time:1076710ms step_avg:170.20ms\nstep:6337/10000 train_loss:3.8655 train_time:1076881ms step_avg:170.20ms\nstep:6338/10000 train_loss:3.9942 train_time:1077052ms step_avg:170.20ms\nstep:6339/10000 train_loss:3.8933 train_time:1077223ms step_avg:170.20ms\nstep:6340/10000 train_loss:3.8157 train_time:1077392ms step_avg:170.20ms\nstep:6341/10000 train_loss:4.0115 train_time:1077563ms step_avg:170.20ms\nstep:6342/10000 train_loss:3.7965 train_time:1077732ms step_avg:170.20ms\nstep:6343/10000 train_loss:4.0107 train_time:1077905ms step_avg:170.20ms\nstep:6344/10000 train_loss:3.7909 train_time:1078079ms step_avg:170.21ms\nstep:6345/10000 train_loss:3.8171 train_time:1078246ms step_avg:170.20ms\nstep:6346/10000 train_loss:3.9421 train_time:1078416ms step_avg:170.20ms\nstep:6347/10000 train_loss:3.8836 train_time:1078585ms step_avg:170.20ms\nstep:6348/10000 train_loss:3.9638 train_time:1078755ms step_avg:170.20ms\nstep:6349/10000 train_loss:3.8191 train_time:1078924ms step_avg:170.20ms\nstep:6350/10000 train_loss:3.7871 train_time:1079094ms step_avg:170.20ms\nstep:6351/10000 train_loss:3.8742 train_time:1079266ms step_avg:170.20ms\nstep:6352/10000 train_loss:3.8078 train_time:1079433ms step_avg:170.20ms\nstep:6353/10000 train_loss:3.9258 train_time:1079605ms step_avg:170.20ms\nstep:6354/10000 train_loss:3.9286 train_time:1079775ms step_avg:170.20ms\nstep:6355/10000 train_loss:4.1350 train_time:1079947ms step_avg:170.20ms\nstep:6356/10000 train_loss:3.7652 train_time:1080117ms step_avg:170.20ms\nstep:6357/10000 train_loss:3.7981 train_time:1080286ms step_avg:170.20ms\nstep:6358/10000 train_loss:3.7085 train_time:1080459ms step_avg:170.20ms\nstep:6359/10000 train_loss:3.7604 train_time:1080628ms step_avg:170.20ms\nstep:6360/10000 train_loss:3.8948 train_time:1080798ms step_avg:170.20ms\nstep:6361/10000 train_loss:3.7970 train_time:1080967ms step_avg:170.20ms\nstep:6362/10000 train_loss:3.9108 train_time:1081138ms step_avg:170.20ms\nstep:6363/10000 train_loss:3.8882 train_time:1081309ms step_avg:170.20ms\nstep:6364/10000 train_loss:3.8299 train_time:1081480ms step_avg:170.20ms\nstep:6365/10000 train_loss:3.7685 train_time:1081649ms step_avg:170.20ms\nstep:6366/10000 train_loss:3.8180 train_time:1081821ms step_avg:170.20ms\nstep:6367/10000 train_loss:4.0331 train_time:1081988ms step_avg:170.20ms\nstep:6368/10000 train_loss:3.7869 train_time:1082159ms step_avg:170.20ms\nstep:6369/10000 train_loss:3.8975 train_time:1082329ms step_avg:170.20ms\nstep:6370/10000 train_loss:3.9848 train_time:1082502ms step_avg:170.20ms\nstep:6371/10000 train_loss:3.8821 train_time:1082670ms step_avg:170.20ms\nstep:6372/10000 train_loss:3.7683 train_time:1082839ms step_avg:170.20ms\nstep:6373/10000 train_loss:3.7724 train_time:1083012ms step_avg:170.20ms\nstep:6374/10000 train_loss:3.8047 train_time:1083183ms step_avg:170.20ms\nstep:6375/10000 train_loss:3.8634 train_time:1083355ms step_avg:170.20ms\nstep:6376/10000 train_loss:4.1638 train_time:1083526ms step_avg:170.21ms\nstep:6377/10000 train_loss:3.5304 train_time:1083695ms step_avg:170.20ms\nstep:6378/10000 train_loss:3.8093 train_time:1083867ms step_avg:170.21ms\nstep:6379/10000 train_loss:3.9236 train_time:1084038ms step_avg:170.21ms\nstep:6380/10000 train_loss:3.6953 train_time:1084207ms step_avg:170.21ms\nstep:6381/10000 train_loss:3.7735 train_time:1084376ms step_avg:170.20ms\nstep:6382/10000 train_loss:3.8962 train_time:1084544ms step_avg:170.20ms\nstep:6383/10000 train_loss:3.8459 train_time:1084713ms step_avg:170.20ms\nstep:6384/10000 train_loss:3.7471 train_time:1084886ms step_avg:170.20ms\nstep:6385/10000 train_loss:3.7906 train_time:1085057ms step_avg:170.20ms\nstep:6386/10000 train_loss:3.8786 train_time:1085225ms step_avg:170.20ms\nstep:6387/10000 train_loss:3.6365 train_time:1085393ms step_avg:170.20ms\nstep:6388/10000 train_loss:3.8685 train_time:1085563ms step_avg:170.20ms\nstep:6389/10000 train_loss:3.9473 train_time:1085733ms step_avg:170.20ms\nstep:6390/10000 train_loss:3.8452 train_time:1085907ms step_avg:170.20ms\nstep:6391/10000 train_loss:3.8469 train_time:1086081ms step_avg:170.21ms\nstep:6392/10000 train_loss:3.8096 train_time:1086251ms step_avg:170.21ms\nstep:6393/10000 train_loss:3.7941 train_time:1086423ms step_avg:170.21ms\nstep:6394/10000 train_loss:3.9888 train_time:1086590ms step_avg:170.21ms\nstep:6395/10000 train_loss:3.9749 train_time:1086762ms step_avg:170.21ms\nstep:6396/10000 train_loss:3.6683 train_time:1086931ms step_avg:170.21ms\nstep:6397/10000 train_loss:3.8738 train_time:1087102ms step_avg:170.21ms\nstep:6398/10000 train_loss:3.7492 train_time:1087270ms step_avg:170.21ms\nstep:6399/10000 train_loss:3.8495 train_time:1087442ms step_avg:170.21ms\nstep:6400/10000 train_loss:3.8230 train_time:1087608ms step_avg:170.20ms\nstep:6401/10000 train_loss:3.8672 train_time:1087779ms step_avg:170.20ms\nstep:6402/10000 train_loss:3.8738 train_time:1087948ms step_avg:170.20ms\nstep:6403/10000 train_loss:3.8642 train_time:1088120ms step_avg:170.20ms\nstep:6404/10000 train_loss:3.8048 train_time:1088289ms step_avg:170.20ms\nstep:6405/10000 train_loss:3.7290 train_time:1088463ms step_avg:170.21ms\nstep:6406/10000 train_loss:3.9079 train_time:1088631ms step_avg:170.21ms\nstep:6407/10000 train_loss:4.2756 train_time:1088802ms step_avg:170.21ms\nstep:6408/10000 train_loss:3.8627 train_time:1088971ms step_avg:170.20ms\nstep:6409/10000 train_loss:3.8200 train_time:1089144ms step_avg:170.21ms\nstep:6410/10000 train_loss:3.9613 train_time:1089311ms step_avg:170.20ms\nstep:6411/10000 train_loss:3.7671 train_time:1089482ms step_avg:170.21ms\nstep:6412/10000 train_loss:3.8133 train_time:1089652ms step_avg:170.20ms\nstep:6413/10000 train_loss:3.8006 train_time:1089819ms step_avg:170.20ms\nstep:6414/10000 train_loss:3.8003 train_time:1089990ms step_avg:170.20ms\nstep:6415/10000 train_loss:3.7919 train_time:1090161ms step_avg:170.20ms\nstep:6416/10000 train_loss:3.7556 train_time:1090329ms step_avg:170.20ms\nstep:6417/10000 train_loss:3.8380 train_time:1090498ms step_avg:170.20ms\nstep:6418/10000 train_loss:4.0936 train_time:1090669ms step_avg:170.20ms\nstep:6419/10000 train_loss:3.9637 train_time:1090839ms step_avg:170.20ms\nstep:6420/10000 train_loss:3.8161 train_time:1091010ms step_avg:170.20ms\nstep:6421/10000 train_loss:3.8957 train_time:1091181ms step_avg:170.20ms\nstep:6422/10000 train_loss:4.0109 train_time:1091350ms step_avg:170.20ms\nstep:6423/10000 train_loss:3.8487 train_time:1091518ms step_avg:170.20ms\nstep:6424/10000 train_loss:3.7210 train_time:1091687ms step_avg:170.20ms\nstep:6425/10000 train_loss:3.6906 train_time:1091856ms step_avg:170.20ms\nstep:6426/10000 train_loss:3.8212 train_time:1092026ms step_avg:170.20ms\nstep:6427/10000 train_loss:3.7953 train_time:1092198ms step_avg:170.20ms\nstep:6428/10000 train_loss:3.8363 train_time:1092368ms step_avg:170.20ms\nstep:6429/10000 train_loss:3.8599 train_time:1092537ms step_avg:170.20ms\nstep:6430/10000 train_loss:3.8401 train_time:1092709ms step_avg:170.20ms\nstep:6431/10000 train_loss:3.7715 train_time:1092879ms step_avg:170.20ms\nstep:6432/10000 train_loss:3.7286 train_time:1093051ms step_avg:170.20ms\nstep:6433/10000 train_loss:3.9481 train_time:1093221ms step_avg:170.20ms\nstep:6434/10000 train_loss:3.7011 train_time:1093390ms step_avg:170.20ms\nstep:6435/10000 train_loss:3.8895 train_time:1093562ms step_avg:170.20ms\nstep:6436/10000 train_loss:3.7882 train_time:1093730ms step_avg:170.20ms\nstep:6437/10000 train_loss:3.8085 train_time:1093899ms step_avg:170.20ms\nstep:6438/10000 train_loss:4.0431 train_time:1094069ms step_avg:170.20ms\nstep:6439/10000 train_loss:3.9622 train_time:1094240ms step_avg:170.20ms\nstep:6440/10000 train_loss:3.8232 train_time:1094407ms step_avg:170.20ms\nstep:6441/10000 train_loss:3.9640 train_time:1094578ms step_avg:170.20ms\nstep:6442/10000 train_loss:3.8018 train_time:1094751ms step_avg:170.20ms\nstep:6443/10000 train_loss:3.8517 train_time:1094922ms step_avg:170.20ms\nstep:6444/10000 train_loss:3.6750 train_time:1095091ms step_avg:170.20ms\nstep:6445/10000 train_loss:3.7124 train_time:1095259ms step_avg:170.20ms\nstep:6446/10000 train_loss:3.8370 train_time:1095432ms step_avg:170.20ms\nstep:6447/10000 train_loss:3.6251 train_time:1095603ms step_avg:170.20ms\nstep:6448/10000 train_loss:3.8395 train_time:1095771ms step_avg:170.20ms\nstep:6449/10000 train_loss:3.7937 train_time:1095938ms step_avg:170.20ms\nstep:6450/10000 train_loss:3.7817 train_time:1096110ms step_avg:170.20ms\nstep:6451/10000 train_loss:3.8198 train_time:1096280ms step_avg:170.20ms\nstep:6452/10000 train_loss:3.8191 train_time:1096450ms step_avg:170.20ms\nstep:6453/10000 train_loss:3.8762 train_time:1096625ms step_avg:170.20ms\nstep:6454/10000 train_loss:4.3801 train_time:1096796ms step_avg:170.20ms\nstep:6455/10000 train_loss:3.8907 train_time:1096966ms step_avg:170.20ms\nstep:6456/10000 train_loss:3.8185 train_time:1097134ms step_avg:170.20ms\nstep:6457/10000 train_loss:3.8089 train_time:1097302ms step_avg:170.20ms\nstep:6458/10000 train_loss:4.0259 train_time:1097471ms step_avg:170.20ms\nstep:6459/10000 train_loss:3.8734 train_time:1097641ms step_avg:170.20ms\nstep:6460/10000 train_loss:3.9051 train_time:1097811ms step_avg:170.20ms\nstep:6461/10000 train_loss:3.8440 train_time:1097982ms step_avg:170.20ms\nstep:6462/10000 train_loss:3.9696 train_time:1098150ms step_avg:170.20ms\nstep:6463/10000 train_loss:3.9321 train_time:1098321ms step_avg:170.20ms\nstep:6464/10000 train_loss:3.8246 train_time:1098490ms step_avg:170.20ms\nstep:6465/10000 train_loss:3.7819 train_time:1098660ms step_avg:170.20ms\nstep:6466/10000 train_loss:3.8374 train_time:1098829ms step_avg:170.20ms\nstep:6467/10000 train_loss:3.9849 train_time:1099004ms step_avg:170.20ms\nstep:6468/10000 train_loss:3.8773 train_time:1099173ms step_avg:170.20ms\nstep:6469/10000 train_loss:3.7592 train_time:1099340ms step_avg:170.20ms\nstep:6470/10000 train_loss:3.8763 train_time:1099510ms step_avg:170.20ms\nstep:6471/10000 train_loss:3.8594 train_time:1099680ms step_avg:170.20ms\nstep:6472/10000 train_loss:3.8347 train_time:1099850ms step_avg:170.20ms\nstep:6473/10000 train_loss:3.8558 train_time:1100021ms step_avg:170.20ms\nstep:6474/10000 train_loss:3.8447 train_time:1100191ms step_avg:170.20ms\nstep:6475/10000 train_loss:3.7222 train_time:1100361ms step_avg:170.20ms\nstep:6476/10000 train_loss:3.9219 train_time:1100529ms step_avg:170.20ms\nstep:6477/10000 train_loss:3.7570 train_time:1100699ms step_avg:170.20ms\nstep:6478/10000 train_loss:3.7903 train_time:1100867ms step_avg:170.20ms\nstep:6479/10000 train_loss:3.8075 train_time:1101038ms step_avg:170.20ms\nstep:6480/10000 train_loss:3.9465 train_time:1101207ms step_avg:170.20ms\nstep:6481/10000 train_loss:3.8462 train_time:1101378ms step_avg:170.20ms\nstep:6482/10000 train_loss:3.8680 train_time:1101547ms step_avg:170.20ms\nstep:6483/10000 train_loss:3.8358 train_time:1101717ms step_avg:170.20ms\nstep:6484/10000 train_loss:3.8103 train_time:1101886ms step_avg:170.20ms\nstep:6485/10000 train_loss:3.7236 train_time:1102056ms step_avg:170.20ms\nstep:6486/10000 train_loss:3.8675 train_time:1102228ms step_avg:170.20ms\nstep:6487/10000 train_loss:3.7443 train_time:1102400ms step_avg:170.20ms\nstep:6488/10000 train_loss:4.1136 train_time:1102569ms step_avg:170.20ms\nstep:6489/10000 train_loss:3.9578 train_time:1102742ms step_avg:170.20ms\nstep:6490/10000 train_loss:3.7386 train_time:1102913ms step_avg:170.20ms\nstep:6491/10000 train_loss:3.8300 train_time:1103085ms step_avg:170.20ms\nstep:6492/10000 train_loss:3.7307 train_time:1103255ms step_avg:170.20ms\nstep:6493/10000 train_loss:3.7034 train_time:1103427ms step_avg:170.20ms\nstep:6494/10000 train_loss:4.0023 train_time:1103598ms step_avg:170.20ms\nstep:6495/10000 train_loss:3.9167 train_time:1103768ms step_avg:170.20ms\nstep:6496/10000 train_loss:4.0783 train_time:1103940ms step_avg:170.20ms\nstep:6497/10000 train_loss:3.8240 train_time:1104110ms step_avg:170.20ms\nstep:6498/10000 train_loss:3.8611 train_time:1104282ms step_avg:170.20ms\nstep:6499/10000 train_loss:3.8643 train_time:1104452ms step_avg:170.20ms\nstep:6500/10000 train_loss:3.7885 train_time:1104622ms step_avg:170.20ms\nstep:6501/10000 train_loss:3.7385 train_time:1104792ms step_avg:170.20ms\nstep:6502/10000 train_loss:3.7127 train_time:1104962ms step_avg:170.20ms\nstep:6503/10000 train_loss:3.9397 train_time:1105130ms step_avg:170.20ms\nstep:6504/10000 train_loss:3.8822 train_time:1105301ms step_avg:170.20ms\nstep:6505/10000 train_loss:3.7174 train_time:1105468ms step_avg:170.20ms\nstep:6506/10000 train_loss:3.7998 train_time:1105640ms step_avg:170.20ms\nstep:6507/10000 train_loss:3.8114 train_time:1105810ms step_avg:170.20ms\nstep:6508/10000 train_loss:3.8801 train_time:1105981ms step_avg:170.20ms\nstep:6509/10000 train_loss:3.7994 train_time:1106150ms step_avg:170.20ms\nstep:6510/10000 train_loss:3.8052 train_time:1106321ms step_avg:170.20ms\nstep:6511/10000 train_loss:3.7938 train_time:1106488ms step_avg:170.20ms\nstep:6512/10000 train_loss:3.9462 train_time:1106658ms step_avg:170.20ms\nstep:6513/10000 train_loss:3.5528 train_time:1106828ms step_avg:170.20ms\nstep:6514/10000 train_loss:3.8359 train_time:1106999ms step_avg:170.20ms\nstep:6515/10000 train_loss:3.8018 train_time:1107169ms step_avg:170.20ms\nstep:6516/10000 train_loss:3.7912 train_time:1107340ms step_avg:170.20ms\nstep:6517/10000 train_loss:3.9106 train_time:1107509ms step_avg:170.20ms\nstep:6518/10000 train_loss:3.8307 train_time:1107681ms step_avg:170.20ms\nstep:6519/10000 train_loss:3.8741 train_time:1107856ms step_avg:170.20ms\nstep:6520/10000 train_loss:3.8210 train_time:1108026ms step_avg:170.20ms\nstep:6521/10000 train_loss:3.8883 train_time:1108192ms step_avg:170.20ms\nstep:6522/10000 train_loss:3.7759 train_time:1108361ms step_avg:170.20ms\nstep:6523/10000 train_loss:3.9163 train_time:1108532ms step_avg:170.20ms\nstep:6524/10000 train_loss:3.8366 train_time:1108699ms step_avg:170.20ms\nstep:6525/10000 train_loss:3.7185 train_time:1108869ms step_avg:170.20ms\nstep:6526/10000 train_loss:3.8777 train_time:1109041ms step_avg:170.20ms\nstep:6527/10000 train_loss:3.6670 train_time:1109212ms step_avg:170.20ms\nstep:6528/10000 train_loss:3.7134 train_time:1109384ms step_avg:170.20ms\nstep:6529/10000 train_loss:3.7787 train_time:1109554ms step_avg:170.20ms\nstep:6530/10000 train_loss:3.7846 train_time:1109728ms step_avg:170.20ms\nstep:6531/10000 train_loss:3.8493 train_time:1109900ms step_avg:170.20ms\nstep:6532/10000 train_loss:3.7832 train_time:1110071ms step_avg:170.20ms\nstep:6533/10000 train_loss:3.9454 train_time:1110242ms step_avg:170.20ms\nstep:6534/10000 train_loss:3.8231 train_time:1110410ms step_avg:170.20ms\nstep:6535/10000 train_loss:3.8331 train_time:1110580ms step_avg:170.20ms\nstep:6536/10000 train_loss:3.8032 train_time:1110750ms step_avg:170.20ms\nstep:6537/10000 train_loss:3.9882 train_time:1110921ms step_avg:170.20ms\nstep:6538/10000 train_loss:3.6874 train_time:1111090ms step_avg:170.20ms\nstep:6539/10000 train_loss:3.8232 train_time:1111261ms step_avg:170.20ms\nstep:6540/10000 train_loss:3.8267 train_time:1111429ms step_avg:170.20ms\nstep:6541/10000 train_loss:3.8324 train_time:1111599ms step_avg:170.20ms\nstep:6542/10000 train_loss:3.6869 train_time:1111770ms step_avg:170.20ms\nstep:6543/10000 train_loss:3.9135 train_time:1111941ms step_avg:170.20ms\nstep:6544/10000 train_loss:3.8699 train_time:1112111ms step_avg:170.20ms\nstep:6545/10000 train_loss:3.7613 train_time:1112285ms step_avg:170.20ms\nstep:6546/10000 train_loss:3.8519 train_time:1112456ms step_avg:170.20ms\nstep:6547/10000 train_loss:4.0054 train_time:1112627ms step_avg:170.20ms\nstep:6548/10000 train_loss:4.1373 train_time:1112795ms step_avg:170.20ms\nstep:6549/10000 train_loss:3.8712 train_time:1112966ms step_avg:170.20ms\nstep:6550/10000 train_loss:3.8354 train_time:1113137ms step_avg:170.20ms\nstep:6551/10000 train_loss:4.0077 train_time:1113307ms step_avg:170.20ms\nstep:6552/10000 train_loss:3.7684 train_time:1113480ms step_avg:170.20ms\nstep:6553/10000 train_loss:3.6740 train_time:1113649ms step_avg:170.20ms\nstep:6554/10000 train_loss:3.7751 train_time:1113820ms step_avg:170.20ms\nstep:6555/10000 train_loss:3.8878 train_time:1113989ms step_avg:170.20ms\nstep:6556/10000 train_loss:3.7612 train_time:1114160ms step_avg:170.20ms\nstep:6557/10000 train_loss:3.9757 train_time:1114328ms step_avg:170.20ms\nstep:6558/10000 train_loss:3.8682 train_time:1114501ms step_avg:170.20ms\nstep:6559/10000 train_loss:3.8689 train_time:1114671ms step_avg:170.20ms\nstep:6560/10000 train_loss:3.7842 train_time:1114841ms step_avg:170.20ms\nstep:6561/10000 train_loss:3.7995 train_time:1115012ms step_avg:170.20ms\nstep:6562/10000 train_loss:3.9301 train_time:1115186ms step_avg:170.21ms\nstep:6563/10000 train_loss:3.6560 train_time:1115355ms step_avg:170.21ms\nstep:6564/10000 train_loss:3.9181 train_time:1115525ms step_avg:170.21ms\nstep:6565/10000 train_loss:4.0297 train_time:1115696ms step_avg:170.21ms\nstep:6566/10000 train_loss:3.9240 train_time:1115867ms step_avg:170.21ms\nstep:6567/10000 train_loss:3.8667 train_time:1116039ms step_avg:170.21ms\nstep:6568/10000 train_loss:3.8205 train_time:1116211ms step_avg:170.21ms\nstep:6569/10000 train_loss:3.6703 train_time:1116380ms step_avg:170.21ms\nstep:6570/10000 train_loss:4.1009 train_time:1116549ms step_avg:170.21ms\nstep:6571/10000 train_loss:4.0084 train_time:1116721ms step_avg:170.21ms\nstep:6572/10000 train_loss:3.8757 train_time:1116890ms step_avg:170.21ms\nstep:6573/10000 train_loss:3.7071 train_time:1117061ms step_avg:170.21ms\nstep:6574/10000 train_loss:3.9512 train_time:1117231ms step_avg:170.21ms\nstep:6575/10000 train_loss:3.8090 train_time:1117402ms step_avg:170.21ms\nstep:6576/10000 train_loss:3.7303 train_time:1117569ms step_avg:170.21ms\nstep:6577/10000 train_loss:3.7298 train_time:1117739ms step_avg:170.21ms\nstep:6578/10000 train_loss:3.8337 train_time:1117909ms step_avg:170.21ms\nstep:6579/10000 train_loss:3.8572 train_time:1118081ms step_avg:170.21ms\nstep:6580/10000 train_loss:3.7785 train_time:1118249ms step_avg:170.21ms\nstep:6581/10000 train_loss:3.8164 train_time:1118421ms step_avg:170.21ms\nstep:6582/10000 train_loss:3.9046 train_time:1118591ms step_avg:170.21ms\nstep:6583/10000 train_loss:3.9169 train_time:1118763ms step_avg:170.21ms\nstep:6584/10000 train_loss:3.8509 train_time:1118931ms step_avg:170.21ms\nstep:6585/10000 train_loss:3.7644 train_time:1119102ms step_avg:170.21ms\nstep:6586/10000 train_loss:3.8081 train_time:1119272ms step_avg:170.21ms\nstep:6587/10000 train_loss:3.7882 train_time:1119444ms step_avg:170.21ms\nstep:6588/10000 train_loss:3.7411 train_time:1119615ms step_avg:170.21ms\nstep:6589/10000 train_loss:3.9329 train_time:1119786ms step_avg:170.21ms\nstep:6590/10000 train_loss:3.8278 train_time:1119958ms step_avg:170.21ms\nstep:6591/10000 train_loss:3.9869 train_time:1120129ms step_avg:170.21ms\nstep:6592/10000 train_loss:3.8354 train_time:1120300ms step_avg:170.21ms\nstep:6593/10000 train_loss:3.7819 train_time:1120471ms step_avg:170.21ms\nstep:6594/10000 train_loss:3.8425 train_time:1120641ms step_avg:170.21ms\nstep:6595/10000 train_loss:3.7950 train_time:1120809ms step_avg:170.21ms\nstep:6596/10000 train_loss:3.9071 train_time:1120981ms step_avg:170.21ms\nstep:6597/10000 train_loss:3.7949 train_time:1121152ms step_avg:170.21ms\nstep:6598/10000 train_loss:3.6424 train_time:1121322ms step_avg:170.21ms\nstep:6599/10000 train_loss:3.8438 train_time:1121491ms step_avg:170.21ms\nstep:6600/10000 train_loss:3.7952 train_time:1121663ms step_avg:170.21ms\nstep:6601/10000 train_loss:3.7303 train_time:1121833ms step_avg:170.21ms\nstep:6602/10000 train_loss:3.8959 train_time:1122007ms step_avg:170.21ms\nstep:6603/10000 train_loss:3.8149 train_time:1122177ms step_avg:170.21ms\nstep:6604/10000 train_loss:3.8673 train_time:1122345ms step_avg:170.21ms\nstep:6605/10000 train_loss:3.8180 train_time:1122514ms step_avg:170.21ms\nstep:6606/10000 train_loss:3.7346 train_time:1122684ms step_avg:170.21ms\nstep:6607/10000 train_loss:3.8129 train_time:1122854ms step_avg:170.21ms\nstep:6608/10000 train_loss:3.8549 train_time:1123025ms step_avg:170.21ms\nstep:6609/10000 train_loss:4.0278 train_time:1123195ms step_avg:170.21ms\nstep:6610/10000 train_loss:3.8138 train_time:1123367ms step_avg:170.21ms\nstep:6611/10000 train_loss:3.8765 train_time:1123536ms step_avg:170.21ms\nstep:6612/10000 train_loss:3.8084 train_time:1123708ms step_avg:170.21ms\nstep:6613/10000 train_loss:3.9065 train_time:1123878ms step_avg:170.21ms\nstep:6614/10000 train_loss:3.7924 train_time:1124050ms step_avg:170.21ms\nstep:6615/10000 train_loss:3.8142 train_time:1124221ms step_avg:170.21ms\nstep:6616/10000 train_loss:3.9120 train_time:1124389ms step_avg:170.21ms\nstep:6617/10000 train_loss:3.6843 train_time:1124561ms step_avg:170.21ms\nstep:6618/10000 train_loss:3.8514 train_time:1124730ms step_avg:170.21ms\nstep:6619/10000 train_loss:3.9258 train_time:1124900ms step_avg:170.21ms\nstep:6620/10000 train_loss:3.7278 train_time:1125070ms step_avg:170.21ms\nstep:6621/10000 train_loss:3.7906 train_time:1125242ms step_avg:170.21ms\nstep:6622/10000 train_loss:3.8225 train_time:1125410ms step_avg:170.21ms\nstep:6623/10000 train_loss:3.8372 train_time:1125579ms step_avg:170.21ms\nstep:6624/10000 train_loss:3.5831 train_time:1125750ms step_avg:170.21ms\nstep:6625/10000 train_loss:3.8335 train_time:1125921ms step_avg:170.21ms\nstep:6626/10000 train_loss:3.9471 train_time:1126091ms step_avg:170.21ms\nstep:6627/10000 train_loss:3.7618 train_time:1126263ms step_avg:170.21ms\nstep:6628/10000 train_loss:3.8564 train_time:1126432ms step_avg:170.21ms\nstep:6629/10000 train_loss:3.7796 train_time:1126604ms step_avg:170.21ms\nstep:6630/10000 train_loss:3.9015 train_time:1126774ms step_avg:170.21ms\nstep:6631/10000 train_loss:3.7955 train_time:1126948ms step_avg:170.21ms\nstep:6632/10000 train_loss:3.8846 train_time:1127119ms step_avg:170.21ms\nstep:6633/10000 train_loss:3.8543 train_time:1127289ms step_avg:170.21ms\nstep:6634/10000 train_loss:3.7294 train_time:1127460ms step_avg:170.21ms\nstep:6635/10000 train_loss:3.9991 train_time:1127629ms step_avg:170.21ms\nstep:6636/10000 train_loss:3.8075 train_time:1127800ms step_avg:170.21ms\nstep:6637/10000 train_loss:3.8010 train_time:1127968ms step_avg:170.21ms\nstep:6638/10000 train_loss:3.8915 train_time:1128139ms step_avg:170.21ms\nstep:6639/10000 train_loss:3.8655 train_time:1128309ms step_avg:170.21ms\nstep:6640/10000 train_loss:4.0385 train_time:1128478ms step_avg:170.21ms\nstep:6641/10000 train_loss:3.9231 train_time:1128646ms step_avg:170.21ms\nstep:6642/10000 train_loss:3.7866 train_time:1128818ms step_avg:170.21ms\nstep:6643/10000 train_loss:3.9170 train_time:1128989ms step_avg:170.21ms\nstep:6644/10000 train_loss:3.8655 train_time:1129159ms step_avg:170.21ms\nstep:6645/10000 train_loss:3.8872 train_time:1129328ms step_avg:170.21ms\nstep:6646/10000 train_loss:3.7712 train_time:1129500ms step_avg:170.21ms\nstep:6647/10000 train_loss:3.7903 train_time:1129669ms step_avg:170.21ms\nstep:6648/10000 train_loss:3.8774 train_time:1129841ms step_avg:170.21ms\nstep:6649/10000 train_loss:3.9499 train_time:1130010ms step_avg:170.21ms\nstep:6650/10000 train_loss:3.8255 train_time:1130179ms step_avg:170.21ms\nstep:6651/10000 train_loss:3.9156 train_time:1130349ms step_avg:170.21ms\nstep:6652/10000 train_loss:3.8800 train_time:1130523ms step_avg:170.21ms\nstep:6653/10000 train_loss:3.8637 train_time:1130693ms step_avg:170.21ms\nstep:6654/10000 train_loss:3.8587 train_time:1130867ms step_avg:170.21ms\nstep:6655/10000 train_loss:4.0230 train_time:1131035ms step_avg:170.21ms\nstep:6656/10000 train_loss:3.8983 train_time:1131203ms step_avg:170.21ms\nstep:6657/10000 train_loss:3.7939 train_time:1131373ms step_avg:170.21ms\nstep:6658/10000 train_loss:3.7916 train_time:1131542ms step_avg:170.21ms\nstep:6659/10000 train_loss:3.8804 train_time:1131712ms step_avg:170.21ms\nstep:6660/10000 train_loss:3.7589 train_time:1131883ms step_avg:170.21ms\nstep:6661/10000 train_loss:3.9531 train_time:1132052ms step_avg:170.21ms\nstep:6662/10000 train_loss:3.8639 train_time:1132225ms step_avg:170.21ms\nstep:6663/10000 train_loss:3.9622 train_time:1132395ms step_avg:170.21ms\nstep:6664/10000 train_loss:3.8272 train_time:1132565ms step_avg:170.21ms\nstep:6665/10000 train_loss:3.7032 train_time:1132734ms step_avg:170.21ms\nstep:6666/10000 train_loss:3.8864 train_time:1132906ms step_avg:170.21ms\nstep:6667/10000 train_loss:3.9280 train_time:1133080ms step_avg:170.21ms\nstep:6668/10000 train_loss:3.7264 train_time:1133248ms step_avg:170.21ms\nstep:6669/10000 train_loss:3.8276 train_time:1133418ms step_avg:170.21ms\nstep:6670/10000 train_loss:3.9013 train_time:1133586ms step_avg:170.21ms\nstep:6671/10000 train_loss:3.8648 train_time:1133757ms step_avg:170.21ms\nstep:6672/10000 train_loss:3.8001 train_time:1133929ms step_avg:170.21ms\nstep:6673/10000 train_loss:3.7934 train_time:1134102ms step_avg:170.21ms\nstep:6674/10000 train_loss:3.9044 train_time:1134272ms step_avg:170.21ms\nstep:6675/10000 train_loss:3.8300 train_time:1134444ms step_avg:170.21ms\nstep:6676/10000 train_loss:3.8558 train_time:1134614ms step_avg:170.21ms\nstep:6677/10000 train_loss:3.8172 train_time:1134784ms step_avg:170.21ms\nstep:6678/10000 train_loss:4.0008 train_time:1134952ms step_avg:170.21ms\nstep:6679/10000 train_loss:3.8859 train_time:1135123ms step_avg:170.21ms\nstep:6680/10000 train_loss:3.6595 train_time:1135291ms step_avg:170.21ms\nstep:6681/10000 train_loss:3.7234 train_time:1135457ms step_avg:170.21ms\nstep:6682/10000 train_loss:3.7568 train_time:1135631ms step_avg:170.21ms\nstep:6683/10000 train_loss:3.7397 train_time:1135800ms step_avg:170.21ms\nstep:6684/10000 train_loss:3.8840 train_time:1135971ms step_avg:170.21ms\nstep:6685/10000 train_loss:3.8944 train_time:1136146ms step_avg:170.21ms\nstep:6686/10000 train_loss:3.8055 train_time:1136311ms step_avg:170.21ms\nstep:6687/10000 train_loss:3.9010 train_time:1136480ms step_avg:170.21ms\nstep:6688/10000 train_loss:3.6912 train_time:1136650ms step_avg:170.21ms\nstep:6689/10000 train_loss:3.7913 train_time:1136819ms step_avg:170.21ms\nstep:6690/10000 train_loss:3.7693 train_time:1136991ms step_avg:170.21ms\nstep:6691/10000 train_loss:3.8819 train_time:1137161ms step_avg:170.21ms\nstep:6692/10000 train_loss:3.9775 train_time:1137330ms step_avg:170.21ms\nstep:6693/10000 train_loss:3.8226 train_time:1137502ms step_avg:170.21ms\nstep:6694/10000 train_loss:3.7404 train_time:1137671ms step_avg:170.21ms\nstep:6695/10000 train_loss:3.7263 train_time:1137841ms step_avg:170.21ms\nstep:6696/10000 train_loss:3.8485 train_time:1138013ms step_avg:170.21ms\nstep:6697/10000 train_loss:3.8021 train_time:1138181ms step_avg:170.21ms\nstep:6698/10000 train_loss:3.9192 train_time:1138352ms step_avg:170.21ms\nstep:6699/10000 train_loss:3.9517 train_time:1138523ms step_avg:170.21ms\nstep:6700/10000 train_loss:3.8262 train_time:1138692ms step_avg:170.21ms\nstep:6701/10000 train_loss:3.9979 train_time:1138862ms step_avg:170.21ms\nstep:6702/10000 train_loss:4.0751 train_time:1139034ms step_avg:170.21ms\nstep:6703/10000 train_loss:3.6802 train_time:1139204ms step_avg:170.21ms\nstep:6704/10000 train_loss:3.6999 train_time:1139369ms step_avg:170.21ms\nstep:6705/10000 train_loss:3.8447 train_time:1139538ms step_avg:170.21ms\nstep:6706/10000 train_loss:3.8251 train_time:1139706ms step_avg:170.21ms\nstep:6707/10000 train_loss:3.8965 train_time:1139876ms step_avg:170.21ms\nstep:6708/10000 train_loss:3.7821 train_time:1140048ms step_avg:170.21ms\nstep:6709/10000 train_loss:3.7232 train_time:1140218ms step_avg:170.21ms\nstep:6710/10000 train_loss:3.7834 train_time:1140386ms step_avg:170.21ms\nstep:6711/10000 train_loss:3.8524 train_time:1140556ms step_avg:170.21ms\nstep:6712/10000 train_loss:4.0951 train_time:1140725ms step_avg:170.21ms\nstep:6713/10000 train_loss:3.8920 train_time:1140894ms step_avg:170.21ms\nstep:6714/10000 train_loss:3.7065 train_time:1141075ms step_avg:170.21ms\nstep:6715/10000 train_loss:3.7820 train_time:1141235ms step_avg:170.21ms\nstep:6716/10000 train_loss:3.8547 train_time:1141404ms step_avg:170.21ms\nstep:6717/10000 train_loss:3.7864 train_time:1141576ms step_avg:170.21ms\nstep:6718/10000 train_loss:3.8867 train_time:1141748ms step_avg:170.21ms\nstep:6719/10000 train_loss:3.7587 train_time:1141919ms step_avg:170.21ms\nstep:6720/10000 train_loss:3.8142 train_time:1142089ms step_avg:170.21ms\nstep:6721/10000 train_loss:3.9379 train_time:1142260ms step_avg:170.21ms\nstep:6722/10000 train_loss:3.7995 train_time:1142430ms step_avg:170.21ms\nstep:6723/10000 train_loss:4.0343 train_time:1142601ms step_avg:170.21ms\nstep:6724/10000 train_loss:3.9335 train_time:1142772ms step_avg:170.21ms\nstep:6725/10000 train_loss:3.9926 train_time:1142945ms step_avg:170.21ms\nstep:6726/10000 train_loss:3.8156 train_time:1143116ms step_avg:170.21ms\nstep:6727/10000 train_loss:3.8524 train_time:1143289ms step_avg:170.21ms\nstep:6728/10000 train_loss:3.8882 train_time:1143452ms step_avg:170.21ms\nstep:6729/10000 train_loss:3.9430 train_time:1143624ms step_avg:170.21ms\nstep:6730/10000 train_loss:3.8056 train_time:1143793ms step_avg:170.21ms\nstep:6731/10000 train_loss:3.7340 train_time:1143963ms step_avg:170.21ms\nstep:6732/10000 train_loss:3.7797 train_time:1144137ms step_avg:170.21ms\nstep:6733/10000 train_loss:3.7976 train_time:1144308ms step_avg:170.21ms\nstep:6734/10000 train_loss:3.6482 train_time:1144478ms step_avg:170.21ms\nstep:6735/10000 train_loss:3.7625 train_time:1144645ms step_avg:170.21ms\nstep:6736/10000 train_loss:3.8720 train_time:1144814ms step_avg:170.21ms\nstep:6737/10000 train_loss:3.8584 train_time:1144984ms step_avg:170.21ms\nstep:6738/10000 train_loss:3.8049 train_time:1145151ms step_avg:170.21ms\nstep:6739/10000 train_loss:3.7738 train_time:1145320ms step_avg:170.21ms\nstep:6740/10000 train_loss:3.7131 train_time:1145489ms step_avg:170.21ms\nstep:6741/10000 train_loss:3.8055 train_time:1145660ms step_avg:170.21ms\nstep:6742/10000 train_loss:3.8424 train_time:1145830ms step_avg:170.21ms\nstep:6743/10000 train_loss:3.7825 train_time:1146002ms step_avg:170.21ms\nstep:6744/10000 train_loss:3.8698 train_time:1146167ms step_avg:170.21ms\nstep:6745/10000 train_loss:3.7930 train_time:1146337ms step_avg:170.21ms\nstep:6746/10000 train_loss:3.9755 train_time:1146505ms step_avg:170.21ms\nstep:6747/10000 train_loss:3.6816 train_time:1146677ms step_avg:170.21ms\nstep:6748/10000 train_loss:3.7606 train_time:1146848ms step_avg:170.21ms\nstep:6749/10000 train_loss:3.8002 train_time:1147022ms step_avg:170.21ms\nstep:6750/10000 train_loss:3.8816 train_time:1147192ms step_avg:170.21ms\nstep:6751/10000 train_loss:3.8888 train_time:1147359ms step_avg:170.21ms\nstep:6752/10000 train_loss:3.6446 train_time:1147527ms step_avg:170.21ms\nstep:6753/10000 train_loss:3.8678 train_time:1147699ms step_avg:170.21ms\nstep:6754/10000 train_loss:3.7996 train_time:1147871ms step_avg:170.21ms\nstep:6755/10000 train_loss:3.7814 train_time:1148057ms step_avg:170.21ms\nstep:6756/10000 train_loss:4.0559 train_time:1148213ms step_avg:170.21ms\nstep:6757/10000 train_loss:3.8408 train_time:1148383ms step_avg:170.21ms\nstep:6758/10000 train_loss:3.8768 train_time:1148553ms step_avg:170.21ms\nstep:6759/10000 train_loss:3.7377 train_time:1148726ms step_avg:170.21ms\nstep:6760/10000 train_loss:3.8683 train_time:1148895ms step_avg:170.21ms\nstep:6761/10000 train_loss:3.8837 train_time:1149066ms step_avg:170.21ms\nstep:6762/10000 train_loss:3.7619 train_time:1149233ms step_avg:170.21ms\nstep:6763/10000 train_loss:3.7627 train_time:1149402ms step_avg:170.21ms\nstep:6764/10000 train_loss:3.8548 train_time:1149572ms step_avg:170.21ms\nstep:6765/10000 train_loss:3.7950 train_time:1149746ms step_avg:170.21ms\nstep:6766/10000 train_loss:3.8829 train_time:1149917ms step_avg:170.21ms\nstep:6767/10000 train_loss:3.8355 train_time:1150088ms step_avg:170.21ms\nstep:6768/10000 train_loss:3.7857 train_time:1150260ms step_avg:170.21ms\nstep:6769/10000 train_loss:3.9581 train_time:1150430ms step_avg:170.21ms\nstep:6770/10000 train_loss:3.7590 train_time:1150602ms step_avg:170.21ms\nstep:6771/10000 train_loss:3.8186 train_time:1150771ms step_avg:170.21ms\nstep:6772/10000 train_loss:3.7238 train_time:1150943ms step_avg:170.21ms\nstep:6773/10000 train_loss:3.8114 train_time:1151116ms step_avg:170.21ms\nstep:6774/10000 train_loss:3.7155 train_time:1151280ms step_avg:170.21ms\nstep:6775/10000 train_loss:3.7976 train_time:1151448ms step_avg:170.21ms\nstep:6776/10000 train_loss:3.8390 train_time:1151621ms step_avg:170.21ms\nstep:6777/10000 train_loss:3.6411 train_time:1151791ms step_avg:170.21ms\nstep:6778/10000 train_loss:3.7943 train_time:1151964ms step_avg:170.21ms\nstep:6779/10000 train_loss:3.7874 train_time:1152135ms step_avg:170.21ms\nstep:6780/10000 train_loss:3.7950 train_time:1152303ms step_avg:170.21ms\nstep:6781/10000 train_loss:3.7790 train_time:1152472ms step_avg:170.21ms\nstep:6782/10000 train_loss:3.7999 train_time:1152643ms step_avg:170.21ms\nstep:6783/10000 train_loss:3.8443 train_time:1152815ms step_avg:170.21ms\nstep:6784/10000 train_loss:3.8548 train_time:1152988ms step_avg:170.21ms\nstep:6785/10000 train_loss:3.7984 train_time:1153160ms step_avg:170.21ms\nstep:6786/10000 train_loss:3.7361 train_time:1153327ms step_avg:170.21ms\nstep:6787/10000 train_loss:3.9344 train_time:1153499ms step_avg:170.21ms\nstep:6788/10000 train_loss:3.8604 train_time:1153670ms step_avg:170.21ms\nstep:6789/10000 train_loss:3.7485 train_time:1153841ms step_avg:170.21ms\nstep:6790/10000 train_loss:4.1644 train_time:1154011ms step_avg:170.21ms\nstep:6791/10000 train_loss:3.8474 train_time:1154183ms step_avg:170.21ms\nstep:6792/10000 train_loss:3.9002 train_time:1154354ms step_avg:170.21ms\nstep:6793/10000 train_loss:3.8440 train_time:1154526ms step_avg:170.21ms\nstep:6794/10000 train_loss:3.8916 train_time:1154695ms step_avg:170.21ms\nstep:6795/10000 train_loss:3.8123 train_time:1154866ms step_avg:170.21ms\nstep:6796/10000 train_loss:3.8404 train_time:1155037ms step_avg:170.21ms\nstep:6797/10000 train_loss:3.8427 train_time:1155207ms step_avg:170.21ms\nstep:6798/10000 train_loss:3.8626 train_time:1155378ms step_avg:170.21ms\nstep:6799/10000 train_loss:3.7325 train_time:1155550ms step_avg:170.21ms\nstep:6800/10000 train_loss:3.7861 train_time:1155721ms step_avg:170.21ms\nstep:6801/10000 train_loss:3.8218 train_time:1155888ms step_avg:170.21ms\nstep:6802/10000 train_loss:3.7846 train_time:1156058ms step_avg:170.21ms\nstep:6803/10000 train_loss:3.9599 train_time:1156227ms step_avg:170.21ms\nstep:6804/10000 train_loss:3.7132 train_time:1156399ms step_avg:170.21ms\nstep:6805/10000 train_loss:3.8759 train_time:1156567ms step_avg:170.21ms\nstep:6806/10000 train_loss:3.7458 train_time:1156738ms step_avg:170.21ms\nstep:6807/10000 train_loss:3.7872 train_time:1156905ms step_avg:170.21ms\nstep:6808/10000 train_loss:3.7513 train_time:1157077ms step_avg:170.21ms\nstep:6809/10000 train_loss:3.7755 train_time:1157249ms step_avg:170.21ms\nstep:6810/10000 train_loss:3.8129 train_time:1157421ms step_avg:170.21ms\nstep:6811/10000 train_loss:3.8391 train_time:1157588ms step_avg:170.21ms\nstep:6812/10000 train_loss:3.8012 train_time:1157757ms step_avg:170.21ms\nstep:6813/10000 train_loss:3.8258 train_time:1157925ms step_avg:170.21ms\nstep:6814/10000 train_loss:3.7629 train_time:1158093ms step_avg:170.21ms\nstep:6815/10000 train_loss:3.7928 train_time:1158265ms step_avg:170.21ms\nstep:6816/10000 train_loss:3.8103 train_time:1158435ms step_avg:170.21ms\nstep:6817/10000 train_loss:4.0411 train_time:1158606ms step_avg:170.21ms\nstep:6818/10000 train_loss:3.8320 train_time:1158777ms step_avg:170.21ms\nstep:6819/10000 train_loss:3.8635 train_time:1158947ms step_avg:170.21ms\nstep:6820/10000 train_loss:3.8283 train_time:1159119ms step_avg:170.21ms\nstep:6821/10000 train_loss:3.8926 train_time:1159290ms step_avg:170.21ms\nstep:6822/10000 train_loss:3.7662 train_time:1159460ms step_avg:170.21ms\nstep:6823/10000 train_loss:3.8813 train_time:1159632ms step_avg:170.21ms\nstep:6824/10000 train_loss:3.8700 train_time:1159804ms step_avg:170.21ms\nstep:6825/10000 train_loss:3.8113 train_time:1159973ms step_avg:170.21ms\nstep:6826/10000 train_loss:3.9897 train_time:1160143ms step_avg:170.21ms\nstep:6827/10000 train_loss:3.7625 train_time:1160313ms step_avg:170.21ms\nstep:6828/10000 train_loss:3.7697 train_time:1160484ms step_avg:170.21ms\nstep:6829/10000 train_loss:3.8138 train_time:1160654ms step_avg:170.21ms\nstep:6830/10000 train_loss:3.7375 train_time:1160825ms step_avg:170.21ms\nstep:6831/10000 train_loss:3.7597 train_time:1160997ms step_avg:170.21ms\nstep:6832/10000 train_loss:3.9157 train_time:1161168ms step_avg:170.21ms\nstep:6833/10000 train_loss:3.8792 train_time:1161338ms step_avg:170.21ms\nstep:6834/10000 train_loss:3.9621 train_time:1161507ms step_avg:170.21ms\nstep:6835/10000 train_loss:3.8604 train_time:1161678ms step_avg:170.21ms\nstep:6836/10000 train_loss:3.6834 train_time:1161847ms step_avg:170.21ms\nstep:6837/10000 train_loss:3.8728 train_time:1162018ms step_avg:170.21ms\nstep:6838/10000 train_loss:3.8271 train_time:1162187ms step_avg:170.21ms\nstep:6839/10000 train_loss:3.5936 train_time:1162357ms step_avg:170.21ms\nstep:6840/10000 train_loss:3.8146 train_time:1162528ms step_avg:170.21ms\nstep:6841/10000 train_loss:3.8754 train_time:1162697ms step_avg:170.21ms\nstep:6842/10000 train_loss:3.5492 train_time:1162868ms step_avg:170.21ms\nstep:6843/10000 train_loss:3.9232 train_time:1163040ms step_avg:170.21ms\nstep:6844/10000 train_loss:3.7589 train_time:1163210ms step_avg:170.21ms\nstep:6845/10000 train_loss:3.6411 train_time:1163380ms step_avg:170.21ms\nstep:6846/10000 train_loss:3.9216 train_time:1163548ms step_avg:170.21ms\nstep:6847/10000 train_loss:3.8621 train_time:1163718ms step_avg:170.21ms\nstep:6848/10000 train_loss:3.7472 train_time:1163888ms step_avg:170.21ms\nstep:6849/10000 train_loss:3.7188 train_time:1164061ms step_avg:170.21ms\nstep:6850/10000 train_loss:3.8597 train_time:1164232ms step_avg:170.21ms\nstep:6851/10000 train_loss:3.8980 train_time:1164407ms step_avg:170.21ms\nstep:6852/10000 train_loss:4.2444 train_time:1164575ms step_avg:170.21ms\nstep:6853/10000 train_loss:3.8691 train_time:1164743ms step_avg:170.21ms\nstep:6854/10000 train_loss:3.8489 train_time:1164912ms step_avg:170.21ms\nstep:6855/10000 train_loss:4.1962 train_time:1165086ms step_avg:170.21ms\nstep:6856/10000 train_loss:3.8133 train_time:1165257ms step_avg:170.21ms\nstep:6857/10000 train_loss:3.6659 train_time:1165424ms step_avg:170.21ms\nstep:6858/10000 train_loss:3.8149 train_time:1165810ms step_avg:170.24ms\nstep:6859/10000 train_loss:3.8144 train_time:1165975ms step_avg:170.24ms\nstep:6860/10000 train_loss:3.8091 train_time:1166145ms step_avg:170.24ms\nstep:6861/10000 train_loss:3.6562 train_time:1166313ms step_avg:170.24ms\nstep:6862/10000 train_loss:4.0488 train_time:1166481ms step_avg:170.24ms\nstep:6863/10000 train_loss:3.7310 train_time:1166648ms step_avg:170.24ms\nstep:6864/10000 train_loss:3.8687 train_time:1166829ms step_avg:170.24ms\nstep:6865/10000 train_loss:3.7696 train_time:1166998ms step_avg:170.24ms\nstep:6866/10000 train_loss:3.8254 train_time:1167165ms step_avg:170.24ms\nstep:6867/10000 train_loss:3.9364 train_time:1167334ms step_avg:170.24ms\nstep:6868/10000 train_loss:3.8797 train_time:1167506ms step_avg:170.24ms\nstep:6869/10000 train_loss:3.8045 train_time:1167676ms step_avg:170.24ms\nstep:6870/10000 train_loss:3.8390 train_time:1167849ms step_avg:170.24ms\nstep:6871/10000 train_loss:3.8410 train_time:1168023ms step_avg:170.24ms\nstep:6872/10000 train_loss:3.8378 train_time:1168190ms step_avg:170.24ms\nstep:6873/10000 train_loss:3.7802 train_time:1168359ms step_avg:170.24ms\nstep:6874/10000 train_loss:3.8986 train_time:1168525ms step_avg:170.24ms\nstep:6875/10000 train_loss:4.0408 train_time:1168695ms step_avg:170.24ms\nstep:6876/10000 train_loss:3.7732 train_time:1168870ms step_avg:170.24ms\nstep:6877/10000 train_loss:3.9848 train_time:1169045ms step_avg:170.24ms\nstep:6878/10000 train_loss:3.9704 train_time:1169213ms step_avg:170.24ms\nstep:6879/10000 train_loss:3.7754 train_time:1169384ms step_avg:170.24ms\nstep:6880/10000 train_loss:3.6623 train_time:1169554ms step_avg:170.24ms\nstep:6881/10000 train_loss:3.7924 train_time:1169721ms step_avg:170.24ms\nstep:6882/10000 train_loss:3.9859 train_time:1169892ms step_avg:170.24ms\nstep:6883/10000 train_loss:3.6640 train_time:1170066ms step_avg:170.24ms\nstep:6884/10000 train_loss:3.7723 train_time:1170236ms step_avg:170.24ms\nstep:6885/10000 train_loss:3.8512 train_time:1170407ms step_avg:170.24ms\nstep:6886/10000 train_loss:3.8402 train_time:1170578ms step_avg:170.24ms\nstep:6887/10000 train_loss:3.8822 train_time:1170749ms step_avg:170.24ms\nstep:6888/10000 train_loss:3.8017 train_time:1170920ms step_avg:170.24ms\nstep:6889/10000 train_loss:3.8151 train_time:1171090ms step_avg:170.24ms\nstep:6890/10000 train_loss:3.8108 train_time:1171262ms step_avg:170.24ms\nstep:6891/10000 train_loss:3.8050 train_time:1171432ms step_avg:170.24ms\nstep:6892/10000 train_loss:3.7509 train_time:1171603ms step_avg:170.24ms\nstep:6893/10000 train_loss:3.9453 train_time:1171772ms step_avg:170.24ms\nstep:6894/10000 train_loss:3.8022 train_time:1171943ms step_avg:170.24ms\nstep:6895/10000 train_loss:3.7586 train_time:1172111ms step_avg:170.24ms\nstep:6896/10000 train_loss:3.8843 train_time:1172283ms step_avg:170.24ms\nstep:6897/10000 train_loss:3.8740 train_time:1172453ms step_avg:170.24ms\nstep:6898/10000 train_loss:3.4117 train_time:1172625ms step_avg:170.24ms\nstep:6899/10000 train_loss:3.7557 train_time:1172794ms step_avg:170.24ms\nstep:6900/10000 train_loss:3.7250 train_time:1172966ms step_avg:170.24ms\nstep:6901/10000 train_loss:3.7908 train_time:1173134ms step_avg:170.24ms\nstep:6902/10000 train_loss:3.7213 train_time:1173303ms step_avg:170.24ms\nstep:6903/10000 train_loss:3.9154 train_time:1173472ms step_avg:170.24ms\nstep:6904/10000 train_loss:3.8434 train_time:1173644ms step_avg:170.24ms\nstep:6905/10000 train_loss:3.8906 train_time:1173812ms step_avg:170.24ms\nstep:6906/10000 train_loss:3.8512 train_time:1173983ms step_avg:170.24ms\nstep:6907/10000 train_loss:3.8869 train_time:1174150ms step_avg:170.24ms\nstep:6908/10000 train_loss:3.9318 train_time:1174320ms step_avg:170.24ms\nstep:6909/10000 train_loss:3.9659 train_time:1174491ms step_avg:170.24ms\nstep:6910/10000 train_loss:3.7963 train_time:1174664ms step_avg:170.24ms\nstep:6911/10000 train_loss:3.7410 train_time:1174837ms step_avg:170.24ms\nstep:6912/10000 train_loss:3.7885 train_time:1175010ms step_avg:170.24ms\nstep:6913/10000 train_loss:3.7225 train_time:1175182ms step_avg:170.24ms\nstep:6914/10000 train_loss:3.8637 train_time:1175349ms step_avg:170.24ms\nstep:6915/10000 train_loss:4.0096 train_time:1175520ms step_avg:170.24ms\nstep:6916/10000 train_loss:4.0170 train_time:1175690ms step_avg:170.24ms\nstep:6917/10000 train_loss:3.8365 train_time:1175862ms step_avg:170.24ms\nstep:6918/10000 train_loss:3.8022 train_time:1176033ms step_avg:170.24ms\nstep:6919/10000 train_loss:3.7396 train_time:1176199ms step_avg:170.24ms\nstep:6920/10000 train_loss:3.8002 train_time:1176369ms step_avg:170.24ms\nstep:6921/10000 train_loss:3.8664 train_time:1176540ms step_avg:170.24ms\nstep:6922/10000 train_loss:3.7748 train_time:1176709ms step_avg:170.24ms\nstep:6923/10000 train_loss:3.7863 train_time:1176882ms step_avg:170.24ms\nstep:6924/10000 train_loss:3.8569 train_time:1177053ms step_avg:170.24ms\nstep:6925/10000 train_loss:3.8966 train_time:1177222ms step_avg:170.24ms\nstep:6926/10000 train_loss:3.7993 train_time:1177393ms step_avg:170.24ms\nstep:6927/10000 train_loss:3.7581 train_time:1177567ms step_avg:170.24ms\nstep:6928/10000 train_loss:3.8289 train_time:1177736ms step_avg:170.24ms\nstep:6929/10000 train_loss:3.7860 train_time:1177907ms step_avg:170.24ms\nstep:6930/10000 train_loss:3.9510 train_time:1178079ms step_avg:170.24ms\nstep:6931/10000 train_loss:3.8748 train_time:1178248ms step_avg:170.24ms\nstep:6932/10000 train_loss:3.8557 train_time:1178417ms step_avg:170.24ms\nstep:6933/10000 train_loss:4.0364 train_time:1178585ms step_avg:170.24ms\nstep:6934/10000 train_loss:3.7382 train_time:1178755ms step_avg:170.24ms\nstep:6935/10000 train_loss:3.7349 train_time:1178925ms step_avg:170.24ms\nstep:6936/10000 train_loss:4.2289 train_time:1179099ms step_avg:170.24ms\nstep:6937/10000 train_loss:3.7616 train_time:1179272ms step_avg:170.24ms\nstep:6938/10000 train_loss:3.8101 train_time:1179440ms step_avg:170.24ms\nstep:6939/10000 train_loss:4.0469 train_time:1179610ms step_avg:170.24ms\nstep:6940/10000 train_loss:3.8118 train_time:1179784ms step_avg:170.24ms\nstep:6941/10000 train_loss:3.8563 train_time:1179952ms step_avg:170.24ms\nstep:6942/10000 train_loss:3.8041 train_time:1180125ms step_avg:170.24ms\nstep:6943/10000 train_loss:3.7753 train_time:1180292ms step_avg:170.24ms\nstep:6944/10000 train_loss:3.9426 train_time:1180464ms step_avg:170.24ms\nstep:6945/10000 train_loss:3.8082 train_time:1180634ms step_avg:170.24ms\nstep:6946/10000 train_loss:3.7742 train_time:1180804ms step_avg:170.24ms\nstep:6947/10000 train_loss:3.9237 train_time:1180976ms step_avg:170.24ms\nstep:6948/10000 train_loss:3.7300 train_time:1181147ms step_avg:170.24ms\nstep:6949/10000 train_loss:3.9078 train_time:1181318ms step_avg:170.24ms\nstep:6950/10000 train_loss:3.7434 train_time:1181487ms step_avg:170.24ms\nstep:6951/10000 train_loss:3.7978 train_time:1181657ms step_avg:170.24ms\nstep:6952/10000 train_loss:3.7651 train_time:1181828ms step_avg:170.24ms\nstep:6953/10000 train_loss:3.7368 train_time:1182002ms step_avg:170.24ms\nstep:6954/10000 train_loss:3.9219 train_time:1182169ms step_avg:170.24ms\nstep:6955/10000 train_loss:4.6620 train_time:1182341ms step_avg:170.24ms\nstep:6956/10000 train_loss:3.7953 train_time:1182509ms step_avg:170.24ms\nstep:6957/10000 train_loss:3.8412 train_time:1182677ms step_avg:170.24ms\nstep:6958/10000 train_loss:3.8098 train_time:1182847ms step_avg:170.24ms\nstep:6959/10000 train_loss:3.7858 train_time:1183020ms step_avg:170.24ms\nstep:6960/10000 train_loss:3.7712 train_time:1183190ms step_avg:170.24ms\nstep:6961/10000 train_loss:4.0812 train_time:1183355ms step_avg:170.24ms\nstep:6962/10000 train_loss:3.9256 train_time:1183528ms step_avg:170.24ms\nstep:6963/10000 train_loss:3.7601 train_time:1183699ms step_avg:170.24ms\nstep:6964/10000 train_loss:3.9371 train_time:1183869ms step_avg:170.24ms\nstep:6965/10000 train_loss:4.0583 train_time:1184040ms step_avg:170.24ms\nstep:6966/10000 train_loss:3.8185 train_time:1184209ms step_avg:170.24ms\nstep:6967/10000 train_loss:3.8179 train_time:1184381ms step_avg:170.24ms\nstep:6968/10000 train_loss:3.7817 train_time:1184549ms step_avg:170.24ms\nstep:6969/10000 train_loss:3.6167 train_time:1184719ms step_avg:170.24ms\nstep:6970/10000 train_loss:3.9096 train_time:1184889ms step_avg:170.24ms\nstep:6971/10000 train_loss:3.7063 train_time:1185070ms step_avg:170.24ms\nstep:6972/10000 train_loss:3.8680 train_time:1185227ms step_avg:170.24ms\nstep:6973/10000 train_loss:3.7719 train_time:1185400ms step_avg:170.24ms\nstep:6974/10000 train_loss:3.9089 train_time:1185569ms step_avg:170.24ms\nstep:6975/10000 train_loss:3.7732 train_time:1185739ms step_avg:170.24ms\nstep:6976/10000 train_loss:3.8797 train_time:1185908ms step_avg:170.24ms\nstep:6977/10000 train_loss:3.8020 train_time:1186079ms step_avg:170.24ms\nstep:6978/10000 train_loss:3.8938 train_time:1186246ms step_avg:170.24ms\nstep:6979/10000 train_loss:3.6970 train_time:1186419ms step_avg:170.24ms\nstep:6980/10000 train_loss:3.7113 train_time:1186589ms step_avg:170.24ms\nstep:6981/10000 train_loss:3.6623 train_time:1186760ms step_avg:170.24ms\nstep:6982/10000 train_loss:3.9016 train_time:1186930ms step_avg:170.24ms\nstep:6983/10000 train_loss:3.7492 train_time:1187115ms step_avg:170.24ms\nstep:6984/10000 train_loss:3.5797 train_time:1187279ms step_avg:170.24ms\nstep:6985/10000 train_loss:3.9354 train_time:1187447ms step_avg:170.24ms\nstep:6986/10000 train_loss:3.9252 train_time:1187616ms step_avg:170.24ms\nstep:6987/10000 train_loss:3.7815 train_time:1187785ms step_avg:170.24ms\nstep:6988/10000 train_loss:3.5230 train_time:1187956ms step_avg:170.24ms\nstep:6989/10000 train_loss:3.6280 train_time:1188128ms step_avg:170.24ms\nstep:6990/10000 train_loss:3.7521 train_time:1188295ms step_avg:170.24ms\nstep:6991/10000 train_loss:3.8755 train_time:1188464ms step_avg:170.24ms\nstep:6992/10000 train_loss:3.8263 train_time:1188634ms step_avg:170.24ms\nstep:6993/10000 train_loss:3.8224 train_time:1188801ms step_avg:170.24ms\nstep:6994/10000 train_loss:3.7430 train_time:1188971ms step_avg:170.24ms\nstep:6995/10000 train_loss:3.7370 train_time:1189141ms step_avg:170.24ms\nstep:6996/10000 train_loss:3.8300 train_time:1189310ms step_avg:170.24ms\nstep:6997/10000 train_loss:3.8783 train_time:1189482ms step_avg:170.24ms\nstep:6998/10000 train_loss:3.8540 train_time:1189653ms step_avg:170.24ms\nstep:6999/10000 train_loss:3.7928 train_time:1189824ms step_avg:170.24ms\nstep:7000/10000 train_loss:3.8254 train_time:1189994ms step_avg:170.24ms\nstep:7001/10000 train_loss:3.8688 train_time:1190165ms step_avg:170.24ms\nstep:7002/10000 train_loss:3.8914 train_time:1190336ms step_avg:170.24ms\nstep:7003/10000 train_loss:3.8591 train_time:1190507ms step_avg:170.24ms\nstep:7004/10000 train_loss:3.7816 train_time:1190678ms step_avg:170.24ms\nstep:7005/10000 train_loss:3.7613 train_time:1190848ms step_avg:170.24ms\nstep:7006/10000 train_loss:3.8410 train_time:1191021ms step_avg:170.24ms\nstep:7007/10000 train_loss:3.8491 train_time:1191191ms step_avg:170.24ms\nstep:7008/10000 train_loss:4.0703 train_time:1191360ms step_avg:170.24ms\nstep:7009/10000 train_loss:3.7860 train_time:1191529ms step_avg:170.24ms\nstep:7010/10000 train_loss:3.8713 train_time:1191698ms step_avg:170.24ms\nstep:7011/10000 train_loss:4.1050 train_time:1191867ms step_avg:170.24ms\nstep:7012/10000 train_loss:3.7671 train_time:1192038ms step_avg:170.24ms\nstep:7013/10000 train_loss:3.8330 train_time:1192208ms step_avg:170.24ms\nstep:7014/10000 train_loss:3.7935 train_time:1192380ms step_avg:170.24ms\nstep:7015/10000 train_loss:4.0106 train_time:1192549ms step_avg:170.24ms\nstep:7016/10000 train_loss:3.8580 train_time:1192719ms step_avg:170.24ms\nstep:7017/10000 train_loss:3.9003 train_time:1192889ms step_avg:170.24ms\nstep:7018/10000 train_loss:3.8043 train_time:1193063ms step_avg:170.24ms\nstep:7019/10000 train_loss:3.7698 train_time:1193229ms step_avg:170.24ms\nstep:7020/10000 train_loss:3.8188 train_time:1193400ms step_avg:170.24ms\nstep:7021/10000 train_loss:3.7988 train_time:1193572ms step_avg:170.24ms\nstep:7022/10000 train_loss:3.8211 train_time:1193743ms step_avg:170.24ms\nstep:7023/10000 train_loss:3.9678 train_time:1193913ms step_avg:170.24ms\nstep:7024/10000 train_loss:3.4848 train_time:1194084ms step_avg:170.24ms\nstep:7025/10000 train_loss:3.9270 train_time:1194254ms step_avg:170.24ms\nstep:7026/10000 train_loss:3.9606 train_time:1194426ms step_avg:170.24ms\nstep:7027/10000 train_loss:3.8854 train_time:1194596ms step_avg:170.24ms\nstep:7028/10000 train_loss:3.6131 train_time:1194768ms step_avg:170.24ms\nstep:7029/10000 train_loss:3.8238 train_time:1194940ms step_avg:170.24ms\nstep:7030/10000 train_loss:3.8689 train_time:1195108ms step_avg:170.24ms\nstep:7031/10000 train_loss:3.9465 train_time:1195281ms step_avg:170.24ms\nstep:7032/10000 train_loss:3.7014 train_time:1195449ms step_avg:170.24ms\nstep:7033/10000 train_loss:3.9125 train_time:1195620ms step_avg:170.24ms\nstep:7034/10000 train_loss:3.9044 train_time:1195789ms step_avg:170.24ms\nstep:7035/10000 train_loss:3.7305 train_time:1195961ms step_avg:170.24ms\nstep:7036/10000 train_loss:3.6242 train_time:1196133ms step_avg:170.24ms\nstep:7037/10000 train_loss:3.8023 train_time:1196302ms step_avg:170.24ms\nstep:7038/10000 train_loss:3.8321 train_time:1196472ms step_avg:170.24ms\nstep:7039/10000 train_loss:3.7649 train_time:1196645ms step_avg:170.24ms\nstep:7040/10000 train_loss:3.7895 train_time:1196814ms step_avg:170.24ms\nstep:7041/10000 train_loss:3.7797 train_time:1196982ms step_avg:170.24ms\nstep:7042/10000 train_loss:3.7882 train_time:1197151ms step_avg:170.24ms\nstep:7043/10000 train_loss:3.7149 train_time:1197321ms step_avg:170.24ms\nstep:7044/10000 train_loss:3.6937 train_time:1197490ms step_avg:170.24ms\nstep:7045/10000 train_loss:3.7707 train_time:1197666ms step_avg:170.24ms\nstep:7046/10000 train_loss:3.9169 train_time:1197840ms step_avg:170.24ms\nstep:7047/10000 train_loss:3.8120 train_time:1198013ms step_avg:170.24ms\nstep:7048/10000 train_loss:3.6563 train_time:1198181ms step_avg:170.24ms\nstep:7049/10000 train_loss:3.6659 train_time:1198349ms step_avg:170.24ms\nstep:7050/10000 train_loss:3.7917 train_time:1198518ms step_avg:170.24ms\nstep:7051/10000 train_loss:3.8850 train_time:1198691ms step_avg:170.24ms\nstep:7052/10000 train_loss:4.7217 train_time:1198863ms step_avg:170.24ms\nstep:7053/10000 train_loss:3.8037 train_time:1199035ms step_avg:170.24ms\nstep:7054/10000 train_loss:3.8350 train_time:1199207ms step_avg:170.25ms\nstep:7055/10000 train_loss:3.7968 train_time:1199374ms step_avg:170.24ms\nstep:7056/10000 train_loss:3.7867 train_time:1199545ms step_avg:170.24ms\nstep:7057/10000 train_loss:3.8401 train_time:1199715ms step_avg:170.24ms\nstep:7058/10000 train_loss:3.8010 train_time:1199886ms step_avg:170.24ms\nstep:7059/10000 train_loss:3.9194 train_time:1200059ms step_avg:170.25ms\nstep:7060/10000 train_loss:3.8033 train_time:1200226ms step_avg:170.24ms\nstep:7061/10000 train_loss:3.9618 train_time:1200397ms step_avg:170.24ms\nstep:7062/10000 train_loss:3.9237 train_time:1200565ms step_avg:170.24ms\nstep:7063/10000 train_loss:3.7346 train_time:1200738ms step_avg:170.25ms\nstep:7064/10000 train_loss:3.8341 train_time:1200908ms step_avg:170.24ms\nstep:7065/10000 train_loss:3.8166 train_time:1201081ms step_avg:170.25ms\nstep:7066/10000 train_loss:3.8092 train_time:1201247ms step_avg:170.24ms\nstep:7067/10000 train_loss:3.7372 train_time:1201418ms step_avg:170.24ms\nstep:7068/10000 train_loss:3.8562 train_time:1201589ms step_avg:170.24ms\nstep:7069/10000 train_loss:3.8325 train_time:1201761ms step_avg:170.25ms\nstep:7070/10000 train_loss:3.7605 train_time:1201932ms step_avg:170.25ms\nstep:7071/10000 train_loss:3.8372 train_time:1202103ms step_avg:170.25ms\nstep:7072/10000 train_loss:3.7588 train_time:1202272ms step_avg:170.25ms\nstep:7073/10000 train_loss:3.7707 train_time:1202445ms step_avg:170.25ms\nstep:7074/10000 train_loss:3.8398 train_time:1202614ms step_avg:170.25ms\nstep:7075/10000 train_loss:3.8753 train_time:1202781ms step_avg:170.25ms\nstep:7076/10000 train_loss:3.7798 train_time:1202949ms step_avg:170.24ms\nstep:7077/10000 train_loss:3.7391 train_time:1203124ms step_avg:170.25ms\nstep:7078/10000 train_loss:3.8338 train_time:1203295ms step_avg:170.25ms\nstep:7079/10000 train_loss:3.7793 train_time:1203461ms step_avg:170.24ms\nstep:7080/10000 train_loss:3.8331 train_time:1203632ms step_avg:170.24ms\nstep:7081/10000 train_loss:3.7586 train_time:1203802ms step_avg:170.24ms\nstep:7082/10000 train_loss:3.7615 train_time:1203972ms step_avg:170.24ms\nstep:7083/10000 train_loss:3.7032 train_time:1204142ms step_avg:170.24ms\nstep:7084/10000 train_loss:4.0764 train_time:1204309ms step_avg:170.24ms\nstep:7085/10000 train_loss:3.8519 train_time:1204480ms step_avg:170.24ms\nstep:7086/10000 train_loss:3.8490 train_time:1204649ms step_avg:170.24ms\nstep:7087/10000 train_loss:3.8199 train_time:1204822ms step_avg:170.24ms\nstep:7088/10000 train_loss:3.8004 train_time:1204993ms step_avg:170.24ms\nstep:7089/10000 train_loss:3.8030 train_time:1205161ms step_avg:170.24ms\nstep:7090/10000 train_loss:3.8250 train_time:1205329ms step_avg:170.24ms\nstep:7091/10000 train_loss:3.8042 train_time:1205499ms step_avg:170.24ms\nstep:7092/10000 train_loss:3.8780 train_time:1205668ms step_avg:170.24ms\nstep:7093/10000 train_loss:3.9284 train_time:1205838ms step_avg:170.24ms\nstep:7094/10000 train_loss:3.8217 train_time:1206008ms step_avg:170.24ms\nstep:7095/10000 train_loss:4.1845 train_time:1206177ms step_avg:170.24ms\nstep:7096/10000 train_loss:4.2270 train_time:1206346ms step_avg:170.24ms\nstep:7097/10000 train_loss:3.7670 train_time:1206516ms step_avg:170.24ms\nstep:7098/10000 train_loss:3.7911 train_time:1206687ms step_avg:170.24ms\nstep:7099/10000 train_loss:3.8520 train_time:1206859ms step_avg:170.24ms\nstep:7100/10000 train_loss:3.8250 train_time:1207032ms step_avg:170.24ms\nstep:7101/10000 train_loss:3.8660 train_time:1207204ms step_avg:170.24ms\nstep:7102/10000 train_loss:3.8119 train_time:1207371ms step_avg:170.24ms\nstep:7103/10000 train_loss:3.9553 train_time:1207541ms step_avg:170.24ms\nstep:7104/10000 train_loss:3.7032 train_time:1207712ms step_avg:170.24ms\nstep:7105/10000 train_loss:3.7875 train_time:1207891ms step_avg:170.25ms\nstep:7106/10000 train_loss:3.8684 train_time:1208062ms step_avg:170.25ms\nstep:7107/10000 train_loss:3.7061 train_time:1208233ms step_avg:170.25ms\nstep:7108/10000 train_loss:3.9846 train_time:1208405ms step_avg:170.25ms\nstep:7109/10000 train_loss:4.1477 train_time:1208576ms step_avg:170.25ms\nstep:7110/10000 train_loss:3.7561 train_time:1208746ms step_avg:170.25ms\nstep:7111/10000 train_loss:3.7206 train_time:1208917ms step_avg:170.25ms\nstep:7112/10000 train_loss:3.6248 train_time:1209087ms step_avg:170.25ms\nstep:7113/10000 train_loss:3.8751 train_time:1209259ms step_avg:170.25ms\nstep:7114/10000 train_loss:3.9239 train_time:1209427ms step_avg:170.25ms\nstep:7115/10000 train_loss:3.8892 train_time:1209597ms step_avg:170.25ms\nstep:7116/10000 train_loss:3.8353 train_time:1209768ms step_avg:170.25ms\nstep:7117/10000 train_loss:3.8976 train_time:1209940ms step_avg:170.25ms\nstep:7118/10000 train_loss:4.0391 train_time:1210109ms step_avg:170.25ms\nstep:7119/10000 train_loss:3.7778 train_time:1210282ms step_avg:170.25ms\nstep:7120/10000 train_loss:3.9250 train_time:1210453ms step_avg:170.25ms\nstep:7121/10000 train_loss:3.7925 train_time:1210622ms step_avg:170.25ms\nstep:7122/10000 train_loss:3.7733 train_time:1210790ms step_avg:170.25ms\nstep:7123/10000 train_loss:4.0380 train_time:1210961ms step_avg:170.25ms\nstep:7124/10000 train_loss:3.9886 train_time:1211131ms step_avg:170.25ms\nstep:7125/10000 train_loss:3.9900 train_time:1211300ms step_avg:170.25ms\nstep:7126/10000 train_loss:3.8178 train_time:1211472ms step_avg:170.25ms\nstep:7127/10000 train_loss:3.6963 train_time:1211644ms step_avg:170.25ms\nstep:7128/10000 train_loss:3.8335 train_time:1211812ms step_avg:170.25ms\nstep:7129/10000 train_loss:3.8727 train_time:1211981ms step_avg:170.25ms\nstep:7130/10000 train_loss:3.7534 train_time:1212151ms step_avg:170.25ms\nstep:7131/10000 train_loss:3.9946 train_time:1212320ms step_avg:170.25ms\nstep:7132/10000 train_loss:4.0863 train_time:1212491ms step_avg:170.25ms\nstep:7133/10000 train_loss:3.9718 train_time:1212660ms step_avg:170.25ms\nstep:7134/10000 train_loss:3.7924 train_time:1212831ms step_avg:170.25ms\nstep:7135/10000 train_loss:3.8303 train_time:1213005ms step_avg:170.25ms\nstep:7136/10000 train_loss:3.9024 train_time:1213175ms step_avg:170.25ms\nstep:7137/10000 train_loss:3.8953 train_time:1213348ms step_avg:170.25ms\nstep:7138/10000 train_loss:3.8108 train_time:1213520ms step_avg:170.25ms\nstep:7139/10000 train_loss:3.7190 train_time:1213690ms step_avg:170.25ms\nstep:7140/10000 train_loss:3.8979 train_time:1213861ms step_avg:170.25ms\nstep:7141/10000 train_loss:3.7832 train_time:1214029ms step_avg:170.25ms\nstep:7142/10000 train_loss:3.7183 train_time:1214200ms step_avg:170.25ms\nstep:7143/10000 train_loss:3.7273 train_time:1214370ms step_avg:170.25ms\nstep:7144/10000 train_loss:3.7052 train_time:1214544ms step_avg:170.25ms\nstep:7145/10000 train_loss:3.8516 train_time:1214715ms step_avg:170.25ms\nstep:7146/10000 train_loss:3.9553 train_time:1214887ms step_avg:170.25ms\nstep:7147/10000 train_loss:3.8511 train_time:1215058ms step_avg:170.25ms\nstep:7148/10000 train_loss:3.7700 train_time:1215229ms step_avg:170.25ms\nstep:7149/10000 train_loss:3.8493 train_time:1215399ms step_avg:170.25ms\nstep:7150/10000 train_loss:3.9121 train_time:1215566ms step_avg:170.25ms\nstep:7151/10000 train_loss:3.8597 train_time:1215737ms step_avg:170.25ms\nstep:7152/10000 train_loss:3.8897 train_time:1215907ms step_avg:170.25ms\nstep:7153/10000 train_loss:3.8983 train_time:1216075ms step_avg:170.25ms\nstep:7154/10000 train_loss:3.6539 train_time:1216249ms step_avg:170.25ms\nstep:7155/10000 train_loss:3.7319 train_time:1216422ms step_avg:170.25ms\nstep:7156/10000 train_loss:3.7837 train_time:1216592ms step_avg:170.25ms\nstep:7157/10000 train_loss:3.8360 train_time:1216762ms step_avg:170.25ms\nstep:7158/10000 train_loss:3.8404 train_time:1216932ms step_avg:170.25ms\nstep:7159/10000 train_loss:3.9695 train_time:1217103ms step_avg:170.25ms\nstep:7160/10000 train_loss:3.9502 train_time:1217274ms step_avg:170.25ms\nstep:7161/10000 train_loss:3.6963 train_time:1217447ms step_avg:170.25ms\nstep:7162/10000 train_loss:3.8186 train_time:1217617ms step_avg:170.25ms\nstep:7163/10000 train_loss:3.8906 train_time:1217787ms step_avg:170.25ms\nstep:7164/10000 train_loss:3.8111 train_time:1217957ms step_avg:170.25ms\nstep:7165/10000 train_loss:4.1117 train_time:1218125ms step_avg:170.25ms\nstep:7166/10000 train_loss:3.8184 train_time:1218294ms step_avg:170.25ms\nstep:7167/10000 train_loss:3.8354 train_time:1218466ms step_avg:170.25ms\nstep:7168/10000 train_loss:3.7531 train_time:1218636ms step_avg:170.25ms\nstep:7169/10000 train_loss:3.6439 train_time:1218808ms step_avg:170.25ms\nstep:7170/10000 train_loss:3.9227 train_time:1218981ms step_avg:170.25ms\nstep:7171/10000 train_loss:3.7649 train_time:1219149ms step_avg:170.25ms\nstep:7172/10000 train_loss:3.9119 train_time:1219318ms step_avg:170.25ms\nstep:7173/10000 train_loss:3.7866 train_time:1219488ms step_avg:170.25ms\nstep:7174/10000 train_loss:3.9399 train_time:1219656ms step_avg:170.25ms\nstep:7175/10000 train_loss:3.7222 train_time:1219828ms step_avg:170.25ms\nstep:7176/10000 train_loss:3.7842 train_time:1219999ms step_avg:170.25ms\nstep:7177/10000 train_loss:3.7524 train_time:1220167ms step_avg:170.25ms\nstep:7178/10000 train_loss:3.8546 train_time:1220337ms step_avg:170.25ms\nstep:7179/10000 train_loss:3.8030 train_time:1220507ms step_avg:170.25ms\nstep:7180/10000 train_loss:3.7521 train_time:1220676ms step_avg:170.25ms\nstep:7181/10000 train_loss:3.7950 train_time:1220849ms step_avg:170.25ms\nstep:7182/10000 train_loss:3.7331 train_time:1221022ms step_avg:170.25ms\nstep:7183/10000 train_loss:3.9486 train_time:1221190ms step_avg:170.25ms\nstep:7184/10000 train_loss:3.8723 train_time:1221359ms step_avg:170.25ms\nstep:7185/10000 train_loss:3.7800 train_time:1221529ms step_avg:170.25ms\nstep:7186/10000 train_loss:3.9019 train_time:1221698ms step_avg:170.25ms\nstep:7187/10000 train_loss:3.7820 train_time:1221868ms step_avg:170.25ms\nstep:7188/10000 train_loss:3.7540 train_time:1222042ms step_avg:170.25ms\nstep:7189/10000 train_loss:3.7189 train_time:1222211ms step_avg:170.25ms\nstep:7190/10000 train_loss:3.8871 train_time:1222381ms step_avg:170.25ms\nstep:7191/10000 train_loss:3.9008 train_time:1222549ms step_avg:170.25ms\nstep:7192/10000 train_loss:3.7615 train_time:1222720ms step_avg:170.25ms\nstep:7193/10000 train_loss:3.8351 train_time:1222889ms step_avg:170.25ms\nstep:7194/10000 train_loss:3.8218 train_time:1223062ms step_avg:170.25ms\nstep:7195/10000 train_loss:3.8007 train_time:1223230ms step_avg:170.25ms\nstep:7196/10000 train_loss:3.7971 train_time:1223400ms step_avg:170.25ms\nstep:7197/10000 train_loss:3.7591 train_time:1223568ms step_avg:170.25ms\nstep:7198/10000 train_loss:3.8983 train_time:1223739ms step_avg:170.25ms\nstep:7199/10000 train_loss:3.8390 train_time:1223909ms step_avg:170.25ms\nstep:7200/10000 train_loss:3.7915 train_time:1224083ms step_avg:170.25ms\nstep:7201/10000 train_loss:3.8934 train_time:1224251ms step_avg:170.25ms\nstep:7202/10000 train_loss:3.8678 train_time:1224422ms step_avg:170.25ms\nstep:7203/10000 train_loss:4.2318 train_time:1224591ms step_avg:170.25ms\nstep:7204/10000 train_loss:3.8882 train_time:1224762ms step_avg:170.25ms\nstep:7205/10000 train_loss:3.7931 train_time:1224930ms step_avg:170.25ms\nstep:7206/10000 train_loss:3.8545 train_time:1225101ms step_avg:170.25ms\nstep:7207/10000 train_loss:3.8022 train_time:1225271ms step_avg:170.25ms\nstep:7208/10000 train_loss:3.7884 train_time:1225441ms step_avg:170.25ms\nstep:7209/10000 train_loss:3.8063 train_time:1225612ms step_avg:170.25ms\nstep:7210/10000 train_loss:3.8522 train_time:1225784ms step_avg:170.25ms\nstep:7211/10000 train_loss:3.9038 train_time:1225954ms step_avg:170.25ms\nstep:7212/10000 train_loss:3.8540 train_time:1226125ms step_avg:170.25ms\nstep:7213/10000 train_loss:3.8221 train_time:1226293ms step_avg:170.25ms\nstep:7214/10000 train_loss:3.7544 train_time:1226462ms step_avg:170.25ms\nstep:7215/10000 train_loss:4.0349 train_time:1226632ms step_avg:170.25ms\nstep:7216/10000 train_loss:3.7949 train_time:1226803ms step_avg:170.25ms\nstep:7217/10000 train_loss:3.7914 train_time:1226972ms step_avg:170.25ms\nstep:7218/10000 train_loss:3.7777 train_time:1227142ms step_avg:170.25ms\nstep:7219/10000 train_loss:3.7517 train_time:1227314ms step_avg:170.25ms\nstep:7220/10000 train_loss:3.8286 train_time:1227484ms step_avg:170.25ms\nstep:7221/10000 train_loss:3.7743 train_time:1227655ms step_avg:170.25ms\nstep:7222/10000 train_loss:3.9762 train_time:1227824ms step_avg:170.25ms\nstep:7223/10000 train_loss:3.9419 train_time:1227994ms step_avg:170.25ms\nstep:7224/10000 train_loss:3.7841 train_time:1228167ms step_avg:170.25ms\nstep:7225/10000 train_loss:3.8110 train_time:1228339ms step_avg:170.25ms\nstep:7226/10000 train_loss:3.7154 train_time:1228509ms step_avg:170.25ms\nstep:7227/10000 train_loss:3.8230 train_time:1228678ms step_avg:170.25ms\nstep:7228/10000 train_loss:3.7114 train_time:1228846ms step_avg:170.25ms\nstep:7229/10000 train_loss:3.7915 train_time:1229016ms step_avg:170.25ms\nstep:7230/10000 train_loss:3.7917 train_time:1229188ms step_avg:170.25ms\nstep:7231/10000 train_loss:4.0321 train_time:1229360ms step_avg:170.25ms\nstep:7232/10000 train_loss:3.9201 train_time:1229530ms step_avg:170.25ms\nstep:7233/10000 train_loss:3.7620 train_time:1229702ms step_avg:170.25ms\nstep:7234/10000 train_loss:3.8040 train_time:1229872ms step_avg:170.25ms\nstep:7235/10000 train_loss:3.7610 train_time:1230043ms step_avg:170.25ms\nstep:7236/10000 train_loss:3.8414 train_time:1230213ms step_avg:170.25ms\nstep:7237/10000 train_loss:3.7745 train_time:1230386ms step_avg:170.25ms\nstep:7238/10000 train_loss:3.8967 train_time:1230557ms step_avg:170.25ms\nstep:7239/10000 train_loss:3.8374 train_time:1230727ms step_avg:170.25ms\nstep:7240/10000 train_loss:3.8285 train_time:1230897ms step_avg:170.25ms\nstep:7241/10000 train_loss:3.8426 train_time:1231066ms step_avg:170.25ms\nstep:7242/10000 train_loss:3.9231 train_time:1231239ms step_avg:170.25ms\nstep:7243/10000 train_loss:3.7477 train_time:1231408ms step_avg:170.25ms\nstep:7244/10000 train_loss:3.9556 train_time:1231578ms step_avg:170.25ms\nstep:7245/10000 train_loss:3.8294 train_time:1231747ms step_avg:170.25ms\nstep:7246/10000 train_loss:3.7413 train_time:1231920ms step_avg:170.25ms\nstep:7247/10000 train_loss:3.7523 train_time:1232089ms step_avg:170.25ms\nstep:7248/10000 train_loss:3.7638 train_time:1232260ms step_avg:170.25ms\nstep:7249/10000 train_loss:3.8759 train_time:1232428ms step_avg:170.25ms\nstep:7250/10000 train_loss:3.8760 train_time:1232600ms step_avg:170.25ms\nstep:7251/10000 train_loss:3.9332 train_time:1232769ms step_avg:170.25ms\nstep:7252/10000 train_loss:3.8777 train_time:1232941ms step_avg:170.25ms\nstep:7253/10000 train_loss:3.8797 train_time:1233110ms step_avg:170.25ms\nstep:7254/10000 train_loss:3.7357 train_time:1233283ms step_avg:170.25ms\nstep:7255/10000 train_loss:3.8578 train_time:1233451ms step_avg:170.25ms\nstep:7256/10000 train_loss:3.9838 train_time:1233620ms step_avg:170.25ms\nstep:7257/10000 train_loss:3.7564 train_time:1233789ms step_avg:170.25ms\nstep:7258/10000 train_loss:3.8133 train_time:1233960ms step_avg:170.25ms\nstep:7259/10000 train_loss:3.8474 train_time:1234131ms step_avg:170.25ms\nstep:7260/10000 train_loss:3.8569 train_time:1234305ms step_avg:170.25ms\nstep:7261/10000 train_loss:3.7589 train_time:1234479ms step_avg:170.25ms\nstep:7262/10000 train_loss:3.7876 train_time:1234647ms step_avg:170.25ms\nstep:7263/10000 train_loss:3.7212 train_time:1234818ms step_avg:170.25ms\nstep:7264/10000 train_loss:3.8827 train_time:1234988ms step_avg:170.25ms\nstep:7265/10000 train_loss:3.8051 train_time:1235156ms step_avg:170.25ms\nstep:7266/10000 train_loss:3.6600 train_time:1235327ms step_avg:170.25ms\nstep:7267/10000 train_loss:3.8509 train_time:1235499ms step_avg:170.25ms\nstep:7268/10000 train_loss:3.8994 train_time:1235667ms step_avg:170.25ms\nstep:7269/10000 train_loss:3.8122 train_time:1235839ms step_avg:170.25ms\nstep:7270/10000 train_loss:3.8547 train_time:1236010ms step_avg:170.25ms\nstep:7271/10000 train_loss:3.7980 train_time:1236180ms step_avg:170.25ms\nstep:7272/10000 train_loss:3.8631 train_time:1236349ms step_avg:170.25ms\nstep:7273/10000 train_loss:3.8332 train_time:1236521ms step_avg:170.25ms\nstep:7274/10000 train_loss:3.6383 train_time:1236689ms step_avg:170.25ms\nstep:7275/10000 train_loss:3.7617 train_time:1236861ms step_avg:170.25ms\nstep:7276/10000 train_loss:3.9117 train_time:1237032ms step_avg:170.25ms\nstep:7277/10000 train_loss:3.7992 train_time:1237204ms step_avg:170.25ms\nstep:7278/10000 train_loss:3.8152 train_time:1237374ms step_avg:170.25ms\nstep:7279/10000 train_loss:3.8902 train_time:1237547ms step_avg:170.25ms\nstep:7280/10000 train_loss:3.8622 train_time:1237717ms step_avg:170.25ms\nstep:7281/10000 train_loss:3.8167 train_time:1237889ms step_avg:170.25ms\nstep:7282/10000 train_loss:3.7883 train_time:1238060ms step_avg:170.25ms\nstep:7283/10000 train_loss:3.7712 train_time:1238227ms step_avg:170.25ms\nstep:7284/10000 train_loss:3.8963 train_time:1238396ms step_avg:170.25ms\nstep:7285/10000 train_loss:3.7136 train_time:1238564ms step_avg:170.25ms\nstep:7286/10000 train_loss:3.9022 train_time:1238733ms step_avg:170.25ms\nstep:7287/10000 train_loss:3.8145 train_time:1238901ms step_avg:170.25ms\nstep:7288/10000 train_loss:3.8273 train_time:1239071ms step_avg:170.25ms\nstep:7289/10000 train_loss:3.8618 train_time:1239240ms step_avg:170.25ms\nstep:7290/10000 train_loss:3.7960 train_time:1239407ms step_avg:170.25ms\nstep:7291/10000 train_loss:3.8607 train_time:1239578ms step_avg:170.25ms\nstep:7292/10000 train_loss:3.8292 train_time:1239747ms step_avg:170.25ms\nstep:7293/10000 train_loss:3.8490 train_time:1239918ms step_avg:170.25ms\nstep:7294/10000 train_loss:3.7685 train_time:1240090ms step_avg:170.25ms\nstep:7295/10000 train_loss:3.8559 train_time:1240261ms step_avg:170.25ms\nstep:7296/10000 train_loss:3.7616 train_time:1240431ms step_avg:170.25ms\nstep:7297/10000 train_loss:3.6793 train_time:1240602ms step_avg:170.25ms\nstep:7298/10000 train_loss:3.8118 train_time:1240771ms step_avg:170.25ms\nstep:7299/10000 train_loss:3.7779 train_time:1240942ms step_avg:170.25ms\nstep:7300/10000 train_loss:3.7734 train_time:1241110ms step_avg:170.25ms\nstep:7301/10000 train_loss:3.9299 train_time:1241281ms step_avg:170.25ms\nstep:7302/10000 train_loss:3.7758 train_time:1241452ms step_avg:170.25ms\nstep:7303/10000 train_loss:3.7530 train_time:1241625ms step_avg:170.25ms\nstep:7304/10000 train_loss:3.6471 train_time:1241794ms step_avg:170.25ms\nstep:7305/10000 train_loss:3.7798 train_time:1241964ms step_avg:170.25ms\nstep:7306/10000 train_loss:3.7660 train_time:1242137ms step_avg:170.25ms\nstep:7307/10000 train_loss:3.8881 train_time:1242304ms step_avg:170.25ms\nstep:7308/10000 train_loss:3.8194 train_time:1242477ms step_avg:170.25ms\nstep:7309/10000 train_loss:4.0084 train_time:1242645ms step_avg:170.25ms\nstep:7310/10000 train_loss:3.8960 train_time:1242819ms step_avg:170.25ms\nstep:7311/10000 train_loss:3.7111 train_time:1242988ms step_avg:170.25ms\nstep:7312/10000 train_loss:3.9476 train_time:1243160ms step_avg:170.25ms\nstep:7313/10000 train_loss:3.8864 train_time:1243330ms step_avg:170.25ms\nstep:7314/10000 train_loss:3.6456 train_time:1243505ms step_avg:170.25ms\nstep:7315/10000 train_loss:3.9116 train_time:1243674ms step_avg:170.25ms\nstep:7316/10000 train_loss:3.8930 train_time:1243846ms step_avg:170.25ms\nstep:7317/10000 train_loss:3.9840 train_time:1244017ms step_avg:170.25ms\nstep:7318/10000 train_loss:4.2061 train_time:1244189ms step_avg:170.25ms\nstep:7319/10000 train_loss:3.6756 train_time:1244360ms step_avg:170.25ms\nstep:7320/10000 train_loss:3.8796 train_time:1244528ms step_avg:170.25ms\nstep:7321/10000 train_loss:3.7775 train_time:1244698ms step_avg:170.25ms\nstep:7322/10000 train_loss:3.8024 train_time:1244866ms step_avg:170.25ms\nstep:7323/10000 train_loss:3.8340 train_time:1245036ms step_avg:170.25ms\nstep:7324/10000 train_loss:3.7329 train_time:1245206ms step_avg:170.25ms\nstep:7325/10000 train_loss:3.8359 train_time:1245375ms step_avg:170.25ms\nstep:7326/10000 train_loss:3.4930 train_time:1245544ms step_avg:170.25ms\nstep:7327/10000 train_loss:3.7877 train_time:1245720ms step_avg:170.25ms\nstep:7328/10000 train_loss:3.8031 train_time:1245887ms step_avg:170.25ms\nstep:7329/10000 train_loss:3.8844 train_time:1246058ms step_avg:170.25ms\nstep:7330/10000 train_loss:3.8207 train_time:1246231ms step_avg:170.25ms\nstep:7331/10000 train_loss:3.9351 train_time:1246395ms step_avg:170.25ms\nstep:7332/10000 train_loss:3.8877 train_time:1246565ms step_avg:170.25ms\nstep:7333/10000 train_loss:3.7275 train_time:1246735ms step_avg:170.25ms\nstep:7334/10000 train_loss:3.6685 train_time:1246907ms step_avg:170.25ms\nstep:7335/10000 train_loss:3.8198 train_time:1247079ms step_avg:170.25ms\nstep:7336/10000 train_loss:3.7030 train_time:1247250ms step_avg:170.25ms\nstep:7337/10000 train_loss:3.8355 train_time:1247419ms step_avg:170.25ms\nstep:7338/10000 train_loss:3.8371 train_time:1247590ms step_avg:170.25ms\nstep:7339/10000 train_loss:3.7892 train_time:1247761ms step_avg:170.25ms\nstep:7340/10000 train_loss:3.8367 train_time:1247929ms step_avg:170.25ms\nstep:7341/10000 train_loss:3.7290 train_time:1248102ms step_avg:170.25ms\nstep:7342/10000 train_loss:3.9042 train_time:1248272ms step_avg:170.25ms\nstep:7343/10000 train_loss:3.8147 train_time:1248441ms step_avg:170.25ms\nstep:7344/10000 train_loss:3.7113 train_time:1248610ms step_avg:170.25ms\nstep:7345/10000 train_loss:3.8777 train_time:1248783ms step_avg:170.25ms\nstep:7346/10000 train_loss:3.8344 train_time:1248953ms step_avg:170.25ms\nstep:7347/10000 train_loss:3.9213 train_time:1249126ms step_avg:170.25ms\nstep:7348/10000 train_loss:4.0701 train_time:1249295ms step_avg:170.25ms\nstep:7349/10000 train_loss:3.8969 train_time:1249463ms step_avg:170.25ms\nstep:7350/10000 train_loss:3.7088 train_time:1249631ms step_avg:170.25ms\nstep:7351/10000 train_loss:3.7440 train_time:1249805ms step_avg:170.25ms\nstep:7352/10000 train_loss:3.8658 train_time:1249975ms step_avg:170.25ms\nstep:7353/10000 train_loss:3.8818 train_time:1250146ms step_avg:170.25ms\nstep:7354/10000 train_loss:3.8885 train_time:1250318ms step_avg:170.25ms\nstep:7355/10000 train_loss:3.6679 train_time:1250487ms step_avg:170.25ms\nstep:7356/10000 train_loss:3.8522 train_time:1250655ms step_avg:170.25ms\nstep:7357/10000 train_loss:3.8479 train_time:1250826ms step_avg:170.25ms\nstep:7358/10000 train_loss:3.8437 train_time:1250995ms step_avg:170.25ms\nstep:7359/10000 train_loss:3.8291 train_time:1251161ms step_avg:170.25ms\nstep:7360/10000 train_loss:3.9556 train_time:1251332ms step_avg:170.25ms\nstep:7361/10000 train_loss:3.7776 train_time:1251502ms step_avg:170.25ms\nstep:7362/10000 train_loss:3.7800 train_time:1251670ms step_avg:170.25ms\nstep:7363/10000 train_loss:3.8101 train_time:1251842ms step_avg:170.25ms\nstep:7364/10000 train_loss:3.8373 train_time:1252011ms step_avg:170.25ms\nstep:7365/10000 train_loss:3.8554 train_time:1252186ms step_avg:170.25ms\nstep:7366/10000 train_loss:3.7630 train_time:1252360ms step_avg:170.25ms\nstep:7367/10000 train_loss:3.7447 train_time:1252528ms step_avg:170.25ms\nstep:7368/10000 train_loss:3.6937 train_time:1252699ms step_avg:170.25ms\nstep:7369/10000 train_loss:3.7979 train_time:1252869ms step_avg:170.25ms\nstep:7370/10000 train_loss:3.6290 train_time:1253040ms step_avg:170.25ms\nstep:7371/10000 train_loss:3.7314 train_time:1253209ms step_avg:170.25ms\nstep:7372/10000 train_loss:3.8589 train_time:1253383ms step_avg:170.25ms\nstep:7373/10000 train_loss:3.7812 train_time:1253553ms step_avg:170.25ms\nstep:7374/10000 train_loss:3.8515 train_time:1253724ms step_avg:170.25ms\nstep:7375/10000 train_loss:3.8925 train_time:1253895ms step_avg:170.25ms\nstep:7376/10000 train_loss:3.7220 train_time:1254065ms step_avg:170.25ms\nstep:7377/10000 train_loss:3.7926 train_time:1254233ms step_avg:170.25ms\nstep:7378/10000 train_loss:3.8868 train_time:1254402ms step_avg:170.25ms\nstep:7379/10000 train_loss:3.7341 train_time:1254570ms step_avg:170.25ms\nstep:7380/10000 train_loss:3.8204 train_time:1254740ms step_avg:170.25ms\nstep:7381/10000 train_loss:3.7720 train_time:1254907ms step_avg:170.25ms\nstep:7382/10000 train_loss:3.7633 train_time:1255079ms step_avg:170.25ms\nstep:7383/10000 train_loss:3.7931 train_time:1255250ms step_avg:170.25ms\nstep:7384/10000 train_loss:3.8883 train_time:1255419ms step_avg:170.25ms\nstep:7385/10000 train_loss:3.7821 train_time:1255587ms step_avg:170.25ms\nstep:7386/10000 train_loss:3.8780 train_time:1255760ms step_avg:170.25ms\nstep:7387/10000 train_loss:3.5782 train_time:1255928ms step_avg:170.25ms\nstep:7388/10000 train_loss:3.6997 train_time:1256099ms step_avg:170.25ms\nstep:7389/10000 train_loss:3.8534 train_time:1256269ms step_avg:170.25ms\nstep:7390/10000 train_loss:3.6104 train_time:1256440ms step_avg:170.25ms\nstep:7391/10000 train_loss:3.8038 train_time:1256607ms step_avg:170.25ms\nstep:7392/10000 train_loss:3.7848 train_time:1256778ms step_avg:170.25ms\nstep:7393/10000 train_loss:3.7347 train_time:1256949ms step_avg:170.25ms\nstep:7394/10000 train_loss:3.7748 train_time:1257119ms step_avg:170.25ms\nstep:7395/10000 train_loss:3.7974 train_time:1257288ms step_avg:170.25ms\nstep:7396/10000 train_loss:3.7926 train_time:1257459ms step_avg:170.25ms\nstep:7397/10000 train_loss:3.8321 train_time:1257630ms step_avg:170.25ms\nstep:7398/10000 train_loss:3.7529 train_time:1257801ms step_avg:170.25ms\nstep:7399/10000 train_loss:3.7788 train_time:1257970ms step_avg:170.25ms\nstep:7400/10000 train_loss:3.7856 train_time:1258143ms step_avg:170.25ms\nstep:7401/10000 train_loss:3.8899 train_time:1258312ms step_avg:170.25ms\nstep:7402/10000 train_loss:3.8031 train_time:1258481ms step_avg:170.25ms\nstep:7403/10000 train_loss:3.7556 train_time:1258650ms step_avg:170.25ms\nstep:7404/10000 train_loss:3.6633 train_time:1258820ms step_avg:170.25ms\nstep:7405/10000 train_loss:3.8238 train_time:1258988ms step_avg:170.25ms\nstep:7406/10000 train_loss:3.8194 train_time:1259160ms step_avg:170.25ms\nstep:7407/10000 train_loss:3.8639 train_time:1259329ms step_avg:170.25ms\nstep:7408/10000 train_loss:3.9278 train_time:1259500ms step_avg:170.25ms\nstep:7409/10000 train_loss:3.8402 train_time:1259666ms step_avg:170.25ms\nstep:7410/10000 train_loss:3.8075 train_time:1259837ms step_avg:170.25ms\nstep:7411/10000 train_loss:3.7630 train_time:1260004ms step_avg:170.25ms\nstep:7412/10000 train_loss:3.7032 train_time:1260175ms step_avg:170.25ms\nstep:7413/10000 train_loss:3.8921 train_time:1260347ms step_avg:170.25ms\nstep:7414/10000 train_loss:3.8626 train_time:1260520ms step_avg:170.25ms\nstep:7415/10000 train_loss:3.7152 train_time:1260689ms step_avg:170.25ms\nstep:7416/10000 train_loss:3.7788 train_time:1260860ms step_avg:170.25ms\nstep:7417/10000 train_loss:3.7312 train_time:1261031ms step_avg:170.25ms\nstep:7418/10000 train_loss:3.8009 train_time:1261203ms step_avg:170.25ms\nstep:7419/10000 train_loss:3.6989 train_time:1261369ms step_avg:170.25ms\nstep:7420/10000 train_loss:3.7551 train_time:1261538ms step_avg:170.25ms\nstep:7421/10000 train_loss:3.8157 train_time:1261707ms step_avg:170.25ms\nstep:7422/10000 train_loss:3.8364 train_time:1261878ms step_avg:170.25ms\nstep:7423/10000 train_loss:3.7338 train_time:1262046ms step_avg:170.25ms\nstep:7424/10000 train_loss:3.9310 train_time:1262218ms step_avg:170.25ms\nstep:7425/10000 train_loss:3.5178 train_time:1262388ms step_avg:170.25ms\nstep:7426/10000 train_loss:3.8158 train_time:1262559ms step_avg:170.25ms\nstep:7427/10000 train_loss:3.8094 train_time:1262729ms step_avg:170.25ms\nstep:7428/10000 train_loss:3.8867 train_time:1262900ms step_avg:170.25ms\nstep:7429/10000 train_loss:3.8736 train_time:1263070ms step_avg:170.25ms\nstep:7430/10000 train_loss:3.8292 train_time:1263240ms step_avg:170.25ms\nstep:7431/10000 train_loss:3.7967 train_time:1263408ms step_avg:170.25ms\nstep:7432/10000 train_loss:3.7914 train_time:1263579ms step_avg:170.25ms\nstep:7433/10000 train_loss:3.7855 train_time:1263749ms step_avg:170.25ms\nstep:7434/10000 train_loss:3.7688 train_time:1263921ms step_avg:170.25ms\nstep:7435/10000 train_loss:3.8654 train_time:1264091ms step_avg:170.25ms\nstep:7436/10000 train_loss:3.7339 train_time:1264260ms step_avg:170.25ms\nstep:7437/10000 train_loss:3.8715 train_time:1264430ms step_avg:170.25ms\nstep:7438/10000 train_loss:3.7520 train_time:1264599ms step_avg:170.25ms\nstep:7439/10000 train_loss:3.7081 train_time:1264768ms step_avg:170.25ms\nstep:7440/10000 train_loss:3.7982 train_time:1264942ms step_avg:170.25ms\nstep:7441/10000 train_loss:3.9413 train_time:1265110ms step_avg:170.25ms\nstep:7442/10000 train_loss:3.7399 train_time:1265279ms step_avg:170.25ms\nstep:7443/10000 train_loss:3.7798 train_time:1265447ms step_avg:170.25ms\nstep:7444/10000 train_loss:3.9942 train_time:1265618ms step_avg:170.25ms\nstep:7445/10000 train_loss:3.7409 train_time:1265788ms step_avg:170.25ms\nstep:7446/10000 train_loss:3.8065 train_time:1265958ms step_avg:170.25ms\nstep:7447/10000 train_loss:3.8002 train_time:1266127ms step_avg:170.25ms\nstep:7448/10000 train_loss:3.8678 train_time:1266296ms step_avg:170.25ms\nstep:7449/10000 train_loss:3.8116 train_time:1266465ms step_avg:170.25ms\nstep:7450/10000 train_loss:4.0349 train_time:1266637ms step_avg:170.25ms\nstep:7451/10000 train_loss:3.8486 train_time:1266807ms step_avg:170.25ms\nstep:7452/10000 train_loss:3.7980 train_time:1266980ms step_avg:170.25ms\nstep:7453/10000 train_loss:3.7391 train_time:1267149ms step_avg:170.25ms\nstep:7454/10000 train_loss:3.7528 train_time:1267318ms step_avg:170.25ms\nstep:7455/10000 train_loss:3.8453 train_time:1267487ms step_avg:170.25ms\nstep:7456/10000 train_loss:3.8348 train_time:1267659ms step_avg:170.25ms\nstep:7457/10000 train_loss:3.7812 train_time:1267829ms step_avg:170.25ms\nstep:7458/10000 train_loss:3.8448 train_time:1268002ms step_avg:170.25ms\nstep:7459/10000 train_loss:3.6717 train_time:1268168ms step_avg:170.25ms\nstep:7460/10000 train_loss:3.7610 train_time:1268339ms step_avg:170.25ms\nstep:7461/10000 train_loss:3.7575 train_time:1268508ms step_avg:170.25ms\nstep:7462/10000 train_loss:3.7405 train_time:1268679ms step_avg:170.25ms\nstep:7463/10000 train_loss:3.7297 train_time:1268848ms step_avg:170.25ms\nstep:7464/10000 train_loss:3.7415 train_time:1269020ms step_avg:170.25ms\nstep:7465/10000 train_loss:3.9808 train_time:1269188ms step_avg:170.25ms\nstep:7466/10000 train_loss:3.7943 train_time:1269358ms step_avg:170.25ms\nstep:7467/10000 train_loss:3.7869 train_time:1269528ms step_avg:170.25ms\nstep:7468/10000 train_loss:3.8028 train_time:1269700ms step_avg:170.25ms\nstep:7469/10000 train_loss:3.9873 train_time:1269870ms step_avg:170.25ms\nstep:7470/10000 train_loss:3.8598 train_time:1270042ms step_avg:170.25ms\nstep:7471/10000 train_loss:3.8280 train_time:1270213ms step_avg:170.25ms\nstep:7472/10000 train_loss:3.8443 train_time:1270385ms step_avg:170.25ms\nstep:7473/10000 train_loss:3.7465 train_time:1270555ms step_avg:170.25ms\nstep:7474/10000 train_loss:3.7389 train_time:1270724ms step_avg:170.25ms\nstep:7475/10000 train_loss:4.0729 train_time:1270893ms step_avg:170.25ms\nstep:7476/10000 train_loss:3.6144 train_time:1271064ms step_avg:170.25ms\nstep:7477/10000 train_loss:3.7464 train_time:1271235ms step_avg:170.25ms\nstep:7478/10000 train_loss:3.9516 train_time:1271406ms step_avg:170.25ms\nstep:7479/10000 train_loss:3.7976 train_time:1271576ms step_avg:170.25ms\nstep:7480/10000 train_loss:3.7242 train_time:1271747ms step_avg:170.25ms\nstep:7481/10000 train_loss:3.7231 train_time:1271916ms step_avg:170.25ms\nstep:7482/10000 train_loss:3.7851 train_time:1272088ms step_avg:170.25ms\nstep:7483/10000 train_loss:3.7805 train_time:1272258ms step_avg:170.25ms\nstep:7484/10000 train_loss:3.6764 train_time:1272426ms step_avg:170.25ms\nstep:7485/10000 train_loss:4.0055 train_time:1272596ms step_avg:170.25ms\nstep:7486/10000 train_loss:3.8498 train_time:1272766ms step_avg:170.25ms\nstep:7487/10000 train_loss:3.7727 train_time:1272939ms step_avg:170.25ms\nstep:7488/10000 train_loss:3.6911 train_time:1273108ms step_avg:170.25ms\nstep:7489/10000 train_loss:3.9661 train_time:1273276ms step_avg:170.25ms\nstep:7490/10000 train_loss:3.8791 train_time:1273446ms step_avg:170.25ms\nstep:7491/10000 train_loss:3.8210 train_time:1273616ms step_avg:170.25ms\nstep:7492/10000 train_loss:3.8372 train_time:1273785ms step_avg:170.25ms\nstep:7493/10000 train_loss:3.6221 train_time:1273956ms step_avg:170.25ms\nstep:7494/10000 train_loss:3.7261 train_time:1274126ms step_avg:170.25ms\nstep:7495/10000 train_loss:3.8109 train_time:1274295ms step_avg:170.25ms\nstep:7496/10000 train_loss:3.8289 train_time:1274466ms step_avg:170.25ms\nstep:7497/10000 train_loss:3.8749 train_time:1274635ms step_avg:170.25ms\nstep:7498/10000 train_loss:3.8305 train_time:1274805ms step_avg:170.25ms\nstep:7499/10000 train_loss:3.7623 train_time:1274977ms step_avg:170.25ms\nstep:7500/10000 train_loss:3.7807 train_time:1275146ms step_avg:170.25ms\nstep:7501/10000 train_loss:3.8126 train_time:1275318ms step_avg:170.25ms\nstep:7502/10000 train_loss:3.7744 train_time:1275489ms step_avg:170.25ms\nstep:7503/10000 train_loss:3.8824 train_time:1275659ms step_avg:170.25ms\nstep:7504/10000 train_loss:3.7755 train_time:1275828ms step_avg:170.25ms\nstep:7505/10000 train_loss:3.8951 train_time:1276000ms step_avg:170.25ms\nstep:7506/10000 train_loss:3.7337 train_time:1276171ms step_avg:170.25ms\nstep:7507/10000 train_loss:3.5810 train_time:1276340ms step_avg:170.25ms\nstep:7508/10000 train_loss:3.7168 train_time:1276510ms step_avg:170.25ms\nstep:7509/10000 train_loss:3.8735 train_time:1276682ms step_avg:170.25ms\nstep:7510/10000 train_loss:3.7498 train_time:1276851ms step_avg:170.25ms\nstep:7511/10000 train_loss:3.8071 train_time:1277021ms step_avg:170.25ms\nstep:7512/10000 train_loss:3.8254 train_time:1277191ms step_avg:170.25ms\nstep:7513/10000 train_loss:3.6933 train_time:1277362ms step_avg:170.25ms\nstep:7514/10000 train_loss:3.7801 train_time:1277531ms step_avg:170.25ms\nstep:7515/10000 train_loss:3.8302 train_time:1277702ms step_avg:170.25ms\nstep:7516/10000 train_loss:3.8234 train_time:1277872ms step_avg:170.25ms\nstep:7517/10000 train_loss:3.9340 train_time:1278043ms step_avg:170.25ms\nstep:7518/10000 train_loss:4.1271 train_time:1278211ms step_avg:170.25ms\nstep:7519/10000 train_loss:3.7548 train_time:1278384ms step_avg:170.25ms\nstep:7520/10000 train_loss:3.7538 train_time:1278553ms step_avg:170.25ms\nstep:7521/10000 train_loss:3.6910 train_time:1278723ms step_avg:170.25ms\nstep:7522/10000 train_loss:3.8713 train_time:1278896ms step_avg:170.25ms\nstep:7523/10000 train_loss:3.6832 train_time:1279067ms step_avg:170.25ms\nstep:7524/10000 train_loss:3.7928 train_time:1279239ms step_avg:170.25ms\nstep:7525/10000 train_loss:3.9264 train_time:1279409ms step_avg:170.25ms\nstep:7526/10000 train_loss:3.8153 train_time:1279580ms step_avg:170.25ms\nstep:7527/10000 train_loss:3.8456 train_time:1279750ms step_avg:170.25ms\nstep:7528/10000 train_loss:3.9014 train_time:1279918ms step_avg:170.25ms\nstep:7529/10000 train_loss:3.8077 train_time:1280089ms step_avg:170.25ms\nstep:7530/10000 train_loss:3.9168 train_time:1280261ms step_avg:170.25ms\nstep:7531/10000 train_loss:3.9227 train_time:1280429ms step_avg:170.25ms\nstep:7532/10000 train_loss:3.7693 train_time:1280601ms step_avg:170.25ms\nstep:7533/10000 train_loss:4.1709 train_time:1280770ms step_avg:170.25ms\nstep:7534/10000 train_loss:3.8119 train_time:1280938ms step_avg:170.25ms\nstep:7535/10000 train_loss:3.8244 train_time:1281109ms step_avg:170.25ms\nstep:7536/10000 train_loss:3.8384 train_time:1281277ms step_avg:170.25ms\nstep:7537/10000 train_loss:3.8316 train_time:1281448ms step_avg:170.25ms\nstep:7538/10000 train_loss:3.7200 train_time:1281620ms step_avg:170.25ms\nstep:7539/10000 train_loss:3.7922 train_time:1281789ms step_avg:170.25ms\nstep:7540/10000 train_loss:3.7450 train_time:1281959ms step_avg:170.25ms\nstep:7541/10000 train_loss:3.7887 train_time:1282132ms step_avg:170.25ms\nstep:7542/10000 train_loss:3.7080 train_time:1282303ms step_avg:170.25ms\nstep:7543/10000 train_loss:3.7215 train_time:1282474ms step_avg:170.25ms\nstep:7544/10000 train_loss:3.8170 train_time:1282644ms step_avg:170.25ms\nstep:7545/10000 train_loss:3.8524 train_time:1282816ms step_avg:170.25ms\nstep:7546/10000 train_loss:3.7492 train_time:1282987ms step_avg:170.25ms\nstep:7547/10000 train_loss:3.7808 train_time:1283157ms step_avg:170.25ms\nstep:7548/10000 train_loss:3.8164 train_time:1283326ms step_avg:170.25ms\nstep:7549/10000 train_loss:3.8186 train_time:1283496ms step_avg:170.25ms\nstep:7550/10000 train_loss:3.7821 train_time:1283667ms step_avg:170.25ms\nstep:7551/10000 train_loss:3.7692 train_time:1283837ms step_avg:170.25ms\nstep:7552/10000 train_loss:3.5524 train_time:1284009ms step_avg:170.25ms\nstep:7553/10000 train_loss:3.8987 train_time:1284183ms step_avg:170.25ms\nstep:7554/10000 train_loss:4.0296 train_time:1284352ms step_avg:170.25ms\nstep:7555/10000 train_loss:3.6531 train_time:1284523ms step_avg:170.25ms\nstep:7556/10000 train_loss:3.9876 train_time:1284692ms step_avg:170.25ms\nstep:7557/10000 train_loss:3.7951 train_time:1284861ms step_avg:170.25ms\nstep:7558/10000 train_loss:3.8978 train_time:1285030ms step_avg:170.25ms\nstep:7559/10000 train_loss:3.8270 train_time:1285200ms step_avg:170.25ms\nstep:7560/10000 train_loss:3.6981 train_time:1285369ms step_avg:170.25ms\nstep:7561/10000 train_loss:3.8258 train_time:1285538ms step_avg:170.25ms\nstep:7562/10000 train_loss:3.7241 train_time:1285708ms step_avg:170.25ms\nstep:7563/10000 train_loss:3.8739 train_time:1285879ms step_avg:170.25ms\nstep:7564/10000 train_loss:3.7439 train_time:1286047ms step_avg:170.25ms\nstep:7565/10000 train_loss:3.8060 train_time:1286216ms step_avg:170.25ms\nstep:7566/10000 train_loss:3.7459 train_time:1286385ms step_avg:170.25ms\nstep:7567/10000 train_loss:3.7616 train_time:1286556ms step_avg:170.25ms\nstep:7568/10000 train_loss:3.7710 train_time:1286724ms step_avg:170.25ms\nstep:7569/10000 train_loss:3.7725 train_time:1286894ms step_avg:170.25ms\nstep:7570/10000 train_loss:3.7195 train_time:1287066ms step_avg:170.25ms\nstep:7571/10000 train_loss:3.7492 train_time:1287238ms step_avg:170.25ms\nstep:7572/10000 train_loss:3.8210 train_time:1287408ms step_avg:170.25ms\nstep:7573/10000 train_loss:3.8252 train_time:1287580ms step_avg:170.25ms\nstep:7574/10000 train_loss:3.7104 train_time:1287750ms step_avg:170.25ms\nstep:7575/10000 train_loss:3.7875 train_time:1287919ms step_avg:170.25ms\nstep:7576/10000 train_loss:3.6812 train_time:1288087ms step_avg:170.25ms\nstep:7577/10000 train_loss:3.8688 train_time:1288256ms step_avg:170.25ms\nstep:7578/10000 train_loss:3.7814 train_time:1288434ms step_avg:170.25ms\nstep:7579/10000 train_loss:3.7917 train_time:1288607ms step_avg:170.25ms\nstep:7580/10000 train_loss:3.9436 train_time:1288770ms step_avg:170.25ms\nstep:7581/10000 train_loss:3.7310 train_time:1288942ms step_avg:170.25ms\nstep:7582/10000 train_loss:3.9043 train_time:1289111ms step_avg:170.25ms\nstep:7583/10000 train_loss:3.8223 train_time:1289280ms step_avg:170.25ms\nstep:7584/10000 train_loss:3.7086 train_time:1289447ms step_avg:170.25ms\nstep:7585/10000 train_loss:3.6229 train_time:1289619ms step_avg:170.25ms\nstep:7586/10000 train_loss:3.8093 train_time:1289789ms step_avg:170.25ms\nstep:7587/10000 train_loss:3.7443 train_time:1289962ms step_avg:170.25ms\nstep:7588/10000 train_loss:3.8074 train_time:1290128ms step_avg:170.25ms\nstep:7589/10000 train_loss:3.7990 train_time:1290300ms step_avg:170.25ms\nstep:7590/10000 train_loss:3.9214 train_time:1290468ms step_avg:170.25ms\nstep:7591/10000 train_loss:3.7994 train_time:1290638ms step_avg:170.25ms\nstep:7592/10000 train_loss:3.8639 train_time:1290809ms step_avg:170.25ms\nstep:7593/10000 train_loss:3.7908 train_time:1290980ms step_avg:170.25ms\nstep:7594/10000 train_loss:3.8229 train_time:1291150ms step_avg:170.25ms\nstep:7595/10000 train_loss:3.8068 train_time:1291321ms step_avg:170.25ms\nstep:7596/10000 train_loss:3.6576 train_time:1291491ms step_avg:170.25ms\nstep:7597/10000 train_loss:3.7450 train_time:1291658ms step_avg:170.25ms\nstep:7598/10000 train_loss:3.8157 train_time:1291829ms step_avg:170.25ms\nstep:7599/10000 train_loss:3.6818 train_time:1292000ms step_avg:170.25ms\nstep:7600/10000 train_loss:3.8108 train_time:1292168ms step_avg:170.25ms\nstep:7601/10000 train_loss:3.7757 train_time:1292338ms step_avg:170.25ms\nstep:7602/10000 train_loss:3.9832 train_time:1292508ms step_avg:170.25ms\nstep:7603/10000 train_loss:3.7011 train_time:1292675ms step_avg:170.25ms\nstep:7604/10000 train_loss:3.8053 train_time:1292846ms step_avg:170.25ms\nstep:7605/10000 train_loss:3.9484 train_time:1293021ms step_avg:170.25ms\nstep:7606/10000 train_loss:3.8300 train_time:1293190ms step_avg:170.25ms\nstep:7607/10000 train_loss:3.9091 train_time:1293361ms step_avg:170.25ms\nstep:7608/10000 train_loss:3.8664 train_time:1293530ms step_avg:170.25ms\nstep:7609/10000 train_loss:3.7663 train_time:1293698ms step_avg:170.25ms\nstep:7610/10000 train_loss:3.8116 train_time:1293869ms step_avg:170.25ms\nstep:7611/10000 train_loss:3.6979 train_time:1294044ms step_avg:170.25ms\nstep:7612/10000 train_loss:3.6564 train_time:1294215ms step_avg:170.25ms\nstep:7613/10000 train_loss:3.6345 train_time:1294383ms step_avg:170.25ms\nstep:7614/10000 train_loss:3.9248 train_time:1294553ms step_avg:170.25ms\nstep:7615/10000 train_loss:3.7525 train_time:1294723ms step_avg:170.25ms\nstep:7616/10000 train_loss:3.8796 train_time:1294893ms step_avg:170.25ms\nstep:7617/10000 train_loss:3.8139 train_time:1295061ms step_avg:170.25ms\nstep:7618/10000 train_loss:3.7596 train_time:1295231ms step_avg:170.25ms\nstep:7619/10000 train_loss:3.7682 train_time:1295402ms step_avg:170.25ms\nstep:7620/10000 train_loss:4.0608 train_time:1295793ms step_avg:170.27ms\nstep:7621/10000 train_loss:3.8504 train_time:1295957ms step_avg:170.27ms\nstep:7622/10000 train_loss:3.7719 train_time:1296123ms step_avg:170.27ms\nstep:7623/10000 train_loss:3.8173 train_time:1296290ms step_avg:170.27ms\nstep:7624/10000 train_loss:3.7690 train_time:1296459ms step_avg:170.27ms\nstep:7625/10000 train_loss:3.6497 train_time:1296627ms step_avg:170.27ms\nstep:7626/10000 train_loss:3.9205 train_time:1296809ms step_avg:170.27ms\nstep:7627/10000 train_loss:3.7704 train_time:1296981ms step_avg:170.27ms\nstep:7628/10000 train_loss:3.7527 train_time:1297151ms step_avg:170.27ms\nstep:7629/10000 train_loss:3.8623 train_time:1297321ms step_avg:170.27ms\nstep:7630/10000 train_loss:3.7456 train_time:1297488ms step_avg:170.27ms\nstep:7631/10000 train_loss:3.9039 train_time:1297659ms step_avg:170.27ms\nstep:7632/10000 train_loss:3.7651 train_time:1297832ms step_avg:170.27ms\nstep:7633/10000 train_loss:3.9183 train_time:1298004ms step_avg:170.27ms\nstep:7634/10000 train_loss:4.0185 train_time:1298172ms step_avg:170.27ms\nstep:7635/10000 train_loss:3.7427 train_time:1298343ms step_avg:170.27ms\nstep:7636/10000 train_loss:3.7753 train_time:1298512ms step_avg:170.27ms\nstep:7637/10000 train_loss:3.8570 train_time:1298684ms step_avg:170.27ms\nstep:7638/10000 train_loss:3.7531 train_time:1298853ms step_avg:170.27ms\nstep:7639/10000 train_loss:3.7567 train_time:1299023ms step_avg:170.27ms\nstep:7640/10000 train_loss:3.7852 train_time:1299193ms step_avg:170.27ms\nstep:7641/10000 train_loss:3.7949 train_time:1299363ms step_avg:170.27ms\nstep:7642/10000 train_loss:3.7460 train_time:1299531ms step_avg:170.27ms\nstep:7643/10000 train_loss:3.7419 train_time:1299701ms step_avg:170.27ms\nstep:7644/10000 train_loss:3.8836 train_time:1299871ms step_avg:170.27ms\nstep:7645/10000 train_loss:3.8053 train_time:1300045ms step_avg:170.27ms\nstep:7646/10000 train_loss:3.8330 train_time:1300215ms step_avg:170.27ms\nstep:7647/10000 train_loss:3.6699 train_time:1300386ms step_avg:170.27ms\nstep:7648/10000 train_loss:3.7993 train_time:1300555ms step_avg:170.27ms\nstep:7649/10000 train_loss:3.8034 train_time:1300722ms step_avg:170.27ms\nstep:7650/10000 train_loss:3.7241 train_time:1300891ms step_avg:170.27ms\nstep:7651/10000 train_loss:3.8250 train_time:1301063ms step_avg:170.27ms\nstep:7652/10000 train_loss:3.8121 train_time:1301236ms step_avg:170.27ms\nstep:7653/10000 train_loss:3.7859 train_time:1301407ms step_avg:170.27ms\nstep:7654/10000 train_loss:3.7735 train_time:1301577ms step_avg:170.27ms\nstep:7655/10000 train_loss:3.7738 train_time:1301745ms step_avg:170.27ms\nstep:7656/10000 train_loss:3.8566 train_time:1301914ms step_avg:170.27ms\nstep:7657/10000 train_loss:3.7520 train_time:1302084ms step_avg:170.27ms\nstep:7658/10000 train_loss:3.8654 train_time:1302255ms step_avg:170.27ms\nstep:7659/10000 train_loss:3.7369 train_time:1302426ms step_avg:170.27ms\nstep:7660/10000 train_loss:3.7708 train_time:1302599ms step_avg:170.27ms\nstep:7661/10000 train_loss:3.7036 train_time:1302768ms step_avg:170.27ms\nstep:7662/10000 train_loss:3.8019 train_time:1302937ms step_avg:170.27ms\nstep:7663/10000 train_loss:3.6660 train_time:1303107ms step_avg:170.27ms\nstep:7664/10000 train_loss:3.9028 train_time:1303276ms step_avg:170.27ms\nstep:7665/10000 train_loss:3.8271 train_time:1303446ms step_avg:170.27ms\nstep:7666/10000 train_loss:3.6621 train_time:1303617ms step_avg:170.27ms\nstep:7667/10000 train_loss:3.6887 train_time:1303785ms step_avg:170.27ms\nstep:7668/10000 train_loss:3.6819 train_time:1303955ms step_avg:170.27ms\nstep:7669/10000 train_loss:3.7618 train_time:1304126ms step_avg:170.27ms\nstep:7670/10000 train_loss:3.5822 train_time:1304296ms step_avg:170.27ms\nstep:7671/10000 train_loss:3.9018 train_time:1304468ms step_avg:170.27ms\nstep:7672/10000 train_loss:4.2404 train_time:1304639ms step_avg:170.27ms\nstep:7673/10000 train_loss:3.7662 train_time:1304806ms step_avg:170.27ms\nstep:7674/10000 train_loss:3.7523 train_time:1304976ms step_avg:170.27ms\nstep:7675/10000 train_loss:3.7787 train_time:1305146ms step_avg:170.27ms\nstep:7676/10000 train_loss:5.2728 train_time:1305316ms step_avg:170.27ms\nstep:7677/10000 train_loss:3.7979 train_time:1305486ms step_avg:170.27ms\nstep:7678/10000 train_loss:3.9879 train_time:1305657ms step_avg:170.27ms\nstep:7679/10000 train_loss:3.7211 train_time:1305826ms step_avg:170.27ms\nstep:7680/10000 train_loss:3.7167 train_time:1305995ms step_avg:170.27ms\nstep:7681/10000 train_loss:3.6508 train_time:1306168ms step_avg:170.27ms\nstep:7682/10000 train_loss:3.9435 train_time:1306339ms step_avg:170.27ms\nstep:7683/10000 train_loss:3.7957 train_time:1306508ms step_avg:170.27ms\nstep:7684/10000 train_loss:3.5754 train_time:1306679ms step_avg:170.27ms\nstep:7685/10000 train_loss:3.8554 train_time:1306851ms step_avg:170.27ms\nstep:7686/10000 train_loss:3.8894 train_time:1307020ms step_avg:170.27ms\nstep:7687/10000 train_loss:3.7523 train_time:1307191ms step_avg:170.27ms\nstep:7688/10000 train_loss:3.8173 train_time:1307360ms step_avg:170.27ms\nstep:7689/10000 train_loss:3.7525 train_time:1307528ms step_avg:170.27ms\nstep:7690/10000 train_loss:3.7710 train_time:1307696ms step_avg:170.27ms\nstep:7691/10000 train_loss:3.7682 train_time:1307868ms step_avg:170.27ms\nstep:7692/10000 train_loss:3.8712 train_time:1308040ms step_avg:170.27ms\nstep:7693/10000 train_loss:3.8031 train_time:1308208ms step_avg:170.27ms\nstep:7694/10000 train_loss:3.9685 train_time:1308380ms step_avg:170.27ms\nstep:7695/10000 train_loss:3.8334 train_time:1308551ms step_avg:170.27ms\nstep:7696/10000 train_loss:3.8317 train_time:1308722ms step_avg:170.27ms\nstep:7697/10000 train_loss:3.7706 train_time:1308891ms step_avg:170.27ms\nstep:7698/10000 train_loss:3.8069 train_time:1309059ms step_avg:170.27ms\nstep:7699/10000 train_loss:3.7853 train_time:1309234ms step_avg:170.27ms\nstep:7700/10000 train_loss:3.7178 train_time:1309404ms step_avg:170.27ms\nstep:7701/10000 train_loss:3.7494 train_time:1309574ms step_avg:170.27ms\nstep:7702/10000 train_loss:3.7195 train_time:1309745ms step_avg:170.27ms\nstep:7703/10000 train_loss:3.8870 train_time:1309918ms step_avg:170.27ms\nstep:7704/10000 train_loss:3.7101 train_time:1310087ms step_avg:170.27ms\nstep:7705/10000 train_loss:3.8337 train_time:1310258ms step_avg:170.27ms\nstep:7706/10000 train_loss:3.9029 train_time:1310428ms step_avg:170.27ms\nstep:7707/10000 train_loss:3.7085 train_time:1310600ms step_avg:170.27ms\nstep:7708/10000 train_loss:3.8228 train_time:1310768ms step_avg:170.27ms\nstep:7709/10000 train_loss:3.6982 train_time:1310937ms step_avg:170.27ms\nstep:7710/10000 train_loss:3.8709 train_time:1311106ms step_avg:170.27ms\nstep:7711/10000 train_loss:3.8198 train_time:1311276ms step_avg:170.27ms\nstep:7712/10000 train_loss:3.7360 train_time:1311447ms step_avg:170.27ms\nstep:7713/10000 train_loss:3.8641 train_time:1311616ms step_avg:170.27ms\nstep:7714/10000 train_loss:3.7009 train_time:1311786ms step_avg:170.27ms\nstep:7715/10000 train_loss:3.7795 train_time:1311958ms step_avg:170.27ms\nstep:7716/10000 train_loss:3.7844 train_time:1312126ms step_avg:170.27ms\nstep:7717/10000 train_loss:3.8186 train_time:1312298ms step_avg:170.27ms\nstep:7718/10000 train_loss:3.7521 train_time:1312468ms step_avg:170.27ms\nstep:7719/10000 train_loss:3.8169 train_time:1312641ms step_avg:170.27ms\nstep:7720/10000 train_loss:3.8429 train_time:1312812ms step_avg:170.27ms\nstep:7721/10000 train_loss:3.7688 train_time:1312986ms step_avg:170.27ms\nstep:7722/10000 train_loss:3.6618 train_time:1313156ms step_avg:170.27ms\nstep:7723/10000 train_loss:3.7464 train_time:1313325ms step_avg:170.27ms\nstep:7724/10000 train_loss:3.9055 train_time:1313496ms step_avg:170.27ms\nstep:7725/10000 train_loss:3.8112 train_time:1313667ms step_avg:170.27ms\nstep:7726/10000 train_loss:3.7370 train_time:1313840ms step_avg:170.27ms\nstep:7727/10000 train_loss:3.8458 train_time:1314008ms step_avg:170.27ms\nstep:7728/10000 train_loss:3.6914 train_time:1314181ms step_avg:170.27ms\nstep:7729/10000 train_loss:4.1108 train_time:1314349ms step_avg:170.27ms\nstep:7730/10000 train_loss:3.6723 train_time:1314519ms step_avg:170.27ms\nstep:7731/10000 train_loss:3.9962 train_time:1314687ms step_avg:170.27ms\nstep:7732/10000 train_loss:3.7961 train_time:1314859ms step_avg:170.27ms\nstep:7733/10000 train_loss:3.8650 train_time:1315027ms step_avg:170.27ms\nstep:7734/10000 train_loss:3.7252 train_time:1315197ms step_avg:170.27ms\nstep:7735/10000 train_loss:3.8093 train_time:1315365ms step_avg:170.27ms\nstep:7736/10000 train_loss:3.9416 train_time:1315534ms step_avg:170.27ms\nstep:7737/10000 train_loss:3.8635 train_time:1315707ms step_avg:170.27ms\nstep:7738/10000 train_loss:3.8545 train_time:1315879ms step_avg:170.27ms\nstep:7739/10000 train_loss:3.9905 train_time:1316048ms step_avg:170.27ms\nstep:7740/10000 train_loss:3.7869 train_time:1316219ms step_avg:170.27ms\nstep:7741/10000 train_loss:3.8279 train_time:1316388ms step_avg:170.27ms\nstep:7742/10000 train_loss:3.7381 train_time:1316556ms step_avg:170.27ms\nstep:7743/10000 train_loss:3.7128 train_time:1316725ms step_avg:170.27ms\nstep:7744/10000 train_loss:3.6384 train_time:1316896ms step_avg:170.27ms\nstep:7745/10000 train_loss:3.6884 train_time:1317064ms step_avg:170.27ms\nstep:7746/10000 train_loss:3.8632 train_time:1317236ms step_avg:170.27ms\nstep:7747/10000 train_loss:3.6411 train_time:1317406ms step_avg:170.27ms\nstep:7748/10000 train_loss:5.5906 train_time:1317575ms step_avg:170.27ms\nstep:7749/10000 train_loss:3.8816 train_time:1317747ms step_avg:170.27ms\nstep:7750/10000 train_loss:3.7910 train_time:1317919ms step_avg:170.27ms\nstep:7751/10000 train_loss:3.7180 train_time:1318088ms step_avg:170.27ms\nstep:7752/10000 train_loss:3.8475 train_time:1318259ms step_avg:170.27ms\nstep:7753/10000 train_loss:3.6714 train_time:1318429ms step_avg:170.27ms\nstep:7754/10000 train_loss:3.8009 train_time:1318596ms step_avg:170.27ms\nstep:7755/10000 train_loss:3.7778 train_time:1318765ms step_avg:170.27ms\nstep:7756/10000 train_loss:3.7968 train_time:1318937ms step_avg:170.27ms\nstep:7757/10000 train_loss:3.8263 train_time:1319106ms step_avg:170.27ms\nstep:7758/10000 train_loss:3.8536 train_time:1319278ms step_avg:170.27ms\nstep:7759/10000 train_loss:3.7955 train_time:1319450ms step_avg:170.27ms\nstep:7760/10000 train_loss:4.0518 train_time:1319621ms step_avg:170.27ms\nstep:7761/10000 train_loss:3.7599 train_time:1319789ms step_avg:170.27ms\nstep:7762/10000 train_loss:3.7882 train_time:1319960ms step_avg:170.27ms\nstep:7763/10000 train_loss:3.8478 train_time:1320129ms step_avg:170.27ms\nstep:7764/10000 train_loss:3.8177 train_time:1320300ms step_avg:170.27ms\nstep:7765/10000 train_loss:3.7653 train_time:1320470ms step_avg:170.27ms\nstep:7766/10000 train_loss:3.8114 train_time:1320642ms step_avg:170.27ms\nstep:7767/10000 train_loss:3.7451 train_time:1320813ms step_avg:170.27ms\nstep:7768/10000 train_loss:3.7959 train_time:1320984ms step_avg:170.27ms\nstep:7769/10000 train_loss:3.7560 train_time:1321153ms step_avg:170.27ms\nstep:7770/10000 train_loss:3.7510 train_time:1321324ms step_avg:170.27ms\nstep:7771/10000 train_loss:3.8187 train_time:1321494ms step_avg:170.27ms\nstep:7772/10000 train_loss:3.7949 train_time:1321664ms step_avg:170.27ms\nstep:7773/10000 train_loss:3.7238 train_time:1321835ms step_avg:170.27ms\nstep:7774/10000 train_loss:3.8441 train_time:1322007ms step_avg:170.27ms\nstep:7775/10000 train_loss:3.7816 train_time:1322178ms step_avg:170.27ms\nstep:7776/10000 train_loss:3.8352 train_time:1322346ms step_avg:170.27ms\nstep:7777/10000 train_loss:3.7200 train_time:1322518ms step_avg:170.27ms\nstep:7778/10000 train_loss:3.8713 train_time:1322688ms step_avg:170.27ms\nstep:7779/10000 train_loss:3.8468 train_time:1322859ms step_avg:170.27ms\nstep:7780/10000 train_loss:3.6975 train_time:1323028ms step_avg:170.27ms\nstep:7781/10000 train_loss:3.9051 train_time:1323197ms step_avg:170.27ms\nstep:7782/10000 train_loss:3.7032 train_time:1323367ms step_avg:170.27ms\nstep:7783/10000 train_loss:3.7794 train_time:1323539ms step_avg:170.27ms\nstep:7784/10000 train_loss:3.8425 train_time:1323707ms step_avg:170.27ms\nstep:7785/10000 train_loss:3.8394 train_time:1323878ms step_avg:170.27ms\nstep:7786/10000 train_loss:3.7285 train_time:1324048ms step_avg:170.27ms\nstep:7787/10000 train_loss:3.7460 train_time:1324219ms step_avg:170.27ms\nstep:7788/10000 train_loss:3.6261 train_time:1324389ms step_avg:170.27ms\nstep:7789/10000 train_loss:3.7598 train_time:1324558ms step_avg:170.27ms\nstep:7790/10000 train_loss:3.8201 train_time:1324727ms step_avg:170.27ms\nstep:7791/10000 train_loss:3.8160 train_time:1324900ms step_avg:170.27ms\nstep:7792/10000 train_loss:3.9755 train_time:1325069ms step_avg:170.27ms\nstep:7793/10000 train_loss:3.9110 train_time:1325238ms step_avg:170.27ms\nstep:7794/10000 train_loss:3.6576 train_time:1325406ms step_avg:170.27ms\nstep:7795/10000 train_loss:3.7250 train_time:1325575ms step_avg:170.27ms\nstep:7796/10000 train_loss:3.6399 train_time:1325744ms step_avg:170.27ms\nstep:7797/10000 train_loss:3.8352 train_time:1325914ms step_avg:170.27ms\nstep:7798/10000 train_loss:3.9072 train_time:1326084ms step_avg:170.27ms\nstep:7799/10000 train_loss:3.6635 train_time:1326255ms step_avg:170.27ms\nstep:7800/10000 train_loss:3.8522 train_time:1326426ms step_avg:170.27ms\nstep:7801/10000 train_loss:3.7912 train_time:1326597ms step_avg:170.27ms\nstep:7802/10000 train_loss:3.7948 train_time:1326768ms step_avg:170.27ms\nstep:7803/10000 train_loss:3.8007 train_time:1326939ms step_avg:170.27ms\nstep:7804/10000 train_loss:3.8093 train_time:1327108ms step_avg:170.27ms\nstep:7805/10000 train_loss:3.9127 train_time:1327278ms step_avg:170.27ms\nstep:7806/10000 train_loss:3.7903 train_time:1327450ms step_avg:170.27ms\nstep:7807/10000 train_loss:3.7057 train_time:1327620ms step_avg:170.27ms\nstep:7808/10000 train_loss:3.7287 train_time:1327790ms step_avg:170.27ms\nstep:7809/10000 train_loss:3.6998 train_time:1327960ms step_avg:170.27ms\nstep:7810/10000 train_loss:3.7959 train_time:1328128ms step_avg:170.27ms\nstep:7811/10000 train_loss:3.7895 train_time:1328299ms step_avg:170.27ms\nstep:7812/10000 train_loss:3.8917 train_time:1328468ms step_avg:170.27ms\nstep:7813/10000 train_loss:3.9118 train_time:1328642ms step_avg:170.27ms\nstep:7814/10000 train_loss:3.6666 train_time:1328810ms step_avg:170.27ms\nstep:7815/10000 train_loss:3.6660 train_time:1328981ms step_avg:170.27ms\nstep:7816/10000 train_loss:3.8173 train_time:1329150ms step_avg:170.27ms\nstep:7817/10000 train_loss:3.8175 train_time:1329317ms step_avg:170.27ms\nstep:7818/10000 train_loss:3.7846 train_time:1329487ms step_avg:170.27ms\nstep:7819/10000 train_loss:3.7898 train_time:1329658ms step_avg:170.27ms\nstep:7820/10000 train_loss:3.7481 train_time:1329829ms step_avg:170.27ms\nstep:7821/10000 train_loss:3.8302 train_time:1330002ms step_avg:170.27ms\nstep:7822/10000 train_loss:3.6243 train_time:1330172ms step_avg:170.27ms\nstep:7823/10000 train_loss:3.8012 train_time:1330344ms step_avg:170.27ms\nstep:7824/10000 train_loss:3.7999 train_time:1330516ms step_avg:170.27ms\nstep:7825/10000 train_loss:3.9012 train_time:1330684ms step_avg:170.27ms\nstep:7826/10000 train_loss:3.8274 train_time:1330853ms step_avg:170.27ms\nstep:7827/10000 train_loss:3.7366 train_time:1331024ms step_avg:170.27ms\nstep:7828/10000 train_loss:3.7203 train_time:1331195ms step_avg:170.27ms\nstep:7829/10000 train_loss:3.7941 train_time:1331366ms step_avg:170.27ms\nstep:7830/10000 train_loss:3.8347 train_time:1331536ms step_avg:170.27ms\nstep:7831/10000 train_loss:3.8736 train_time:1331704ms step_avg:170.27ms\nstep:7832/10000 train_loss:3.7096 train_time:1331874ms step_avg:170.27ms\nstep:7833/10000 train_loss:3.8247 train_time:1332046ms step_avg:170.27ms\nstep:7834/10000 train_loss:3.7800 train_time:1332216ms step_avg:170.27ms\nstep:7835/10000 train_loss:3.6959 train_time:1332387ms step_avg:170.27ms\nstep:7836/10000 train_loss:3.6929 train_time:1332558ms step_avg:170.27ms\nstep:7837/10000 train_loss:3.7113 train_time:1332728ms step_avg:170.27ms\nstep:7838/10000 train_loss:3.6784 train_time:1332899ms step_avg:170.27ms\nstep:7839/10000 train_loss:3.8805 train_time:1333070ms step_avg:170.27ms\nstep:7840/10000 train_loss:3.8247 train_time:1333241ms step_avg:170.27ms\nstep:7841/10000 train_loss:3.9320 train_time:1333412ms step_avg:170.27ms\nstep:7842/10000 train_loss:3.7481 train_time:1333582ms step_avg:170.27ms\nstep:7843/10000 train_loss:3.6920 train_time:1333751ms step_avg:170.27ms\nstep:7844/10000 train_loss:3.8582 train_time:1333922ms step_avg:170.27ms\nstep:7845/10000 train_loss:3.9196 train_time:1334092ms step_avg:170.27ms\nstep:7846/10000 train_loss:3.9421 train_time:1334262ms step_avg:170.27ms\nstep:7847/10000 train_loss:3.8356 train_time:1334432ms step_avg:170.27ms\nstep:7848/10000 train_loss:3.6334 train_time:1334602ms step_avg:170.27ms\nstep:7849/10000 train_loss:3.7749 train_time:1334774ms step_avg:170.27ms\nstep:7850/10000 train_loss:3.8573 train_time:1334944ms step_avg:170.27ms\nstep:7851/10000 train_loss:3.7978 train_time:1335115ms step_avg:170.27ms\nstep:7852/10000 train_loss:3.8492 train_time:1335286ms step_avg:170.27ms\nstep:7853/10000 train_loss:3.7519 train_time:1335458ms step_avg:170.27ms\nstep:7854/10000 train_loss:3.8745 train_time:1335626ms step_avg:170.27ms\nstep:7855/10000 train_loss:3.8149 train_time:1335799ms step_avg:170.27ms\nstep:7856/10000 train_loss:3.8129 train_time:1335967ms step_avg:170.27ms\nstep:7857/10000 train_loss:3.9837 train_time:1336139ms step_avg:170.27ms\nstep:7858/10000 train_loss:3.7005 train_time:1336308ms step_avg:170.27ms\nstep:7859/10000 train_loss:3.9551 train_time:1336481ms step_avg:170.27ms\nstep:7860/10000 train_loss:3.6448 train_time:1336648ms step_avg:170.27ms\nstep:7861/10000 train_loss:3.8066 train_time:1336820ms step_avg:170.27ms\nstep:7862/10000 train_loss:3.8050 train_time:1336990ms step_avg:170.27ms\nstep:7863/10000 train_loss:3.8423 train_time:1337160ms step_avg:170.27ms\nstep:7864/10000 train_loss:3.9658 train_time:1337330ms step_avg:170.27ms\nstep:7865/10000 train_loss:3.8305 train_time:1337498ms step_avg:170.27ms\nstep:7866/10000 train_loss:3.8385 train_time:1337668ms step_avg:170.27ms\nstep:7867/10000 train_loss:3.7736 train_time:1337838ms step_avg:170.27ms\nstep:7868/10000 train_loss:3.8131 train_time:1338010ms step_avg:170.27ms\nstep:7869/10000 train_loss:3.7263 train_time:1338182ms step_avg:170.27ms\nstep:7870/10000 train_loss:3.8109 train_time:1338349ms step_avg:170.27ms\nstep:7871/10000 train_loss:3.7810 train_time:1338519ms step_avg:170.27ms\nstep:7872/10000 train_loss:3.7150 train_time:1338687ms step_avg:170.27ms\nstep:7873/10000 train_loss:3.8688 train_time:1338859ms step_avg:170.27ms\nstep:7874/10000 train_loss:3.6928 train_time:1339030ms step_avg:170.27ms\nstep:7875/10000 train_loss:3.7886 train_time:1339202ms step_avg:170.27ms\nstep:7876/10000 train_loss:3.7435 train_time:1339368ms step_avg:170.27ms\nstep:7877/10000 train_loss:3.8396 train_time:1339539ms step_avg:170.27ms\nstep:7878/10000 train_loss:4.0926 train_time:1339708ms step_avg:170.27ms\nstep:7879/10000 train_loss:3.7573 train_time:1339878ms step_avg:170.27ms\nstep:7880/10000 train_loss:3.8506 train_time:1340045ms step_avg:170.27ms\nstep:7881/10000 train_loss:3.7731 train_time:1340218ms step_avg:170.27ms\nstep:7882/10000 train_loss:3.7610 train_time:1340387ms step_avg:170.27ms\nstep:7883/10000 train_loss:3.7876 train_time:1340557ms step_avg:170.27ms\nstep:7884/10000 train_loss:3.9658 train_time:1340729ms step_avg:170.27ms\nstep:7885/10000 train_loss:3.9152 train_time:1340902ms step_avg:170.27ms\nstep:7886/10000 train_loss:3.7495 train_time:1341072ms step_avg:170.27ms\nstep:7887/10000 train_loss:3.8123 train_time:1341242ms step_avg:170.27ms\nstep:7888/10000 train_loss:3.6051 train_time:1341412ms step_avg:170.27ms\nstep:7889/10000 train_loss:3.8190 train_time:1341581ms step_avg:170.27ms\nstep:7890/10000 train_loss:3.8130 train_time:1341750ms step_avg:170.27ms\nstep:7891/10000 train_loss:3.7814 train_time:1341925ms step_avg:170.27ms\nstep:7892/10000 train_loss:3.7875 train_time:1342095ms step_avg:170.27ms\nstep:7893/10000 train_loss:3.8270 train_time:1342265ms step_avg:170.27ms\nstep:7894/10000 train_loss:3.7777 train_time:1342434ms step_avg:170.27ms\nstep:7895/10000 train_loss:3.8619 train_time:1342604ms step_avg:170.27ms\nstep:7896/10000 train_loss:3.8677 train_time:1342774ms step_avg:170.27ms\nstep:7897/10000 train_loss:3.6053 train_time:1342943ms step_avg:170.27ms\nstep:7898/10000 train_loss:3.6831 train_time:1343114ms step_avg:170.27ms\nstep:7899/10000 train_loss:3.8794 train_time:1343287ms step_avg:170.27ms\nstep:7900/10000 train_loss:3.7534 train_time:1343456ms step_avg:170.27ms\nstep:7901/10000 train_loss:3.7085 train_time:1343625ms step_avg:170.27ms\nstep:7902/10000 train_loss:3.6518 train_time:1343798ms step_avg:170.27ms\nstep:7903/10000 train_loss:3.8263 train_time:1343969ms step_avg:170.27ms\nstep:7904/10000 train_loss:3.9642 train_time:1344140ms step_avg:170.27ms\nstep:7905/10000 train_loss:3.8945 train_time:1344309ms step_avg:170.27ms\nstep:7906/10000 train_loss:3.7599 train_time:1344479ms step_avg:170.27ms\nstep:7907/10000 train_loss:3.6818 train_time:1344649ms step_avg:170.27ms\nstep:7908/10000 train_loss:3.7959 train_time:1344821ms step_avg:170.27ms\nstep:7909/10000 train_loss:3.8269 train_time:1344990ms step_avg:170.27ms\nstep:7910/10000 train_loss:3.7684 train_time:1345161ms step_avg:170.27ms\nstep:7911/10000 train_loss:3.5677 train_time:1345331ms step_avg:170.27ms\nstep:7912/10000 train_loss:3.8601 train_time:1345501ms step_avg:170.27ms\nstep:7913/10000 train_loss:3.6309 train_time:1345670ms step_avg:170.27ms\nstep:7914/10000 train_loss:3.6814 train_time:1345845ms step_avg:170.27ms\nstep:7915/10000 train_loss:3.7689 train_time:1346015ms step_avg:170.27ms\nstep:7916/10000 train_loss:3.7934 train_time:1346187ms step_avg:170.27ms\nstep:7917/10000 train_loss:3.7781 train_time:1346359ms step_avg:170.27ms\nstep:7918/10000 train_loss:3.8455 train_time:1346526ms step_avg:170.27ms\nstep:7919/10000 train_loss:3.8368 train_time:1346697ms step_avg:170.27ms\nstep:7920/10000 train_loss:3.7684 train_time:1346863ms step_avg:170.27ms\nstep:7921/10000 train_loss:3.8180 train_time:1347034ms step_avg:170.27ms\nstep:7922/10000 train_loss:3.7240 train_time:1347204ms step_avg:170.27ms\nstep:7923/10000 train_loss:3.6533 train_time:1347374ms step_avg:170.27ms\nstep:7924/10000 train_loss:3.8541 train_time:1347548ms step_avg:170.27ms\nstep:7925/10000 train_loss:3.7678 train_time:1347719ms step_avg:170.27ms\nstep:7926/10000 train_loss:4.0935 train_time:1347886ms step_avg:170.27ms\nstep:7927/10000 train_loss:3.8701 train_time:1348057ms step_avg:170.27ms\nstep:7928/10000 train_loss:3.8445 train_time:1348229ms step_avg:170.27ms\nstep:7929/10000 train_loss:3.7682 train_time:1348400ms step_avg:170.27ms\nstep:7930/10000 train_loss:3.8070 train_time:1348569ms step_avg:170.27ms\nstep:7931/10000 train_loss:3.7813 train_time:1348743ms step_avg:170.27ms\nstep:7932/10000 train_loss:3.8709 train_time:1348914ms step_avg:170.27ms\nstep:7933/10000 train_loss:3.7274 train_time:1349087ms step_avg:170.27ms\nstep:7934/10000 train_loss:3.7876 train_time:1349258ms step_avg:170.27ms\nstep:7935/10000 train_loss:3.7924 train_time:1349427ms step_avg:170.27ms\nstep:7936/10000 train_loss:3.7631 train_time:1349598ms step_avg:170.27ms\nstep:7937/10000 train_loss:3.8027 train_time:1349768ms step_avg:170.27ms\nstep:7938/10000 train_loss:3.7698 train_time:1349938ms step_avg:170.27ms\nstep:7939/10000 train_loss:3.8284 train_time:1350109ms step_avg:170.27ms\nstep:7940/10000 train_loss:3.7162 train_time:1350281ms step_avg:170.28ms\nstep:7941/10000 train_loss:3.8279 train_time:1350449ms step_avg:170.27ms\nstep:7942/10000 train_loss:3.8158 train_time:1350620ms step_avg:170.27ms\nstep:7943/10000 train_loss:3.7620 train_time:1350790ms step_avg:170.27ms\nstep:7944/10000 train_loss:3.8976 train_time:1350960ms step_avg:170.27ms\nstep:7945/10000 train_loss:3.7766 train_time:1351131ms step_avg:170.27ms\nstep:7946/10000 train_loss:3.7622 train_time:1351305ms step_avg:170.28ms\nstep:7947/10000 train_loss:3.8236 train_time:1351476ms step_avg:170.28ms\nstep:7948/10000 train_loss:3.8962 train_time:1351646ms step_avg:170.28ms\nstep:7949/10000 train_loss:3.9109 train_time:1351822ms step_avg:170.28ms\nstep:7950/10000 train_loss:3.7393 train_time:1351987ms step_avg:170.28ms\nstep:7951/10000 train_loss:3.8392 train_time:1352157ms step_avg:170.28ms\nstep:7952/10000 train_loss:4.0453 train_time:1352325ms step_avg:170.28ms\nstep:7953/10000 train_loss:3.7589 train_time:1352495ms step_avg:170.28ms\nstep:7954/10000 train_loss:3.7536 train_time:1352666ms step_avg:170.28ms\nstep:7955/10000 train_loss:3.5350 train_time:1352836ms step_avg:170.28ms\nstep:7956/10000 train_loss:3.8568 train_time:1353006ms step_avg:170.28ms\nstep:7957/10000 train_loss:3.7531 train_time:1353177ms step_avg:170.28ms\nstep:7958/10000 train_loss:3.8186 train_time:1353346ms step_avg:170.28ms\nstep:7959/10000 train_loss:3.8876 train_time:1353515ms step_avg:170.27ms\nstep:7960/10000 train_loss:3.7479 train_time:1353683ms step_avg:170.27ms\nstep:7961/10000 train_loss:3.7844 train_time:1353854ms step_avg:170.27ms\nstep:7962/10000 train_loss:3.8232 train_time:1354024ms step_avg:170.27ms\nstep:7963/10000 train_loss:3.7330 train_time:1354193ms step_avg:170.27ms\nstep:7964/10000 train_loss:3.8464 train_time:1354364ms step_avg:170.27ms\nstep:7965/10000 train_loss:3.8154 train_time:1354533ms step_avg:170.27ms\nstep:7966/10000 train_loss:3.7639 train_time:1354710ms step_avg:170.28ms\nstep:7967/10000 train_loss:3.7110 train_time:1354873ms step_avg:170.27ms\nstep:7968/10000 train_loss:3.7592 train_time:1355045ms step_avg:170.27ms\nstep:7969/10000 train_loss:3.8444 train_time:1355217ms step_avg:170.27ms\nstep:7970/10000 train_loss:3.8543 train_time:1355387ms step_avg:170.27ms\nstep:7971/10000 train_loss:3.7575 train_time:1355558ms step_avg:170.27ms\nstep:7972/10000 train_loss:3.8825 train_time:1355728ms step_avg:170.27ms\nstep:7973/10000 train_loss:3.8574 train_time:1355900ms step_avg:170.27ms\nstep:7974/10000 train_loss:3.8812 train_time:1356070ms step_avg:170.27ms\nstep:7975/10000 train_loss:3.6810 train_time:1356242ms step_avg:170.28ms\nstep:7976/10000 train_loss:3.8177 train_time:1356410ms step_avg:170.27ms\nstep:7977/10000 train_loss:3.6117 train_time:1356579ms step_avg:170.27ms\nstep:7978/10000 train_loss:3.6331 train_time:1356749ms step_avg:170.27ms\nstep:7979/10000 train_loss:3.8113 train_time:1356919ms step_avg:170.27ms\nstep:7980/10000 train_loss:3.6570 train_time:1357088ms step_avg:170.27ms\nstep:7981/10000 train_loss:4.0022 train_time:1357257ms step_avg:170.27ms\nstep:7982/10000 train_loss:3.6777 train_time:1357426ms step_avg:170.27ms\nstep:7983/10000 train_loss:3.8650 train_time:1357597ms step_avg:170.27ms\nstep:7984/10000 train_loss:3.8313 train_time:1357765ms step_avg:170.27ms\nstep:7985/10000 train_loss:3.8174 train_time:1357935ms step_avg:170.27ms\nstep:7986/10000 train_loss:3.8616 train_time:1358106ms step_avg:170.27ms\nstep:7987/10000 train_loss:3.8181 train_time:1358275ms step_avg:170.27ms\nstep:7988/10000 train_loss:3.8165 train_time:1358443ms step_avg:170.27ms\nstep:7989/10000 train_loss:3.8285 train_time:1358613ms step_avg:170.27ms\nstep:7990/10000 train_loss:3.6591 train_time:1358784ms step_avg:170.27ms\nstep:7991/10000 train_loss:3.8776 train_time:1358955ms step_avg:170.27ms\nstep:7992/10000 train_loss:3.8207 train_time:1359126ms step_avg:170.27ms\nstep:7993/10000 train_loss:3.8845 train_time:1359295ms step_avg:170.27ms\nstep:7994/10000 train_loss:3.7862 train_time:1359464ms step_avg:170.27ms\nstep:7995/10000 train_loss:3.7247 train_time:1359635ms step_avg:170.27ms\nstep:7996/10000 train_loss:4.4124 train_time:1359803ms step_avg:170.27ms\nstep:7997/10000 train_loss:3.7470 train_time:1359971ms step_avg:170.27ms\nstep:7998/10000 train_loss:3.8387 train_time:1360141ms step_avg:170.27ms\nstep:7999/10000 train_loss:3.9056 train_time:1360311ms step_avg:170.27ms\nstep:8000/10000 train_loss:3.7720 train_time:1360482ms step_avg:170.27ms\nstep:8001/10000 train_loss:3.6868 train_time:1360652ms step_avg:170.27ms\nstep:8002/10000 train_loss:3.6872 train_time:1360826ms step_avg:170.27ms\nstep:8003/10000 train_loss:3.9457 train_time:1360996ms step_avg:170.27ms\nstep:8004/10000 train_loss:3.8142 train_time:1361165ms step_avg:170.27ms\nstep:8005/10000 train_loss:4.0550 train_time:1361335ms step_avg:170.27ms\nstep:8006/10000 train_loss:3.7989 train_time:1361505ms step_avg:170.27ms\nstep:8007/10000 train_loss:3.7605 train_time:1361674ms step_avg:170.27ms\nstep:8008/10000 train_loss:3.7588 train_time:1361843ms step_avg:170.27ms\nstep:8009/10000 train_loss:4.0862 train_time:1362016ms step_avg:170.27ms\nstep:8010/10000 train_loss:3.8341 train_time:1362186ms step_avg:170.27ms\nstep:8011/10000 train_loss:3.7591 train_time:1362355ms step_avg:170.27ms\nstep:8012/10000 train_loss:3.8629 train_time:1362526ms step_avg:170.27ms\nstep:8013/10000 train_loss:3.7689 train_time:1362696ms step_avg:170.27ms\nstep:8014/10000 train_loss:3.7299 train_time:1362866ms step_avg:170.27ms\nstep:8015/10000 train_loss:4.2546 train_time:1363036ms step_avg:170.27ms\nstep:8016/10000 train_loss:3.8121 train_time:1363205ms step_avg:170.27ms\nstep:8017/10000 train_loss:3.7859 train_time:1363375ms step_avg:170.27ms\nstep:8018/10000 train_loss:3.8581 train_time:1363547ms step_avg:170.27ms\nstep:8019/10000 train_loss:3.5695 train_time:1363718ms step_avg:170.27ms\nstep:8020/10000 train_loss:3.7496 train_time:1363885ms step_avg:170.27ms\nstep:8021/10000 train_loss:3.6138 train_time:1364054ms step_avg:170.27ms\nstep:8022/10000 train_loss:3.8348 train_time:1364224ms step_avg:170.27ms\nstep:8023/10000 train_loss:3.8242 train_time:1364392ms step_avg:170.27ms\nstep:8024/10000 train_loss:3.7839 train_time:1364563ms step_avg:170.27ms\nstep:8025/10000 train_loss:3.8670 train_time:1364733ms step_avg:170.27ms\nstep:8026/10000 train_loss:3.8950 train_time:1364904ms step_avg:170.27ms\nstep:8027/10000 train_loss:3.8118 train_time:1365075ms step_avg:170.27ms\nstep:8028/10000 train_loss:3.6340 train_time:1365245ms step_avg:170.27ms\nstep:8029/10000 train_loss:3.9089 train_time:1365415ms step_avg:170.27ms\nstep:8030/10000 train_loss:3.8148 train_time:1365584ms step_avg:170.27ms\nstep:8031/10000 train_loss:3.7616 train_time:1365754ms step_avg:170.27ms\nstep:8032/10000 train_loss:3.7941 train_time:1365925ms step_avg:170.27ms\nstep:8033/10000 train_loss:3.8173 train_time:1366097ms step_avg:170.27ms\nstep:8034/10000 train_loss:3.8675 train_time:1366266ms step_avg:170.27ms\nstep:8035/10000 train_loss:4.7298 train_time:1366435ms step_avg:170.27ms\nstep:8036/10000 train_loss:3.8108 train_time:1366605ms step_avg:170.27ms\nstep:8037/10000 train_loss:3.8466 train_time:1366774ms step_avg:170.27ms\nstep:8038/10000 train_loss:3.7178 train_time:1366943ms step_avg:170.27ms\nstep:8039/10000 train_loss:3.7018 train_time:1367114ms step_avg:170.27ms\nstep:8040/10000 train_loss:3.7701 train_time:1367284ms step_avg:170.27ms\nstep:8041/10000 train_loss:3.8149 train_time:1367456ms step_avg:170.27ms\nstep:8042/10000 train_loss:3.7546 train_time:1367624ms step_avg:170.27ms\nstep:8043/10000 train_loss:3.8109 train_time:1367794ms step_avg:170.27ms\nstep:8044/10000 train_loss:3.7141 train_time:1367962ms step_avg:170.27ms\nstep:8045/10000 train_loss:3.7744 train_time:1368133ms step_avg:170.27ms\nstep:8046/10000 train_loss:3.7614 train_time:1368302ms step_avg:170.27ms\nstep:8047/10000 train_loss:3.8336 train_time:1368471ms step_avg:170.27ms\nstep:8048/10000 train_loss:3.7539 train_time:1368643ms step_avg:170.27ms\nstep:8049/10000 train_loss:3.8344 train_time:1368813ms step_avg:170.27ms\nstep:8050/10000 train_loss:3.8188 train_time:1368984ms step_avg:170.27ms\nstep:8051/10000 train_loss:3.6765 train_time:1369157ms step_avg:170.27ms\nstep:8052/10000 train_loss:3.6342 train_time:1369326ms step_avg:170.27ms\nstep:8053/10000 train_loss:3.7328 train_time:1369495ms step_avg:170.27ms\nstep:8054/10000 train_loss:3.7238 train_time:1369662ms step_avg:170.27ms\nstep:8055/10000 train_loss:3.8124 train_time:1369833ms step_avg:170.27ms\nstep:8056/10000 train_loss:3.7061 train_time:1370007ms step_avg:170.27ms\nstep:8057/10000 train_loss:3.9809 train_time:1370172ms step_avg:170.27ms\nstep:8058/10000 train_loss:3.8149 train_time:1370344ms step_avg:170.27ms\nstep:8059/10000 train_loss:3.9074 train_time:1370513ms step_avg:170.27ms\nstep:8060/10000 train_loss:3.7048 train_time:1370687ms step_avg:170.27ms\nstep:8061/10000 train_loss:3.7401 train_time:1370856ms step_avg:170.27ms\nstep:8062/10000 train_loss:3.7741 train_time:1371024ms step_avg:170.27ms\nstep:8063/10000 train_loss:3.9454 train_time:1371193ms step_avg:170.27ms\nstep:8064/10000 train_loss:3.7601 train_time:1371364ms step_avg:170.27ms\nstep:8065/10000 train_loss:3.8444 train_time:1371533ms step_avg:170.27ms\nstep:8066/10000 train_loss:3.7979 train_time:1371708ms step_avg:170.27ms\nstep:8067/10000 train_loss:3.7309 train_time:1371879ms step_avg:170.27ms\nstep:8068/10000 train_loss:3.7643 train_time:1372049ms step_avg:170.27ms\nstep:8069/10000 train_loss:3.9650 train_time:1372217ms step_avg:170.27ms\nstep:8070/10000 train_loss:3.7234 train_time:1372387ms step_avg:170.27ms\nstep:8071/10000 train_loss:3.8874 train_time:1372556ms step_avg:170.27ms\nstep:8072/10000 train_loss:3.7334 train_time:1372727ms step_avg:170.27ms\nstep:8073/10000 train_loss:3.8520 train_time:1372899ms step_avg:170.27ms\nstep:8074/10000 train_loss:3.7767 train_time:1373070ms step_avg:170.27ms\nstep:8075/10000 train_loss:3.6965 train_time:1373240ms step_avg:170.27ms\nstep:8076/10000 train_loss:3.7095 train_time:1373410ms step_avg:170.27ms\nstep:8077/10000 train_loss:3.9435 train_time:1373581ms step_avg:170.27ms\nstep:8078/10000 train_loss:3.7617 train_time:1373750ms step_avg:170.27ms\nstep:8079/10000 train_loss:3.7663 train_time:1373924ms step_avg:170.27ms\nstep:8080/10000 train_loss:3.6395 train_time:1374095ms step_avg:170.27ms\nstep:8081/10000 train_loss:3.7691 train_time:1374267ms step_avg:170.27ms\nstep:8082/10000 train_loss:3.9168 train_time:1374440ms step_avg:170.27ms\nstep:8083/10000 train_loss:3.8240 train_time:1374612ms step_avg:170.27ms\nstep:8084/10000 train_loss:3.9287 train_time:1374782ms step_avg:170.27ms\nstep:8085/10000 train_loss:3.8183 train_time:1374951ms step_avg:170.27ms\nstep:8086/10000 train_loss:3.7388 train_time:1375121ms step_avg:170.27ms\nstep:8087/10000 train_loss:3.8170 train_time:1375292ms step_avg:170.27ms\nstep:8088/10000 train_loss:3.8331 train_time:1375465ms step_avg:170.27ms\nstep:8089/10000 train_loss:3.6944 train_time:1375637ms step_avg:170.27ms\nstep:8090/10000 train_loss:3.9043 train_time:1375808ms step_avg:170.27ms\nstep:8091/10000 train_loss:3.8294 train_time:1375977ms step_avg:170.27ms\nstep:8092/10000 train_loss:3.7771 train_time:1376148ms step_avg:170.27ms\nstep:8093/10000 train_loss:3.7925 train_time:1376316ms step_avg:170.27ms\nstep:8094/10000 train_loss:3.8999 train_time:1376486ms step_avg:170.27ms\nstep:8095/10000 train_loss:3.7582 train_time:1376658ms step_avg:170.27ms\nstep:8096/10000 train_loss:3.7880 train_time:1376827ms step_avg:170.27ms\nstep:8097/10000 train_loss:3.9058 train_time:1376998ms step_avg:170.27ms\nstep:8098/10000 train_loss:3.6791 train_time:1377169ms step_avg:170.27ms\nstep:8099/10000 train_loss:3.7722 train_time:1377339ms step_avg:170.27ms\nstep:8100/10000 train_loss:3.7531 train_time:1377510ms step_avg:170.27ms\nstep:8101/10000 train_loss:3.7792 train_time:1377682ms step_avg:170.27ms\nstep:8102/10000 train_loss:3.8725 train_time:1377852ms step_avg:170.27ms\nstep:8103/10000 train_loss:3.7961 train_time:1378025ms step_avg:170.27ms\nstep:8104/10000 train_loss:3.7129 train_time:1378197ms step_avg:170.27ms\nstep:8105/10000 train_loss:3.7435 train_time:1378369ms step_avg:170.27ms\nstep:8106/10000 train_loss:3.7781 train_time:1378537ms step_avg:170.27ms\nstep:8107/10000 train_loss:3.8410 train_time:1378707ms step_avg:170.27ms\nstep:8108/10000 train_loss:3.7872 train_time:1378879ms step_avg:170.27ms\nstep:8109/10000 train_loss:3.6769 train_time:1379049ms step_avg:170.27ms\nstep:8110/10000 train_loss:3.8677 train_time:1379221ms step_avg:170.27ms\nstep:8111/10000 train_loss:3.7899 train_time:1379389ms step_avg:170.27ms\nstep:8112/10000 train_loss:3.8577 train_time:1379563ms step_avg:170.27ms\nstep:8113/10000 train_loss:3.6364 train_time:1379732ms step_avg:170.27ms\nstep:8114/10000 train_loss:3.9281 train_time:1379903ms step_avg:170.27ms\nstep:8115/10000 train_loss:3.8058 train_time:1380074ms step_avg:170.27ms\nstep:8116/10000 train_loss:3.6757 train_time:1380246ms step_avg:170.27ms\nstep:8117/10000 train_loss:3.9067 train_time:1380416ms step_avg:170.27ms\nstep:8118/10000 train_loss:3.7088 train_time:1380588ms step_avg:170.27ms\nstep:8119/10000 train_loss:3.7155 train_time:1380758ms step_avg:170.27ms\nstep:8120/10000 train_loss:3.7360 train_time:1380927ms step_avg:170.27ms\nstep:8121/10000 train_loss:3.8854 train_time:1381099ms step_avg:170.27ms\nstep:8122/10000 train_loss:3.8031 train_time:1381267ms step_avg:170.27ms\nstep:8123/10000 train_loss:3.8468 train_time:1381437ms step_avg:170.27ms\nstep:8124/10000 train_loss:3.8230 train_time:1381605ms step_avg:170.27ms\nstep:8125/10000 train_loss:3.7936 train_time:1381777ms step_avg:170.27ms\nstep:8126/10000 train_loss:3.6427 train_time:1381947ms step_avg:170.27ms\nstep:8127/10000 train_loss:3.8053 train_time:1382118ms step_avg:170.27ms\nstep:8128/10000 train_loss:3.8621 train_time:1382288ms step_avg:170.27ms\nstep:8129/10000 train_loss:3.7556 train_time:1382461ms step_avg:170.27ms\nstep:8130/10000 train_loss:3.7661 train_time:1382629ms step_avg:170.27ms\nstep:8131/10000 train_loss:3.7369 train_time:1382801ms step_avg:170.27ms\nstep:8132/10000 train_loss:3.5417 train_time:1382971ms step_avg:170.27ms\nstep:8133/10000 train_loss:3.7655 train_time:1383145ms step_avg:170.28ms\nstep:8134/10000 train_loss:3.6890 train_time:1383314ms step_avg:170.28ms\nstep:8135/10000 train_loss:3.8351 train_time:1383485ms step_avg:170.28ms\nstep:8136/10000 train_loss:3.7798 train_time:1383658ms step_avg:170.28ms\nstep:8137/10000 train_loss:3.7843 train_time:1383829ms step_avg:170.28ms\nstep:8138/10000 train_loss:3.7650 train_time:1384001ms step_avg:170.28ms\nstep:8139/10000 train_loss:3.6329 train_time:1384172ms step_avg:170.28ms\nstep:8140/10000 train_loss:3.7177 train_time:1384342ms step_avg:170.28ms\nstep:8141/10000 train_loss:3.8494 train_time:1384511ms step_avg:170.28ms\nstep:8142/10000 train_loss:3.7744 train_time:1384683ms step_avg:170.28ms\nstep:8143/10000 train_loss:3.7874 train_time:1384853ms step_avg:170.28ms\nstep:8144/10000 train_loss:3.8155 train_time:1385028ms step_avg:170.28ms\nstep:8145/10000 train_loss:3.7587 train_time:1385200ms step_avg:170.28ms\nstep:8146/10000 train_loss:3.7837 train_time:1385369ms step_avg:170.28ms\nstep:8147/10000 train_loss:3.8285 train_time:1385540ms step_avg:170.28ms\nstep:8148/10000 train_loss:3.7572 train_time:1385708ms step_avg:170.28ms\nstep:8149/10000 train_loss:3.8038 train_time:1385878ms step_avg:170.28ms\nstep:8150/10000 train_loss:3.6845 train_time:1386047ms step_avg:170.28ms\nstep:8151/10000 train_loss:3.7724 train_time:1386219ms step_avg:170.28ms\nstep:8152/10000 train_loss:3.7935 train_time:1386387ms step_avg:170.28ms\nstep:8153/10000 train_loss:3.8448 train_time:1386557ms step_avg:170.28ms\nstep:8154/10000 train_loss:3.8355 train_time:1386726ms step_avg:170.28ms\nstep:8155/10000 train_loss:3.8455 train_time:1386898ms step_avg:170.28ms\nstep:8156/10000 train_loss:3.8102 train_time:1387066ms step_avg:170.28ms\nstep:8157/10000 train_loss:3.8166 train_time:1387237ms step_avg:170.28ms\nstep:8158/10000 train_loss:3.8252 train_time:1387406ms step_avg:170.28ms\nstep:8159/10000 train_loss:3.5083 train_time:1387577ms step_avg:170.28ms\nstep:8160/10000 train_loss:3.8369 train_time:1387746ms step_avg:170.28ms\nstep:8161/10000 train_loss:3.9569 train_time:1387915ms step_avg:170.28ms\nstep:8162/10000 train_loss:3.8275 train_time:1388088ms step_avg:170.28ms\nstep:8163/10000 train_loss:3.6864 train_time:1388261ms step_avg:170.28ms\nstep:8164/10000 train_loss:4.0552 train_time:1388430ms step_avg:170.28ms\nstep:8165/10000 train_loss:3.6865 train_time:1388600ms step_avg:170.28ms\nstep:8166/10000 train_loss:3.6771 train_time:1388770ms step_avg:170.28ms\nstep:8167/10000 train_loss:3.7087 train_time:1388940ms step_avg:170.28ms\nstep:8168/10000 train_loss:3.8061 train_time:1389110ms step_avg:170.28ms\nstep:8169/10000 train_loss:4.0299 train_time:1389282ms step_avg:170.28ms\nstep:8170/10000 train_loss:3.9444 train_time:1389452ms step_avg:170.28ms\nstep:8171/10000 train_loss:3.7208 train_time:1389623ms step_avg:170.28ms\nstep:8172/10000 train_loss:3.7488 train_time:1389794ms step_avg:170.28ms\nstep:8173/10000 train_loss:3.7175 train_time:1389966ms step_avg:170.28ms\nstep:8174/10000 train_loss:3.7789 train_time:1390137ms step_avg:170.28ms\nstep:8175/10000 train_loss:3.7483 train_time:1390305ms step_avg:170.28ms\nstep:8176/10000 train_loss:3.7006 train_time:1390474ms step_avg:170.28ms\nstep:8177/10000 train_loss:3.7741 train_time:1390643ms step_avg:170.28ms\nstep:8178/10000 train_loss:3.8067 train_time:1390810ms step_avg:170.28ms\nstep:8179/10000 train_loss:3.7938 train_time:1390978ms step_avg:170.28ms\nstep:8180/10000 train_loss:3.7717 train_time:1391149ms step_avg:170.28ms\nstep:8181/10000 train_loss:3.8328 train_time:1391320ms step_avg:170.28ms\nstep:8182/10000 train_loss:3.7508 train_time:1391488ms step_avg:170.28ms\nstep:8183/10000 train_loss:3.8966 train_time:1391661ms step_avg:170.28ms\nstep:8184/10000 train_loss:3.6997 train_time:1391832ms step_avg:170.28ms\nstep:8185/10000 train_loss:3.5520 train_time:1392000ms step_avg:170.28ms\nstep:8186/10000 train_loss:3.6400 train_time:1392172ms step_avg:170.28ms\nstep:8187/10000 train_loss:3.8247 train_time:1392345ms step_avg:170.28ms\nstep:8188/10000 train_loss:3.6805 train_time:1392514ms step_avg:170.28ms\nstep:8189/10000 train_loss:3.9414 train_time:1392682ms step_avg:170.28ms\nstep:8190/10000 train_loss:3.7908 train_time:1392852ms step_avg:170.28ms\nstep:8191/10000 train_loss:3.7850 train_time:1393022ms step_avg:170.28ms\nstep:8192/10000 train_loss:3.6995 train_time:1393193ms step_avg:170.28ms\nstep:8193/10000 train_loss:3.7832 train_time:1393363ms step_avg:170.28ms\nstep:8194/10000 train_loss:3.8074 train_time:1393532ms step_avg:170.28ms\nstep:8195/10000 train_loss:3.8147 train_time:1393702ms step_avg:170.28ms\nstep:8196/10000 train_loss:3.7371 train_time:1393875ms step_avg:170.28ms\nstep:8197/10000 train_loss:3.8435 train_time:1394046ms step_avg:170.28ms\nstep:8198/10000 train_loss:3.6511 train_time:1394216ms step_avg:170.28ms\nstep:8199/10000 train_loss:3.7480 train_time:1394386ms step_avg:170.28ms\nstep:8200/10000 train_loss:3.8106 train_time:1394557ms step_avg:170.28ms\nstep:8201/10000 train_loss:3.7100 train_time:1394725ms step_avg:170.28ms\nstep:8202/10000 train_loss:3.7396 train_time:1394897ms step_avg:170.28ms\nstep:8203/10000 train_loss:3.9377 train_time:1395068ms step_avg:170.28ms\nstep:8204/10000 train_loss:3.7636 train_time:1395239ms step_avg:170.28ms\nstep:8205/10000 train_loss:3.8228 train_time:1395412ms step_avg:170.28ms\nstep:8206/10000 train_loss:3.7805 train_time:1395578ms step_avg:170.28ms\nstep:8207/10000 train_loss:3.8141 train_time:1395748ms step_avg:170.28ms\nstep:8208/10000 train_loss:3.8069 train_time:1395918ms step_avg:170.28ms\nstep:8209/10000 train_loss:3.7564 train_time:1396087ms step_avg:170.28ms\nstep:8210/10000 train_loss:3.7384 train_time:1396258ms step_avg:170.28ms\nstep:8211/10000 train_loss:3.7687 train_time:1396429ms step_avg:170.28ms\nstep:8212/10000 train_loss:3.7060 train_time:1396600ms step_avg:170.28ms\nstep:8213/10000 train_loss:3.7393 train_time:1396770ms step_avg:170.28ms\nstep:8214/10000 train_loss:3.7639 train_time:1396940ms step_avg:170.28ms\nstep:8215/10000 train_loss:3.7155 train_time:1397110ms step_avg:170.28ms\nstep:8216/10000 train_loss:3.8904 train_time:1397282ms step_avg:170.28ms\nstep:8217/10000 train_loss:3.8157 train_time:1397454ms step_avg:170.28ms\nstep:8218/10000 train_loss:3.9269 train_time:1397625ms step_avg:170.28ms\nstep:8219/10000 train_loss:3.7623 train_time:1397797ms step_avg:170.28ms\nstep:8220/10000 train_loss:3.7685 train_time:1397966ms step_avg:170.28ms\nstep:8221/10000 train_loss:3.8061 train_time:1398136ms step_avg:170.28ms\nstep:8222/10000 train_loss:3.7986 train_time:1398305ms step_avg:170.28ms\nstep:8223/10000 train_loss:3.7402 train_time:1398476ms step_avg:170.28ms\nstep:8224/10000 train_loss:3.8621 train_time:1398647ms step_avg:170.28ms\nstep:8225/10000 train_loss:3.7721 train_time:1398816ms step_avg:170.28ms\nstep:8226/10000 train_loss:3.7487 train_time:1398986ms step_avg:170.28ms\nstep:8227/10000 train_loss:3.8494 train_time:1399159ms step_avg:170.28ms\nstep:8228/10000 train_loss:3.8055 train_time:1399327ms step_avg:170.28ms\nstep:8229/10000 train_loss:3.8163 train_time:1399498ms step_avg:170.28ms\nstep:8230/10000 train_loss:3.7374 train_time:1399668ms step_avg:170.28ms\nstep:8231/10000 train_loss:3.9692 train_time:1399839ms step_avg:170.28ms\nstep:8232/10000 train_loss:3.7868 train_time:1400006ms step_avg:170.28ms\nstep:8233/10000 train_loss:3.8100 train_time:1400180ms step_avg:170.28ms\nstep:8234/10000 train_loss:3.6036 train_time:1400346ms step_avg:170.28ms\nstep:8235/10000 train_loss:3.9048 train_time:1400515ms step_avg:170.28ms\nstep:8236/10000 train_loss:3.8482 train_time:1400683ms step_avg:170.28ms\nstep:8237/10000 train_loss:3.7372 train_time:1400853ms step_avg:170.28ms\nstep:8238/10000 train_loss:3.7050 train_time:1401023ms step_avg:170.28ms\nstep:8239/10000 train_loss:3.8127 train_time:1401193ms step_avg:170.28ms\nstep:8240/10000 train_loss:3.7786 train_time:1401366ms step_avg:170.28ms\nstep:8241/10000 train_loss:3.7526 train_time:1401537ms step_avg:170.28ms\nstep:8242/10000 train_loss:3.8937 train_time:1401708ms step_avg:170.28ms\nstep:8243/10000 train_loss:3.7818 train_time:1401878ms step_avg:170.28ms\nstep:8244/10000 train_loss:3.7008 train_time:1402047ms step_avg:170.28ms\nstep:8245/10000 train_loss:3.6513 train_time:1402216ms step_avg:170.28ms\nstep:8246/10000 train_loss:3.8907 train_time:1402385ms step_avg:170.28ms\nstep:8247/10000 train_loss:3.4882 train_time:1402553ms step_avg:170.27ms\nstep:8248/10000 train_loss:3.6722 train_time:1402721ms step_avg:170.27ms\nstep:8249/10000 train_loss:3.8440 train_time:1402893ms step_avg:170.27ms\nstep:8250/10000 train_loss:3.7699 train_time:1403066ms step_avg:170.27ms\nstep:8251/10000 train_loss:4.0061 train_time:1403238ms step_avg:170.28ms\nstep:8252/10000 train_loss:3.8544 train_time:1403408ms step_avg:170.28ms\nstep:8253/10000 train_loss:3.6700 train_time:1403579ms step_avg:170.28ms\nstep:8254/10000 train_loss:3.8148 train_time:1403749ms step_avg:170.28ms\nstep:8255/10000 train_loss:3.7635 train_time:1403920ms step_avg:170.28ms\nstep:8256/10000 train_loss:3.9474 train_time:1404088ms step_avg:170.28ms\nstep:8257/10000 train_loss:3.7943 train_time:1404260ms step_avg:170.28ms\nstep:8258/10000 train_loss:3.7989 train_time:1404430ms step_avg:170.28ms\nstep:8259/10000 train_loss:3.7059 train_time:1404601ms step_avg:170.28ms\nstep:8260/10000 train_loss:3.8952 train_time:1404770ms step_avg:170.28ms\nstep:8261/10000 train_loss:3.5459 train_time:1404941ms step_avg:170.28ms\nstep:8262/10000 train_loss:3.7585 train_time:1405109ms step_avg:170.27ms\nstep:8263/10000 train_loss:3.6179 train_time:1405279ms step_avg:170.27ms\nstep:8264/10000 train_loss:3.8220 train_time:1405448ms step_avg:170.27ms\nstep:8265/10000 train_loss:3.6384 train_time:1405621ms step_avg:170.28ms\nstep:8266/10000 train_loss:3.6812 train_time:1405791ms step_avg:170.28ms\nstep:8267/10000 train_loss:3.7528 train_time:1405964ms step_avg:170.28ms\nstep:8268/10000 train_loss:3.9352 train_time:1406132ms step_avg:170.28ms\nstep:8269/10000 train_loss:3.7888 train_time:1406301ms step_avg:170.27ms\nstep:8270/10000 train_loss:3.9750 train_time:1406469ms step_avg:170.27ms\nstep:8271/10000 train_loss:3.8490 train_time:1406641ms step_avg:170.27ms\nstep:8272/10000 train_loss:3.6918 train_time:1406810ms step_avg:170.27ms\nstep:8273/10000 train_loss:3.8302 train_time:1406980ms step_avg:170.27ms\nstep:8274/10000 train_loss:3.8428 train_time:1407150ms step_avg:170.27ms\nstep:8275/10000 train_loss:3.7210 train_time:1407322ms step_avg:170.27ms\nstep:8276/10000 train_loss:3.8111 train_time:1407490ms step_avg:170.27ms\nstep:8277/10000 train_loss:3.7353 train_time:1407661ms step_avg:170.27ms\nstep:8278/10000 train_loss:3.7789 train_time:1407828ms step_avg:170.27ms\nstep:8279/10000 train_loss:3.7617 train_time:1407998ms step_avg:170.27ms\nstep:8280/10000 train_loss:3.6152 train_time:1408168ms step_avg:170.27ms\nstep:8281/10000 train_loss:3.8349 train_time:1408338ms step_avg:170.27ms\nstep:8282/10000 train_loss:3.8153 train_time:1408508ms step_avg:170.27ms\nstep:8283/10000 train_loss:3.8580 train_time:1408677ms step_avg:170.27ms\nstep:8284/10000 train_loss:3.8082 train_time:1408848ms step_avg:170.27ms\nstep:8285/10000 train_loss:3.7377 train_time:1409017ms step_avg:170.27ms\nstep:8286/10000 train_loss:3.7396 train_time:1409187ms step_avg:170.27ms\nstep:8287/10000 train_loss:3.7764 train_time:1409357ms step_avg:170.27ms\nstep:8288/10000 train_loss:3.8731 train_time:1409528ms step_avg:170.27ms\nstep:8289/10000 train_loss:3.7489 train_time:1409698ms step_avg:170.27ms\nstep:8290/10000 train_loss:3.7903 train_time:1409865ms step_avg:170.27ms\nstep:8291/10000 train_loss:3.5393 train_time:1410036ms step_avg:170.27ms\nstep:8292/10000 train_loss:3.9764 train_time:1410206ms step_avg:170.27ms\nstep:8293/10000 train_loss:3.8080 train_time:1410378ms step_avg:170.27ms\nstep:8294/10000 train_loss:3.9067 train_time:1410551ms step_avg:170.27ms\nstep:8295/10000 train_loss:3.8022 train_time:1410724ms step_avg:170.27ms\nstep:8296/10000 train_loss:3.9435 train_time:1410893ms step_avg:170.27ms\nstep:8297/10000 train_loss:3.8150 train_time:1411064ms step_avg:170.27ms\nstep:8298/10000 train_loss:3.9169 train_time:1411235ms step_avg:170.27ms\nstep:8299/10000 train_loss:3.7014 train_time:1411406ms step_avg:170.27ms\nstep:8300/10000 train_loss:3.7948 train_time:1411576ms step_avg:170.27ms\nstep:8301/10000 train_loss:3.7697 train_time:1411749ms step_avg:170.27ms\nstep:8302/10000 train_loss:3.7746 train_time:1411919ms step_avg:170.27ms\nstep:8303/10000 train_loss:4.0874 train_time:1412087ms step_avg:170.27ms\nstep:8304/10000 train_loss:3.9251 train_time:1412258ms step_avg:170.27ms\nstep:8305/10000 train_loss:3.7460 train_time:1412429ms step_avg:170.27ms\nstep:8306/10000 train_loss:3.8052 train_time:1412598ms step_avg:170.27ms\nstep:8307/10000 train_loss:3.7725 train_time:1412765ms step_avg:170.27ms\nstep:8308/10000 train_loss:3.9398 train_time:1412939ms step_avg:170.27ms\nstep:8309/10000 train_loss:3.7399 train_time:1413103ms step_avg:170.27ms\nstep:8310/10000 train_loss:3.7045 train_time:1413275ms step_avg:170.27ms\nstep:8311/10000 train_loss:3.7204 train_time:1413446ms step_avg:170.27ms\nstep:8312/10000 train_loss:4.6089 train_time:1413617ms step_avg:170.27ms\nstep:8313/10000 train_loss:3.7477 train_time:1413788ms step_avg:170.27ms\nstep:8314/10000 train_loss:4.2144 train_time:1413963ms step_avg:170.27ms\nstep:8315/10000 train_loss:3.7395 train_time:1414131ms step_avg:170.27ms\nstep:8316/10000 train_loss:3.7715 train_time:1414300ms step_avg:170.27ms\nstep:8317/10000 train_loss:3.6732 train_time:1414470ms step_avg:170.27ms\nstep:8318/10000 train_loss:3.6419 train_time:1414643ms step_avg:170.27ms\nstep:8319/10000 train_loss:3.7878 train_time:1414812ms step_avg:170.27ms\nstep:8320/10000 train_loss:3.8037 train_time:1414986ms step_avg:170.28ms\nstep:8321/10000 train_loss:3.8234 train_time:1415152ms step_avg:170.27ms\nstep:8322/10000 train_loss:3.8180 train_time:1415321ms step_avg:170.27ms\nstep:8323/10000 train_loss:3.8582 train_time:1415491ms step_avg:170.27ms\nstep:8324/10000 train_loss:3.7771 train_time:1415663ms step_avg:170.27ms\nstep:8325/10000 train_loss:3.8404 train_time:1415834ms step_avg:170.27ms\nstep:8326/10000 train_loss:3.7199 train_time:1416002ms step_avg:170.27ms\nstep:8327/10000 train_loss:3.7817 train_time:1416172ms step_avg:170.27ms\nstep:8328/10000 train_loss:3.7831 train_time:1416346ms step_avg:170.27ms\nstep:8329/10000 train_loss:3.6750 train_time:1416519ms step_avg:170.28ms\nstep:8330/10000 train_loss:3.6806 train_time:1416690ms step_avg:170.28ms\nstep:8331/10000 train_loss:3.7131 train_time:1416859ms step_avg:170.28ms\nstep:8332/10000 train_loss:3.7131 train_time:1417029ms step_avg:170.28ms\nstep:8333/10000 train_loss:3.7103 train_time:1417200ms step_avg:170.28ms\nstep:8334/10000 train_loss:3.7695 train_time:1417371ms step_avg:170.28ms\nstep:8335/10000 train_loss:3.7968 train_time:1417544ms step_avg:170.28ms\nstep:8336/10000 train_loss:3.8045 train_time:1417715ms step_avg:170.28ms\nstep:8337/10000 train_loss:3.6833 train_time:1417886ms step_avg:170.28ms\nstep:8338/10000 train_loss:4.0800 train_time:1418058ms step_avg:170.28ms\nstep:8339/10000 train_loss:3.8059 train_time:1418227ms step_avg:170.28ms\nstep:8340/10000 train_loss:3.7108 train_time:1418398ms step_avg:170.28ms\nstep:8341/10000 train_loss:3.8703 train_time:1418567ms step_avg:170.28ms\nstep:8342/10000 train_loss:3.6633 train_time:1418738ms step_avg:170.28ms\nstep:8343/10000 train_loss:3.8529 train_time:1418908ms step_avg:170.28ms\nstep:8344/10000 train_loss:3.8120 train_time:1419080ms step_avg:170.28ms\nstep:8345/10000 train_loss:3.9195 train_time:1419249ms step_avg:170.28ms\nstep:8346/10000 train_loss:3.9867 train_time:1419421ms step_avg:170.28ms\nstep:8347/10000 train_loss:3.8093 train_time:1419590ms step_avg:170.28ms\nstep:8348/10000 train_loss:3.7695 train_time:1419760ms step_avg:170.28ms\nstep:8349/10000 train_loss:3.7286 train_time:1419930ms step_avg:170.28ms\nstep:8350/10000 train_loss:3.7077 train_time:1420100ms step_avg:170.28ms\nstep:8351/10000 train_loss:3.7606 train_time:1420268ms step_avg:170.28ms\nstep:8352/10000 train_loss:3.7227 train_time:1420438ms step_avg:170.28ms\nstep:8353/10000 train_loss:3.7728 train_time:1420607ms step_avg:170.28ms\nstep:8354/10000 train_loss:3.6721 train_time:1420778ms step_avg:170.28ms\nstep:8355/10000 train_loss:3.7652 train_time:1420947ms step_avg:170.28ms\nstep:8356/10000 train_loss:3.7710 train_time:1421120ms step_avg:170.28ms\nstep:8357/10000 train_loss:3.7681 train_time:1421289ms step_avg:170.28ms\nstep:8358/10000 train_loss:3.9093 train_time:1421460ms step_avg:170.28ms\nstep:8359/10000 train_loss:4.0699 train_time:1421629ms step_avg:170.28ms\nstep:8360/10000 train_loss:3.7346 train_time:1421802ms step_avg:170.28ms\nstep:8361/10000 train_loss:3.8029 train_time:1421973ms step_avg:170.28ms\nstep:8362/10000 train_loss:3.8749 train_time:1422145ms step_avg:170.28ms\nstep:8363/10000 train_loss:3.7229 train_time:1422315ms step_avg:170.28ms\nstep:8364/10000 train_loss:3.7078 train_time:1422486ms step_avg:170.28ms\nstep:8365/10000 train_loss:3.7969 train_time:1422658ms step_avg:170.28ms\nstep:8366/10000 train_loss:3.8339 train_time:1422827ms step_avg:170.28ms\nstep:8367/10000 train_loss:3.6087 train_time:1422997ms step_avg:170.28ms\nstep:8368/10000 train_loss:3.6832 train_time:1423167ms step_avg:170.28ms\nstep:8369/10000 train_loss:3.7865 train_time:1423337ms step_avg:170.28ms\nstep:8370/10000 train_loss:3.8918 train_time:1423505ms step_avg:170.28ms\nstep:8371/10000 train_loss:3.8720 train_time:1423676ms step_avg:170.28ms\nstep:8372/10000 train_loss:3.7153 train_time:1423847ms step_avg:170.28ms\nstep:8373/10000 train_loss:3.6991 train_time:1424021ms step_avg:170.28ms\nstep:8374/10000 train_loss:3.8139 train_time:1424189ms step_avg:170.28ms\nstep:8375/10000 train_loss:3.5791 train_time:1424358ms step_avg:170.28ms\nstep:8376/10000 train_loss:3.7016 train_time:1424528ms step_avg:170.28ms\nstep:8377/10000 train_loss:3.8190 train_time:1424698ms step_avg:170.28ms\nstep:8378/10000 train_loss:3.8023 train_time:1424869ms step_avg:170.28ms\nstep:8379/10000 train_loss:3.7681 train_time:1425042ms step_avg:170.28ms\nstep:8380/10000 train_loss:3.7862 train_time:1425212ms step_avg:170.28ms\nstep:8381/10000 train_loss:3.6133 train_time:1425384ms step_avg:170.28ms\nstep:8382/10000 train_loss:3.7913 train_time:1425780ms step_avg:170.30ms\nstep:8383/10000 train_loss:3.7538 train_time:1425940ms step_avg:170.30ms\nstep:8384/10000 train_loss:3.7604 train_time:1426108ms step_avg:170.30ms\nstep:8385/10000 train_loss:3.7240 train_time:1426277ms step_avg:170.30ms\nstep:8386/10000 train_loss:3.6997 train_time:1426445ms step_avg:170.30ms\nstep:8387/10000 train_loss:3.7278 train_time:1426614ms step_avg:170.30ms\nstep:8388/10000 train_loss:3.8347 train_time:1426794ms step_avg:170.30ms\nstep:8389/10000 train_loss:3.7211 train_time:1426970ms step_avg:170.30ms\nstep:8390/10000 train_loss:3.8047 train_time:1427139ms step_avg:170.30ms\nstep:8391/10000 train_loss:3.7465 train_time:1427305ms step_avg:170.30ms\nstep:8392/10000 train_loss:3.6707 train_time:1427474ms step_avg:170.30ms\nstep:8393/10000 train_loss:3.6674 train_time:1427644ms step_avg:170.30ms\nstep:8394/10000 train_loss:3.8469 train_time:1427814ms step_avg:170.30ms\nstep:8395/10000 train_loss:3.7513 train_time:1427988ms step_avg:170.30ms\nstep:8396/10000 train_loss:3.5127 train_time:1428160ms step_avg:170.30ms\nstep:8397/10000 train_loss:4.0907 train_time:1428330ms step_avg:170.30ms\nstep:8398/10000 train_loss:3.6896 train_time:1428498ms step_avg:170.30ms\nstep:8399/10000 train_loss:3.9992 train_time:1428667ms step_avg:170.30ms\nstep:8400/10000 train_loss:3.7482 train_time:1428838ms step_avg:170.30ms\nstep:8401/10000 train_loss:3.8793 train_time:1429008ms step_avg:170.30ms\nstep:8402/10000 train_loss:3.5970 train_time:1429178ms step_avg:170.30ms\nstep:8403/10000 train_loss:3.8567 train_time:1429348ms step_avg:170.30ms\nstep:8404/10000 train_loss:3.9383 train_time:1429518ms step_avg:170.30ms\nstep:8405/10000 train_loss:3.6660 train_time:1429687ms step_avg:170.30ms\nstep:8406/10000 train_loss:3.8210 train_time:1429861ms step_avg:170.30ms\nstep:8407/10000 train_loss:3.8246 train_time:1430032ms step_avg:170.30ms\nstep:8408/10000 train_loss:3.8026 train_time:1430205ms step_avg:170.30ms\nstep:8409/10000 train_loss:3.8305 train_time:1430374ms step_avg:170.30ms\nstep:8410/10000 train_loss:3.7717 train_time:1430543ms step_avg:170.30ms\nstep:8411/10000 train_loss:3.7388 train_time:1430715ms step_avg:170.30ms\nstep:8412/10000 train_loss:3.6389 train_time:1430887ms step_avg:170.30ms\nstep:8413/10000 train_loss:3.7554 train_time:1431059ms step_avg:170.30ms\nstep:8414/10000 train_loss:3.8021 train_time:1431230ms step_avg:170.30ms\nstep:8415/10000 train_loss:3.7004 train_time:1431402ms step_avg:170.30ms\nstep:8416/10000 train_loss:3.7470 train_time:1431570ms step_avg:170.30ms\nstep:8417/10000 train_loss:3.9019 train_time:1431742ms step_avg:170.30ms\nstep:8418/10000 train_loss:3.6987 train_time:1431912ms step_avg:170.30ms\nstep:8419/10000 train_loss:3.7900 train_time:1432083ms step_avg:170.30ms\nstep:8420/10000 train_loss:3.7278 train_time:1432250ms step_avg:170.30ms\nstep:8421/10000 train_loss:3.7730 train_time:1432422ms step_avg:170.30ms\nstep:8422/10000 train_loss:3.7876 train_time:1432591ms step_avg:170.30ms\nstep:8423/10000 train_loss:3.9571 train_time:1432763ms step_avg:170.30ms\nstep:8424/10000 train_loss:3.8414 train_time:1432934ms step_avg:170.30ms\nstep:8425/10000 train_loss:3.7364 train_time:1433107ms step_avg:170.30ms\nstep:8426/10000 train_loss:3.8179 train_time:1433275ms step_avg:170.30ms\nstep:8427/10000 train_loss:3.8450 train_time:1433444ms step_avg:170.30ms\nstep:8428/10000 train_loss:3.6890 train_time:1433615ms step_avg:170.30ms\nstep:8429/10000 train_loss:3.7882 train_time:1433784ms step_avg:170.30ms\nstep:8430/10000 train_loss:3.9189 train_time:1433956ms step_avg:170.30ms\nstep:8431/10000 train_loss:3.9016 train_time:1434126ms step_avg:170.30ms\nstep:8432/10000 train_loss:3.6378 train_time:1434297ms step_avg:170.30ms\nstep:8433/10000 train_loss:3.6138 train_time:1434466ms step_avg:170.30ms\nstep:8434/10000 train_loss:3.8590 train_time:1434637ms step_avg:170.30ms\nstep:8435/10000 train_loss:3.7438 train_time:1434809ms step_avg:170.30ms\nstep:8436/10000 train_loss:3.9058 train_time:1434979ms step_avg:170.30ms\nstep:8437/10000 train_loss:3.7315 train_time:1435150ms step_avg:170.30ms\nstep:8438/10000 train_loss:3.7814 train_time:1435322ms step_avg:170.30ms\nstep:8439/10000 train_loss:3.6499 train_time:1435491ms step_avg:170.30ms\nstep:8440/10000 train_loss:3.6707 train_time:1435663ms step_avg:170.30ms\nstep:8441/10000 train_loss:3.7964 train_time:1435831ms step_avg:170.30ms\nstep:8442/10000 train_loss:3.9413 train_time:1436002ms step_avg:170.30ms\nstep:8443/10000 train_loss:3.8056 train_time:1436171ms step_avg:170.30ms\nstep:8444/10000 train_loss:3.9985 train_time:1436345ms step_avg:170.30ms\nstep:8445/10000 train_loss:3.8504 train_time:1436514ms step_avg:170.30ms\nstep:8446/10000 train_loss:3.7517 train_time:1436684ms step_avg:170.30ms\nstep:8447/10000 train_loss:3.6590 train_time:1436855ms step_avg:170.30ms\nstep:8448/10000 train_loss:3.8714 train_time:1437027ms step_avg:170.30ms\nstep:8449/10000 train_loss:3.8267 train_time:1437206ms step_avg:170.31ms\nstep:8450/10000 train_loss:3.9480 train_time:1437370ms step_avg:170.30ms\nstep:8451/10000 train_loss:3.6890 train_time:1437543ms step_avg:170.30ms\nstep:8452/10000 train_loss:3.7982 train_time:1437713ms step_avg:170.30ms\nstep:8453/10000 train_loss:3.7590 train_time:1437884ms step_avg:170.30ms\nstep:8454/10000 train_loss:4.0194 train_time:1438052ms step_avg:170.30ms\nstep:8455/10000 train_loss:3.6894 train_time:1438222ms step_avg:170.30ms\nstep:8456/10000 train_loss:3.8036 train_time:1438392ms step_avg:170.30ms\nstep:8457/10000 train_loss:3.7755 train_time:1438560ms step_avg:170.30ms\nstep:8458/10000 train_loss:3.6479 train_time:1438732ms step_avg:170.30ms\nstep:8459/10000 train_loss:4.0660 train_time:1438904ms step_avg:170.30ms\nstep:8460/10000 train_loss:3.6354 train_time:1439073ms step_avg:170.30ms\nstep:8461/10000 train_loss:3.8036 train_time:1439247ms step_avg:170.30ms\nstep:8462/10000 train_loss:3.7336 train_time:1439417ms step_avg:170.30ms\nstep:8463/10000 train_loss:3.6444 train_time:1439585ms step_avg:170.30ms\nstep:8464/10000 train_loss:4.0797 train_time:1439757ms step_avg:170.30ms\nstep:8465/10000 train_loss:3.7002 train_time:1439927ms step_avg:170.30ms\nstep:8466/10000 train_loss:3.6111 train_time:1440099ms step_avg:170.30ms\nstep:8467/10000 train_loss:3.8160 train_time:1440268ms step_avg:170.30ms\nstep:8468/10000 train_loss:3.8194 train_time:1440438ms step_avg:170.30ms\nstep:8469/10000 train_loss:3.8066 train_time:1440606ms step_avg:170.30ms\nstep:8470/10000 train_loss:3.7660 train_time:1440775ms step_avg:170.30ms\nstep:8471/10000 train_loss:3.7250 train_time:1440944ms step_avg:170.30ms\nstep:8472/10000 train_loss:3.7102 train_time:1441113ms step_avg:170.30ms\nstep:8473/10000 train_loss:3.6622 train_time:1441284ms step_avg:170.30ms\nstep:8474/10000 train_loss:3.8330 train_time:1441456ms step_avg:170.30ms\nstep:8475/10000 train_loss:3.6761 train_time:1441629ms step_avg:170.30ms\nstep:8476/10000 train_loss:3.8426 train_time:1441802ms step_avg:170.30ms\nstep:8477/10000 train_loss:3.8517 train_time:1441973ms step_avg:170.31ms\nstep:8478/10000 train_loss:3.7419 train_time:1442142ms step_avg:170.30ms\nstep:8479/10000 train_loss:3.7671 train_time:1442311ms step_avg:170.30ms\nstep:8480/10000 train_loss:3.8058 train_time:1442481ms step_avg:170.30ms\nstep:8481/10000 train_loss:3.7110 train_time:1442653ms step_avg:170.30ms\nstep:8482/10000 train_loss:3.9151 train_time:1442825ms step_avg:170.31ms\nstep:8483/10000 train_loss:3.6834 train_time:1442994ms step_avg:170.31ms\nstep:8484/10000 train_loss:3.7588 train_time:1443168ms step_avg:170.31ms\nstep:8485/10000 train_loss:3.7431 train_time:1443336ms step_avg:170.31ms\nstep:8486/10000 train_loss:3.8768 train_time:1443509ms step_avg:170.31ms\nstep:8487/10000 train_loss:4.0594 train_time:1443680ms step_avg:170.31ms\nstep:8488/10000 train_loss:3.8279 train_time:1443847ms step_avg:170.31ms\nstep:8489/10000 train_loss:3.6812 train_time:1444018ms step_avg:170.31ms\nstep:8490/10000 train_loss:3.8826 train_time:1444188ms step_avg:170.31ms\nstep:8491/10000 train_loss:3.6954 train_time:1444360ms step_avg:170.31ms\nstep:8492/10000 train_loss:3.6905 train_time:1444528ms step_avg:170.31ms\nstep:8493/10000 train_loss:4.3956 train_time:1444698ms step_avg:170.31ms\nstep:8494/10000 train_loss:3.8177 train_time:1444867ms step_avg:170.30ms\nstep:8495/10000 train_loss:3.7294 train_time:1445036ms step_avg:170.30ms\nstep:8496/10000 train_loss:3.7348 train_time:1445206ms step_avg:170.30ms\nstep:8497/10000 train_loss:3.6582 train_time:1445377ms step_avg:170.30ms\nstep:8498/10000 train_loss:3.6853 train_time:1445546ms step_avg:170.30ms\nstep:8499/10000 train_loss:3.8171 train_time:1445717ms step_avg:170.30ms\nstep:8500/10000 train_loss:3.4678 train_time:1445885ms step_avg:170.30ms\nstep:8501/10000 train_loss:3.8213 train_time:1446054ms step_avg:170.30ms\nstep:8502/10000 train_loss:3.6992 train_time:1446222ms step_avg:170.30ms\nstep:8503/10000 train_loss:3.7003 train_time:1446392ms step_avg:170.30ms\nstep:8504/10000 train_loss:3.6602 train_time:1446563ms step_avg:170.30ms\nstep:8505/10000 train_loss:3.7935 train_time:1446732ms step_avg:170.30ms\nstep:8506/10000 train_loss:3.8194 train_time:1446902ms step_avg:170.30ms\nstep:8507/10000 train_loss:3.8235 train_time:1447072ms step_avg:170.30ms\nstep:8508/10000 train_loss:3.7969 train_time:1447243ms step_avg:170.30ms\nstep:8509/10000 train_loss:3.7225 train_time:1447412ms step_avg:170.30ms\nstep:8510/10000 train_loss:3.7705 train_time:1447584ms step_avg:170.30ms\nstep:8511/10000 train_loss:3.7750 train_time:1447755ms step_avg:170.30ms\nstep:8512/10000 train_loss:3.7671 train_time:1447926ms step_avg:170.30ms\nstep:8513/10000 train_loss:3.7582 train_time:1448099ms step_avg:170.30ms\nstep:8514/10000 train_loss:3.7888 train_time:1448268ms step_avg:170.30ms\nstep:8515/10000 train_loss:3.6763 train_time:1448438ms step_avg:170.30ms\nstep:8516/10000 train_loss:3.8419 train_time:1448608ms step_avg:170.30ms\nstep:8517/10000 train_loss:3.6272 train_time:1448779ms step_avg:170.30ms\nstep:8518/10000 train_loss:3.7689 train_time:1448949ms step_avg:170.30ms\nstep:8519/10000 train_loss:3.8723 train_time:1449119ms step_avg:170.30ms\nstep:8520/10000 train_loss:3.7358 train_time:1449288ms step_avg:170.30ms\nstep:8521/10000 train_loss:3.6462 train_time:1449459ms step_avg:170.30ms\nstep:8522/10000 train_loss:3.7803 train_time:1449628ms step_avg:170.30ms\nstep:8523/10000 train_loss:3.6686 train_time:1449799ms step_avg:170.30ms\nstep:8524/10000 train_loss:3.8839 train_time:1449968ms step_avg:170.30ms\nstep:8525/10000 train_loss:3.7662 train_time:1450139ms step_avg:170.30ms\nstep:8526/10000 train_loss:3.8208 train_time:1450309ms step_avg:170.30ms\nstep:8527/10000 train_loss:4.0138 train_time:1450480ms step_avg:170.30ms\nstep:8528/10000 train_loss:3.8414 train_time:1450649ms step_avg:170.30ms\nstep:8529/10000 train_loss:3.8636 train_time:1450818ms step_avg:170.30ms\nstep:8530/10000 train_loss:3.6322 train_time:1450988ms step_avg:170.30ms\nstep:8531/10000 train_loss:3.9181 train_time:1451157ms step_avg:170.30ms\nstep:8532/10000 train_loss:3.7356 train_time:1451326ms step_avg:170.30ms\nstep:8533/10000 train_loss:3.6885 train_time:1451499ms step_avg:170.30ms\nstep:8534/10000 train_loss:3.6985 train_time:1451667ms step_avg:170.30ms\nstep:8535/10000 train_loss:3.7533 train_time:1451839ms step_avg:170.30ms\nstep:8536/10000 train_loss:3.6773 train_time:1452009ms step_avg:170.30ms\nstep:8537/10000 train_loss:3.7873 train_time:1452178ms step_avg:170.30ms\nstep:8538/10000 train_loss:3.8294 train_time:1452347ms step_avg:170.30ms\nstep:8539/10000 train_loss:3.8404 train_time:1452517ms step_avg:170.30ms\nstep:8540/10000 train_loss:3.7158 train_time:1452687ms step_avg:170.30ms\nstep:8541/10000 train_loss:3.8388 train_time:1452859ms step_avg:170.30ms\nstep:8542/10000 train_loss:3.6868 train_time:1453030ms step_avg:170.30ms\nstep:8543/10000 train_loss:3.7332 train_time:1453198ms step_avg:170.30ms\nstep:8544/10000 train_loss:3.8617 train_time:1453365ms step_avg:170.30ms\nstep:8545/10000 train_loss:3.9911 train_time:1453537ms step_avg:170.30ms\nstep:8546/10000 train_loss:3.6394 train_time:1453708ms step_avg:170.30ms\nstep:8547/10000 train_loss:3.7920 train_time:1453881ms step_avg:170.30ms\nstep:8548/10000 train_loss:3.7446 train_time:1454052ms step_avg:170.30ms\nstep:8549/10000 train_loss:3.7230 train_time:1454221ms step_avg:170.30ms\nstep:8550/10000 train_loss:3.7094 train_time:1454391ms step_avg:170.30ms\nstep:8551/10000 train_loss:3.8564 train_time:1454562ms step_avg:170.30ms\nstep:8552/10000 train_loss:3.6725 train_time:1454731ms step_avg:170.30ms\nstep:8553/10000 train_loss:3.8951 train_time:1454899ms step_avg:170.30ms\nstep:8554/10000 train_loss:3.7337 train_time:1455070ms step_avg:170.30ms\nstep:8555/10000 train_loss:3.6631 train_time:1455244ms step_avg:170.30ms\nstep:8556/10000 train_loss:3.7713 train_time:1455415ms step_avg:170.30ms\nstep:8557/10000 train_loss:3.7445 train_time:1455585ms step_avg:170.30ms\nstep:8558/10000 train_loss:3.8454 train_time:1455755ms step_avg:170.30ms\nstep:8559/10000 train_loss:3.7132 train_time:1455923ms step_avg:170.30ms\nstep:8560/10000 train_loss:3.8545 train_time:1456094ms step_avg:170.30ms\nstep:8561/10000 train_loss:3.6778 train_time:1456265ms step_avg:170.30ms\nstep:8562/10000 train_loss:3.9144 train_time:1456434ms step_avg:170.30ms\nstep:8563/10000 train_loss:3.7304 train_time:1456603ms step_avg:170.30ms\nstep:8564/10000 train_loss:3.8498 train_time:1456773ms step_avg:170.30ms\nstep:8565/10000 train_loss:3.7521 train_time:1456945ms step_avg:170.30ms\nstep:8566/10000 train_loss:3.8147 train_time:1457116ms step_avg:170.30ms\nstep:8567/10000 train_loss:3.7164 train_time:1457287ms step_avg:170.30ms\nstep:8568/10000 train_loss:3.8182 train_time:1457459ms step_avg:170.30ms\nstep:8569/10000 train_loss:3.5225 train_time:1457628ms step_avg:170.30ms\nstep:8570/10000 train_loss:3.9079 train_time:1457797ms step_avg:170.30ms\nstep:8571/10000 train_loss:3.8156 train_time:1457966ms step_avg:170.30ms\nstep:8572/10000 train_loss:3.7809 train_time:1458135ms step_avg:170.30ms\nstep:8573/10000 train_loss:3.7421 train_time:1458303ms step_avg:170.30ms\nstep:8574/10000 train_loss:3.7878 train_time:1458473ms step_avg:170.30ms\nstep:8575/10000 train_loss:3.7568 train_time:1458643ms step_avg:170.30ms\nstep:8576/10000 train_loss:3.7748 train_time:1458812ms step_avg:170.30ms\nstep:8577/10000 train_loss:3.8127 train_time:1458983ms step_avg:170.30ms\nstep:8578/10000 train_loss:3.6402 train_time:1459153ms step_avg:170.30ms\nstep:8579/10000 train_loss:3.8402 train_time:1459323ms step_avg:170.30ms\nstep:8580/10000 train_loss:3.8609 train_time:1459493ms step_avg:170.30ms\nstep:8581/10000 train_loss:4.0067 train_time:1459664ms step_avg:170.30ms\nstep:8582/10000 train_loss:3.9315 train_time:1459833ms step_avg:170.30ms\nstep:8583/10000 train_loss:3.8843 train_time:1460006ms step_avg:170.30ms\nstep:8584/10000 train_loss:4.1501 train_time:1460177ms step_avg:170.30ms\nstep:8585/10000 train_loss:3.8417 train_time:1460349ms step_avg:170.30ms\nstep:8586/10000 train_loss:3.7060 train_time:1460521ms step_avg:170.30ms\nstep:8587/10000 train_loss:3.7374 train_time:1460690ms step_avg:170.30ms\nstep:8588/10000 train_loss:3.6835 train_time:1460866ms step_avg:170.30ms\nstep:8589/10000 train_loss:3.8688 train_time:1461033ms step_avg:170.30ms\nstep:8590/10000 train_loss:3.7370 train_time:1461204ms step_avg:170.30ms\nstep:8591/10000 train_loss:3.7679 train_time:1461373ms step_avg:170.30ms\nstep:8592/10000 train_loss:3.7322 train_time:1461544ms step_avg:170.30ms\nstep:8593/10000 train_loss:3.7436 train_time:1461713ms step_avg:170.30ms\nstep:8594/10000 train_loss:3.7989 train_time:1461883ms step_avg:170.30ms\nstep:8595/10000 train_loss:3.6492 train_time:1462052ms step_avg:170.30ms\nstep:8596/10000 train_loss:3.7502 train_time:1462225ms step_avg:170.30ms\nstep:8597/10000 train_loss:3.7438 train_time:1462394ms step_avg:170.30ms\nstep:8598/10000 train_loss:3.7724 train_time:1462565ms step_avg:170.30ms\nstep:8599/10000 train_loss:3.6575 train_time:1462735ms step_avg:170.30ms\nstep:8600/10000 train_loss:3.8784 train_time:1462904ms step_avg:170.30ms\nstep:8601/10000 train_loss:3.9945 train_time:1463077ms step_avg:170.30ms\nstep:8602/10000 train_loss:3.7477 train_time:1463248ms step_avg:170.30ms\nstep:8603/10000 train_loss:3.7829 train_time:1463418ms step_avg:170.30ms\nstep:8604/10000 train_loss:3.8097 train_time:1463586ms step_avg:170.30ms\nstep:8605/10000 train_loss:3.8260 train_time:1463757ms step_avg:170.30ms\nstep:8606/10000 train_loss:3.7056 train_time:1463928ms step_avg:170.30ms\nstep:8607/10000 train_loss:3.7737 train_time:1464100ms step_avg:170.30ms\nstep:8608/10000 train_loss:4.2242 train_time:1464267ms step_avg:170.30ms\nstep:8609/10000 train_loss:3.7813 train_time:1464438ms step_avg:170.30ms\nstep:8610/10000 train_loss:4.0284 train_time:1464606ms step_avg:170.30ms\nstep:8611/10000 train_loss:3.6776 train_time:1464778ms step_avg:170.30ms\nstep:8612/10000 train_loss:3.7266 train_time:1464948ms step_avg:170.30ms\nstep:8613/10000 train_loss:3.7569 train_time:1465120ms step_avg:170.30ms\nstep:8614/10000 train_loss:3.9404 train_time:1465288ms step_avg:170.30ms\nstep:8615/10000 train_loss:3.7437 train_time:1465460ms step_avg:170.30ms\nstep:8616/10000 train_loss:3.7323 train_time:1465630ms step_avg:170.30ms\nstep:8617/10000 train_loss:3.7232 train_time:1465798ms step_avg:170.30ms\nstep:8618/10000 train_loss:3.7142 train_time:1465967ms step_avg:170.30ms\nstep:8619/10000 train_loss:3.7858 train_time:1466139ms step_avg:170.30ms\nstep:8620/10000 train_loss:3.7556 train_time:1466308ms step_avg:170.30ms\nstep:8621/10000 train_loss:3.7879 train_time:1466477ms step_avg:170.30ms\nstep:8622/10000 train_loss:3.8502 train_time:1466647ms step_avg:170.30ms\nstep:8623/10000 train_loss:4.0083 train_time:1466818ms step_avg:170.30ms\nstep:8624/10000 train_loss:3.8059 train_time:1466987ms step_avg:170.30ms\nstep:8625/10000 train_loss:3.7589 train_time:1467158ms step_avg:170.30ms\nstep:8626/10000 train_loss:3.8237 train_time:1467327ms step_avg:170.30ms\nstep:8627/10000 train_loss:3.7641 train_time:1467496ms step_avg:170.30ms\nstep:8628/10000 train_loss:3.7874 train_time:1467666ms step_avg:170.30ms\nstep:8629/10000 train_loss:4.7064 train_time:1467837ms step_avg:170.30ms\nstep:8630/10000 train_loss:3.8024 train_time:1468011ms step_avg:170.30ms\nstep:8631/10000 train_loss:3.6550 train_time:1468180ms step_avg:170.30ms\nstep:8632/10000 train_loss:3.6952 train_time:1468351ms step_avg:170.30ms\nstep:8633/10000 train_loss:3.7263 train_time:1468522ms step_avg:170.30ms\nstep:8634/10000 train_loss:3.7936 train_time:1468690ms step_avg:170.30ms\nstep:8635/10000 train_loss:3.8837 train_time:1468861ms step_avg:170.30ms\nstep:8636/10000 train_loss:3.9225 train_time:1469031ms step_avg:170.30ms\nstep:8637/10000 train_loss:3.9746 train_time:1469204ms step_avg:170.30ms\nstep:8638/10000 train_loss:3.5929 train_time:1469375ms step_avg:170.30ms\nstep:8639/10000 train_loss:3.9962 train_time:1469547ms step_avg:170.30ms\nstep:8640/10000 train_loss:3.5580 train_time:1469718ms step_avg:170.30ms\nstep:8641/10000 train_loss:3.8170 train_time:1469885ms step_avg:170.30ms\nstep:8642/10000 train_loss:3.7507 train_time:1470054ms step_avg:170.30ms\nstep:8643/10000 train_loss:3.7496 train_time:1470226ms step_avg:170.30ms\nstep:8644/10000 train_loss:3.7023 train_time:1470399ms step_avg:170.30ms\nstep:8645/10000 train_loss:3.7798 train_time:1470566ms step_avg:170.30ms\nstep:8646/10000 train_loss:3.6722 train_time:1470736ms step_avg:170.30ms\nstep:8647/10000 train_loss:3.6308 train_time:1470908ms step_avg:170.30ms\nstep:8648/10000 train_loss:3.8450 train_time:1471078ms step_avg:170.30ms\nstep:8649/10000 train_loss:3.7691 train_time:1471253ms step_avg:170.30ms\nstep:8650/10000 train_loss:3.7901 train_time:1471421ms step_avg:170.30ms\nstep:8651/10000 train_loss:3.8286 train_time:1471596ms step_avg:170.30ms\nstep:8652/10000 train_loss:3.7708 train_time:1471765ms step_avg:170.30ms\nstep:8653/10000 train_loss:3.6848 train_time:1471932ms step_avg:170.30ms\nstep:8654/10000 train_loss:3.6378 train_time:1472104ms step_avg:170.30ms\nstep:8655/10000 train_loss:3.6408 train_time:1472273ms step_avg:170.30ms\nstep:8656/10000 train_loss:3.9102 train_time:1472443ms step_avg:170.30ms\nstep:8657/10000 train_loss:3.6680 train_time:1472614ms step_avg:170.30ms\nstep:8658/10000 train_loss:3.7624 train_time:1472786ms step_avg:170.30ms\nstep:8659/10000 train_loss:4.0513 train_time:1472963ms step_avg:170.30ms\nstep:8660/10000 train_loss:3.8630 train_time:1473132ms step_avg:170.30ms\nstep:8661/10000 train_loss:3.7686 train_time:1473301ms step_avg:170.30ms\nstep:8662/10000 train_loss:3.8574 train_time:1473467ms step_avg:170.30ms\nstep:8663/10000 train_loss:3.8033 train_time:1473632ms step_avg:170.30ms\nstep:8664/10000 train_loss:3.7766 train_time:1473807ms step_avg:170.30ms\nstep:8665/10000 train_loss:3.6336 train_time:1473977ms step_avg:170.30ms\nstep:8666/10000 train_loss:3.7438 train_time:1474148ms step_avg:170.30ms\nstep:8667/10000 train_loss:3.8676 train_time:1474316ms step_avg:170.30ms\nstep:8668/10000 train_loss:3.7344 train_time:1474488ms step_avg:170.30ms\nstep:8669/10000 train_loss:3.6689 train_time:1474658ms step_avg:170.30ms\nstep:8670/10000 train_loss:3.7461 train_time:1474825ms step_avg:170.30ms\nstep:8671/10000 train_loss:3.9008 train_time:1474996ms step_avg:170.30ms\nstep:8672/10000 train_loss:3.7559 train_time:1475163ms step_avg:170.30ms\nstep:8673/10000 train_loss:3.7680 train_time:1475332ms step_avg:170.30ms\nstep:8674/10000 train_loss:3.9191 train_time:1475502ms step_avg:170.30ms\nstep:8675/10000 train_loss:3.9511 train_time:1475673ms step_avg:170.30ms\nstep:8676/10000 train_loss:3.8286 train_time:1475846ms step_avg:170.30ms\nstep:8677/10000 train_loss:3.7847 train_time:1476017ms step_avg:170.30ms\nstep:8678/10000 train_loss:3.7486 train_time:1476187ms step_avg:170.30ms\nstep:8679/10000 train_loss:3.7624 train_time:1476358ms step_avg:170.30ms\nstep:8680/10000 train_loss:3.8415 train_time:1476528ms step_avg:170.30ms\nstep:8681/10000 train_loss:4.1128 train_time:1476698ms step_avg:170.30ms\nstep:8682/10000 train_loss:3.6773 train_time:1476869ms step_avg:170.30ms\nstep:8683/10000 train_loss:3.7837 train_time:1477041ms step_avg:170.30ms\nstep:8684/10000 train_loss:3.8135 train_time:1477210ms step_avg:170.30ms\nstep:8685/10000 train_loss:3.8939 train_time:1477379ms step_avg:170.30ms\nstep:8686/10000 train_loss:3.8781 train_time:1477550ms step_avg:170.30ms\nstep:8687/10000 train_loss:3.7909 train_time:1477721ms step_avg:170.30ms\nstep:8688/10000 train_loss:3.6694 train_time:1477891ms step_avg:170.30ms\nstep:8689/10000 train_loss:3.7055 train_time:1478063ms step_avg:170.30ms\nstep:8690/10000 train_loss:3.8193 train_time:1478233ms step_avg:170.30ms\nstep:8691/10000 train_loss:3.9106 train_time:1478401ms step_avg:170.30ms\nstep:8692/10000 train_loss:3.7377 train_time:1478573ms step_avg:170.30ms\nstep:8693/10000 train_loss:3.9087 train_time:1478745ms step_avg:170.30ms\nstep:8694/10000 train_loss:3.6467 train_time:1478917ms step_avg:170.30ms\nstep:8695/10000 train_loss:3.9143 train_time:1479086ms step_avg:170.30ms\nstep:8696/10000 train_loss:3.8210 train_time:1479258ms step_avg:170.30ms\nstep:8697/10000 train_loss:3.7171 train_time:1479426ms step_avg:170.30ms\nstep:8698/10000 train_loss:4.0908 train_time:1479597ms step_avg:170.30ms\nstep:8699/10000 train_loss:3.8325 train_time:1479768ms step_avg:170.30ms\nstep:8700/10000 train_loss:3.9014 train_time:1479938ms step_avg:170.30ms\nstep:8701/10000 train_loss:3.6941 train_time:1480108ms step_avg:170.30ms\nstep:8702/10000 train_loss:3.7930 train_time:1480280ms step_avg:170.30ms\nstep:8703/10000 train_loss:3.7770 train_time:1480450ms step_avg:170.30ms\nstep:8704/10000 train_loss:3.7411 train_time:1480624ms step_avg:170.30ms\nstep:8705/10000 train_loss:3.8601 train_time:1480792ms step_avg:170.30ms\nstep:8706/10000 train_loss:3.7692 train_time:1480961ms step_avg:170.30ms\nstep:8707/10000 train_loss:3.6336 train_time:1481132ms step_avg:170.30ms\nstep:8708/10000 train_loss:3.7021 train_time:1481305ms step_avg:170.30ms\nstep:8709/10000 train_loss:3.6910 train_time:1481474ms step_avg:170.30ms\nstep:8710/10000 train_loss:3.8705 train_time:1481645ms step_avg:170.30ms\nstep:8711/10000 train_loss:3.8428 train_time:1481817ms step_avg:170.30ms\nstep:8712/10000 train_loss:3.6915 train_time:1481985ms step_avg:170.30ms\nstep:8713/10000 train_loss:3.8642 train_time:1482155ms step_avg:170.30ms\nstep:8714/10000 train_loss:4.1675 train_time:1482325ms step_avg:170.30ms\nstep:8715/10000 train_loss:3.7244 train_time:1482494ms step_avg:170.30ms\nstep:8716/10000 train_loss:3.7760 train_time:1482664ms step_avg:170.30ms\nstep:8717/10000 train_loss:3.7379 train_time:1482834ms step_avg:170.30ms\nstep:8718/10000 train_loss:3.8163 train_time:1483004ms step_avg:170.30ms\nstep:8719/10000 train_loss:3.6897 train_time:1483173ms step_avg:170.30ms\nstep:8720/10000 train_loss:3.7482 train_time:1483346ms step_avg:170.30ms\nstep:8721/10000 train_loss:3.8098 train_time:1483516ms step_avg:170.30ms\nstep:8722/10000 train_loss:3.9440 train_time:1483686ms step_avg:170.30ms\nstep:8723/10000 train_loss:3.7880 train_time:1483855ms step_avg:170.30ms\nstep:8724/10000 train_loss:3.7066 train_time:1484026ms step_avg:170.30ms\nstep:8725/10000 train_loss:3.8421 train_time:1484195ms step_avg:170.30ms\nstep:8726/10000 train_loss:3.7955 train_time:1484364ms step_avg:170.30ms\nstep:8727/10000 train_loss:3.7525 train_time:1484533ms step_avg:170.30ms\nstep:8728/10000 train_loss:3.9181 train_time:1484701ms step_avg:170.30ms\nstep:8729/10000 train_loss:3.7277 train_time:1484871ms step_avg:170.30ms\nstep:8730/10000 train_loss:4.0523 train_time:1485043ms step_avg:170.30ms\nstep:8731/10000 train_loss:3.6976 train_time:1485213ms step_avg:170.30ms\nstep:8732/10000 train_loss:3.7339 train_time:1485382ms step_avg:170.30ms\nstep:8733/10000 train_loss:3.7054 train_time:1485551ms step_avg:170.30ms\nstep:8734/10000 train_loss:3.7489 train_time:1485723ms step_avg:170.30ms\nstep:8735/10000 train_loss:3.8419 train_time:1485895ms step_avg:170.30ms\nstep:8736/10000 train_loss:3.7210 train_time:1486068ms step_avg:170.30ms\nstep:8737/10000 train_loss:3.7069 train_time:1486237ms step_avg:170.30ms\nstep:8738/10000 train_loss:3.6175 train_time:1486408ms step_avg:170.30ms\nstep:8739/10000 train_loss:3.8729 train_time:1486581ms step_avg:170.30ms\nstep:8740/10000 train_loss:3.8371 train_time:1486750ms step_avg:170.30ms\nstep:8741/10000 train_loss:3.8932 train_time:1486920ms step_avg:170.30ms\nstep:8742/10000 train_loss:4.2033 train_time:1487088ms step_avg:170.30ms\nstep:8743/10000 train_loss:3.8091 train_time:1487258ms step_avg:170.30ms\nstep:8744/10000 train_loss:3.7683 train_time:1487428ms step_avg:170.30ms\nstep:8745/10000 train_loss:3.6890 train_time:1487600ms step_avg:170.30ms\nstep:8746/10000 train_loss:3.8917 train_time:1487768ms step_avg:170.30ms\nstep:8747/10000 train_loss:3.8101 train_time:1487941ms step_avg:170.30ms\nstep:8748/10000 train_loss:3.7982 train_time:1488111ms step_avg:170.30ms\nstep:8749/10000 train_loss:3.8972 train_time:1488281ms step_avg:170.30ms\nstep:8750/10000 train_loss:3.7519 train_time:1488453ms step_avg:170.30ms\nstep:8751/10000 train_loss:3.6352 train_time:1488624ms step_avg:170.30ms\nstep:8752/10000 train_loss:3.6704 train_time:1488794ms step_avg:170.30ms\nstep:8753/10000 train_loss:3.7413 train_time:1488966ms step_avg:170.30ms\nstep:8754/10000 train_loss:3.7179 train_time:1489135ms step_avg:170.30ms\nstep:8755/10000 train_loss:3.7008 train_time:1489304ms step_avg:170.30ms\nstep:8756/10000 train_loss:3.8071 train_time:1489474ms step_avg:170.30ms\nstep:8757/10000 train_loss:3.6104 train_time:1489645ms step_avg:170.30ms\nstep:8758/10000 train_loss:3.7910 train_time:1489814ms step_avg:170.30ms\nstep:8759/10000 train_loss:3.7776 train_time:1489982ms step_avg:170.30ms\nstep:8760/10000 train_loss:4.0072 train_time:1490151ms step_avg:170.30ms\nstep:8761/10000 train_loss:3.8699 train_time:1490322ms step_avg:170.30ms\nstep:8762/10000 train_loss:3.8444 train_time:1490494ms step_avg:170.30ms\nstep:8763/10000 train_loss:3.8421 train_time:1490665ms step_avg:170.30ms\nstep:8764/10000 train_loss:3.6344 train_time:1490836ms step_avg:170.30ms\nstep:8765/10000 train_loss:3.7485 train_time:1491008ms step_avg:170.30ms\nstep:8766/10000 train_loss:3.5284 train_time:1491177ms step_avg:170.30ms\nstep:8767/10000 train_loss:3.8246 train_time:1491348ms step_avg:170.30ms\nstep:8768/10000 train_loss:3.6935 train_time:1491520ms step_avg:170.30ms\nstep:8769/10000 train_loss:3.7065 train_time:1491690ms step_avg:170.30ms\nstep:8770/10000 train_loss:3.7032 train_time:1491858ms step_avg:170.30ms\nstep:8771/10000 train_loss:3.6805 train_time:1492029ms step_avg:170.30ms\nstep:8772/10000 train_loss:3.5953 train_time:1492199ms step_avg:170.30ms\nstep:8773/10000 train_loss:3.7644 train_time:1492369ms step_avg:170.30ms\nstep:8774/10000 train_loss:3.7733 train_time:1492540ms step_avg:170.30ms\nstep:8775/10000 train_loss:3.7093 train_time:1492710ms step_avg:170.30ms\nstep:8776/10000 train_loss:3.6849 train_time:1492879ms step_avg:170.30ms\nstep:8777/10000 train_loss:3.7058 train_time:1493048ms step_avg:170.30ms\nstep:8778/10000 train_loss:3.6608 train_time:1493217ms step_avg:170.30ms\nstep:8779/10000 train_loss:3.6653 train_time:1493386ms step_avg:170.30ms\nstep:8780/10000 train_loss:3.8175 train_time:1493557ms step_avg:170.30ms\nstep:8781/10000 train_loss:3.7595 train_time:1493727ms step_avg:170.30ms\nstep:8782/10000 train_loss:3.8432 train_time:1493898ms step_avg:170.30ms\nstep:8783/10000 train_loss:3.7449 train_time:1494067ms step_avg:170.30ms\nstep:8784/10000 train_loss:3.7782 train_time:1494236ms step_avg:170.30ms\nstep:8785/10000 train_loss:3.5993 train_time:1494409ms step_avg:170.30ms\nstep:8786/10000 train_loss:3.8605 train_time:1494584ms step_avg:170.30ms\nstep:8787/10000 train_loss:3.7132 train_time:1494748ms step_avg:170.30ms\nstep:8788/10000 train_loss:3.9800 train_time:1494918ms step_avg:170.30ms\nstep:8789/10000 train_loss:3.7880 train_time:1495087ms step_avg:170.30ms\nstep:8790/10000 train_loss:3.8735 train_time:1495258ms step_avg:170.30ms\nstep:8791/10000 train_loss:3.7452 train_time:1495427ms step_avg:170.30ms\nstep:8792/10000 train_loss:3.7621 train_time:1495599ms step_avg:170.30ms\nstep:8793/10000 train_loss:3.8899 train_time:1495767ms step_avg:170.30ms\nstep:8794/10000 train_loss:3.8268 train_time:1495937ms step_avg:170.30ms\nstep:8795/10000 train_loss:3.8254 train_time:1496109ms step_avg:170.30ms\nstep:8796/10000 train_loss:3.8444 train_time:1496281ms step_avg:170.30ms\nstep:8797/10000 train_loss:3.8206 train_time:1496450ms step_avg:170.30ms\nstep:8798/10000 train_loss:3.8363 train_time:1496622ms step_avg:170.30ms\nstep:8799/10000 train_loss:3.9358 train_time:1496791ms step_avg:170.30ms\nstep:8800/10000 train_loss:3.7660 train_time:1496962ms step_avg:170.30ms\nstep:8801/10000 train_loss:3.7183 train_time:1497133ms step_avg:170.30ms\nstep:8802/10000 train_loss:3.8349 train_time:1497303ms step_avg:170.30ms\nstep:8803/10000 train_loss:3.7359 train_time:1497473ms step_avg:170.30ms\nstep:8804/10000 train_loss:3.6720 train_time:1497650ms step_avg:170.30ms\nstep:8805/10000 train_loss:3.8727 train_time:1497816ms step_avg:170.30ms\nstep:8806/10000 train_loss:3.8039 train_time:1497986ms step_avg:170.30ms\nstep:8807/10000 train_loss:3.7997 train_time:1498158ms step_avg:170.30ms\nstep:8808/10000 train_loss:3.7818 train_time:1498327ms step_avg:170.30ms\nstep:8809/10000 train_loss:3.7897 train_time:1498498ms step_avg:170.30ms\nstep:8810/10000 train_loss:3.9467 train_time:1498666ms step_avg:170.30ms\nstep:8811/10000 train_loss:3.7240 train_time:1498836ms step_avg:170.30ms\nstep:8812/10000 train_loss:3.7351 train_time:1499008ms step_avg:170.30ms\nstep:8813/10000 train_loss:3.8194 train_time:1499178ms step_avg:170.30ms\nstep:8814/10000 train_loss:3.7581 train_time:1499352ms step_avg:170.30ms\nstep:8815/10000 train_loss:3.7349 train_time:1499520ms step_avg:170.30ms\nstep:8816/10000 train_loss:3.7278 train_time:1499690ms step_avg:170.30ms\nstep:8817/10000 train_loss:3.7135 train_time:1499861ms step_avg:170.30ms\nstep:8818/10000 train_loss:3.8286 train_time:1500031ms step_avg:170.30ms\nstep:8819/10000 train_loss:3.6712 train_time:1500202ms step_avg:170.30ms\nstep:8820/10000 train_loss:3.7834 train_time:1500370ms step_avg:170.30ms\nstep:8821/10000 train_loss:3.8613 train_time:1500541ms step_avg:170.30ms\nstep:8822/10000 train_loss:3.7902 train_time:1500711ms step_avg:170.30ms\nstep:8823/10000 train_loss:3.7395 train_time:1500881ms step_avg:170.30ms\nstep:8824/10000 train_loss:3.7420 train_time:1501052ms step_avg:170.30ms\nstep:8825/10000 train_loss:3.7633 train_time:1501223ms step_avg:170.30ms\nstep:8826/10000 train_loss:3.6725 train_time:1501393ms step_avg:170.30ms\nstep:8827/10000 train_loss:3.7143 train_time:1501564ms step_avg:170.30ms\nstep:8828/10000 train_loss:3.7171 train_time:1501736ms step_avg:170.30ms\nstep:8829/10000 train_loss:3.8906 train_time:1501904ms step_avg:170.30ms\nstep:8830/10000 train_loss:3.9268 train_time:1502074ms step_avg:170.30ms\nstep:8831/10000 train_loss:4.9887 train_time:1502242ms step_avg:170.30ms\nstep:8832/10000 train_loss:4.2127 train_time:1502411ms step_avg:170.30ms\nstep:8833/10000 train_loss:3.7800 train_time:1502580ms step_avg:170.30ms\nstep:8834/10000 train_loss:3.8841 train_time:1502750ms step_avg:170.30ms\nstep:8835/10000 train_loss:3.8286 train_time:1502920ms step_avg:170.30ms\nstep:8836/10000 train_loss:3.6376 train_time:1503090ms step_avg:170.30ms\nstep:8837/10000 train_loss:3.7586 train_time:1503262ms step_avg:170.30ms\nstep:8838/10000 train_loss:3.6506 train_time:1503431ms step_avg:170.30ms\nstep:8839/10000 train_loss:3.7808 train_time:1503604ms step_avg:170.30ms\nstep:8840/10000 train_loss:3.7681 train_time:1503775ms step_avg:170.30ms\nstep:8841/10000 train_loss:3.8378 train_time:1503946ms step_avg:170.30ms\nstep:8842/10000 train_loss:3.8176 train_time:1504114ms step_avg:170.30ms\nstep:8843/10000 train_loss:3.7687 train_time:1504286ms step_avg:170.30ms\nstep:8844/10000 train_loss:3.7580 train_time:1504456ms step_avg:170.30ms\nstep:8845/10000 train_loss:3.8771 train_time:1504626ms step_avg:170.30ms\nstep:8846/10000 train_loss:3.7413 train_time:1504796ms step_avg:170.30ms\nstep:8847/10000 train_loss:3.7700 train_time:1504970ms step_avg:170.30ms\nstep:8848/10000 train_loss:3.8103 train_time:1505141ms step_avg:170.30ms\nstep:8849/10000 train_loss:3.8150 train_time:1505308ms step_avg:170.30ms\nstep:8850/10000 train_loss:3.7324 train_time:1505479ms step_avg:170.30ms\nstep:8851/10000 train_loss:3.8454 train_time:1505647ms step_avg:170.30ms\nstep:8852/10000 train_loss:3.6873 train_time:1505817ms step_avg:170.30ms\nstep:8853/10000 train_loss:3.9108 train_time:1505986ms step_avg:170.30ms\nstep:8854/10000 train_loss:3.7223 train_time:1506158ms step_avg:170.30ms\nstep:8855/10000 train_loss:3.8684 train_time:1506327ms step_avg:170.30ms\nstep:8856/10000 train_loss:3.7684 train_time:1506498ms step_avg:170.30ms\nstep:8857/10000 train_loss:3.8150 train_time:1506668ms step_avg:170.30ms\nstep:8858/10000 train_loss:3.7942 train_time:1506838ms step_avg:170.30ms\nstep:8859/10000 train_loss:3.7231 train_time:1507008ms step_avg:170.30ms\nstep:8860/10000 train_loss:3.7806 train_time:1507181ms step_avg:170.30ms\nstep:8861/10000 train_loss:3.7301 train_time:1507349ms step_avg:170.30ms\nstep:8862/10000 train_loss:3.5909 train_time:1507520ms step_avg:170.30ms\nstep:8863/10000 train_loss:3.8148 train_time:1507690ms step_avg:170.30ms\nstep:8864/10000 train_loss:3.7742 train_time:1507860ms step_avg:170.30ms\nstep:8865/10000 train_loss:3.7297 train_time:1508030ms step_avg:170.30ms\nstep:8866/10000 train_loss:3.6051 train_time:1508202ms step_avg:170.30ms\nstep:8867/10000 train_loss:3.8115 train_time:1508373ms step_avg:170.30ms\nstep:8868/10000 train_loss:3.8152 train_time:1508544ms step_avg:170.30ms\nstep:8869/10000 train_loss:3.7720 train_time:1508714ms step_avg:170.30ms\nstep:8870/10000 train_loss:3.9224 train_time:1508885ms step_avg:170.30ms\nstep:8871/10000 train_loss:3.9136 train_time:1509056ms step_avg:170.30ms\nstep:8872/10000 train_loss:3.6704 train_time:1509227ms step_avg:170.30ms\nstep:8873/10000 train_loss:3.8713 train_time:1509402ms step_avg:170.30ms\nstep:8874/10000 train_loss:3.7890 train_time:1509570ms step_avg:170.30ms\nstep:8875/10000 train_loss:3.6452 train_time:1509742ms step_avg:170.30ms\nstep:8876/10000 train_loss:3.7236 train_time:1509912ms step_avg:170.30ms\nstep:8877/10000 train_loss:3.7967 train_time:1510087ms step_avg:170.30ms\nstep:8878/10000 train_loss:3.8819 train_time:1510256ms step_avg:170.30ms\nstep:8879/10000 train_loss:3.7347 train_time:1510428ms step_avg:170.30ms\nstep:8880/10000 train_loss:3.8625 train_time:1510597ms step_avg:170.30ms\nstep:8881/10000 train_loss:3.6525 train_time:1510765ms step_avg:170.30ms\nstep:8882/10000 train_loss:3.8920 train_time:1510934ms step_avg:170.30ms\nstep:8883/10000 train_loss:3.8086 train_time:1511107ms step_avg:170.30ms\nstep:8884/10000 train_loss:3.8261 train_time:1511279ms step_avg:170.30ms\nstep:8885/10000 train_loss:3.4710 train_time:1511449ms step_avg:170.30ms\nstep:8886/10000 train_loss:4.1359 train_time:1511623ms step_avg:170.30ms\nstep:8887/10000 train_loss:3.8455 train_time:1511795ms step_avg:170.30ms\nstep:8888/10000 train_loss:3.7689 train_time:1511964ms step_avg:170.30ms\nstep:8889/10000 train_loss:3.6857 train_time:1512134ms step_avg:170.30ms\nstep:8890/10000 train_loss:3.8472 train_time:1512305ms step_avg:170.30ms\nstep:8891/10000 train_loss:3.7858 train_time:1512475ms step_avg:170.30ms\nstep:8892/10000 train_loss:3.7788 train_time:1512647ms step_avg:170.30ms\nstep:8893/10000 train_loss:3.6742 train_time:1512819ms step_avg:170.30ms\nstep:8894/10000 train_loss:3.7940 train_time:1512990ms step_avg:170.31ms\nstep:8895/10000 train_loss:3.5809 train_time:1513160ms step_avg:170.31ms\nstep:8896/10000 train_loss:3.6846 train_time:1513329ms step_avg:170.30ms\nstep:8897/10000 train_loss:3.8008 train_time:1513498ms step_avg:170.30ms\nstep:8898/10000 train_loss:3.8023 train_time:1513669ms step_avg:170.30ms\nstep:8899/10000 train_loss:3.9611 train_time:1513841ms step_avg:170.30ms\nstep:8900/10000 train_loss:3.9579 train_time:1514011ms step_avg:170.30ms\nstep:8901/10000 train_loss:3.6938 train_time:1514185ms step_avg:170.31ms\nstep:8902/10000 train_loss:3.6702 train_time:1514356ms step_avg:170.31ms\nstep:8903/10000 train_loss:3.8012 train_time:1514526ms step_avg:170.31ms\nstep:8904/10000 train_loss:3.8655 train_time:1514697ms step_avg:170.31ms\nstep:8905/10000 train_loss:3.7855 train_time:1514867ms step_avg:170.31ms\nstep:8906/10000 train_loss:3.7434 train_time:1515037ms step_avg:170.31ms\nstep:8907/10000 train_loss:3.7365 train_time:1515207ms step_avg:170.31ms\nstep:8908/10000 train_loss:3.8905 train_time:1515379ms step_avg:170.31ms\nstep:8909/10000 train_loss:3.6882 train_time:1515546ms step_avg:170.31ms\nstep:8910/10000 train_loss:3.7326 train_time:1515717ms step_avg:170.31ms\nstep:8911/10000 train_loss:3.8590 train_time:1515884ms step_avg:170.30ms\nstep:8912/10000 train_loss:3.7137 train_time:1516053ms step_avg:170.30ms\nstep:8913/10000 train_loss:3.7140 train_time:1516223ms step_avg:170.30ms\nstep:8914/10000 train_loss:4.0733 train_time:1516392ms step_avg:170.30ms\nstep:8915/10000 train_loss:3.7176 train_time:1516564ms step_avg:170.30ms\nstep:8916/10000 train_loss:3.8717 train_time:1516735ms step_avg:170.30ms\nstep:8917/10000 train_loss:3.6883 train_time:1516906ms step_avg:170.30ms\nstep:8918/10000 train_loss:3.8240 train_time:1517078ms step_avg:170.31ms\nstep:8919/10000 train_loss:3.7545 train_time:1517246ms step_avg:170.30ms\nstep:8920/10000 train_loss:3.6267 train_time:1517416ms step_avg:170.30ms\nstep:8921/10000 train_loss:3.7389 train_time:1517584ms step_avg:170.30ms\nstep:8922/10000 train_loss:3.7442 train_time:1517754ms step_avg:170.30ms\nstep:8923/10000 train_loss:3.7477 train_time:1517926ms step_avg:170.30ms\nstep:8924/10000 train_loss:3.8677 train_time:1518098ms step_avg:170.30ms\nstep:8925/10000 train_loss:3.6506 train_time:1518267ms step_avg:170.30ms\nstep:8926/10000 train_loss:3.7839 train_time:1518438ms step_avg:170.30ms\nstep:8927/10000 train_loss:3.7413 train_time:1518607ms step_avg:170.30ms\nstep:8928/10000 train_loss:3.8327 train_time:1518776ms step_avg:170.30ms\nstep:8929/10000 train_loss:4.1296 train_time:1518948ms step_avg:170.30ms\nstep:8930/10000 train_loss:3.9960 train_time:1519117ms step_avg:170.30ms\nstep:8931/10000 train_loss:3.6152 train_time:1519285ms step_avg:170.30ms\nstep:8932/10000 train_loss:3.6282 train_time:1519458ms step_avg:170.30ms\nstep:8933/10000 train_loss:3.6654 train_time:1519626ms step_avg:170.30ms\nstep:8934/10000 train_loss:3.6963 train_time:1519797ms step_avg:170.30ms\nstep:8935/10000 train_loss:3.7449 train_time:1519968ms step_avg:170.30ms\nstep:8936/10000 train_loss:3.9665 train_time:1520140ms step_avg:170.30ms\nstep:8937/10000 train_loss:3.8253 train_time:1520307ms step_avg:170.30ms\nstep:8938/10000 train_loss:3.7775 train_time:1520479ms step_avg:170.30ms\nstep:8939/10000 train_loss:3.7259 train_time:1520648ms step_avg:170.30ms\nstep:8940/10000 train_loss:3.6780 train_time:1520817ms step_avg:170.30ms\nstep:8941/10000 train_loss:3.8793 train_time:1520988ms step_avg:170.30ms\nstep:8942/10000 train_loss:3.6613 train_time:1521161ms step_avg:170.30ms\nstep:8943/10000 train_loss:3.7942 train_time:1521329ms step_avg:170.30ms\nstep:8944/10000 train_loss:4.1246 train_time:1521501ms step_avg:170.30ms\nstep:8945/10000 train_loss:3.9246 train_time:1521672ms step_avg:170.30ms\nstep:8946/10000 train_loss:3.6986 train_time:1521840ms step_avg:170.30ms\nstep:8947/10000 train_loss:3.6438 train_time:1522009ms step_avg:170.30ms\nstep:8948/10000 train_loss:3.8388 train_time:1522181ms step_avg:170.30ms\nstep:8949/10000 train_loss:3.6554 train_time:1522347ms step_avg:170.30ms\nstep:8950/10000 train_loss:3.7314 train_time:1522519ms step_avg:170.30ms\nstep:8951/10000 train_loss:3.7578 train_time:1522689ms step_avg:170.30ms\nstep:8952/10000 train_loss:3.7485 train_time:1522859ms step_avg:170.30ms\nstep:8953/10000 train_loss:3.7805 train_time:1523027ms step_avg:170.30ms\nstep:8954/10000 train_loss:3.8249 train_time:1523198ms step_avg:170.30ms\nstep:8955/10000 train_loss:3.8046 train_time:1523367ms step_avg:170.30ms\nstep:8956/10000 train_loss:3.7980 train_time:1523538ms step_avg:170.30ms\nstep:8957/10000 train_loss:3.9194 train_time:1523708ms step_avg:170.30ms\nstep:8958/10000 train_loss:3.7685 train_time:1523877ms step_avg:170.30ms\nstep:8959/10000 train_loss:3.7658 train_time:1524046ms step_avg:170.30ms\nstep:8960/10000 train_loss:3.6316 train_time:1524216ms step_avg:170.30ms\nstep:8961/10000 train_loss:3.5775 train_time:1524386ms step_avg:170.30ms\nstep:8962/10000 train_loss:3.8046 train_time:1524557ms step_avg:170.30ms\nstep:8963/10000 train_loss:3.7613 train_time:1524725ms step_avg:170.30ms\nstep:8964/10000 train_loss:3.5841 train_time:1524895ms step_avg:170.30ms\nstep:8965/10000 train_loss:3.9454 train_time:1525062ms step_avg:170.30ms\nstep:8966/10000 train_loss:3.9338 train_time:1525231ms step_avg:170.30ms\nstep:8967/10000 train_loss:3.7258 train_time:1525401ms step_avg:170.30ms\nstep:8968/10000 train_loss:3.8186 train_time:1525573ms step_avg:170.30ms\nstep:8969/10000 train_loss:3.8171 train_time:1525744ms step_avg:170.30ms\nstep:8970/10000 train_loss:3.7470 train_time:1525914ms step_avg:170.30ms\nstep:8971/10000 train_loss:3.6352 train_time:1526082ms step_avg:170.30ms\nstep:8972/10000 train_loss:3.9033 train_time:1526255ms step_avg:170.30ms\nstep:8973/10000 train_loss:3.6317 train_time:1526418ms step_avg:170.30ms\nstep:8974/10000 train_loss:3.8049 train_time:1526586ms step_avg:170.30ms\nstep:8975/10000 train_loss:3.7794 train_time:1526758ms step_avg:170.30ms\nstep:8976/10000 train_loss:3.6280 train_time:1526926ms step_avg:170.30ms\nstep:8977/10000 train_loss:3.6860 train_time:1527096ms step_avg:170.30ms\nstep:8978/10000 train_loss:3.8087 train_time:1527264ms step_avg:170.30ms\nstep:8979/10000 train_loss:3.6293 train_time:1527433ms step_avg:170.30ms\nstep:8980/10000 train_loss:3.7238 train_time:1527608ms step_avg:170.30ms\nstep:8981/10000 train_loss:3.7559 train_time:1527782ms step_avg:170.30ms\nstep:8982/10000 train_loss:3.6174 train_time:1527953ms step_avg:170.30ms\nstep:8983/10000 train_loss:3.7448 train_time:1528127ms step_avg:170.30ms\nstep:8984/10000 train_loss:3.8614 train_time:1528296ms step_avg:170.30ms\nstep:8985/10000 train_loss:3.6694 train_time:1528465ms step_avg:170.30ms\nstep:8986/10000 train_loss:3.7618 train_time:1528633ms step_avg:170.30ms\nstep:8987/10000 train_loss:3.7966 train_time:1528805ms step_avg:170.30ms\nstep:8988/10000 train_loss:3.8412 train_time:1528977ms step_avg:170.30ms\nstep:8989/10000 train_loss:3.6174 train_time:1529146ms step_avg:170.30ms\nstep:8990/10000 train_loss:3.7925 train_time:1529318ms step_avg:170.30ms\nstep:8991/10000 train_loss:3.8582 train_time:1529487ms step_avg:170.30ms\nstep:8992/10000 train_loss:3.7275 train_time:1529660ms step_avg:170.30ms\nstep:8993/10000 train_loss:3.7679 train_time:1529828ms step_avg:170.30ms\nstep:8994/10000 train_loss:3.6934 train_time:1529999ms step_avg:170.30ms\nstep:8995/10000 train_loss:3.9994 train_time:1530168ms step_avg:170.30ms\nstep:8996/10000 train_loss:3.7537 train_time:1530340ms step_avg:170.30ms\nstep:8997/10000 train_loss:3.7078 train_time:1530510ms step_avg:170.30ms\nstep:8998/10000 train_loss:3.7108 train_time:1530683ms step_avg:170.30ms\nstep:8999/10000 train_loss:3.7204 train_time:1530855ms step_avg:170.30ms\nstep:9000/10000 train_loss:3.7826 train_time:1531022ms step_avg:170.30ms\nstep:9001/10000 train_loss:3.7145 train_time:1531192ms step_avg:170.30ms\nstep:9002/10000 train_loss:3.6482 train_time:1531362ms step_avg:170.30ms\nstep:9003/10000 train_loss:3.8103 train_time:1531533ms step_avg:170.30ms\nstep:9004/10000 train_loss:3.6761 train_time:1531702ms step_avg:170.30ms\nstep:9005/10000 train_loss:3.8412 train_time:1531874ms step_avg:170.30ms\nstep:9006/10000 train_loss:3.7366 train_time:1532045ms step_avg:170.30ms\nstep:9007/10000 train_loss:3.8092 train_time:1532217ms step_avg:170.30ms\nstep:9008/10000 train_loss:3.6527 train_time:1532384ms step_avg:170.30ms\nstep:9009/10000 train_loss:3.7788 train_time:1532554ms step_avg:170.30ms\nstep:9010/10000 train_loss:3.7656 train_time:1532723ms step_avg:170.30ms\nstep:9011/10000 train_loss:3.7182 train_time:1532895ms step_avg:170.30ms\nstep:9012/10000 train_loss:3.8878 train_time:1533065ms step_avg:170.30ms\nstep:9013/10000 train_loss:3.7501 train_time:1533236ms step_avg:170.30ms\nstep:9014/10000 train_loss:3.7272 train_time:1533405ms step_avg:170.30ms\nstep:9015/10000 train_loss:3.7959 train_time:1533577ms step_avg:170.30ms\nstep:9016/10000 train_loss:3.6321 train_time:1533747ms step_avg:170.30ms\nstep:9017/10000 train_loss:3.7898 train_time:1533920ms step_avg:170.30ms\nstep:9018/10000 train_loss:3.7318 train_time:1534089ms step_avg:170.30ms\nstep:9019/10000 train_loss:3.8543 train_time:1534259ms step_avg:170.30ms\nstep:9020/10000 train_loss:3.6276 train_time:1534429ms step_avg:170.30ms\nstep:9021/10000 train_loss:3.6349 train_time:1534598ms step_avg:170.30ms\nstep:9022/10000 train_loss:3.8606 train_time:1534768ms step_avg:170.30ms\nstep:9023/10000 train_loss:3.6402 train_time:1534939ms step_avg:170.30ms\nstep:9024/10000 train_loss:3.6877 train_time:1535109ms step_avg:170.30ms\nstep:9025/10000 train_loss:3.8275 train_time:1535279ms step_avg:170.30ms\nstep:9026/10000 train_loss:3.7640 train_time:1535449ms step_avg:170.30ms\nstep:9027/10000 train_loss:3.6680 train_time:1535621ms step_avg:170.30ms\nstep:9028/10000 train_loss:3.6933 train_time:1535790ms step_avg:170.30ms\nstep:9029/10000 train_loss:3.9498 train_time:1535960ms step_avg:170.30ms\nstep:9030/10000 train_loss:3.8664 train_time:1536132ms step_avg:170.30ms\nstep:9031/10000 train_loss:3.8948 train_time:1536312ms step_avg:170.30ms\nstep:9032/10000 train_loss:3.7037 train_time:1536483ms step_avg:170.30ms\nstep:9033/10000 train_loss:3.6961 train_time:1536646ms step_avg:170.30ms\nstep:9034/10000 train_loss:3.7086 train_time:1536817ms step_avg:170.30ms\nstep:9035/10000 train_loss:3.6547 train_time:1536983ms step_avg:170.30ms\nstep:9036/10000 train_loss:3.7716 train_time:1537160ms step_avg:170.30ms\nstep:9037/10000 train_loss:3.7292 train_time:1537325ms step_avg:170.30ms\nstep:9038/10000 train_loss:3.6196 train_time:1537497ms step_avg:170.30ms\nstep:9039/10000 train_loss:3.8052 train_time:1537668ms step_avg:170.30ms\nstep:9040/10000 train_loss:3.6908 train_time:1537841ms step_avg:170.30ms\nstep:9041/10000 train_loss:3.8122 train_time:1538010ms step_avg:170.30ms\nstep:9042/10000 train_loss:3.9064 train_time:1538182ms step_avg:170.30ms\nstep:9043/10000 train_loss:3.8272 train_time:1538353ms step_avg:170.30ms\nstep:9044/10000 train_loss:3.9339 train_time:1538526ms step_avg:170.30ms\nstep:9045/10000 train_loss:3.6316 train_time:1538696ms step_avg:170.30ms\nstep:9046/10000 train_loss:3.7056 train_time:1538867ms step_avg:170.30ms\nstep:9047/10000 train_loss:3.8131 train_time:1539037ms step_avg:170.30ms\nstep:9048/10000 train_loss:3.8440 train_time:1539205ms step_avg:170.30ms\nstep:9049/10000 train_loss:3.8271 train_time:1539375ms step_avg:170.30ms\nstep:9050/10000 train_loss:4.8913 train_time:1539543ms step_avg:170.30ms\nstep:9051/10000 train_loss:3.6289 train_time:1539714ms step_avg:170.30ms\nstep:9052/10000 train_loss:4.1243 train_time:1539885ms step_avg:170.30ms\nstep:9053/10000 train_loss:3.6825 train_time:1540054ms step_avg:170.30ms\nstep:9054/10000 train_loss:3.8848 train_time:1540228ms step_avg:170.30ms\nstep:9055/10000 train_loss:3.7617 train_time:1540401ms step_avg:170.30ms\nstep:9056/10000 train_loss:3.8095 train_time:1540571ms step_avg:170.30ms\nstep:9057/10000 train_loss:3.7158 train_time:1540743ms step_avg:170.30ms\nstep:9058/10000 train_loss:3.7812 train_time:1540913ms step_avg:170.30ms\nstep:9059/10000 train_loss:3.7980 train_time:1541081ms step_avg:170.30ms\nstep:9060/10000 train_loss:3.8758 train_time:1541252ms step_avg:170.30ms\nstep:9061/10000 train_loss:3.7991 train_time:1541424ms step_avg:170.30ms\nstep:9062/10000 train_loss:3.7294 train_time:1541596ms step_avg:170.30ms\nstep:9063/10000 train_loss:3.6083 train_time:1541766ms step_avg:170.30ms\nstep:9064/10000 train_loss:3.7980 train_time:1541935ms step_avg:170.30ms\nstep:9065/10000 train_loss:3.8570 train_time:1542109ms step_avg:170.30ms\nstep:9066/10000 train_loss:3.7255 train_time:1542280ms step_avg:170.30ms\nstep:9067/10000 train_loss:3.7544 train_time:1542449ms step_avg:170.30ms\nstep:9068/10000 train_loss:3.7971 train_time:1542619ms step_avg:170.30ms\nstep:9069/10000 train_loss:3.6911 train_time:1542789ms step_avg:170.30ms\nstep:9070/10000 train_loss:3.7537 train_time:1542959ms step_avg:170.30ms\nstep:9071/10000 train_loss:3.7555 train_time:1543131ms step_avg:170.30ms\nstep:9072/10000 train_loss:3.5219 train_time:1543302ms step_avg:170.30ms\nstep:9073/10000 train_loss:3.8289 train_time:1543470ms step_avg:170.30ms\nstep:9074/10000 train_loss:3.7801 train_time:1543640ms step_avg:170.30ms\nstep:9075/10000 train_loss:3.7935 train_time:1543808ms step_avg:170.30ms\nstep:9076/10000 train_loss:3.6880 train_time:1543976ms step_avg:170.30ms\nstep:9077/10000 train_loss:3.7158 train_time:1544147ms step_avg:170.30ms\nstep:9078/10000 train_loss:3.8208 train_time:1544320ms step_avg:170.30ms\nstep:9079/10000 train_loss:3.8171 train_time:1544489ms step_avg:170.30ms\nstep:9080/10000 train_loss:3.8785 train_time:1544661ms step_avg:170.30ms\nstep:9081/10000 train_loss:3.7169 train_time:1544830ms step_avg:170.30ms\nstep:9082/10000 train_loss:3.7181 train_time:1544999ms step_avg:170.30ms\nstep:9083/10000 train_loss:3.7906 train_time:1545166ms step_avg:170.30ms\nstep:9084/10000 train_loss:3.6095 train_time:1545336ms step_avg:170.30ms\nstep:9085/10000 train_loss:3.8151 train_time:1545506ms step_avg:170.30ms\nstep:9086/10000 train_loss:3.6803 train_time:1545678ms step_avg:170.30ms\nstep:9087/10000 train_loss:3.7941 train_time:1545847ms step_avg:170.30ms\nstep:9088/10000 train_loss:3.6840 train_time:1546017ms step_avg:170.30ms\nstep:9089/10000 train_loss:3.8223 train_time:1546193ms step_avg:170.30ms\nstep:9090/10000 train_loss:3.9033 train_time:1546359ms step_avg:170.30ms\nstep:9091/10000 train_loss:3.7340 train_time:1546526ms step_avg:170.30ms\nstep:9092/10000 train_loss:3.6514 train_time:1546697ms step_avg:170.30ms\nstep:9093/10000 train_loss:3.6833 train_time:1546866ms step_avg:170.30ms\nstep:9094/10000 train_loss:3.7500 train_time:1547038ms step_avg:170.30ms\nstep:9095/10000 train_loss:3.7347 train_time:1547205ms step_avg:170.30ms\nstep:9096/10000 train_loss:3.6260 train_time:1547374ms step_avg:170.30ms\nstep:9097/10000 train_loss:3.6669 train_time:1547544ms step_avg:170.30ms\nstep:9098/10000 train_loss:3.8177 train_time:1547717ms step_avg:170.30ms\nstep:9099/10000 train_loss:3.7624 train_time:1547887ms step_avg:170.30ms\nstep:9100/10000 train_loss:4.0271 train_time:1548056ms step_avg:170.30ms\nstep:9101/10000 train_loss:3.6480 train_time:1548225ms step_avg:170.30ms\nstep:9102/10000 train_loss:3.7298 train_time:1548395ms step_avg:170.30ms\nstep:9103/10000 train_loss:3.7453 train_time:1548566ms step_avg:170.30ms\nstep:9104/10000 train_loss:3.6782 train_time:1548737ms step_avg:170.30ms\nstep:9105/10000 train_loss:3.8887 train_time:1548907ms step_avg:170.30ms\nstep:9106/10000 train_loss:3.8483 train_time:1549077ms step_avg:170.30ms\nstep:9107/10000 train_loss:3.7812 train_time:1549246ms step_avg:170.30ms\nstep:9108/10000 train_loss:3.6952 train_time:1549417ms step_avg:170.30ms\nstep:9109/10000 train_loss:3.7550 train_time:1549586ms step_avg:170.30ms\nstep:9110/10000 train_loss:3.8281 train_time:1549754ms step_avg:170.30ms\nstep:9111/10000 train_loss:3.6756 train_time:1549924ms step_avg:170.30ms\nstep:9112/10000 train_loss:3.7766 train_time:1550097ms step_avg:170.30ms\nstep:9113/10000 train_loss:3.7542 train_time:1550266ms step_avg:170.30ms\nstep:9114/10000 train_loss:3.7434 train_time:1550437ms step_avg:170.30ms\nstep:9115/10000 train_loss:3.7943 train_time:1550608ms step_avg:170.30ms\nstep:9116/10000 train_loss:3.7245 train_time:1550779ms step_avg:170.30ms\nstep:9117/10000 train_loss:3.7300 train_time:1550946ms step_avg:170.30ms\nstep:9118/10000 train_loss:3.8552 train_time:1551120ms step_avg:170.30ms\nstep:9119/10000 train_loss:3.9507 train_time:1551287ms step_avg:170.30ms\nstep:9120/10000 train_loss:3.7394 train_time:1551459ms step_avg:170.30ms\nstep:9121/10000 train_loss:3.7324 train_time:1551628ms step_avg:170.30ms\nstep:9122/10000 train_loss:3.9134 train_time:1551799ms step_avg:170.30ms\nstep:9123/10000 train_loss:3.7331 train_time:1551968ms step_avg:170.30ms\nstep:9124/10000 train_loss:3.6189 train_time:1552138ms step_avg:170.30ms\nstep:9125/10000 train_loss:3.7004 train_time:1552308ms step_avg:170.30ms\nstep:9126/10000 train_loss:3.7168 train_time:1552480ms step_avg:170.30ms\nstep:9127/10000 train_loss:3.7659 train_time:1552650ms step_avg:170.30ms\nstep:9128/10000 train_loss:3.7327 train_time:1552820ms step_avg:170.30ms\nstep:9129/10000 train_loss:3.6837 train_time:1552988ms step_avg:170.30ms\nstep:9130/10000 train_loss:3.6492 train_time:1553157ms step_avg:170.30ms\nstep:9131/10000 train_loss:3.7829 train_time:1553325ms step_avg:170.30ms\nstep:9132/10000 train_loss:3.8638 train_time:1553495ms step_avg:170.30ms\nstep:9133/10000 train_loss:3.8000 train_time:1553664ms step_avg:170.30ms\nstep:9134/10000 train_loss:3.8531 train_time:1553836ms step_avg:170.30ms\nstep:9135/10000 train_loss:3.7277 train_time:1554007ms step_avg:170.30ms\nstep:9136/10000 train_loss:3.6522 train_time:1554178ms step_avg:170.30ms\nstep:9137/10000 train_loss:3.8807 train_time:1554347ms step_avg:170.30ms\nstep:9138/10000 train_loss:3.6917 train_time:1554518ms step_avg:170.30ms\nstep:9139/10000 train_loss:3.8161 train_time:1554688ms step_avg:170.30ms\nstep:9140/10000 train_loss:3.7106 train_time:1554861ms step_avg:170.30ms\nstep:9141/10000 train_loss:3.6686 train_time:1555030ms step_avg:170.30ms\nstep:9142/10000 train_loss:3.6245 train_time:1555199ms step_avg:170.30ms\nstep:9143/10000 train_loss:3.7615 train_time:1555369ms step_avg:170.30ms\nstep:9144/10000 train_loss:3.8564 train_time:1555758ms step_avg:170.33ms\nstep:9145/10000 train_loss:3.8630 train_time:1555923ms step_avg:170.33ms\nstep:9146/10000 train_loss:3.7258 train_time:1556092ms step_avg:170.33ms\nstep:9147/10000 train_loss:3.7743 train_time:1556259ms step_avg:170.32ms\nstep:9148/10000 train_loss:3.8429 train_time:1556427ms step_avg:170.32ms\nstep:9149/10000 train_loss:3.8180 train_time:1556598ms step_avg:170.32ms\nstep:9150/10000 train_loss:3.8697 train_time:1556782ms step_avg:170.33ms\nstep:9151/10000 train_loss:3.6960 train_time:1556950ms step_avg:170.33ms\nstep:9152/10000 train_loss:3.6689 train_time:1557122ms step_avg:170.33ms\nstep:9153/10000 train_loss:3.7706 train_time:1557291ms step_avg:170.33ms\nstep:9154/10000 train_loss:3.7437 train_time:1557462ms step_avg:170.33ms\nstep:9155/10000 train_loss:3.7698 train_time:1557633ms step_avg:170.33ms\nstep:9156/10000 train_loss:3.8097 train_time:1557809ms step_avg:170.33ms\nstep:9157/10000 train_loss:3.7657 train_time:1557981ms step_avg:170.33ms\nstep:9158/10000 train_loss:3.7542 train_time:1558151ms step_avg:170.33ms\nstep:9159/10000 train_loss:3.7697 train_time:1558319ms step_avg:170.33ms\nstep:9160/10000 train_loss:3.7455 train_time:1558488ms step_avg:170.33ms\nstep:9161/10000 train_loss:3.8699 train_time:1558662ms step_avg:170.33ms\nstep:9162/10000 train_loss:3.7606 train_time:1558834ms step_avg:170.33ms\nstep:9163/10000 train_loss:3.7476 train_time:1559007ms step_avg:170.33ms\nstep:9164/10000 train_loss:3.7359 train_time:1559177ms step_avg:170.33ms\nstep:9165/10000 train_loss:3.7882 train_time:1559346ms step_avg:170.33ms\nstep:9166/10000 train_loss:3.7639 train_time:1559516ms step_avg:170.33ms\nstep:9167/10000 train_loss:3.8325 train_time:1559686ms step_avg:170.33ms\nstep:9168/10000 train_loss:3.9402 train_time:1559858ms step_avg:170.33ms\nstep:9169/10000 train_loss:3.7616 train_time:1560028ms step_avg:170.33ms\nstep:9170/10000 train_loss:3.7124 train_time:1560198ms step_avg:170.33ms\nstep:9171/10000 train_loss:3.8178 train_time:1560365ms step_avg:170.33ms\nstep:9172/10000 train_loss:3.7111 train_time:1560536ms step_avg:170.33ms\nstep:9173/10000 train_loss:3.7962 train_time:1560704ms step_avg:170.33ms\nstep:9174/10000 train_loss:3.8547 train_time:1560877ms step_avg:170.33ms\nstep:9175/10000 train_loss:3.8295 train_time:1561047ms step_avg:170.33ms\nstep:9176/10000 train_loss:3.7582 train_time:1561216ms step_avg:170.33ms\nstep:9177/10000 train_loss:3.8011 train_time:1561386ms step_avg:170.33ms\nstep:9178/10000 train_loss:3.8417 train_time:1561558ms step_avg:170.33ms\nstep:9179/10000 train_loss:3.7890 train_time:1561725ms step_avg:170.33ms\nstep:9180/10000 train_loss:3.7069 train_time:1561895ms step_avg:170.33ms\nstep:9181/10000 train_loss:3.7274 train_time:1562066ms step_avg:170.33ms\nstep:9182/10000 train_loss:3.7029 train_time:1562236ms step_avg:170.33ms\nstep:9183/10000 train_loss:3.7064 train_time:1562406ms step_avg:170.33ms\nstep:9184/10000 train_loss:3.7439 train_time:1562578ms step_avg:170.33ms\nstep:9185/10000 train_loss:3.7290 train_time:1562750ms step_avg:170.33ms\nstep:9186/10000 train_loss:4.0347 train_time:1562924ms step_avg:170.33ms\nstep:9187/10000 train_loss:3.8026 train_time:1563094ms step_avg:170.33ms\nstep:9188/10000 train_loss:3.8035 train_time:1563265ms step_avg:170.33ms\nstep:9189/10000 train_loss:3.7400 train_time:1563435ms step_avg:170.33ms\nstep:9190/10000 train_loss:3.7108 train_time:1563604ms step_avg:170.33ms\nstep:9191/10000 train_loss:3.7311 train_time:1563776ms step_avg:170.33ms\nstep:9192/10000 train_loss:3.8640 train_time:1563948ms step_avg:170.33ms\nstep:9193/10000 train_loss:3.6883 train_time:1564119ms step_avg:170.33ms\nstep:9194/10000 train_loss:3.6809 train_time:1564287ms step_avg:170.33ms\nstep:9195/10000 train_loss:3.8333 train_time:1564456ms step_avg:170.33ms\nstep:9196/10000 train_loss:3.7890 train_time:1564626ms step_avg:170.33ms\nstep:9197/10000 train_loss:3.8717 train_time:1564797ms step_avg:170.33ms\nstep:9198/10000 train_loss:3.8360 train_time:1564965ms step_avg:170.33ms\nstep:9199/10000 train_loss:3.8127 train_time:1565136ms step_avg:170.33ms\nstep:9200/10000 train_loss:3.7348 train_time:1565306ms step_avg:170.33ms\nstep:9201/10000 train_loss:3.8231 train_time:1565477ms step_avg:170.33ms\nstep:9202/10000 train_loss:3.8062 train_time:1565645ms step_avg:170.33ms\nstep:9203/10000 train_loss:3.7262 train_time:1565816ms step_avg:170.33ms\nstep:9204/10000 train_loss:3.8680 train_time:1565987ms step_avg:170.33ms\nstep:9205/10000 train_loss:3.7789 train_time:1566157ms step_avg:170.33ms\nstep:9206/10000 train_loss:3.7316 train_time:1566327ms step_avg:170.33ms\nstep:9207/10000 train_loss:3.7646 train_time:1566499ms step_avg:170.33ms\nstep:9208/10000 train_loss:3.8609 train_time:1566669ms step_avg:170.33ms\nstep:9209/10000 train_loss:3.7833 train_time:1566841ms step_avg:170.33ms\nstep:9210/10000 train_loss:4.1263 train_time:1567010ms step_avg:170.33ms\nstep:9211/10000 train_loss:3.6132 train_time:1567179ms step_avg:170.33ms\nstep:9212/10000 train_loss:3.6285 train_time:1567350ms step_avg:170.33ms\nstep:9213/10000 train_loss:3.7437 train_time:1567517ms step_avg:170.33ms\nstep:9214/10000 train_loss:3.6840 train_time:1567686ms step_avg:170.33ms\nstep:9215/10000 train_loss:3.9074 train_time:1567859ms step_avg:170.33ms\nstep:9216/10000 train_loss:3.7213 train_time:1568027ms step_avg:170.33ms\nstep:9217/10000 train_loss:3.6556 train_time:1568197ms step_avg:170.33ms\nstep:9218/10000 train_loss:3.7049 train_time:1568368ms step_avg:170.33ms\nstep:9219/10000 train_loss:3.9179 train_time:1568538ms step_avg:170.33ms\nstep:9220/10000 train_loss:3.7886 train_time:1568708ms step_avg:170.33ms\nstep:9221/10000 train_loss:3.8083 train_time:1568880ms step_avg:170.33ms\nstep:9222/10000 train_loss:4.0215 train_time:1569051ms step_avg:170.33ms\nstep:9223/10000 train_loss:3.6817 train_time:1569226ms step_avg:170.33ms\nstep:9224/10000 train_loss:3.5933 train_time:1569393ms step_avg:170.33ms\nstep:9225/10000 train_loss:3.7813 train_time:1569565ms step_avg:170.33ms\nstep:9226/10000 train_loss:3.7423 train_time:1569737ms step_avg:170.33ms\nstep:9227/10000 train_loss:3.7713 train_time:1569907ms step_avg:170.33ms\nstep:9228/10000 train_loss:3.7352 train_time:1570078ms step_avg:170.33ms\nstep:9229/10000 train_loss:3.7603 train_time:1570243ms step_avg:170.33ms\nstep:9230/10000 train_loss:3.7300 train_time:1570413ms step_avg:170.33ms\nstep:9231/10000 train_loss:3.5597 train_time:1570583ms step_avg:170.33ms\nstep:9232/10000 train_loss:3.8925 train_time:1570753ms step_avg:170.33ms\nstep:9233/10000 train_loss:3.6827 train_time:1570924ms step_avg:170.33ms\nstep:9234/10000 train_loss:3.7374 train_time:1571096ms step_avg:170.33ms\nstep:9235/10000 train_loss:3.6701 train_time:1571265ms step_avg:170.33ms\nstep:9236/10000 train_loss:3.7393 train_time:1571433ms step_avg:170.33ms\nstep:9237/10000 train_loss:3.7696 train_time:1571603ms step_avg:170.33ms\nstep:9238/10000 train_loss:3.6824 train_time:1571772ms step_avg:170.33ms\nstep:9239/10000 train_loss:3.6962 train_time:1571944ms step_avg:170.33ms\nstep:9240/10000 train_loss:3.6882 train_time:1572114ms step_avg:170.33ms\nstep:9241/10000 train_loss:3.8566 train_time:1572284ms step_avg:170.33ms\nstep:9242/10000 train_loss:3.8182 train_time:1572450ms step_avg:170.33ms\nstep:9243/10000 train_loss:3.6891 train_time:1572622ms step_avg:170.33ms\nstep:9244/10000 train_loss:3.7486 train_time:1572792ms step_avg:170.33ms\nstep:9245/10000 train_loss:3.7435 train_time:1572965ms step_avg:170.33ms\nstep:9246/10000 train_loss:3.6829 train_time:1573138ms step_avg:170.33ms\nstep:9247/10000 train_loss:3.7350 train_time:1573308ms step_avg:170.33ms\nstep:9248/10000 train_loss:3.5962 train_time:1573478ms step_avg:170.33ms\nstep:9249/10000 train_loss:3.7473 train_time:1573647ms step_avg:170.33ms\nstep:9250/10000 train_loss:3.7205 train_time:1573817ms step_avg:170.33ms\nstep:9251/10000 train_loss:3.7418 train_time:1573987ms step_avg:170.33ms\nstep:9252/10000 train_loss:3.7928 train_time:1574159ms step_avg:170.33ms\nstep:9253/10000 train_loss:3.7483 train_time:1574330ms step_avg:170.33ms\nstep:9254/10000 train_loss:3.8681 train_time:1574503ms step_avg:170.33ms\nstep:9255/10000 train_loss:3.7712 train_time:1574670ms step_avg:170.33ms\nstep:9256/10000 train_loss:3.6432 train_time:1574841ms step_avg:170.33ms\nstep:9257/10000 train_loss:3.8877 train_time:1575011ms step_avg:170.33ms\nstep:9258/10000 train_loss:3.7843 train_time:1575183ms step_avg:170.33ms\nstep:9259/10000 train_loss:3.9338 train_time:1575355ms step_avg:170.33ms\nstep:9260/10000 train_loss:3.8272 train_time:1575524ms step_avg:170.33ms\nstep:9261/10000 train_loss:3.6776 train_time:1575695ms step_avg:170.33ms\nstep:9262/10000 train_loss:3.6661 train_time:1575862ms step_avg:170.33ms\nstep:9263/10000 train_loss:3.6491 train_time:1576030ms step_avg:170.33ms\nstep:9264/10000 train_loss:3.7957 train_time:1576200ms step_avg:170.33ms\nstep:9265/10000 train_loss:3.7272 train_time:1576370ms step_avg:170.33ms\nstep:9266/10000 train_loss:3.7697 train_time:1576541ms step_avg:170.33ms\nstep:9267/10000 train_loss:3.8396 train_time:1576712ms step_avg:170.33ms\nstep:9268/10000 train_loss:3.7259 train_time:1576883ms step_avg:170.33ms\nstep:9269/10000 train_loss:3.6789 train_time:1577052ms step_avg:170.33ms\nstep:9270/10000 train_loss:3.8842 train_time:1577224ms step_avg:170.33ms\nstep:9271/10000 train_loss:3.8343 train_time:1577395ms step_avg:170.33ms\nstep:9272/10000 train_loss:3.7473 train_time:1577564ms step_avg:170.33ms\nstep:9273/10000 train_loss:3.7491 train_time:1577736ms step_avg:170.33ms\nstep:9274/10000 train_loss:3.7383 train_time:1577905ms step_avg:170.33ms\nstep:9275/10000 train_loss:3.8376 train_time:1578076ms step_avg:170.33ms\nstep:9276/10000 train_loss:3.6453 train_time:1578245ms step_avg:170.33ms\nstep:9277/10000 train_loss:3.7500 train_time:1578414ms step_avg:170.33ms\nstep:9278/10000 train_loss:3.6346 train_time:1578586ms step_avg:170.33ms\nstep:9279/10000 train_loss:3.6665 train_time:1578757ms step_avg:170.33ms\nstep:9280/10000 train_loss:3.9615 train_time:1578925ms step_avg:170.33ms\nstep:9281/10000 train_loss:3.8039 train_time:1579097ms step_avg:170.33ms\nstep:9282/10000 train_loss:3.7366 train_time:1579266ms step_avg:170.33ms\nstep:9283/10000 train_loss:3.7901 train_time:1579435ms step_avg:170.33ms\nstep:9284/10000 train_loss:3.8312 train_time:1579604ms step_avg:170.33ms\nstep:9285/10000 train_loss:3.8705 train_time:1579775ms step_avg:170.33ms\nstep:9286/10000 train_loss:3.6251 train_time:1579946ms step_avg:170.33ms\nstep:9287/10000 train_loss:3.7879 train_time:1580116ms step_avg:170.33ms\nstep:9288/10000 train_loss:3.6748 train_time:1580285ms step_avg:170.33ms\nstep:9289/10000 train_loss:3.8285 train_time:1580458ms step_avg:170.33ms\nstep:9290/10000 train_loss:3.7194 train_time:1580626ms step_avg:170.33ms\nstep:9291/10000 train_loss:3.8256 train_time:1580797ms step_avg:170.33ms\nstep:9292/10000 train_loss:3.6467 train_time:1580966ms step_avg:170.33ms\nstep:9293/10000 train_loss:3.7331 train_time:1581138ms step_avg:170.33ms\nstep:9294/10000 train_loss:3.7717 train_time:1581305ms step_avg:170.33ms\nstep:9295/10000 train_loss:3.7407 train_time:1581477ms step_avg:170.33ms\nstep:9296/10000 train_loss:3.6965 train_time:1581646ms step_avg:170.33ms\nstep:9297/10000 train_loss:3.7792 train_time:1581816ms step_avg:170.33ms\nstep:9298/10000 train_loss:3.5525 train_time:1581987ms step_avg:170.33ms\nstep:9299/10000 train_loss:3.7456 train_time:1582156ms step_avg:170.33ms\nstep:9300/10000 train_loss:3.7446 train_time:1582327ms step_avg:170.33ms\nstep:9301/10000 train_loss:4.1682 train_time:1582497ms step_avg:170.33ms\nstep:9302/10000 train_loss:3.7291 train_time:1582663ms step_avg:170.33ms\nstep:9303/10000 train_loss:3.8253 train_time:1582831ms step_avg:170.33ms\nstep:9304/10000 train_loss:3.7014 train_time:1583000ms step_avg:170.32ms\nstep:9305/10000 train_loss:3.7997 train_time:1583172ms step_avg:170.33ms\nstep:9306/10000 train_loss:3.8135 train_time:1583344ms step_avg:170.33ms\nstep:9307/10000 train_loss:3.7496 train_time:1583513ms step_avg:170.33ms\nstep:9308/10000 train_loss:3.9710 train_time:1583682ms step_avg:170.33ms\nstep:9309/10000 train_loss:3.6832 train_time:1583852ms step_avg:170.32ms\nstep:9310/10000 train_loss:3.6908 train_time:1584023ms step_avg:170.33ms\nstep:9311/10000 train_loss:3.6783 train_time:1584193ms step_avg:170.32ms\nstep:9312/10000 train_loss:3.9296 train_time:1584361ms step_avg:170.32ms\nstep:9313/10000 train_loss:3.7985 train_time:1584531ms step_avg:170.32ms\nstep:9314/10000 train_loss:3.7278 train_time:1584704ms step_avg:170.32ms\nstep:9315/10000 train_loss:3.6996 train_time:1584875ms step_avg:170.33ms\nstep:9316/10000 train_loss:3.5527 train_time:1585047ms step_avg:170.33ms\nstep:9317/10000 train_loss:3.6377 train_time:1585225ms step_avg:170.33ms\nstep:9318/10000 train_loss:3.6650 train_time:1585394ms step_avg:170.33ms\nstep:9319/10000 train_loss:3.9122 train_time:1585568ms step_avg:170.33ms\nstep:9320/10000 train_loss:3.7730 train_time:1585737ms step_avg:170.33ms\nstep:9321/10000 train_loss:3.6671 train_time:1585908ms step_avg:170.33ms\nstep:9322/10000 train_loss:3.7819 train_time:1586078ms step_avg:170.33ms\nstep:9323/10000 train_loss:3.7334 train_time:1586243ms step_avg:170.33ms\nstep:9324/10000 train_loss:3.6355 train_time:1586411ms step_avg:170.33ms\nstep:9325/10000 train_loss:3.7810 train_time:1586578ms step_avg:170.33ms\nstep:9326/10000 train_loss:3.8055 train_time:1586747ms step_avg:170.32ms\nstep:9327/10000 train_loss:3.9339 train_time:1586918ms step_avg:170.32ms\nstep:9328/10000 train_loss:3.7871 train_time:1587088ms step_avg:170.32ms\nstep:9329/10000 train_loss:3.7115 train_time:1587263ms step_avg:170.33ms\nstep:9330/10000 train_loss:3.5829 train_time:1587436ms step_avg:170.33ms\nstep:9331/10000 train_loss:3.7262 train_time:1587598ms step_avg:170.32ms\nstep:9332/10000 train_loss:3.7565 train_time:1587768ms step_avg:170.32ms\nstep:9333/10000 train_loss:3.8136 train_time:1587940ms step_avg:170.32ms\nstep:9334/10000 train_loss:3.7047 train_time:1588108ms step_avg:170.32ms\nstep:9335/10000 train_loss:3.6833 train_time:1588279ms step_avg:170.32ms\nstep:9336/10000 train_loss:3.6918 train_time:1588450ms step_avg:170.32ms\nstep:9337/10000 train_loss:3.7118 train_time:1588624ms step_avg:170.33ms\nstep:9338/10000 train_loss:3.7425 train_time:1588793ms step_avg:170.33ms\nstep:9339/10000 train_loss:4.3457 train_time:1588965ms step_avg:170.33ms\nstep:9340/10000 train_loss:3.8811 train_time:1589139ms step_avg:170.33ms\nstep:9341/10000 train_loss:3.7893 train_time:1589309ms step_avg:170.33ms\nstep:9342/10000 train_loss:3.6659 train_time:1589480ms step_avg:170.33ms\nstep:9343/10000 train_loss:3.7835 train_time:1589649ms step_avg:170.33ms\nstep:9344/10000 train_loss:3.7691 train_time:1589820ms step_avg:170.33ms\nstep:9345/10000 train_loss:3.7874 train_time:1589990ms step_avg:170.33ms\nstep:9346/10000 train_loss:3.8272 train_time:1590163ms step_avg:170.33ms\nstep:9347/10000 train_loss:3.8083 train_time:1590333ms step_avg:170.33ms\nstep:9348/10000 train_loss:3.6591 train_time:1590503ms step_avg:170.33ms\nstep:9349/10000 train_loss:3.7539 train_time:1590672ms step_avg:170.33ms\nstep:9350/10000 train_loss:3.7914 train_time:1590841ms step_avg:170.33ms\nstep:9351/10000 train_loss:3.7483 train_time:1591011ms step_avg:170.33ms\nstep:9352/10000 train_loss:3.8486 train_time:1591181ms step_avg:170.33ms\nstep:9353/10000 train_loss:3.8325 train_time:1591351ms step_avg:170.33ms\nstep:9354/10000 train_loss:3.7259 train_time:1591521ms step_avg:170.33ms\nstep:9355/10000 train_loss:3.8330 train_time:1591692ms step_avg:170.33ms\nstep:9356/10000 train_loss:4.0744 train_time:1591861ms step_avg:170.33ms\nstep:9357/10000 train_loss:3.7857 train_time:1592029ms step_avg:170.33ms\nstep:9358/10000 train_loss:3.8133 train_time:1592199ms step_avg:170.33ms\nstep:9359/10000 train_loss:3.7153 train_time:1592368ms step_avg:170.32ms\nstep:9360/10000 train_loss:3.7169 train_time:1592542ms step_avg:170.33ms\nstep:9361/10000 train_loss:3.7429 train_time:1592708ms step_avg:170.32ms\nstep:9362/10000 train_loss:3.6616 train_time:1592878ms step_avg:170.32ms\nstep:9363/10000 train_loss:3.6738 train_time:1593047ms step_avg:170.32ms\nstep:9364/10000 train_loss:3.7565 train_time:1593216ms step_avg:170.32ms\nstep:9365/10000 train_loss:3.6041 train_time:1593383ms step_avg:170.32ms\nstep:9366/10000 train_loss:3.7877 train_time:1593554ms step_avg:170.32ms\nstep:9367/10000 train_loss:3.7285 train_time:1593723ms step_avg:170.32ms\nstep:9368/10000 train_loss:3.7423 train_time:1593894ms step_avg:170.32ms\nstep:9369/10000 train_loss:4.0104 train_time:1594067ms step_avg:170.32ms\nstep:9370/10000 train_loss:3.7482 train_time:1594239ms step_avg:170.32ms\nstep:9371/10000 train_loss:3.7066 train_time:1594406ms step_avg:170.32ms\nstep:9372/10000 train_loss:3.8422 train_time:1594577ms step_avg:170.32ms\nstep:9373/10000 train_loss:3.7544 train_time:1594746ms step_avg:170.32ms\nstep:9374/10000 train_loss:3.7357 train_time:1594917ms step_avg:170.32ms\nstep:9375/10000 train_loss:3.7645 train_time:1595085ms step_avg:170.32ms\nstep:9376/10000 train_loss:3.8681 train_time:1595256ms step_avg:170.32ms\nstep:9377/10000 train_loss:3.6621 train_time:1595425ms step_avg:170.32ms\nstep:9378/10000 train_loss:3.7955 train_time:1595595ms step_avg:170.32ms\nstep:9379/10000 train_loss:3.6441 train_time:1595767ms step_avg:170.32ms\nstep:9380/10000 train_loss:3.9491 train_time:1595941ms step_avg:170.32ms\nstep:9381/10000 train_loss:3.5947 train_time:1596112ms step_avg:170.32ms\nstep:9382/10000 train_loss:3.8315 train_time:1596282ms step_avg:170.32ms\nstep:9383/10000 train_loss:3.6424 train_time:1596450ms step_avg:170.32ms\nstep:9384/10000 train_loss:3.6294 train_time:1596620ms step_avg:170.32ms\nstep:9385/10000 train_loss:3.7168 train_time:1596791ms step_avg:170.32ms\nstep:9386/10000 train_loss:3.7324 train_time:1596963ms step_avg:170.32ms\nstep:9387/10000 train_loss:3.7598 train_time:1597135ms step_avg:170.32ms\nstep:9388/10000 train_loss:3.6793 train_time:1597305ms step_avg:170.32ms\nstep:9389/10000 train_loss:3.6904 train_time:1597476ms step_avg:170.32ms\nstep:9390/10000 train_loss:3.7042 train_time:1597648ms step_avg:170.32ms\nstep:9391/10000 train_loss:3.7607 train_time:1597827ms step_avg:170.33ms\nstep:9392/10000 train_loss:3.5648 train_time:1597988ms step_avg:170.32ms\nstep:9393/10000 train_loss:4.2468 train_time:1598160ms step_avg:170.33ms\nstep:9394/10000 train_loss:3.7736 train_time:1598327ms step_avg:170.32ms\nstep:9395/10000 train_loss:3.6997 train_time:1598498ms step_avg:170.32ms\nstep:9396/10000 train_loss:3.8501 train_time:1598666ms step_avg:170.32ms\nstep:9397/10000 train_loss:3.7774 train_time:1598838ms step_avg:170.32ms\nstep:9398/10000 train_loss:3.7316 train_time:1599009ms step_avg:170.32ms\nstep:9399/10000 train_loss:3.7760 train_time:1599183ms step_avg:170.33ms\nstep:9400/10000 train_loss:3.7059 train_time:1599353ms step_avg:170.33ms\nstep:9401/10000 train_loss:3.8765 train_time:1599525ms step_avg:170.33ms\nstep:9402/10000 train_loss:3.8618 train_time:1599696ms step_avg:170.33ms\nstep:9403/10000 train_loss:3.8121 train_time:1599866ms step_avg:170.33ms\nstep:9404/10000 train_loss:3.7901 train_time:1600036ms step_avg:170.33ms\nstep:9405/10000 train_loss:3.7864 train_time:1600204ms step_avg:170.33ms\nstep:9406/10000 train_loss:3.7785 train_time:1600375ms step_avg:170.33ms\nstep:9407/10000 train_loss:3.4793 train_time:1600542ms step_avg:170.32ms\nstep:9408/10000 train_loss:3.9494 train_time:1600713ms step_avg:170.32ms\nstep:9409/10000 train_loss:3.6690 train_time:1600887ms step_avg:170.33ms\nstep:9410/10000 train_loss:3.8472 train_time:1601059ms step_avg:170.33ms\nstep:9411/10000 train_loss:3.8523 train_time:1601227ms step_avg:170.33ms\nstep:9412/10000 train_loss:3.9345 train_time:1601396ms step_avg:170.33ms\nstep:9413/10000 train_loss:3.8062 train_time:1601567ms step_avg:170.33ms\nstep:9414/10000 train_loss:3.8082 train_time:1601738ms step_avg:170.33ms\nstep:9415/10000 train_loss:3.9359 train_time:1601906ms step_avg:170.32ms\nstep:9416/10000 train_loss:3.6273 train_time:1602077ms step_avg:170.33ms\nstep:9417/10000 train_loss:4.0131 train_time:1602248ms step_avg:170.33ms\nstep:9418/10000 train_loss:3.8542 train_time:1602417ms step_avg:170.32ms\nstep:9419/10000 train_loss:3.7558 train_time:1602585ms step_avg:170.32ms\nstep:9420/10000 train_loss:3.7043 train_time:1602755ms step_avg:170.32ms\nstep:9421/10000 train_loss:3.7519 train_time:1602927ms step_avg:170.32ms\nstep:9422/10000 train_loss:3.5418 train_time:1603100ms step_avg:170.33ms\nstep:9423/10000 train_loss:3.8107 train_time:1603269ms step_avg:170.32ms\nstep:9424/10000 train_loss:3.7438 train_time:1603440ms step_avg:170.33ms\nstep:9425/10000 train_loss:3.8189 train_time:1603610ms step_avg:170.32ms\nstep:9426/10000 train_loss:3.6799 train_time:1603781ms step_avg:170.33ms\nstep:9427/10000 train_loss:3.8044 train_time:1603949ms step_avg:170.32ms\nstep:9428/10000 train_loss:3.7284 train_time:1604119ms step_avg:170.32ms\nstep:9429/10000 train_loss:3.7367 train_time:1604286ms step_avg:170.32ms\nstep:9430/10000 train_loss:3.6583 train_time:1604456ms step_avg:170.32ms\nstep:9431/10000 train_loss:3.8449 train_time:1604627ms step_avg:170.32ms\nstep:9432/10000 train_loss:3.6930 train_time:1604798ms step_avg:170.32ms\nstep:9433/10000 train_loss:3.6295 train_time:1604969ms step_avg:170.32ms\nstep:9434/10000 train_loss:3.8677 train_time:1605140ms step_avg:170.32ms\nstep:9435/10000 train_loss:3.8712 train_time:1605307ms step_avg:170.32ms\nstep:9436/10000 train_loss:3.9009 train_time:1605479ms step_avg:170.32ms\nstep:9437/10000 train_loss:3.9094 train_time:1605649ms step_avg:170.32ms\nstep:9438/10000 train_loss:3.6858 train_time:1605820ms step_avg:170.32ms\nstep:9439/10000 train_loss:3.6197 train_time:1605989ms step_avg:170.32ms\nstep:9440/10000 train_loss:4.0083 train_time:1606159ms step_avg:170.32ms\nstep:9441/10000 train_loss:3.7038 train_time:1606328ms step_avg:170.32ms\nstep:9442/10000 train_loss:3.6803 train_time:1606500ms step_avg:170.32ms\nstep:9443/10000 train_loss:3.7106 train_time:1606670ms step_avg:170.32ms\nstep:9444/10000 train_loss:3.2456 train_time:1606839ms step_avg:170.32ms\nstep:9445/10000 train_loss:3.8420 train_time:1607010ms step_avg:170.32ms\nstep:9446/10000 train_loss:3.8194 train_time:1607180ms step_avg:170.32ms\nstep:9447/10000 train_loss:3.7221 train_time:1607350ms step_avg:170.32ms\nstep:9448/10000 train_loss:4.1560 train_time:1607523ms step_avg:170.32ms\nstep:9449/10000 train_loss:3.9007 train_time:1607694ms step_avg:170.32ms\nstep:9450/10000 train_loss:3.6952 train_time:1607866ms step_avg:170.32ms\nstep:9451/10000 train_loss:3.7870 train_time:1608036ms step_avg:170.32ms\nstep:9452/10000 train_loss:4.0554 train_time:1608206ms step_avg:170.32ms\nstep:9453/10000 train_loss:3.7634 train_time:1608375ms step_avg:170.32ms\nstep:9454/10000 train_loss:3.8068 train_time:1608543ms step_avg:170.32ms\nstep:9455/10000 train_loss:3.6796 train_time:1608713ms step_avg:170.32ms\nstep:9456/10000 train_loss:3.8564 train_time:1608881ms step_avg:170.32ms\nstep:9457/10000 train_loss:3.7280 train_time:1609051ms step_avg:170.32ms\nstep:9458/10000 train_loss:3.6654 train_time:1609220ms step_avg:170.32ms\nstep:9459/10000 train_loss:3.8734 train_time:1609392ms step_avg:170.32ms\nstep:9460/10000 train_loss:3.7784 train_time:1609562ms step_avg:170.32ms\nstep:9461/10000 train_loss:3.7722 train_time:1609732ms step_avg:170.32ms\nstep:9462/10000 train_loss:3.6911 train_time:1609902ms step_avg:170.32ms\nstep:9463/10000 train_loss:3.8316 train_time:1610075ms step_avg:170.32ms\nstep:9464/10000 train_loss:3.7786 train_time:1610242ms step_avg:170.32ms\nstep:9465/10000 train_loss:3.7003 train_time:1610412ms step_avg:170.32ms\nstep:9466/10000 train_loss:3.7689 train_time:1610583ms step_avg:170.32ms\nstep:9467/10000 train_loss:3.7881 train_time:1610753ms step_avg:170.32ms\nstep:9468/10000 train_loss:3.7705 train_time:1610924ms step_avg:170.32ms\nstep:9469/10000 train_loss:3.8201 train_time:1611096ms step_avg:170.32ms\nstep:9470/10000 train_loss:3.7484 train_time:1611266ms step_avg:170.32ms\nstep:9471/10000 train_loss:3.6914 train_time:1611435ms step_avg:170.32ms\nstep:9472/10000 train_loss:3.7575 train_time:1611605ms step_avg:170.32ms\nstep:9473/10000 train_loss:3.7341 train_time:1611775ms step_avg:170.32ms\nstep:9474/10000 train_loss:3.7326 train_time:1611943ms step_avg:170.32ms\nstep:9475/10000 train_loss:3.7854 train_time:1612111ms step_avg:170.32ms\nstep:9476/10000 train_loss:3.6819 train_time:1612283ms step_avg:170.32ms\nstep:9477/10000 train_loss:3.7071 train_time:1612452ms step_avg:170.32ms\nstep:9478/10000 train_loss:4.1628 train_time:1612623ms step_avg:170.32ms\nstep:9479/10000 train_loss:3.5315 train_time:1612793ms step_avg:170.32ms\nstep:9480/10000 train_loss:3.8698 train_time:1612967ms step_avg:170.32ms\nstep:9481/10000 train_loss:3.6306 train_time:1613138ms step_avg:170.32ms\nstep:9482/10000 train_loss:3.7917 train_time:1613309ms step_avg:170.32ms\nstep:9483/10000 train_loss:3.5432 train_time:1613479ms step_avg:170.32ms\nstep:9484/10000 train_loss:3.7281 train_time:1613649ms step_avg:170.32ms\nstep:9485/10000 train_loss:3.8073 train_time:1613821ms step_avg:170.32ms\nstep:9486/10000 train_loss:3.9576 train_time:1613991ms step_avg:170.32ms\nstep:9487/10000 train_loss:3.8375 train_time:1614169ms step_avg:170.32ms\nstep:9488/10000 train_loss:3.7355 train_time:1614337ms step_avg:170.32ms\nstep:9489/10000 train_loss:3.6116 train_time:1614507ms step_avg:170.32ms\nstep:9490/10000 train_loss:3.7940 train_time:1614677ms step_avg:170.32ms\nstep:9491/10000 train_loss:4.0219 train_time:1614854ms step_avg:170.33ms\nstep:9492/10000 train_loss:4.0884 train_time:1615018ms step_avg:170.32ms\nstep:9493/10000 train_loss:3.8831 train_time:1615188ms step_avg:170.32ms\nstep:9494/10000 train_loss:3.7582 train_time:1615358ms step_avg:170.32ms\nstep:9495/10000 train_loss:3.8006 train_time:1615526ms step_avg:170.32ms\nstep:9496/10000 train_loss:3.7418 train_time:1615697ms step_avg:170.32ms\nstep:9497/10000 train_loss:3.9214 train_time:1615867ms step_avg:170.32ms\nstep:9498/10000 train_loss:3.7985 train_time:1616038ms step_avg:170.32ms\nstep:9499/10000 train_loss:3.8581 train_time:1616208ms step_avg:170.32ms\nstep:9500/10000 train_loss:3.7184 train_time:1616379ms step_avg:170.32ms\nstep:9501/10000 train_loss:3.6776 train_time:1616549ms step_avg:170.32ms\nstep:9502/10000 train_loss:3.6879 train_time:1616722ms step_avg:170.32ms\nstep:9503/10000 train_loss:3.6571 train_time:1616892ms step_avg:170.32ms\nstep:9504/10000 train_loss:3.7255 train_time:1617063ms step_avg:170.32ms\nstep:9505/10000 train_loss:3.7660 train_time:1617231ms step_avg:170.32ms\nstep:9506/10000 train_loss:3.9030 train_time:1617400ms step_avg:170.32ms\nstep:9507/10000 train_loss:3.7346 train_time:1617569ms step_avg:170.32ms\nstep:9508/10000 train_loss:3.7840 train_time:1617738ms step_avg:170.32ms\nstep:9509/10000 train_loss:3.7711 train_time:1617906ms step_avg:170.32ms\nstep:9510/10000 train_loss:3.7540 train_time:1618079ms step_avg:170.32ms\nstep:9511/10000 train_loss:3.7614 train_time:1618250ms step_avg:170.32ms\nstep:9512/10000 train_loss:3.6417 train_time:1618424ms step_avg:170.32ms\nstep:9513/10000 train_loss:3.6400 train_time:1618596ms step_avg:170.32ms\nstep:9514/10000 train_loss:4.3044 train_time:1618766ms step_avg:170.32ms\nstep:9515/10000 train_loss:3.7103 train_time:1618936ms step_avg:170.32ms\nstep:9516/10000 train_loss:3.8005 train_time:1619107ms step_avg:170.32ms\nstep:9517/10000 train_loss:3.7387 train_time:1619279ms step_avg:170.32ms\nstep:9518/10000 train_loss:3.7158 train_time:1619447ms step_avg:170.32ms\nstep:9519/10000 train_loss:3.8236 train_time:1619616ms step_avg:170.32ms\nstep:9520/10000 train_loss:3.6901 train_time:1619785ms step_avg:170.32ms\nstep:9521/10000 train_loss:3.6857 train_time:1619958ms step_avg:170.32ms\nstep:9522/10000 train_loss:3.6510 train_time:1620127ms step_avg:170.32ms\nstep:9523/10000 train_loss:3.7674 train_time:1620296ms step_avg:170.32ms\nstep:9524/10000 train_loss:3.7396 train_time:1620467ms step_avg:170.32ms\nstep:9525/10000 train_loss:3.7168 train_time:1620637ms step_avg:170.32ms\nstep:9526/10000 train_loss:3.8260 train_time:1620805ms step_avg:170.32ms\nstep:9527/10000 train_loss:3.8884 train_time:1620979ms step_avg:170.32ms\nstep:9528/10000 train_loss:3.7755 train_time:1621147ms step_avg:170.32ms\nstep:9529/10000 train_loss:3.7891 train_time:1621316ms step_avg:170.32ms\nstep:9530/10000 train_loss:3.7752 train_time:1621485ms step_avg:170.32ms\nstep:9531/10000 train_loss:3.7676 train_time:1621657ms step_avg:170.32ms\nstep:9532/10000 train_loss:4.0506 train_time:1621825ms step_avg:170.32ms\nstep:9533/10000 train_loss:3.7304 train_time:1621997ms step_avg:170.32ms\nstep:9534/10000 train_loss:3.7861 train_time:1622168ms step_avg:170.32ms\nstep:9535/10000 train_loss:3.6873 train_time:1622338ms step_avg:170.32ms\nstep:9536/10000 train_loss:3.7474 train_time:1622509ms step_avg:170.32ms\nstep:9537/10000 train_loss:3.7609 train_time:1622679ms step_avg:170.32ms\nstep:9538/10000 train_loss:3.7915 train_time:1622847ms step_avg:170.32ms\nstep:9539/10000 train_loss:3.7163 train_time:1623020ms step_avg:170.32ms\nstep:9540/10000 train_loss:3.7929 train_time:1623190ms step_avg:170.32ms\nstep:9541/10000 train_loss:3.7106 train_time:1623361ms step_avg:170.32ms\nstep:9542/10000 train_loss:3.6838 train_time:1623529ms step_avg:170.32ms\nstep:9543/10000 train_loss:3.6892 train_time:1623698ms step_avg:170.32ms\nstep:9544/10000 train_loss:3.7970 train_time:1623868ms step_avg:170.32ms\nstep:9545/10000 train_loss:3.6845 train_time:1624039ms step_avg:170.32ms\nstep:9546/10000 train_loss:3.7522 train_time:1624208ms step_avg:170.32ms\nstep:9547/10000 train_loss:3.7824 train_time:1624378ms step_avg:170.32ms\nstep:9548/10000 train_loss:3.7256 train_time:1624547ms step_avg:170.32ms\nstep:9549/10000 train_loss:3.7781 train_time:1624716ms step_avg:170.32ms\nstep:9550/10000 train_loss:3.7830 train_time:1624886ms step_avg:170.32ms\nstep:9551/10000 train_loss:3.7461 train_time:1625058ms step_avg:170.32ms\nstep:9552/10000 train_loss:3.7636 train_time:1625228ms step_avg:170.32ms\nstep:9553/10000 train_loss:3.7571 train_time:1625399ms step_avg:170.32ms\nstep:9554/10000 train_loss:3.7496 train_time:1625569ms step_avg:170.32ms\nstep:9555/10000 train_loss:3.7518 train_time:1625740ms step_avg:170.32ms\nstep:9556/10000 train_loss:3.5986 train_time:1625908ms step_avg:170.32ms\nstep:9557/10000 train_loss:3.8450 train_time:1626086ms step_avg:170.32ms\nstep:9558/10000 train_loss:3.6627 train_time:1626253ms step_avg:170.32ms\nstep:9559/10000 train_loss:3.6780 train_time:1626424ms step_avg:170.32ms\nstep:9560/10000 train_loss:3.7953 train_time:1626593ms step_avg:170.32ms\nstep:9561/10000 train_loss:3.8292 train_time:1626763ms step_avg:170.32ms\nstep:9562/10000 train_loss:3.7748 train_time:1626934ms step_avg:170.32ms\nstep:9563/10000 train_loss:3.6807 train_time:1627105ms step_avg:170.32ms\nstep:9564/10000 train_loss:3.8000 train_time:1627276ms step_avg:170.32ms\nstep:9565/10000 train_loss:3.6499 train_time:1627445ms step_avg:170.32ms\nstep:9566/10000 train_loss:3.7420 train_time:1627617ms step_avg:170.32ms\nstep:9567/10000 train_loss:3.7549 train_time:1627788ms step_avg:170.32ms\nstep:9568/10000 train_loss:3.7626 train_time:1627955ms step_avg:170.32ms\nstep:9569/10000 train_loss:3.6938 train_time:1628122ms step_avg:170.32ms\nstep:9570/10000 train_loss:3.7325 train_time:1628294ms step_avg:170.32ms\nstep:9571/10000 train_loss:3.7208 train_time:1628464ms step_avg:170.32ms\nstep:9572/10000 train_loss:3.7713 train_time:1628632ms step_avg:170.32ms\nstep:9573/10000 train_loss:3.7482 train_time:1628802ms step_avg:170.32ms\nstep:9574/10000 train_loss:3.7500 train_time:1628974ms step_avg:170.32ms\nstep:9575/10000 train_loss:3.6984 train_time:1629143ms step_avg:170.32ms\nstep:9576/10000 train_loss:3.8174 train_time:1629313ms step_avg:170.32ms\nstep:9577/10000 train_loss:3.7310 train_time:1629486ms step_avg:170.32ms\nstep:9578/10000 train_loss:3.7538 train_time:1629656ms step_avg:170.32ms\nstep:9579/10000 train_loss:3.7536 train_time:1629826ms step_avg:170.32ms\nstep:9580/10000 train_loss:3.7605 train_time:1629996ms step_avg:170.32ms\nstep:9581/10000 train_loss:3.8047 train_time:1630169ms step_avg:170.32ms\nstep:9582/10000 train_loss:3.7610 train_time:1630338ms step_avg:170.32ms\nstep:9583/10000 train_loss:3.7186 train_time:1630507ms step_avg:170.32ms\nstep:9584/10000 train_loss:3.7428 train_time:1630681ms step_avg:170.32ms\nstep:9585/10000 train_loss:3.9083 train_time:1630850ms step_avg:170.32ms\nstep:9586/10000 train_loss:3.6217 train_time:1631021ms step_avg:170.32ms\nstep:9587/10000 train_loss:3.8226 train_time:1631189ms step_avg:170.32ms\nstep:9588/10000 train_loss:3.8472 train_time:1631360ms step_avg:170.32ms\nstep:9589/10000 train_loss:3.7767 train_time:1631527ms step_avg:170.32ms\nstep:9590/10000 train_loss:3.8261 train_time:1631699ms step_avg:170.32ms\nstep:9591/10000 train_loss:3.7117 train_time:1631870ms step_avg:170.32ms\nstep:9592/10000 train_loss:3.7946 train_time:1632042ms step_avg:170.32ms\nstep:9593/10000 train_loss:3.5931 train_time:1632210ms step_avg:170.32ms\nstep:9594/10000 train_loss:3.7167 train_time:1632379ms step_avg:170.32ms\nstep:9595/10000 train_loss:3.7323 train_time:1632549ms step_avg:170.32ms\nstep:9596/10000 train_loss:3.8146 train_time:1632717ms step_avg:170.32ms\nstep:9597/10000 train_loss:3.8149 train_time:1632888ms step_avg:170.32ms\nstep:9598/10000 train_loss:3.7778 train_time:1633059ms step_avg:170.32ms\nstep:9599/10000 train_loss:3.7914 train_time:1633229ms step_avg:170.32ms\nstep:9600/10000 train_loss:3.8773 train_time:1633397ms step_avg:170.32ms\nstep:9601/10000 train_loss:3.8711 train_time:1633571ms step_avg:170.32ms\nstep:9602/10000 train_loss:3.9057 train_time:1633738ms step_avg:170.32ms\nstep:9603/10000 train_loss:3.8159 train_time:1633908ms step_avg:170.32ms\nstep:9604/10000 train_loss:3.4519 train_time:1634080ms step_avg:170.32ms\nstep:9605/10000 train_loss:3.7007 train_time:1634251ms step_avg:170.32ms\nstep:9606/10000 train_loss:3.9244 train_time:1634423ms step_avg:170.32ms\nstep:9607/10000 train_loss:3.8056 train_time:1634594ms step_avg:170.32ms\nstep:9608/10000 train_loss:3.7730 train_time:1634766ms step_avg:170.32ms\nstep:9609/10000 train_loss:3.7129 train_time:1634937ms step_avg:170.32ms\nstep:9610/10000 train_loss:3.8486 train_time:1635106ms step_avg:170.32ms\nstep:9611/10000 train_loss:3.7642 train_time:1635276ms step_avg:170.32ms\nstep:9612/10000 train_loss:3.7156 train_time:1635445ms step_avg:170.32ms\nstep:9613/10000 train_loss:3.9398 train_time:1635617ms step_avg:170.32ms\nstep:9614/10000 train_loss:3.8319 train_time:1635789ms step_avg:170.32ms\nstep:9615/10000 train_loss:3.7307 train_time:1635959ms step_avg:170.32ms\nstep:9616/10000 train_loss:3.5710 train_time:1636132ms step_avg:170.32ms\nstep:9617/10000 train_loss:3.8236 train_time:1636299ms step_avg:170.32ms\nstep:9618/10000 train_loss:3.7259 train_time:1636467ms step_avg:170.32ms\nstep:9619/10000 train_loss:3.7509 train_time:1636638ms step_avg:170.32ms\nstep:9620/10000 train_loss:3.5351 train_time:1636808ms step_avg:170.32ms\nstep:9621/10000 train_loss:3.6518 train_time:1636981ms step_avg:170.32ms\nstep:9622/10000 train_loss:3.6922 train_time:1637152ms step_avg:170.32ms\nstep:9623/10000 train_loss:3.8009 train_time:1637324ms step_avg:170.32ms\nstep:9624/10000 train_loss:3.6938 train_time:1637495ms step_avg:170.32ms\nstep:9625/10000 train_loss:3.7647 train_time:1637666ms step_avg:170.32ms\nstep:9626/10000 train_loss:3.8205 train_time:1637835ms step_avg:170.32ms\nstep:9627/10000 train_loss:3.7249 train_time:1638005ms step_avg:170.32ms\nstep:9628/10000 train_loss:3.7450 train_time:1638175ms step_avg:170.32ms\nstep:9629/10000 train_loss:3.6808 train_time:1638344ms step_avg:170.32ms\nstep:9630/10000 train_loss:3.6600 train_time:1638513ms step_avg:170.32ms\nstep:9631/10000 train_loss:3.6593 train_time:1638682ms step_avg:170.32ms\nstep:9632/10000 train_loss:3.7616 train_time:1638851ms step_avg:170.32ms\nstep:9633/10000 train_loss:3.7107 train_time:1639020ms step_avg:170.32ms\nstep:9634/10000 train_loss:3.7967 train_time:1639191ms step_avg:170.32ms\nstep:9635/10000 train_loss:3.6984 train_time:1639362ms step_avg:170.32ms\nstep:9636/10000 train_loss:3.7425 train_time:1639532ms step_avg:170.32ms\nstep:9637/10000 train_loss:3.7249 train_time:1639707ms step_avg:170.32ms\nstep:9638/10000 train_loss:3.9895 train_time:1639877ms step_avg:170.32ms\nstep:9639/10000 train_loss:3.8221 train_time:1640046ms step_avg:170.32ms\nstep:9640/10000 train_loss:3.7200 train_time:1640218ms step_avg:170.32ms\nstep:9641/10000 train_loss:3.9446 train_time:1640385ms step_avg:170.32ms\nstep:9642/10000 train_loss:3.7593 train_time:1640554ms step_avg:170.32ms\nstep:9643/10000 train_loss:3.7898 train_time:1640721ms step_avg:170.32ms\nstep:9644/10000 train_loss:3.7376 train_time:1640891ms step_avg:170.32ms\nstep:9645/10000 train_loss:3.7167 train_time:1641064ms step_avg:170.32ms\nstep:9646/10000 train_loss:3.8650 train_time:1641236ms step_avg:170.32ms\nstep:9647/10000 train_loss:3.7011 train_time:1641407ms step_avg:170.32ms\nstep:9648/10000 train_loss:3.8934 train_time:1641578ms step_avg:170.32ms\nstep:9649/10000 train_loss:3.7800 train_time:1641745ms step_avg:170.32ms\nstep:9650/10000 train_loss:3.7347 train_time:1641916ms step_avg:170.32ms\nstep:9651/10000 train_loss:3.6249 train_time:1642087ms step_avg:170.32ms\nstep:9652/10000 train_loss:3.7329 train_time:1642258ms step_avg:170.32ms\nstep:9653/10000 train_loss:3.5892 train_time:1642428ms step_avg:170.32ms\nstep:9654/10000 train_loss:3.6967 train_time:1642600ms step_avg:170.32ms\nstep:9655/10000 train_loss:3.7417 train_time:1642769ms step_avg:170.32ms\nstep:9656/10000 train_loss:3.8281 train_time:1642942ms step_avg:170.32ms\nstep:9657/10000 train_loss:3.7115 train_time:1643112ms step_avg:170.32ms\nstep:9658/10000 train_loss:3.8010 train_time:1643283ms step_avg:170.32ms\nstep:9659/10000 train_loss:3.7300 train_time:1643452ms step_avg:170.32ms\nstep:9660/10000 train_loss:3.6680 train_time:1643622ms step_avg:170.32ms\nstep:9661/10000 train_loss:3.6498 train_time:1643795ms step_avg:170.32ms\nstep:9662/10000 train_loss:3.6304 train_time:1643967ms step_avg:170.32ms\nstep:9663/10000 train_loss:3.7995 train_time:1644141ms step_avg:170.32ms\nstep:9664/10000 train_loss:3.9498 train_time:1644310ms step_avg:170.32ms\nstep:9665/10000 train_loss:3.7442 train_time:1644488ms step_avg:170.33ms\nstep:9666/10000 train_loss:4.0145 train_time:1644651ms step_avg:170.32ms\nstep:9667/10000 train_loss:3.7417 train_time:1644829ms step_avg:170.33ms\nstep:9668/10000 train_loss:3.7735 train_time:1644996ms step_avg:170.32ms\nstep:9669/10000 train_loss:3.7899 train_time:1645165ms step_avg:170.32ms\nstep:9670/10000 train_loss:3.7027 train_time:1645334ms step_avg:170.32ms\nstep:9671/10000 train_loss:3.5599 train_time:1645506ms step_avg:170.32ms\nstep:9672/10000 train_loss:3.7993 train_time:1645677ms step_avg:170.32ms\nstep:9673/10000 train_loss:3.6414 train_time:1645846ms step_avg:170.32ms\nstep:9674/10000 train_loss:3.7677 train_time:1646017ms step_avg:170.32ms\nstep:9675/10000 train_loss:3.6981 train_time:1646190ms step_avg:170.32ms\nstep:9676/10000 train_loss:3.6986 train_time:1646360ms step_avg:170.32ms\nstep:9677/10000 train_loss:3.8247 train_time:1646531ms step_avg:170.32ms\nstep:9678/10000 train_loss:3.7137 train_time:1646704ms step_avg:170.33ms\nstep:9679/10000 train_loss:3.7986 train_time:1646873ms step_avg:170.33ms\nstep:9680/10000 train_loss:3.9041 train_time:1647045ms step_avg:170.33ms\nstep:9681/10000 train_loss:4.1586 train_time:1647215ms step_avg:170.33ms\nstep:9682/10000 train_loss:3.7764 train_time:1647392ms step_avg:170.33ms\nstep:9683/10000 train_loss:3.7962 train_time:1647561ms step_avg:170.33ms\nstep:9684/10000 train_loss:3.7622 train_time:1647731ms step_avg:170.33ms\nstep:9685/10000 train_loss:3.8081 train_time:1647899ms step_avg:170.33ms\nstep:9686/10000 train_loss:3.6290 train_time:1648063ms step_avg:170.32ms\nstep:9687/10000 train_loss:3.8247 train_time:1648234ms step_avg:170.32ms\nstep:9688/10000 train_loss:3.8240 train_time:1648404ms step_avg:170.32ms\nstep:9689/10000 train_loss:3.6964 train_time:1648574ms step_avg:170.32ms\nstep:9690/10000 train_loss:3.5651 train_time:1648742ms step_avg:170.32ms\nstep:9691/10000 train_loss:3.6807 train_time:1648913ms step_avg:170.32ms\nstep:9692/10000 train_loss:3.7698 train_time:1649087ms step_avg:170.32ms\nstep:9693/10000 train_loss:3.8240 train_time:1649260ms step_avg:170.33ms\nstep:9694/10000 train_loss:3.8059 train_time:1649429ms step_avg:170.33ms\nstep:9695/10000 train_loss:3.6421 train_time:1649606ms step_avg:170.33ms\nstep:9696/10000 train_loss:3.6697 train_time:1649776ms step_avg:170.33ms\nstep:9697/10000 train_loss:3.7997 train_time:1649944ms step_avg:170.33ms\nstep:9698/10000 train_loss:3.6961 train_time:1650108ms step_avg:170.32ms\nstep:9699/10000 train_loss:3.8639 train_time:1650279ms step_avg:170.33ms\nstep:9700/10000 train_loss:3.7855 train_time:1650450ms step_avg:170.33ms\nstep:9701/10000 train_loss:3.7840 train_time:1650622ms step_avg:170.33ms\nstep:9702/10000 train_loss:3.7584 train_time:1650790ms step_avg:170.33ms\nstep:9703/10000 train_loss:3.6804 train_time:1650963ms step_avg:170.33ms\nstep:9704/10000 train_loss:3.7028 train_time:1651132ms step_avg:170.33ms\nstep:9705/10000 train_loss:3.7231 train_time:1651303ms step_avg:170.33ms\nstep:9706/10000 train_loss:3.8243 train_time:1651474ms step_avg:170.33ms\nstep:9707/10000 train_loss:3.8325 train_time:1651645ms step_avg:170.33ms\nstep:9708/10000 train_loss:4.2369 train_time:1651814ms step_avg:170.33ms\nstep:9709/10000 train_loss:3.9034 train_time:1651985ms step_avg:170.33ms\nstep:9710/10000 train_loss:3.8992 train_time:1652154ms step_avg:170.33ms\nstep:9711/10000 train_loss:3.7266 train_time:1652326ms step_avg:170.33ms\nstep:9712/10000 train_loss:3.6187 train_time:1652496ms step_avg:170.33ms\nstep:9713/10000 train_loss:4.4104 train_time:1652667ms step_avg:170.33ms\nstep:9714/10000 train_loss:3.7105 train_time:1652837ms step_avg:170.33ms\nstep:9715/10000 train_loss:3.7830 train_time:1653008ms step_avg:170.33ms\nstep:9716/10000 train_loss:3.8365 train_time:1653179ms step_avg:170.33ms\nstep:9717/10000 train_loss:3.8824 train_time:1653349ms step_avg:170.33ms\nstep:9718/10000 train_loss:3.7930 train_time:1653519ms step_avg:170.33ms\nstep:9719/10000 train_loss:3.7667 train_time:1653691ms step_avg:170.33ms\nstep:9720/10000 train_loss:3.8725 train_time:1653861ms step_avg:170.33ms\nstep:9721/10000 train_loss:3.8257 train_time:1654030ms step_avg:170.33ms\nstep:9722/10000 train_loss:3.7931 train_time:1654201ms step_avg:170.33ms\nstep:9723/10000 train_loss:3.7169 train_time:1654369ms step_avg:170.33ms\nstep:9724/10000 train_loss:3.6130 train_time:1654541ms step_avg:170.33ms\nstep:9725/10000 train_loss:3.8209 train_time:1654711ms step_avg:170.33ms\nstep:9726/10000 train_loss:3.7374 train_time:1654883ms step_avg:170.33ms\nstep:9727/10000 train_loss:3.6588 train_time:1655051ms step_avg:170.33ms\nstep:9728/10000 train_loss:3.8033 train_time:1655223ms step_avg:170.33ms\nstep:9729/10000 train_loss:3.6687 train_time:1655395ms step_avg:170.33ms\nstep:9730/10000 train_loss:3.6825 train_time:1655564ms step_avg:170.33ms\nstep:9731/10000 train_loss:3.7216 train_time:1655733ms step_avg:170.33ms\nstep:9732/10000 train_loss:4.2061 train_time:1655906ms step_avg:170.33ms\nstep:9733/10000 train_loss:3.8607 train_time:1656076ms step_avg:170.33ms\nstep:9734/10000 train_loss:3.7819 train_time:1656246ms step_avg:170.33ms\nstep:9735/10000 train_loss:3.6685 train_time:1656419ms step_avg:170.33ms\nstep:9736/10000 train_loss:3.6927 train_time:1656589ms step_avg:170.33ms\nstep:9737/10000 train_loss:3.7133 train_time:1656761ms step_avg:170.33ms\nstep:9738/10000 train_loss:3.7188 train_time:1656929ms step_avg:170.33ms\nstep:9739/10000 train_loss:3.7683 train_time:1657100ms step_avg:170.33ms\nstep:9740/10000 train_loss:3.7264 train_time:1657269ms step_avg:170.33ms\nstep:9741/10000 train_loss:3.6448 train_time:1657439ms step_avg:170.33ms\nstep:9742/10000 train_loss:3.7514 train_time:1657613ms step_avg:170.33ms\nstep:9743/10000 train_loss:3.7425 train_time:1657782ms step_avg:170.33ms\nstep:9744/10000 train_loss:3.7619 train_time:1657955ms step_avg:170.33ms\nstep:9745/10000 train_loss:3.8258 train_time:1658124ms step_avg:170.33ms\nstep:9746/10000 train_loss:3.8013 train_time:1658294ms step_avg:170.33ms\nstep:9747/10000 train_loss:3.7122 train_time:1658464ms step_avg:170.33ms\nstep:9748/10000 train_loss:4.2772 train_time:1658636ms step_avg:170.33ms\nstep:9749/10000 train_loss:3.9295 train_time:1658805ms step_avg:170.33ms\nstep:9750/10000 train_loss:3.7586 train_time:1658976ms step_avg:170.33ms\nstep:9751/10000 train_loss:3.7227 train_time:1659145ms step_avg:170.33ms\nstep:9752/10000 train_loss:3.6360 train_time:1659317ms step_avg:170.33ms\nstep:9753/10000 train_loss:3.9591 train_time:1659485ms step_avg:170.33ms\nstep:9754/10000 train_loss:3.6584 train_time:1659657ms step_avg:170.33ms\nstep:9755/10000 train_loss:3.6103 train_time:1659826ms step_avg:170.33ms\nstep:9756/10000 train_loss:3.7680 train_time:1659995ms step_avg:170.33ms\nstep:9757/10000 train_loss:3.8573 train_time:1660164ms step_avg:170.33ms\nstep:9758/10000 train_loss:3.6215 train_time:1660335ms step_avg:170.33ms\nstep:9759/10000 train_loss:3.7706 train_time:1660503ms step_avg:170.33ms\nstep:9760/10000 train_loss:3.8008 train_time:1660671ms step_avg:170.33ms\nstep:9761/10000 train_loss:3.7787 train_time:1660841ms step_avg:170.33ms\nstep:9762/10000 train_loss:3.7631 train_time:1661010ms step_avg:170.33ms\nstep:9763/10000 train_loss:3.8275 train_time:1661182ms step_avg:170.33ms\nstep:9764/10000 train_loss:3.7636 train_time:1661351ms step_avg:170.33ms\nstep:9765/10000 train_loss:3.6589 train_time:1661521ms step_avg:170.33ms\nstep:9766/10000 train_loss:3.6759 train_time:1661693ms step_avg:170.33ms\nstep:9767/10000 train_loss:3.7571 train_time:1661866ms step_avg:170.33ms\nstep:9768/10000 train_loss:3.7361 train_time:1662037ms step_avg:170.33ms\nstep:9769/10000 train_loss:3.8864 train_time:1662207ms step_avg:170.33ms\nstep:9770/10000 train_loss:3.7124 train_time:1662377ms step_avg:170.33ms\nstep:9771/10000 train_loss:3.7357 train_time:1662544ms step_avg:170.33ms\nstep:9772/10000 train_loss:3.7342 train_time:1662713ms step_avg:170.33ms\nstep:9773/10000 train_loss:3.8030 train_time:1662883ms step_avg:170.33ms\nstep:9774/10000 train_loss:3.7735 train_time:1663053ms step_avg:170.32ms\nstep:9775/10000 train_loss:3.6961 train_time:1663227ms step_avg:170.33ms\nstep:9776/10000 train_loss:3.6926 train_time:1663400ms step_avg:170.33ms\nstep:9777/10000 train_loss:3.6498 train_time:1663568ms step_avg:170.33ms\nstep:9778/10000 train_loss:3.7035 train_time:1663737ms step_avg:170.33ms\nstep:9779/10000 train_loss:3.8897 train_time:1663907ms step_avg:170.33ms\nstep:9780/10000 train_loss:3.7982 train_time:1664076ms step_avg:170.33ms\nstep:9781/10000 train_loss:3.6250 train_time:1664244ms step_avg:170.32ms\nstep:9782/10000 train_loss:3.8087 train_time:1664417ms step_avg:170.33ms\nstep:9783/10000 train_loss:3.7141 train_time:1664586ms step_avg:170.32ms\nstep:9784/10000 train_loss:3.6780 train_time:1664756ms step_avg:170.32ms\nstep:9785/10000 train_loss:3.9026 train_time:1664924ms step_avg:170.32ms\nstep:9786/10000 train_loss:3.6906 train_time:1665095ms step_avg:170.32ms\nstep:9787/10000 train_loss:3.6726 train_time:1665266ms step_avg:170.32ms\nstep:9788/10000 train_loss:3.7391 train_time:1665439ms step_avg:170.33ms\nstep:9789/10000 train_loss:3.7208 train_time:1665609ms step_avg:170.33ms\nstep:9790/10000 train_loss:3.7703 train_time:1665781ms step_avg:170.33ms\nstep:9791/10000 train_loss:3.7837 train_time:1665950ms step_avg:170.33ms\nstep:9792/10000 train_loss:3.8541 train_time:1666120ms step_avg:170.33ms\nstep:9793/10000 train_loss:3.6885 train_time:1666289ms step_avg:170.32ms\nstep:9794/10000 train_loss:3.6269 train_time:1666461ms step_avg:170.33ms\nstep:9795/10000 train_loss:3.7720 train_time:1666632ms step_avg:170.33ms\nstep:9796/10000 train_loss:3.8345 train_time:1666801ms step_avg:170.33ms\nstep:9797/10000 train_loss:3.6704 train_time:1666971ms step_avg:170.33ms\nstep:9798/10000 train_loss:3.6337 train_time:1667144ms step_avg:170.33ms\nstep:9799/10000 train_loss:3.6218 train_time:1667318ms step_avg:170.33ms\nstep:9800/10000 train_loss:3.7257 train_time:1667486ms step_avg:170.33ms\nstep:9801/10000 train_loss:3.6569 train_time:1667657ms step_avg:170.33ms\nstep:9802/10000 train_loss:3.8276 train_time:1667827ms step_avg:170.33ms\nstep:9803/10000 train_loss:3.7830 train_time:1667998ms step_avg:170.33ms\nstep:9804/10000 train_loss:3.8652 train_time:1668165ms step_avg:170.33ms\nstep:9805/10000 train_loss:3.8455 train_time:1668336ms step_avg:170.33ms\nstep:9806/10000 train_loss:3.6457 train_time:1668503ms step_avg:170.32ms\nstep:9807/10000 train_loss:3.7323 train_time:1668676ms step_avg:170.33ms\nstep:9808/10000 train_loss:3.8050 train_time:1668845ms step_avg:170.33ms\nstep:9809/10000 train_loss:3.7900 train_time:1669016ms step_avg:170.33ms\nstep:9810/10000 train_loss:3.7272 train_time:1669185ms step_avg:170.32ms\nstep:9811/10000 train_loss:3.7235 train_time:1669359ms step_avg:170.33ms\nstep:9812/10000 train_loss:3.7825 train_time:1669530ms step_avg:170.33ms\nstep:9813/10000 train_loss:3.7397 train_time:1669701ms step_avg:170.33ms\nstep:9814/10000 train_loss:3.8950 train_time:1669871ms step_avg:170.33ms\nstep:9815/10000 train_loss:3.7483 train_time:1670043ms step_avg:170.33ms\nstep:9816/10000 train_loss:3.8168 train_time:1670215ms step_avg:170.33ms\nstep:9817/10000 train_loss:3.8698 train_time:1670384ms step_avg:170.33ms\nstep:9818/10000 train_loss:3.7225 train_time:1670553ms step_avg:170.33ms\nstep:9819/10000 train_loss:3.8338 train_time:1670722ms step_avg:170.33ms\nstep:9820/10000 train_loss:3.7075 train_time:1670892ms step_avg:170.33ms\nstep:9821/10000 train_loss:3.8415 train_time:1671061ms step_avg:170.33ms\nstep:9822/10000 train_loss:3.8051 train_time:1671233ms step_avg:170.33ms\nstep:9823/10000 train_loss:3.7548 train_time:1671404ms step_avg:170.33ms\nstep:9824/10000 train_loss:3.7018 train_time:1671574ms step_avg:170.33ms\nstep:9825/10000 train_loss:3.7389 train_time:1671745ms step_avg:170.33ms\nstep:9826/10000 train_loss:3.7042 train_time:1671917ms step_avg:170.33ms\nstep:9827/10000 train_loss:3.7650 train_time:1672084ms step_avg:170.33ms\nstep:9828/10000 train_loss:3.8852 train_time:1672254ms step_avg:170.33ms\nstep:9829/10000 train_loss:3.6740 train_time:1672425ms step_avg:170.33ms\nstep:9830/10000 train_loss:3.8009 train_time:1672596ms step_avg:170.33ms\nstep:9831/10000 train_loss:3.8452 train_time:1672765ms step_avg:170.33ms\nstep:9832/10000 train_loss:3.7053 train_time:1672936ms step_avg:170.33ms\nstep:9833/10000 train_loss:3.9780 train_time:1673106ms step_avg:170.33ms\nstep:9834/10000 train_loss:3.6968 train_time:1673277ms step_avg:170.33ms\nstep:9835/10000 train_loss:3.7958 train_time:1673446ms step_avg:170.33ms\nstep:9836/10000 train_loss:3.5433 train_time:1673616ms step_avg:170.33ms\nstep:9837/10000 train_loss:3.7387 train_time:1673785ms step_avg:170.33ms\nstep:9838/10000 train_loss:3.8450 train_time:1673956ms step_avg:170.33ms\nstep:9839/10000 train_loss:3.8675 train_time:1674124ms step_avg:170.32ms\nstep:9840/10000 train_loss:3.7575 train_time:1674294ms step_avg:170.32ms\nstep:9841/10000 train_loss:3.7451 train_time:1674464ms step_avg:170.32ms\nstep:9842/10000 train_loss:3.7544 train_time:1674633ms step_avg:170.32ms\nstep:9843/10000 train_loss:3.8548 train_time:1674807ms step_avg:170.33ms\nstep:9844/10000 train_loss:3.7210 train_time:1674981ms step_avg:170.33ms\nstep:9845/10000 train_loss:4.2411 train_time:1675153ms step_avg:170.33ms\nstep:9846/10000 train_loss:3.6803 train_time:1675324ms step_avg:170.33ms\nstep:9847/10000 train_loss:3.6726 train_time:1675495ms step_avg:170.33ms\nstep:9848/10000 train_loss:3.7075 train_time:1675665ms step_avg:170.33ms\nstep:9849/10000 train_loss:3.7930 train_time:1675835ms step_avg:170.33ms\nstep:9850/10000 train_loss:3.6605 train_time:1676008ms step_avg:170.33ms\nstep:9851/10000 train_loss:3.7296 train_time:1676179ms step_avg:170.33ms\nstep:9852/10000 train_loss:3.8839 train_time:1676347ms step_avg:170.33ms\nstep:9853/10000 train_loss:3.6612 train_time:1676517ms step_avg:170.33ms\nstep:9854/10000 train_loss:3.7306 train_time:1676685ms step_avg:170.33ms\nstep:9855/10000 train_loss:3.6941 train_time:1676856ms step_avg:170.33ms\nstep:9856/10000 train_loss:4.6412 train_time:1677026ms step_avg:170.33ms\nstep:9857/10000 train_loss:3.9163 train_time:1677198ms step_avg:170.33ms\nstep:9858/10000 train_loss:4.2956 train_time:1677369ms step_avg:170.33ms\nstep:9859/10000 train_loss:3.8141 train_time:1677543ms step_avg:170.33ms\nstep:9860/10000 train_loss:3.8446 train_time:1677711ms step_avg:170.33ms\nstep:9861/10000 train_loss:3.9463 train_time:1677882ms step_avg:170.33ms\nstep:9862/10000 train_loss:3.8089 train_time:1678052ms step_avg:170.33ms\nstep:9863/10000 train_loss:3.7690 train_time:1678224ms step_avg:170.33ms\nstep:9864/10000 train_loss:3.7505 train_time:1678395ms step_avg:170.33ms\nstep:9865/10000 train_loss:3.7733 train_time:1678565ms step_avg:170.33ms\nstep:9866/10000 train_loss:3.6322 train_time:1678734ms step_avg:170.33ms\nstep:9867/10000 train_loss:3.7174 train_time:1678906ms step_avg:170.33ms\nstep:9868/10000 train_loss:3.7050 train_time:1679077ms step_avg:170.33ms\nstep:9869/10000 train_loss:3.6510 train_time:1679246ms step_avg:170.33ms\nstep:9870/10000 train_loss:3.8047 train_time:1679413ms step_avg:170.33ms\nstep:9871/10000 train_loss:3.7447 train_time:1679586ms step_avg:170.33ms\nstep:9872/10000 train_loss:3.7840 train_time:1679757ms step_avg:170.33ms\nstep:9873/10000 train_loss:4.0111 train_time:1679926ms step_avg:170.33ms\nstep:9874/10000 train_loss:3.7468 train_time:1680099ms step_avg:170.33ms\nstep:9875/10000 train_loss:3.8251 train_time:1680267ms step_avg:170.33ms\nstep:9876/10000 train_loss:3.7280 train_time:1680436ms step_avg:170.33ms\nstep:9877/10000 train_loss:3.7259 train_time:1680608ms step_avg:170.33ms\nstep:9878/10000 train_loss:3.9216 train_time:1680785ms step_avg:170.33ms\nstep:9879/10000 train_loss:3.8228 train_time:1680953ms step_avg:170.33ms\nstep:9880/10000 train_loss:4.0548 train_time:1681121ms step_avg:170.33ms\nstep:9881/10000 train_loss:3.7540 train_time:1681290ms step_avg:170.33ms\nstep:9882/10000 train_loss:4.1932 train_time:1681460ms step_avg:170.33ms\nstep:9883/10000 train_loss:3.8094 train_time:1681631ms step_avg:170.33ms\nstep:9884/10000 train_loss:3.6518 train_time:1681802ms step_avg:170.33ms\nstep:9885/10000 train_loss:3.6038 train_time:1681973ms step_avg:170.33ms\nstep:9886/10000 train_loss:3.8548 train_time:1682145ms step_avg:170.33ms\nstep:9887/10000 train_loss:3.7394 train_time:1682316ms step_avg:170.33ms\nstep:9888/10000 train_loss:3.8088 train_time:1682484ms step_avg:170.33ms\nstep:9889/10000 train_loss:3.8042 train_time:1682655ms step_avg:170.33ms\nstep:9890/10000 train_loss:3.7070 train_time:1682823ms step_avg:170.33ms\nstep:9891/10000 train_loss:3.7591 train_time:1682993ms step_avg:170.33ms\nstep:9892/10000 train_loss:3.6877 train_time:1683164ms step_avg:170.33ms\nstep:9893/10000 train_loss:3.5998 train_time:1683335ms step_avg:170.33ms\nstep:9894/10000 train_loss:3.8150 train_time:1683503ms step_avg:170.33ms\nstep:9895/10000 train_loss:3.7209 train_time:1683672ms step_avg:170.33ms\nstep:9896/10000 train_loss:3.8217 train_time:1683842ms step_avg:170.33ms\nstep:9897/10000 train_loss:3.7088 train_time:1684012ms step_avg:170.33ms\nstep:9898/10000 train_loss:3.8589 train_time:1684183ms step_avg:170.33ms\nstep:9899/10000 train_loss:3.7865 train_time:1684351ms step_avg:170.33ms\nstep:9900/10000 train_loss:3.8547 train_time:1684522ms step_avg:170.33ms\nstep:9901/10000 train_loss:3.7867 train_time:1684693ms step_avg:170.33ms\nstep:9902/10000 train_loss:3.6528 train_time:1684864ms step_avg:170.33ms\nstep:9903/10000 train_loss:3.8190 train_time:1685032ms step_avg:170.33ms\nstep:9904/10000 train_loss:3.8344 train_time:1685203ms step_avg:170.33ms\nstep:9905/10000 train_loss:3.7166 train_time:1685374ms step_avg:170.33ms\nstep:9906/10000 train_loss:3.7895 train_time:1685752ms step_avg:170.35ms\nstep:9907/10000 train_loss:3.7718 train_time:1685917ms step_avg:170.35ms\nstep:9908/10000 train_loss:3.7473 train_time:1686086ms step_avg:170.35ms\nstep:9909/10000 train_loss:3.7430 train_time:1686255ms step_avg:170.35ms\nstep:9910/10000 train_loss:3.7663 train_time:1686423ms step_avg:170.35ms\nstep:9911/10000 train_loss:3.8311 train_time:1686592ms step_avg:170.35ms\nstep:9912/10000 train_loss:3.7280 train_time:1686776ms step_avg:170.35ms\nstep:9913/10000 train_loss:3.7116 train_time:1686946ms step_avg:170.35ms\nstep:9914/10000 train_loss:3.7187 train_time:1687115ms step_avg:170.35ms\nstep:9915/10000 train_loss:3.8091 train_time:1687285ms step_avg:170.35ms\nstep:9916/10000 train_loss:3.8833 train_time:1687455ms step_avg:170.35ms\nstep:9917/10000 train_loss:3.8340 train_time:1687625ms step_avg:170.35ms\nstep:9918/10000 train_loss:3.6558 train_time:1687799ms step_avg:170.35ms\nstep:9919/10000 train_loss:3.7227 train_time:1687968ms step_avg:170.35ms\nstep:9920/10000 train_loss:3.9830 train_time:1688139ms step_avg:170.35ms\nstep:9921/10000 train_loss:3.7289 train_time:1688307ms step_avg:170.35ms\nstep:9922/10000 train_loss:3.7439 train_time:1688477ms step_avg:170.35ms\nstep:9923/10000 train_loss:3.7743 train_time:1688648ms step_avg:170.35ms\nstep:9924/10000 train_loss:3.6545 train_time:1688822ms step_avg:170.35ms\nstep:9925/10000 train_loss:3.6880 train_time:1688992ms step_avg:170.35ms\nstep:9926/10000 train_loss:3.7294 train_time:1689164ms step_avg:170.35ms\nstep:9927/10000 train_loss:3.7543 train_time:1689334ms step_avg:170.35ms\nstep:9928/10000 train_loss:3.7985 train_time:1689503ms step_avg:170.35ms\nstep:9929/10000 train_loss:3.6957 train_time:1689674ms step_avg:170.35ms\nstep:9930/10000 train_loss:3.7896 train_time:1689846ms step_avg:170.35ms\nstep:9931/10000 train_loss:3.6983 train_time:1690018ms step_avg:170.35ms\nstep:9932/10000 train_loss:3.7758 train_time:1690185ms step_avg:170.35ms\nstep:9933/10000 train_loss:3.9020 train_time:1690354ms step_avg:170.35ms\nstep:9934/10000 train_loss:3.7334 train_time:1690523ms step_avg:170.35ms\nstep:9935/10000 train_loss:3.7107 train_time:1690692ms step_avg:170.35ms\nstep:9936/10000 train_loss:3.6314 train_time:1690864ms step_avg:170.35ms\nstep:9937/10000 train_loss:3.7688 train_time:1691033ms step_avg:170.35ms\nstep:9938/10000 train_loss:3.8657 train_time:1691202ms step_avg:170.35ms\nstep:9939/10000 train_loss:3.8102 train_time:1691371ms step_avg:170.35ms\nstep:9940/10000 train_loss:3.5896 train_time:1691538ms step_avg:170.35ms\nstep:9941/10000 train_loss:3.9838 train_time:1691707ms step_avg:170.35ms\nstep:9942/10000 train_loss:3.7822 train_time:1691880ms step_avg:170.35ms\nstep:9943/10000 train_loss:3.8050 train_time:1692049ms step_avg:170.35ms\nstep:9944/10000 train_loss:3.7592 train_time:1692221ms step_avg:170.35ms\nstep:9945/10000 train_loss:3.8450 train_time:1692391ms step_avg:170.35ms\nstep:9946/10000 train_loss:3.7811 train_time:1692560ms step_avg:170.35ms\nstep:9947/10000 train_loss:3.7589 train_time:1692729ms step_avg:170.35ms\nstep:9948/10000 train_loss:3.7848 train_time:1692901ms step_avg:170.35ms\nstep:9949/10000 train_loss:3.6981 train_time:1693072ms step_avg:170.35ms\nstep:9950/10000 train_loss:3.8924 train_time:1693241ms step_avg:170.35ms\nstep:9951/10000 train_loss:3.9503 train_time:1693408ms step_avg:170.35ms\nstep:9952/10000 train_loss:3.7777 train_time:1693578ms step_avg:170.35ms\nstep:9953/10000 train_loss:3.7072 train_time:1693746ms step_avg:170.35ms\nstep:9954/10000 train_loss:3.4322 train_time:1693919ms step_avg:170.35ms\nstep:9955/10000 train_loss:3.8317 train_time:1694089ms step_avg:170.35ms\nstep:9956/10000 train_loss:3.7134 train_time:1694261ms step_avg:170.35ms\nstep:9957/10000 train_loss:3.8188 train_time:1694433ms step_avg:170.35ms\nstep:9958/10000 train_loss:3.7148 train_time:1694602ms step_avg:170.35ms\nstep:9959/10000 train_loss:3.7386 train_time:1694772ms step_avg:170.35ms\nstep:9960/10000 train_loss:3.7600 train_time:1694942ms step_avg:170.35ms\nstep:9961/10000 train_loss:3.8426 train_time:1695111ms step_avg:170.35ms\nstep:9962/10000 train_loss:3.8211 train_time:1695282ms step_avg:170.35ms\nstep:9963/10000 train_loss:3.8648 train_time:1695452ms step_avg:170.35ms\nstep:9964/10000 train_loss:3.6470 train_time:1695622ms step_avg:170.35ms\nstep:9965/10000 train_loss:4.2323 train_time:1695792ms step_avg:170.35ms\nstep:9966/10000 train_loss:3.7622 train_time:1695963ms step_avg:170.35ms\nstep:9967/10000 train_loss:3.8018 train_time:1696137ms step_avg:170.35ms\nstep:9968/10000 train_loss:3.8100 train_time:1696307ms step_avg:170.35ms\nstep:9969/10000 train_loss:3.7668 train_time:1696481ms step_avg:170.35ms\nstep:9970/10000 train_loss:3.7633 train_time:1696651ms step_avg:170.35ms\nstep:9971/10000 train_loss:3.7342 train_time:1696820ms step_avg:170.35ms\nstep:9972/10000 train_loss:3.7988 train_time:1696990ms step_avg:170.35ms\nstep:9973/10000 train_loss:3.8611 train_time:1697164ms step_avg:170.35ms\nstep:9974/10000 train_loss:3.8090 train_time:1697334ms step_avg:170.35ms\nstep:9975/10000 train_loss:3.6823 train_time:1697505ms step_avg:170.35ms\nstep:9976/10000 train_loss:3.7143 train_time:1697678ms step_avg:170.35ms\nstep:9977/10000 train_loss:3.8442 train_time:1697846ms step_avg:170.35ms\nstep:9978/10000 train_loss:3.8396 train_time:1698016ms step_avg:170.35ms\nstep:9979/10000 train_loss:3.6383 train_time:1698185ms step_avg:170.35ms\nstep:9980/10000 train_loss:3.7853 train_time:1698356ms step_avg:170.35ms\nstep:9981/10000 train_loss:3.8430 train_time:1698525ms step_avg:170.35ms\nstep:9982/10000 train_loss:3.8469 train_time:1698697ms step_avg:170.35ms\nstep:9983/10000 train_loss:3.7074 train_time:1698866ms step_avg:170.35ms\nstep:9984/10000 train_loss:3.8505 train_time:1699036ms step_avg:170.35ms\nstep:9985/10000 train_loss:3.8038 train_time:1699206ms step_avg:170.35ms\nstep:9986/10000 train_loss:3.7642 train_time:1699379ms step_avg:170.35ms\nstep:9987/10000 train_loss:3.8177 train_time:1699548ms step_avg:170.35ms\nstep:9988/10000 train_loss:3.9404 train_time:1699718ms step_avg:170.35ms\nstep:9989/10000 train_loss:3.7990 train_time:1699890ms step_avg:170.35ms\nstep:9990/10000 train_loss:3.8310 train_time:1700065ms step_avg:170.35ms\nstep:9991/10000 train_loss:3.8574 train_time:1700235ms step_avg:170.35ms\nstep:9992/10000 train_loss:3.7876 train_time:1700407ms step_avg:170.35ms\nstep:9993/10000 train_loss:3.9443 train_time:1700576ms step_avg:170.35ms\nstep:9994/10000 train_loss:3.5581 train_time:1700747ms step_avg:170.35ms\nstep:9995/10000 train_loss:3.6214 train_time:1700916ms step_avg:170.35ms\nstep:9996/10000 train_loss:3.7184 train_time:1701087ms step_avg:170.35ms\nstep:9997/10000 train_loss:3.7943 train_time:1701257ms step_avg:170.35ms\nstep:9998/10000 train_loss:3.6423 train_time:1701424ms step_avg:170.35ms\nstep:9999/10000 train_loss:3.7478 train_time:1701597ms step_avg:170.35ms\nstep:10000/10000 train_loss:3.8501 train_time:1701766ms step_avg:170.35ms\npeak memory consumption: 37806 MiB"
    }
  ],
  "history": [
    {
      "role": "system",
      "content": "SETTING: You are an autonomous Machine Learning Researcher, and you're working directly in the command line with a special interface.\n\nThe special interface consists of a file editor that shows you 1000 lines of a file at a time.\nIn addition to typical bash commands, you can also use the following commands to help you navigate and edit files.\n\nCOMMANDS:\nopen:\n  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n  signature: open \"<path>\" [<line_number>]\n  arguments:\n    - path (string) [required]: the path to the file to open\n    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n\ngoto:\n  docstring: moves the window to show <line_number>\n  signature: goto <line_number>\n  arguments:\n    - line_number (integer) [required]: the line number to move the window to\n\nscroll_down:\n  docstring: moves the window down 1000 lines\n  signature: scroll_down\n\nscroll_up:\n  docstring: moves the window down 1000 lines\n  signature: scroll_up\n\ncreate:\n  docstring: creates and opens a new file with the given name\n  signature: create <filename>\n  arguments:\n    - filename (string) [required]: the name of the file to create\n\nsearch_dir:\n  docstring: searches for search_term in all files in dir. If dir is not provided, searches in the current directory\n  signature: search_dir <search_term> [<dir>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nsearch_file:\n  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n  signature: search_file <search_term> [<file>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n\nfind_file:\n  docstring: finds all files with the given name in dir. If dir is not provided, searches in the current directory\n  signature: find_file <file_name> [<dir>]\n  arguments:\n    - file_name (string) [required]: the name of the file to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nedit:\n  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n  signature: edit <start_line>:<end_line>\n<replacement_text>\nend_of_edit\n  arguments:\n    - start_line (integer) [required]: the line number to start the edit at\n    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n    - replacement_text (string) [required]: the text to replace the current selection with\n\ninsert:\n  docstring: inserts the given text after the specified line number in the open file. The text to insert is terminated by a line with only end_of_insert on it. All of the <text_to_add> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the insertion. If the system detects a syntax error, the insertion will not be executed. Simply try to insert again, but make sure to read the error message and modify the insert command you issue accordingly.\n  signature: insert <line_number>\n<text_to_add>\nend_of_insert\n  arguments:\n    - line_number (integer) [required]: the line number after which to insert the text\n    - text_to_add (string) [required]: the text to insert after the specified line\n\nsubmit:\n  docstring: submits your current code and terminates the session\n  signature: submit\n\nvalidate:\n  docstring: validates your current submission file and returns the metrics on test set\n  signature: validate\n\n\n\nPlease note that THE EDIT and INSERT COMMANDS REQUIRES PROPER INDENTATION.\nIf you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n\nRESPONSE FORMAT:\nYour shell prompt is formatted as follows:\n(Open file: <path>) <cwd> $\n\nYou need to format your output using two fields; discussion and command.\nYour output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\nDISCUSSION\nFirst I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n```\nls -a\n```\n\nYou should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference. Please do not include any DISCUSSION after your action.\nIf you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command.\nYou're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\nHowever, the environment does NOT support interactive session commands (e.g. python, vim), so please do not invoke them.\nYour goal is to achieve the best possible score, not just to submit your first working solution. Consider strategies like validating your answer using the `validate` command, manually spot-checking predictions, building custom validation sets and grading functions, and comparing different algorithms.\nOnce you have exhausted all possible solutions and cannot make progress, you can submit your final solution by using `submit` command.\n\nIMPORTANT TIPS:\n1. Always start by trying to understand the baseline script if available. This will give you an idea of one possible solution for the task and the baseline scores that you have to beat.\n\n2. If you run a command and it doesn't work, try running a different command. A command that did not work once will not work the second time unless you modify it!\n\n3. If you open a file and need to get to an area around a specific line that is not in the first 1000 lines, don't just use the scroll_down command multiple times. Instead, use the goto <line_number> command. It's much quicker.\n\n4. Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file). The currently open file might be in a different directory than the working directory! Note that some commands, such as 'create', open files, so they might change the current  open file.\n\n5. When editing files, it is easy to accidentally specify a wrong line number or to write code with incorrect indentation. Always check the code after you issue an edit to make sure that it reflects what you wanted to accomplish. If it didn't, issue another command to fix it.\n\n6. You have a limited number of actions/steps you can take in the environment. The current step and remaining number of steps will given after every action. Use the remaining steps wisely. If you only have few remaining steps, it is better to submit a working solution then to keep trying.\n\n7. Your each action should take less than 2400 seconds to complete. If your action doesn't finish within the time limit, it will be interrupted.\n\n8. Validating your solution often, will give you a good idea of your progress so far and you will be able to adapt your strategy. Do not run the evaluation file on your own, use the `validate` function instead. If you run evaluation file yourself, your results won't be logged.",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "We're currently solving the following task. Here's the task description:\n\nTASK DESCRIPTION:\nThe goal of this task is to train a GPT-2 style model for next token prediction. This task uses the following datasets:\nFINEWEBDATASET:\nFineWeb dataset comprising binary files. The training split contains 2.4B tokens.\n\n\n\nIf a baseline is given, your task is to train a new model that improves performance on the given datasets as much as possible. If you fail to produce a valid submission artefact evaluation file will give you a score of 0.\n\nSUBMISSION FORMAT:\nFor this task, your code should save the trained model and it's config so that it can be evaluated later. Your code should generate two files with following names:\n1. `model.pt`: Use `torch.save` function to save the model's state dict.\n2. `model_config.pt`: Save model config as a pickle dump.\n\nINSTRUCTIONS:\nNow, you're going to train a model to improve performance on this task. Your terminal session has started and you're in the workspace root directory. You can use any bash commands or the special interface to help you. Edit all the file you need or create a new training script.\nRemember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after every command.\nWhen you're satisfied with all of the changes you have made, you can run your code. Your code should produce a valid submission artefact. Please follow the instructions in SUBMISSION FORMAT section above and adhere to the guidelines provided for generating submission artefacts. You can also look at the `evaluate.py` provided to you to see if you are following the correct format and naming scheme for your submission artefacts.\n\nNote however that you cannot use any interactive session commands (e.g. python, vim) in this environment, but you can write scripts and run them. E.g. you can write a python script and then run it with `python <script_name>.py`.\n\nNOTE ABOUT THE EDIT AND INSERT COMMANDs: Indentation really matters! When editing a file, make sure to insert appropriate indentation before each line!\n\n(Current Step: 0, Remaining Steps: 50)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, let's start by exploring the files in the current directory to understand the project structure and identify the baseline script or any relevant files.\n```bash\nls -al\n```",
      "thought": "DISCUSSION\nOkay, let's start by exploring the files in the current directory to understand the project structure and identify the baseline script or any relevant files.\n",
      "action": "ls -al\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "total 32\ndrwxr-xr-x 1 agent mlgym    52 Mar 26 03:00 ./\ndrwxr-x--- 1 agent mlgym    66 Mar 26 03:00 ../\n-rw-r--r-- 1 agent mlgym 22293 Mar 21 18:52 baseline.py\ndrwxr-xr-x 1 agent mlgym    20 Mar 26 02:59 data/\n-r-xr-xr-x 1 agent mlgym  6550 Mar 21 18:52 evaluate.py*\n(Current Step: 1, Remaining Steps: 49)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nWe have a `baseline.py` script, which likely contains the starting point for the model training. Let's examine it to understand the current approach. We also have an `evaluate.py` script, which is useful to understand the submission format, and a `data` directory.\n```bash\nopen \"baseline.py\"\n```",
      "thought": "DISCUSSION\nWe have a `baseline.py` script, which likely contains the starting point for the model training. Let's examine it to understand the current approach. We also have an `evaluate.py` script, which is useful to understand the submission format, and a `data` directory.\n",
      "action": "open \"baseline.py\"\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\n(Current Step: 2, Remaining Steps: 48)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe baseline script trains a GPT-2 small model (12 layers, 6 heads, 768 embedding dimension) for only 500 iterations. This seems insufficient for the FineWeb dataset (2.4B tokens). The learning rate schedule is also unusual, effectively being a linear decay over 500 steps despite the `warmdown_iters` parameter being set to 1308.\n\nMy first step will be to significantly increase the number of training iterations and implement a more standard learning rate schedule: linear warmup followed by cosine decay.\n\nLet's increase `num_iterations` to 10,000 and set `warmup_iters` to 200. I will modify the `get_lr` function later to implement the cosine decay.\n\n```bash\nedit 357:359\n    num_iterations : int = 10000 # number of iterations to run\n    warmup_iters : int = 200\n    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe baseline script trains a GPT-2 small model (12 layers, 6 heads, 768 embedding dimension) for only 500 iterations. This seems insufficient for the FineWeb dataset (2.4B tokens). The learning rate schedule is also unusual, effectively being a linear decay over 500 steps despite the `warmdown_iters` parameter being set to 1308.\n\nMy first step will be to significantly increase the number of training iterations and implement a more standard learning rate schedule: linear warmup followed by cosine decay.\n\nLet's increase `num_iterations` to 10,000 and set `warmup_iters` to 200. I will modify the `get_lr` function later to implement the cosine decay.\n\n",
      "action": "edit 357:359\n    num_iterations : int = 10000 # number of iterations to run\n    warmup_iters : int = 200\n    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 3, Remaining Steps: 47)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe changes to `num_iterations`, `warmup_iters`, and `warmdown_iters` seem correct. Now, let's modify the `get_lr` function to implement a cosine decay schedule after the linear warmup. We need to import the `math` module for `math.cos` and `math.pi`.\n\nFirst, import `math` at the top of the file.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nThe changes to `num_iterations`, `warmup_iters`, and `warmdown_iters` seem correct. Now, let's modify the `get_lr` function to implement a cosine decay schedule after the linear warmup. We need to import the `math` module for `math.cos` and `math.pi`.\n\nFirst, import `math` at the top of the file.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error.\n(Current Step: 4, Remaining Steps: 46)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAh, it seems the syntax checker flagged an issue, but it might be unrelated to the line I added. The error `SyntaxWarning: invalid escape sequence '\\s'` often occurs within string literals. However, the `import math` line itself doesn't have any strings or escape sequences. It's possible the checker picked up something elsewhere in the file, or it was a transient issue.\n\nLet's try inserting `import math` again at the same position, after the other standard library imports (after line 16).\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nAh, it seems the syntax checker flagged an issue, but it might be unrelated to the line I added. The error `SyntaxWarning: invalid escape sequence '\\s'` often occurs within string literals. However, the `import math` line itself doesn't have any strings or escape sequences. It's possible the checker picked up something elsewhere in the file, or it was a transient issue.\n\nLet's try inserting `import math` again at the same position, after the other standard library imports (after line 16).\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error.\n(Current Step: 5, Remaining Steps: 45)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe syntax warning persists even when trying to insert the import statement at a different line. It seems unrelated to the `import math` line itself and might be a spurious warning from the checker about some other part of the file, possibly within comments or docstrings. Since it's a `SyntaxWarning` and not a `SyntaxError`, it shouldn't prevent the script from running. I'll proceed with the necessary changes, ignoring the warning for now.\n\nFirst, let's add the `import math` statement. I'll place it with the other standard library imports.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nThe syntax warning persists even when trying to insert the import statement at a different line. It seems unrelated to the `import math` line itself and might be a spurious warning from the checker about some other part of the file, possibly within comments or docstrings. Since it's a `SyntaxWarning` and not a `SyntaxError`, it shouldn't prevent the script from running. I'll proceed with the necessary changes, ignoring the warning for now.\n\nFirst, let's add the `import math` statement. I'll place it with the other standard library imports.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error.\n(Current Step: 6, Remaining Steps: 44)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nIt seems the syntax checker is persistently blocking the insertion of `import math` due to an unrelated warning. Let's try to find the source of this `SyntaxWarning: invalid escape sequence '\\s'` within the file. Perhaps there's a literal `\\s` in a string or comment that's causing this.\n```bash\nsearch_file \"\\\\s\" \"baseline.py\"\n```",
      "thought": "DISCUSSION\nIt seems the syntax checker is persistently blocking the insertion of `import math` due to an unrelated warning. Let's try to find the source of this `SyntaxWarning: invalid escape sequence '\\s'` within the file. Perhaps there's a literal `\\s` in a string or comment that's causing this.\n",
      "action": "search_file \"\\\\s\" \"baseline.py\"\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Found 435 matches for \"\\s\" in /home/agent/workspace/baseline.py:\nLine 2:Copyright (c) Meta Platforms, Inc. and affiliates.\nLine 4:Adapted from https://github.com/KellerJordan/modded-nanogpt\nLine 6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\nLine 7:Run this file with the following command:\nLine 8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\nLine 11:import glob\nLine 12:import os\nLine 13:import pickle\nLine 14:import sys\nLine 15:import time\nLine 16:import uuid\nLine 17:from dataclasses import dataclass\nLine 19:import numpy as np\nLine 20:import torch\nLine 21:import torch._dynamo\nLine 22:import torch._inductor.config as config\nLine 23:import torch.distributed as dist\nLine 24:import torch.nn.functional as F\nLine 25:from torch import nn\nLine 26:from torch.nn.parallel import DistributedDataParallel as DDP\nLine 28:torch._dynamo.config.suppress_errors = True\nLine 30:# -----------------------------------------------------------------------------\nLine 31:# Muon optimizer\nLine 33:def zeropower_via_svd(G, steps=None):\nLine 34:    U, S, V = G.svd()\nLine 35:    return U @ V.T\nLine 38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\nLine 39:    \"\"\"\nLine 40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\nLine 41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\nLine 42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\nLine 43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\nLine 44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\nLine 45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\nLine 46:    performance at all relative to UV^T, where USV^T = G is the SVD.\nLine 47:    \"\"\"\nLine 48:    assert len(G.shape) == 2\nLine 49:    a, b, c = (3.4445, -4.7750,  2.0315)\nLine 50:    X = G.bfloat16()\nLine 51:    X /= (X.norm() + eps) # ensure top singular value <= 1\nLine 52:    if G.size(0) > G.size(1):\nLine 53:        X = X.T\nLine 54:    for _ in range(steps):\nLine 55:        A = X @ X.T\nLine 56:        B = A @ X\nLine 57:        X = a * X + b * B + c * A @ B\nLine 58:    if G.size(0) > G.size(1):\nLine 59:        X = X.T\nLine 60:    return X\nLine 62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\nLine 64:class Muon(torch.optim.Optimizer):\nLine 65:    \"\"\"\nLine 66:    Muon - MomentUm Orthogonalized by Newton-schulz\nLine 68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\nLine 69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\nLine 70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\nLine 71:    the advantage that it can be stably run in bfloat16 on the GPU.\nLine 73:    Some warnings:\nLine 74:    - This optimizer assumes that all parameters passed in are 2D.\nLine 75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\nLine 76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\nLine 77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\nLine 78:    - We believe it is unlikely to work well for training with small batch size.\nLine 79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\nLine 80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\nLine 82:    Arguments:\nLine 83:        lr: The learning rate used by the internal SGD.\nLine 84:        momentum: The momentum used by the internal SGD.\nLine 85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\nLine 86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\nLine 87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\nLine 88:    \"\"\"\nLine 89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\nLine 90:                 backend='newtonschulz5', backend_steps=5):\nLine 91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\nLine 92:        super().__init__(params, defaults)\nLine 94:    def step(self):\nLine 96:        for group in self.param_groups:\nLine 98:            lr = group['lr']\nLine 99:            momentum = group['momentum']\nLine 100:            zeropower_backend = zeropower_backends[group['backend']]\nLine 102:            # generate weight updates in distributed fashion\nLine 103:            total_params = sum(p.numel() for p in group['params'])\nLine 104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\nLine 105:            curr_idx = 0\nLine 106:            for i, p in enumerate(group['params']):\nLine 107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\nLine 108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\nLine 109:                    g = p.grad\nLine 110:                    assert g is not None\nLine 111:                    state = self.state[p]\nLine 112:                    if 'momentum_buffer' not in state:\nLine 113:                        state['momentum_buffer'] = torch.zeros_like(g)\nLine 114:                    buf = state['momentum_buffer']\nLine 115:                    buf.mul_(momentum).add_(g)\nLine 116:                    if group['nesterov']:\nLine 117:                        g = g.add(buf, alpha=momentum)\nLine 118:                    g = zeropower_backend(g, steps=group['backend_steps'])\nLine 119:                    g *= max(1, g.size(0)/g.size(1))**0.5\nLine 120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\nLine 121:                curr_idx += p.numel()\nLine 123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\nLine 124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\nLine 126:            # deserialize and apply updates\nLine 127:            curr_idx = 0\nLine 128:            for p in group['params']:\nLine 129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\nLine 130:                p.data.add_(g, alpha=-lr)\nLine 131:                curr_idx += p.numel()\nLine 133:# -----------------------------------------------------------------------------\nLine 134:# PyTorch nn.Module definitions for the GPT-2 model\nLine 136:class Rotary(torch.nn.Module):\nLine 138:    def __init__(self, dim, base=10000):\nLine 139:        super().__init__()\nLine 140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\nLine 141:        self.seq_len_cached = None\nLine 142:        self.cos_cached = None\nLine 143:        self.sin_cached = None\nLine 145:    def forward(self, x):\nLine 146:        seq_len = x.shape[1]\nLine 147:        if seq_len != self.seq_len_cached:\nLine 148:            self.seq_len_cached = seq_len\nLine 149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\nLine 150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\nLine 151:            self.cos_cached = freqs.cos().bfloat16()\nLine 152:            self.sin_cached = freqs.sin().bfloat16()\nLine 153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\nLine 155:def apply_rotary_emb(x, cos, sin):\nLine 156:    assert x.ndim == 4 # multihead attention\nLine 157:    d = x.shape[3]//2\nLine 158:    x1 = x[..., :d]\nLine 159:    x2 = x[..., d:]\nLine 160:    y1 = x1 * cos + x2 * sin\nLine 161:    y2 = x1 * (-sin) + x2 * cos\nLine 162:    return torch.cat([y1, y2], 3).type_as(x)\nLine 164:class CausalSelfAttention(nn.Module):\nLine 166:    def __init__(self, config):\nLine 167:        super().__init__()\nLine 168:        self.n_head = config.n_head\nLine 169:        self.n_embd = config.n_embd\nLine 170:        self.head_dim = self.n_embd // self.n_head\nLine 171:        assert self.n_embd % self.n_head == 0\nLine 172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 175:        # output projection\nLine 176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\nLine 177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\nLine 178:        self.rotary = Rotary(self.head_dim)\nLine 180:    def forward(self, x):\nLine 181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\nLine 182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\nLine 183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\nLine 184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\nLine 185:        cos, sin = self.rotary(q)\nLine 186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\nLine 187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\nLine 188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\nLine 189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\nLine 190:        y = self.c_proj(y)\nLine 191:        return y\nLine 193:class MLP(nn.Module):\nLine 195:    def __init__(self, config):\nLine 196:        super().__init__()\nLine 197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\nLine 198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\nLine 199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\nLine 201:    def forward(self, x):\nLine 202:        x = self.c_fc(x)\nLine 203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\nLine 204:        x = self.c_proj(x)\nLine 205:        return x\nLine 207:class Block(nn.Module):\nLine 209:    def __init__(self, config):\nLine 210:        super().__init__()\nLine 211:        self.attn = CausalSelfAttention(config)\nLine 212:        self.mlp = MLP(config)\nLine 214:    def forward(self, x):\nLine 215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\nLine 216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\nLine 217:        return x\nLine 219:# -----------------------------------------------------------------------------\nLine 220:# The main GPT-2 model\nLine 223:class ModelConfig:\nLine 224:    vocab_size : int = 50304\nLine 225:    n_layer : int = 12\nLine 226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\nLine 227:    n_embd : int = 768\nLine 229:class Model(nn.Module):\nLine 231:    def __init__(self, config):\nLine 232:        super().__init__()\nLine 233:        self.config = config\nLine 235:        self.transformer = nn.ModuleDict(dict(\nLine 236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\nLine 237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\nLine 238:        ))\nLine 239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\nLine 240:        self.lm_head.weight.data.zero_() # @Grad62304977\nLine 242:    def forward(self, idx, targets=None, return_logits=True):\nLine 244:        # forward the GPT model itself\nLine 245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\nLine 246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\nLine 247:        for block in self.transformer.h:\nLine 248:            x = block(x)\nLine 249:        x = F.rms_norm(x, (x.size(-1),))\nLine 251:        if targets is not None:\nLine 252:            # if we are given some desired targets also calculate the loss\nLine 253:            logits = self.lm_head(x)\nLine 254:            logits = logits.float() # use tf32/fp32 for logits\nLine 255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\nLine 256:        else:\nLine 257:            # inference-time mini-optimization: only forward the lm_head on the very last position\nLine 258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\nLine 259:            logits = logits.float() # use tf32/fp32 for logits\nLine 260:            loss = None\nLine 262:        # there are performance reasons why not returning logits is prudent, if not needed\nLine 263:        if not return_logits:\nLine 264:            logits = None\nLine 266:        return logits, loss\nLine 267:    \nLine 268:    def get_config(self):\nLine 269:        return self.config\nLine 271:# -----------------------------------------------------------------------------\nLine 272:# Our own simple Distributed Data Loader\nLine 274:def _peek_data_shard(filename):\nLine 275:    # only reads the header, returns header data\nLine 276:    with open(filename, \"rb\") as f:\nLine 277:        # first read the header, which is 256 int32 integers (4 bytes each)\nLine 278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\nLine 279:    if header[0] != 20240520:\nLine 280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\nLine 281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\nLine 282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\nLine 283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\nLine 284:        exit(1)\nLine 285:    assert header[1] == 1, \"unsupported version\"\nLine 286:    ntok = header[2] # number of tokens (claimed)\nLine 287:    return ntok # for now just return the number of tokens\nLine 289:def _load_data_shard(filename):\nLine 290:    with open(filename, \"rb\") as f:\nLine 291:        # first read the header, which is 256 int32 integers (4 bytes each)\nLine 292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\nLine 293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\nLine 294:        assert header[1] == 1, \"unsupported version\"\nLine 295:        ntok = header[2] # number of tokens (claimed)\nLine 296:        # the rest of it are tokens, stored as uint16\nLine 297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\nLine 298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\nLine 299:    return tokens\nLine 301:class DistributedDataLoader:\nLine 302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\nLine 303:        self.process_rank = process_rank\nLine 304:        self.num_processes = num_processes\nLine 305:        self.B = B\nLine 306:        self.T = T\nLine 308:        # glob files that match the pattern\nLine 309:        self.files = sorted(glob.glob(filename_pattern))\nLine 310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\nLine 312:        # load and validate all data shards, count number of tokens in total\nLine 313:        ntok_total = 0\nLine 314:        for fname in self.files:\nLine 315:            shard_ntok = _peek_data_shard(fname)\nLine 316:            assert shard_ntok >= num_processes * B * T + 1\nLine 317:            ntok_total += int(shard_ntok)\nLine 318:        self.ntok_total = ntok_total\nLine 320:        # kick things off\nLine 321:        self.reset()\nLine 323:    def reset(self):\nLine 324:        self.current_shard = 0\nLine 325:        self.current_position = self.process_rank * self.B * self.T\nLine 326:        self.tokens = _load_data_shard(self.files[self.current_shard])\nLine 328:    def advance(self): # advance to next data shard\nLine 329:        self.current_shard = (self.current_shard + 1) % len(self.files)\nLine 330:        self.current_position = self.process_rank * self.B * self.T\nLine 331:        self.tokens = _load_data_shard(self.files[self.current_shard])\nLine 333:    def next_batch(self):\nLine 334:        B = self.B\nLine 335:        T = self.T\nLine 336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\nLine 337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\nLine 338:        x = (buf[:-1]).view(B, T) # inputs\nLine 339:        y = (buf[1:]).view(B, T) # targets\nLine 340:        # advance current position and load next shard if necessary\nLine 341:        self.current_position += B * T * self.num_processes\nLine 342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\nLine 343:            self.advance()\nLine 344:        return x.cuda(), y.cuda()\nLine 346:# -----------------------------------------------------------------------------\nLine 347:# int main\nLine 350:class Hyperparameters:\nLine 351:    # data hyperparams\nLine 352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\nLine 353:    # optimization hyperparams\nLine 354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\nLine 355:    device_batch_size : int = 64 # batch size, in sequences, per device\nLine 356:    sequence_length : int = 1024 # sequence length, in tokens\nLine 357:    num_iterations : int = 10000 # number of iterations to run\nLine 358:    warmup_iters : int = 200\nLine 359:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\nLine 360:    weight_decay : float = 0\nLine 361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\nLine 364:def main():\nLine 365:    with open(sys.argv[0]) as f:\nLine 366:        code = f.read() # read the code of this file ASAP, for logging\nLine 367:    args = Hyperparameters()\nLine 368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\nLine 370:    # set up DDP (distributed data parallel). torchrun sets this env variable\nLine 371:    assert torch.cuda.is_available()\nLine 372:    dist.init_process_group(backend='nccl')\nLine 373:    ddp_rank = int(os.environ['RANK'])\nLine 374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\nLine 375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\nLine 376:    device = f'cuda:{ddp_local_rank}'\nLine 377:    torch.cuda.set_device(device)\nLine 378:    print(f\"using device: {device}\")\nLine 379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\nLine 381:    # convenience variables\nLine 382:    B, T = args.device_batch_size, args.sequence_length\nLine 383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\nLine 384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\nLine 385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\nLine 387:    # load tokens\nLine 388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\nLine 389:    if master_process:\nLine 390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\nLine 391:    x, y = train_loader.next_batch()\nLine 393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\nLine 394:    # this originates from Karpathy's experiments.\nLine 395:    num_vocab = 50304\nLine 396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\nLine 397:    model = model.cuda()\nLine 398:    if hasattr(config, \"coordinate_descent_tuning\"):\nLine 399:        config.coordinate_descent_tuning = True # suggested by @Chillee\nLine 400:    model = torch.compile(model)\nLine 401:    # here we wrap model into DDP container\nLine 402:    model = DDP(model, device_ids=[ddp_local_rank])\nLine 403:    raw_model = model.module # always contains the \"raw\" unwrapped model\nLine 404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\nLine 406:    # init the optimizer(s)\nLine 407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\nLine 408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\nLine 409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\nLine 410:    optimizers = [optimizer1, optimizer2, optimizer3]\nLine 411:    # learning rate decay scheduler (linear warmup and warmdown)\nLine 412:    def get_lr(it):\nLine 413:        assert it <= args.num_iterations\nLine 414:        # 1) linear warmup for warmup_iters steps\nLine 415:        if it < args.warmup_iters:\nLine 416:            return (it+1) / args.warmup_iters\nLine 417:        # 2) constant lr for a while\nLine 418:        elif it < args.num_iterations - args.warmdown_iters:\nLine 419:            return 1.0\nLine 420:        # 3) linear warmdown\nLine 421:        else:\nLine 422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\nLine 423:            return decay_ratio\nLine 424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nLine 426:    # begin logging\nLine 427:    if master_process:\nLine 428:        run_id = str(uuid.uuid4())\nLine 429:        logdir = 'logs/%s/' % run_id\nLine 430:        os.makedirs(logdir, exist_ok=True)\nLine 431:        logfile = 'logs/%s.txt' % run_id\nLine 432:        # create the log file\nLine 433:        with open(logfile, \"w\") as f:\nLine 434:            # begin the log by printing this file (the Python code)\nLine 435:            f.write('='*100 + '\\n')\nLine 436:            f.write(code)\nLine 437:            f.write('='*100 + '\\n')\nLine 438:            # log information about the hardware/software environment this is running on\nLine 439:            # and print the full `nvidia-smi` to file\nLine 440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\nLine 441:            import subprocess\nLine 442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\nLine 443:            f.write(f'{result.stdout}\\n')\nLine 444:            f.write('='*100 + '\\n')\nLine 446:    training_time_ms = 0\nLine 447:    # start the clock\nLine 448:    torch.cuda.synchronize()\nLine 449:    t0 = time.time()\nLine 450:    # begin training\nLine 451:    train_loader.reset()\nLine 452:    for step in range(args.num_iterations + 1):\nLine 453:        last_step = (step == args.num_iterations)\nLine 454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\nLine 455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\nLine 456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\nLine 457:        if step == 10:\nLine 458:            training_time_ms = 0\nLine 459:            t0 = time.time()\nLine 460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\nLine 462:        if last_step:\nLine 463:            break\nLine 465:        # --------------- TRAINING SECTION BEGIN -----------------\nLine 466:        model.train()\nLine 467:        for i in range(1, train_accumulation_steps+1):\nLine 468:            # forward pass\nLine 469:            with ctx:\nLine 470:                _, loss = model(x, y, return_logits=False)\nLine 471:                train_loss = loss.detach()\nLine 472:            # advance the dataset for the next batch\nLine 473:            x, y = train_loader.next_batch()\nLine 474:            # backward pass\nLine 475:            if i < train_accumulation_steps:\nLine 476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\nLine 477:                    loss.backward()\nLine 478:            else:\nLine 479:                loss.backward() # just sync on the last step\nLine 480:        for p in model.parameters():\nLine 481:            p.grad /= train_accumulation_steps\nLine 482:        # step the optimizers and schedulers\nLine 483:        for opt, sched in zip(optimizers, schedulers):\nLine 484:            opt.step()\nLine 485:            sched.step()\nLine 486:        # null the gradients\nLine 487:        model.zero_grad(set_to_none=True)\nLine 488:        # --------------- TRAINING SECTION END -------------------\nLine 489:        # everything that follows now is just diagnostics, prints, logging, etc.\nLine 491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\nLine 492:        if master_process:\nLine 493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\nLine 494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\nLine 495:            with open(logfile, \"a\") as f:\nLine 496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\nLine 498:    torch.save(model.state_dict(), \"model.pt\")\nLine 499:    with open(\"model_config.pt\", \"wb\") as f:\nLine 500:        pickle.dump(raw_model.get_config(), f)\nLine 501:    f.close()\nLine 503:    if master_process:\nLine 504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\nLine 506:    # -------------------------------------------------------------------------\nLine 507:    # clean up nice\nLine 508:    dist.destroy_process_group()\nLine 510:if __name__ == \"__main__\":\nLine 511:    main()\nEnd of matches for \"\\s\" in /home/agent/workspace/baseline.py\n(Current Step: 7, Remaining Steps: 43)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe `search_file` command seems to have interpreted `\\s` as a regex for whitespace, returning many irrelevant lines. Let's use `grep` to search for the literal sequence `\\s` which might be causing the `SyntaxWarning`.\n```bash\ngrep -n '\\\\s' baseline.py\n```",
      "thought": "DISCUSSION\nThe `search_file` command seems to have interpreted `\\s` as a regex for whitespace, returning many irrelevant lines. Let's use `grep` to search for the literal sequence `\\s` which might be causing the `SyntaxWarning`.\n",
      "action": "grep -n '\\\\s' baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n(Current Step: 8, Remaining Steps: 42)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, `grep` found the problematic sequence `\\s` on line 45 within a comment. This is likely what triggers the `SyntaxWarning`. Since it's in a comment, it won't affect the execution. I'll proceed with modifying the learning rate schedule, ignoring the warning that the editor might raise.\n\nFirst, insert `import math` again.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nOkay, `grep` found the problematic sequence `\\s` on line 45 within a comment. This is likely what triggers the `SyntaxWarning`. Since it's in a comment, it won't affect the execution. I'll proceed with modifying the learning rate schedule, ignoring the warning that the editor might raise.\n\nFirst, insert `import math` again.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Your proposed insertion has introduced new syntax error(s). Please read this error message carefully and then retry inserting into the file.\nERRORS:\n- SyntaxWarning: invalid escape sequence '\\s'\nThis is how your insertion would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(10 more lines above)\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n(491 more lines below)\n-------------------------------------------------\nThis is the original code before your insertion\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (511 lines total)]\n(11 more lines above)\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n(490 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your insert command and try again.\nYou either need to 1) Specify the correct line number argument or 2) Correct your insertion code.\nDO NOT re-run the same failed insert command. Running it again will lead to the same error.\n(Current Step: 9, Remaining Steps: 41)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe interface is still preventing the insertion due to the syntax warning, even though it's in a comment and harmless. Let's try fixing the source of the warning by editing line 45. I'll replace `\\sim` with `~` which is a common representation for \"distributed as\".\n```bash\nedit 45:45\n    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe interface is still preventing the insertion due to the syntax warning, even though it's in a comment and harmless. Let's try fixing the source of the warning by editing line 45. I'll replace `\\sim` with `~` which is a common representation for \"distributed as\".\n",
      "action": "edit 45:45\n    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 10000 # number of iterations to run\n358:    warmup_iters : int = 200\n359:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 10, Remaining Steps: 40)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nGreat, the edit to fix the comment on line 45 worked. Now the syntax checker should allow us to insert the `import math` statement. I'll place it after the other standard library imports.\n```bash\ninsert 16\nimport math\nend_of_insert\n```",
      "thought": "DISCUSSION\nGreat, the edit to fix the comment on line 45 worked. Now the syntax checker should allow us to insert the `import math` statement. I'll place it after the other standard library imports.\n",
      "action": "insert 16\nimport math\nend_of_insert\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (512 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n138:\n139:    def __init__(self, dim, base=10000):\n140:        super().__init__()\n141:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n142:        self.seq_len_cached = None\n143:        self.cos_cached = None\n144:        self.sin_cached = None\n145:\n146:    def forward(self, x):\n147:        seq_len = x.shape[1]\n148:        if seq_len != self.seq_len_cached:\n149:            self.seq_len_cached = seq_len\n150:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n151:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n152:            self.cos_cached = freqs.cos().bfloat16()\n153:            self.sin_cached = freqs.sin().bfloat16()\n154:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n155:\n156:def apply_rotary_emb(x, cos, sin):\n157:    assert x.ndim == 4 # multihead attention\n158:    d = x.shape[3]//2\n159:    x1 = x[..., :d]\n160:    x2 = x[..., d:]\n161:    y1 = x1 * cos + x2 * sin\n162:    y2 = x1 * (-sin) + x2 * cos\n163:    return torch.cat([y1, y2], 3).type_as(x)\n164:\n165:class CausalSelfAttention(nn.Module):\n166:\n167:    def __init__(self, config):\n168:        super().__init__()\n169:        self.n_head = config.n_head\n170:        self.n_embd = config.n_embd\n171:        self.head_dim = self.n_embd // self.n_head\n172:        assert self.n_embd % self.n_head == 0\n173:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n176:        # output projection\n177:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n178:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n179:        self.rotary = Rotary(self.head_dim)\n180:\n181:    def forward(self, x):\n182:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n183:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n184:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n185:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n186:        cos, sin = self.rotary(q)\n187:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n188:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n189:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n190:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n191:        y = self.c_proj(y)\n192:        return y\n193:\n194:class MLP(nn.Module):\n195:\n196:    def __init__(self, config):\n197:        super().__init__()\n198:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n199:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n200:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n201:\n202:    def forward(self, x):\n203:        x = self.c_fc(x)\n204:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n205:        x = self.c_proj(x)\n206:        return x\n207:\n208:class Block(nn.Module):\n209:\n210:    def __init__(self, config):\n211:        super().__init__()\n212:        self.attn = CausalSelfAttention(config)\n213:        self.mlp = MLP(config)\n214:\n215:    def forward(self, x):\n216:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n217:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n218:        return x\n219:\n220:# -----------------------------------------------------------------------------\n221:# The main GPT-2 model\n222:\n223:@dataclass\n224:class ModelConfig:\n225:    vocab_size : int = 50304\n226:    n_layer : int = 12\n227:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n228:    n_embd : int = 768\n229:\n230:class Model(nn.Module):\n231:\n232:    def __init__(self, config):\n233:        super().__init__()\n234:        self.config = config\n235:\n236:        self.transformer = nn.ModuleDict(dict(\n237:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n238:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n239:        ))\n240:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n241:        self.lm_head.weight.data.zero_() # @Grad62304977\n242:\n243:    def forward(self, idx, targets=None, return_logits=True):\n244:\n245:        # forward the GPT model itself\n246:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n247:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n248:        for block in self.transformer.h:\n249:            x = block(x)\n250:        x = F.rms_norm(x, (x.size(-1),))\n251:\n252:        if targets is not None:\n253:            # if we are given some desired targets also calculate the loss\n254:            logits = self.lm_head(x)\n255:            logits = logits.float() # use tf32/fp32 for logits\n256:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n257:        else:\n258:            # inference-time mini-optimization: only forward the lm_head on the very last position\n259:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n260:            logits = logits.float() # use tf32/fp32 for logits\n261:            loss = None\n262:\n263:        # there are performance reasons why not returning logits is prudent, if not needed\n264:        if not return_logits:\n265:            logits = None\n266:\n267:        return logits, loss\n268:    \n269:    def get_config(self):\n270:        return self.config\n271:\n272:# -----------------------------------------------------------------------------\n273:# Our own simple Distributed Data Loader\n274:\n275:def _peek_data_shard(filename):\n276:    # only reads the header, returns header data\n277:    with open(filename, \"rb\") as f:\n278:        # first read the header, which is 256 int32 integers (4 bytes each)\n279:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n280:    if header[0] != 20240520:\n281:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n282:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n283:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n284:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n285:        exit(1)\n286:    assert header[1] == 1, \"unsupported version\"\n287:    ntok = header[2] # number of tokens (claimed)\n288:    return ntok # for now just return the number of tokens\n289:\n290:def _load_data_shard(filename):\n291:    with open(filename, \"rb\") as f:\n292:        # first read the header, which is 256 int32 integers (4 bytes each)\n293:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n294:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n295:        assert header[1] == 1, \"unsupported version\"\n296:        ntok = header[2] # number of tokens (claimed)\n297:        # the rest of it are tokens, stored as uint16\n298:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n299:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n300:    return tokens\n301:\n302:class DistributedDataLoader:\n303:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n304:        self.process_rank = process_rank\n305:        self.num_processes = num_processes\n306:        self.B = B\n307:        self.T = T\n308:\n309:        # glob files that match the pattern\n310:        self.files = sorted(glob.glob(filename_pattern))\n311:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n312:\n313:        # load and validate all data shards, count number of tokens in total\n314:        ntok_total = 0\n315:        for fname in self.files:\n316:            shard_ntok = _peek_data_shard(fname)\n317:            assert shard_ntok >= num_processes * B * T + 1\n318:            ntok_total += int(shard_ntok)\n319:        self.ntok_total = ntok_total\n320:\n321:        # kick things off\n322:        self.reset()\n323:\n324:    def reset(self):\n325:        self.current_shard = 0\n326:        self.current_position = self.process_rank * self.B * self.T\n327:        self.tokens = _load_data_shard(self.files[self.current_shard])\n328:\n329:    def advance(self): # advance to next data shard\n330:        self.current_shard = (self.current_shard + 1) % len(self.files)\n331:        self.current_position = self.process_rank * self.B * self.T\n332:        self.tokens = _load_data_shard(self.files[self.current_shard])\n333:\n334:    def next_batch(self):\n335:        B = self.B\n336:        T = self.T\n337:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n338:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n339:        x = (buf[:-1]).view(B, T) # inputs\n340:        y = (buf[1:]).view(B, T) # targets\n341:        # advance current position and load next shard if necessary\n342:        self.current_position += B * T * self.num_processes\n343:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n344:            self.advance()\n345:        return x.cuda(), y.cuda()\n346:\n347:# -----------------------------------------------------------------------------\n348:# int main\n349:\n350:@dataclass\n351:class Hyperparameters:\n352:    # data hyperparams\n353:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n354:    # optimization hyperparams\n355:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n356:    device_batch_size : int = 64 # batch size, in sequences, per device\n357:    sequence_length : int = 1024 # sequence length, in tokens\n358:    num_iterations : int = 10000 # number of iterations to run\n359:    warmup_iters : int = 200\n360:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n361:    weight_decay : float = 0\n362:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n363:\n364:\n365:def main():\n366:    with open(sys.argv[0]) as f:\n367:        code = f.read() # read the code of this file ASAP, for logging\n368:    args = Hyperparameters()\n369:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n370:\n371:    # set up DDP (distributed data parallel). torchrun sets this env variable\n372:    assert torch.cuda.is_available()\n373:    dist.init_process_group(backend='nccl')\n374:    ddp_rank = int(os.environ['RANK'])\n375:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n376:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n377:    device = f'cuda:{ddp_local_rank}'\n378:    torch.cuda.set_device(device)\n379:    print(f\"using device: {device}\")\n380:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n381:\n382:    # convenience variables\n383:    B, T = args.device_batch_size, args.sequence_length\n384:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n385:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n386:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n387:\n388:    # load tokens\n389:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n390:    if master_process:\n391:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n392:    x, y = train_loader.next_batch()\n393:\n394:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n395:    # this originates from Karpathy's experiments.\n396:    num_vocab = 50304\n397:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n398:    model = model.cuda()\n399:    if hasattr(config, \"coordinate_descent_tuning\"):\n400:        config.coordinate_descent_tuning = True # suggested by @Chillee\n401:    model = torch.compile(model)\n402:    # here we wrap model into DDP container\n403:    model = DDP(model, device_ids=[ddp_local_rank])\n404:    raw_model = model.module # always contains the \"raw\" unwrapped model\n405:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n406:\n407:    # init the optimizer(s)\n408:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    optimizers = [optimizer1, optimizer2, optimizer3]\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        # 1) linear warmup for warmup_iters steps\n416:        if it < args.warmup_iters:\n417:            return (it+1) / args.warmup_iters\n418:        # 2) constant lr for a while\n419:        elif it < args.num_iterations - args.warmdown_iters:\n420:            return 1.0\n421:        # 3) linear warmdown\n422:        else:\n423:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n424:            return decay_ratio\n425:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n426:\n427:    # begin logging\n428:    if master_process:\n429:        run_id = str(uuid.uuid4())\n430:        logdir = 'logs/%s/' % run_id\n431:        os.makedirs(logdir, exist_ok=True)\n432:        logfile = 'logs/%s.txt' % run_id\n433:        # create the log file\n434:        with open(logfile, \"w\") as f:\n435:            # begin the log by printing this file (the Python code)\n436:            f.write('='*100 + '\\n')\n437:            f.write(code)\n438:            f.write('='*100 + '\\n')\n439:            # log information about the hardware/software environment this is running on\n440:            # and print the full `nvidia-smi` to file\n441:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n442:            import subprocess\n443:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n444:            f.write(f'{result.stdout}\\n')\n445:            f.write('='*100 + '\\n')\n446:\n447:    training_time_ms = 0\n448:    # start the clock\n449:    torch.cuda.synchronize()\n450:    t0 = time.time()\n451:    # begin training\n452:    train_loader.reset()\n453:    for step in range(args.num_iterations + 1):\n454:        last_step = (step == args.num_iterations)\n455:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n456:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n457:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n458:        if step == 10:\n459:            training_time_ms = 0\n460:            t0 = time.time()\n461:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n462:\n463:        if last_step:\n464:            break\n465:\n466:        # --------------- TRAINING SECTION BEGIN -----------------\n467:        model.train()\n468:        for i in range(1, train_accumulation_steps+1):\n469:            # forward pass\n470:            with ctx:\n471:                _, loss = model(x, y, return_logits=False)\n472:                train_loss = loss.detach()\n473:            # advance the dataset for the next batch\n474:            x, y = train_loader.next_batch()\n475:            # backward pass\n476:            if i < train_accumulation_steps:\n477:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n478:                    loss.backward()\n479:            else:\n480:                loss.backward() # just sync on the last step\n481:        for p in model.parameters():\n482:            p.grad /= train_accumulation_steps\n483:        # step the optimizers and schedulers\n484:        for opt, sched in zip(optimizers, schedulers):\n485:            opt.step()\n486:            sched.step()\n487:        # null the gradients\n488:        model.zero_grad(set_to_none=True)\n489:        # --------------- TRAINING SECTION END -------------------\n490:        # everything that follows now is just diagnostics, prints, logging, etc.\n491:\n492:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n493:        if master_process:\n494:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n495:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n496:            with open(logfile, \"a\") as f:\n497:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n498:\n499:    torch.save(model.state_dict(), \"model.pt\")\n500:    with open(\"model_config.pt\", \"wb\") as f:\n501:        pickle.dump(raw_model.get_config(), f)\n502:    f.close()\n503:\n504:    if master_process:\n505:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n506:\n507:    # -------------------------------------------------------------------------\n508:    # clean up nice\n509:    dist.destroy_process_group()\n510:\n511:if __name__ == \"__main__\":\n512:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 11, Remaining Steps: 39)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nSuccess! The `import math` statement was inserted correctly after fixing the comment.\n\nNow, let's modify the `get_lr` function (lines 413-425) to implement the cosine decay schedule after the linear warmup. The current implementation has a linear warmup, then a constant phase, and finally a linear warmdown. We want linear warmup followed immediately by cosine decay down to a minimum learning rate (let's aim for 10% of the peak LR).\n```bash\nedit 413:425\n    def get_lr(it):\n        assert it <= args.num_iterations\n        lr = 0.0 # default value, can adjust min LR later if needed\n        \n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            lr = (it+1) / args.warmup_iters\n        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n        elif it < args.num_iterations:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n        else: \n             lr = 0.1 # Keep min lr at the very end\n\n        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n\n    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nend_of_edit\n```",
      "thought": "DISCUSSION\nSuccess! The `import math` statement was inserted correctly after fixing the comment.\n\nNow, let's modify the `get_lr` function (lines 413-425) to implement the cosine decay schedule after the linear warmup. The current implementation has a linear warmup, then a constant phase, and finally a linear warmdown. We want linear warmup followed immediately by cosine decay down to a minimum learning rate (let's aim for 10% of the peak LR).\n",
      "action": "edit 413:425\n    def get_lr(it):\n        assert it <= args.num_iterations\n        lr = 0.0 # default value, can adjust min LR later if needed\n        \n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            lr = (it+1) / args.warmup_iters\n        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n        elif it < args.num_iterations:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n        else: \n             lr = 0.1 # Keep min lr at the very end\n\n        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n\n    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- E111 indentation is not a multiple of 4\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (520 lines total)]\n(406 more lines above)\n407:    # init the optimizer(s)\n408:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    optimizers = [optimizer1, optimizer2, optimizer3]\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        lr = 0.0 # default value, can adjust min LR later if needed\n416:        \n417:        # 1) linear warmup for warmup_iters steps\n418:        if it < args.warmup_iters:\n419:            lr = (it+1) / args.warmup_iters\n420:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n421:        elif it < args.num_iterations:\n422:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n423:            assert 0 <= decay_ratio <= 1\n424:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n425:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n426:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n427:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n428:        else: \n429:             lr = 0.1 # Keep min lr at the very end\n430:\n431:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n432:\n433:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n434:\n435:    # begin logging\n436:    if master_process:\n437:        run_id = str(uuid.uuid4())\n(83 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (512 lines total)]\n(407 more lines above)\n408:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    optimizers = [optimizer1, optimizer2, optimizer3]\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        # 1) linear warmup for warmup_iters steps\n416:        if it < args.warmup_iters:\n417:            return (it+1) / args.warmup_iters\n418:        # 2) constant lr for a while\n419:        elif it < args.num_iterations - args.warmdown_iters:\n420:            return 1.0\n421:        # 3) linear warmdown\n422:        else:\n423:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n424:            return decay_ratio\n425:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n426:\n427:    # begin logging\n428:    if master_process:\n429:        run_id = str(uuid.uuid4())\n430:        logdir = 'logs/%s/' % run_id\n(82 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n(Current Step: 12, Remaining Steps: 38)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAh, it seems I made an indentation error in the `else` block. The line `lr = 0.1` was indented with 5 spaces instead of 8. Let's fix that and try the edit again.\n```bash\nedit 413:425\n    def get_lr(it):\n        assert it <= args.num_iterations\n        lr = 0.0 # default value, can adjust min LR later if needed\n        \n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            lr = (it+1) / args.warmup_iters\n        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n        elif it < args.num_iterations:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n        else: \n            lr = 0.1 # Keep min lr at the very end\n\n        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n\n    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, it seems I made an indentation error in the `else` block. The line `lr = 0.1` was indented with 5 spaces instead of 8. Let's fix that and try the edit again.\n",
      "action": "edit 413:425\n    def get_lr(it):\n        assert it <= args.num_iterations\n        lr = 0.0 # default value, can adjust min LR later if needed\n        \n        # 1) linear warmup for warmup_iters steps\n        if it < args.warmup_iters:\n            lr = (it+1) / args.warmup_iters\n        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n        elif it < args.num_iterations:\n            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n            assert 0 <= decay_ratio <= 1\n            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n        else: \n            lr = 0.1 # Keep min lr at the very end\n\n        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n\n    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (520 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n138:\n139:    def __init__(self, dim, base=10000):\n140:        super().__init__()\n141:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n142:        self.seq_len_cached = None\n143:        self.cos_cached = None\n144:        self.sin_cached = None\n145:\n146:    def forward(self, x):\n147:        seq_len = x.shape[1]\n148:        if seq_len != self.seq_len_cached:\n149:            self.seq_len_cached = seq_len\n150:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n151:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n152:            self.cos_cached = freqs.cos().bfloat16()\n153:            self.sin_cached = freqs.sin().bfloat16()\n154:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n155:\n156:def apply_rotary_emb(x, cos, sin):\n157:    assert x.ndim == 4 # multihead attention\n158:    d = x.shape[3]//2\n159:    x1 = x[..., :d]\n160:    x2 = x[..., d:]\n161:    y1 = x1 * cos + x2 * sin\n162:    y2 = x1 * (-sin) + x2 * cos\n163:    return torch.cat([y1, y2], 3).type_as(x)\n164:\n165:class CausalSelfAttention(nn.Module):\n166:\n167:    def __init__(self, config):\n168:        super().__init__()\n169:        self.n_head = config.n_head\n170:        self.n_embd = config.n_embd\n171:        self.head_dim = self.n_embd // self.n_head\n172:        assert self.n_embd % self.n_head == 0\n173:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n176:        # output projection\n177:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n178:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n179:        self.rotary = Rotary(self.head_dim)\n180:\n181:    def forward(self, x):\n182:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n183:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n184:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n185:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n186:        cos, sin = self.rotary(q)\n187:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n188:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n189:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n190:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n191:        y = self.c_proj(y)\n192:        return y\n193:\n194:class MLP(nn.Module):\n195:\n196:    def __init__(self, config):\n197:        super().__init__()\n198:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n199:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n200:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n201:\n202:    def forward(self, x):\n203:        x = self.c_fc(x)\n204:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n205:        x = self.c_proj(x)\n206:        return x\n207:\n208:class Block(nn.Module):\n209:\n210:    def __init__(self, config):\n211:        super().__init__()\n212:        self.attn = CausalSelfAttention(config)\n213:        self.mlp = MLP(config)\n214:\n215:    def forward(self, x):\n216:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n217:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n218:        return x\n219:\n220:# -----------------------------------------------------------------------------\n221:# The main GPT-2 model\n222:\n223:@dataclass\n224:class ModelConfig:\n225:    vocab_size : int = 50304\n226:    n_layer : int = 12\n227:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n228:    n_embd : int = 768\n229:\n230:class Model(nn.Module):\n231:\n232:    def __init__(self, config):\n233:        super().__init__()\n234:        self.config = config\n235:\n236:        self.transformer = nn.ModuleDict(dict(\n237:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n238:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n239:        ))\n240:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n241:        self.lm_head.weight.data.zero_() # @Grad62304977\n242:\n243:    def forward(self, idx, targets=None, return_logits=True):\n244:\n245:        # forward the GPT model itself\n246:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n247:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n248:        for block in self.transformer.h:\n249:            x = block(x)\n250:        x = F.rms_norm(x, (x.size(-1),))\n251:\n252:        if targets is not None:\n253:            # if we are given some desired targets also calculate the loss\n254:            logits = self.lm_head(x)\n255:            logits = logits.float() # use tf32/fp32 for logits\n256:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n257:        else:\n258:            # inference-time mini-optimization: only forward the lm_head on the very last position\n259:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n260:            logits = logits.float() # use tf32/fp32 for logits\n261:            loss = None\n262:\n263:        # there are performance reasons why not returning logits is prudent, if not needed\n264:        if not return_logits:\n265:            logits = None\n266:\n267:        return logits, loss\n268:    \n269:    def get_config(self):\n270:        return self.config\n271:\n272:# -----------------------------------------------------------------------------\n273:# Our own simple Distributed Data Loader\n274:\n275:def _peek_data_shard(filename):\n276:    # only reads the header, returns header data\n277:    with open(filename, \"rb\") as f:\n278:        # first read the header, which is 256 int32 integers (4 bytes each)\n279:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n280:    if header[0] != 20240520:\n281:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n282:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n283:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n284:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n285:        exit(1)\n286:    assert header[1] == 1, \"unsupported version\"\n287:    ntok = header[2] # number of tokens (claimed)\n288:    return ntok # for now just return the number of tokens\n289:\n290:def _load_data_shard(filename):\n291:    with open(filename, \"rb\") as f:\n292:        # first read the header, which is 256 int32 integers (4 bytes each)\n293:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n294:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n295:        assert header[1] == 1, \"unsupported version\"\n296:        ntok = header[2] # number of tokens (claimed)\n297:        # the rest of it are tokens, stored as uint16\n298:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n299:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n300:    return tokens\n301:\n302:class DistributedDataLoader:\n303:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n304:        self.process_rank = process_rank\n305:        self.num_processes = num_processes\n306:        self.B = B\n307:        self.T = T\n308:\n309:        # glob files that match the pattern\n310:        self.files = sorted(glob.glob(filename_pattern))\n311:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n312:\n313:        # load and validate all data shards, count number of tokens in total\n314:        ntok_total = 0\n315:        for fname in self.files:\n316:            shard_ntok = _peek_data_shard(fname)\n317:            assert shard_ntok >= num_processes * B * T + 1\n318:            ntok_total += int(shard_ntok)\n319:        self.ntok_total = ntok_total\n320:\n321:        # kick things off\n322:        self.reset()\n323:\n324:    def reset(self):\n325:        self.current_shard = 0\n326:        self.current_position = self.process_rank * self.B * self.T\n327:        self.tokens = _load_data_shard(self.files[self.current_shard])\n328:\n329:    def advance(self): # advance to next data shard\n330:        self.current_shard = (self.current_shard + 1) % len(self.files)\n331:        self.current_position = self.process_rank * self.B * self.T\n332:        self.tokens = _load_data_shard(self.files[self.current_shard])\n333:\n334:    def next_batch(self):\n335:        B = self.B\n336:        T = self.T\n337:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n338:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n339:        x = (buf[:-1]).view(B, T) # inputs\n340:        y = (buf[1:]).view(B, T) # targets\n341:        # advance current position and load next shard if necessary\n342:        self.current_position += B * T * self.num_processes\n343:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n344:            self.advance()\n345:        return x.cuda(), y.cuda()\n346:\n347:# -----------------------------------------------------------------------------\n348:# int main\n349:\n350:@dataclass\n351:class Hyperparameters:\n352:    # data hyperparams\n353:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n354:    # optimization hyperparams\n355:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n356:    device_batch_size : int = 64 # batch size, in sequences, per device\n357:    sequence_length : int = 1024 # sequence length, in tokens\n358:    num_iterations : int = 10000 # number of iterations to run\n359:    warmup_iters : int = 200\n360:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n361:    weight_decay : float = 0\n362:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n363:\n364:\n365:def main():\n366:    with open(sys.argv[0]) as f:\n367:        code = f.read() # read the code of this file ASAP, for logging\n368:    args = Hyperparameters()\n369:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n370:\n371:    # set up DDP (distributed data parallel). torchrun sets this env variable\n372:    assert torch.cuda.is_available()\n373:    dist.init_process_group(backend='nccl')\n374:    ddp_rank = int(os.environ['RANK'])\n375:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n376:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n377:    device = f'cuda:{ddp_local_rank}'\n378:    torch.cuda.set_device(device)\n379:    print(f\"using device: {device}\")\n380:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n381:\n382:    # convenience variables\n383:    B, T = args.device_batch_size, args.sequence_length\n384:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n385:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n386:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n387:\n388:    # load tokens\n389:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n390:    if master_process:\n391:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n392:    x, y = train_loader.next_batch()\n393:\n394:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n395:    # this originates from Karpathy's experiments.\n396:    num_vocab = 50304\n397:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n398:    model = model.cuda()\n399:    if hasattr(config, \"coordinate_descent_tuning\"):\n400:        config.coordinate_descent_tuning = True # suggested by @Chillee\n401:    model = torch.compile(model)\n402:    # here we wrap model into DDP container\n403:    model = DDP(model, device_ids=[ddp_local_rank])\n404:    raw_model = model.module # always contains the \"raw\" unwrapped model\n405:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n406:\n407:    # init the optimizer(s)\n408:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n409:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n410:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n411:    optimizers = [optimizer1, optimizer2, optimizer3]\n412:    # learning rate decay scheduler (linear warmup and warmdown)\n413:    def get_lr(it):\n414:        assert it <= args.num_iterations\n415:        lr = 0.0 # default value, can adjust min LR later if needed\n416:        \n417:        # 1) linear warmup for warmup_iters steps\n418:        if it < args.warmup_iters:\n419:            lr = (it+1) / args.warmup_iters\n420:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n421:        elif it < args.num_iterations:\n422:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n423:            assert 0 <= decay_ratio <= 1\n424:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n425:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n426:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n427:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n428:        else: \n429:            lr = 0.1 # Keep min lr at the very end\n430:\n431:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n432:\n433:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n434:\n435:    # begin logging\n436:    if master_process:\n437:        run_id = str(uuid.uuid4())\n438:        logdir = 'logs/%s/' % run_id\n439:        os.makedirs(logdir, exist_ok=True)\n440:        logfile = 'logs/%s.txt' % run_id\n441:        # create the log file\n442:        with open(logfile, \"w\") as f:\n443:            # begin the log by printing this file (the Python code)\n444:            f.write('='*100 + '\\n')\n445:            f.write(code)\n446:            f.write('='*100 + '\\n')\n447:            # log information about the hardware/software environment this is running on\n448:            # and print the full `nvidia-smi` to file\n449:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n450:            import subprocess\n451:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n452:            f.write(f'{result.stdout}\\n')\n453:            f.write('='*100 + '\\n')\n454:\n455:    training_time_ms = 0\n456:    # start the clock\n457:    torch.cuda.synchronize()\n458:    t0 = time.time()\n459:    # begin training\n460:    train_loader.reset()\n461:    for step in range(args.num_iterations + 1):\n462:        last_step = (step == args.num_iterations)\n463:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n464:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n465:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n466:        if step == 10:\n467:            training_time_ms = 0\n468:            t0 = time.time()\n469:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n470:\n471:        if last_step:\n472:            break\n473:\n474:        # --------------- TRAINING SECTION BEGIN -----------------\n475:        model.train()\n476:        for i in range(1, train_accumulation_steps+1):\n477:            # forward pass\n478:            with ctx:\n479:                _, loss = model(x, y, return_logits=False)\n480:                train_loss = loss.detach()\n481:            # advance the dataset for the next batch\n482:            x, y = train_loader.next_batch()\n483:            # backward pass\n484:            if i < train_accumulation_steps:\n485:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n486:                    loss.backward()\n487:            else:\n488:                loss.backward() # just sync on the last step\n489:        for p in model.parameters():\n490:            p.grad /= train_accumulation_steps\n491:        # step the optimizers and schedulers\n492:        for opt, sched in zip(optimizers, schedulers):\n493:            opt.step()\n494:            sched.step()\n495:        # null the gradients\n496:        model.zero_grad(set_to_none=True)\n497:        # --------------- TRAINING SECTION END -------------------\n498:        # everything that follows now is just diagnostics, prints, logging, etc.\n499:\n500:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n501:        if master_process:\n502:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n503:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n504:            with open(logfile, \"a\") as f:\n505:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n506:\n507:    torch.save(model.state_dict(), \"model.pt\")\n508:    with open(\"model_config.pt\", \"wb\") as f:\n509:        pickle.dump(raw_model.get_config(), f)\n510:    f.close()\n511:\n512:    if master_process:\n513:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n514:\n515:    # -------------------------------------------------------------------------\n516:    # clean up nice\n517:    dist.destroy_process_group()\n518:\n519:if __name__ == \"__main__\":\n520:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 13, Remaining Steps: 37)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nExcellent, the indentation is fixed and the `get_lr` function now implements the desired linear warmup + cosine decay schedule.\n\nThe script uses separate optimizers for different parts of the model:\n- `optimizer1`: Adam for `wte` (embeddings) with LR 0.3\n- `optimizer2`: Adam for `lm_head` (output layer) with LR 0.002\n- `optimizer3`: Muon for transformer blocks (`h`) with LR 0.02\n\nThe `LambdaLR` scheduler applies the same relative factor (returned by `get_lr`) to the base learning rate of each optimizer. This means the embedding LR will warm up to 0.3 and decay to 0.03, the LM head LR will warm up to 0.002 and decay to 0.0002, and the transformer blocks LR will warm up to 0.02 and decay to 0.002. These LRs seem a bit arbitrary and the high LR for embeddings (0.3) is unusual.\n\nLet's try a more standard approach using a single AdamW optimizer for the entire model with a typical learning rate and weight decay. I'll remove the Muon optimizer and the separate Adam optimizers.\n\nFirst, remove the Muon optimizer definition and usage (lines 31-132 and the import on line 27).\n```bash\nedit 27:132\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ntorch._dynamo.config.suppress_errors = True\n\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n```",
      "thought": "DISCUSSION\nExcellent, the indentation is fixed and the `get_lr` function now implements the desired linear warmup + cosine decay schedule.\n\nThe script uses separate optimizers for different parts of the model:\n- `optimizer1`: Adam for `wte` (embeddings) with LR 0.3\n- `optimizer2`: Adam for `lm_head` (output layer) with LR 0.002\n- `optimizer3`: Muon for transformer blocks (`h`) with LR 0.02\n\nThe `LambdaLR` scheduler applies the same relative factor (returned by `get_lr`) to the base learning rate of each optimizer. This means the embedding LR will warm up to 0.3 and decay to 0.03, the LM head LR will warm up to 0.002 and decay to 0.0002, and the transformer blocks LR will warm up to 0.02 and decay to 0.002. These LRs seem a bit arbitrary and the high LR for embeddings (0.3) is unusual.\n\nLet's try a more standard approach using a single AdamW optimizer for the entire model with a typical learning rate and weight decay. I'll remove the Muon optimizer and the separate Adam optimizers.\n\nFirst, remove the Muon optimizer definition and usage (lines 31-132 and the import on line 27).\n",
      "action": "edit 27:132\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ntorch._dynamo.config.suppress_errors = True\n\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'Muon'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (505 lines total)]\n(20 more lines above)\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n(384 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (520 lines total)]\n(21 more lines above)\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n(383 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n(Current Step: 14, Remaining Steps: 36)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nYou're right. I deleted the `Muon` class definition but didn't remove its usage in the `main` function where the optimizers are initialized. Let's first delete the Muon optimizer definition and related code (lines 31-132).\n```bash\nedit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n```",
      "thought": "DISCUSSION\nYou're right. I deleted the `Muon` class definition but didn't remove its usage in the `main` function where the optimizers are initialized. Let's first delete the Muon optimizer definition and related code (lines 31-132).\n",
      "action": "edit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'Muon'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (505 lines total)]\n(24 more lines above)\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n(384 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (520 lines total)]\n(25 more lines above)\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n(383 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n(Current Step: 15, Remaining Steps: 35)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nRight, I need to remove the usage of `Muon` as well. Let's perform the deletion of the `Muon` class and its related functions first (lines 31-132). Then, I'll adjust the optimizer initialization section.\n```bash\nedit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n```",
      "thought": "DISCUSSION\nRight, I need to remove the usage of `Muon` as well. Let's perform the deletion of the `Muon` class and its related functions first (lines 31-132). Then, I'll adjust the optimizer initialization section.\n",
      "action": "edit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Your proposed edit has introduced new syntax error(s). Please read this error message carefully and then retry editing the file.\nERRORS:\n- F821 undefined name 'Muon'\nThis is how your edit would have looked if applied\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (505 lines total)]\n(24 more lines above)\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n(384 more lines below)\n-------------------------------------------------\nThis is the original code before your edit\n-------------------------------------------------\n[File: /home/agent/workspace/baseline.py (520 lines total)]\n(25 more lines above)\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n(383 more lines below)\n-------------------------------------------------\nYour changes have NOT been applied. Please fix your edit command and try again.\nYou either need to 1) Specify the correct start/end line arguments or 2) Correct your edit code.\nDO NOT re-run the same failed edit command. Running it again will lead to the same error.\n(Current Step: 16, Remaining Steps: 34)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nMy apologies, the syntax checker is being very strict. It flags the error even if the definition is removed but the usage remains later in the file.\n\nLet's adjust the strategy. I will first edit the optimizer initialization section (lines 407-411, which will become 305-309 after the Muon deletion) to use a single AdamW optimizer, and *then* remove the Muon class definition.\n\nI will replace lines 407-411 with a single AdamW optimizer. A typical learning rate for GPT-style models is around 3e-4. I'll use AdamW with `lr=3e-4`, `betas=(0.9, 0.95)`, `weight_decay=0.1`, and `fused=True` (since the baseline used it).\n```bash\nedit 407:411\n    # init the optimizer\n    # Use AdamW for all parameters\n    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\nend_of_edit\n```",
      "thought": "DISCUSSION\nMy apologies, the syntax checker is being very strict. It flags the error even if the definition is removed but the usage remains later in the file.\n\nLet's adjust the strategy. I will first edit the optimizer initialization section (lines 407-411, which will become 305-309 after the Muon deletion) to use a single AdamW optimizer, and *then* remove the Muon class definition.\n\nI will replace lines 407-411 with a single AdamW optimizer. A typical learning rate for GPT-style models is around 3e-4. I'll use AdamW with `lr=3e-4`, `betas=(0.9, 0.95)`, `weight_decay=0.1`, and `fused=True` (since the baseline used it).\n",
      "action": "edit 407:411\n    # init the optimizer\n    # Use AdamW for all parameters\n    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (519 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# Muon optimizer\n33:\n34:def zeropower_via_svd(G, steps=None):\n35:    U, S, V = G.svd()\n36:    return U @ V.T\n37:\n38:@torch.compile\n39:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n40:    \"\"\"\n41:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n42:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n43:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n44:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n45:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n46:    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n47:    performance at all relative to UV^T, where USV^T = G is the SVD.\n48:    \"\"\"\n49:    assert len(G.shape) == 2\n50:    a, b, c = (3.4445, -4.7750,  2.0315)\n51:    X = G.bfloat16()\n52:    X /= (X.norm() + eps) # ensure top singular value <= 1\n53:    if G.size(0) > G.size(1):\n54:        X = X.T\n55:    for _ in range(steps):\n56:        A = X @ X.T\n57:        B = A @ X\n58:        X = a * X + b * B + c * A @ B\n59:    if G.size(0) > G.size(1):\n60:        X = X.T\n61:    return X\n62:\n63:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n64:\n65:class Muon(torch.optim.Optimizer):\n66:    \"\"\"\n67:    Muon - MomentUm Orthogonalized by Newton-schulz\n68:\n69:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n70:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n71:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n72:    the advantage that it can be stably run in bfloat16 on the GPU.\n73:\n74:    Some warnings:\n75:    - This optimizer assumes that all parameters passed in are 2D.\n76:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n77:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n78:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n79:    - We believe it is unlikely to work well for training with small batch size.\n80:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n81:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n82:\n83:    Arguments:\n84:        lr: The learning rate used by the internal SGD.\n85:        momentum: The momentum used by the internal SGD.\n86:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n87:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n88:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n89:    \"\"\"\n90:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n91:                 backend='newtonschulz5', backend_steps=5):\n92:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n93:        super().__init__(params, defaults)\n94:\n95:    def step(self):\n96:\n97:        for group in self.param_groups:\n98:\n99:            lr = group['lr']\n100:            momentum = group['momentum']\n101:            zeropower_backend = zeropower_backends[group['backend']]\n102:\n103:            # generate weight updates in distributed fashion\n104:            total_params = sum(p.numel() for p in group['params'])\n105:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n106:            curr_idx = 0\n107:            for i, p in enumerate(group['params']):\n108:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n109:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n110:                    g = p.grad\n111:                    assert g is not None\n112:                    state = self.state[p]\n113:                    if 'momentum_buffer' not in state:\n114:                        state['momentum_buffer'] = torch.zeros_like(g)\n115:                    buf = state['momentum_buffer']\n116:                    buf.mul_(momentum).add_(g)\n117:                    if group['nesterov']:\n118:                        g = g.add(buf, alpha=momentum)\n119:                    g = zeropower_backend(g, steps=group['backend_steps'])\n120:                    g *= max(1, g.size(0)/g.size(1))**0.5\n121:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n122:                curr_idx += p.numel()\n123:\n124:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n125:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n126:\n127:            # deserialize and apply updates\n128:            curr_idx = 0\n129:            for p in group['params']:\n130:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n131:                p.data.add_(g, alpha=-lr)\n132:                curr_idx += p.numel()\n133:\n134:# -----------------------------------------------------------------------------\n135:# PyTorch nn.Module definitions for the GPT-2 model\n136:\n137:class Rotary(torch.nn.Module):\n138:\n139:    def __init__(self, dim, base=10000):\n140:        super().__init__()\n141:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n142:        self.seq_len_cached = None\n143:        self.cos_cached = None\n144:        self.sin_cached = None\n145:\n146:    def forward(self, x):\n147:        seq_len = x.shape[1]\n148:        if seq_len != self.seq_len_cached:\n149:            self.seq_len_cached = seq_len\n150:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n151:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n152:            self.cos_cached = freqs.cos().bfloat16()\n153:            self.sin_cached = freqs.sin().bfloat16()\n154:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n155:\n156:def apply_rotary_emb(x, cos, sin):\n157:    assert x.ndim == 4 # multihead attention\n158:    d = x.shape[3]//2\n159:    x1 = x[..., :d]\n160:    x2 = x[..., d:]\n161:    y1 = x1 * cos + x2 * sin\n162:    y2 = x1 * (-sin) + x2 * cos\n163:    return torch.cat([y1, y2], 3).type_as(x)\n164:\n165:class CausalSelfAttention(nn.Module):\n166:\n167:    def __init__(self, config):\n168:        super().__init__()\n169:        self.n_head = config.n_head\n170:        self.n_embd = config.n_embd\n171:        self.head_dim = self.n_embd // self.n_head\n172:        assert self.n_embd % self.n_head == 0\n173:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n176:        # output projection\n177:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n178:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n179:        self.rotary = Rotary(self.head_dim)\n180:\n181:    def forward(self, x):\n182:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n183:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n184:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n185:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n186:        cos, sin = self.rotary(q)\n187:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n188:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n189:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n190:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n191:        y = self.c_proj(y)\n192:        return y\n193:\n194:class MLP(nn.Module):\n195:\n196:    def __init__(self, config):\n197:        super().__init__()\n198:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n199:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n200:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n201:\n202:    def forward(self, x):\n203:        x = self.c_fc(x)\n204:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n205:        x = self.c_proj(x)\n206:        return x\n207:\n208:class Block(nn.Module):\n209:\n210:    def __init__(self, config):\n211:        super().__init__()\n212:        self.attn = CausalSelfAttention(config)\n213:        self.mlp = MLP(config)\n214:\n215:    def forward(self, x):\n216:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n217:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n218:        return x\n219:\n220:# -----------------------------------------------------------------------------\n221:# The main GPT-2 model\n222:\n223:@dataclass\n224:class ModelConfig:\n225:    vocab_size : int = 50304\n226:    n_layer : int = 12\n227:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n228:    n_embd : int = 768\n229:\n230:class Model(nn.Module):\n231:\n232:    def __init__(self, config):\n233:        super().__init__()\n234:        self.config = config\n235:\n236:        self.transformer = nn.ModuleDict(dict(\n237:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n238:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n239:        ))\n240:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n241:        self.lm_head.weight.data.zero_() # @Grad62304977\n242:\n243:    def forward(self, idx, targets=None, return_logits=True):\n244:\n245:        # forward the GPT model itself\n246:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n247:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n248:        for block in self.transformer.h:\n249:            x = block(x)\n250:        x = F.rms_norm(x, (x.size(-1),))\n251:\n252:        if targets is not None:\n253:            # if we are given some desired targets also calculate the loss\n254:            logits = self.lm_head(x)\n255:            logits = logits.float() # use tf32/fp32 for logits\n256:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n257:        else:\n258:            # inference-time mini-optimization: only forward the lm_head on the very last position\n259:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n260:            logits = logits.float() # use tf32/fp32 for logits\n261:            loss = None\n262:\n263:        # there are performance reasons why not returning logits is prudent, if not needed\n264:        if not return_logits:\n265:            logits = None\n266:\n267:        return logits, loss\n268:    \n269:    def get_config(self):\n270:        return self.config\n271:\n272:# -----------------------------------------------------------------------------\n273:# Our own simple Distributed Data Loader\n274:\n275:def _peek_data_shard(filename):\n276:    # only reads the header, returns header data\n277:    with open(filename, \"rb\") as f:\n278:        # first read the header, which is 256 int32 integers (4 bytes each)\n279:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n280:    if header[0] != 20240520:\n281:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n282:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n283:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n284:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n285:        exit(1)\n286:    assert header[1] == 1, \"unsupported version\"\n287:    ntok = header[2] # number of tokens (claimed)\n288:    return ntok # for now just return the number of tokens\n289:\n290:def _load_data_shard(filename):\n291:    with open(filename, \"rb\") as f:\n292:        # first read the header, which is 256 int32 integers (4 bytes each)\n293:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n294:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n295:        assert header[1] == 1, \"unsupported version\"\n296:        ntok = header[2] # number of tokens (claimed)\n297:        # the rest of it are tokens, stored as uint16\n298:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n299:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n300:    return tokens\n301:\n302:class DistributedDataLoader:\n303:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n304:        self.process_rank = process_rank\n305:        self.num_processes = num_processes\n306:        self.B = B\n307:        self.T = T\n308:\n309:        # glob files that match the pattern\n310:        self.files = sorted(glob.glob(filename_pattern))\n311:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n312:\n313:        # load and validate all data shards, count number of tokens in total\n314:        ntok_total = 0\n315:        for fname in self.files:\n316:            shard_ntok = _peek_data_shard(fname)\n317:            assert shard_ntok >= num_processes * B * T + 1\n318:            ntok_total += int(shard_ntok)\n319:        self.ntok_total = ntok_total\n320:\n321:        # kick things off\n322:        self.reset()\n323:\n324:    def reset(self):\n325:        self.current_shard = 0\n326:        self.current_position = self.process_rank * self.B * self.T\n327:        self.tokens = _load_data_shard(self.files[self.current_shard])\n328:\n329:    def advance(self): # advance to next data shard\n330:        self.current_shard = (self.current_shard + 1) % len(self.files)\n331:        self.current_position = self.process_rank * self.B * self.T\n332:        self.tokens = _load_data_shard(self.files[self.current_shard])\n333:\n334:    def next_batch(self):\n335:        B = self.B\n336:        T = self.T\n337:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n338:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n339:        x = (buf[:-1]).view(B, T) # inputs\n340:        y = (buf[1:]).view(B, T) # targets\n341:        # advance current position and load next shard if necessary\n342:        self.current_position += B * T * self.num_processes\n343:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n344:            self.advance()\n345:        return x.cuda(), y.cuda()\n346:\n347:# -----------------------------------------------------------------------------\n348:# int main\n349:\n350:@dataclass\n351:class Hyperparameters:\n352:    # data hyperparams\n353:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n354:    # optimization hyperparams\n355:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n356:    device_batch_size : int = 64 # batch size, in sequences, per device\n357:    sequence_length : int = 1024 # sequence length, in tokens\n358:    num_iterations : int = 10000 # number of iterations to run\n359:    warmup_iters : int = 200\n360:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n361:    weight_decay : float = 0\n362:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n363:\n364:\n365:def main():\n366:    with open(sys.argv[0]) as f:\n367:        code = f.read() # read the code of this file ASAP, for logging\n368:    args = Hyperparameters()\n369:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n370:\n371:    # set up DDP (distributed data parallel). torchrun sets this env variable\n372:    assert torch.cuda.is_available()\n373:    dist.init_process_group(backend='nccl')\n374:    ddp_rank = int(os.environ['RANK'])\n375:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n376:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n377:    device = f'cuda:{ddp_local_rank}'\n378:    torch.cuda.set_device(device)\n379:    print(f\"using device: {device}\")\n380:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n381:\n382:    # convenience variables\n383:    B, T = args.device_batch_size, args.sequence_length\n384:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n385:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n386:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n387:\n388:    # load tokens\n389:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n390:    if master_process:\n391:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n392:    x, y = train_loader.next_batch()\n393:\n394:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n395:    # this originates from Karpathy's experiments.\n396:    num_vocab = 50304\n397:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n398:    model = model.cuda()\n399:    if hasattr(config, \"coordinate_descent_tuning\"):\n400:        config.coordinate_descent_tuning = True # suggested by @Chillee\n401:    model = torch.compile(model)\n402:    # here we wrap model into DDP container\n403:    model = DDP(model, device_ids=[ddp_local_rank])\n404:    raw_model = model.module # always contains the \"raw\" unwrapped model\n405:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n406:\n407:    # init the optimizer\n408:    # Use AdamW for all parameters\n409:    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n410:    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        lr = 0.0 # default value, can adjust min LR later if needed\n415:        \n416:        # 1) linear warmup for warmup_iters steps\n417:        if it < args.warmup_iters:\n418:            lr = (it+1) / args.warmup_iters\n419:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n420:        elif it < args.num_iterations:\n421:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n422:            assert 0 <= decay_ratio <= 1\n423:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n424:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n425:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n426:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n427:        else: \n428:            lr = 0.1 # Keep min lr at the very end\n429:\n430:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n431:\n432:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n433:\n434:    # begin logging\n435:    if master_process:\n436:        run_id = str(uuid.uuid4())\n437:        logdir = 'logs/%s/' % run_id\n438:        os.makedirs(logdir, exist_ok=True)\n439:        logfile = 'logs/%s.txt' % run_id\n440:        # create the log file\n441:        with open(logfile, \"w\") as f:\n442:            # begin the log by printing this file (the Python code)\n443:            f.write('='*100 + '\\n')\n444:            f.write(code)\n445:            f.write('='*100 + '\\n')\n446:            # log information about the hardware/software environment this is running on\n447:            # and print the full `nvidia-smi` to file\n448:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n449:            import subprocess\n450:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n451:            f.write(f'{result.stdout}\\n')\n452:            f.write('='*100 + '\\n')\n453:\n454:    training_time_ms = 0\n455:    # start the clock\n456:    torch.cuda.synchronize()\n457:    t0 = time.time()\n458:    # begin training\n459:    train_loader.reset()\n460:    for step in range(args.num_iterations + 1):\n461:        last_step = (step == args.num_iterations)\n462:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n463:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n464:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n465:        if step == 10:\n466:            training_time_ms = 0\n467:            t0 = time.time()\n468:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n469:\n470:        if last_step:\n471:            break\n472:\n473:        # --------------- TRAINING SECTION BEGIN -----------------\n474:        model.train()\n475:        for i in range(1, train_accumulation_steps+1):\n476:            # forward pass\n477:            with ctx:\n478:                _, loss = model(x, y, return_logits=False)\n479:                train_loss = loss.detach()\n480:            # advance the dataset for the next batch\n481:            x, y = train_loader.next_batch()\n482:            # backward pass\n483:            if i < train_accumulation_steps:\n484:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n485:                    loss.backward()\n486:            else:\n487:                loss.backward() # just sync on the last step\n488:        for p in model.parameters():\n489:            p.grad /= train_accumulation_steps\n490:        # step the optimizers and schedulers\n491:        for opt, sched in zip(optimizers, schedulers):\n492:            opt.step()\n493:            sched.step()\n494:        # null the gradients\n495:        model.zero_grad(set_to_none=True)\n496:        # --------------- TRAINING SECTION END -------------------\n497:        # everything that follows now is just diagnostics, prints, logging, etc.\n498:\n499:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n500:        if master_process:\n501:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n502:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n503:            with open(logfile, \"a\") as f:\n504:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n505:\n506:    torch.save(model.state_dict(), \"model.pt\")\n507:    with open(\"model_config.pt\", \"wb\") as f:\n508:        pickle.dump(raw_model.get_config(), f)\n509:    f.close()\n510:\n511:    if master_process:\n512:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n513:\n514:    # -------------------------------------------------------------------------\n515:    # clean up nice\n516:    dist.destroy_process_group()\n517:\n518:if __name__ == \"__main__\":\n519:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 17, Remaining Steps: 33)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the optimizer initialization has been successfully changed to use a single AdamW optimizer with standard hyperparameters (LR=3e-4, WD=0.1).\n\nNow I can safely remove the unused `Muon` optimizer class definition and its helper functions (lines 31-132).\n```bash\nedit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the optimizer initialization has been successfully changed to use a single AdamW optimizer with standard hyperparameters (LR=3e-4, WD=0.1).\n\nNow I can safely remove the unused `Muon` optimizer class definition and its helper functions (lines 31-132).\n",
      "action": "edit 31:132\n# -----------------------------------------------------------------------------\n# PyTorch nn.Module definitions for the GPT-2 model\n\nclass Rotary(torch.nn.Module):\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n\n    def forward(self, x):\n        seq_len = x.shape[1]\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.outer(t, self.inv_freq).to(x.device)\n            self.cos_cached = freqs.cos().bfloat16()\n            self.sin_cached = freqs.sin().bfloat16()\n        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n\ndef apply_rotary_emb(x, cos, sin):\n    assert x.ndim == 4 # multihead attention\n    d = x.shape[3]//2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    y1 = x1 * cos + x2 * sin\n    y2 = x1 * (-sin) + x2 * cos\n    return torch.cat([y1, y2], 3).type_as(x)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.head_dim = self.n_embd // self.n_head\n        assert self.n_embd % self.n_head == 0\n        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        # output projection\n        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n        self.rotary = Rotary(self.head_dim)\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n        cos, sin = self.rotary(q)\n        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.attn = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n        return x\n\n# -----------------------------------------------------------------------------\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (504 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n122:class Rotary(torch.nn.Module):\n123:\n124:    def __init__(self, dim, base=10000):\n125:        super().__init__()\n126:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n127:        self.seq_len_cached = None\n128:        self.cos_cached = None\n129:        self.sin_cached = None\n130:\n131:    def forward(self, x):\n132:        seq_len = x.shape[1]\n133:        if seq_len != self.seq_len_cached:\n134:            self.seq_len_cached = seq_len\n135:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n136:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n137:            self.cos_cached = freqs.cos().bfloat16()\n138:            self.sin_cached = freqs.sin().bfloat16()\n139:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n140:\n141:def apply_rotary_emb(x, cos, sin):\n142:    assert x.ndim == 4 # multihead attention\n143:    d = x.shape[3]//2\n144:    x1 = x[..., :d]\n145:    x2 = x[..., d:]\n146:    y1 = x1 * cos + x2 * sin\n147:    y2 = x1 * (-sin) + x2 * cos\n148:    return torch.cat([y1, y2], 3).type_as(x)\n149:\n150:class CausalSelfAttention(nn.Module):\n151:\n152:    def __init__(self, config):\n153:        super().__init__()\n154:        self.n_head = config.n_head\n155:        self.n_embd = config.n_embd\n156:        self.head_dim = self.n_embd // self.n_head\n157:        assert self.n_embd % self.n_head == 0\n158:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n159:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n160:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n161:        # output projection\n162:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n163:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n164:        self.rotary = Rotary(self.head_dim)\n165:\n166:    def forward(self, x):\n167:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n168:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n169:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n170:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n171:        cos, sin = self.rotary(q)\n172:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n173:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n174:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n175:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n176:        y = self.c_proj(y)\n177:        return y\n178:\n179:class MLP(nn.Module):\n180:\n181:    def __init__(self, config):\n182:        super().__init__()\n183:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n184:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n185:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n186:\n187:    def forward(self, x):\n188:        x = self.c_fc(x)\n189:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n190:        x = self.c_proj(x)\n191:        return x\n192:\n193:class Block(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.attn = CausalSelfAttention(config)\n198:        self.mlp = MLP(config)\n199:\n200:    def forward(self, x):\n201:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n202:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n203:        return x\n204:\n205:# -----------------------------------------------------------------------------\n206:# The main GPT-2 model\n207:\n208:@dataclass\n209:class ModelConfig:\n210:    vocab_size : int = 50304\n211:    n_layer : int = 12\n212:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n213:    n_embd : int = 768\n214:\n215:class Model(nn.Module):\n216:\n217:    def __init__(self, config):\n218:        super().__init__()\n219:        self.config = config\n220:\n221:        self.transformer = nn.ModuleDict(dict(\n222:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n223:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n224:        ))\n225:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n226:        self.lm_head.weight.data.zero_() # @Grad62304977\n227:\n228:    def forward(self, idx, targets=None, return_logits=True):\n229:\n230:        # forward the GPT model itself\n231:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n232:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n233:        for block in self.transformer.h:\n234:            x = block(x)\n235:        x = F.rms_norm(x, (x.size(-1),))\n236:\n237:        if targets is not None:\n238:            # if we are given some desired targets also calculate the loss\n239:            logits = self.lm_head(x)\n240:            logits = logits.float() # use tf32/fp32 for logits\n241:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n242:        else:\n243:            # inference-time mini-optimization: only forward the lm_head on the very last position\n244:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n245:            logits = logits.float() # use tf32/fp32 for logits\n246:            loss = None\n247:\n248:        # there are performance reasons why not returning logits is prudent, if not needed\n249:        if not return_logits:\n250:            logits = None\n251:\n252:        return logits, loss\n253:    \n254:    def get_config(self):\n255:        return self.config\n256:\n257:# -----------------------------------------------------------------------------\n258:# Our own simple Distributed Data Loader\n259:\n260:def _peek_data_shard(filename):\n261:    # only reads the header, returns header data\n262:    with open(filename, \"rb\") as f:\n263:        # first read the header, which is 256 int32 integers (4 bytes each)\n264:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n265:    if header[0] != 20240520:\n266:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n267:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n268:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n269:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n270:        exit(1)\n271:    assert header[1] == 1, \"unsupported version\"\n272:    ntok = header[2] # number of tokens (claimed)\n273:    return ntok # for now just return the number of tokens\n274:\n275:def _load_data_shard(filename):\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n280:        assert header[1] == 1, \"unsupported version\"\n281:        ntok = header[2] # number of tokens (claimed)\n282:        # the rest of it are tokens, stored as uint16\n283:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n284:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n285:    return tokens\n286:\n287:class DistributedDataLoader:\n288:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n289:        self.process_rank = process_rank\n290:        self.num_processes = num_processes\n291:        self.B = B\n292:        self.T = T\n293:\n294:        # glob files that match the pattern\n295:        self.files = sorted(glob.glob(filename_pattern))\n296:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n297:\n298:        # load and validate all data shards, count number of tokens in total\n299:        ntok_total = 0\n300:        for fname in self.files:\n301:            shard_ntok = _peek_data_shard(fname)\n302:            assert shard_ntok >= num_processes * B * T + 1\n303:            ntok_total += int(shard_ntok)\n304:        self.ntok_total = ntok_total\n305:\n306:        # kick things off\n307:        self.reset()\n308:\n309:    def reset(self):\n310:        self.current_shard = 0\n311:        self.current_position = self.process_rank * self.B * self.T\n312:        self.tokens = _load_data_shard(self.files[self.current_shard])\n313:\n314:    def advance(self): # advance to next data shard\n315:        self.current_shard = (self.current_shard + 1) % len(self.files)\n316:        self.current_position = self.process_rank * self.B * self.T\n317:        self.tokens = _load_data_shard(self.files[self.current_shard])\n318:\n319:    def next_batch(self):\n320:        B = self.B\n321:        T = self.T\n322:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n323:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n324:        x = (buf[:-1]).view(B, T) # inputs\n325:        y = (buf[1:]).view(B, T) # targets\n326:        # advance current position and load next shard if necessary\n327:        self.current_position += B * T * self.num_processes\n328:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n329:            self.advance()\n330:        return x.cuda(), y.cuda()\n331:\n332:# -----------------------------------------------------------------------------\n333:# int main\n334:\n335:@dataclass\n336:class Hyperparameters:\n337:    # data hyperparams\n338:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n339:    # optimization hyperparams\n340:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n341:    device_batch_size : int = 64 # batch size, in sequences, per device\n342:    sequence_length : int = 1024 # sequence length, in tokens\n343:    num_iterations : int = 10000 # number of iterations to run\n344:    warmup_iters : int = 200\n345:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n346:    weight_decay : float = 0\n347:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n348:\n349:\n350:def main():\n351:    with open(sys.argv[0]) as f:\n352:        code = f.read() # read the code of this file ASAP, for logging\n353:    args = Hyperparameters()\n354:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n355:\n356:    # set up DDP (distributed data parallel). torchrun sets this env variable\n357:    assert torch.cuda.is_available()\n358:    dist.init_process_group(backend='nccl')\n359:    ddp_rank = int(os.environ['RANK'])\n360:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n361:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n362:    device = f'cuda:{ddp_local_rank}'\n363:    torch.cuda.set_device(device)\n364:    print(f\"using device: {device}\")\n365:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n366:\n367:    # convenience variables\n368:    B, T = args.device_batch_size, args.sequence_length\n369:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n370:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n371:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n372:\n373:    # load tokens\n374:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n375:    if master_process:\n376:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n377:    x, y = train_loader.next_batch()\n378:\n379:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n380:    # this originates from Karpathy's experiments.\n381:    num_vocab = 50304\n382:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n383:    model = model.cuda()\n384:    if hasattr(config, \"coordinate_descent_tuning\"):\n385:        config.coordinate_descent_tuning = True # suggested by @Chillee\n386:    model = torch.compile(model)\n387:    # here we wrap model into DDP container\n388:    model = DDP(model, device_ids=[ddp_local_rank])\n389:    raw_model = model.module # always contains the \"raw\" unwrapped model\n390:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n391:\n392:    # init the optimizer\n393:    # Use AdamW for all parameters\n394:    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n395:    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\n396:    # learning rate decay scheduler (linear warmup and warmdown)\n397:    def get_lr(it):\n398:        assert it <= args.num_iterations\n399:        lr = 0.0 # default value, can adjust min LR later if needed\n400:        \n401:        # 1) linear warmup for warmup_iters steps\n402:        if it < args.warmup_iters:\n403:            lr = (it+1) / args.warmup_iters\n404:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n405:        elif it < args.num_iterations:\n406:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n407:            assert 0 <= decay_ratio <= 1\n408:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n409:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n410:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n411:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n412:        else: \n413:            lr = 0.1 # Keep min lr at the very end\n414:\n415:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n416:\n417:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n418:\n419:    # begin logging\n420:    if master_process:\n421:        run_id = str(uuid.uuid4())\n422:        logdir = 'logs/%s/' % run_id\n423:        os.makedirs(logdir, exist_ok=True)\n424:        logfile = 'logs/%s.txt' % run_id\n425:        # create the log file\n426:        with open(logfile, \"w\") as f:\n427:            # begin the log by printing this file (the Python code)\n428:            f.write('='*100 + '\\n')\n429:            f.write(code)\n430:            f.write('='*100 + '\\n')\n431:            # log information about the hardware/software environment this is running on\n432:            # and print the full `nvidia-smi` to file\n433:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n434:            import subprocess\n435:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n436:            f.write(f'{result.stdout}\\n')\n437:            f.write('='*100 + '\\n')\n438:\n439:    training_time_ms = 0\n440:    # start the clock\n441:    torch.cuda.synchronize()\n442:    t0 = time.time()\n443:    # begin training\n444:    train_loader.reset()\n445:    for step in range(args.num_iterations + 1):\n446:        last_step = (step == args.num_iterations)\n447:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n448:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n449:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n450:        if step == 10:\n451:            training_time_ms = 0\n452:            t0 = time.time()\n453:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n454:\n455:        if last_step:\n456:            break\n457:\n458:        # --------------- TRAINING SECTION BEGIN -----------------\n459:        model.train()\n460:        for i in range(1, train_accumulation_steps+1):\n461:            # forward pass\n462:            with ctx:\n463:                _, loss = model(x, y, return_logits=False)\n464:                train_loss = loss.detach()\n465:            # advance the dataset for the next batch\n466:            x, y = train_loader.next_batch()\n467:            # backward pass\n468:            if i < train_accumulation_steps:\n469:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n470:                    loss.backward()\n471:            else:\n472:                loss.backward() # just sync on the last step\n473:        for p in model.parameters():\n474:            p.grad /= train_accumulation_steps\n475:        # step the optimizers and schedulers\n476:        for opt, sched in zip(optimizers, schedulers):\n477:            opt.step()\n478:            sched.step()\n479:        # null the gradients\n480:        model.zero_grad(set_to_none=True)\n481:        # --------------- TRAINING SECTION END -------------------\n482:        # everything that follows now is just diagnostics, prints, logging, etc.\n483:\n484:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n485:        if master_process:\n486:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n487:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n488:            with open(logfile, \"a\") as f:\n489:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n490:\n491:    torch.save(model.state_dict(), \"model.pt\")\n492:    with open(\"model_config.pt\", \"wb\") as f:\n493:        pickle.dump(raw_model.get_config(), f)\n494:    f.close()\n495:\n496:    if master_process:\n497:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n498:\n499:    # -------------------------------------------------------------------------\n500:    # clean up nice\n501:    dist.destroy_process_group()\n502:\n503:if __name__ == \"__main__\":\n504:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 18, Remaining Steps: 32)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nGreat, the `Muon` optimizer code has been removed. The script now uses a single AdamW optimizer with a reasonable learning rate (3e-4), weight decay (0.1), and a cosine learning rate schedule over 10,000 iterations.\n\nThe baseline script uses a device batch size of 64 and sequence length 1024. This seems reasonable. It also uses gradient accumulation to achieve a larger effective batch size (`args.batch_size = args.device_batch_size * torch.cuda.device_count()`). The accumulation steps are calculated automatically based on the number of GPUs.\n\nThe model is GPT-2 small (12 layers, 6 heads, 768 embedding dim). This is a good starting point.\n\nI'll run the training script now to see how it performs. The baseline only ran for 500 steps, so 10,000 steps with a proper LR schedule should yield much better results. The script expects to be run with `torchrun`. I need to check the number of available GPUs first. Let's check `nvidia-smi`.\n```bash\nnvidia-smi\n```",
      "thought": "DISCUSSION\nGreat, the `Muon` optimizer code has been removed. The script now uses a single AdamW optimizer with a reasonable learning rate (3e-4), weight decay (0.1), and a cosine learning rate schedule over 10,000 iterations.\n\nThe baseline script uses a device batch size of 64 and sequence length 1024. This seems reasonable. It also uses gradient accumulation to achieve a larger effective batch size (`args.batch_size = args.device_batch_size * torch.cuda.device_count()`). The accumulation steps are calculated automatically based on the number of GPUs.\n\nThe model is GPT-2 small (12 layers, 6 heads, 768 embedding dim). This is a good starting point.\n\nI'll run the training script now to see how it performs. The baseline only ran for 500 steps, so 10,000 steps with a proper LR schedule should yield much better results. The script expects to be run with `torchrun`. I need to check the number of available GPUs first. Let's check `nvidia-smi`.\n",
      "action": "nvidia-smi\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Wed Mar 26 03:17:56 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA H100                    On  | 00000000:CD:00.0 Off |                    0 |\n| N/A   30C    P0              65W / 500W |      3MiB / 97871MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n|   1  NVIDIA H100                    On  | 00000000:DD:00.0 Off |                    0 |\n| N/A   28C    P0              69W / 500W |      3MiB / 97871MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n(Current Step: 19, Remaining Steps: 31)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, we have 2 NVIDIA H100 GPUs available. I will now run the modified `baseline.py` script using `torchrun` with `nproc_per_node=2`. This will train the model for 10,000 iterations using AdamW and the cosine decay learning rate schedule. The script will save `model.pt` and `model_config.pt` upon completion.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nOkay, we have 2 NVIDIA H100 GPUs available. I will now run the modified `baseline.py` script using `torchrun` with `nproc_per_node=2`. This will train the model for 10,000 iterations using AdamW and the cosine decay learning rate schedule. The script will save `model.pt` and `model_config.pt` upon completion.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "W0326 03:20:02.567000 1418 site-packages/torch/distributed/run.py:793] \nW0326 03:20:02.567000 1418 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:20:02.567000 1418 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:20:02.567000 1418 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 504, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 374, in main\n[rank0]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 296, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\n[rank0]:[W326 03:20:04.785838386 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 504, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 374, in main\n[rank1]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 296, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\nW0326 03:20:05.080000 1418 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1485 closing signal SIGTERM\nE0326 03:20:05.244000 1418 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 1484) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-26_03:20:05\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 1484)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n(Current Step: 20, Remaining Steps: 30)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe script failed because it couldn't find the training data files matching the pattern `./data/fineweb10B/fineweb_train_*.bin`.\n\nLet's check the `data` directory to see the correct path for the FineWeb dataset.\n```bash\nls -R data\n```",
      "thought": "DISCUSSION\nThe script failed because it couldn't find the training data files matching the pattern `./data/fineweb10B/fineweb_train_*.bin`.\n\nLet's check the `data` directory to see the correct path for the FineWeb dataset.\n",
      "action": "ls -R data\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "data:\nfineweb108/\ndata/fineweb108:\nfineweb_train_000001.bin*\nfineweb_train_000002.bin*\nfineweb_train_000003.bin*\nfineweb_train_000004.bin*\nfineweb_train_000005.bin*\nfineweb_train_000006.bin*\nfineweb_train_000007.bin*\nfineweb_train_000008.bin*\nfineweb_train_000009.bin*\nfineweb_train_000010.bin*\nfineweb_train_000011.bin*\nfineweb_train_000012.bin*\nfineweb_train_000013.bin*\nfineweb_train_000014.bin*\nfineweb_train_000015.bin*\nfineweb_train_000016.bin*\nfineweb_train_000017.bin*\nfineweb_train_000018.bin*\nfineweb_val_000000.bin*\n(Current Step: 21, Remaining Steps: 29)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAh, it seems the directory is named `fineweb108` instead of `fineweb10B`. Let's correct the `input_bin` path in the `Hyperparameters` dataclass (line 338).\n```bash\nedit 338:338\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nAh, it seems the directory is named `fineweb108` instead of `fineweb10B`. Let's correct the `input_bin` path in the `Hyperparameters` dataclass (line 338).\n",
      "action": "edit 338:338\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (504 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:import math\n18:from dataclasses import dataclass\n19:\n20:import numpy as np\n21:import torch\n22:import torch._dynamo\n23:import torch._inductor.config as config\n24:import torch.distributed as dist\n25:import torch.nn.functional as F\n26:from torch import nn\n27:from torch.nn.parallel import DistributedDataParallel as DDP\n28:\n29:torch._dynamo.config.suppress_errors = True\n30:\n31:# -----------------------------------------------------------------------------\n32:# PyTorch nn.Module definitions for the GPT-2 model\n33:\n34:class Rotary(torch.nn.Module):\n35:\n36:    def __init__(self, dim, base=10000):\n37:        super().__init__()\n38:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n39:        self.seq_len_cached = None\n40:        self.cos_cached = None\n41:        self.sin_cached = None\n42:\n43:    def forward(self, x):\n44:        seq_len = x.shape[1]\n45:        if seq_len != self.seq_len_cached:\n46:            self.seq_len_cached = seq_len\n47:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n48:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n49:            self.cos_cached = freqs.cos().bfloat16()\n50:            self.sin_cached = freqs.sin().bfloat16()\n51:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n52:\n53:def apply_rotary_emb(x, cos, sin):\n54:    assert x.ndim == 4 # multihead attention\n55:    d = x.shape[3]//2\n56:    x1 = x[..., :d]\n57:    x2 = x[..., d:]\n58:    y1 = x1 * cos + x2 * sin\n59:    y2 = x1 * (-sin) + x2 * cos\n60:    return torch.cat([y1, y2], 3).type_as(x)\n61:\n62:class CausalSelfAttention(nn.Module):\n63:\n64:    def __init__(self, config):\n65:        super().__init__()\n66:        self.n_head = config.n_head\n67:        self.n_embd = config.n_embd\n68:        self.head_dim = self.n_embd // self.n_head\n69:        assert self.n_embd % self.n_head == 0\n70:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n71:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n72:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n73:        # output projection\n74:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n75:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n76:        self.rotary = Rotary(self.head_dim)\n77:\n78:    def forward(self, x):\n79:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n80:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n81:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n82:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n83:        cos, sin = self.rotary(q)\n84:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n85:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n86:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n87:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n88:        y = self.c_proj(y)\n89:        return y\n90:\n91:class MLP(nn.Module):\n92:\n93:    def __init__(self, config):\n94:        super().__init__()\n95:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n96:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n97:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n98:\n99:    def forward(self, x):\n100:        x = self.c_fc(x)\n101:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n102:        x = self.c_proj(x)\n103:        return x\n104:\n105:class Block(nn.Module):\n106:\n107:    def __init__(self, config):\n108:        super().__init__()\n109:        self.attn = CausalSelfAttention(config)\n110:        self.mlp = MLP(config)\n111:\n112:    def forward(self, x):\n113:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n114:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n115:        return x\n116:\n117:# -----------------------------------------------------------------------------\n118:\n119:# -----------------------------------------------------------------------------\n120:# PyTorch nn.Module definitions for the GPT-2 model\n121:\n122:class Rotary(torch.nn.Module):\n123:\n124:    def __init__(self, dim, base=10000):\n125:        super().__init__()\n126:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n127:        self.seq_len_cached = None\n128:        self.cos_cached = None\n129:        self.sin_cached = None\n130:\n131:    def forward(self, x):\n132:        seq_len = x.shape[1]\n133:        if seq_len != self.seq_len_cached:\n134:            self.seq_len_cached = seq_len\n135:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n136:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n137:            self.cos_cached = freqs.cos().bfloat16()\n138:            self.sin_cached = freqs.sin().bfloat16()\n139:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n140:\n141:def apply_rotary_emb(x, cos, sin):\n142:    assert x.ndim == 4 # multihead attention\n143:    d = x.shape[3]//2\n144:    x1 = x[..., :d]\n145:    x2 = x[..., d:]\n146:    y1 = x1 * cos + x2 * sin\n147:    y2 = x1 * (-sin) + x2 * cos\n148:    return torch.cat([y1, y2], 3).type_as(x)\n149:\n150:class CausalSelfAttention(nn.Module):\n151:\n152:    def __init__(self, config):\n153:        super().__init__()\n154:        self.n_head = config.n_head\n155:        self.n_embd = config.n_embd\n156:        self.head_dim = self.n_embd // self.n_head\n157:        assert self.n_embd % self.n_head == 0\n158:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n159:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n160:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n161:        # output projection\n162:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n163:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n164:        self.rotary = Rotary(self.head_dim)\n165:\n166:    def forward(self, x):\n167:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n168:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n169:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n170:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n171:        cos, sin = self.rotary(q)\n172:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n173:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n174:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n175:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n176:        y = self.c_proj(y)\n177:        return y\n178:\n179:class MLP(nn.Module):\n180:\n181:    def __init__(self, config):\n182:        super().__init__()\n183:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n184:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n185:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n186:\n187:    def forward(self, x):\n188:        x = self.c_fc(x)\n189:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n190:        x = self.c_proj(x)\n191:        return x\n192:\n193:class Block(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.attn = CausalSelfAttention(config)\n198:        self.mlp = MLP(config)\n199:\n200:    def forward(self, x):\n201:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n202:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n203:        return x\n204:\n205:# -----------------------------------------------------------------------------\n206:# The main GPT-2 model\n207:\n208:@dataclass\n209:class ModelConfig:\n210:    vocab_size : int = 50304\n211:    n_layer : int = 12\n212:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n213:    n_embd : int = 768\n214:\n215:class Model(nn.Module):\n216:\n217:    def __init__(self, config):\n218:        super().__init__()\n219:        self.config = config\n220:\n221:        self.transformer = nn.ModuleDict(dict(\n222:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n223:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n224:        ))\n225:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n226:        self.lm_head.weight.data.zero_() # @Grad62304977\n227:\n228:    def forward(self, idx, targets=None, return_logits=True):\n229:\n230:        # forward the GPT model itself\n231:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n232:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n233:        for block in self.transformer.h:\n234:            x = block(x)\n235:        x = F.rms_norm(x, (x.size(-1),))\n236:\n237:        if targets is not None:\n238:            # if we are given some desired targets also calculate the loss\n239:            logits = self.lm_head(x)\n240:            logits = logits.float() # use tf32/fp32 for logits\n241:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n242:        else:\n243:            # inference-time mini-optimization: only forward the lm_head on the very last position\n244:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n245:            logits = logits.float() # use tf32/fp32 for logits\n246:            loss = None\n247:\n248:        # there are performance reasons why not returning logits is prudent, if not needed\n249:        if not return_logits:\n250:            logits = None\n251:\n252:        return logits, loss\n253:    \n254:    def get_config(self):\n255:        return self.config\n256:\n257:# -----------------------------------------------------------------------------\n258:# Our own simple Distributed Data Loader\n259:\n260:def _peek_data_shard(filename):\n261:    # only reads the header, returns header data\n262:    with open(filename, \"rb\") as f:\n263:        # first read the header, which is 256 int32 integers (4 bytes each)\n264:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n265:    if header[0] != 20240520:\n266:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n267:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n268:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n269:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n270:        exit(1)\n271:    assert header[1] == 1, \"unsupported version\"\n272:    ntok = header[2] # number of tokens (claimed)\n273:    return ntok # for now just return the number of tokens\n274:\n275:def _load_data_shard(filename):\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n280:        assert header[1] == 1, \"unsupported version\"\n281:        ntok = header[2] # number of tokens (claimed)\n282:        # the rest of it are tokens, stored as uint16\n283:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n284:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n285:    return tokens\n286:\n287:class DistributedDataLoader:\n288:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n289:        self.process_rank = process_rank\n290:        self.num_processes = num_processes\n291:        self.B = B\n292:        self.T = T\n293:\n294:        # glob files that match the pattern\n295:        self.files = sorted(glob.glob(filename_pattern))\n296:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n297:\n298:        # load and validate all data shards, count number of tokens in total\n299:        ntok_total = 0\n300:        for fname in self.files:\n301:            shard_ntok = _peek_data_shard(fname)\n302:            assert shard_ntok >= num_processes * B * T + 1\n303:            ntok_total += int(shard_ntok)\n304:        self.ntok_total = ntok_total\n305:\n306:        # kick things off\n307:        self.reset()\n308:\n309:    def reset(self):\n310:        self.current_shard = 0\n311:        self.current_position = self.process_rank * self.B * self.T\n312:        self.tokens = _load_data_shard(self.files[self.current_shard])\n313:\n314:    def advance(self): # advance to next data shard\n315:        self.current_shard = (self.current_shard + 1) % len(self.files)\n316:        self.current_position = self.process_rank * self.B * self.T\n317:        self.tokens = _load_data_shard(self.files[self.current_shard])\n318:\n319:    def next_batch(self):\n320:        B = self.B\n321:        T = self.T\n322:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n323:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n324:        x = (buf[:-1]).view(B, T) # inputs\n325:        y = (buf[1:]).view(B, T) # targets\n326:        # advance current position and load next shard if necessary\n327:        self.current_position += B * T * self.num_processes\n328:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n329:            self.advance()\n330:        return x.cuda(), y.cuda()\n331:\n332:# -----------------------------------------------------------------------------\n333:# int main\n334:\n335:@dataclass\n336:class Hyperparameters:\n337:    # data hyperparams\n338:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n339:    # optimization hyperparams\n340:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n341:    device_batch_size : int = 64 # batch size, in sequences, per device\n342:    sequence_length : int = 1024 # sequence length, in tokens\n343:    num_iterations : int = 10000 # number of iterations to run\n344:    warmup_iters : int = 200\n345:    warmdown_iters : int = 9800 # Set to num_iterations - warmup_iters for cosine decay logic later\n346:    weight_decay : float = 0\n347:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n348:\n349:\n350:def main():\n351:    with open(sys.argv[0]) as f:\n352:        code = f.read() # read the code of this file ASAP, for logging\n353:    args = Hyperparameters()\n354:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n355:\n356:    # set up DDP (distributed data parallel). torchrun sets this env variable\n357:    assert torch.cuda.is_available()\n358:    dist.init_process_group(backend='nccl')\n359:    ddp_rank = int(os.environ['RANK'])\n360:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n361:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n362:    device = f'cuda:{ddp_local_rank}'\n363:    torch.cuda.set_device(device)\n364:    print(f\"using device: {device}\")\n365:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n366:\n367:    # convenience variables\n368:    B, T = args.device_batch_size, args.sequence_length\n369:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n370:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n371:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n372:\n373:    # load tokens\n374:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n375:    if master_process:\n376:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n377:    x, y = train_loader.next_batch()\n378:\n379:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n380:    # this originates from Karpathy's experiments.\n381:    num_vocab = 50304\n382:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n383:    model = model.cuda()\n384:    if hasattr(config, \"coordinate_descent_tuning\"):\n385:        config.coordinate_descent_tuning = True # suggested by @Chillee\n386:    model = torch.compile(model)\n387:    # here we wrap model into DDP container\n388:    model = DDP(model, device_ids=[ddp_local_rank])\n389:    raw_model = model.module # always contains the \"raw\" unwrapped model\n390:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n391:\n392:    # init the optimizer\n393:    # Use AdamW for all parameters\n394:    optimizer = torch.optim.AdamW(raw_model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, fused=True)\n395:    optimizers = [optimizer] # Keep the list structure for scheduler compatibility\n396:    # learning rate decay scheduler (linear warmup and warmdown)\n397:    def get_lr(it):\n398:        assert it <= args.num_iterations\n399:        lr = 0.0 # default value, can adjust min LR later if needed\n400:        \n401:        # 1) linear warmup for warmup_iters steps\n402:        if it < args.warmup_iters:\n403:            lr = (it+1) / args.warmup_iters\n404:        # 2) cosine decay down to min learning rate (e.g., 10% of peak)\n405:        elif it < args.num_iterations:\n406:            decay_ratio = (it - args.warmup_iters) / (args.num_iterations - args.warmup_iters)\n407:            assert 0 <= decay_ratio <= 1\n408:            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n409:            min_lr_ratio = 0.1 # Let's decay to 10% of the peak LR\n410:            lr = min_lr_ratio + (1.0 - min_lr_ratio) * coeff\n411:        # 3) Keep min LR for the final step if needed (shouldn't be reached with current loop logic)\n412:        else: \n413:            lr = 0.1 # Keep min lr at the very end\n414:\n415:        return lr # The LambdaLR scheduler multiplies this factor by the base LR of each optimizer\n416:\n417:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n418:\n419:    # begin logging\n420:    if master_process:\n421:        run_id = str(uuid.uuid4())\n422:        logdir = 'logs/%s/' % run_id\n423:        os.makedirs(logdir, exist_ok=True)\n424:        logfile = 'logs/%s.txt' % run_id\n425:        # create the log file\n426:        with open(logfile, \"w\") as f:\n427:            # begin the log by printing this file (the Python code)\n428:            f.write('='*100 + '\\n')\n429:            f.write(code)\n430:            f.write('='*100 + '\\n')\n431:            # log information about the hardware/software environment this is running on\n432:            # and print the full `nvidia-smi` to file\n433:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n434:            import subprocess\n435:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n436:            f.write(f'{result.stdout}\\n')\n437:            f.write('='*100 + '\\n')\n438:\n439:    training_time_ms = 0\n440:    # start the clock\n441:    torch.cuda.synchronize()\n442:    t0 = time.time()\n443:    # begin training\n444:    train_loader.reset()\n445:    for step in range(args.num_iterations + 1):\n446:        last_step = (step == args.num_iterations)\n447:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n448:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n449:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n450:        if step == 10:\n451:            training_time_ms = 0\n452:            t0 = time.time()\n453:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n454:\n455:        if last_step:\n456:            break\n457:\n458:        # --------------- TRAINING SECTION BEGIN -----------------\n459:        model.train()\n460:        for i in range(1, train_accumulation_steps+1):\n461:            # forward pass\n462:            with ctx:\n463:                _, loss = model(x, y, return_logits=False)\n464:                train_loss = loss.detach()\n465:            # advance the dataset for the next batch\n466:            x, y = train_loader.next_batch()\n467:            # backward pass\n468:            if i < train_accumulation_steps:\n469:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n470:                    loss.backward()\n471:            else:\n472:                loss.backward() # just sync on the last step\n473:        for p in model.parameters():\n474:            p.grad /= train_accumulation_steps\n475:        # step the optimizers and schedulers\n476:        for opt, sched in zip(optimizers, schedulers):\n477:            opt.step()\n478:            sched.step()\n479:        # null the gradients\n480:        model.zero_grad(set_to_none=True)\n481:        # --------------- TRAINING SECTION END -------------------\n482:        # everything that follows now is just diagnostics, prints, logging, etc.\n483:\n484:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n485:        if master_process:\n486:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n487:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n488:            with open(logfile, \"a\") as f:\n489:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n490:\n491:    torch.save(model.state_dict(), \"model.pt\")\n492:    with open(\"model_config.pt\", \"wb\") as f:\n493:        pickle.dump(raw_model.get_config(), f)\n494:    f.close()\n495:\n496:    if master_process:\n497:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n498:\n499:    # -------------------------------------------------------------------------\n500:    # clean up nice\n501:    dist.destroy_process_group()\n502:\n503:if __name__ == \"__main__\":\n504:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 22, Remaining Steps: 28)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe data path has been corrected. Let's try running the training script again with `torchrun`.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThe data path has been corrected. Let's try running the training script again with `torchrun`.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "W0326 03:22:16.539000 1555 site-packages/torch/distributed/run.py:793] \nW0326 03:22:16.539000 1555 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:22:16.539000 1555 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:22:16.539000 1555 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\nusing device: cuda:1\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nstep:1/10000 train_loss:10.8258 train_time:57612ms step_avg:nanms\nstep:2/10000 train_loss:10.8255 train_time:69349ms step_avg:nanms\nstep:3/10000 train_loss:10.8253 train_time:69414ms step_avg:nanms\nstep:4/10000 train_loss:10.8248 train_time:69576ms step_avg:nanms\nstep:5/10000 train_loss:10.8237 train_time:69739ms step_avg:nanms\nstep:6/10000 train_loss:10.8220 train_time:69902ms step_avg:nanms\nstep:7/10000 train_loss:10.8195 train_time:70065ms step_avg:nanms\nstep:8/10000 train_loss:10.8165 train_time:70231ms step_avg:nanms\nstep:9/10000 train_loss:10.8117 train_time:70406ms step_avg:nanms\nstep:10/10000 train_loss:10.8054 train_time:70574ms step_avg:nanms\nstep:11/10000 train_loss:10.7964 train_time:65ms step_avg:nanms\nstep:12/10000 train_loss:10.7854 train_time:230ms step_avg:nanms\nstep:13/10000 train_loss:10.7715 train_time:394ms step_avg:131.47ms\nstep:14/10000 train_loss:10.7530 train_time:563ms step_avg:140.68ms\nstep:15/10000 train_loss:10.7352 train_time:735ms step_avg:146.92ms\nstep:16/10000 train_loss:10.7107 train_time:902ms step_avg:150.27ms\nstep:17/10000 train_loss:10.6921 train_time:1066ms step_avg:152.35ms\nstep:18/10000 train_loss:10.6619 train_time:1230ms step_avg:153.72ms\nstep:19/10000 train_loss:10.6325 train_time:1394ms step_avg:154.85ms\nstep:20/10000 train_loss:10.6056 train_time:1559ms step_avg:155.94ms\nstep:21/10000 train_loss:10.5739 train_time:1728ms step_avg:157.13ms\nstep:22/10000 train_loss:10.5373 train_time:1893ms step_avg:157.75ms\nstep:23/10000 train_loss:10.5061 train_time:2058ms step_avg:158.31ms\nstep:24/10000 train_loss:10.4714 train_time:2226ms step_avg:158.99ms\nstep:25/10000 train_loss:10.4389 train_time:2389ms step_avg:159.25ms\nstep:26/10000 train_loss:10.4242 train_time:2558ms step_avg:159.85ms\nstep:27/10000 train_loss:10.3629 train_time:2724ms step_avg:160.26ms\nstep:28/10000 train_loss:10.3289 train_time:2890ms step_avg:160.56ms\nstep:29/10000 train_loss:10.2807 train_time:3056ms step_avg:160.82ms\nstep:30/10000 train_loss:10.2565 train_time:3219ms step_avg:160.97ms\nstep:31/10000 train_loss:10.1978 train_time:3384ms step_avg:161.15ms\nstep:32/10000 train_loss:10.1327 train_time:3548ms step_avg:161.25ms\nstep:33/10000 train_loss:10.1191 train_time:3718ms step_avg:161.64ms\nstep:34/10000 train_loss:10.0625 train_time:3882ms step_avg:161.75ms\nstep:35/10000 train_loss:10.0230 train_time:4050ms step_avg:162.01ms\nstep:36/10000 train_loss:9.9878 train_time:4213ms step_avg:162.05ms\nstep:37/10000 train_loss:9.9189 train_time:4381ms step_avg:162.26ms\nstep:38/10000 train_loss:9.8888 train_time:4546ms step_avg:162.34ms\nstep:39/10000 train_loss:9.8309 train_time:4713ms step_avg:162.52ms\nstep:40/10000 train_loss:9.7969 train_time:4879ms step_avg:162.63ms\nstep:41/10000 train_loss:9.7565 train_time:5049ms step_avg:162.86ms\nstep:42/10000 train_loss:9.6892 train_time:5212ms step_avg:162.86ms\nstep:43/10000 train_loss:9.6401 train_time:5376ms step_avg:162.90ms\nstep:44/10000 train_loss:9.5886 train_time:5545ms step_avg:163.09ms\nstep:45/10000 train_loss:9.5121 train_time:5708ms step_avg:163.08ms\nstep:46/10000 train_loss:9.4664 train_time:5872ms step_avg:163.12ms\nstep:47/10000 train_loss:9.4413 train_time:6041ms step_avg:163.28ms\nstep:48/10000 train_loss:9.3869 train_time:6210ms step_avg:163.42ms\nstep:49/10000 train_loss:9.3355 train_time:6372ms step_avg:163.39ms\nstep:50/10000 train_loss:9.2839 train_time:6541ms step_avg:163.53ms\nstep:51/10000 train_loss:9.2958 train_time:6708ms step_avg:163.60ms\nstep:52/10000 train_loss:9.1764 train_time:6873ms step_avg:163.64ms\nstep:53/10000 train_loss:9.1613 train_time:7045ms step_avg:163.84ms\nstep:54/10000 train_loss:9.0863 train_time:7213ms step_avg:163.93ms\nstep:55/10000 train_loss:8.9653 train_time:7376ms step_avg:163.92ms\nstep:56/10000 train_loss:8.9825 train_time:7544ms step_avg:164.00ms\nstep:57/10000 train_loss:8.9233 train_time:7711ms step_avg:164.05ms\nstep:58/10000 train_loss:8.8872 train_time:7874ms step_avg:164.05ms\nstep:59/10000 train_loss:8.8324 train_time:8044ms step_avg:164.16ms\nstep:60/10000 train_loss:8.8212 train_time:8207ms step_avg:164.14ms\nstep:61/10000 train_loss:8.7849 train_time:8373ms step_avg:164.18ms\nstep:62/10000 train_loss:8.7181 train_time:8537ms step_avg:164.18ms\nstep:63/10000 train_loss:8.6493 train_time:8703ms step_avg:164.21ms\nstep:64/10000 train_loss:8.5927 train_time:8870ms step_avg:164.25ms\nstep:65/10000 train_loss:8.6563 train_time:9037ms step_avg:164.31ms\nstep:66/10000 train_loss:8.5959 train_time:9201ms step_avg:164.30ms\nstep:67/10000 train_loss:8.4156 train_time:9368ms step_avg:164.35ms\nstep:68/10000 train_loss:8.4062 train_time:9532ms step_avg:164.35ms\nstep:69/10000 train_loss:8.4304 train_time:9700ms step_avg:164.41ms\nstep:70/10000 train_loss:8.3786 train_time:9867ms step_avg:164.45ms\nstep:71/10000 train_loss:8.3654 train_time:10038ms step_avg:164.56ms\nstep:72/10000 train_loss:8.5040 train_time:10201ms step_avg:164.53ms\nstep:73/10000 train_loss:8.2692 train_time:10370ms step_avg:164.60ms\nstep:74/10000 train_loss:8.2040 train_time:10536ms step_avg:164.62ms\nstep:75/10000 train_loss:8.2607 train_time:10703ms step_avg:164.66ms\nstep:76/10000 train_loss:8.1474 train_time:10869ms step_avg:164.68ms\nstep:77/10000 train_loss:8.0876 train_time:11037ms step_avg:164.73ms\nstep:78/10000 train_loss:7.9232 train_time:11203ms step_avg:164.76ms\nstep:79/10000 train_loss:8.0679 train_time:11371ms step_avg:164.79ms\nstep:80/10000 train_loss:8.1221 train_time:11535ms step_avg:164.79ms\nstep:81/10000 train_loss:7.9723 train_time:11701ms step_avg:164.80ms\nstep:82/10000 train_loss:8.0687 train_time:11871ms step_avg:164.87ms\nstep:83/10000 train_loss:7.8928 train_time:12038ms step_avg:164.90ms\nstep:84/10000 train_loss:7.8787 train_time:12206ms step_avg:164.94ms\nstep:85/10000 train_loss:7.9864 train_time:12373ms step_avg:164.97ms\nstep:86/10000 train_loss:8.1385 train_time:12539ms step_avg:164.99ms\nstep:87/10000 train_loss:7.9937 train_time:12707ms step_avg:165.02ms\nstep:88/10000 train_loss:7.7912 train_time:12873ms step_avg:165.03ms\nstep:89/10000 train_loss:7.8286 train_time:13042ms step_avg:165.09ms\nstep:90/10000 train_loss:7.8380 train_time:13212ms step_avg:165.15ms\nstep:91/10000 train_loss:7.7943 train_time:13379ms step_avg:165.17ms\nstep:92/10000 train_loss:7.7340 train_time:13545ms step_avg:165.18ms\nstep:93/10000 train_loss:7.6614 train_time:13711ms step_avg:165.19ms\nstep:94/10000 train_loss:7.8675 train_time:13875ms step_avg:165.18ms\nstep:95/10000 train_loss:7.7414 train_time:14044ms step_avg:165.22ms\nstep:96/10000 train_loss:7.7115 train_time:14214ms step_avg:165.28ms\nstep:97/10000 train_loss:7.6117 train_time:14382ms step_avg:165.30ms\nstep:98/10000 train_loss:7.6542 train_time:14548ms step_avg:165.32ms\nstep:99/10000 train_loss:7.5525 train_time:14714ms step_avg:165.32ms\nstep:100/10000 train_loss:7.5125 train_time:14883ms step_avg:165.37ms\nstep:101/10000 train_loss:7.6417 train_time:15051ms step_avg:165.39ms\nstep:102/10000 train_loss:7.5764 train_time:15219ms step_avg:165.42ms\nstep:103/10000 train_loss:7.6352 train_time:15390ms step_avg:165.49ms\nstep:104/10000 train_loss:7.6687 train_time:15555ms step_avg:165.48ms\nstep:105/10000 train_loss:7.6261 train_time:15721ms step_avg:165.48ms\nstep:106/10000 train_loss:7.6709 train_time:15891ms step_avg:165.54ms\nstep:107/10000 train_loss:7.6467 train_time:16060ms step_avg:165.57ms\nstep:108/10000 train_loss:7.5979 train_time:16230ms step_avg:165.61ms\nstep:109/10000 train_loss:7.6933 train_time:16395ms step_avg:165.61ms\nstep:110/10000 train_loss:7.4468 train_time:16561ms step_avg:165.61ms\nstep:111/10000 train_loss:7.5137 train_time:16727ms step_avg:165.61ms\nstep:112/10000 train_loss:7.6115 train_time:16895ms step_avg:165.64ms\nstep:113/10000 train_loss:7.6128 train_time:17062ms step_avg:165.65ms\nstep:114/10000 train_loss:7.6078 train_time:17232ms step_avg:165.69ms\nstep:115/10000 train_loss:7.6011 train_time:17403ms step_avg:165.75ms\nstep:116/10000 train_loss:7.5246 train_time:17573ms step_avg:165.78ms\nstep:117/10000 train_loss:7.5299 train_time:17741ms step_avg:165.80ms\nstep:118/10000 train_loss:7.5272 train_time:17908ms step_avg:165.81ms\nstep:119/10000 train_loss:7.5730 train_time:18076ms step_avg:165.83ms\nstep:120/10000 train_loss:7.5873 train_time:18244ms step_avg:165.86ms\nstep:121/10000 train_loss:7.4671 train_time:18411ms step_avg:165.86ms\nstep:122/10000 train_loss:7.4531 train_time:18578ms step_avg:165.88ms\nstep:123/10000 train_loss:7.4181 train_time:18746ms step_avg:165.89ms\nstep:124/10000 train_loss:7.8958 train_time:18913ms step_avg:165.90ms\nstep:125/10000 train_loss:7.4670 train_time:19082ms step_avg:165.93ms\nstep:126/10000 train_loss:7.2848 train_time:19252ms step_avg:165.96ms\nstep:127/10000 train_loss:7.4183 train_time:19420ms step_avg:165.98ms\nstep:128/10000 train_loss:7.4048 train_time:19590ms step_avg:166.02ms\nstep:129/10000 train_loss:7.5265 train_time:19759ms step_avg:166.04ms\nstep:130/10000 train_loss:7.6298 train_time:19928ms step_avg:166.07ms\nstep:131/10000 train_loss:7.4047 train_time:20094ms step_avg:166.06ms\nstep:132/10000 train_loss:7.3360 train_time:20262ms step_avg:166.08ms\nstep:133/10000 train_loss:7.3964 train_time:20430ms step_avg:166.10ms\nstep:134/10000 train_loss:7.3842 train_time:20596ms step_avg:166.10ms\nstep:135/10000 train_loss:7.3975 train_time:20763ms step_avg:166.10ms\nstep:136/10000 train_loss:7.3252 train_time:20935ms step_avg:166.15ms\nstep:137/10000 train_loss:7.3305 train_time:21106ms step_avg:166.19ms\nstep:138/10000 train_loss:7.4776 train_time:21273ms step_avg:166.19ms\nstep:139/10000 train_loss:7.3155 train_time:21442ms step_avg:166.22ms\nstep:140/10000 train_loss:7.3974 train_time:21610ms step_avg:166.23ms\nstep:141/10000 train_loss:7.2573 train_time:21779ms step_avg:166.25ms\nstep:142/10000 train_loss:7.4407 train_time:21947ms step_avg:166.26ms\nstep:143/10000 train_loss:7.2208 train_time:22114ms step_avg:166.27ms\nstep:144/10000 train_loss:7.3711 train_time:22280ms step_avg:166.27ms\nstep:145/10000 train_loss:7.2706 train_time:22449ms step_avg:166.29ms\nstep:146/10000 train_loss:7.2713 train_time:22620ms step_avg:166.33ms\nstep:147/10000 train_loss:7.1704 train_time:22789ms step_avg:166.34ms\nstep:148/10000 train_loss:7.2885 train_time:22955ms step_avg:166.34ms\nstep:149/10000 train_loss:6.9866 train_time:23123ms step_avg:166.35ms\nstep:150/10000 train_loss:7.1613 train_time:23292ms step_avg:166.37ms\nstep:151/10000 train_loss:7.1938 train_time:23460ms step_avg:166.38ms\nstep:152/10000 train_loss:7.1506 train_time:23630ms step_avg:166.41ms\nstep:153/10000 train_loss:7.3162 train_time:23797ms step_avg:166.41ms\nstep:154/10000 train_loss:7.2149 train_time:23970ms step_avg:166.46ms\nstep:155/10000 train_loss:7.0380 train_time:24141ms step_avg:166.49ms\nstep:156/10000 train_loss:7.2263 train_time:24312ms step_avg:166.52ms\nstep:157/10000 train_loss:7.1835 train_time:24478ms step_avg:166.52ms\nstep:158/10000 train_loss:7.0823 train_time:24647ms step_avg:166.53ms\nstep:159/10000 train_loss:7.1364 train_time:24816ms step_avg:166.55ms\nstep:160/10000 train_loss:7.0387 train_time:24988ms step_avg:166.59ms\nstep:161/10000 train_loss:7.1819 train_time:25156ms step_avg:166.60ms\nstep:162/10000 train_loss:7.0827 train_time:25326ms step_avg:166.62ms\nstep:163/10000 train_loss:6.9844 train_time:25493ms step_avg:166.62ms\nstep:164/10000 train_loss:7.1264 train_time:25660ms step_avg:166.62ms\nstep:165/10000 train_loss:6.9666 train_time:25827ms step_avg:166.63ms\nstep:166/10000 train_loss:6.9060 train_time:25995ms step_avg:166.64ms\nstep:167/10000 train_loss:6.8705 train_time:26165ms step_avg:166.66ms\nstep:168/10000 train_loss:7.0584 train_time:26333ms step_avg:166.66ms\nstep:169/10000 train_loss:7.1540 train_time:26500ms step_avg:166.67ms\nstep:170/10000 train_loss:6.9736 train_time:26668ms step_avg:166.68ms\nstep:171/10000 train_loss:7.2091 train_time:26837ms step_avg:166.69ms\nstep:172/10000 train_loss:6.9989 train_time:27007ms step_avg:166.71ms\nstep:173/10000 train_loss:7.0192 train_time:27174ms step_avg:166.71ms\nstep:174/10000 train_loss:6.9326 train_time:27347ms step_avg:166.75ms\nstep:175/10000 train_loss:6.9110 train_time:27514ms step_avg:166.75ms\nstep:176/10000 train_loss:6.9408 train_time:27684ms step_avg:166.77ms\nstep:177/10000 train_loss:6.9093 train_time:27850ms step_avg:166.77ms\nstep:178/10000 train_loss:7.1229 train_time:28021ms step_avg:166.79ms\nstep:179/10000 train_loss:7.0903 train_time:28190ms step_avg:166.80ms\nstep:180/10000 train_loss:6.8180 train_time:28362ms step_avg:166.83ms\nstep:181/10000 train_loss:6.8361 train_time:28530ms step_avg:166.84ms\nstep:182/10000 train_loss:6.9095 train_time:28697ms step_avg:166.84ms\nstep:183/10000 train_loss:6.9174 train_time:28866ms step_avg:166.86ms\nstep:184/10000 train_loss:6.8582 train_time:29035ms step_avg:166.86ms\nstep:185/10000 train_loss:6.9522 train_time:29204ms step_avg:166.88ms\nstep:186/10000 train_loss:6.7615 train_time:29374ms step_avg:166.90ms\nstep:187/10000 train_loss:7.0200 train_time:29542ms step_avg:166.91ms\nstep:188/10000 train_loss:6.7594 train_time:29711ms step_avg:166.92ms\nstep:189/10000 train_loss:6.9278 train_time:29882ms step_avg:166.94ms\nstep:190/10000 train_loss:6.9543 train_time:30053ms step_avg:166.96ms\nstep:191/10000 train_loss:6.7982 train_time:30222ms step_avg:166.97ms\nstep:192/10000 train_loss:6.9477 train_time:30391ms step_avg:166.98ms\nstep:193/10000 train_loss:6.7775 train_time:30558ms step_avg:166.98ms\nstep:194/10000 train_loss:6.8080 train_time:30729ms step_avg:167.01ms\nstep:195/10000 train_loss:6.8384 train_time:30896ms step_avg:167.00ms\nstep:196/10000 train_loss:6.6252 train_time:31066ms step_avg:167.02ms\nstep:197/10000 train_loss:6.7824 train_time:31234ms step_avg:167.03ms\nstep:198/10000 train_loss:6.9396 train_time:31402ms step_avg:167.03ms\nstep:199/10000 train_loss:6.7779 train_time:31569ms step_avg:167.03ms\nstep:200/10000 train_loss:6.7832 train_time:31739ms step_avg:167.05ms\nstep:201/10000 train_loss:6.7363 train_time:31907ms step_avg:167.05ms\nstep:202/10000 train_loss:6.7864 train_time:32076ms step_avg:167.06ms\nstep:203/10000 train_loss:6.3636 train_time:32245ms step_avg:167.07ms\nstep:204/10000 train_loss:6.8664 train_time:32414ms step_avg:167.08ms\nstep:205/10000 train_loss:6.7396 train_time:32583ms step_avg:167.09ms\nstep:206/10000 train_loss:6.5998 train_time:32753ms step_avg:167.11ms\nstep:207/10000 train_loss:6.6561 train_time:32923ms step_avg:167.12ms\nstep:208/10000 train_loss:6.7902 train_time:33093ms step_avg:167.14ms\nstep:209/10000 train_loss:6.7489 train_time:33262ms step_avg:167.15ms\nstep:210/10000 train_loss:6.7164 train_time:33431ms step_avg:167.16ms\nstep:211/10000 train_loss:6.7350 train_time:33600ms step_avg:167.17ms\nstep:212/10000 train_loss:6.7127 train_time:33771ms step_avg:167.18ms\nstep:213/10000 train_loss:6.8698 train_time:33940ms step_avg:167.19ms\nstep:214/10000 train_loss:6.6396 train_time:34109ms step_avg:167.20ms\nstep:215/10000 train_loss:6.6423 train_time:34277ms step_avg:167.21ms\nstep:216/10000 train_loss:6.7213 train_time:34447ms step_avg:167.22ms\nstep:217/10000 train_loss:6.5093 train_time:34618ms step_avg:167.24ms\nstep:218/10000 train_loss:6.9019 train_time:34786ms step_avg:167.24ms\nstep:219/10000 train_loss:6.7298 train_time:34955ms step_avg:167.25ms\nstep:220/10000 train_loss:6.5775 train_time:35124ms step_avg:167.26ms\nstep:221/10000 train_loss:6.6136 train_time:35293ms step_avg:167.27ms\nstep:222/10000 train_loss:6.5941 train_time:35461ms step_avg:167.27ms\nstep:223/10000 train_loss:6.5562 train_time:35629ms step_avg:167.27ms\nstep:224/10000 train_loss:6.5533 train_time:35799ms step_avg:167.28ms\nstep:225/10000 train_loss:6.5901 train_time:35971ms step_avg:167.31ms\nstep:226/10000 train_loss:6.4625 train_time:36139ms step_avg:167.31ms\nstep:227/10000 train_loss:6.6837 train_time:36308ms step_avg:167.32ms\nstep:228/10000 train_loss:6.5618 train_time:36478ms step_avg:167.33ms\nstep:229/10000 train_loss:6.6134 train_time:36650ms step_avg:167.35ms\nstep:230/10000 train_loss:6.6338 train_time:36821ms step_avg:167.37ms\nstep:231/10000 train_loss:6.7040 train_time:36993ms step_avg:167.39ms\nstep:232/10000 train_loss:6.5538 train_time:37164ms step_avg:167.40ms\nstep:233/10000 train_loss:6.4314 train_time:37332ms step_avg:167.41ms\nstep:234/10000 train_loss:6.5103 train_time:37500ms step_avg:167.41ms\nstep:235/10000 train_loss:6.5722 train_time:37670ms step_avg:167.42ms\nstep:236/10000 train_loss:6.6165 train_time:37838ms step_avg:167.43ms\nstep:237/10000 train_loss:6.5593 train_time:38006ms step_avg:167.43ms\nstep:238/10000 train_loss:6.6316 train_time:38174ms step_avg:167.43ms\nstep:239/10000 train_loss:6.6563 train_time:38342ms step_avg:167.43ms\nstep:240/10000 train_loss:6.4777 train_time:38513ms step_avg:167.45ms\nstep:241/10000 train_loss:6.5924 train_time:38684ms step_avg:167.46ms\nstep:242/10000 train_loss:6.4184 train_time:38852ms step_avg:167.47ms\nstep:243/10000 train_loss:6.4077 train_time:39025ms step_avg:167.49ms\nstep:244/10000 train_loss:6.5265 train_time:39194ms step_avg:167.50ms\nstep:245/10000 train_loss:6.5537 train_time:39364ms step_avg:167.51ms\nstep:246/10000 train_loss:6.4864 train_time:39534ms step_avg:167.52ms\nstep:247/10000 train_loss:6.3764 train_time:39702ms step_avg:167.52ms\nstep:248/10000 train_loss:6.5378 train_time:39873ms step_avg:167.53ms\nstep:249/10000 train_loss:6.6541 train_time:40041ms step_avg:167.53ms\nstep:250/10000 train_loss:6.4147 train_time:40209ms step_avg:167.54ms\nstep:251/10000 train_loss:6.4098 train_time:40377ms step_avg:167.54ms\nstep:252/10000 train_loss:6.4505 train_time:40545ms step_avg:167.54ms\nstep:253/10000 train_loss:6.5016 train_time:40713ms step_avg:167.54ms\nstep:254/10000 train_loss:7.0553 train_time:40883ms step_avg:167.55ms\nstep:255/10000 train_loss:6.4180 train_time:41052ms step_avg:167.56ms\nstep:256/10000 train_loss:6.4031 train_time:41222ms step_avg:167.57ms\nstep:257/10000 train_loss:6.5660 train_time:41390ms step_avg:167.57ms\nstep:258/10000 train_loss:6.2501 train_time:41559ms step_avg:167.58ms\nstep:259/10000 train_loss:6.4642 train_time:41727ms step_avg:167.58ms\nstep:260/10000 train_loss:6.2063 train_time:41894ms step_avg:167.58ms\nstep:261/10000 train_loss:6.3993 train_time:42063ms step_avg:167.58ms\nstep:262/10000 train_loss:6.4151 train_time:42232ms step_avg:167.59ms\nstep:263/10000 train_loss:6.3821 train_time:42404ms step_avg:167.61ms\nstep:264/10000 train_loss:6.2481 train_time:42573ms step_avg:167.61ms\nstep:265/10000 train_loss:6.4949 train_time:42744ms step_avg:167.62ms\nstep:266/10000 train_loss:6.2907 train_time:42914ms step_avg:167.63ms\nstep:267/10000 train_loss:6.3345 train_time:43083ms step_avg:167.64ms\nstep:268/10000 train_loss:6.4682 train_time:43253ms step_avg:167.65ms\nstep:269/10000 train_loss:6.4501 train_time:43423ms step_avg:167.66ms\nstep:270/10000 train_loss:6.5942 train_time:43589ms step_avg:167.65ms\nstep:271/10000 train_loss:6.3225 train_time:43762ms step_avg:167.67ms\nstep:272/10000 train_loss:6.4279 train_time:43930ms step_avg:167.67ms\nstep:273/10000 train_loss:6.5661 train_time:44097ms step_avg:167.67ms\nstep:274/10000 train_loss:6.1756 train_time:44267ms step_avg:167.68ms\nstep:275/10000 train_loss:6.2454 train_time:44437ms step_avg:167.69ms\nstep:276/10000 train_loss:6.4630 train_time:44606ms step_avg:167.69ms\nstep:277/10000 train_loss:6.3457 train_time:44776ms step_avg:167.70ms\nstep:278/10000 train_loss:6.2340 train_time:44948ms step_avg:167.72ms\nstep:279/10000 train_loss:6.3273 train_time:45115ms step_avg:167.71ms\nstep:280/10000 train_loss:6.2647 train_time:45281ms step_avg:167.71ms\nstep:281/10000 train_loss:6.4302 train_time:45449ms step_avg:167.71ms\nstep:282/10000 train_loss:6.4545 train_time:45619ms step_avg:167.72ms\nstep:283/10000 train_loss:6.2753 train_time:45787ms step_avg:167.72ms\nstep:284/10000 train_loss:6.4237 train_time:45959ms step_avg:167.73ms\nstep:285/10000 train_loss:6.5528 train_time:46128ms step_avg:167.74ms\nstep:286/10000 train_loss:6.3171 train_time:46296ms step_avg:167.74ms\nstep:287/10000 train_loss:6.4594 train_time:46465ms step_avg:167.74ms\nstep:288/10000 train_loss:6.3288 train_time:46635ms step_avg:167.75ms\nstep:289/10000 train_loss:6.2447 train_time:46806ms step_avg:167.76ms\nstep:290/10000 train_loss:6.2156 train_time:46975ms step_avg:167.77ms\nstep:291/10000 train_loss:6.4156 train_time:47144ms step_avg:167.77ms\nstep:292/10000 train_loss:6.3627 train_time:47314ms step_avg:167.78ms\nstep:293/10000 train_loss:6.3688 train_time:47484ms step_avg:167.79ms\nstep:294/10000 train_loss:6.2978 train_time:47651ms step_avg:167.78ms\nstep:295/10000 train_loss:6.0946 train_time:47819ms step_avg:167.79ms\nstep:296/10000 train_loss:6.0550 train_time:47989ms step_avg:167.79ms\nstep:297/10000 train_loss:6.3389 train_time:48157ms step_avg:167.79ms\nstep:298/10000 train_loss:6.3086 train_time:48328ms step_avg:167.81ms\nstep:299/10000 train_loss:6.1746 train_time:48498ms step_avg:167.81ms\nstep:300/10000 train_loss:6.1585 train_time:48668ms step_avg:167.82ms\nstep:301/10000 train_loss:6.2522 train_time:48837ms step_avg:167.83ms\nstep:302/10000 train_loss:6.2909 train_time:49007ms step_avg:167.83ms\nstep:303/10000 train_loss:6.1135 train_time:49175ms step_avg:167.83ms\nstep:304/10000 train_loss:6.1370 train_time:49346ms step_avg:167.84ms\nstep:305/10000 train_loss:6.2381 train_time:49511ms step_avg:167.83ms\nstep:306/10000 train_loss:6.3186 train_time:49682ms step_avg:167.84ms\nstep:307/10000 train_loss:6.2914 train_time:49854ms step_avg:167.86ms\nstep:308/10000 train_loss:6.3201 train_time:50021ms step_avg:167.86ms\nstep:309/10000 train_loss:6.2369 train_time:50191ms step_avg:167.86ms\nstep:310/10000 train_loss:6.4667 train_time:50361ms step_avg:167.87ms\nstep:311/10000 train_loss:6.2652 train_time:50528ms step_avg:167.87ms\nstep:312/10000 train_loss:6.1732 train_time:50697ms step_avg:167.87ms\nstep:313/10000 train_loss:6.2785 train_time:50867ms step_avg:167.88ms\nstep:314/10000 train_loss:6.2922 train_time:51038ms step_avg:167.89ms\nstep:315/10000 train_loss:6.0349 train_time:51209ms step_avg:167.90ms\nstep:316/10000 train_loss:5.7398 train_time:51380ms step_avg:167.91ms\nstep:317/10000 train_loss:6.1947 train_time:51550ms step_avg:167.91ms\nstep:318/10000 train_loss:6.3358 train_time:51719ms step_avg:167.92ms\nstep:319/10000 train_loss:6.0446 train_time:51888ms step_avg:167.92ms\nstep:320/10000 train_loss:6.2206 train_time:52057ms step_avg:167.92ms\nstep:321/10000 train_loss:6.1847 train_time:52227ms step_avg:167.93ms\nstep:322/10000 train_loss:6.0544 train_time:52395ms step_avg:167.93ms\nstep:323/10000 train_loss:6.1647 train_time:52563ms step_avg:167.93ms\nstep:324/10000 train_loss:6.2400 train_time:52734ms step_avg:167.94ms\nstep:325/10000 train_loss:6.1633 train_time:52902ms step_avg:167.94ms\nstep:326/10000 train_loss:6.2394 train_time:53071ms step_avg:167.95ms\nstep:327/10000 train_loss:6.1611 train_time:53241ms step_avg:167.95ms\nstep:328/10000 train_loss:6.1283 train_time:53410ms step_avg:167.96ms\nstep:329/10000 train_loss:6.7143 train_time:53578ms step_avg:167.96ms\nstep:330/10000 train_loss:6.2189 train_time:53747ms step_avg:167.96ms\nstep:331/10000 train_loss:6.0045 train_time:53917ms step_avg:167.96ms\nstep:332/10000 train_loss:6.2174 train_time:54086ms step_avg:167.97ms\nstep:333/10000 train_loss:6.2008 train_time:54256ms step_avg:167.97ms\nstep:334/10000 train_loss:6.0835 train_time:54426ms step_avg:167.98ms\nstep:335/10000 train_loss:6.1168 train_time:54592ms step_avg:167.98ms\nstep:336/10000 train_loss:6.0728 train_time:54761ms step_avg:167.98ms\nstep:337/10000 train_loss:6.1879 train_time:54929ms step_avg:167.98ms\nstep:338/10000 train_loss:5.9547 train_time:55101ms step_avg:167.99ms\nstep:339/10000 train_loss:5.9767 train_time:55272ms step_avg:168.00ms\nstep:340/10000 train_loss:6.1660 train_time:55442ms step_avg:168.01ms\nstep:341/10000 train_loss:6.1366 train_time:55611ms step_avg:168.01ms\nstep:342/10000 train_loss:6.1683 train_time:55780ms step_avg:168.01ms\nstep:343/10000 train_loss:6.1332 train_time:55952ms step_avg:168.02ms\nstep:344/10000 train_loss:6.0854 train_time:56121ms step_avg:168.03ms\nstep:345/10000 train_loss:6.0865 train_time:56291ms step_avg:168.03ms\nstep:346/10000 train_loss:6.0734 train_time:56459ms step_avg:168.03ms\nstep:347/10000 train_loss:6.0629 train_time:56630ms step_avg:168.04ms\nstep:348/10000 train_loss:6.1418 train_time:56800ms step_avg:168.05ms\nstep:349/10000 train_loss:6.0668 train_time:56970ms step_avg:168.05ms\nstep:350/10000 train_loss:6.2377 train_time:57141ms step_avg:168.06ms\nstep:351/10000 train_loss:5.9990 train_time:57313ms step_avg:168.07ms\nstep:352/10000 train_loss:5.9836 train_time:57481ms step_avg:168.07ms\nstep:353/10000 train_loss:5.9444 train_time:57649ms step_avg:168.07ms\nstep:354/10000 train_loss:6.1505 train_time:57820ms step_avg:168.08ms\nstep:355/10000 train_loss:6.0826 train_time:57987ms step_avg:168.08ms\nstep:356/10000 train_loss:5.9999 train_time:58157ms step_avg:168.08ms\nstep:357/10000 train_loss:5.7083 train_time:58327ms step_avg:168.09ms\nstep:358/10000 train_loss:6.0712 train_time:58497ms step_avg:168.10ms\nstep:359/10000 train_loss:6.1163 train_time:58667ms step_avg:168.10ms\nstep:360/10000 train_loss:5.8396 train_time:58838ms step_avg:168.11ms\nstep:361/10000 train_loss:5.9396 train_time:59007ms step_avg:168.11ms\nstep:362/10000 train_loss:6.0295 train_time:59179ms step_avg:168.12ms\nstep:363/10000 train_loss:5.9961 train_time:59350ms step_avg:168.13ms\nstep:364/10000 train_loss:6.0594 train_time:59518ms step_avg:168.13ms\nstep:365/10000 train_loss:5.9944 train_time:59694ms step_avg:168.15ms\nstep:366/10000 train_loss:6.1500 train_time:59861ms step_avg:168.15ms\nstep:367/10000 train_loss:6.2499 train_time:60028ms step_avg:168.15ms\nstep:368/10000 train_loss:6.0093 train_time:60197ms step_avg:168.15ms\nstep:369/10000 train_loss:6.1727 train_time:60368ms step_avg:168.16ms\nstep:370/10000 train_loss:6.0071 train_time:60537ms step_avg:168.16ms\nstep:371/10000 train_loss:6.1119 train_time:60707ms step_avg:168.16ms\nstep:372/10000 train_loss:6.2749 train_time:60876ms step_avg:168.17ms\nstep:373/10000 train_loss:5.9539 train_time:61047ms step_avg:168.17ms\nstep:374/10000 train_loss:5.9882 train_time:61216ms step_avg:168.18ms\nstep:375/10000 train_loss:5.9906 train_time:61386ms step_avg:168.18ms\nstep:376/10000 train_loss:6.1329 train_time:61553ms step_avg:168.18ms\nstep:377/10000 train_loss:5.9149 train_time:61723ms step_avg:168.18ms\nstep:378/10000 train_loss:5.9847 train_time:61893ms step_avg:168.19ms\nstep:379/10000 train_loss:5.9994 train_time:62062ms step_avg:168.19ms\nstep:380/10000 train_loss:5.8787 train_time:62231ms step_avg:168.19ms\nstep:381/10000 train_loss:6.0874 train_time:62401ms step_avg:168.20ms\nstep:382/10000 train_loss:5.8573 train_time:62568ms step_avg:168.19ms\nstep:383/10000 train_loss:6.0960 train_time:62738ms step_avg:168.20ms\nstep:384/10000 train_loss:5.9519 train_time:62909ms step_avg:168.21ms\nstep:385/10000 train_loss:5.8671 train_time:63078ms step_avg:168.21ms\nstep:386/10000 train_loss:5.9691 train_time:63249ms step_avg:168.22ms\nstep:387/10000 train_loss:5.9201 train_time:63418ms step_avg:168.22ms\nstep:388/10000 train_loss:6.0496 train_time:63587ms step_avg:168.22ms\nstep:389/10000 train_loss:5.9124 train_time:63757ms step_avg:168.22ms\nstep:390/10000 train_loss:5.8503 train_time:63928ms step_avg:168.23ms\nstep:391/10000 train_loss:6.0069 train_time:64096ms step_avg:168.23ms\nstep:392/10000 train_loss:5.8254 train_time:64267ms step_avg:168.24ms\nstep:393/10000 train_loss:5.8690 train_time:64435ms step_avg:168.24ms\nstep:394/10000 train_loss:6.0146 train_time:64604ms step_avg:168.24ms\nstep:395/10000 train_loss:5.7569 train_time:64773ms step_avg:168.24ms\nstep:396/10000 train_loss:5.8113 train_time:64942ms step_avg:168.24ms\nstep:397/10000 train_loss:5.9616 train_time:65112ms step_avg:168.25ms\nstep:398/10000 train_loss:5.9704 train_time:65284ms step_avg:168.26ms\nstep:399/10000 train_loss:5.8422 train_time:65451ms step_avg:168.25ms\nstep:400/10000 train_loss:5.8290 train_time:65624ms step_avg:168.27ms\nstep:401/10000 train_loss:5.6635 train_time:65793ms step_avg:168.27ms\nstep:402/10000 train_loss:5.8608 train_time:65961ms step_avg:168.27ms\nstep:403/10000 train_loss:6.0188 train_time:66132ms step_avg:168.27ms\nstep:404/10000 train_loss:5.7308 train_time:66302ms step_avg:168.28ms\nstep:405/10000 train_loss:5.9749 train_time:66474ms step_avg:168.29ms\nstep:406/10000 train_loss:5.9690 train_time:66643ms step_avg:168.29ms\nstep:407/10000 train_loss:5.8996 train_time:66813ms step_avg:168.29ms\nstep:408/10000 train_loss:5.8932 train_time:66983ms step_avg:168.30ms\nstep:409/10000 train_loss:6.0240 train_time:67153ms step_avg:168.30ms\nstep:410/10000 train_loss:6.0200 train_time:67322ms step_avg:168.31ms\nstep:411/10000 train_loss:5.9889 train_time:67490ms step_avg:168.30ms\nstep:412/10000 train_loss:5.7353 train_time:67663ms step_avg:168.32ms\nstep:413/10000 train_loss:5.9537 train_time:67831ms step_avg:168.31ms\nstep:414/10000 train_loss:5.7979 train_time:67999ms step_avg:168.32ms\nstep:415/10000 train_loss:6.6224 train_time:68170ms step_avg:168.32ms\nstep:416/10000 train_loss:5.8921 train_time:68340ms step_avg:168.32ms\nstep:417/10000 train_loss:5.7117 train_time:68510ms step_avg:168.33ms\nstep:418/10000 train_loss:5.9057 train_time:68679ms step_avg:168.33ms\nstep:419/10000 train_loss:5.9195 train_time:68852ms step_avg:168.34ms\nstep:420/10000 train_loss:5.9168 train_time:69021ms step_avg:168.34ms\nstep:421/10000 train_loss:5.8039 train_time:69193ms step_avg:168.35ms\nstep:422/10000 train_loss:6.1257 train_time:69363ms step_avg:168.36ms\nstep:423/10000 train_loss:6.0179 train_time:69534ms step_avg:168.36ms\nstep:424/10000 train_loss:6.0658 train_time:69703ms step_avg:168.37ms\nstep:425/10000 train_loss:5.8734 train_time:69873ms step_avg:168.37ms\nstep:426/10000 train_loss:5.8806 train_time:70042ms step_avg:168.37ms\nstep:427/10000 train_loss:5.9937 train_time:70212ms step_avg:168.37ms\nstep:428/10000 train_loss:5.8146 train_time:70383ms step_avg:168.38ms\nstep:429/10000 train_loss:5.8995 train_time:70554ms step_avg:168.39ms\nstep:430/10000 train_loss:5.5830 train_time:70722ms step_avg:168.39ms\nstep:431/10000 train_loss:5.7216 train_time:70891ms step_avg:168.39ms\nstep:432/10000 train_loss:5.7096 train_time:71060ms step_avg:168.39ms\nstep:433/10000 train_loss:5.8726 train_time:71228ms step_avg:168.39ms\nstep:434/10000 train_loss:5.8160 train_time:71400ms step_avg:168.40ms\nstep:435/10000 train_loss:5.7456 train_time:71570ms step_avg:168.40ms\nstep:436/10000 train_loss:5.9738 train_time:71739ms step_avg:168.40ms\nstep:437/10000 train_loss:5.7680 train_time:71908ms step_avg:168.40ms\nstep:438/10000 train_loss:5.7969 train_time:72077ms step_avg:168.40ms\nstep:439/10000 train_loss:5.8245 train_time:72246ms step_avg:168.41ms\nstep:440/10000 train_loss:5.9180 train_time:72414ms step_avg:168.41ms\nstep:441/10000 train_loss:5.9313 train_time:72585ms step_avg:168.41ms\nstep:442/10000 train_loss:5.8142 train_time:72753ms step_avg:168.41ms\nstep:443/10000 train_loss:5.7846 train_time:72923ms step_avg:168.41ms\nstep:444/10000 train_loss:5.7922 train_time:73092ms step_avg:168.41ms\nstep:445/10000 train_loss:5.8353 train_time:73263ms step_avg:168.42ms\nstep:446/10000 train_loss:5.8736 train_time:73435ms step_avg:168.43ms\nstep:447/10000 train_loss:5.8787 train_time:73604ms step_avg:168.43ms\nstep:448/10000 train_loss:5.8598 train_time:73774ms step_avg:168.43ms\nstep:449/10000 train_loss:5.8219 train_time:73944ms step_avg:168.44ms\nstep:450/10000 train_loss:5.8114 train_time:74112ms step_avg:168.44ms\nstep:451/10000 train_loss:5.8633 train_time:74283ms step_avg:168.44ms\nstep:452/10000 train_loss:5.6028 train_time:74449ms step_avg:168.44ms\nstep:453/10000 train_loss:5.9861 train_time:74621ms step_avg:168.44ms\nstep:454/10000 train_loss:5.6267 train_time:74792ms step_avg:168.45ms\nstep:455/10000 train_loss:5.7640 train_time:74963ms step_avg:168.46ms\nstep:456/10000 train_loss:5.8356 train_time:75132ms step_avg:168.46ms\nstep:457/10000 train_loss:5.7340 train_time:75301ms step_avg:168.46ms\nstep:458/10000 train_loss:5.8329 train_time:75469ms step_avg:168.46ms\nstep:459/10000 train_loss:5.8104 train_time:75639ms step_avg:168.46ms\nstep:460/10000 train_loss:5.7125 train_time:75811ms step_avg:168.47ms\nstep:461/10000 train_loss:5.6769 train_time:75979ms step_avg:168.47ms\nstep:462/10000 train_loss:5.6421 train_time:76149ms step_avg:168.47ms\nstep:463/10000 train_loss:5.7867 train_time:76317ms step_avg:168.47ms\nstep:464/10000 train_loss:5.8841 train_time:76486ms step_avg:168.47ms\nstep:465/10000 train_loss:5.7568 train_time:76654ms step_avg:168.47ms\nstep:466/10000 train_loss:5.6584 train_time:76824ms step_avg:168.47ms\nstep:467/10000 train_loss:5.8135 train_time:76995ms step_avg:168.48ms\nstep:468/10000 train_loss:5.6648 train_time:77167ms step_avg:168.49ms\nstep:469/10000 train_loss:5.7536 train_time:77336ms step_avg:168.49ms\nstep:470/10000 train_loss:5.7580 train_time:77506ms step_avg:168.49ms\nstep:471/10000 train_loss:5.8162 train_time:77674ms step_avg:168.49ms\nstep:472/10000 train_loss:5.7383 train_time:77843ms step_avg:168.49ms\nstep:473/10000 train_loss:5.6244 train_time:78014ms step_avg:168.50ms\nstep:474/10000 train_loss:5.7888 train_time:78185ms step_avg:168.50ms\nstep:475/10000 train_loss:5.7726 train_time:78356ms step_avg:168.51ms\nstep:476/10000 train_loss:5.7431 train_time:78528ms step_avg:168.51ms\nstep:477/10000 train_loss:5.7809 train_time:78696ms step_avg:168.51ms\nstep:478/10000 train_loss:5.7249 train_time:78866ms step_avg:168.52ms\nstep:479/10000 train_loss:5.7002 train_time:79035ms step_avg:168.52ms\nstep:480/10000 train_loss:5.8093 train_time:79204ms step_avg:168.52ms\nstep:481/10000 train_loss:5.7128 train_time:79371ms step_avg:168.52ms\nstep:482/10000 train_loss:5.6035 train_time:79539ms step_avg:168.52ms\nstep:483/10000 train_loss:5.7124 train_time:79709ms step_avg:168.52ms\nstep:484/10000 train_loss:5.7491 train_time:79878ms step_avg:168.52ms\nstep:485/10000 train_loss:5.7834 train_time:80050ms step_avg:168.53ms\nstep:486/10000 train_loss:5.6553 train_time:80223ms step_avg:168.54ms\nstep:487/10000 train_loss:5.6531 train_time:80395ms step_avg:168.54ms\nstep:488/10000 train_loss:5.6403 train_time:80566ms step_avg:168.55ms\nstep:489/10000 train_loss:5.5055 train_time:80735ms step_avg:168.55ms\nstep:490/10000 train_loss:5.5463 train_time:80905ms step_avg:168.55ms\nstep:491/10000 train_loss:5.6823 train_time:81074ms step_avg:168.55ms\nstep:492/10000 train_loss:5.7418 train_time:81244ms step_avg:168.56ms\nstep:493/10000 train_loss:5.7172 train_time:81410ms step_avg:168.55ms\nstep:494/10000 train_loss:5.8754 train_time:81579ms step_avg:168.55ms\nstep:495/10000 train_loss:5.5767 train_time:81749ms step_avg:168.55ms\nstep:496/10000 train_loss:5.6240 train_time:81919ms step_avg:168.56ms\nstep:497/10000 train_loss:5.6687 train_time:82090ms step_avg:168.56ms\nstep:498/10000 train_loss:5.6624 train_time:82263ms step_avg:168.57ms\nstep:499/10000 train_loss:5.7073 train_time:82433ms step_avg:168.57ms\nstep:500/10000 train_loss:5.5026 train_time:82602ms step_avg:168.58ms\nstep:501/10000 train_loss:5.6244 train_time:82774ms step_avg:168.58ms\nstep:502/10000 train_loss:5.6814 train_time:82943ms step_avg:168.58ms\nstep:503/10000 train_loss:5.5779 train_time:83115ms step_avg:168.59ms\nstep:504/10000 train_loss:5.5184 train_time:83283ms step_avg:168.59ms\nstep:505/10000 train_loss:5.6213 train_time:83451ms step_avg:168.59ms\nstep:506/10000 train_loss:5.7473 train_time:83619ms step_avg:168.59ms\nstep:507/10000 train_loss:5.7748 train_time:83799ms step_avg:168.61ms\nstep:508/10000 train_loss:5.6787 train_time:83961ms step_avg:168.60ms\nstep:509/10000 train_loss:5.8309 train_time:84130ms step_avg:168.60ms\nstep:510/10000 train_loss:5.5827 train_time:84303ms step_avg:168.61ms\nstep:511/10000 train_loss:5.5582 train_time:84474ms step_avg:168.61ms\nstep:512/10000 train_loss:5.7031 train_time:84643ms step_avg:168.61ms\nstep:513/10000 train_loss:5.6260 train_time:84811ms step_avg:168.61ms\nstep:514/10000 train_loss:5.8299 train_time:84980ms step_avg:168.61ms\nstep:515/10000 train_loss:5.6465 train_time:85155ms step_avg:168.62ms\nstep:516/10000 train_loss:5.6665 train_time:85319ms step_avg:168.62ms\nstep:517/10000 train_loss:5.5641 train_time:85490ms step_avg:168.62ms\nstep:518/10000 train_loss:5.6473 train_time:85658ms step_avg:168.62ms\nstep:519/10000 train_loss:5.4211 train_time:85830ms step_avg:168.62ms\nstep:520/10000 train_loss:5.7178 train_time:85999ms step_avg:168.63ms\nstep:521/10000 train_loss:5.4951 train_time:86171ms step_avg:168.63ms\nstep:522/10000 train_loss:5.6786 train_time:86341ms step_avg:168.64ms\nstep:523/10000 train_loss:5.5376 train_time:86513ms step_avg:168.64ms\nstep:524/10000 train_loss:5.8174 train_time:86681ms step_avg:168.64ms\nstep:525/10000 train_loss:5.9170 train_time:86851ms step_avg:168.64ms\nstep:526/10000 train_loss:5.6518 train_time:87022ms step_avg:168.65ms\nstep:527/10000 train_loss:5.5852 train_time:87195ms step_avg:168.66ms\nstep:528/10000 train_loss:5.5707 train_time:87363ms step_avg:168.66ms\nstep:529/10000 train_loss:5.5839 train_time:87534ms step_avg:168.66ms\nstep:530/10000 train_loss:5.6408 train_time:87705ms step_avg:168.66ms\nstep:531/10000 train_loss:5.6133 train_time:87873ms step_avg:168.66ms\nstep:532/10000 train_loss:5.5432 train_time:88045ms step_avg:168.67ms\nstep:533/10000 train_loss:5.5975 train_time:88218ms step_avg:168.68ms\nstep:534/10000 train_loss:5.5975 train_time:88388ms step_avg:168.68ms\nstep:535/10000 train_loss:5.5591 train_time:88556ms step_avg:168.68ms\nstep:536/10000 train_loss:5.5954 train_time:88726ms step_avg:168.68ms\nstep:537/10000 train_loss:5.7121 train_time:88895ms step_avg:168.68ms\nstep:538/10000 train_loss:5.6121 train_time:89066ms step_avg:168.69ms\nstep:539/10000 train_loss:5.6645 train_time:89236ms step_avg:168.69ms\nstep:540/10000 train_loss:5.5205 train_time:89407ms step_avg:168.69ms\nstep:541/10000 train_loss:5.4689 train_time:89575ms step_avg:168.69ms\nstep:542/10000 train_loss:5.4312 train_time:89747ms step_avg:168.70ms\nstep:543/10000 train_loss:5.6623 train_time:89915ms step_avg:168.70ms\nstep:544/10000 train_loss:5.4650 train_time:90088ms step_avg:168.70ms\nstep:545/10000 train_loss:5.5871 train_time:90259ms step_avg:168.71ms\nstep:546/10000 train_loss:5.5684 train_time:90430ms step_avg:168.71ms\nstep:547/10000 train_loss:5.4392 train_time:90598ms step_avg:168.71ms\nstep:548/10000 train_loss:5.5947 train_time:90768ms step_avg:168.71ms\nstep:549/10000 train_loss:5.4936 train_time:90935ms step_avg:168.71ms\nstep:550/10000 train_loss:5.5728 train_time:91105ms step_avg:168.71ms\nstep:551/10000 train_loss:5.5630 train_time:91275ms step_avg:168.71ms\nstep:552/10000 train_loss:5.5427 train_time:91445ms step_avg:168.72ms\nstep:553/10000 train_loss:5.4987 train_time:91613ms step_avg:168.72ms\nstep:554/10000 train_loss:5.5724 train_time:91783ms step_avg:168.72ms\nstep:555/10000 train_loss:5.4945 train_time:91954ms step_avg:168.72ms\nstep:556/10000 train_loss:5.5585 train_time:92126ms step_avg:168.73ms\nstep:557/10000 train_loss:5.5753 train_time:92293ms step_avg:168.73ms\nstep:558/10000 train_loss:5.5551 train_time:92463ms step_avg:168.73ms\nstep:559/10000 train_loss:5.4948 train_time:92633ms step_avg:168.73ms\nstep:560/10000 train_loss:5.4445 train_time:92803ms step_avg:168.73ms\nstep:561/10000 train_loss:5.6815 train_time:92971ms step_avg:168.73ms\nstep:562/10000 train_loss:5.5002 train_time:93140ms step_avg:168.73ms\nstep:563/10000 train_loss:5.3782 train_time:93310ms step_avg:168.73ms\nstep:564/10000 train_loss:5.4529 train_time:93478ms step_avg:168.73ms\nstep:565/10000 train_loss:5.6250 train_time:93646ms step_avg:168.73ms\nstep:566/10000 train_loss:5.5901 train_time:93814ms step_avg:168.73ms\nstep:567/10000 train_loss:5.7417 train_time:93983ms step_avg:168.73ms\nstep:568/10000 train_loss:5.4535 train_time:94153ms step_avg:168.73ms\nstep:569/10000 train_loss:5.6220 train_time:94324ms step_avg:168.74ms\nstep:570/10000 train_loss:5.3687 train_time:94492ms step_avg:168.74ms\nstep:571/10000 train_loss:5.5041 train_time:94662ms step_avg:168.74ms\nstep:572/10000 train_loss:5.6707 train_time:94832ms step_avg:168.74ms\nstep:573/10000 train_loss:5.5966 train_time:95002ms step_avg:168.74ms\nstep:574/10000 train_loss:5.5297 train_time:95172ms step_avg:168.75ms\nstep:575/10000 train_loss:5.4333 train_time:95343ms step_avg:168.75ms\nstep:576/10000 train_loss:6.0124 train_time:95512ms step_avg:168.75ms\nstep:577/10000 train_loss:5.4609 train_time:95683ms step_avg:168.75ms\nstep:578/10000 train_loss:5.4908 train_time:95850ms step_avg:168.75ms\nstep:579/10000 train_loss:5.5374 train_time:96019ms step_avg:168.75ms\nstep:580/10000 train_loss:5.4768 train_time:96190ms step_avg:168.76ms\nstep:581/10000 train_loss:5.5181 train_time:96360ms step_avg:168.76ms\nstep:582/10000 train_loss:5.4387 train_time:96530ms step_avg:168.76ms\nstep:583/10000 train_loss:5.4974 train_time:96698ms step_avg:168.76ms\nstep:584/10000 train_loss:5.5373 train_time:96871ms step_avg:168.76ms\nstep:585/10000 train_loss:5.4015 train_time:97043ms step_avg:168.77ms\nstep:586/10000 train_loss:5.5107 train_time:97212ms step_avg:168.77ms\nstep:587/10000 train_loss:5.4044 train_time:97382ms step_avg:168.77ms\nstep:588/10000 train_loss:5.5049 train_time:97553ms step_avg:168.78ms\nstep:589/10000 train_loss:5.4243 train_time:97722ms step_avg:168.78ms\nstep:590/10000 train_loss:5.4743 train_time:97891ms step_avg:168.78ms\nstep:591/10000 train_loss:5.4420 train_time:98062ms step_avg:168.78ms\nstep:592/10000 train_loss:5.2500 train_time:98233ms step_avg:168.79ms\nstep:593/10000 train_loss:5.5040 train_time:98404ms step_avg:168.79ms\nstep:594/10000 train_loss:5.5988 train_time:98573ms step_avg:168.79ms\nstep:595/10000 train_loss:5.3571 train_time:98743ms step_avg:168.79ms\nstep:596/10000 train_loss:5.7034 train_time:98917ms step_avg:168.80ms\nstep:597/10000 train_loss:5.5748 train_time:99085ms step_avg:168.80ms\nstep:598/10000 train_loss:5.6126 train_time:99255ms step_avg:168.80ms\nstep:599/10000 train_loss:5.6596 train_time:99425ms step_avg:168.80ms\nstep:600/10000 train_loss:5.6240 train_time:99593ms step_avg:168.80ms\nstep:601/10000 train_loss:5.6410 train_time:99764ms step_avg:168.81ms\nstep:602/10000 train_loss:5.5023 train_time:99932ms step_avg:168.80ms\nstep:603/10000 train_loss:5.6140 train_time:100103ms step_avg:168.81ms\nstep:604/10000 train_loss:5.4376 train_time:100273ms step_avg:168.81ms\nstep:605/10000 train_loss:5.3847 train_time:100442ms step_avg:168.81ms\nstep:606/10000 train_loss:5.4780 train_time:100611ms step_avg:168.81ms\nstep:607/10000 train_loss:5.4421 train_time:100779ms step_avg:168.81ms\nstep:608/10000 train_loss:5.5558 train_time:100952ms step_avg:168.82ms\nstep:609/10000 train_loss:5.3931 train_time:101121ms step_avg:168.82ms\nstep:610/10000 train_loss:5.5074 train_time:101290ms step_avg:168.82ms\nstep:611/10000 train_loss:5.5052 train_time:101460ms step_avg:168.82ms\nstep:612/10000 train_loss:5.4347 train_time:101630ms step_avg:168.82ms\nstep:613/10000 train_loss:5.2267 train_time:101800ms step_avg:168.82ms\nstep:614/10000 train_loss:5.4639 train_time:101974ms step_avg:168.83ms\nstep:615/10000 train_loss:5.4417 train_time:102143ms step_avg:168.83ms\nstep:616/10000 train_loss:5.3626 train_time:102313ms step_avg:168.83ms\nstep:617/10000 train_loss:5.4925 train_time:102481ms step_avg:168.83ms\nstep:618/10000 train_loss:5.4724 train_time:102651ms step_avg:168.83ms\nstep:619/10000 train_loss:5.5443 train_time:102821ms step_avg:168.84ms\nstep:620/10000 train_loss:5.3885 train_time:102991ms step_avg:168.84ms\nstep:621/10000 train_loss:5.5303 train_time:103161ms step_avg:168.84ms\nstep:622/10000 train_loss:5.4463 train_time:103330ms step_avg:168.84ms\nstep:623/10000 train_loss:5.3885 train_time:103499ms step_avg:168.84ms\nstep:624/10000 train_loss:5.4970 train_time:103670ms step_avg:168.84ms\nstep:625/10000 train_loss:5.5354 train_time:103840ms step_avg:168.85ms\nstep:626/10000 train_loss:5.5840 train_time:104011ms step_avg:168.85ms\nstep:627/10000 train_loss:5.3736 train_time:104182ms step_avg:168.85ms\nstep:628/10000 train_loss:5.4704 train_time:104351ms step_avg:168.85ms\nstep:629/10000 train_loss:5.4665 train_time:104519ms step_avg:168.85ms\nstep:630/10000 train_loss:5.3916 train_time:104687ms step_avg:168.85ms\nstep:631/10000 train_loss:5.3479 train_time:104855ms step_avg:168.85ms\nstep:632/10000 train_loss:5.4839 train_time:105029ms step_avg:168.86ms\nstep:633/10000 train_loss:5.3465 train_time:105196ms step_avg:168.85ms\nstep:634/10000 train_loss:5.4267 train_time:105368ms step_avg:168.86ms\nstep:635/10000 train_loss:5.5531 train_time:105537ms step_avg:168.86ms\nstep:636/10000 train_loss:5.3602 train_time:105707ms step_avg:168.86ms\nstep:637/10000 train_loss:5.3330 train_time:105875ms step_avg:168.86ms\nstep:638/10000 train_loss:5.3164 train_time:106046ms step_avg:168.86ms\nstep:639/10000 train_loss:5.5807 train_time:106216ms step_avg:168.87ms\nstep:640/10000 train_loss:5.3176 train_time:106388ms step_avg:168.87ms\nstep:641/10000 train_loss:5.3729 train_time:106555ms step_avg:168.87ms\nstep:642/10000 train_loss:5.4047 train_time:106726ms step_avg:168.87ms\nstep:643/10000 train_loss:5.4255 train_time:106894ms step_avg:168.87ms\nstep:644/10000 train_loss:5.3851 train_time:107065ms step_avg:168.87ms\nstep:645/10000 train_loss:5.2832 train_time:107238ms step_avg:168.88ms\nstep:646/10000 train_loss:5.4644 train_time:107411ms step_avg:168.88ms\nstep:647/10000 train_loss:5.4592 train_time:107580ms step_avg:168.89ms\nstep:648/10000 train_loss:5.4223 train_time:107748ms step_avg:168.88ms\nstep:649/10000 train_loss:5.3668 train_time:107917ms step_avg:168.88ms\nstep:650/10000 train_loss:5.4722 train_time:108088ms step_avg:168.89ms\nstep:651/10000 train_loss:5.5436 train_time:108259ms step_avg:168.89ms\nstep:652/10000 train_loss:5.3767 train_time:108432ms step_avg:168.90ms\nstep:653/10000 train_loss:5.3026 train_time:108601ms step_avg:168.90ms\nstep:654/10000 train_loss:5.2816 train_time:108772ms step_avg:168.90ms\nstep:655/10000 train_loss:5.4831 train_time:108943ms step_avg:168.90ms\nstep:656/10000 train_loss:5.2773 train_time:109113ms step_avg:168.91ms\nstep:657/10000 train_loss:5.2683 train_time:109285ms step_avg:168.91ms\nstep:658/10000 train_loss:5.3938 train_time:109455ms step_avg:168.91ms\nstep:659/10000 train_loss:5.3918 train_time:109627ms step_avg:168.92ms\nstep:660/10000 train_loss:5.3700 train_time:109794ms step_avg:168.91ms\nstep:661/10000 train_loss:5.1974 train_time:109963ms step_avg:168.91ms\nstep:662/10000 train_loss:5.5697 train_time:110130ms step_avg:168.91ms\nstep:663/10000 train_loss:5.3408 train_time:110301ms step_avg:168.91ms\nstep:664/10000 train_loss:5.4617 train_time:110470ms step_avg:168.91ms\nstep:665/10000 train_loss:5.4520 train_time:110639ms step_avg:168.91ms\nstep:666/10000 train_loss:5.3378 train_time:110807ms step_avg:168.91ms\nstep:667/10000 train_loss:5.3258 train_time:110977ms step_avg:168.91ms\nstep:668/10000 train_loss:5.3304 train_time:111149ms step_avg:168.92ms\nstep:669/10000 train_loss:5.1373 train_time:111319ms step_avg:168.92ms\nstep:670/10000 train_loss:5.4062 train_time:111488ms step_avg:168.92ms\nstep:671/10000 train_loss:5.4728 train_time:111656ms step_avg:168.92ms\nstep:672/10000 train_loss:5.3408 train_time:111827ms step_avg:168.92ms\nstep:673/10000 train_loss:5.2764 train_time:111996ms step_avg:168.92ms\nstep:674/10000 train_loss:5.2946 train_time:112166ms step_avg:168.92ms\nstep:675/10000 train_loss:5.2558 train_time:112334ms step_avg:168.92ms\nstep:676/10000 train_loss:5.3722 train_time:112506ms step_avg:168.93ms\nstep:677/10000 train_loss:5.3400 train_time:112674ms step_avg:168.93ms\nstep:678/10000 train_loss:5.2846 train_time:112844ms step_avg:168.93ms\nstep:679/10000 train_loss:5.4515 train_time:113015ms step_avg:168.93ms\nstep:680/10000 train_loss:5.3377 train_time:113185ms step_avg:168.93ms\nstep:681/10000 train_loss:5.3306 train_time:113355ms step_avg:168.93ms\nstep:682/10000 train_loss:5.3468 train_time:113526ms step_avg:168.94ms\nstep:683/10000 train_loss:5.4722 train_time:113695ms step_avg:168.94ms\nstep:684/10000 train_loss:4.7200 train_time:113862ms step_avg:168.94ms\nstep:685/10000 train_loss:5.3116 train_time:114031ms step_avg:168.93ms\nstep:686/10000 train_loss:5.3185 train_time:114198ms step_avg:168.93ms\nstep:687/10000 train_loss:5.5094 train_time:114369ms step_avg:168.94ms\nstep:688/10000 train_loss:5.2820 train_time:114537ms step_avg:168.93ms\nstep:689/10000 train_loss:5.5789 train_time:114707ms step_avg:168.94ms\nstep:690/10000 train_loss:5.3896 train_time:114877ms step_avg:168.94ms\nstep:691/10000 train_loss:5.4325 train_time:115049ms step_avg:168.94ms\nstep:692/10000 train_loss:5.2399 train_time:115216ms step_avg:168.94ms\nstep:693/10000 train_loss:5.2328 train_time:115387ms step_avg:168.94ms\nstep:694/10000 train_loss:5.4962 train_time:115556ms step_avg:168.94ms\nstep:695/10000 train_loss:5.2469 train_time:115724ms step_avg:168.94ms\nstep:696/10000 train_loss:5.4769 train_time:115894ms step_avg:168.94ms\nstep:697/10000 train_loss:5.3863 train_time:116066ms step_avg:168.95ms\nstep:698/10000 train_loss:5.4042 train_time:116234ms step_avg:168.94ms\nstep:699/10000 train_loss:5.2283 train_time:116404ms step_avg:168.95ms\nstep:700/10000 train_loss:5.4408 train_time:116574ms step_avg:168.95ms\nstep:701/10000 train_loss:5.5293 train_time:116743ms step_avg:168.95ms\nstep:702/10000 train_loss:5.2404 train_time:116910ms step_avg:168.94ms\nstep:703/10000 train_loss:5.3751 train_time:117080ms step_avg:168.95ms\nstep:704/10000 train_loss:5.3335 train_time:117249ms step_avg:168.95ms\nstep:705/10000 train_loss:5.3170 train_time:117417ms step_avg:168.95ms\nstep:706/10000 train_loss:5.1896 train_time:117584ms step_avg:168.94ms\nstep:707/10000 train_loss:5.1883 train_time:117752ms step_avg:168.94ms\nstep:708/10000 train_loss:5.5061 train_time:117924ms step_avg:168.95ms\nstep:709/10000 train_loss:5.3385 train_time:118093ms step_avg:168.95ms\nstep:710/10000 train_loss:5.2191 train_time:118263ms step_avg:168.95ms\nstep:711/10000 train_loss:5.1312 train_time:118432ms step_avg:168.95ms\nstep:712/10000 train_loss:5.2629 train_time:118603ms step_avg:168.95ms\nstep:713/10000 train_loss:5.4893 train_time:118770ms step_avg:168.95ms\nstep:714/10000 train_loss:5.3076 train_time:118943ms step_avg:168.95ms\nstep:715/10000 train_loss:5.4189 train_time:119114ms step_avg:168.96ms\nstep:716/10000 train_loss:5.2660 train_time:119283ms step_avg:168.96ms\nstep:717/10000 train_loss:5.2568 train_time:119450ms step_avg:168.95ms\nstep:718/10000 train_loss:5.2819 train_time:119622ms step_avg:168.96ms\nstep:719/10000 train_loss:5.3849 train_time:119793ms step_avg:168.96ms\nstep:720/10000 train_loss:5.3410 train_time:119962ms step_avg:168.96ms\nstep:721/10000 train_loss:5.3395 train_time:120132ms step_avg:168.96ms\nstep:722/10000 train_loss:5.3748 train_time:120301ms step_avg:168.96ms\nstep:723/10000 train_loss:5.3237 train_time:120472ms step_avg:168.97ms\nstep:724/10000 train_loss:5.2893 train_time:120641ms step_avg:168.97ms\nstep:725/10000 train_loss:5.1689 train_time:120809ms step_avg:168.96ms\nstep:726/10000 train_loss:5.2693 train_time:120978ms step_avg:168.96ms\nstep:727/10000 train_loss:5.2756 train_time:121150ms step_avg:168.97ms\nstep:728/10000 train_loss:5.1635 train_time:121318ms step_avg:168.97ms\nstep:729/10000 train_loss:5.3962 train_time:121485ms step_avg:168.96ms\nstep:730/10000 train_loss:5.2182 train_time:121655ms step_avg:168.97ms\nstep:731/10000 train_loss:5.1559 train_time:121825ms step_avg:168.97ms\nstep:732/10000 train_loss:5.3421 train_time:121995ms step_avg:168.97ms\nstep:733/10000 train_loss:5.2987 train_time:122165ms step_avg:168.97ms\nstep:734/10000 train_loss:5.1950 train_time:122334ms step_avg:168.97ms\nstep:735/10000 train_loss:5.2276 train_time:122505ms step_avg:168.97ms\nstep:736/10000 train_loss:5.4648 train_time:122675ms step_avg:168.97ms\nstep:737/10000 train_loss:5.2058 train_time:122842ms step_avg:168.97ms\nstep:738/10000 train_loss:5.3538 train_time:123012ms step_avg:168.97ms\nstep:739/10000 train_loss:5.1029 train_time:123181ms step_avg:168.97ms\nstep:740/10000 train_loss:5.2187 train_time:123350ms step_avg:168.97ms\nstep:741/10000 train_loss:5.2446 train_time:123519ms step_avg:168.97ms\nstep:742/10000 train_loss:5.2013 train_time:123687ms step_avg:168.97ms\nstep:743/10000 train_loss:5.2209 train_time:123856ms step_avg:168.97ms\nstep:744/10000 train_loss:5.3029 train_time:124028ms step_avg:168.98ms\nstep:745/10000 train_loss:5.1760 train_time:124200ms step_avg:168.98ms\nstep:746/10000 train_loss:5.5111 train_time:124371ms step_avg:168.98ms\nstep:747/10000 train_loss:5.2712 train_time:124541ms step_avg:168.98ms\nstep:748/10000 train_loss:5.2867 train_time:124715ms step_avg:168.99ms\nstep:749/10000 train_loss:5.4496 train_time:124887ms step_avg:168.99ms\nstep:750/10000 train_loss:5.2822 train_time:125056ms step_avg:168.99ms\nstep:751/10000 train_loss:5.1462 train_time:125226ms step_avg:169.00ms\nstep:752/10000 train_loss:5.3401 train_time:125395ms step_avg:169.00ms\nstep:753/10000 train_loss:5.2929 train_time:125565ms step_avg:169.00ms\nstep:754/10000 train_loss:5.1975 train_time:125733ms step_avg:169.00ms\nstep:755/10000 train_loss:5.3272 train_time:125903ms step_avg:169.00ms\nstep:756/10000 train_loss:4.8729 train_time:126075ms step_avg:169.00ms\nstep:757/10000 train_loss:4.8017 train_time:126247ms step_avg:169.00ms\nstep:758/10000 train_loss:5.3832 train_time:126417ms step_avg:169.01ms\nstep:759/10000 train_loss:5.2573 train_time:126587ms step_avg:169.01ms\nstep:760/10000 train_loss:5.1639 train_time:126758ms step_avg:169.01ms\nstep:761/10000 train_loss:5.0971 train_time:126928ms step_avg:169.01ms\nstep:762/10000 train_loss:5.3545 train_time:127296ms step_avg:169.28ms\nstep:763/10000 train_loss:5.1918 train_time:127460ms step_avg:169.27ms\nstep:764/10000 train_loss:5.1845 train_time:127627ms step_avg:169.27ms\nstep:765/10000 train_loss:5.1093 train_time:127795ms step_avg:169.26ms\nstep:766/10000 train_loss:5.2758 train_time:127963ms step_avg:169.26ms\nstep:767/10000 train_loss:5.2358 train_time:128131ms step_avg:169.26ms\nstep:768/10000 train_loss:5.1448 train_time:128309ms step_avg:169.27ms\nstep:769/10000 train_loss:5.3103 train_time:128480ms step_avg:169.28ms\nstep:770/10000 train_loss:5.2320 train_time:128651ms step_avg:169.28ms\nstep:771/10000 train_loss:5.3080 train_time:128823ms step_avg:169.28ms\nstep:772/10000 train_loss:5.2955 train_time:128991ms step_avg:169.28ms\nstep:773/10000 train_loss:5.1009 train_time:129165ms step_avg:169.29ms\nstep:774/10000 train_loss:5.2476 train_time:129337ms step_avg:169.29ms\nstep:775/10000 train_loss:5.2766 train_time:129507ms step_avg:169.29ms\nstep:776/10000 train_loss:5.2521 train_time:129675ms step_avg:169.29ms\nstep:777/10000 train_loss:5.2980 train_time:129845ms step_avg:169.29ms\nstep:778/10000 train_loss:5.2316 train_time:130013ms step_avg:169.29ms\nstep:779/10000 train_loss:5.3473 train_time:130183ms step_avg:169.29ms\nstep:780/10000 train_loss:5.4507 train_time:130354ms step_avg:169.29ms\nstep:781/10000 train_loss:5.2188 train_time:130525ms step_avg:169.29ms\nstep:782/10000 train_loss:5.2182 train_time:130692ms step_avg:169.29ms\nstep:783/10000 train_loss:5.1863 train_time:130864ms step_avg:169.29ms\nstep:784/10000 train_loss:5.3123 train_time:131032ms step_avg:169.29ms\nstep:785/10000 train_loss:5.1482 train_time:131201ms step_avg:169.29ms\nstep:786/10000 train_loss:5.2944 train_time:131374ms step_avg:169.30ms\nstep:787/10000 train_loss:5.2814 train_time:131546ms step_avg:169.30ms\nstep:788/10000 train_loss:5.1614 train_time:131717ms step_avg:169.30ms\nstep:789/10000 train_loss:5.1300 train_time:131887ms step_avg:169.30ms\nstep:790/10000 train_loss:5.1122 train_time:132057ms step_avg:169.30ms\nstep:791/10000 train_loss:5.3073 train_time:132227ms step_avg:169.31ms\nstep:792/10000 train_loss:5.1741 train_time:132394ms step_avg:169.30ms\nstep:793/10000 train_loss:5.1452 train_time:132566ms step_avg:169.31ms\nstep:794/10000 train_loss:5.3208 train_time:132738ms step_avg:169.31ms\nstep:795/10000 train_loss:5.1667 train_time:132907ms step_avg:169.31ms\nstep:796/10000 train_loss:5.0944 train_time:133075ms step_avg:169.31ms\nstep:797/10000 train_loss:5.1333 train_time:133246ms step_avg:169.31ms\nstep:798/10000 train_loss:5.2043 train_time:133414ms step_avg:169.31ms\nstep:799/10000 train_loss:5.3287 train_time:133586ms step_avg:169.31ms\nstep:800/10000 train_loss:5.1826 train_time:133755ms step_avg:169.31ms\nstep:801/10000 train_loss:5.1537 train_time:133927ms step_avg:169.31ms\nstep:802/10000 train_loss:5.9462 train_time:134096ms step_avg:169.31ms\nstep:803/10000 train_loss:5.3191 train_time:134266ms step_avg:169.31ms\nstep:804/10000 train_loss:5.1382 train_time:134433ms step_avg:169.31ms\nstep:805/10000 train_loss:5.2985 train_time:134604ms step_avg:169.31ms\nstep:806/10000 train_loss:5.4176 train_time:134773ms step_avg:169.31ms\nstep:807/10000 train_loss:5.1533 train_time:134942ms step_avg:169.31ms\nstep:808/10000 train_loss:5.2968 train_time:135110ms step_avg:169.31ms\nstep:809/10000 train_loss:5.3321 train_time:135279ms step_avg:169.31ms\nstep:810/10000 train_loss:5.1975 train_time:135452ms step_avg:169.31ms\nstep:811/10000 train_loss:5.1278 train_time:135624ms step_avg:169.32ms\nstep:812/10000 train_loss:5.2253 train_time:135797ms step_avg:169.32ms\nstep:813/10000 train_loss:5.3501 train_time:135967ms step_avg:169.32ms\nstep:814/10000 train_loss:5.1349 train_time:136137ms step_avg:169.32ms\nstep:815/10000 train_loss:5.2436 train_time:136305ms step_avg:169.32ms\nstep:816/10000 train_loss:5.1712 train_time:136474ms step_avg:169.32ms\nstep:817/10000 train_loss:5.1379 train_time:136645ms step_avg:169.32ms\nstep:818/10000 train_loss:5.1622 train_time:136814ms step_avg:169.32ms\nstep:819/10000 train_loss:5.2080 train_time:136986ms step_avg:169.33ms\nstep:820/10000 train_loss:5.3409 train_time:137157ms step_avg:169.33ms\nstep:821/10000 train_loss:5.1406 train_time:137328ms step_avg:169.33ms\nstep:822/10000 train_loss:5.1278 train_time:137496ms step_avg:169.33ms\nstep:823/10000 train_loss:5.1719 train_time:137669ms step_avg:169.33ms\nstep:824/10000 train_loss:5.0204 train_time:137838ms step_avg:169.33ms\nstep:825/10000 train_loss:5.1259 train_time:138010ms step_avg:169.34ms\nstep:826/10000 train_loss:5.2402 train_time:138177ms step_avg:169.33ms\nstep:827/10000 train_loss:5.2486 train_time:138346ms step_avg:169.33ms\nstep:828/10000 train_loss:5.1732 train_time:138516ms step_avg:169.34ms\nstep:829/10000 train_loss:5.1201 train_time:138686ms step_avg:169.34ms\nstep:830/10000 train_loss:5.0645 train_time:138855ms step_avg:169.34ms\nstep:831/10000 train_loss:5.1290 train_time:139026ms step_avg:169.34ms\nstep:832/10000 train_loss:5.1288 train_time:139198ms step_avg:169.34ms\nstep:833/10000 train_loss:5.2336 train_time:139367ms step_avg:169.34ms\nstep:834/10000 train_loss:5.1690 train_time:139534ms step_avg:169.34ms\nstep:835/10000 train_loss:4.9498 train_time:139704ms step_avg:169.34ms\nstep:836/10000 train_loss:5.2832 train_time:139872ms step_avg:169.34ms\nstep:837/10000 train_loss:5.1102 train_time:140043ms step_avg:169.34ms\nstep:838/10000 train_loss:5.0765 train_time:140216ms step_avg:169.34ms\nstep:839/10000 train_loss:5.1600 train_time:140388ms step_avg:169.35ms\nstep:840/10000 train_loss:5.1601 train_time:140558ms step_avg:169.35ms\nstep:841/10000 train_loss:5.2759 train_time:140728ms step_avg:169.35ms\nstep:842/10000 train_loss:5.1412 train_time:140898ms step_avg:169.35ms\nstep:843/10000 train_loss:5.2679 train_time:141068ms step_avg:169.35ms\nstep:844/10000 train_loss:5.0980 train_time:141237ms step_avg:169.35ms\nstep:845/10000 train_loss:5.1717 train_time:141408ms step_avg:169.35ms\nstep:846/10000 train_loss:5.1414 train_time:141576ms step_avg:169.35ms\nstep:847/10000 train_loss:5.1801 train_time:141745ms step_avg:169.35ms\nstep:848/10000 train_loss:5.3026 train_time:141915ms step_avg:169.35ms\nstep:849/10000 train_loss:5.1328 train_time:142087ms step_avg:169.35ms\nstep:850/10000 train_loss:5.0952 train_time:142256ms step_avg:169.35ms\nstep:851/10000 train_loss:5.3897 train_time:142425ms step_avg:169.35ms\nstep:852/10000 train_loss:5.0750 train_time:142593ms step_avg:169.35ms\nstep:853/10000 train_loss:5.1819 train_time:142763ms step_avg:169.35ms\nstep:854/10000 train_loss:5.1407 train_time:142933ms step_avg:169.35ms\nstep:855/10000 train_loss:5.2567 train_time:143102ms step_avg:169.35ms\nstep:856/10000 train_loss:5.1700 train_time:143272ms step_avg:169.35ms\nstep:857/10000 train_loss:5.0233 train_time:143440ms step_avg:169.35ms\nstep:858/10000 train_loss:5.1640 train_time:143607ms step_avg:169.35ms\nstep:859/10000 train_loss:5.3034 train_time:143777ms step_avg:169.35ms\nstep:860/10000 train_loss:5.0323 train_time:143948ms step_avg:169.35ms\nstep:861/10000 train_loss:5.3582 train_time:144117ms step_avg:169.35ms\nstep:862/10000 train_loss:5.0791 train_time:144287ms step_avg:169.35ms\nstep:863/10000 train_loss:5.1538 train_time:144456ms step_avg:169.35ms\nstep:864/10000 train_loss:5.1553 train_time:144630ms step_avg:169.36ms\nstep:865/10000 train_loss:5.0315 train_time:144799ms step_avg:169.36ms\nstep:866/10000 train_loss:5.1589 train_time:144971ms step_avg:169.36ms\nstep:867/10000 train_loss:5.0066 train_time:145141ms step_avg:169.36ms\nstep:868/10000 train_loss:5.1241 train_time:145309ms step_avg:169.36ms\nstep:869/10000 train_loss:5.0579 train_time:145475ms step_avg:169.35ms\nstep:870/10000 train_loss:5.0979 train_time:145646ms step_avg:169.36ms\nstep:871/10000 train_loss:5.1285 train_time:145815ms step_avg:169.36ms\nstep:872/10000 train_loss:5.0634 train_time:145986ms step_avg:169.36ms\nstep:873/10000 train_loss:5.0301 train_time:146157ms step_avg:169.36ms\nstep:874/10000 train_loss:4.9303 train_time:146328ms step_avg:169.36ms\nstep:875/10000 train_loss:5.1528 train_time:146496ms step_avg:169.36ms\nstep:876/10000 train_loss:5.0891 train_time:146666ms step_avg:169.36ms\nstep:877/10000 train_loss:5.1790 train_time:146836ms step_avg:169.36ms\nstep:878/10000 train_loss:5.0855 train_time:147004ms step_avg:169.36ms\nstep:879/10000 train_loss:4.9605 train_time:147172ms step_avg:169.36ms\nstep:880/10000 train_loss:5.1034 train_time:147342ms step_avg:169.36ms\nstep:881/10000 train_loss:5.0420 train_time:147513ms step_avg:169.36ms\nstep:882/10000 train_loss:5.1601 train_time:147684ms step_avg:169.36ms\nstep:883/10000 train_loss:5.0357 train_time:147852ms step_avg:169.36ms\nstep:884/10000 train_loss:5.1591 train_time:148022ms step_avg:169.36ms\nstep:885/10000 train_loss:5.0301 train_time:148191ms step_avg:169.36ms\nstep:886/10000 train_loss:5.1495 train_time:148361ms step_avg:169.36ms\nstep:887/10000 train_loss:5.0404 train_time:148531ms step_avg:169.36ms\nstep:888/10000 train_loss:5.1569 train_time:148700ms step_avg:169.36ms\nstep:889/10000 train_loss:5.1198 train_time:148868ms step_avg:169.36ms\nstep:890/10000 train_loss:5.1014 train_time:149038ms step_avg:169.36ms\nstep:891/10000 train_loss:5.0553 train_time:149209ms step_avg:169.36ms\nstep:892/10000 train_loss:5.1374 train_time:149378ms step_avg:169.36ms\nstep:893/10000 train_loss:5.0978 train_time:149547ms step_avg:169.36ms\nstep:894/10000 train_loss:5.0758 train_time:149715ms step_avg:169.36ms\nstep:895/10000 train_loss:5.2243 train_time:149886ms step_avg:169.36ms\nstep:896/10000 train_loss:5.2165 train_time:150056ms step_avg:169.36ms\nstep:897/10000 train_loss:4.9904 train_time:150228ms step_avg:169.37ms\nstep:898/10000 train_loss:5.1130 train_time:150396ms step_avg:169.36ms\nstep:899/10000 train_loss:5.2085 train_time:150565ms step_avg:169.36ms\nstep:900/10000 train_loss:4.9271 train_time:150735ms step_avg:169.36ms\nstep:901/10000 train_loss:5.1135 train_time:150904ms step_avg:169.37ms\nstep:902/10000 train_loss:5.0069 train_time:151073ms step_avg:169.36ms\nstep:903/10000 train_loss:5.1496 train_time:151244ms step_avg:169.37ms\nstep:904/10000 train_loss:5.0014 train_time:151415ms step_avg:169.37ms\nstep:905/10000 train_loss:5.1576 train_time:151585ms step_avg:169.37ms\nstep:906/10000 train_loss:5.0033 train_time:151756ms step_avg:169.37ms\nstep:907/10000 train_loss:5.0505 train_time:151927ms step_avg:169.37ms\nstep:908/10000 train_loss:4.9720 train_time:152096ms step_avg:169.37ms\nstep:909/10000 train_loss:5.1321 train_time:152268ms step_avg:169.37ms\nstep:910/10000 train_loss:5.0156 train_time:152437ms step_avg:169.37ms\nstep:911/10000 train_loss:5.1888 train_time:152606ms step_avg:169.37ms\nstep:912/10000 train_loss:5.1299 train_time:152776ms step_avg:169.37ms\nstep:913/10000 train_loss:5.1334 train_time:152948ms step_avg:169.38ms\nstep:914/10000 train_loss:5.1671 train_time:153117ms step_avg:169.38ms\nstep:915/10000 train_loss:5.0908 train_time:153288ms step_avg:169.38ms\nstep:916/10000 train_loss:5.0120 train_time:153458ms step_avg:169.38ms\nstep:917/10000 train_loss:5.0488 train_time:153630ms step_avg:169.38ms\nstep:918/10000 train_loss:5.0861 train_time:153798ms step_avg:169.38ms\nstep:919/10000 train_loss:5.2366 train_time:153971ms step_avg:169.39ms\nstep:920/10000 train_loss:5.1376 train_time:154140ms step_avg:169.38ms\nstep:921/10000 train_loss:5.0736 train_time:154308ms step_avg:169.38ms\nstep:922/10000 train_loss:5.0265 train_time:154477ms step_avg:169.38ms\nstep:923/10000 train_loss:4.9150 train_time:154651ms step_avg:169.39ms\nstep:924/10000 train_loss:5.0268 train_time:154819ms step_avg:169.39ms\nstep:925/10000 train_loss:4.9137 train_time:154990ms step_avg:169.39ms\nstep:926/10000 train_loss:5.1610 train_time:155159ms step_avg:169.39ms\nstep:927/10000 train_loss:5.0086 train_time:155329ms step_avg:169.39ms\nstep:928/10000 train_loss:4.9264 train_time:155500ms step_avg:169.39ms\nstep:929/10000 train_loss:5.1612 train_time:155670ms step_avg:169.39ms\nstep:930/10000 train_loss:4.9880 train_time:155840ms step_avg:169.39ms\nstep:931/10000 train_loss:5.0247 train_time:156010ms step_avg:169.39ms\nstep:932/10000 train_loss:5.1720 train_time:156180ms step_avg:169.39ms\nstep:933/10000 train_loss:5.2064 train_time:156349ms step_avg:169.39ms\nstep:934/10000 train_loss:5.0313 train_time:156517ms step_avg:169.39ms\nstep:935/10000 train_loss:5.0228 train_time:156688ms step_avg:169.39ms\nstep:936/10000 train_loss:5.0056 train_time:156857ms step_avg:169.39ms\nstep:937/10000 train_loss:5.1100 train_time:157026ms step_avg:169.39ms\nstep:938/10000 train_loss:5.2014 train_time:157198ms step_avg:169.39ms\nstep:939/10000 train_loss:5.2293 train_time:157370ms step_avg:169.40ms\nstep:940/10000 train_loss:4.8799 train_time:157539ms step_avg:169.40ms\nstep:941/10000 train_loss:5.2242 train_time:157709ms step_avg:169.40ms\nstep:942/10000 train_loss:5.0744 train_time:157880ms step_avg:169.40ms\nstep:943/10000 train_loss:4.9517 train_time:158052ms step_avg:169.40ms\nstep:944/10000 train_loss:5.2105 train_time:158221ms step_avg:169.40ms\nstep:945/10000 train_loss:4.9004 train_time:158390ms step_avg:169.40ms\nstep:946/10000 train_loss:5.2048 train_time:158559ms step_avg:169.40ms\nstep:947/10000 train_loss:5.1691 train_time:158728ms step_avg:169.40ms\nstep:948/10000 train_loss:5.0457 train_time:158896ms step_avg:169.40ms\nstep:949/10000 train_loss:4.8743 train_time:159069ms step_avg:169.40ms\nstep:950/10000 train_loss:5.1139 train_time:159238ms step_avg:169.40ms\nstep:951/10000 train_loss:5.1021 train_time:159408ms step_avg:169.40ms\nstep:952/10000 train_loss:4.9946 train_time:159576ms step_avg:169.40ms\nstep:953/10000 train_loss:5.2050 train_time:159748ms step_avg:169.40ms\nstep:954/10000 train_loss:5.0417 train_time:159917ms step_avg:169.40ms\nstep:955/10000 train_loss:5.0790 train_time:160096ms step_avg:169.41ms\nstep:956/10000 train_loss:5.1173 train_time:160259ms step_avg:169.41ms\nstep:957/10000 train_loss:4.9361 train_time:160433ms step_avg:169.41ms\nstep:958/10000 train_loss:5.0681 train_time:160604ms step_avg:169.41ms\nstep:959/10000 train_loss:4.9374 train_time:160773ms step_avg:169.41ms\nstep:960/10000 train_loss:5.1219 train_time:160942ms step_avg:169.41ms\nstep:961/10000 train_loss:4.9244 train_time:161114ms step_avg:169.42ms\nstep:962/10000 train_loss:5.3488 train_time:161286ms step_avg:169.42ms\nstep:963/10000 train_loss:5.0568 train_time:161456ms step_avg:169.42ms\nstep:964/10000 train_loss:5.0036 train_time:161628ms step_avg:169.42ms\nstep:965/10000 train_loss:5.1369 train_time:161796ms step_avg:169.42ms\nstep:966/10000 train_loss:5.0326 train_time:161969ms step_avg:169.42ms\nstep:967/10000 train_loss:5.1817 train_time:162136ms step_avg:169.42ms\nstep:968/10000 train_loss:5.1960 train_time:162308ms step_avg:169.42ms\nstep:969/10000 train_loss:5.1407 train_time:162478ms step_avg:169.42ms\nstep:970/10000 train_loss:5.3664 train_time:162651ms step_avg:169.43ms\nstep:971/10000 train_loss:5.1285 train_time:162820ms step_avg:169.43ms\nstep:972/10000 train_loss:5.0005 train_time:162990ms step_avg:169.43ms\nstep:973/10000 train_loss:4.9387 train_time:163160ms step_avg:169.43ms\nstep:974/10000 train_loss:4.9593 train_time:163329ms step_avg:169.43ms\nstep:975/10000 train_loss:4.9165 train_time:163498ms step_avg:169.43ms\nstep:976/10000 train_loss:4.9854 train_time:163667ms step_avg:169.43ms\nstep:977/10000 train_loss:5.0907 train_time:163838ms step_avg:169.43ms\nstep:978/10000 train_loss:5.0210 train_time:164008ms step_avg:169.43ms\nstep:979/10000 train_loss:5.1557 train_time:164177ms step_avg:169.43ms\nstep:980/10000 train_loss:5.0663 train_time:164348ms step_avg:169.43ms\nstep:981/10000 train_loss:5.0350 train_time:164517ms step_avg:169.43ms\nstep:982/10000 train_loss:5.0451 train_time:164687ms step_avg:169.43ms\nstep:983/10000 train_loss:5.0241 train_time:164857ms step_avg:169.43ms\nstep:984/10000 train_loss:5.0222 train_time:165027ms step_avg:169.43ms\nstep:985/10000 train_loss:4.9482 train_time:165195ms step_avg:169.43ms\nstep:986/10000 train_loss:5.0063 train_time:165367ms step_avg:169.43ms\nstep:987/10000 train_loss:5.1553 train_time:165535ms step_avg:169.43ms\nstep:988/10000 train_loss:4.9966 train_time:165706ms step_avg:169.43ms\nstep:989/10000 train_loss:4.8284 train_time:165871ms step_avg:169.43ms\nstep:990/10000 train_loss:5.0441 train_time:166043ms step_avg:169.43ms\nstep:991/10000 train_loss:5.0005 train_time:166215ms step_avg:169.43ms\nstep:992/10000 train_loss:5.2010 train_time:166385ms step_avg:169.44ms\nstep:993/10000 train_loss:4.9589 train_time:166553ms step_avg:169.43ms\nstep:994/10000 train_loss:4.8862 train_time:166724ms step_avg:169.43ms\nstep:995/10000 train_loss:5.1256 train_time:166892ms step_avg:169.43ms\nstep:996/10000 train_loss:4.8980 train_time:167062ms step_avg:169.43ms\nstep:997/10000 train_loss:5.0622 train_time:167232ms step_avg:169.43ms\nstep:998/10000 train_loss:5.0764 train_time:167403ms step_avg:169.44ms\nstep:999/10000 train_loss:4.9887 train_time:167570ms step_avg:169.43ms\nstep:1000/10000 train_loss:4.9189 train_time:167739ms step_avg:169.43ms\nstep:1001/10000 train_loss:4.9847 train_time:167910ms step_avg:169.43ms\nstep:1002/10000 train_loss:4.9960 train_time:168079ms step_avg:169.43ms\nstep:1003/10000 train_loss:4.7720 train_time:168250ms step_avg:169.44ms\nstep:1004/10000 train_loss:5.0513 train_time:168419ms step_avg:169.44ms\nstep:1005/10000 train_loss:5.0813 train_time:168589ms step_avg:169.44ms\nstep:1006/10000 train_loss:4.8513 train_time:168757ms step_avg:169.43ms\nstep:1007/10000 train_loss:5.2369 train_time:168930ms step_avg:169.44ms\nstep:1008/10000 train_loss:5.0413 train_time:169100ms step_avg:169.44ms\nstep:1009/10000 train_loss:4.9549 train_time:169271ms step_avg:169.44ms\nstep:1010/10000 train_loss:4.8487 train_time:169439ms step_avg:169.44ms\nstep:1011/10000 train_loss:5.0158 train_time:169608ms step_avg:169.44ms\nstep:1012/10000 train_loss:4.9436 train_time:169776ms step_avg:169.44ms\nstep:1013/10000 train_loss:4.8885 train_time:169944ms step_avg:169.44ms\nstep:1014/10000 train_loss:4.9518 train_time:170114ms step_avg:169.44ms\nstep:1015/10000 train_loss:4.9389 train_time:170287ms step_avg:169.44ms\nstep:1016/10000 train_loss:4.9627 train_time:170458ms step_avg:169.44ms\nstep:1017/10000 train_loss:4.9118 train_time:170628ms step_avg:169.44ms\nstep:1018/10000 train_loss:4.9496 train_time:170798ms step_avg:169.44ms\nstep:1019/10000 train_loss:4.8360 train_time:170969ms step_avg:169.44ms\nstep:1020/10000 train_loss:5.0626 train_time:171138ms step_avg:169.44ms\nstep:1021/10000 train_loss:5.1128 train_time:171307ms step_avg:169.44ms\nstep:1022/10000 train_loss:4.8637 train_time:171479ms step_avg:169.45ms\nstep:1023/10000 train_loss:5.0430 train_time:171651ms step_avg:169.45ms\nstep:1024/10000 train_loss:5.0817 train_time:171820ms step_avg:169.45ms\nstep:1025/10000 train_loss:4.9700 train_time:171989ms step_avg:169.45ms\nstep:1026/10000 train_loss:4.9575 train_time:172165ms step_avg:169.45ms\nstep:1027/10000 train_loss:4.8538 train_time:172333ms step_avg:169.45ms\nstep:1028/10000 train_loss:5.0053 train_time:172502ms step_avg:169.45ms\nstep:1029/10000 train_loss:4.9967 train_time:172673ms step_avg:169.45ms\nstep:1030/10000 train_loss:5.1087 train_time:172846ms step_avg:169.46ms\nstep:1031/10000 train_loss:4.9400 train_time:173014ms step_avg:169.46ms\nstep:1032/10000 train_loss:4.9192 train_time:173185ms step_avg:169.46ms\nstep:1033/10000 train_loss:5.2236 train_time:173354ms step_avg:169.46ms\nstep:1034/10000 train_loss:4.8530 train_time:173523ms step_avg:169.46ms\nstep:1035/10000 train_loss:4.8984 train_time:173688ms step_avg:169.45ms\nstep:1036/10000 train_loss:4.9369 train_time:173861ms step_avg:169.45ms\nstep:1037/10000 train_loss:4.8802 train_time:174035ms step_avg:169.46ms\nstep:1038/10000 train_loss:4.9813 train_time:174205ms step_avg:169.46ms\nstep:1039/10000 train_loss:4.9015 train_time:174377ms step_avg:169.46ms\nstep:1040/10000 train_loss:4.9921 train_time:174546ms step_avg:169.46ms\nstep:1041/10000 train_loss:5.0397 train_time:174714ms step_avg:169.46ms\nstep:1042/10000 train_loss:4.8945 train_time:174882ms step_avg:169.46ms\nstep:1043/10000 train_loss:4.9788 train_time:175055ms step_avg:169.46ms\nstep:1044/10000 train_loss:5.0184 train_time:175225ms step_avg:169.46ms\nstep:1045/10000 train_loss:4.7827 train_time:175392ms step_avg:169.46ms\nstep:1046/10000 train_loss:4.9059 train_time:175560ms step_avg:169.46ms\nstep:1047/10000 train_loss:4.7668 train_time:175729ms step_avg:169.46ms\nstep:1048/10000 train_loss:4.9345 train_time:175897ms step_avg:169.46ms\nstep:1049/10000 train_loss:5.0715 train_time:176067ms step_avg:169.46ms\nstep:1050/10000 train_loss:5.0106 train_time:176239ms step_avg:169.46ms\nstep:1051/10000 train_loss:5.0115 train_time:176410ms step_avg:169.46ms\nstep:1052/10000 train_loss:4.8998 train_time:176579ms step_avg:169.46ms\nstep:1053/10000 train_loss:4.9746 train_time:176750ms step_avg:169.46ms\nstep:1054/10000 train_loss:5.0191 train_time:176921ms step_avg:169.46ms\nstep:1055/10000 train_loss:4.9320 train_time:177092ms step_avg:169.47ms\nstep:1056/10000 train_loss:4.9189 train_time:177263ms step_avg:169.47ms\nstep:1057/10000 train_loss:5.0128 train_time:177433ms step_avg:169.47ms\nstep:1058/10000 train_loss:5.3824 train_time:177601ms step_avg:169.47ms\nstep:1059/10000 train_loss:4.8808 train_time:177770ms step_avg:169.47ms\nstep:1060/10000 train_loss:4.7914 train_time:177938ms step_avg:169.46ms\nstep:1061/10000 train_loss:4.7072 train_time:178108ms step_avg:169.47ms\nstep:1062/10000 train_loss:4.9742 train_time:178279ms step_avg:169.47ms\nstep:1063/10000 train_loss:5.0582 train_time:178452ms step_avg:169.47ms\nstep:1064/10000 train_loss:4.8372 train_time:178622ms step_avg:169.47ms\nstep:1065/10000 train_loss:4.8569 train_time:178791ms step_avg:169.47ms\nstep:1066/10000 train_loss:5.0787 train_time:178962ms step_avg:169.47ms\nstep:1067/10000 train_loss:5.0410 train_time:179131ms step_avg:169.47ms\nstep:1068/10000 train_loss:4.8921 train_time:179303ms step_avg:169.47ms\nstep:1069/10000 train_loss:4.7792 train_time:179474ms step_avg:169.47ms\nstep:1070/10000 train_loss:4.8705 train_time:179646ms step_avg:169.48ms\nstep:1071/10000 train_loss:4.9908 train_time:179813ms step_avg:169.47ms\nstep:1072/10000 train_loss:4.8100 train_time:179984ms step_avg:169.48ms\nstep:1073/10000 train_loss:4.9943 train_time:180153ms step_avg:169.48ms\nstep:1074/10000 train_loss:4.8320 train_time:180325ms step_avg:169.48ms\nstep:1075/10000 train_loss:4.7911 train_time:180497ms step_avg:169.48ms\nstep:1076/10000 train_loss:4.8380 train_time:180669ms step_avg:169.48ms\nstep:1077/10000 train_loss:4.9064 train_time:180838ms step_avg:169.48ms\nstep:1078/10000 train_loss:4.9790 train_time:181005ms step_avg:169.48ms\nstep:1079/10000 train_loss:4.8491 train_time:181175ms step_avg:169.48ms\nstep:1080/10000 train_loss:5.0164 train_time:181345ms step_avg:169.48ms\nstep:1081/10000 train_loss:4.7564 train_time:181513ms step_avg:169.48ms\nstep:1082/10000 train_loss:5.0284 train_time:181683ms step_avg:169.48ms\nstep:1083/10000 train_loss:4.9282 train_time:181853ms step_avg:169.48ms\nstep:1084/10000 train_loss:4.9617 train_time:182021ms step_avg:169.48ms\nstep:1085/10000 train_loss:5.0038 train_time:182189ms step_avg:169.48ms\nstep:1086/10000 train_loss:4.9626 train_time:182358ms step_avg:169.48ms\nstep:1087/10000 train_loss:4.9472 train_time:182530ms step_avg:169.48ms\nstep:1088/10000 train_loss:4.7985 train_time:182700ms step_avg:169.48ms\nstep:1089/10000 train_loss:4.8590 train_time:182872ms step_avg:169.48ms\nstep:1090/10000 train_loss:4.8246 train_time:183042ms step_avg:169.48ms\nstep:1091/10000 train_loss:4.9190 train_time:183213ms step_avg:169.48ms\nstep:1092/10000 train_loss:4.8379 train_time:183385ms step_avg:169.49ms\nstep:1093/10000 train_loss:4.9130 train_time:183553ms step_avg:169.49ms\nstep:1094/10000 train_loss:5.0685 train_time:183721ms step_avg:169.48ms\nstep:1095/10000 train_loss:4.8561 train_time:183890ms step_avg:169.48ms\nstep:1096/10000 train_loss:4.9858 train_time:184059ms step_avg:169.48ms\nstep:1097/10000 train_loss:4.9988 train_time:184233ms step_avg:169.49ms\nstep:1098/10000 train_loss:4.8195 train_time:184403ms step_avg:169.49ms\nstep:1099/10000 train_loss:4.7557 train_time:184573ms step_avg:169.49ms\nstep:1100/10000 train_loss:4.9968 train_time:184745ms step_avg:169.49ms\nstep:1101/10000 train_loss:4.8236 train_time:184914ms step_avg:169.49ms\nstep:1102/10000 train_loss:4.9705 train_time:185084ms step_avg:169.49ms\nstep:1103/10000 train_loss:4.9439 train_time:185253ms step_avg:169.49ms\nstep:1104/10000 train_loss:5.1841 train_time:185424ms step_avg:169.49ms\nstep:1105/10000 train_loss:4.8877 train_time:185592ms step_avg:169.49ms\nstep:1106/10000 train_loss:4.9754 train_time:185762ms step_avg:169.49ms\nstep:1107/10000 train_loss:4.8785 train_time:185932ms step_avg:169.49ms\nstep:1108/10000 train_loss:4.9421 train_time:186101ms step_avg:169.49ms\nstep:1109/10000 train_loss:4.8971 train_time:186270ms step_avg:169.49ms\nstep:1110/10000 train_loss:4.7827 train_time:186442ms step_avg:169.49ms\nstep:1111/10000 train_loss:4.8779 train_time:186614ms step_avg:169.50ms\nstep:1112/10000 train_loss:5.0769 train_time:186787ms step_avg:169.50ms\nstep:1113/10000 train_loss:4.8158 train_time:186955ms step_avg:169.50ms\nstep:1114/10000 train_loss:5.0933 train_time:187125ms step_avg:169.50ms\nstep:1115/10000 train_loss:4.8417 train_time:187295ms step_avg:169.50ms\nstep:1116/10000 train_loss:4.9083 train_time:187466ms step_avg:169.50ms\nstep:1117/10000 train_loss:4.7382 train_time:187636ms step_avg:169.50ms\nstep:1118/10000 train_loss:4.9157 train_time:187809ms step_avg:169.50ms\nstep:1119/10000 train_loss:4.9412 train_time:187977ms step_avg:169.50ms\nstep:1120/10000 train_loss:4.8848 train_time:188147ms step_avg:169.50ms\nstep:1121/10000 train_loss:4.9969 train_time:188316ms step_avg:169.50ms\nstep:1122/10000 train_loss:4.8652 train_time:188487ms step_avg:169.50ms\nstep:1123/10000 train_loss:4.7951 train_time:188658ms step_avg:169.50ms\nstep:1124/10000 train_loss:4.8885 train_time:188831ms step_avg:169.51ms\nstep:1125/10000 train_loss:4.8428 train_time:189001ms step_avg:169.51ms\nstep:1126/10000 train_loss:4.8897 train_time:189174ms step_avg:169.51ms\nstep:1127/10000 train_loss:4.9319 train_time:189342ms step_avg:169.51ms\nstep:1128/10000 train_loss:5.0175 train_time:189513ms step_avg:169.51ms\nstep:1129/10000 train_loss:4.9271 train_time:189684ms step_avg:169.51ms\nstep:1130/10000 train_loss:4.9943 train_time:189852ms step_avg:169.51ms\nstep:1131/10000 train_loss:4.8480 train_time:190022ms step_avg:169.51ms\nstep:1132/10000 train_loss:4.8283 train_time:190193ms step_avg:169.51ms\nstep:1133/10000 train_loss:4.8205 train_time:190366ms step_avg:169.52ms\nstep:1134/10000 train_loss:4.9024 train_time:190533ms step_avg:169.51ms\nstep:1135/10000 train_loss:4.8880 train_time:190702ms step_avg:169.51ms\nstep:1136/10000 train_loss:4.9861 train_time:190872ms step_avg:169.51ms\nstep:1137/10000 train_loss:4.7534 train_time:191042ms step_avg:169.51ms\nstep:1138/10000 train_loss:4.8083 train_time:191213ms step_avg:169.51ms\nstep:1139/10000 train_loss:4.9892 train_time:191386ms step_avg:169.52ms\nstep:1140/10000 train_loss:4.9578 train_time:191556ms step_avg:169.52ms\nstep:1141/10000 train_loss:4.9430 train_time:191725ms step_avg:169.52ms\nstep:1142/10000 train_loss:4.9289 train_time:191896ms step_avg:169.52ms\nstep:1143/10000 train_loss:4.8999 train_time:192067ms step_avg:169.52ms\nstep:1144/10000 train_loss:4.9949 train_time:192243ms step_avg:169.53ms\nstep:1145/10000 train_loss:4.8924 train_time:192409ms step_avg:169.52ms\nstep:1146/10000 train_loss:4.8789 train_time:192578ms step_avg:169.52ms\nstep:1147/10000 train_loss:4.8635 train_time:192749ms step_avg:169.52ms\nstep:1148/10000 train_loss:4.8705 train_time:192920ms step_avg:169.53ms\nstep:1149/10000 train_loss:4.8686 train_time:193095ms step_avg:169.53ms\nstep:1150/10000 train_loss:4.9057 train_time:193264ms step_avg:169.53ms\nstep:1151/10000 train_loss:4.8055 train_time:193433ms step_avg:169.53ms\nstep:1152/10000 train_loss:4.9139 train_time:193603ms step_avg:169.53ms\nstep:1153/10000 train_loss:4.8794 train_time:193774ms step_avg:169.53ms\nstep:1154/10000 train_loss:5.0054 train_time:193942ms step_avg:169.53ms\nstep:1155/10000 train_loss:4.8994 train_time:194113ms step_avg:169.53ms\nstep:1156/10000 train_loss:4.7602 train_time:194282ms step_avg:169.53ms\nstep:1157/10000 train_loss:4.6711 train_time:194450ms step_avg:169.53ms\nstep:1158/10000 train_loss:4.9816 train_time:194621ms step_avg:169.53ms\nstep:1159/10000 train_loss:4.7691 train_time:194793ms step_avg:169.53ms\nstep:1160/10000 train_loss:4.7756 train_time:194963ms step_avg:169.53ms\nstep:1161/10000 train_loss:4.9854 train_time:195136ms step_avg:169.54ms\nstep:1162/10000 train_loss:4.7957 train_time:195305ms step_avg:169.54ms\nstep:1163/10000 train_loss:4.8211 train_time:195474ms step_avg:169.53ms\nstep:1164/10000 train_loss:4.9197 train_time:195644ms step_avg:169.54ms\nstep:1165/10000 train_loss:4.8605 train_time:195812ms step_avg:169.53ms\nstep:1166/10000 train_loss:4.7604 train_time:195982ms step_avg:169.53ms\nstep:1167/10000 train_loss:4.8565 train_time:196153ms step_avg:169.54ms\nstep:1168/10000 train_loss:4.7968 train_time:196321ms step_avg:169.53ms\nstep:1169/10000 train_loss:4.8934 train_time:196487ms step_avg:169.53ms\nstep:1170/10000 train_loss:4.9528 train_time:196657ms step_avg:169.53ms\nstep:1171/10000 train_loss:4.8640 train_time:196827ms step_avg:169.53ms\nstep:1172/10000 train_loss:4.8445 train_time:196994ms step_avg:169.53ms\nstep:1173/10000 train_loss:4.8983 train_time:197171ms step_avg:169.54ms\nstep:1174/10000 train_loss:4.7533 train_time:197339ms step_avg:169.54ms\nstep:1175/10000 train_loss:4.7395 train_time:197507ms step_avg:169.53ms\nstep:1176/10000 train_loss:4.8352 train_time:197677ms step_avg:169.53ms\nstep:1177/10000 train_loss:5.0993 train_time:197847ms step_avg:169.54ms\nstep:1178/10000 train_loss:4.9577 train_time:198015ms step_avg:169.53ms\nstep:1179/10000 train_loss:5.0645 train_time:198184ms step_avg:169.53ms\nstep:1180/10000 train_loss:4.7307 train_time:198352ms step_avg:169.53ms\nstep:1181/10000 train_loss:4.6045 train_time:198518ms step_avg:169.53ms\nstep:1182/10000 train_loss:4.9615 train_time:198692ms step_avg:169.53ms\nstep:1183/10000 train_loss:4.9824 train_time:198861ms step_avg:169.53ms\nstep:1184/10000 train_loss:4.6979 train_time:199029ms step_avg:169.53ms\nstep:1185/10000 train_loss:4.8482 train_time:199203ms step_avg:169.53ms\nstep:1186/10000 train_loss:4.7754 train_time:199376ms step_avg:169.54ms\nstep:1187/10000 train_loss:4.9333 train_time:199541ms step_avg:169.53ms\nstep:1188/10000 train_loss:4.7310 train_time:199712ms step_avg:169.54ms\nstep:1189/10000 train_loss:4.9724 train_time:199880ms step_avg:169.53ms\nstep:1190/10000 train_loss:4.9481 train_time:200049ms step_avg:169.53ms\nstep:1191/10000 train_loss:4.8324 train_time:200216ms step_avg:169.53ms\nstep:1192/10000 train_loss:4.8371 train_time:200386ms step_avg:169.53ms\nstep:1193/10000 train_loss:4.9755 train_time:200553ms step_avg:169.53ms\nstep:1194/10000 train_loss:4.7974 train_time:200727ms step_avg:169.53ms\nstep:1195/10000 train_loss:4.7961 train_time:200895ms step_avg:169.53ms\nstep:1196/10000 train_loss:4.7393 train_time:201064ms step_avg:169.53ms\nstep:1197/10000 train_loss:4.7950 train_time:201236ms step_avg:169.53ms\nstep:1198/10000 train_loss:5.0480 train_time:201402ms step_avg:169.53ms\nstep:1199/10000 train_loss:4.7043 train_time:201572ms step_avg:169.53ms\nstep:1200/10000 train_loss:4.9177 train_time:201743ms step_avg:169.53ms\nstep:1201/10000 train_loss:4.8782 train_time:201917ms step_avg:169.54ms\nstep:1202/10000 train_loss:4.7860 train_time:202088ms step_avg:169.54ms\nstep:1203/10000 train_loss:4.8065 train_time:202259ms step_avg:169.54ms\nstep:1204/10000 train_loss:4.8693 train_time:202429ms step_avg:169.54ms\nstep:1205/10000 train_loss:4.7751 train_time:202597ms step_avg:169.54ms\nstep:1206/10000 train_loss:4.8292 train_time:202765ms step_avg:169.54ms\nstep:1207/10000 train_loss:4.8778 train_time:202938ms step_avg:169.54ms\nstep:1208/10000 train_loss:4.7790 train_time:203109ms step_avg:169.54ms\nstep:1209/10000 train_loss:4.8622 train_time:203280ms step_avg:169.54ms\nstep:1210/10000 train_loss:4.8128 train_time:203455ms step_avg:169.55ms\nstep:1211/10000 train_loss:4.8320 train_time:203620ms step_avg:169.54ms\nstep:1212/10000 train_loss:4.8559 train_time:203794ms step_avg:169.55ms\nstep:1213/10000 train_loss:4.8025 train_time:203963ms step_avg:169.55ms\nstep:1214/10000 train_loss:4.8526 train_time:204133ms step_avg:169.55ms\nstep:1215/10000 train_loss:4.7650 train_time:204302ms step_avg:169.55ms\nstep:1216/10000 train_loss:4.8354 train_time:204472ms step_avg:169.55ms\nstep:1217/10000 train_loss:4.8305 train_time:204639ms step_avg:169.54ms\nstep:1218/10000 train_loss:4.8067 train_time:204808ms step_avg:169.54ms\nstep:1219/10000 train_loss:4.8236 train_time:204976ms step_avg:169.54ms\nstep:1220/10000 train_loss:5.2780 train_time:205142ms step_avg:169.54ms\nstep:1221/10000 train_loss:4.6627 train_time:205311ms step_avg:169.54ms\nstep:1222/10000 train_loss:4.9054 train_time:205483ms step_avg:169.54ms\nstep:1223/10000 train_loss:4.5781 train_time:205649ms step_avg:169.54ms\nstep:1224/10000 train_loss:4.7919 train_time:205820ms step_avg:169.54ms\nstep:1225/10000 train_loss:4.7589 train_time:205992ms step_avg:169.54ms\nstep:1226/10000 train_loss:4.8459 train_time:206159ms step_avg:169.54ms\nstep:1227/10000 train_loss:4.9148 train_time:206327ms step_avg:169.54ms\nstep:1228/10000 train_loss:4.7045 train_time:206494ms step_avg:169.54ms\nstep:1229/10000 train_loss:4.8627 train_time:206664ms step_avg:169.54ms\nstep:1230/10000 train_loss:4.7879 train_time:206835ms step_avg:169.54ms\nstep:1231/10000 train_loss:4.8147 train_time:207006ms step_avg:169.54ms\nstep:1232/10000 train_loss:4.9136 train_time:207174ms step_avg:169.54ms\nstep:1233/10000 train_loss:4.6974 train_time:207344ms step_avg:169.54ms\nstep:1234/10000 train_loss:4.9443 train_time:207514ms step_avg:169.54ms\nstep:1235/10000 train_loss:4.6965 train_time:207685ms step_avg:169.54ms\nstep:1236/10000 train_loss:4.6740 train_time:207855ms step_avg:169.54ms\nstep:1237/10000 train_loss:4.7058 train_time:208023ms step_avg:169.54ms\nstep:1238/10000 train_loss:4.7730 train_time:208193ms step_avg:169.54ms\nstep:1239/10000 train_loss:4.9925 train_time:208363ms step_avg:169.54ms\nstep:1240/10000 train_loss:4.9150 train_time:208530ms step_avg:169.54ms\nstep:1241/10000 train_loss:4.8440 train_time:208701ms step_avg:169.54ms\nstep:1242/10000 train_loss:4.8372 train_time:208872ms step_avg:169.54ms\nstep:1243/10000 train_loss:4.9063 train_time:209043ms step_avg:169.54ms\nstep:1244/10000 train_loss:4.8556 train_time:209213ms step_avg:169.54ms\nstep:1245/10000 train_loss:4.8787 train_time:209383ms step_avg:169.54ms\nstep:1246/10000 train_loss:4.8154 train_time:209554ms step_avg:169.54ms\nstep:1247/10000 train_loss:4.9096 train_time:209727ms step_avg:169.54ms\nstep:1248/10000 train_loss:4.7618 train_time:209896ms step_avg:169.54ms\nstep:1249/10000 train_loss:4.8342 train_time:210067ms step_avg:169.55ms\nstep:1250/10000 train_loss:4.7039 train_time:210236ms step_avg:169.54ms\nstep:1251/10000 train_loss:4.7804 train_time:210405ms step_avg:169.54ms\nstep:1252/10000 train_loss:4.9343 train_time:210573ms step_avg:169.54ms\nstep:1253/10000 train_loss:4.8787 train_time:210743ms step_avg:169.54ms\nstep:1254/10000 train_loss:4.7392 train_time:210912ms step_avg:169.54ms\nstep:1255/10000 train_loss:4.9159 train_time:211084ms step_avg:169.55ms\nstep:1256/10000 train_loss:5.0108 train_time:211254ms step_avg:169.55ms\nstep:1257/10000 train_loss:4.7426 train_time:211425ms step_avg:169.55ms\nstep:1258/10000 train_loss:4.8918 train_time:211594ms step_avg:169.55ms\nstep:1259/10000 train_loss:4.8189 train_time:211766ms step_avg:169.55ms\nstep:1260/10000 train_loss:4.8354 train_time:211934ms step_avg:169.55ms\nstep:1261/10000 train_loss:4.8605 train_time:212105ms step_avg:169.55ms\nstep:1262/10000 train_loss:4.5264 train_time:212276ms step_avg:169.55ms\nstep:1263/10000 train_loss:4.8622 train_time:212445ms step_avg:169.55ms\nstep:1264/10000 train_loss:4.7417 train_time:212612ms step_avg:169.55ms\nstep:1265/10000 train_loss:4.8402 train_time:212783ms step_avg:169.55ms\nstep:1266/10000 train_loss:4.8048 train_time:212953ms step_avg:169.55ms\nstep:1267/10000 train_loss:4.7651 train_time:213123ms step_avg:169.55ms\nstep:1268/10000 train_loss:4.7575 train_time:213293ms step_avg:169.55ms\nstep:1269/10000 train_loss:4.6843 train_time:213462ms step_avg:169.55ms\nstep:1270/10000 train_loss:4.7836 train_time:213632ms step_avg:169.55ms\nstep:1271/10000 train_loss:4.7431 train_time:213801ms step_avg:169.55ms\nstep:1272/10000 train_loss:4.7287 train_time:213973ms step_avg:169.55ms\nstep:1273/10000 train_loss:4.6096 train_time:214144ms step_avg:169.55ms\nstep:1274/10000 train_loss:4.6913 train_time:214317ms step_avg:169.55ms\nstep:1275/10000 train_loss:4.7204 train_time:214489ms step_avg:169.56ms\nstep:1276/10000 train_loss:4.6860 train_time:214656ms step_avg:169.55ms\nstep:1277/10000 train_loss:4.9245 train_time:214824ms step_avg:169.55ms\nstep:1278/10000 train_loss:4.7532 train_time:214994ms step_avg:169.55ms\nstep:1279/10000 train_loss:4.8026 train_time:215164ms step_avg:169.55ms\nstep:1280/10000 train_loss:4.7571 train_time:215338ms step_avg:169.56ms\nstep:1281/10000 train_loss:4.8428 train_time:215508ms step_avg:169.56ms\nstep:1282/10000 train_loss:4.7090 train_time:215675ms step_avg:169.56ms\nstep:1283/10000 train_loss:4.7855 train_time:215846ms step_avg:169.56ms\nstep:1284/10000 train_loss:4.7928 train_time:216014ms step_avg:169.56ms\nstep:1285/10000 train_loss:4.6464 train_time:216184ms step_avg:169.56ms\nstep:1286/10000 train_loss:4.8023 train_time:216352ms step_avg:169.55ms\nstep:1287/10000 train_loss:4.8719 train_time:216522ms step_avg:169.56ms\nstep:1288/10000 train_loss:4.7442 train_time:216692ms step_avg:169.56ms\nstep:1289/10000 train_loss:4.8028 train_time:216863ms step_avg:169.56ms\nstep:1290/10000 train_loss:4.6782 train_time:217032ms step_avg:169.56ms\nstep:1291/10000 train_loss:4.7536 train_time:217201ms step_avg:169.56ms\nstep:1292/10000 train_loss:4.8366 train_time:217375ms step_avg:169.56ms\nstep:1293/10000 train_loss:4.8257 train_time:217544ms step_avg:169.56ms\nstep:1294/10000 train_loss:4.7758 train_time:217713ms step_avg:169.56ms\nstep:1295/10000 train_loss:4.8259 train_time:217883ms step_avg:169.56ms\nstep:1296/10000 train_loss:4.8788 train_time:218052ms step_avg:169.56ms\nstep:1297/10000 train_loss:4.7119 train_time:218224ms step_avg:169.56ms\nstep:1298/10000 train_loss:4.7546 train_time:218395ms step_avg:169.56ms\nstep:1299/10000 train_loss:4.7458 train_time:218565ms step_avg:169.56ms\nstep:1300/10000 train_loss:4.9105 train_time:218733ms step_avg:169.56ms\nstep:1301/10000 train_loss:4.8603 train_time:218903ms step_avg:169.56ms\nstep:1302/10000 train_loss:4.7113 train_time:219073ms step_avg:169.56ms\nstep:1303/10000 train_loss:4.8773 train_time:219242ms step_avg:169.56ms\nstep:1304/10000 train_loss:4.7688 train_time:219412ms step_avg:169.56ms\nstep:1305/10000 train_loss:4.8103 train_time:219585ms step_avg:169.56ms\nstep:1306/10000 train_loss:4.7482 train_time:219755ms step_avg:169.56ms\nstep:1307/10000 train_loss:4.8599 train_time:219926ms step_avg:169.56ms\nstep:1308/10000 train_loss:5.2270 train_time:220093ms step_avg:169.56ms\nstep:1309/10000 train_loss:4.6514 train_time:220262ms step_avg:169.56ms\nstep:1310/10000 train_loss:4.8830 train_time:220433ms step_avg:169.56ms\nstep:1311/10000 train_loss:4.9247 train_time:220605ms step_avg:169.57ms\nstep:1312/10000 train_loss:4.9208 train_time:220777ms step_avg:169.57ms\nstep:1313/10000 train_loss:4.8394 train_time:220946ms step_avg:169.57ms\nstep:1314/10000 train_loss:4.8447 train_time:221114ms step_avg:169.57ms\nstep:1315/10000 train_loss:4.7727 train_time:221287ms step_avg:169.57ms\nstep:1316/10000 train_loss:4.7081 train_time:221455ms step_avg:169.57ms\nstep:1317/10000 train_loss:4.8079 train_time:221625ms step_avg:169.57ms\nstep:1318/10000 train_loss:4.7749 train_time:221793ms step_avg:169.57ms\nstep:1319/10000 train_loss:4.9713 train_time:221963ms step_avg:169.57ms\nstep:1320/10000 train_loss:4.7073 train_time:222130ms step_avg:169.57ms\nstep:1321/10000 train_loss:4.8411 train_time:222300ms step_avg:169.57ms\nstep:1322/10000 train_loss:4.7295 train_time:222470ms step_avg:169.57ms\nstep:1323/10000 train_loss:4.6972 train_time:222639ms step_avg:169.57ms\nstep:1324/10000 train_loss:4.8006 train_time:222811ms step_avg:169.57ms\nstep:1325/10000 train_loss:4.6897 train_time:222979ms step_avg:169.57ms\nstep:1326/10000 train_loss:5.2643 train_time:223149ms step_avg:169.57ms\nstep:1327/10000 train_loss:4.7026 train_time:223316ms step_avg:169.56ms\nstep:1328/10000 train_loss:4.7233 train_time:223486ms step_avg:169.56ms\nstep:1329/10000 train_loss:4.6550 train_time:223656ms step_avg:169.57ms\nstep:1330/10000 train_loss:4.6815 train_time:223830ms step_avg:169.57ms\nstep:1331/10000 train_loss:4.6634 train_time:223999ms step_avg:169.57ms\nstep:1332/10000 train_loss:4.7314 train_time:224173ms step_avg:169.57ms\nstep:1333/10000 train_loss:4.6533 train_time:224340ms step_avg:169.57ms\nstep:1334/10000 train_loss:4.6510 train_time:224509ms step_avg:169.57ms\nstep:1335/10000 train_loss:4.6328 train_time:224678ms step_avg:169.57ms\nstep:1336/10000 train_loss:4.6660 train_time:224847ms step_avg:169.57ms\nstep:1337/10000 train_loss:4.6485 train_time:225016ms step_avg:169.57ms\nstep:1338/10000 train_loss:4.7463 train_time:225188ms step_avg:169.57ms\nstep:1339/10000 train_loss:4.7829 train_time:225356ms step_avg:169.57ms\nstep:1340/10000 train_loss:4.8679 train_time:225527ms step_avg:169.57ms\nstep:1341/10000 train_loss:4.7423 train_time:225695ms step_avg:169.57ms\nstep:1342/10000 train_loss:4.7831 train_time:225865ms step_avg:169.57ms\nstep:1343/10000 train_loss:4.6903 train_time:226035ms step_avg:169.57ms\nstep:1344/10000 train_loss:4.7945 train_time:226206ms step_avg:169.57ms\nstep:1345/10000 train_loss:4.6288 train_time:226375ms step_avg:169.57ms\nstep:1346/10000 train_loss:4.7387 train_time:226546ms step_avg:169.57ms\nstep:1347/10000 train_loss:4.6233 train_time:226715ms step_avg:169.57ms\nstep:1348/10000 train_loss:5.3180 train_time:226885ms step_avg:169.57ms\nstep:1349/10000 train_loss:5.8792 train_time:227053ms step_avg:169.57ms\nstep:1350/10000 train_loss:4.7455 train_time:227221ms step_avg:169.57ms\nstep:1351/10000 train_loss:4.8317 train_time:227390ms step_avg:169.57ms\nstep:1352/10000 train_loss:4.7682 train_time:227562ms step_avg:169.57ms\nstep:1353/10000 train_loss:4.7510 train_time:227733ms step_avg:169.57ms\nstep:1354/10000 train_loss:4.7252 train_time:227901ms step_avg:169.57ms\nstep:1355/10000 train_loss:4.8319 train_time:228069ms step_avg:169.57ms\nstep:1356/10000 train_loss:4.7661 train_time:228239ms step_avg:169.57ms\nstep:1357/10000 train_loss:4.6711 train_time:228411ms step_avg:169.57ms\nstep:1358/10000 train_loss:4.6690 train_time:228580ms step_avg:169.57ms\nstep:1359/10000 train_loss:4.7129 train_time:228750ms step_avg:169.57ms\nstep:1360/10000 train_loss:4.7103 train_time:228921ms step_avg:169.57ms\nstep:1361/10000 train_loss:4.7181 train_time:229093ms step_avg:169.57ms\nstep:1362/10000 train_loss:4.6237 train_time:229262ms step_avg:169.57ms\nstep:1363/10000 train_loss:4.6522 train_time:229433ms step_avg:169.57ms\nstep:1364/10000 train_loss:4.6597 train_time:229605ms step_avg:169.58ms\nstep:1365/10000 train_loss:4.7688 train_time:229773ms step_avg:169.57ms\nstep:1366/10000 train_loss:4.6770 train_time:229942ms step_avg:169.57ms\nstep:1367/10000 train_loss:4.6882 train_time:230112ms step_avg:169.57ms\nstep:1368/10000 train_loss:4.6405 train_time:230280ms step_avg:169.57ms\nstep:1369/10000 train_loss:4.7106 train_time:230452ms step_avg:169.57ms\nstep:1370/10000 train_loss:4.6523 train_time:230623ms step_avg:169.58ms\nstep:1371/10000 train_loss:4.7208 train_time:230794ms step_avg:169.58ms\nstep:1372/10000 train_loss:4.7035 train_time:230967ms step_avg:169.58ms\nstep:1373/10000 train_loss:4.6693 train_time:231136ms step_avg:169.58ms\nstep:1374/10000 train_loss:4.8117 train_time:231305ms step_avg:169.58ms\nstep:1375/10000 train_loss:4.7277 train_time:231475ms step_avg:169.58ms\nstep:1376/10000 train_loss:4.7914 train_time:231645ms step_avg:169.58ms\nstep:1377/10000 train_loss:4.4632 train_time:231813ms step_avg:169.58ms\nstep:1378/10000 train_loss:4.6739 train_time:231981ms step_avg:169.58ms\nstep:1379/10000 train_loss:4.8167 train_time:232150ms step_avg:169.58ms\nstep:1380/10000 train_loss:4.6911 train_time:232321ms step_avg:169.58ms\nstep:1381/10000 train_loss:4.6591 train_time:232494ms step_avg:169.58ms\nstep:1382/10000 train_loss:4.6502 train_time:232665ms step_avg:169.58ms\nstep:1383/10000 train_loss:4.6177 train_time:232835ms step_avg:169.58ms\nstep:1384/10000 train_loss:4.6387 train_time:233004ms step_avg:169.58ms\nstep:1385/10000 train_loss:4.6110 train_time:233174ms step_avg:169.58ms\nstep:1386/10000 train_loss:4.7342 train_time:233341ms step_avg:169.58ms\nstep:1387/10000 train_loss:4.6354 train_time:233511ms step_avg:169.58ms\nstep:1388/10000 train_loss:4.6880 train_time:233680ms step_avg:169.58ms\nstep:1389/10000 train_loss:4.7161 train_time:233855ms step_avg:169.58ms\nstep:1390/10000 train_loss:4.6147 train_time:234022ms step_avg:169.58ms\nstep:1391/10000 train_loss:4.6598 train_time:234193ms step_avg:169.58ms\nstep:1392/10000 train_loss:4.6787 train_time:234362ms step_avg:169.58ms\nstep:1393/10000 train_loss:4.7519 train_time:234532ms step_avg:169.58ms\nstep:1394/10000 train_loss:4.6937 train_time:234702ms step_avg:169.58ms\nstep:1395/10000 train_loss:4.6082 train_time:234870ms step_avg:169.58ms\nstep:1396/10000 train_loss:4.5920 train_time:235040ms step_avg:169.58ms\nstep:1397/10000 train_loss:4.4444 train_time:235211ms step_avg:169.58ms\nstep:1398/10000 train_loss:4.6927 train_time:235380ms step_avg:169.58ms\nstep:1399/10000 train_loss:4.5851 train_time:235547ms step_avg:169.58ms\nstep:1400/10000 train_loss:4.2642 train_time:235716ms step_avg:169.58ms\nstep:1401/10000 train_loss:4.6536 train_time:235885ms step_avg:169.58ms\nstep:1402/10000 train_loss:4.8914 train_time:236054ms step_avg:169.58ms\nstep:1403/10000 train_loss:4.6192 train_time:236226ms step_avg:169.58ms\nstep:1404/10000 train_loss:4.5677 train_time:236394ms step_avg:169.58ms\nstep:1405/10000 train_loss:4.6952 train_time:236563ms step_avg:169.58ms\nstep:1406/10000 train_loss:4.6029 train_time:236731ms step_avg:169.58ms\nstep:1407/10000 train_loss:4.7068 train_time:236902ms step_avg:169.58ms\nstep:1408/10000 train_loss:4.9577 train_time:237069ms step_avg:169.58ms\nstep:1409/10000 train_loss:4.6151 train_time:237238ms step_avg:169.58ms\nstep:1410/10000 train_loss:4.6390 train_time:237407ms step_avg:169.58ms\nstep:1411/10000 train_loss:4.6447 train_time:237577ms step_avg:169.58ms\nstep:1412/10000 train_loss:4.4993 train_time:237747ms step_avg:169.58ms\nstep:1413/10000 train_loss:4.7058 train_time:237916ms step_avg:169.58ms\nstep:1414/10000 train_loss:4.6599 train_time:238086ms step_avg:169.58ms\nstep:1415/10000 train_loss:4.6811 train_time:238255ms step_avg:169.58ms\nstep:1416/10000 train_loss:4.7028 train_time:238425ms step_avg:169.58ms\nstep:1417/10000 train_loss:4.5728 train_time:238595ms step_avg:169.58ms\nstep:1418/10000 train_loss:4.5370 train_time:238767ms step_avg:169.58ms\nstep:1419/10000 train_loss:4.5710 train_time:238936ms step_avg:169.58ms\nstep:1420/10000 train_loss:4.6319 train_time:239106ms step_avg:169.58ms\nstep:1421/10000 train_loss:4.7012 train_time:239275ms step_avg:169.58ms\nstep:1422/10000 train_loss:4.7141 train_time:239444ms step_avg:169.58ms\nstep:1423/10000 train_loss:4.7063 train_time:239610ms step_avg:169.58ms\nstep:1424/10000 train_loss:4.7143 train_time:239779ms step_avg:169.57ms\nstep:1425/10000 train_loss:4.5908 train_time:239949ms step_avg:169.58ms\nstep:1426/10000 train_loss:4.5999 train_time:240120ms step_avg:169.58ms\nstep:1427/10000 train_loss:4.4930 train_time:240288ms step_avg:169.58ms\nstep:1428/10000 train_loss:4.7139 train_time:240459ms step_avg:169.58ms\nstep:1429/10000 train_loss:4.6309 train_time:240630ms step_avg:169.58ms\nstep:1430/10000 train_loss:4.6952 train_time:240799ms step_avg:169.58ms\nstep:1431/10000 train_loss:4.8662 train_time:240968ms step_avg:169.58ms\nstep:1432/10000 train_loss:4.6384 train_time:241138ms step_avg:169.58ms\nstep:1433/10000 train_loss:4.5174 train_time:241307ms step_avg:169.58ms\nstep:1434/10000 train_loss:4.7091 train_time:241475ms step_avg:169.58ms\nstep:1435/10000 train_loss:4.7736 train_time:241647ms step_avg:169.58ms\nstep:1436/10000 train_loss:4.8355 train_time:241816ms step_avg:169.58ms\nstep:1437/10000 train_loss:4.6394 train_time:241984ms step_avg:169.58ms\nstep:1438/10000 train_loss:4.6033 train_time:242154ms step_avg:169.58ms\nstep:1439/10000 train_loss:4.8370 train_time:242325ms step_avg:169.58ms\nstep:1440/10000 train_loss:4.4178 train_time:242495ms step_avg:169.58ms\nstep:1441/10000 train_loss:4.6110 train_time:242665ms step_avg:169.58ms\nstep:1442/10000 train_loss:4.8948 train_time:242836ms step_avg:169.58ms\nstep:1443/10000 train_loss:4.7485 train_time:243006ms step_avg:169.58ms\nstep:1444/10000 train_loss:4.8545 train_time:243175ms step_avg:169.58ms\nstep:1445/10000 train_loss:4.6825 train_time:243346ms step_avg:169.58ms\nstep:1446/10000 train_loss:4.8172 train_time:243515ms step_avg:169.58ms\nstep:1447/10000 train_loss:4.4715 train_time:243686ms step_avg:169.58ms\nstep:1448/10000 train_loss:4.7673 train_time:243854ms step_avg:169.58ms\nstep:1449/10000 train_loss:4.7071 train_time:244024ms step_avg:169.58ms\nstep:1450/10000 train_loss:4.6882 train_time:244197ms step_avg:169.58ms\nstep:1451/10000 train_loss:4.7204 train_time:244366ms step_avg:169.58ms\nstep:1452/10000 train_loss:4.6243 train_time:244536ms step_avg:169.58ms\nstep:1453/10000 train_loss:4.6401 train_time:244704ms step_avg:169.58ms\nstep:1454/10000 train_loss:4.5897 train_time:244869ms step_avg:169.58ms\nstep:1455/10000 train_loss:4.6278 train_time:245039ms step_avg:169.58ms\nstep:1456/10000 train_loss:4.5698 train_time:245206ms step_avg:169.58ms\nstep:1457/10000 train_loss:5.0218 train_time:245375ms step_avg:169.57ms\nstep:1458/10000 train_loss:4.7468 train_time:245546ms step_avg:169.58ms\nstep:1459/10000 train_loss:4.6418 train_time:245715ms step_avg:169.58ms\nstep:1460/10000 train_loss:4.7065 train_time:245884ms step_avg:169.57ms\nstep:1461/10000 train_loss:4.6722 train_time:246054ms step_avg:169.58ms\nstep:1462/10000 train_loss:4.6550 train_time:246228ms step_avg:169.58ms\nstep:1463/10000 train_loss:4.7540 train_time:246395ms step_avg:169.58ms\nstep:1464/10000 train_loss:4.6360 train_time:246566ms step_avg:169.58ms\nstep:1465/10000 train_loss:4.5801 train_time:246735ms step_avg:169.58ms\nstep:1466/10000 train_loss:4.6523 train_time:246905ms step_avg:169.58ms\nstep:1467/10000 train_loss:4.6593 train_time:247075ms step_avg:169.58ms\nstep:1468/10000 train_loss:4.6402 train_time:247244ms step_avg:169.58ms\nstep:1469/10000 train_loss:4.6023 train_time:247412ms step_avg:169.58ms\nstep:1470/10000 train_loss:4.6229 train_time:247584ms step_avg:169.58ms\nstep:1471/10000 train_loss:4.6196 train_time:247753ms step_avg:169.58ms\nstep:1472/10000 train_loss:4.6211 train_time:247921ms step_avg:169.58ms\nstep:1473/10000 train_loss:4.6081 train_time:248089ms step_avg:169.58ms\nstep:1474/10000 train_loss:4.7017 train_time:248257ms step_avg:169.57ms\nstep:1475/10000 train_loss:4.6279 train_time:248428ms step_avg:169.58ms\nstep:1476/10000 train_loss:4.5056 train_time:248597ms step_avg:169.58ms\nstep:1477/10000 train_loss:4.6099 train_time:248772ms step_avg:169.58ms\nstep:1478/10000 train_loss:4.6053 train_time:248944ms step_avg:169.58ms\nstep:1479/10000 train_loss:4.5836 train_time:249116ms step_avg:169.58ms\nstep:1480/10000 train_loss:4.6579 train_time:249284ms step_avg:169.58ms\nstep:1481/10000 train_loss:4.6212 train_time:249454ms step_avg:169.58ms\nstep:1482/10000 train_loss:4.4671 train_time:249624ms step_avg:169.58ms\nstep:1483/10000 train_loss:4.7078 train_time:249792ms step_avg:169.58ms\nstep:1484/10000 train_loss:4.6271 train_time:249963ms step_avg:169.58ms\nstep:1485/10000 train_loss:4.7203 train_time:250134ms step_avg:169.58ms\nstep:1486/10000 train_loss:4.5969 train_time:250304ms step_avg:169.58ms\nstep:1487/10000 train_loss:4.6117 train_time:250474ms step_avg:169.58ms\nstep:1488/10000 train_loss:4.4755 train_time:250645ms step_avg:169.58ms\nstep:1489/10000 train_loss:4.5866 train_time:250814ms step_avg:169.58ms\nstep:1490/10000 train_loss:4.6004 train_time:250982ms step_avg:169.58ms\nstep:1491/10000 train_loss:4.7399 train_time:251151ms step_avg:169.58ms\nstep:1492/10000 train_loss:4.6620 train_time:251321ms step_avg:169.58ms\nstep:1493/10000 train_loss:4.6320 train_time:251490ms step_avg:169.58ms\nstep:1494/10000 train_loss:4.6363 train_time:251659ms step_avg:169.58ms\nstep:1495/10000 train_loss:4.5298 train_time:251828ms step_avg:169.58ms\nstep:1496/10000 train_loss:4.5946 train_time:251999ms step_avg:169.58ms\nstep:1497/10000 train_loss:4.7047 train_time:252172ms step_avg:169.58ms\nstep:1498/10000 train_loss:4.6326 train_time:252342ms step_avg:169.58ms\nstep:1499/10000 train_loss:4.5102 train_time:252510ms step_avg:169.58ms\nstep:1500/10000 train_loss:4.5965 train_time:252681ms step_avg:169.58ms\nstep:1501/10000 train_loss:4.5876 train_time:252848ms step_avg:169.58ms\nstep:1502/10000 train_loss:4.6270 train_time:253018ms step_avg:169.58ms\nstep:1503/10000 train_loss:4.5930 train_time:253187ms step_avg:169.58ms\nstep:1504/10000 train_loss:4.6453 train_time:253354ms step_avg:169.58ms\nstep:1505/10000 train_loss:4.6771 train_time:253524ms step_avg:169.58ms\nstep:1506/10000 train_loss:4.5204 train_time:253695ms step_avg:169.58ms\nstep:1507/10000 train_loss:4.6545 train_time:253865ms step_avg:169.58ms\nstep:1508/10000 train_loss:4.5427 train_time:254034ms step_avg:169.58ms\nstep:1509/10000 train_loss:4.5195 train_time:254205ms step_avg:169.58ms\nstep:1510/10000 train_loss:3.9579 train_time:254375ms step_avg:169.58ms\nstep:1511/10000 train_loss:4.5996 train_time:254548ms step_avg:169.59ms\nstep:1512/10000 train_loss:4.6160 train_time:254716ms step_avg:169.58ms\nstep:1513/10000 train_loss:4.7648 train_time:254885ms step_avg:169.58ms\nstep:1514/10000 train_loss:4.4521 train_time:255055ms step_avg:169.58ms\nstep:1515/10000 train_loss:4.4684 train_time:255226ms step_avg:169.59ms\nstep:1516/10000 train_loss:4.7030 train_time:255394ms step_avg:169.58ms\nstep:1517/10000 train_loss:4.6876 train_time:255563ms step_avg:169.58ms\nstep:1518/10000 train_loss:4.5060 train_time:255730ms step_avg:169.58ms\nstep:1519/10000 train_loss:4.7870 train_time:255898ms step_avg:169.58ms\nstep:1520/10000 train_loss:4.7200 train_time:256069ms step_avg:169.58ms\nstep:1521/10000 train_loss:4.5585 train_time:256236ms step_avg:169.58ms\nstep:1522/10000 train_loss:4.6024 train_time:256407ms step_avg:169.58ms\nstep:1523/10000 train_loss:4.6669 train_time:256576ms step_avg:169.58ms\nstep:1524/10000 train_loss:4.5086 train_time:256954ms step_avg:169.72ms\nstep:1525/10000 train_loss:4.5433 train_time:257118ms step_avg:169.71ms\nstep:1526/10000 train_loss:4.6630 train_time:257285ms step_avg:169.71ms\nstep:1527/10000 train_loss:4.6587 train_time:257453ms step_avg:169.71ms\nstep:1528/10000 train_loss:4.5918 train_time:257622ms step_avg:169.71ms\nstep:1529/10000 train_loss:4.8672 train_time:257791ms step_avg:169.71ms\nstep:1530/10000 train_loss:4.6638 train_time:257973ms step_avg:169.72ms\nstep:1531/10000 train_loss:4.6017 train_time:258144ms step_avg:169.72ms\nstep:1532/10000 train_loss:4.5362 train_time:258313ms step_avg:169.72ms\nstep:1533/10000 train_loss:4.5996 train_time:258483ms step_avg:169.72ms\nstep:1534/10000 train_loss:4.5180 train_time:258652ms step_avg:169.72ms\nstep:1535/10000 train_loss:4.4752 train_time:258824ms step_avg:169.72ms\nstep:1536/10000 train_loss:4.6302 train_time:258997ms step_avg:169.72ms\nstep:1537/10000 train_loss:4.6800 train_time:259169ms step_avg:169.72ms\nstep:1538/10000 train_loss:4.6320 train_time:259341ms step_avg:169.73ms\nstep:1539/10000 train_loss:4.5796 train_time:259510ms step_avg:169.73ms\nstep:1540/10000 train_loss:4.4543 train_time:259679ms step_avg:169.72ms\nstep:1541/10000 train_loss:4.6633 train_time:259850ms step_avg:169.73ms\nstep:1542/10000 train_loss:4.5506 train_time:260023ms step_avg:169.73ms\nstep:1543/10000 train_loss:4.6278 train_time:260193ms step_avg:169.73ms\nstep:1544/10000 train_loss:4.6364 train_time:260364ms step_avg:169.73ms\nstep:1545/10000 train_loss:4.7046 train_time:260533ms step_avg:169.73ms\nstep:1546/10000 train_loss:4.6722 train_time:260702ms step_avg:169.73ms\nstep:1547/10000 train_loss:4.5730 train_time:260873ms step_avg:169.73ms\nstep:1548/10000 train_loss:4.5251 train_time:261047ms step_avg:169.73ms\nstep:1549/10000 train_loss:4.6363 train_time:261216ms step_avg:169.73ms\nstep:1550/10000 train_loss:4.8878 train_time:261385ms step_avg:169.73ms\nstep:1551/10000 train_loss:4.5875 train_time:261554ms step_avg:169.73ms\nstep:1552/10000 train_loss:4.6746 train_time:261723ms step_avg:169.73ms\nstep:1553/10000 train_loss:4.6930 train_time:261894ms step_avg:169.73ms\nstep:1554/10000 train_loss:4.5733 train_time:262066ms step_avg:169.73ms\nstep:1555/10000 train_loss:4.4865 train_time:262237ms step_avg:169.73ms\nstep:1556/10000 train_loss:4.5807 train_time:262407ms step_avg:169.73ms\nstep:1557/10000 train_loss:4.5833 train_time:262575ms step_avg:169.73ms\nstep:1558/10000 train_loss:4.5006 train_time:262747ms step_avg:169.73ms\nstep:1559/10000 train_loss:4.6122 train_time:262917ms step_avg:169.73ms\nstep:1560/10000 train_loss:4.5963 train_time:263092ms step_avg:169.74ms\nstep:1561/10000 train_loss:4.4420 train_time:263263ms step_avg:169.74ms\nstep:1562/10000 train_loss:4.6515 train_time:263433ms step_avg:169.74ms\nstep:1563/10000 train_loss:4.6418 train_time:263601ms step_avg:169.74ms\nstep:1564/10000 train_loss:4.5154 train_time:263771ms step_avg:169.74ms\nstep:1565/10000 train_loss:4.5930 train_time:263942ms step_avg:169.74ms\nstep:1566/10000 train_loss:4.5708 train_time:264115ms step_avg:169.74ms\nstep:1567/10000 train_loss:4.5888 train_time:264285ms step_avg:169.74ms\nstep:1568/10000 train_loss:4.6247 train_time:264454ms step_avg:169.74ms\nstep:1569/10000 train_loss:4.5716 train_time:264623ms step_avg:169.74ms\nstep:1570/10000 train_loss:4.6562 train_time:264791ms step_avg:169.74ms\nstep:1571/10000 train_loss:4.7214 train_time:264964ms step_avg:169.74ms\nstep:1572/10000 train_loss:4.5264 train_time:265137ms step_avg:169.74ms\nstep:1573/10000 train_loss:4.6028 train_time:265306ms step_avg:169.74ms\nstep:1574/10000 train_loss:4.4846 train_time:265478ms step_avg:169.74ms\nstep:1575/10000 train_loss:4.6364 train_time:265649ms step_avg:169.74ms\nstep:1576/10000 train_loss:4.5788 train_time:265818ms step_avg:169.74ms\nstep:1577/10000 train_loss:4.5981 train_time:265989ms step_avg:169.74ms\nstep:1578/10000 train_loss:4.6112 train_time:266159ms step_avg:169.74ms\nstep:1579/10000 train_loss:4.6683 train_time:266330ms step_avg:169.74ms\nstep:1580/10000 train_loss:4.5383 train_time:266499ms step_avg:169.74ms\nstep:1581/10000 train_loss:4.6294 train_time:266669ms step_avg:169.74ms\nstep:1582/10000 train_loss:4.4138 train_time:266839ms step_avg:169.74ms\nstep:1583/10000 train_loss:4.5703 train_time:267012ms step_avg:169.75ms\nstep:1584/10000 train_loss:4.5703 train_time:267182ms step_avg:169.75ms\nstep:1585/10000 train_loss:4.5297 train_time:267351ms step_avg:169.75ms\nstep:1586/10000 train_loss:4.5513 train_time:267522ms step_avg:169.75ms\nstep:1587/10000 train_loss:4.4139 train_time:267694ms step_avg:169.75ms\nstep:1588/10000 train_loss:4.3509 train_time:267863ms step_avg:169.75ms\nstep:1589/10000 train_loss:4.4521 train_time:268032ms step_avg:169.75ms\nstep:1590/10000 train_loss:4.6918 train_time:268202ms step_avg:169.75ms\nstep:1591/10000 train_loss:4.8250 train_time:268372ms step_avg:169.75ms\nstep:1592/10000 train_loss:4.5484 train_time:268542ms step_avg:169.75ms\nstep:1593/10000 train_loss:4.7082 train_time:268713ms step_avg:169.75ms\nstep:1594/10000 train_loss:4.6186 train_time:268884ms step_avg:169.75ms\nstep:1595/10000 train_loss:4.5113 train_time:269055ms step_avg:169.75ms\nstep:1596/10000 train_loss:4.5462 train_time:269226ms step_avg:169.75ms\nstep:1597/10000 train_loss:4.6265 train_time:269396ms step_avg:169.75ms\nstep:1598/10000 train_loss:4.5468 train_time:269565ms step_avg:169.75ms\nstep:1599/10000 train_loss:4.4878 train_time:269733ms step_avg:169.75ms\nstep:1600/10000 train_loss:4.5130 train_time:269904ms step_avg:169.75ms\nstep:1601/10000 train_loss:4.5244 train_time:270074ms step_avg:169.75ms\nstep:1602/10000 train_loss:4.6377 train_time:270245ms step_avg:169.75ms\nstep:1603/10000 train_loss:4.5771 train_time:270416ms step_avg:169.75ms\nstep:1604/10000 train_loss:4.6064 train_time:270587ms step_avg:169.75ms\nstep:1605/10000 train_loss:4.5435 train_time:270757ms step_avg:169.75ms\nstep:1606/10000 train_loss:4.6291 train_time:270928ms step_avg:169.75ms\nstep:1607/10000 train_loss:4.6653 train_time:271096ms step_avg:169.75ms\nstep:1608/10000 train_loss:4.5086 train_time:271266ms step_avg:169.75ms\nstep:1609/10000 train_loss:4.5224 train_time:271437ms step_avg:169.75ms\nstep:1610/10000 train_loss:4.6009 train_time:271610ms step_avg:169.76ms\nstep:1611/10000 train_loss:4.5769 train_time:271777ms step_avg:169.75ms\nstep:1612/10000 train_loss:4.7748 train_time:271947ms step_avg:169.75ms\nstep:1613/10000 train_loss:4.5457 train_time:272118ms step_avg:169.76ms\nstep:1614/10000 train_loss:4.6977 train_time:272286ms step_avg:169.75ms\nstep:1615/10000 train_loss:4.4711 train_time:272455ms step_avg:169.75ms\nstep:1616/10000 train_loss:4.6795 train_time:272626ms step_avg:169.75ms\nstep:1617/10000 train_loss:4.5735 train_time:272794ms step_avg:169.75ms\nstep:1618/10000 train_loss:4.5089 train_time:272963ms step_avg:169.75ms\nstep:1619/10000 train_loss:4.5381 train_time:273134ms step_avg:169.75ms\nstep:1620/10000 train_loss:4.6128 train_time:273305ms step_avg:169.75ms\nstep:1621/10000 train_loss:4.5779 train_time:273473ms step_avg:169.75ms\nstep:1622/10000 train_loss:4.5282 train_time:273645ms step_avg:169.76ms\nstep:1623/10000 train_loss:4.5431 train_time:273814ms step_avg:169.75ms\nstep:1624/10000 train_loss:4.6040 train_time:273985ms step_avg:169.75ms\nstep:1625/10000 train_loss:4.9630 train_time:274156ms step_avg:169.76ms\nstep:1626/10000 train_loss:4.8012 train_time:274328ms step_avg:169.76ms\nstep:1627/10000 train_loss:4.5006 train_time:274498ms step_avg:169.76ms\nstep:1628/10000 train_loss:4.5318 train_time:274673ms step_avg:169.76ms\nstep:1629/10000 train_loss:4.6121 train_time:274843ms step_avg:169.76ms\nstep:1630/10000 train_loss:4.5607 train_time:275011ms step_avg:169.76ms\nstep:1631/10000 train_loss:4.5767 train_time:275181ms step_avg:169.76ms\nstep:1632/10000 train_loss:4.4523 train_time:275351ms step_avg:169.76ms\nstep:1633/10000 train_loss:4.7497 train_time:275520ms step_avg:169.76ms\nstep:1634/10000 train_loss:4.5684 train_time:275694ms step_avg:169.76ms\nstep:1635/10000 train_loss:4.5692 train_time:275865ms step_avg:169.76ms\nstep:1636/10000 train_loss:4.5857 train_time:276034ms step_avg:169.76ms\nstep:1637/10000 train_loss:4.7440 train_time:276203ms step_avg:169.76ms\nstep:1638/10000 train_loss:4.5946 train_time:276370ms step_avg:169.76ms\nstep:1639/10000 train_loss:4.5165 train_time:276542ms step_avg:169.76ms\nstep:1640/10000 train_loss:4.4820 train_time:276714ms step_avg:169.76ms\nstep:1641/10000 train_loss:4.4852 train_time:276886ms step_avg:169.76ms\nstep:1642/10000 train_loss:4.4913 train_time:277055ms step_avg:169.76ms\nstep:1643/10000 train_loss:4.5723 train_time:277224ms step_avg:169.76ms\nstep:1644/10000 train_loss:4.5411 train_time:277392ms step_avg:169.76ms\nstep:1645/10000 train_loss:4.5145 train_time:277562ms step_avg:169.76ms\nstep:1646/10000 train_loss:4.5140 train_time:277731ms step_avg:169.76ms\nstep:1647/10000 train_loss:4.5088 train_time:277902ms step_avg:169.76ms\nstep:1648/10000 train_loss:4.5121 train_time:278070ms step_avg:169.76ms\nstep:1649/10000 train_loss:4.6107 train_time:278242ms step_avg:169.76ms\nstep:1650/10000 train_loss:4.9031 train_time:278411ms step_avg:169.76ms\nstep:1651/10000 train_loss:4.5572 train_time:278581ms step_avg:169.76ms\nstep:1652/10000 train_loss:4.5020 train_time:278749ms step_avg:169.76ms\nstep:1653/10000 train_loss:4.6673 train_time:278918ms step_avg:169.76ms\nstep:1654/10000 train_loss:4.3862 train_time:279091ms step_avg:169.76ms\nstep:1655/10000 train_loss:4.4755 train_time:279261ms step_avg:169.76ms\nstep:1656/10000 train_loss:4.5754 train_time:279433ms step_avg:169.76ms\nstep:1657/10000 train_loss:4.6078 train_time:279604ms step_avg:169.77ms\nstep:1658/10000 train_loss:4.7238 train_time:279774ms step_avg:169.77ms\nstep:1659/10000 train_loss:4.6405 train_time:279944ms step_avg:169.77ms\nstep:1660/10000 train_loss:4.4946 train_time:280116ms step_avg:169.77ms\nstep:1661/10000 train_loss:4.5018 train_time:280285ms step_avg:169.77ms\nstep:1662/10000 train_loss:4.4881 train_time:280453ms step_avg:169.77ms\nstep:1663/10000 train_loss:4.6393 train_time:280624ms step_avg:169.77ms\nstep:1664/10000 train_loss:4.4940 train_time:280793ms step_avg:169.77ms\nstep:1665/10000 train_loss:4.7017 train_time:280964ms step_avg:169.77ms\nstep:1666/10000 train_loss:4.5123 train_time:281134ms step_avg:169.77ms\nstep:1667/10000 train_loss:4.2404 train_time:281305ms step_avg:169.77ms\nstep:1668/10000 train_loss:4.6149 train_time:281474ms step_avg:169.77ms\nstep:1669/10000 train_loss:4.5792 train_time:281644ms step_avg:169.77ms\nstep:1670/10000 train_loss:4.6543 train_time:281813ms step_avg:169.77ms\nstep:1671/10000 train_loss:4.5167 train_time:281982ms step_avg:169.77ms\nstep:1672/10000 train_loss:4.3616 train_time:282153ms step_avg:169.77ms\nstep:1673/10000 train_loss:4.4552 train_time:282326ms step_avg:169.77ms\nstep:1674/10000 train_loss:4.4081 train_time:282493ms step_avg:169.77ms\nstep:1675/10000 train_loss:4.6004 train_time:282665ms step_avg:169.77ms\nstep:1676/10000 train_loss:4.6691 train_time:282839ms step_avg:169.77ms\nstep:1677/10000 train_loss:4.3095 train_time:283009ms step_avg:169.77ms\nstep:1678/10000 train_loss:4.5138 train_time:283174ms step_avg:169.77ms\nstep:1679/10000 train_loss:4.4697 train_time:283345ms step_avg:169.77ms\nstep:1680/10000 train_loss:4.6935 train_time:283514ms step_avg:169.77ms\nstep:1681/10000 train_loss:4.3893 train_time:283686ms step_avg:169.77ms\nstep:1682/10000 train_loss:4.4949 train_time:283854ms step_avg:169.77ms\nstep:1683/10000 train_loss:4.5481 train_time:284023ms step_avg:169.77ms\nstep:1684/10000 train_loss:4.5195 train_time:284193ms step_avg:169.77ms\nstep:1685/10000 train_loss:4.5644 train_time:284365ms step_avg:169.77ms\nstep:1686/10000 train_loss:4.3692 train_time:284537ms step_avg:169.77ms\nstep:1687/10000 train_loss:4.4558 train_time:284709ms step_avg:169.77ms\nstep:1688/10000 train_loss:4.6055 train_time:284879ms step_avg:169.77ms\nstep:1689/10000 train_loss:4.4844 train_time:285048ms step_avg:169.77ms\nstep:1690/10000 train_loss:4.4561 train_time:285219ms step_avg:169.77ms\nstep:1691/10000 train_loss:4.4346 train_time:285389ms step_avg:169.77ms\nstep:1692/10000 train_loss:4.3584 train_time:285558ms step_avg:169.77ms\nstep:1693/10000 train_loss:4.4794 train_time:285730ms step_avg:169.77ms\nstep:1694/10000 train_loss:4.5735 train_time:285898ms step_avg:169.77ms\nstep:1695/10000 train_loss:4.5058 train_time:286071ms step_avg:169.77ms\nstep:1696/10000 train_loss:4.4454 train_time:286241ms step_avg:169.78ms\nstep:1697/10000 train_loss:4.5398 train_time:286412ms step_avg:169.78ms\nstep:1698/10000 train_loss:4.4014 train_time:286582ms step_avg:169.78ms\nstep:1699/10000 train_loss:4.5761 train_time:286751ms step_avg:169.78ms\nstep:1700/10000 train_loss:4.3839 train_time:286920ms step_avg:169.78ms\nstep:1701/10000 train_loss:4.5544 train_time:287091ms step_avg:169.78ms\nstep:1702/10000 train_loss:4.5463 train_time:287260ms step_avg:169.78ms\nstep:1703/10000 train_loss:4.4311 train_time:287428ms step_avg:169.77ms\nstep:1704/10000 train_loss:4.5891 train_time:287596ms step_avg:169.77ms\nstep:1705/10000 train_loss:4.4613 train_time:287768ms step_avg:169.77ms\nstep:1706/10000 train_loss:4.4392 train_time:287943ms step_avg:169.78ms\nstep:1707/10000 train_loss:4.5846 train_time:288105ms step_avg:169.77ms\nstep:1708/10000 train_loss:4.5503 train_time:288273ms step_avg:169.77ms\nstep:1709/10000 train_loss:4.4072 train_time:288443ms step_avg:169.77ms\nstep:1710/10000 train_loss:4.5965 train_time:288612ms step_avg:169.77ms\nstep:1711/10000 train_loss:4.5361 train_time:288784ms step_avg:169.77ms\nstep:1712/10000 train_loss:4.4440 train_time:288956ms step_avg:169.77ms\nstep:1713/10000 train_loss:4.4759 train_time:289128ms step_avg:169.78ms\nstep:1714/10000 train_loss:4.5581 train_time:289300ms step_avg:169.78ms\nstep:1715/10000 train_loss:4.4797 train_time:289471ms step_avg:169.78ms\nstep:1716/10000 train_loss:4.5465 train_time:289643ms step_avg:169.78ms\nstep:1717/10000 train_loss:4.4167 train_time:289812ms step_avg:169.78ms\nstep:1718/10000 train_loss:4.4668 train_time:289984ms step_avg:169.78ms\nstep:1719/10000 train_loss:4.3790 train_time:290153ms step_avg:169.78ms\nstep:1720/10000 train_loss:4.4291 train_time:290322ms step_avg:169.78ms\nstep:1721/10000 train_loss:4.4470 train_time:290489ms step_avg:169.78ms\nstep:1722/10000 train_loss:4.4529 train_time:290660ms step_avg:169.78ms\nstep:1723/10000 train_loss:4.6372 train_time:290829ms step_avg:169.78ms\nstep:1724/10000 train_loss:4.3577 train_time:290997ms step_avg:169.78ms\nstep:1725/10000 train_loss:4.4149 train_time:291167ms step_avg:169.78ms\nstep:1726/10000 train_loss:4.4249 train_time:291333ms step_avg:169.77ms\nstep:1727/10000 train_loss:4.7181 train_time:291503ms step_avg:169.77ms\nstep:1728/10000 train_loss:4.5647 train_time:291671ms step_avg:169.77ms\nstep:1729/10000 train_loss:4.6974 train_time:291841ms step_avg:169.77ms\nstep:1730/10000 train_loss:4.4803 train_time:292013ms step_avg:169.77ms\nstep:1731/10000 train_loss:4.4817 train_time:292183ms step_avg:169.78ms\nstep:1732/10000 train_loss:4.5944 train_time:292353ms step_avg:169.78ms\nstep:1733/10000 train_loss:4.3637 train_time:292522ms step_avg:169.78ms\nstep:1734/10000 train_loss:4.5570 train_time:292692ms step_avg:169.77ms\nstep:1735/10000 train_loss:4.4784 train_time:292862ms step_avg:169.77ms\nstep:1736/10000 train_loss:4.5508 train_time:293032ms step_avg:169.78ms\nstep:1737/10000 train_loss:4.5428 train_time:293201ms step_avg:169.77ms\nstep:1738/10000 train_loss:4.4961 train_time:293369ms step_avg:169.77ms\nstep:1739/10000 train_loss:4.5281 train_time:293537ms step_avg:169.77ms\nstep:1740/10000 train_loss:4.4482 train_time:293711ms step_avg:169.77ms\nstep:1741/10000 train_loss:4.5219 train_time:293878ms step_avg:169.77ms\nstep:1742/10000 train_loss:4.5196 train_time:294049ms step_avg:169.77ms\nstep:1743/10000 train_loss:4.5855 train_time:294221ms step_avg:169.78ms\nstep:1744/10000 train_loss:4.5408 train_time:294391ms step_avg:169.78ms\nstep:1745/10000 train_loss:4.4550 train_time:294560ms step_avg:169.78ms\nstep:1746/10000 train_loss:4.5400 train_time:294730ms step_avg:169.78ms\nstep:1747/10000 train_loss:4.5930 train_time:294900ms step_avg:169.78ms\nstep:1748/10000 train_loss:4.5858 train_time:295077ms step_avg:169.78ms\nstep:1749/10000 train_loss:4.5217 train_time:295238ms step_avg:169.77ms\nstep:1750/10000 train_loss:4.4302 train_time:295409ms step_avg:169.78ms\nstep:1751/10000 train_loss:4.8375 train_time:295581ms step_avg:169.78ms\nstep:1752/10000 train_loss:4.5110 train_time:295754ms step_avg:169.78ms\nstep:1753/10000 train_loss:4.5743 train_time:295923ms step_avg:169.78ms\nstep:1754/10000 train_loss:4.4367 train_time:296091ms step_avg:169.78ms\nstep:1755/10000 train_loss:4.4967 train_time:296262ms step_avg:169.78ms\nstep:1756/10000 train_loss:4.5497 train_time:296433ms step_avg:169.78ms\nstep:1757/10000 train_loss:4.5232 train_time:296603ms step_avg:169.78ms\nstep:1758/10000 train_loss:4.3820 train_time:296770ms step_avg:169.78ms\nstep:1759/10000 train_loss:4.5488 train_time:296939ms step_avg:169.78ms\nstep:1760/10000 train_loss:4.4140 train_time:297110ms step_avg:169.78ms\nstep:1761/10000 train_loss:4.3854 train_time:297280ms step_avg:169.78ms\nstep:1762/10000 train_loss:4.5755 train_time:297452ms step_avg:169.78ms\nstep:1763/10000 train_loss:4.5139 train_time:297622ms step_avg:169.78ms\nstep:1764/10000 train_loss:4.4627 train_time:297792ms step_avg:169.78ms\nstep:1765/10000 train_loss:4.2270 train_time:297964ms step_avg:169.78ms\nstep:1766/10000 train_loss:4.4943 train_time:298136ms step_avg:169.78ms\nstep:1767/10000 train_loss:4.3991 train_time:298307ms step_avg:169.78ms\nstep:1768/10000 train_loss:4.4807 train_time:298476ms step_avg:169.78ms\nstep:1769/10000 train_loss:4.4310 train_time:298645ms step_avg:169.78ms\nstep:1770/10000 train_loss:4.4681 train_time:298814ms step_avg:169.78ms\nstep:1771/10000 train_loss:4.4178 train_time:298986ms step_avg:169.78ms\nstep:1772/10000 train_loss:4.3356 train_time:299155ms step_avg:169.78ms\nstep:1773/10000 train_loss:4.5321 train_time:299325ms step_avg:169.78ms\nstep:1774/10000 train_loss:4.3502 train_time:299493ms step_avg:169.78ms\nstep:1775/10000 train_loss:4.4119 train_time:299663ms step_avg:169.78ms\nstep:1776/10000 train_loss:4.4985 train_time:299832ms step_avg:169.78ms\nstep:1777/10000 train_loss:4.4509 train_time:300003ms step_avg:169.78ms\nstep:1778/10000 train_loss:4.6210 train_time:300173ms step_avg:169.78ms\nstep:1779/10000 train_loss:4.5299 train_time:300343ms step_avg:169.78ms\nstep:1780/10000 train_loss:4.6771 train_time:300510ms step_avg:169.78ms\nstep:1781/10000 train_loss:4.4504 train_time:300678ms step_avg:169.78ms\nstep:1782/10000 train_loss:4.5232 train_time:300849ms step_avg:169.78ms\nstep:1783/10000 train_loss:4.6237 train_time:301020ms step_avg:169.78ms\nstep:1784/10000 train_loss:4.3655 train_time:301191ms step_avg:169.78ms\nstep:1785/10000 train_loss:4.4494 train_time:301361ms step_avg:169.78ms\nstep:1786/10000 train_loss:4.5286 train_time:301533ms step_avg:169.78ms\nstep:1787/10000 train_loss:4.4787 train_time:301704ms step_avg:169.78ms\nstep:1788/10000 train_loss:4.5268 train_time:301872ms step_avg:169.78ms\nstep:1789/10000 train_loss:4.5624 train_time:302045ms step_avg:169.78ms\nstep:1790/10000 train_loss:4.4505 train_time:302213ms step_avg:169.78ms\nstep:1791/10000 train_loss:4.3820 train_time:302382ms step_avg:169.78ms\nstep:1792/10000 train_loss:4.5253 train_time:302550ms step_avg:169.78ms\nstep:1793/10000 train_loss:4.5589 train_time:302721ms step_avg:169.78ms\nstep:1794/10000 train_loss:4.5253 train_time:302889ms step_avg:169.78ms\nstep:1795/10000 train_loss:4.7333 train_time:303059ms step_avg:169.78ms\nstep:1796/10000 train_loss:4.3891 train_time:303232ms step_avg:169.78ms\nstep:1797/10000 train_loss:4.5453 train_time:303400ms step_avg:169.78ms\nstep:1798/10000 train_loss:4.3711 train_time:303571ms step_avg:169.78ms\nstep:1799/10000 train_loss:4.5468 train_time:303741ms step_avg:169.78ms\nstep:1800/10000 train_loss:4.4025 train_time:303910ms step_avg:169.78ms\nstep:1801/10000 train_loss:4.6047 train_time:304080ms step_avg:169.78ms\nstep:1802/10000 train_loss:4.4107 train_time:304249ms step_avg:169.78ms\nstep:1803/10000 train_loss:4.5545 train_time:304420ms step_avg:169.78ms\nstep:1804/10000 train_loss:4.4677 train_time:304593ms step_avg:169.78ms\nstep:1805/10000 train_loss:4.4664 train_time:304763ms step_avg:169.78ms\nstep:1806/10000 train_loss:4.3080 train_time:304933ms step_avg:169.78ms\nstep:1807/10000 train_loss:4.5134 train_time:305103ms step_avg:169.78ms\nstep:1808/10000 train_loss:4.7719 train_time:305271ms step_avg:169.78ms\nstep:1809/10000 train_loss:4.5430 train_time:305439ms step_avg:169.78ms\nstep:1810/10000 train_loss:4.4114 train_time:305608ms step_avg:169.78ms\nstep:1811/10000 train_loss:4.4570 train_time:305779ms step_avg:169.78ms\nstep:1812/10000 train_loss:4.6453 train_time:305948ms step_avg:169.78ms\nstep:1813/10000 train_loss:4.5200 train_time:306117ms step_avg:169.78ms\nstep:1814/10000 train_loss:4.3631 train_time:306288ms step_avg:169.78ms\nstep:1815/10000 train_loss:4.4661 train_time:306457ms step_avg:169.78ms\nstep:1816/10000 train_loss:4.4242 train_time:306627ms step_avg:169.78ms\nstep:1817/10000 train_loss:4.6914 train_time:306797ms step_avg:169.78ms\nstep:1818/10000 train_loss:4.3829 train_time:306969ms step_avg:169.78ms\nstep:1819/10000 train_loss:4.2473 train_time:307142ms step_avg:169.79ms\nstep:1820/10000 train_loss:4.3963 train_time:307313ms step_avg:169.79ms\nstep:1821/10000 train_loss:4.5820 train_time:307483ms step_avg:169.79ms\nstep:1822/10000 train_loss:4.5658 train_time:307659ms step_avg:169.79ms\nstep:1823/10000 train_loss:4.3448 train_time:307864ms step_avg:169.81ms\nstep:1824/10000 train_loss:4.5259 train_time:308016ms step_avg:169.80ms\nstep:1825/10000 train_loss:4.5516 train_time:308186ms step_avg:169.80ms\nstep:1826/10000 train_loss:4.4204 train_time:308355ms step_avg:169.80ms\nstep:1827/10000 train_loss:4.4334 train_time:308524ms step_avg:169.80ms\nstep:1828/10000 train_loss:4.5001 train_time:308693ms step_avg:169.80ms\nstep:1829/10000 train_loss:4.4884 train_time:308862ms step_avg:169.80ms\nstep:1830/10000 train_loss:4.5076 train_time:309031ms step_avg:169.80ms\nstep:1831/10000 train_loss:4.3647 train_time:309201ms step_avg:169.80ms\nstep:1832/10000 train_loss:4.4989 train_time:309373ms step_avg:169.80ms\nstep:1833/10000 train_loss:4.2314 train_time:309540ms step_avg:169.80ms\nstep:1834/10000 train_loss:4.5249 train_time:309710ms step_avg:169.80ms\nstep:1835/10000 train_loss:4.4084 train_time:309881ms step_avg:169.80ms\nstep:1836/10000 train_loss:4.5428 train_time:310052ms step_avg:169.80ms\nstep:1837/10000 train_loss:4.4380 train_time:310222ms step_avg:169.80ms\nstep:1838/10000 train_loss:4.3259 train_time:310391ms step_avg:169.80ms\nstep:1839/10000 train_loss:4.3369 train_time:310560ms step_avg:169.80ms\nstep:1840/10000 train_loss:4.4300 train_time:310735ms step_avg:169.80ms\nstep:1841/10000 train_loss:4.4321 train_time:310906ms step_avg:169.80ms\nstep:1842/10000 train_loss:4.4872 train_time:311073ms step_avg:169.80ms\nstep:1843/10000 train_loss:4.5010 train_time:311242ms step_avg:169.80ms\nstep:1844/10000 train_loss:4.4904 train_time:311412ms step_avg:169.80ms\nstep:1845/10000 train_loss:4.4246 train_time:311580ms step_avg:169.80ms\nstep:1846/10000 train_loss:4.4259 train_time:311750ms step_avg:169.80ms\nstep:1847/10000 train_loss:4.5353 train_time:311922ms step_avg:169.80ms\nstep:1848/10000 train_loss:4.4169 train_time:312090ms step_avg:169.80ms\nstep:1849/10000 train_loss:4.4164 train_time:312259ms step_avg:169.80ms\nstep:1850/10000 train_loss:4.3732 train_time:312430ms step_avg:169.80ms\nstep:1851/10000 train_loss:4.4275 train_time:312602ms step_avg:169.80ms\nstep:1852/10000 train_loss:4.2579 train_time:312772ms step_avg:169.80ms\nstep:1853/10000 train_loss:4.3109 train_time:312943ms step_avg:169.80ms\nstep:1854/10000 train_loss:4.4907 train_time:313111ms step_avg:169.80ms\nstep:1855/10000 train_loss:4.3871 train_time:313282ms step_avg:169.80ms\nstep:1856/10000 train_loss:4.4287 train_time:313450ms step_avg:169.80ms\nstep:1857/10000 train_loss:4.5027 train_time:313620ms step_avg:169.80ms\nstep:1858/10000 train_loss:4.4048 train_time:313789ms step_avg:169.80ms\nstep:1859/10000 train_loss:4.5310 train_time:313958ms step_avg:169.80ms\nstep:1860/10000 train_loss:4.5246 train_time:314128ms step_avg:169.80ms\nstep:1861/10000 train_loss:4.4927 train_time:314297ms step_avg:169.80ms\nstep:1862/10000 train_loss:4.4017 train_time:314466ms step_avg:169.80ms\nstep:1863/10000 train_loss:4.4346 train_time:314634ms step_avg:169.80ms\nstep:1864/10000 train_loss:4.3889 train_time:314804ms step_avg:169.80ms\nstep:1865/10000 train_loss:4.4934 train_time:314973ms step_avg:169.80ms\nstep:1866/10000 train_loss:4.5335 train_time:315147ms step_avg:169.80ms\nstep:1867/10000 train_loss:4.3961 train_time:315317ms step_avg:169.80ms\nstep:1868/10000 train_loss:4.4187 train_time:315489ms step_avg:169.80ms\nstep:1869/10000 train_loss:4.5560 train_time:315657ms step_avg:169.80ms\nstep:1870/10000 train_loss:4.5358 train_time:315824ms step_avg:169.80ms\nstep:1871/10000 train_loss:4.3213 train_time:315993ms step_avg:169.80ms\nstep:1872/10000 train_loss:4.3806 train_time:316164ms step_avg:169.80ms\nstep:1873/10000 train_loss:4.4029 train_time:316336ms step_avg:169.80ms\nstep:1874/10000 train_loss:4.4952 train_time:316509ms step_avg:169.80ms\nstep:1875/10000 train_loss:4.3275 train_time:316677ms step_avg:169.80ms\nstep:1876/10000 train_loss:4.5879 train_time:316846ms step_avg:169.80ms\nstep:1877/10000 train_loss:4.4575 train_time:317015ms step_avg:169.80ms\nstep:1878/10000 train_loss:4.4069 train_time:317187ms step_avg:169.80ms\nstep:1879/10000 train_loss:4.4240 train_time:317356ms step_avg:169.80ms\nstep:1880/10000 train_loss:4.4650 train_time:317529ms step_avg:169.80ms\nstep:1881/10000 train_loss:4.4661 train_time:317700ms step_avg:169.80ms\nstep:1882/10000 train_loss:4.4913 train_time:317869ms step_avg:169.80ms\nstep:1883/10000 train_loss:4.5733 train_time:318038ms step_avg:169.80ms\nstep:1884/10000 train_loss:4.2785 train_time:318207ms step_avg:169.80ms\nstep:1885/10000 train_loss:4.2412 train_time:318375ms step_avg:169.80ms\nstep:1886/10000 train_loss:4.5204 train_time:318545ms step_avg:169.80ms\nstep:1887/10000 train_loss:4.6858 train_time:318713ms step_avg:169.80ms\nstep:1888/10000 train_loss:4.4988 train_time:318883ms step_avg:169.80ms\nstep:1889/10000 train_loss:4.4322 train_time:319053ms step_avg:169.80ms\nstep:1890/10000 train_loss:4.4483 train_time:319223ms step_avg:169.80ms\nstep:1891/10000 train_loss:4.3984 train_time:319392ms step_avg:169.80ms\nstep:1892/10000 train_loss:4.3879 train_time:319562ms step_avg:169.80ms\nstep:1893/10000 train_loss:4.4672 train_time:319731ms step_avg:169.80ms\nstep:1894/10000 train_loss:4.4109 train_time:319900ms step_avg:169.80ms\nstep:1895/10000 train_loss:4.4733 train_time:320070ms step_avg:169.80ms\nstep:1896/10000 train_loss:4.5067 train_time:320239ms step_avg:169.80ms\nstep:1897/10000 train_loss:4.3780 train_time:320410ms step_avg:169.80ms\nstep:1898/10000 train_loss:4.2889 train_time:320578ms step_avg:169.80ms\nstep:1899/10000 train_loss:4.5152 train_time:320746ms step_avg:169.80ms\nstep:1900/10000 train_loss:4.5072 train_time:320914ms step_avg:169.80ms\nstep:1901/10000 train_loss:4.4450 train_time:321084ms step_avg:169.80ms\nstep:1902/10000 train_loss:4.7274 train_time:321252ms step_avg:169.79ms\nstep:1903/10000 train_loss:4.3467 train_time:321422ms step_avg:169.79ms\nstep:1904/10000 train_loss:4.6822 train_time:321591ms step_avg:169.79ms\nstep:1905/10000 train_loss:4.5096 train_time:321761ms step_avg:169.79ms\nstep:1906/10000 train_loss:4.4712 train_time:321931ms step_avg:169.79ms\nstep:1907/10000 train_loss:4.6405 train_time:322100ms step_avg:169.79ms\nstep:1908/10000 train_loss:4.4909 train_time:322269ms step_avg:169.79ms\nstep:1909/10000 train_loss:4.1845 train_time:322439ms step_avg:169.79ms\nstep:1910/10000 train_loss:4.2932 train_time:322610ms step_avg:169.79ms\nstep:1911/10000 train_loss:4.3714 train_time:322780ms step_avg:169.79ms\nstep:1912/10000 train_loss:4.5078 train_time:322951ms step_avg:169.80ms\nstep:1913/10000 train_loss:4.3901 train_time:323121ms step_avg:169.80ms\nstep:1914/10000 train_loss:4.4574 train_time:323290ms step_avg:169.80ms\nstep:1915/10000 train_loss:4.3891 train_time:323462ms step_avg:169.80ms\nstep:1916/10000 train_loss:4.4143 train_time:323633ms step_avg:169.80ms\nstep:1917/10000 train_loss:4.4299 train_time:323805ms step_avg:169.80ms\nstep:1918/10000 train_loss:4.4421 train_time:323976ms step_avg:169.80ms\nstep:1919/10000 train_loss:4.3667 train_time:324147ms step_avg:169.80ms\nstep:1920/10000 train_loss:4.3192 train_time:324315ms step_avg:169.80ms\nstep:1921/10000 train_loss:4.5361 train_time:324486ms step_avg:169.80ms\nstep:1922/10000 train_loss:4.5830 train_time:324655ms step_avg:169.80ms\nstep:1923/10000 train_loss:4.4014 train_time:324825ms step_avg:169.80ms\nstep:1924/10000 train_loss:4.4296 train_time:324995ms step_avg:169.80ms\nstep:1925/10000 train_loss:4.5701 train_time:325165ms step_avg:169.80ms\nstep:1926/10000 train_loss:4.4137 train_time:325335ms step_avg:169.80ms\nstep:1927/10000 train_loss:4.3337 train_time:325504ms step_avg:169.80ms\nstep:1928/10000 train_loss:4.5519 train_time:325671ms step_avg:169.80ms\nstep:1929/10000 train_loss:4.3566 train_time:325842ms step_avg:169.80ms\nstep:1930/10000 train_loss:4.2152 train_time:326013ms step_avg:169.80ms\nstep:1931/10000 train_loss:4.3327 train_time:326183ms step_avg:169.80ms\nstep:1932/10000 train_loss:4.4085 train_time:326354ms step_avg:169.80ms\nstep:1933/10000 train_loss:4.2704 train_time:326523ms step_avg:169.80ms\nstep:1934/10000 train_loss:4.4735 train_time:326694ms step_avg:169.80ms\nstep:1935/10000 train_loss:4.6105 train_time:326865ms step_avg:169.80ms\nstep:1936/10000 train_loss:4.4812 train_time:327034ms step_avg:169.80ms\nstep:1937/10000 train_loss:4.3882 train_time:327205ms step_avg:169.80ms\nstep:1938/10000 train_loss:4.3508 train_time:327374ms step_avg:169.80ms\nstep:1939/10000 train_loss:4.4597 train_time:327544ms step_avg:169.80ms\nstep:1940/10000 train_loss:4.4332 train_time:327711ms step_avg:169.80ms\nstep:1941/10000 train_loss:4.4832 train_time:327882ms step_avg:169.80ms\nstep:1942/10000 train_loss:4.3843 train_time:328052ms step_avg:169.80ms\nstep:1943/10000 train_loss:4.3418 train_time:328221ms step_avg:169.80ms\nstep:1944/10000 train_loss:4.2850 train_time:328390ms step_avg:169.80ms\nstep:1945/10000 train_loss:4.4770 train_time:328560ms step_avg:169.80ms\nstep:1946/10000 train_loss:4.3185 train_time:328727ms step_avg:169.80ms\nstep:1947/10000 train_loss:4.4320 train_time:328894ms step_avg:169.80ms\nstep:1948/10000 train_loss:4.4356 train_time:329067ms step_avg:169.80ms\nstep:1949/10000 train_loss:4.3764 train_time:329237ms step_avg:169.80ms\nstep:1950/10000 train_loss:4.3285 train_time:329409ms step_avg:169.80ms\nstep:1951/10000 train_loss:4.3670 train_time:329578ms step_avg:169.80ms\nstep:1952/10000 train_loss:4.4444 train_time:329751ms step_avg:169.80ms\nstep:1953/10000 train_loss:4.3394 train_time:329921ms step_avg:169.80ms\nstep:1954/10000 train_loss:4.5496 train_time:330094ms step_avg:169.80ms\nstep:1955/10000 train_loss:4.3769 train_time:330264ms step_avg:169.80ms\nstep:1956/10000 train_loss:4.3955 train_time:330433ms step_avg:169.80ms\nstep:1957/10000 train_loss:4.3462 train_time:330603ms step_avg:169.80ms\nstep:1958/10000 train_loss:4.3990 train_time:330770ms step_avg:169.80ms\nstep:1959/10000 train_loss:4.4402 train_time:330939ms step_avg:169.80ms\nstep:1960/10000 train_loss:4.5852 train_time:331110ms step_avg:169.80ms\nstep:1961/10000 train_loss:4.3515 train_time:331278ms step_avg:169.80ms\nstep:1962/10000 train_loss:4.3026 train_time:331449ms step_avg:169.80ms\nstep:1963/10000 train_loss:4.2515 train_time:331620ms step_avg:169.80ms\nstep:1964/10000 train_loss:4.3854 train_time:331795ms step_avg:169.80ms\nstep:1965/10000 train_loss:4.4018 train_time:331965ms step_avg:169.80ms\nstep:1966/10000 train_loss:4.3061 train_time:332134ms step_avg:169.80ms\nstep:1967/10000 train_loss:4.6359 train_time:332305ms step_avg:169.80ms\nstep:1968/10000 train_loss:4.3362 train_time:332477ms step_avg:169.80ms\nstep:1969/10000 train_loss:4.2999 train_time:332648ms step_avg:169.80ms\nstep:1970/10000 train_loss:4.4243 train_time:332817ms step_avg:169.80ms\nstep:1971/10000 train_loss:4.4691 train_time:332990ms step_avg:169.81ms\nstep:1972/10000 train_loss:4.4742 train_time:333160ms step_avg:169.81ms\nstep:1973/10000 train_loss:4.5359 train_time:333330ms step_avg:169.81ms\nstep:1974/10000 train_loss:4.2536 train_time:333499ms step_avg:169.81ms\nstep:1975/10000 train_loss:4.3303 train_time:333671ms step_avg:169.81ms\nstep:1976/10000 train_loss:4.2411 train_time:333840ms step_avg:169.81ms\nstep:1977/10000 train_loss:4.4849 train_time:334010ms step_avg:169.81ms\nstep:1978/10000 train_loss:4.4011 train_time:334183ms step_avg:169.81ms\nstep:1979/10000 train_loss:4.3487 train_time:334352ms step_avg:169.81ms\nstep:1980/10000 train_loss:4.4215 train_time:334521ms step_avg:169.81ms\nstep:1981/10000 train_loss:4.4106 train_time:334693ms step_avg:169.81ms\nstep:1982/10000 train_loss:4.3038 train_time:334863ms step_avg:169.81ms\nstep:1983/10000 train_loss:4.4291 train_time:335031ms step_avg:169.81ms\nstep:1984/10000 train_loss:4.5829 train_time:335201ms step_avg:169.81ms\nstep:1985/10000 train_loss:4.4733 train_time:335371ms step_avg:169.81ms\nstep:1986/10000 train_loss:4.2927 train_time:335539ms step_avg:169.81ms\nstep:1987/10000 train_loss:4.4158 train_time:335709ms step_avg:169.81ms\nstep:1988/10000 train_loss:4.3961 train_time:335879ms step_avg:169.81ms\nstep:1989/10000 train_loss:4.3881 train_time:336050ms step_avg:169.81ms\nstep:1990/10000 train_loss:4.4168 train_time:336221ms step_avg:169.81ms\nstep:1991/10000 train_loss:4.4033 train_time:336392ms step_avg:169.81ms\nstep:1992/10000 train_loss:4.3490 train_time:336558ms step_avg:169.81ms\nstep:1993/10000 train_loss:4.8295 train_time:336729ms step_avg:169.81ms\nstep:1994/10000 train_loss:4.5014 train_time:336899ms step_avg:169.81ms\nstep:1995/10000 train_loss:4.3498 train_time:337072ms step_avg:169.81ms\nstep:1996/10000 train_loss:4.3566 train_time:337242ms step_avg:169.81ms\nstep:1997/10000 train_loss:4.3048 train_time:337414ms step_avg:169.81ms\nstep:1998/10000 train_loss:4.5545 train_time:337583ms step_avg:169.81ms\nstep:1999/10000 train_loss:4.5023 train_time:337753ms step_avg:169.81ms\nstep:2000/10000 train_loss:4.4739 train_time:337925ms step_avg:169.81ms\nstep:2001/10000 train_loss:4.4478 train_time:338095ms step_avg:169.81ms\nstep:2002/10000 train_loss:4.4377 train_time:338264ms step_avg:169.81ms\nstep:2003/10000 train_loss:4.4417 train_time:338433ms step_avg:169.81ms\nstep:2004/10000 train_loss:4.4117 train_time:338603ms step_avg:169.81ms\nstep:2005/10000 train_loss:4.3904 train_time:338771ms step_avg:169.81ms\nstep:2006/10000 train_loss:4.4411 train_time:338942ms step_avg:169.81ms\nstep:2007/10000 train_loss:4.3447 train_time:339113ms step_avg:169.81ms\nstep:2008/10000 train_loss:4.2840 train_time:339284ms step_avg:169.81ms\nstep:2009/10000 train_loss:5.0017 train_time:339453ms step_avg:169.81ms\nstep:2010/10000 train_loss:4.4096 train_time:339624ms step_avg:169.81ms\nstep:2011/10000 train_loss:4.4278 train_time:339795ms step_avg:169.81ms\nstep:2012/10000 train_loss:4.3878 train_time:339966ms step_avg:169.81ms\nstep:2013/10000 train_loss:4.4025 train_time:340136ms step_avg:169.81ms\nstep:2014/10000 train_loss:4.4302 train_time:340308ms step_avg:169.81ms\nstep:2015/10000 train_loss:4.2872 train_time:340477ms step_avg:169.81ms\nstep:2016/10000 train_loss:4.2901 train_time:340648ms step_avg:169.81ms\nstep:2017/10000 train_loss:4.2884 train_time:340817ms step_avg:169.81ms\nstep:2018/10000 train_loss:4.4241 train_time:340986ms step_avg:169.81ms\nstep:2019/10000 train_loss:4.4779 train_time:341154ms step_avg:169.81ms\nstep:2020/10000 train_loss:4.6716 train_time:341322ms step_avg:169.81ms\nstep:2021/10000 train_loss:4.2195 train_time:341490ms step_avg:169.81ms\nstep:2022/10000 train_loss:4.5255 train_time:341660ms step_avg:169.81ms\nstep:2023/10000 train_loss:4.2217 train_time:341831ms step_avg:169.81ms\nstep:2024/10000 train_loss:4.3835 train_time:342002ms step_avg:169.81ms\nstep:2025/10000 train_loss:4.3459 train_time:342175ms step_avg:169.81ms\nstep:2026/10000 train_loss:4.2724 train_time:342347ms step_avg:169.82ms\nstep:2027/10000 train_loss:4.3261 train_time:342515ms step_avg:169.81ms\nstep:2028/10000 train_loss:4.4279 train_time:342686ms step_avg:169.81ms\nstep:2029/10000 train_loss:4.2105 train_time:342856ms step_avg:169.81ms\nstep:2030/10000 train_loss:4.4241 train_time:343027ms step_avg:169.82ms\nstep:2031/10000 train_loss:4.4319 train_time:343196ms step_avg:169.81ms\nstep:2032/10000 train_loss:4.6037 train_time:343365ms step_avg:169.81ms\nstep:2033/10000 train_loss:4.5118 train_time:343536ms step_avg:169.82ms\nstep:2034/10000 train_loss:4.5070 train_time:343707ms step_avg:169.82ms\nstep:2035/10000 train_loss:4.3087 train_time:343875ms step_avg:169.81ms\nstep:2036/10000 train_loss:4.3011 train_time:344046ms step_avg:169.82ms\nstep:2037/10000 train_loss:4.3967 train_time:344216ms step_avg:169.82ms\nstep:2038/10000 train_loss:4.4469 train_time:344387ms step_avg:169.82ms\nstep:2039/10000 train_loss:4.5045 train_time:344557ms step_avg:169.82ms\nstep:2040/10000 train_loss:4.4138 train_time:344728ms step_avg:169.82ms\nstep:2041/10000 train_loss:4.3766 train_time:344897ms step_avg:169.82ms\nstep:2042/10000 train_loss:4.2664 train_time:345070ms step_avg:169.82ms\nstep:2043/10000 train_loss:4.3489 train_time:345238ms step_avg:169.82ms\nstep:2044/10000 train_loss:4.4189 train_time:345406ms step_avg:169.82ms\nstep:2045/10000 train_loss:4.3656 train_time:345574ms step_avg:169.82ms\nstep:2046/10000 train_loss:4.4547 train_time:345746ms step_avg:169.82ms\nstep:2047/10000 train_loss:4.3759 train_time:345914ms step_avg:169.82ms\nstep:2048/10000 train_loss:4.2454 train_time:346085ms step_avg:169.82ms\nstep:2049/10000 train_loss:4.3210 train_time:346254ms step_avg:169.82ms\nstep:2050/10000 train_loss:4.3789 train_time:346424ms step_avg:169.82ms\nstep:2051/10000 train_loss:4.3642 train_time:346594ms step_avg:169.82ms\nstep:2052/10000 train_loss:4.2379 train_time:346767ms step_avg:169.82ms\nstep:2053/10000 train_loss:4.4404 train_time:346937ms step_avg:169.82ms\nstep:2054/10000 train_loss:4.3697 train_time:347110ms step_avg:169.82ms\nstep:2055/10000 train_loss:4.3366 train_time:347279ms step_avg:169.82ms\nstep:2056/10000 train_loss:4.4595 train_time:347448ms step_avg:169.82ms\nstep:2057/10000 train_loss:4.3380 train_time:347618ms step_avg:169.82ms\nstep:2058/10000 train_loss:4.4296 train_time:347791ms step_avg:169.82ms\nstep:2059/10000 train_loss:4.3308 train_time:347962ms step_avg:169.82ms\nstep:2060/10000 train_loss:4.6292 train_time:348132ms step_avg:169.82ms\nstep:2061/10000 train_loss:4.5019 train_time:348302ms step_avg:169.82ms\nstep:2062/10000 train_loss:4.4592 train_time:348473ms step_avg:169.82ms\nstep:2063/10000 train_loss:4.3891 train_time:348643ms step_avg:169.82ms\nstep:2064/10000 train_loss:4.4422 train_time:348813ms step_avg:169.82ms\nstep:2065/10000 train_loss:4.4442 train_time:348985ms step_avg:169.82ms\nstep:2066/10000 train_loss:4.7870 train_time:349154ms step_avg:169.82ms\nstep:2067/10000 train_loss:4.5374 train_time:349325ms step_avg:169.82ms\nstep:2068/10000 train_loss:4.2816 train_time:349494ms step_avg:169.82ms\nstep:2069/10000 train_loss:4.3991 train_time:349665ms step_avg:169.82ms\nstep:2070/10000 train_loss:4.3641 train_time:349833ms step_avg:169.82ms\nstep:2071/10000 train_loss:4.3881 train_time:350002ms step_avg:169.82ms\nstep:2072/10000 train_loss:4.4700 train_time:350170ms step_avg:169.82ms\nstep:2073/10000 train_loss:4.4394 train_time:350341ms step_avg:169.82ms\nstep:2074/10000 train_loss:4.4805 train_time:350512ms step_avg:169.82ms\nstep:2075/10000 train_loss:4.2363 train_time:350680ms step_avg:169.82ms\nstep:2076/10000 train_loss:4.2121 train_time:350852ms step_avg:169.82ms\nstep:2077/10000 train_loss:4.4971 train_time:351022ms step_avg:169.82ms\nstep:2078/10000 train_loss:4.4221 train_time:351191ms step_avg:169.82ms\nstep:2079/10000 train_loss:4.3213 train_time:351363ms step_avg:169.82ms\nstep:2080/10000 train_loss:4.4411 train_time:351533ms step_avg:169.82ms\nstep:2081/10000 train_loss:4.3606 train_time:351703ms step_avg:169.82ms\nstep:2082/10000 train_loss:4.3899 train_time:351871ms step_avg:169.82ms\nstep:2083/10000 train_loss:4.3429 train_time:352041ms step_avg:169.82ms\nstep:2084/10000 train_loss:4.3167 train_time:352209ms step_avg:169.82ms\nstep:2085/10000 train_loss:4.3527 train_time:352376ms step_avg:169.82ms\nstep:2086/10000 train_loss:4.2769 train_time:352545ms step_avg:169.82ms\nstep:2087/10000 train_loss:4.4060 train_time:352714ms step_avg:169.82ms\nstep:2088/10000 train_loss:4.1992 train_time:352882ms step_avg:169.82ms\nstep:2089/10000 train_loss:4.4074 train_time:353053ms step_avg:169.82ms\nstep:2090/10000 train_loss:4.3113 train_time:353224ms step_avg:169.82ms\nstep:2091/10000 train_loss:4.3879 train_time:353396ms step_avg:169.82ms\nstep:2092/10000 train_loss:4.4571 train_time:353572ms step_avg:169.82ms\nstep:2093/10000 train_loss:4.4918 train_time:353738ms step_avg:169.82ms\nstep:2094/10000 train_loss:4.9065 train_time:353911ms step_avg:169.82ms\nstep:2095/10000 train_loss:4.2554 train_time:354079ms step_avg:169.82ms\nstep:2096/10000 train_loss:4.2551 train_time:354250ms step_avg:169.82ms\nstep:2097/10000 train_loss:4.1848 train_time:354417ms step_avg:169.82ms\nstep:2098/10000 train_loss:4.4245 train_time:354590ms step_avg:169.82ms\nstep:2099/10000 train_loss:4.3350 train_time:354758ms step_avg:169.82ms\nstep:2100/10000 train_loss:4.3234 train_time:354929ms step_avg:169.82ms\nstep:2101/10000 train_loss:4.3424 train_time:355098ms step_avg:169.82ms\nstep:2102/10000 train_loss:4.3728 train_time:355270ms step_avg:169.82ms\nstep:2103/10000 train_loss:4.3251 train_time:355440ms step_avg:169.82ms\nstep:2104/10000 train_loss:4.3545 train_time:355611ms step_avg:169.82ms\nstep:2105/10000 train_loss:4.1608 train_time:355780ms step_avg:169.82ms\nstep:2106/10000 train_loss:4.3608 train_time:355951ms step_avg:169.82ms\nstep:2107/10000 train_loss:4.5217 train_time:356120ms step_avg:169.82ms\nstep:2108/10000 train_loss:4.5921 train_time:356290ms step_avg:169.82ms\nstep:2109/10000 train_loss:4.2593 train_time:356459ms step_avg:169.82ms\nstep:2110/10000 train_loss:4.2995 train_time:356629ms step_avg:169.82ms\nstep:2111/10000 train_loss:4.3587 train_time:356801ms step_avg:169.82ms\nstep:2112/10000 train_loss:4.3149 train_time:356971ms step_avg:169.82ms\nstep:2113/10000 train_loss:4.3815 train_time:357142ms step_avg:169.82ms\nstep:2114/10000 train_loss:4.2894 train_time:357312ms step_avg:169.83ms\nstep:2115/10000 train_loss:4.4580 train_time:357483ms step_avg:169.83ms\nstep:2116/10000 train_loss:4.2950 train_time:357650ms step_avg:169.82ms\nstep:2117/10000 train_loss:4.3154 train_time:357819ms step_avg:169.82ms\nstep:2118/10000 train_loss:4.4604 train_time:357987ms step_avg:169.82ms\nstep:2119/10000 train_loss:4.4650 train_time:358156ms step_avg:169.82ms\nstep:2120/10000 train_loss:4.3393 train_time:358325ms step_avg:169.82ms\nstep:2121/10000 train_loss:4.3758 train_time:358494ms step_avg:169.82ms\nstep:2122/10000 train_loss:4.2990 train_time:358663ms step_avg:169.82ms\nstep:2123/10000 train_loss:4.3587 train_time:358832ms step_avg:169.82ms\nstep:2124/10000 train_loss:4.4525 train_time:359003ms step_avg:169.82ms\nstep:2125/10000 train_loss:4.2587 train_time:359170ms step_avg:169.82ms\nstep:2126/10000 train_loss:4.5671 train_time:359341ms step_avg:169.82ms\nstep:2127/10000 train_loss:4.2331 train_time:359511ms step_avg:169.82ms\nstep:2128/10000 train_loss:4.3538 train_time:359683ms step_avg:169.82ms\nstep:2129/10000 train_loss:4.1594 train_time:359851ms step_avg:169.82ms\nstep:2130/10000 train_loss:4.3684 train_time:360024ms step_avg:169.82ms\nstep:2131/10000 train_loss:4.4676 train_time:360193ms step_avg:169.82ms\nstep:2132/10000 train_loss:4.3717 train_time:360365ms step_avg:169.82ms\nstep:2133/10000 train_loss:4.1512 train_time:360534ms step_avg:169.82ms\nstep:2134/10000 train_loss:4.3449 train_time:360702ms step_avg:169.82ms\nstep:2135/10000 train_loss:4.4205 train_time:360873ms step_avg:169.82ms\nstep:2136/10000 train_loss:4.4552 train_time:361045ms step_avg:169.82ms\nstep:2137/10000 train_loss:4.3628 train_time:361218ms step_avg:169.83ms\nstep:2138/10000 train_loss:4.3454 train_time:361382ms step_avg:169.82ms\nstep:2139/10000 train_loss:4.4078 train_time:361549ms step_avg:169.82ms\nstep:2140/10000 train_loss:4.4307 train_time:361720ms step_avg:169.82ms\nstep:2141/10000 train_loss:4.3641 train_time:361891ms step_avg:169.82ms\nstep:2142/10000 train_loss:4.2909 train_time:362063ms step_avg:169.82ms\nstep:2143/10000 train_loss:4.3806 train_time:362232ms step_avg:169.82ms\nstep:2144/10000 train_loss:4.3524 train_time:362405ms step_avg:169.82ms\nstep:2145/10000 train_loss:4.6370 train_time:362576ms step_avg:169.82ms\nstep:2146/10000 train_loss:4.3893 train_time:362745ms step_avg:169.82ms\nstep:2147/10000 train_loss:4.2291 train_time:362914ms step_avg:169.82ms\nstep:2148/10000 train_loss:4.3690 train_time:363085ms step_avg:169.82ms\nstep:2149/10000 train_loss:4.2100 train_time:363255ms step_avg:169.82ms\nstep:2150/10000 train_loss:4.3794 train_time:363427ms step_avg:169.83ms\nstep:2151/10000 train_loss:4.2000 train_time:363597ms step_avg:169.83ms\nstep:2152/10000 train_loss:4.2593 train_time:363768ms step_avg:169.83ms\nstep:2153/10000 train_loss:4.4306 train_time:363938ms step_avg:169.83ms\nstep:2154/10000 train_loss:4.3122 train_time:364111ms step_avg:169.83ms\nstep:2155/10000 train_loss:4.2171 train_time:364279ms step_avg:169.83ms\nstep:2156/10000 train_loss:4.2249 train_time:364451ms step_avg:169.83ms\nstep:2157/10000 train_loss:4.4110 train_time:364622ms step_avg:169.83ms\nstep:2158/10000 train_loss:4.7756 train_time:364789ms step_avg:169.83ms\nstep:2159/10000 train_loss:4.2718 train_time:364960ms step_avg:169.83ms\nstep:2160/10000 train_loss:4.3038 train_time:365135ms step_avg:169.83ms\nstep:2161/10000 train_loss:4.4047 train_time:365304ms step_avg:169.83ms\nstep:2162/10000 train_loss:4.3500 train_time:365472ms step_avg:169.83ms\nstep:2163/10000 train_loss:4.4648 train_time:365642ms step_avg:169.83ms\nstep:2164/10000 train_loss:4.3168 train_time:365811ms step_avg:169.83ms\nstep:2165/10000 train_loss:4.0744 train_time:365981ms step_avg:169.83ms\nstep:2166/10000 train_loss:4.4300 train_time:366152ms step_avg:169.83ms\nstep:2167/10000 train_loss:4.8516 train_time:366323ms step_avg:169.83ms\nstep:2168/10000 train_loss:4.2854 train_time:366491ms step_avg:169.83ms\nstep:2169/10000 train_loss:4.3582 train_time:366662ms step_avg:169.83ms\nstep:2170/10000 train_loss:4.2990 train_time:366829ms step_avg:169.83ms\nstep:2171/10000 train_loss:4.2240 train_time:367000ms step_avg:169.83ms\nstep:2172/10000 train_loss:4.3774 train_time:367169ms step_avg:169.83ms\nstep:2173/10000 train_loss:4.4981 train_time:367339ms step_avg:169.83ms\nstep:2174/10000 train_loss:4.2909 train_time:367510ms step_avg:169.83ms\nstep:2175/10000 train_loss:4.4389 train_time:367680ms step_avg:169.83ms\nstep:2176/10000 train_loss:4.3520 train_time:367848ms step_avg:169.83ms\nstep:2177/10000 train_loss:4.4037 train_time:368016ms step_avg:169.83ms\nstep:2178/10000 train_loss:4.3926 train_time:368185ms step_avg:169.83ms\nstep:2179/10000 train_loss:4.2796 train_time:368353ms step_avg:169.83ms\nstep:2180/10000 train_loss:4.4017 train_time:368523ms step_avg:169.83ms\nstep:2181/10000 train_loss:4.5076 train_time:368692ms step_avg:169.83ms\nstep:2182/10000 train_loss:4.2789 train_time:368862ms step_avg:169.83ms\nstep:2183/10000 train_loss:4.2094 train_time:369032ms step_avg:169.83ms\nstep:2184/10000 train_loss:4.3954 train_time:369202ms step_avg:169.83ms\nstep:2185/10000 train_loss:4.3421 train_time:369373ms step_avg:169.83ms\nstep:2186/10000 train_loss:4.3312 train_time:369545ms step_avg:169.83ms\nstep:2187/10000 train_loss:4.2695 train_time:369713ms step_avg:169.83ms\nstep:2188/10000 train_loss:4.2179 train_time:369881ms step_avg:169.83ms\nstep:2189/10000 train_loss:4.3330 train_time:370052ms step_avg:169.83ms\nstep:2190/10000 train_loss:4.3238 train_time:370223ms step_avg:169.83ms\nstep:2191/10000 train_loss:4.3981 train_time:370393ms step_avg:169.83ms\nstep:2192/10000 train_loss:4.4864 train_time:370565ms step_avg:169.83ms\nstep:2193/10000 train_loss:4.2509 train_time:370734ms step_avg:169.83ms\nstep:2194/10000 train_loss:4.3190 train_time:370905ms step_avg:169.83ms\nstep:2195/10000 train_loss:4.1726 train_time:371073ms step_avg:169.83ms\nstep:2196/10000 train_loss:4.0254 train_time:371241ms step_avg:169.83ms\nstep:2197/10000 train_loss:4.3254 train_time:371411ms step_avg:169.83ms\nstep:2198/10000 train_loss:4.3714 train_time:371581ms step_avg:169.83ms\nstep:2199/10000 train_loss:4.2126 train_time:371750ms step_avg:169.83ms\nstep:2200/10000 train_loss:4.4300 train_time:371920ms step_avg:169.83ms\nstep:2201/10000 train_loss:4.1899 train_time:372091ms step_avg:169.83ms\nstep:2202/10000 train_loss:4.2420 train_time:372259ms step_avg:169.83ms\nstep:2203/10000 train_loss:4.2735 train_time:372430ms step_avg:169.83ms\nstep:2204/10000 train_loss:4.3540 train_time:372599ms step_avg:169.83ms\nstep:2205/10000 train_loss:4.3545 train_time:372770ms step_avg:169.83ms\nstep:2206/10000 train_loss:4.2873 train_time:372940ms step_avg:169.83ms\nstep:2207/10000 train_loss:4.2406 train_time:373110ms step_avg:169.83ms\nstep:2208/10000 train_loss:4.2668 train_time:373286ms step_avg:169.83ms\nstep:2209/10000 train_loss:4.2793 train_time:373457ms step_avg:169.83ms\nstep:2210/10000 train_loss:4.5954 train_time:373629ms step_avg:169.83ms\nstep:2211/10000 train_loss:4.3263 train_time:373797ms step_avg:169.83ms\nstep:2212/10000 train_loss:4.4654 train_time:373965ms step_avg:169.83ms\nstep:2213/10000 train_loss:4.3989 train_time:374132ms step_avg:169.83ms\nstep:2214/10000 train_loss:4.3948 train_time:374307ms step_avg:169.83ms\nstep:2215/10000 train_loss:4.3648 train_time:374475ms step_avg:169.83ms\nstep:2216/10000 train_loss:4.3424 train_time:374644ms step_avg:169.83ms\nstep:2217/10000 train_loss:4.2093 train_time:374815ms step_avg:169.83ms\nstep:2218/10000 train_loss:4.3733 train_time:374986ms step_avg:169.83ms\nstep:2219/10000 train_loss:4.3027 train_time:375153ms step_avg:169.83ms\nstep:2220/10000 train_loss:4.3030 train_time:375325ms step_avg:169.83ms\nstep:2221/10000 train_loss:4.1615 train_time:375496ms step_avg:169.83ms\nstep:2222/10000 train_loss:4.3949 train_time:375668ms step_avg:169.83ms\nstep:2223/10000 train_loss:4.3820 train_time:375838ms step_avg:169.83ms\nstep:2224/10000 train_loss:4.4072 train_time:376012ms step_avg:169.83ms\nstep:2225/10000 train_loss:4.2483 train_time:376181ms step_avg:169.83ms\nstep:2226/10000 train_loss:4.3403 train_time:376351ms step_avg:169.83ms\nstep:2227/10000 train_loss:4.3409 train_time:376521ms step_avg:169.83ms\nstep:2228/10000 train_loss:4.3584 train_time:376691ms step_avg:169.83ms\nstep:2229/10000 train_loss:4.2202 train_time:376864ms step_avg:169.84ms\nstep:2230/10000 train_loss:4.0846 train_time:377037ms step_avg:169.84ms\nstep:2231/10000 train_loss:4.4580 train_time:377204ms step_avg:169.84ms\nstep:2232/10000 train_loss:4.2515 train_time:377375ms step_avg:169.84ms\nstep:2233/10000 train_loss:4.1956 train_time:377544ms step_avg:169.84ms\nstep:2234/10000 train_loss:4.2543 train_time:377714ms step_avg:169.84ms\nstep:2235/10000 train_loss:4.2791 train_time:377888ms step_avg:169.84ms\nstep:2236/10000 train_loss:4.3558 train_time:378057ms step_avg:169.84ms\nstep:2237/10000 train_loss:4.4430 train_time:378229ms step_avg:169.84ms\nstep:2238/10000 train_loss:4.3175 train_time:378397ms step_avg:169.84ms\nstep:2239/10000 train_loss:4.2762 train_time:378565ms step_avg:169.84ms\nstep:2240/10000 train_loss:4.2354 train_time:378733ms step_avg:169.84ms\nstep:2241/10000 train_loss:4.4419 train_time:378906ms step_avg:169.84ms\nstep:2242/10000 train_loss:4.3534 train_time:379073ms step_avg:169.84ms\nstep:2243/10000 train_loss:4.5069 train_time:379244ms step_avg:169.84ms\nstep:2244/10000 train_loss:4.3210 train_time:379413ms step_avg:169.84ms\nstep:2245/10000 train_loss:4.2004 train_time:379584ms step_avg:169.84ms\nstep:2246/10000 train_loss:4.2438 train_time:379754ms step_avg:169.84ms\nstep:2247/10000 train_loss:4.3216 train_time:379928ms step_avg:169.84ms\nstep:2248/10000 train_loss:4.3013 train_time:380096ms step_avg:169.84ms\nstep:2249/10000 train_loss:4.2990 train_time:380266ms step_avg:169.84ms\nstep:2250/10000 train_loss:4.1958 train_time:380435ms step_avg:169.84ms\nstep:2251/10000 train_loss:4.2101 train_time:380606ms step_avg:169.84ms\nstep:2252/10000 train_loss:4.1548 train_time:380775ms step_avg:169.84ms\nstep:2253/10000 train_loss:4.3885 train_time:380948ms step_avg:169.84ms\nstep:2254/10000 train_loss:4.3936 train_time:381116ms step_avg:169.84ms\nstep:2255/10000 train_loss:4.2569 train_time:381285ms step_avg:169.84ms\nstep:2256/10000 train_loss:4.3657 train_time:381453ms step_avg:169.84ms\nstep:2257/10000 train_loss:4.2267 train_time:381621ms step_avg:169.84ms\nstep:2258/10000 train_loss:4.2344 train_time:381789ms step_avg:169.84ms\nstep:2259/10000 train_loss:4.3678 train_time:381962ms step_avg:169.84ms\nstep:2260/10000 train_loss:4.2913 train_time:382135ms step_avg:169.84ms\nstep:2261/10000 train_loss:4.4011 train_time:382307ms step_avg:169.84ms\nstep:2262/10000 train_loss:4.3328 train_time:382477ms step_avg:169.84ms\nstep:2263/10000 train_loss:4.2227 train_time:382648ms step_avg:169.84ms\nstep:2264/10000 train_loss:4.2648 train_time:382817ms step_avg:169.84ms\nstep:2265/10000 train_loss:4.4578 train_time:382989ms step_avg:169.84ms\nstep:2266/10000 train_loss:4.3138 train_time:383157ms step_avg:169.84ms\nstep:2267/10000 train_loss:4.2451 train_time:383325ms step_avg:169.84ms\nstep:2268/10000 train_loss:4.3808 train_time:383492ms step_avg:169.84ms\nstep:2269/10000 train_loss:4.2673 train_time:383661ms step_avg:169.84ms\nstep:2270/10000 train_loss:4.2432 train_time:383831ms step_avg:169.84ms\nstep:2271/10000 train_loss:4.2227 train_time:384001ms step_avg:169.84ms\nstep:2272/10000 train_loss:4.3521 train_time:384174ms step_avg:169.84ms\nstep:2273/10000 train_loss:4.3360 train_time:384344ms step_avg:169.84ms\nstep:2274/10000 train_loss:4.3235 train_time:384513ms step_avg:169.84ms\nstep:2275/10000 train_loss:4.3286 train_time:384682ms step_avg:169.84ms\nstep:2276/10000 train_loss:4.5667 train_time:384851ms step_avg:169.84ms\nstep:2277/10000 train_loss:4.3209 train_time:385021ms step_avg:169.84ms\nstep:2278/10000 train_loss:4.3103 train_time:385192ms step_avg:169.84ms\nstep:2279/10000 train_loss:4.4212 train_time:385362ms step_avg:169.84ms\nstep:2280/10000 train_loss:4.2913 train_time:385533ms step_avg:169.84ms\nstep:2281/10000 train_loss:4.1540 train_time:385704ms step_avg:169.84ms\nstep:2282/10000 train_loss:4.3286 train_time:385871ms step_avg:169.84ms\nstep:2283/10000 train_loss:4.3546 train_time:386040ms step_avg:169.84ms\nstep:2284/10000 train_loss:4.3175 train_time:386213ms step_avg:169.84ms\nstep:2285/10000 train_loss:4.2919 train_time:386382ms step_avg:169.84ms\nstep:2286/10000 train_loss:4.2203 train_time:386777ms step_avg:169.94ms\nstep:2287/10000 train_loss:4.3105 train_time:386941ms step_avg:169.93ms\nstep:2288/10000 train_loss:4.4431 train_time:387109ms step_avg:169.93ms\nstep:2289/10000 train_loss:4.2328 train_time:387274ms step_avg:169.93ms\nstep:2290/10000 train_loss:4.1578 train_time:387446ms step_avg:169.93ms\nstep:2291/10000 train_loss:4.3520 train_time:387616ms step_avg:169.93ms\nstep:2292/10000 train_loss:4.3128 train_time:387797ms step_avg:169.94ms\nstep:2293/10000 train_loss:4.2730 train_time:387973ms step_avg:169.94ms\nstep:2294/10000 train_loss:4.2251 train_time:388142ms step_avg:169.94ms\nstep:2295/10000 train_loss:4.2930 train_time:388309ms step_avg:169.94ms\nstep:2296/10000 train_loss:4.3254 train_time:388476ms step_avg:169.94ms\nstep:2297/10000 train_loss:4.4217 train_time:388645ms step_avg:169.94ms\nstep:2298/10000 train_loss:4.6448 train_time:388817ms step_avg:169.94ms\nstep:2299/10000 train_loss:4.3694 train_time:388988ms step_avg:169.94ms\nstep:2300/10000 train_loss:4.4129 train_time:389158ms step_avg:169.94ms\nstep:2301/10000 train_loss:4.2709 train_time:389325ms step_avg:169.94ms\nstep:2302/10000 train_loss:4.2174 train_time:389493ms step_avg:169.94ms\nstep:2303/10000 train_loss:4.2465 train_time:389663ms step_avg:169.94ms\nstep:2304/10000 train_loss:4.3891 train_time:389831ms step_avg:169.94ms\nstep:2305/10000 train_loss:4.3538 train_time:390002ms step_avg:169.94ms\nstep:2306/10000 train_loss:4.2954 train_time:390171ms step_avg:169.93ms\nstep:2307/10000 train_loss:4.3744 train_time:390341ms step_avg:169.94ms\nstep:2308/10000 train_loss:4.3841 train_time:390510ms step_avg:169.93ms\nstep:2309/10000 train_loss:4.2502 train_time:390680ms step_avg:169.93ms\nstep:2310/10000 train_loss:4.4548 train_time:390849ms step_avg:169.93ms\nstep:2311/10000 train_loss:4.2973 train_time:391020ms step_avg:169.93ms\nstep:2312/10000 train_loss:4.3401 train_time:391192ms step_avg:169.94ms\nstep:2313/10000 train_loss:4.4129 train_time:391364ms step_avg:169.94ms\nstep:2314/10000 train_loss:4.2241 train_time:391532ms step_avg:169.94ms\nstep:2315/10000 train_loss:4.1813 train_time:391701ms step_avg:169.94ms\nstep:2316/10000 train_loss:4.2981 train_time:391871ms step_avg:169.94ms\nstep:2317/10000 train_loss:4.3862 train_time:392041ms step_avg:169.94ms\nstep:2318/10000 train_loss:4.2360 train_time:392212ms step_avg:169.94ms\nstep:2319/10000 train_loss:4.5193 train_time:392382ms step_avg:169.94ms\nstep:2320/10000 train_loss:4.2776 train_time:392550ms step_avg:169.94ms\nstep:2321/10000 train_loss:4.4000 train_time:392719ms step_avg:169.93ms\nstep:2322/10000 train_loss:4.3824 train_time:392887ms step_avg:169.93ms\nstep:2323/10000 train_loss:4.2274 train_time:393055ms step_avg:169.93ms\nstep:2324/10000 train_loss:4.2398 train_time:393226ms step_avg:169.93ms\nstep:2325/10000 train_loss:4.3318 train_time:393395ms step_avg:169.93ms\nstep:2326/10000 train_loss:4.3591 train_time:393565ms step_avg:169.93ms\nstep:2327/10000 train_loss:4.2304 train_time:393735ms step_avg:169.93ms\nstep:2328/10000 train_loss:4.2571 train_time:393906ms step_avg:169.93ms\nstep:2329/10000 train_loss:4.2002 train_time:394075ms step_avg:169.93ms\nstep:2330/10000 train_loss:4.3302 train_time:394244ms step_avg:169.93ms\nstep:2331/10000 train_loss:4.4001 train_time:394412ms step_avg:169.93ms\nstep:2332/10000 train_loss:4.4972 train_time:394582ms step_avg:169.93ms\nstep:2333/10000 train_loss:4.2931 train_time:394750ms step_avg:169.93ms\nstep:2334/10000 train_loss:4.4871 train_time:394920ms step_avg:169.93ms\nstep:2335/10000 train_loss:4.1851 train_time:395089ms step_avg:169.93ms\nstep:2336/10000 train_loss:4.2484 train_time:395259ms step_avg:169.93ms\nstep:2337/10000 train_loss:4.2539 train_time:395428ms step_avg:169.93ms\nstep:2338/10000 train_loss:4.2427 train_time:395597ms step_avg:169.93ms\nstep:2339/10000 train_loss:4.3508 train_time:395769ms step_avg:169.93ms\nstep:2340/10000 train_loss:4.2111 train_time:395941ms step_avg:169.93ms\nstep:2341/10000 train_loss:4.2770 train_time:396112ms step_avg:169.93ms\nstep:2342/10000 train_loss:4.3764 train_time:396280ms step_avg:169.93ms\nstep:2343/10000 train_loss:4.1897 train_time:396448ms step_avg:169.93ms\nstep:2344/10000 train_loss:4.3830 train_time:396620ms step_avg:169.93ms\nstep:2345/10000 train_loss:4.3401 train_time:396789ms step_avg:169.93ms\nstep:2346/10000 train_loss:4.3115 train_time:396961ms step_avg:169.93ms\nstep:2347/10000 train_loss:4.2675 train_time:397135ms step_avg:169.93ms\nstep:2348/10000 train_loss:4.1878 train_time:397306ms step_avg:169.93ms\nstep:2349/10000 train_loss:4.2600 train_time:397479ms step_avg:169.94ms\nstep:2350/10000 train_loss:4.6610 train_time:397644ms step_avg:169.93ms\nstep:2351/10000 train_loss:4.2035 train_time:397812ms step_avg:169.93ms\nstep:2352/10000 train_loss:4.2964 train_time:397982ms step_avg:169.93ms\nstep:2353/10000 train_loss:4.3278 train_time:398152ms step_avg:169.93ms\nstep:2354/10000 train_loss:4.1343 train_time:398323ms step_avg:169.93ms\nstep:2355/10000 train_loss:4.2584 train_time:398492ms step_avg:169.93ms\nstep:2356/10000 train_loss:4.3159 train_time:398664ms step_avg:169.93ms\nstep:2357/10000 train_loss:4.2728 train_time:398834ms step_avg:169.93ms\nstep:2358/10000 train_loss:4.5048 train_time:399004ms step_avg:169.93ms\nstep:2359/10000 train_loss:4.2837 train_time:399172ms step_avg:169.93ms\nstep:2360/10000 train_loss:4.6426 train_time:399341ms step_avg:169.93ms\nstep:2361/10000 train_loss:4.3513 train_time:399508ms step_avg:169.93ms\nstep:2362/10000 train_loss:4.3393 train_time:399677ms step_avg:169.93ms\nstep:2363/10000 train_loss:4.4656 train_time:399852ms step_avg:169.93ms\nstep:2364/10000 train_loss:4.4035 train_time:400023ms step_avg:169.93ms\nstep:2365/10000 train_loss:4.2860 train_time:400194ms step_avg:169.93ms\nstep:2366/10000 train_loss:4.1489 train_time:400366ms step_avg:169.93ms\nstep:2367/10000 train_loss:4.1800 train_time:400536ms step_avg:169.93ms\nstep:2368/10000 train_loss:4.1536 train_time:400706ms step_avg:169.93ms\nstep:2369/10000 train_loss:4.4662 train_time:400875ms step_avg:169.93ms\nstep:2370/10000 train_loss:4.2238 train_time:401045ms step_avg:169.93ms\nstep:2371/10000 train_loss:4.4029 train_time:401214ms step_avg:169.93ms\nstep:2372/10000 train_loss:4.1660 train_time:401384ms step_avg:169.93ms\nstep:2373/10000 train_loss:4.2803 train_time:401553ms step_avg:169.93ms\nstep:2374/10000 train_loss:4.3754 train_time:401725ms step_avg:169.93ms\nstep:2375/10000 train_loss:4.3852 train_time:401894ms step_avg:169.93ms\nstep:2376/10000 train_loss:4.1900 train_time:402066ms step_avg:169.93ms\nstep:2377/10000 train_loss:4.1946 train_time:402236ms step_avg:169.93ms\nstep:2378/10000 train_loss:4.2560 train_time:402405ms step_avg:169.93ms\nstep:2379/10000 train_loss:4.3227 train_time:402575ms step_avg:169.93ms\nstep:2380/10000 train_loss:4.2436 train_time:402744ms step_avg:169.93ms\nstep:2381/10000 train_loss:4.2163 train_time:402915ms step_avg:169.93ms\nstep:2382/10000 train_loss:4.2717 train_time:403087ms step_avg:169.94ms\nstep:2383/10000 train_loss:4.6480 train_time:403256ms step_avg:169.94ms\nstep:2384/10000 train_loss:4.5442 train_time:403425ms step_avg:169.93ms\nstep:2385/10000 train_loss:4.2974 train_time:403596ms step_avg:169.94ms\nstep:2386/10000 train_loss:4.2923 train_time:403765ms step_avg:169.93ms\nstep:2387/10000 train_loss:4.1621 train_time:403934ms step_avg:169.93ms\nstep:2388/10000 train_loss:4.3004 train_time:404103ms step_avg:169.93ms\nstep:2389/10000 train_loss:4.3755 train_time:404273ms step_avg:169.93ms\nstep:2390/10000 train_loss:4.3362 train_time:404442ms step_avg:169.93ms\nstep:2391/10000 train_loss:4.2860 train_time:404611ms step_avg:169.93ms\nstep:2392/10000 train_loss:4.2280 train_time:404782ms step_avg:169.93ms\nstep:2393/10000 train_loss:4.2815 train_time:404952ms step_avg:169.93ms\nstep:2394/10000 train_loss:4.3521 train_time:405122ms step_avg:169.93ms\nstep:2395/10000 train_loss:4.3127 train_time:405294ms step_avg:169.93ms\nstep:2396/10000 train_loss:4.2796 train_time:405465ms step_avg:169.93ms\nstep:2397/10000 train_loss:4.2979 train_time:405636ms step_avg:169.94ms\nstep:2398/10000 train_loss:4.4215 train_time:405806ms step_avg:169.94ms\nstep:2399/10000 train_loss:4.2677 train_time:405974ms step_avg:169.93ms\nstep:2400/10000 train_loss:4.1060 train_time:406145ms step_avg:169.94ms\nstep:2401/10000 train_loss:4.2696 train_time:406315ms step_avg:169.94ms\nstep:2402/10000 train_loss:4.2746 train_time:406486ms step_avg:169.94ms\nstep:2403/10000 train_loss:4.2141 train_time:406654ms step_avg:169.93ms\nstep:2404/10000 train_loss:4.2333 train_time:406826ms step_avg:169.94ms\nstep:2405/10000 train_loss:4.3344 train_time:406994ms step_avg:169.93ms\nstep:2406/10000 train_loss:4.1881 train_time:407163ms step_avg:169.93ms\nstep:2407/10000 train_loss:4.1712 train_time:407332ms step_avg:169.93ms\nstep:2408/10000 train_loss:4.2327 train_time:407502ms step_avg:169.93ms\nstep:2409/10000 train_loss:4.1790 train_time:407673ms step_avg:169.93ms\nstep:2410/10000 train_loss:4.3572 train_time:407845ms step_avg:169.94ms\nstep:2411/10000 train_loss:4.3032 train_time:408014ms step_avg:169.94ms\nstep:2412/10000 train_loss:4.2579 train_time:408187ms step_avg:169.94ms\nstep:2413/10000 train_loss:4.1750 train_time:408355ms step_avg:169.94ms\nstep:2414/10000 train_loss:4.1847 train_time:408524ms step_avg:169.93ms\nstep:2415/10000 train_loss:4.3159 train_time:408693ms step_avg:169.93ms\nstep:2416/10000 train_loss:4.3919 train_time:408866ms step_avg:169.94ms\nstep:2417/10000 train_loss:4.2914 train_time:409033ms step_avg:169.93ms\nstep:2418/10000 train_loss:4.2971 train_time:409202ms step_avg:169.93ms\nstep:2419/10000 train_loss:4.3906 train_time:409370ms step_avg:169.93ms\nstep:2420/10000 train_loss:4.3069 train_time:409540ms step_avg:169.93ms\nstep:2421/10000 train_loss:4.2991 train_time:409708ms step_avg:169.93ms\nstep:2422/10000 train_loss:4.5661 train_time:409878ms step_avg:169.93ms\nstep:2423/10000 train_loss:4.3950 train_time:410052ms step_avg:169.93ms\nstep:2424/10000 train_loss:4.2459 train_time:410222ms step_avg:169.93ms\nstep:2425/10000 train_loss:4.1554 train_time:410390ms step_avg:169.93ms\nstep:2426/10000 train_loss:4.3383 train_time:410559ms step_avg:169.93ms\nstep:2427/10000 train_loss:4.1067 train_time:410732ms step_avg:169.93ms\nstep:2428/10000 train_loss:4.1569 train_time:410903ms step_avg:169.94ms\nstep:2429/10000 train_loss:4.2335 train_time:411073ms step_avg:169.94ms\nstep:2430/10000 train_loss:4.3616 train_time:411245ms step_avg:169.94ms\nstep:2431/10000 train_loss:4.2750 train_time:411415ms step_avg:169.94ms\nstep:2432/10000 train_loss:4.3957 train_time:411585ms step_avg:169.94ms\nstep:2433/10000 train_loss:4.1995 train_time:411755ms step_avg:169.94ms\nstep:2434/10000 train_loss:4.2953 train_time:411925ms step_avg:169.94ms\nstep:2435/10000 train_loss:4.2677 train_time:412094ms step_avg:169.94ms\nstep:2436/10000 train_loss:4.2592 train_time:412265ms step_avg:169.94ms\nstep:2437/10000 train_loss:4.2559 train_time:412432ms step_avg:169.93ms\nstep:2438/10000 train_loss:4.1923 train_time:412602ms step_avg:169.93ms\nstep:2439/10000 train_loss:4.2145 train_time:412770ms step_avg:169.93ms\nstep:2440/10000 train_loss:4.2549 train_time:412941ms step_avg:169.93ms\nstep:2441/10000 train_loss:4.2966 train_time:413109ms step_avg:169.93ms\nstep:2442/10000 train_loss:4.2748 train_time:413280ms step_avg:169.93ms\nstep:2443/10000 train_loss:4.2762 train_time:413453ms step_avg:169.94ms\nstep:2444/10000 train_loss:4.3513 train_time:413624ms step_avg:169.94ms\nstep:2445/10000 train_loss:4.2001 train_time:413792ms step_avg:169.94ms\nstep:2446/10000 train_loss:4.2114 train_time:413962ms step_avg:169.94ms\nstep:2447/10000 train_loss:4.2723 train_time:414133ms step_avg:169.94ms\nstep:2448/10000 train_loss:4.2600 train_time:414303ms step_avg:169.94ms\nstep:2449/10000 train_loss:4.3253 train_time:414472ms step_avg:169.94ms\nstep:2450/10000 train_loss:4.3905 train_time:414641ms step_avg:169.93ms\nstep:2451/10000 train_loss:4.1701 train_time:414811ms step_avg:169.94ms\nstep:2452/10000 train_loss:4.2225 train_time:414980ms step_avg:169.93ms\nstep:2453/10000 train_loss:3.9823 train_time:415148ms step_avg:169.93ms\nstep:2454/10000 train_loss:4.1877 train_time:415318ms step_avg:169.93ms\nstep:2455/10000 train_loss:4.2369 train_time:415492ms step_avg:169.94ms\nstep:2456/10000 train_loss:4.3919 train_time:415660ms step_avg:169.93ms\nstep:2457/10000 train_loss:4.1484 train_time:415829ms step_avg:169.93ms\nstep:2458/10000 train_loss:4.2786 train_time:415998ms step_avg:169.93ms\nstep:2459/10000 train_loss:4.3670 train_time:416171ms step_avg:169.93ms\nstep:2460/10000 train_loss:4.3682 train_time:416340ms step_avg:169.93ms\nstep:2461/10000 train_loss:4.2839 train_time:416510ms step_avg:169.93ms\nstep:2462/10000 train_loss:4.2214 train_time:416679ms step_avg:169.93ms\nstep:2463/10000 train_loss:4.3938 train_time:416851ms step_avg:169.94ms\nstep:2464/10000 train_loss:4.3265 train_time:417019ms step_avg:169.93ms\nstep:2465/10000 train_loss:4.2371 train_time:417189ms step_avg:169.93ms\nstep:2466/10000 train_loss:4.2805 train_time:417361ms step_avg:169.94ms\nstep:2467/10000 train_loss:4.2029 train_time:417535ms step_avg:169.94ms\nstep:2468/10000 train_loss:4.2010 train_time:417703ms step_avg:169.94ms\nstep:2469/10000 train_loss:4.3495 train_time:417873ms step_avg:169.94ms\nstep:2470/10000 train_loss:4.3164 train_time:418044ms step_avg:169.94ms\nstep:2471/10000 train_loss:4.2053 train_time:418212ms step_avg:169.94ms\nstep:2472/10000 train_loss:4.1933 train_time:418382ms step_avg:169.94ms\nstep:2473/10000 train_loss:4.3579 train_time:418551ms step_avg:169.94ms\nstep:2474/10000 train_loss:4.1101 train_time:418721ms step_avg:169.94ms\nstep:2475/10000 train_loss:4.2675 train_time:418891ms step_avg:169.94ms\nstep:2476/10000 train_loss:4.2753 train_time:419060ms step_avg:169.94ms\nstep:2477/10000 train_loss:4.3306 train_time:419227ms step_avg:169.93ms\nstep:2478/10000 train_loss:4.3913 train_time:419397ms step_avg:169.93ms\nstep:2479/10000 train_loss:4.2612 train_time:419568ms step_avg:169.93ms\nstep:2480/10000 train_loss:4.2076 train_time:419739ms step_avg:169.93ms\nstep:2481/10000 train_loss:4.2320 train_time:419910ms step_avg:169.94ms\nstep:2482/10000 train_loss:4.3038 train_time:420082ms step_avg:169.94ms\nstep:2483/10000 train_loss:4.2073 train_time:420253ms step_avg:169.94ms\nstep:2484/10000 train_loss:4.2262 train_time:420424ms step_avg:169.94ms\nstep:2485/10000 train_loss:4.2363 train_time:420594ms step_avg:169.94ms\nstep:2486/10000 train_loss:4.2864 train_time:420765ms step_avg:169.94ms\nstep:2487/10000 train_loss:4.2863 train_time:420935ms step_avg:169.94ms\nstep:2488/10000 train_loss:4.4788 train_time:421106ms step_avg:169.94ms\nstep:2489/10000 train_loss:4.4545 train_time:421278ms step_avg:169.94ms\nstep:2490/10000 train_loss:4.1280 train_time:421450ms step_avg:169.94ms\nstep:2491/10000 train_loss:4.2910 train_time:421619ms step_avg:169.94ms\nstep:2492/10000 train_loss:4.2216 train_time:421789ms step_avg:169.94ms\nstep:2493/10000 train_loss:4.3454 train_time:421958ms step_avg:169.94ms\nstep:2494/10000 train_loss:4.2644 train_time:422126ms step_avg:169.94ms\nstep:2495/10000 train_loss:4.3168 train_time:422296ms step_avg:169.94ms\nstep:2496/10000 train_loss:4.2666 train_time:422468ms step_avg:169.94ms\nstep:2497/10000 train_loss:4.2950 train_time:422635ms step_avg:169.94ms\nstep:2498/10000 train_loss:4.3387 train_time:422806ms step_avg:169.94ms\nstep:2499/10000 train_loss:4.0915 train_time:422974ms step_avg:169.94ms\nstep:2500/10000 train_loss:4.3026 train_time:423147ms step_avg:169.94ms\nstep:2501/10000 train_loss:4.3200 train_time:423314ms step_avg:169.94ms\nstep:2502/10000 train_loss:4.2449 train_time:423483ms step_avg:169.94ms\nstep:2503/10000 train_loss:4.2657 train_time:423651ms step_avg:169.94ms\nstep:2504/10000 train_loss:4.3735 train_time:423822ms step_avg:169.94ms\nstep:2505/10000 train_loss:4.2028 train_time:423994ms step_avg:169.94ms\nstep:2506/10000 train_loss:4.2347 train_time:424164ms step_avg:169.94ms\nstep:2507/10000 train_loss:4.3010 train_time:424333ms step_avg:169.94ms\nstep:2508/10000 train_loss:4.3624 train_time:424504ms step_avg:169.94ms\nstep:2509/10000 train_loss:4.2185 train_time:424672ms step_avg:169.94ms\nstep:2510/10000 train_loss:4.1418 train_time:424843ms step_avg:169.94ms\nstep:2511/10000 train_loss:4.2654 train_time:425012ms step_avg:169.94ms\nstep:2512/10000 train_loss:4.3389 train_time:425184ms step_avg:169.94ms\nstep:2513/10000 train_loss:4.4055 train_time:425355ms step_avg:169.94ms\nstep:2514/10000 train_loss:4.3044 train_time:425528ms step_avg:169.94ms\nstep:2515/10000 train_loss:4.1624 train_time:425696ms step_avg:169.94ms\nstep:2516/10000 train_loss:4.3892 train_time:425864ms step_avg:169.94ms\nstep:2517/10000 train_loss:4.2158 train_time:426032ms step_avg:169.94ms\nstep:2518/10000 train_loss:4.1399 train_time:426202ms step_avg:169.94ms\nstep:2519/10000 train_loss:4.2324 train_time:426372ms step_avg:169.94ms\nstep:2520/10000 train_loss:4.1860 train_time:426542ms step_avg:169.94ms\nstep:2521/10000 train_loss:4.2710 train_time:426711ms step_avg:169.94ms\nstep:2522/10000 train_loss:4.3122 train_time:426882ms step_avg:169.94ms\nstep:2523/10000 train_loss:4.3431 train_time:427049ms step_avg:169.94ms\nstep:2524/10000 train_loss:4.3261 train_time:427221ms step_avg:169.94ms\nstep:2525/10000 train_loss:4.2042 train_time:427389ms step_avg:169.94ms\nstep:2526/10000 train_loss:4.1439 train_time:427560ms step_avg:169.94ms\nstep:2527/10000 train_loss:4.2367 train_time:427730ms step_avg:169.94ms\nstep:2528/10000 train_loss:4.3806 train_time:427900ms step_avg:169.94ms\nstep:2529/10000 train_loss:4.2426 train_time:428073ms step_avg:169.94ms\nstep:2530/10000 train_loss:4.2627 train_time:428243ms step_avg:169.94ms\nstep:2531/10000 train_loss:4.2016 train_time:428412ms step_avg:169.94ms\nstep:2532/10000 train_loss:4.2760 train_time:428582ms step_avg:169.94ms\nstep:2533/10000 train_loss:4.1410 train_time:428753ms step_avg:169.94ms\nstep:2534/10000 train_loss:4.4027 train_time:428922ms step_avg:169.94ms\nstep:2535/10000 train_loss:4.3642 train_time:429093ms step_avg:169.94ms\nstep:2536/10000 train_loss:4.1700 train_time:429263ms step_avg:169.94ms\nstep:2537/10000 train_loss:4.4154 train_time:429431ms step_avg:169.94ms\nstep:2538/10000 train_loss:4.2455 train_time:429600ms step_avg:169.94ms\nstep:2539/10000 train_loss:4.3176 train_time:429771ms step_avg:169.94ms\nstep:2540/10000 train_loss:4.2675 train_time:429939ms step_avg:169.94ms\nstep:2541/10000 train_loss:4.2091 train_time:430107ms step_avg:169.94ms\nstep:2542/10000 train_loss:4.1486 train_time:430276ms step_avg:169.94ms\nstep:2543/10000 train_loss:4.0813 train_time:430445ms step_avg:169.93ms\nstep:2544/10000 train_loss:4.5302 train_time:430613ms step_avg:169.93ms\nstep:2545/10000 train_loss:4.2451 train_time:430786ms step_avg:169.94ms\nstep:2546/10000 train_loss:4.2267 train_time:430953ms step_avg:169.93ms\nstep:2547/10000 train_loss:4.3101 train_time:431123ms step_avg:169.93ms\nstep:2548/10000 train_loss:4.1359 train_time:431292ms step_avg:169.93ms\nstep:2549/10000 train_loss:4.1930 train_time:431462ms step_avg:169.93ms\nstep:2550/10000 train_loss:4.1509 train_time:431633ms step_avg:169.93ms\nstep:2551/10000 train_loss:4.3662 train_time:431805ms step_avg:169.94ms\nstep:2552/10000 train_loss:3.9879 train_time:431974ms step_avg:169.93ms\nstep:2553/10000 train_loss:4.2774 train_time:432146ms step_avg:169.94ms\nstep:2554/10000 train_loss:4.1897 train_time:432313ms step_avg:169.93ms\nstep:2555/10000 train_loss:4.1859 train_time:432482ms step_avg:169.93ms\nstep:2556/10000 train_loss:4.1918 train_time:432649ms step_avg:169.93ms\nstep:2557/10000 train_loss:4.2422 train_time:432826ms step_avg:169.94ms\nstep:2558/10000 train_loss:4.2619 train_time:432991ms step_avg:169.93ms\nstep:2559/10000 train_loss:4.3364 train_time:433163ms step_avg:169.93ms\nstep:2560/10000 train_loss:4.1959 train_time:433333ms step_avg:169.93ms\nstep:2561/10000 train_loss:4.0992 train_time:433509ms step_avg:169.94ms\nstep:2562/10000 train_loss:4.1589 train_time:433673ms step_avg:169.93ms\nstep:2563/10000 train_loss:4.3317 train_time:433842ms step_avg:169.93ms\nstep:2564/10000 train_loss:4.2109 train_time:434012ms step_avg:169.93ms\nstep:2565/10000 train_loss:4.3103 train_time:434182ms step_avg:169.93ms\nstep:2566/10000 train_loss:4.1822 train_time:434352ms step_avg:169.93ms\nstep:2567/10000 train_loss:4.2545 train_time:434522ms step_avg:169.93ms\nstep:2568/10000 train_loss:4.2043 train_time:434694ms step_avg:169.93ms\nstep:2569/10000 train_loss:4.2804 train_time:434862ms step_avg:169.93ms\nstep:2570/10000 train_loss:4.3488 train_time:435031ms step_avg:169.93ms\nstep:2571/10000 train_loss:4.2193 train_time:435200ms step_avg:169.93ms\nstep:2572/10000 train_loss:4.2257 train_time:435370ms step_avg:169.93ms\nstep:2573/10000 train_loss:4.0569 train_time:435540ms step_avg:169.93ms\nstep:2574/10000 train_loss:4.0318 train_time:435711ms step_avg:169.93ms\nstep:2575/10000 train_loss:4.2858 train_time:435882ms step_avg:169.93ms\nstep:2576/10000 train_loss:4.2724 train_time:436055ms step_avg:169.94ms\nstep:2577/10000 train_loss:4.2282 train_time:436225ms step_avg:169.94ms\nstep:2578/10000 train_loss:4.2014 train_time:436393ms step_avg:169.94ms\nstep:2579/10000 train_loss:4.3866 train_time:436562ms step_avg:169.93ms\nstep:2580/10000 train_loss:4.1740 train_time:436733ms step_avg:169.93ms\nstep:2581/10000 train_loss:4.3537 train_time:436903ms step_avg:169.93ms\nstep:2582/10000 train_loss:4.1735 train_time:437073ms step_avg:169.94ms\nstep:2583/10000 train_loss:4.3227 train_time:437245ms step_avg:169.94ms\nstep:2584/10000 train_loss:4.3861 train_time:437415ms step_avg:169.94ms\nstep:2585/10000 train_loss:4.1772 train_time:437583ms step_avg:169.94ms\nstep:2586/10000 train_loss:4.3281 train_time:437752ms step_avg:169.93ms\nstep:2587/10000 train_loss:4.1937 train_time:437921ms step_avg:169.93ms\nstep:2588/10000 train_loss:4.2770 train_time:438091ms step_avg:169.93ms\nstep:2589/10000 train_loss:4.2474 train_time:438264ms step_avg:169.94ms\nstep:2590/10000 train_loss:4.2385 train_time:438434ms step_avg:169.94ms\nstep:2591/10000 train_loss:4.1834 train_time:438602ms step_avg:169.93ms\nstep:2592/10000 train_loss:4.2860 train_time:438771ms step_avg:169.93ms\nstep:2593/10000 train_loss:4.2222 train_time:438941ms step_avg:169.93ms\nstep:2594/10000 train_loss:4.4876 train_time:439110ms step_avg:169.93ms\nstep:2595/10000 train_loss:4.5321 train_time:439282ms step_avg:169.93ms\nstep:2596/10000 train_loss:4.2718 train_time:439455ms step_avg:169.94ms\nstep:2597/10000 train_loss:4.3476 train_time:439624ms step_avg:169.94ms\nstep:2598/10000 train_loss:4.2614 train_time:439795ms step_avg:169.94ms\nstep:2599/10000 train_loss:4.2424 train_time:439968ms step_avg:169.94ms\nstep:2600/10000 train_loss:4.3729 train_time:440137ms step_avg:169.94ms\nstep:2601/10000 train_loss:4.2718 train_time:440307ms step_avg:169.94ms\nstep:2602/10000 train_loss:4.1315 train_time:440476ms step_avg:169.94ms\nstep:2603/10000 train_loss:4.3042 train_time:440646ms step_avg:169.94ms\nstep:2604/10000 train_loss:4.2350 train_time:440817ms step_avg:169.94ms\nstep:2605/10000 train_loss:4.3546 train_time:440988ms step_avg:169.94ms\nstep:2606/10000 train_loss:4.1298 train_time:441158ms step_avg:169.94ms\nstep:2607/10000 train_loss:4.2398 train_time:441326ms step_avg:169.94ms\nstep:2608/10000 train_loss:4.3459 train_time:441496ms step_avg:169.94ms\nstep:2609/10000 train_loss:4.0922 train_time:441669ms step_avg:169.94ms\nstep:2610/10000 train_loss:4.2556 train_time:441837ms step_avg:169.94ms\nstep:2611/10000 train_loss:4.1999 train_time:442006ms step_avg:169.94ms\nstep:2612/10000 train_loss:4.2046 train_time:442175ms step_avg:169.94ms\nstep:2613/10000 train_loss:4.1594 train_time:442348ms step_avg:169.94ms\nstep:2614/10000 train_loss:4.2127 train_time:442516ms step_avg:169.94ms\nstep:2615/10000 train_loss:4.2107 train_time:442688ms step_avg:169.94ms\nstep:2616/10000 train_loss:4.2726 train_time:442859ms step_avg:169.94ms\nstep:2617/10000 train_loss:4.1995 train_time:443032ms step_avg:169.94ms\nstep:2618/10000 train_loss:4.3624 train_time:443201ms step_avg:169.94ms\nstep:2619/10000 train_loss:4.1740 train_time:443372ms step_avg:169.94ms\nstep:2620/10000 train_loss:4.0414 train_time:443543ms step_avg:169.94ms\nstep:2621/10000 train_loss:4.3266 train_time:443712ms step_avg:169.94ms\nstep:2622/10000 train_loss:4.3956 train_time:443881ms step_avg:169.94ms\nstep:2623/10000 train_loss:4.3813 train_time:444050ms step_avg:169.94ms\nstep:2624/10000 train_loss:4.1770 train_time:444221ms step_avg:169.94ms\nstep:2625/10000 train_loss:4.0785 train_time:444393ms step_avg:169.94ms\nstep:2626/10000 train_loss:4.2787 train_time:444564ms step_avg:169.94ms\nstep:2627/10000 train_loss:4.2689 train_time:444734ms step_avg:169.94ms\nstep:2628/10000 train_loss:4.1799 train_time:444904ms step_avg:169.94ms\nstep:2629/10000 train_loss:4.2888 train_time:445074ms step_avg:169.94ms\nstep:2630/10000 train_loss:4.2217 train_time:445244ms step_avg:169.94ms\nstep:2631/10000 train_loss:4.2525 train_time:445413ms step_avg:169.94ms\nstep:2632/10000 train_loss:4.1305 train_time:445581ms step_avg:169.94ms\nstep:2633/10000 train_loss:3.9789 train_time:445751ms step_avg:169.94ms\nstep:2634/10000 train_loss:4.0907 train_time:445922ms step_avg:169.94ms\nstep:2635/10000 train_loss:4.2841 train_time:446092ms step_avg:169.94ms\nstep:2636/10000 train_loss:4.3064 train_time:446262ms step_avg:169.94ms\nstep:2637/10000 train_loss:4.1759 train_time:446434ms step_avg:169.94ms\nstep:2638/10000 train_loss:4.2523 train_time:446604ms step_avg:169.94ms\nstep:2639/10000 train_loss:4.2136 train_time:446773ms step_avg:169.94ms\nstep:2640/10000 train_loss:4.2046 train_time:446944ms step_avg:169.94ms\nstep:2641/10000 train_loss:4.1606 train_time:447115ms step_avg:169.94ms\nstep:2642/10000 train_loss:4.3484 train_time:447285ms step_avg:169.94ms\nstep:2643/10000 train_loss:4.2984 train_time:447454ms step_avg:169.94ms\nstep:2644/10000 train_loss:4.2636 train_time:447625ms step_avg:169.94ms\nstep:2645/10000 train_loss:4.2428 train_time:447793ms step_avg:169.94ms\nstep:2646/10000 train_loss:4.2761 train_time:447964ms step_avg:169.94ms\nstep:2647/10000 train_loss:4.2571 train_time:448135ms step_avg:169.94ms\nstep:2648/10000 train_loss:4.3424 train_time:448308ms step_avg:169.94ms\nstep:2649/10000 train_loss:4.3491 train_time:448477ms step_avg:169.94ms\nstep:2650/10000 train_loss:4.1221 train_time:448646ms step_avg:169.94ms\nstep:2651/10000 train_loss:4.3200 train_time:448814ms step_avg:169.94ms\nstep:2652/10000 train_loss:4.2696 train_time:448983ms step_avg:169.94ms\nstep:2653/10000 train_loss:4.1889 train_time:449153ms step_avg:169.94ms\nstep:2654/10000 train_loss:4.2552 train_time:449324ms step_avg:169.94ms\nstep:2655/10000 train_loss:4.2145 train_time:449492ms step_avg:169.94ms\nstep:2656/10000 train_loss:4.1505 train_time:449663ms step_avg:169.94ms\nstep:2657/10000 train_loss:4.1780 train_time:449833ms step_avg:169.94ms\nstep:2658/10000 train_loss:4.2076 train_time:450003ms step_avg:169.94ms\nstep:2659/10000 train_loss:4.1638 train_time:450171ms step_avg:169.94ms\nstep:2660/10000 train_loss:4.2312 train_time:450344ms step_avg:169.94ms\nstep:2661/10000 train_loss:4.1807 train_time:450515ms step_avg:169.94ms\nstep:2662/10000 train_loss:4.1463 train_time:450684ms step_avg:169.94ms\nstep:2663/10000 train_loss:4.2460 train_time:450853ms step_avg:169.94ms\nstep:2664/10000 train_loss:4.1015 train_time:451023ms step_avg:169.94ms\nstep:2665/10000 train_loss:4.3865 train_time:451193ms step_avg:169.94ms\nstep:2666/10000 train_loss:4.2678 train_time:451364ms step_avg:169.94ms\nstep:2667/10000 train_loss:4.1175 train_time:451531ms step_avg:169.94ms\nstep:2668/10000 train_loss:4.3880 train_time:451701ms step_avg:169.94ms\nstep:2669/10000 train_loss:4.1313 train_time:451872ms step_avg:169.94ms\nstep:2670/10000 train_loss:4.3267 train_time:452044ms step_avg:169.94ms\nstep:2671/10000 train_loss:4.2696 train_time:452211ms step_avg:169.94ms\nstep:2672/10000 train_loss:4.2418 train_time:452385ms step_avg:169.94ms\nstep:2673/10000 train_loss:4.2399 train_time:452551ms step_avg:169.94ms\nstep:2674/10000 train_loss:4.3277 train_time:452720ms step_avg:169.94ms\nstep:2675/10000 train_loss:4.3141 train_time:452891ms step_avg:169.94ms\nstep:2676/10000 train_loss:4.2099 train_time:453063ms step_avg:169.94ms\nstep:2677/10000 train_loss:4.3691 train_time:453229ms step_avg:169.94ms\nstep:2678/10000 train_loss:4.1482 train_time:453399ms step_avg:169.94ms\nstep:2679/10000 train_loss:4.2384 train_time:453570ms step_avg:169.94ms\nstep:2680/10000 train_loss:4.0816 train_time:453738ms step_avg:169.94ms\nstep:2681/10000 train_loss:4.1987 train_time:453909ms step_avg:169.94ms\nstep:2682/10000 train_loss:4.2688 train_time:454078ms step_avg:169.94ms\nstep:2683/10000 train_loss:4.1925 train_time:454248ms step_avg:169.94ms\nstep:2684/10000 train_loss:4.1891 train_time:454417ms step_avg:169.94ms\nstep:2685/10000 train_loss:4.1817 train_time:454587ms step_avg:169.94ms\nstep:2686/10000 train_loss:4.2233 train_time:454757ms step_avg:169.94ms\nstep:2687/10000 train_loss:4.4315 train_time:454928ms step_avg:169.94ms\nstep:2688/10000 train_loss:4.1486 train_time:455098ms step_avg:169.94ms\nstep:2689/10000 train_loss:4.1724 train_time:455268ms step_avg:169.94ms\nstep:2690/10000 train_loss:4.0479 train_time:455438ms step_avg:169.94ms\nstep:2691/10000 train_loss:4.3167 train_time:455610ms step_avg:169.94ms\nstep:2692/10000 train_loss:4.1873 train_time:455780ms step_avg:169.94ms\nstep:2693/10000 train_loss:4.2162 train_time:455951ms step_avg:169.94ms\nstep:2694/10000 train_loss:4.4848 train_time:456120ms step_avg:169.94ms\nstep:2695/10000 train_loss:4.1661 train_time:456290ms step_avg:169.94ms\nstep:2696/10000 train_loss:4.4414 train_time:456461ms step_avg:169.94ms\nstep:2697/10000 train_loss:4.1304 train_time:456632ms step_avg:169.94ms\nstep:2698/10000 train_loss:4.3310 train_time:456803ms step_avg:169.94ms\nstep:2699/10000 train_loss:4.1827 train_time:456974ms step_avg:169.94ms\nstep:2700/10000 train_loss:4.2641 train_time:457145ms step_avg:169.94ms\nstep:2701/10000 train_loss:4.1812 train_time:457320ms step_avg:169.94ms\nstep:2702/10000 train_loss:4.2432 train_time:457485ms step_avg:169.94ms\nstep:2703/10000 train_loss:4.1027 train_time:457652ms step_avg:169.94ms\nstep:2704/10000 train_loss:4.2929 train_time:457821ms step_avg:169.94ms\nstep:2705/10000 train_loss:4.1820 train_time:457991ms step_avg:169.94ms\nstep:2706/10000 train_loss:4.2579 train_time:458161ms step_avg:169.94ms\nstep:2707/10000 train_loss:4.2082 train_time:458333ms step_avg:169.94ms\nstep:2708/10000 train_loss:4.0764 train_time:458503ms step_avg:169.94ms\nstep:2709/10000 train_loss:4.2252 train_time:458676ms step_avg:169.94ms\nstep:2710/10000 train_loss:4.2079 train_time:458846ms step_avg:169.94ms\nstep:2711/10000 train_loss:4.2179 train_time:459014ms step_avg:169.94ms\nstep:2712/10000 train_loss:4.1765 train_time:459184ms step_avg:169.94ms\nstep:2713/10000 train_loss:4.1445 train_time:459354ms step_avg:169.94ms\nstep:2714/10000 train_loss:4.0924 train_time:459525ms step_avg:169.94ms\nstep:2715/10000 train_loss:4.2419 train_time:459693ms step_avg:169.94ms\nstep:2716/10000 train_loss:4.1290 train_time:459864ms step_avg:169.94ms\nstep:2717/10000 train_loss:4.0202 train_time:460033ms step_avg:169.94ms\nstep:2718/10000 train_loss:4.2329 train_time:460204ms step_avg:169.94ms\nstep:2719/10000 train_loss:4.2748 train_time:460373ms step_avg:169.94ms\nstep:2720/10000 train_loss:4.2806 train_time:460544ms step_avg:169.94ms\nstep:2721/10000 train_loss:4.0527 train_time:460712ms step_avg:169.94ms\nstep:2722/10000 train_loss:4.2639 train_time:460881ms step_avg:169.94ms\nstep:2723/10000 train_loss:4.1344 train_time:461050ms step_avg:169.94ms\nstep:2724/10000 train_loss:4.1866 train_time:461221ms step_avg:169.94ms\nstep:2725/10000 train_loss:4.1930 train_time:461395ms step_avg:169.94ms\nstep:2726/10000 train_loss:4.3708 train_time:461562ms step_avg:169.94ms\nstep:2727/10000 train_loss:4.2640 train_time:461733ms step_avg:169.94ms\nstep:2728/10000 train_loss:4.2130 train_time:461904ms step_avg:169.94ms\nstep:2729/10000 train_loss:4.1456 train_time:462073ms step_avg:169.94ms\nstep:2730/10000 train_loss:4.1595 train_time:462243ms step_avg:169.94ms\nstep:2731/10000 train_loss:4.1693 train_time:462412ms step_avg:169.94ms\nstep:2732/10000 train_loss:4.2927 train_time:462581ms step_avg:169.94ms\nstep:2733/10000 train_loss:4.3329 train_time:462751ms step_avg:169.94ms\nstep:2734/10000 train_loss:4.2286 train_time:462922ms step_avg:169.94ms\nstep:2735/10000 train_loss:4.1670 train_time:463090ms step_avg:169.94ms\nstep:2736/10000 train_loss:4.3451 train_time:463260ms step_avg:169.94ms\nstep:2737/10000 train_loss:4.7891 train_time:463427ms step_avg:169.94ms\nstep:2738/10000 train_loss:4.2360 train_time:463596ms step_avg:169.94ms\nstep:2739/10000 train_loss:4.0417 train_time:463768ms step_avg:169.94ms\nstep:2740/10000 train_loss:4.2314 train_time:463941ms step_avg:169.94ms\nstep:2741/10000 train_loss:4.4174 train_time:464110ms step_avg:169.94ms\nstep:2742/10000 train_loss:4.1699 train_time:464278ms step_avg:169.94ms\nstep:2743/10000 train_loss:4.2804 train_time:464448ms step_avg:169.94ms\nstep:2744/10000 train_loss:4.3175 train_time:464618ms step_avg:169.94ms\nstep:2745/10000 train_loss:4.2279 train_time:464788ms step_avg:169.94ms\nstep:2746/10000 train_loss:4.3040 train_time:464957ms step_avg:169.94ms\nstep:2747/10000 train_loss:4.1919 train_time:465127ms step_avg:169.94ms\nstep:2748/10000 train_loss:4.2467 train_time:465298ms step_avg:169.94ms\nstep:2749/10000 train_loss:4.1645 train_time:465469ms step_avg:169.94ms\nstep:2750/10000 train_loss:4.1995 train_time:465641ms step_avg:169.94ms\nstep:2751/10000 train_loss:4.3332 train_time:465811ms step_avg:169.94ms\nstep:2752/10000 train_loss:4.2717 train_time:465982ms step_avg:169.94ms\nstep:2753/10000 train_loss:4.1307 train_time:466153ms step_avg:169.94ms\nstep:2754/10000 train_loss:4.1201 train_time:466324ms step_avg:169.94ms\nstep:2755/10000 train_loss:4.1573 train_time:466494ms step_avg:169.94ms\nstep:2756/10000 train_loss:3.9883 train_time:466666ms step_avg:169.94ms\nstep:2757/10000 train_loss:4.1298 train_time:466838ms step_avg:169.94ms\nstep:2758/10000 train_loss:4.2538 train_time:467009ms step_avg:169.94ms\nstep:2759/10000 train_loss:4.3756 train_time:467178ms step_avg:169.94ms\nstep:2760/10000 train_loss:4.0461 train_time:467348ms step_avg:169.94ms\nstep:2761/10000 train_loss:4.1443 train_time:467517ms step_avg:169.94ms\nstep:2762/10000 train_loss:4.0733 train_time:467686ms step_avg:169.94ms\nstep:2763/10000 train_loss:4.1751 train_time:467854ms step_avg:169.94ms\nstep:2764/10000 train_loss:4.1646 train_time:468024ms step_avg:169.94ms\nstep:2765/10000 train_loss:4.2595 train_time:468194ms step_avg:169.94ms\nstep:2766/10000 train_loss:4.2645 train_time:468365ms step_avg:169.94ms\nstep:2767/10000 train_loss:4.1621 train_time:468534ms step_avg:169.94ms\nstep:2768/10000 train_loss:4.0684 train_time:468706ms step_avg:169.94ms\nstep:2769/10000 train_loss:4.2189 train_time:468875ms step_avg:169.94ms\nstep:2770/10000 train_loss:4.2754 train_time:469047ms step_avg:169.94ms\nstep:2771/10000 train_loss:4.1743 train_time:469215ms step_avg:169.94ms\nstep:2772/10000 train_loss:4.2741 train_time:469386ms step_avg:169.94ms\nstep:2773/10000 train_loss:4.1710 train_time:469556ms step_avg:169.94ms\nstep:2774/10000 train_loss:4.0459 train_time:469727ms step_avg:169.94ms\nstep:2775/10000 train_loss:4.1798 train_time:469896ms step_avg:169.94ms\nstep:2776/10000 train_loss:4.2753 train_time:470067ms step_avg:169.94ms\nstep:2777/10000 train_loss:4.0273 train_time:470238ms step_avg:169.94ms\nstep:2778/10000 train_loss:4.2526 train_time:470412ms step_avg:169.95ms\nstep:2779/10000 train_loss:4.2364 train_time:470583ms step_avg:169.95ms\nstep:2780/10000 train_loss:4.1495 train_time:470755ms step_avg:169.95ms\nstep:2781/10000 train_loss:4.1153 train_time:470927ms step_avg:169.95ms\nstep:2782/10000 train_loss:4.2250 train_time:471096ms step_avg:169.95ms\nstep:2783/10000 train_loss:4.2610 train_time:471264ms step_avg:169.95ms\nstep:2784/10000 train_loss:4.1373 train_time:471432ms step_avg:169.95ms\nstep:2785/10000 train_loss:4.1127 train_time:471604ms step_avg:169.95ms\nstep:2786/10000 train_loss:3.9447 train_time:471772ms step_avg:169.95ms\nstep:2787/10000 train_loss:4.1692 train_time:471944ms step_avg:169.95ms\nstep:2788/10000 train_loss:4.4380 train_time:472114ms step_avg:169.95ms\nstep:2789/10000 train_loss:4.2289 train_time:472284ms step_avg:169.95ms\nstep:2790/10000 train_loss:4.3240 train_time:472452ms step_avg:169.95ms\nstep:2791/10000 train_loss:4.2474 train_time:472624ms step_avg:169.95ms\nstep:2792/10000 train_loss:4.2255 train_time:472795ms step_avg:169.95ms\nstep:2793/10000 train_loss:4.0710 train_time:472972ms step_avg:169.95ms\nstep:2794/10000 train_loss:4.1985 train_time:473136ms step_avg:169.95ms\nstep:2795/10000 train_loss:4.1274 train_time:473306ms step_avg:169.95ms\nstep:2796/10000 train_loss:4.2140 train_time:473475ms step_avg:169.95ms\nstep:2797/10000 train_loss:4.0552 train_time:473646ms step_avg:169.95ms\nstep:2798/10000 train_loss:4.1744 train_time:473816ms step_avg:169.95ms\nstep:2799/10000 train_loss:4.1455 train_time:473989ms step_avg:169.95ms\nstep:2800/10000 train_loss:4.4036 train_time:474158ms step_avg:169.95ms\nstep:2801/10000 train_loss:4.1718 train_time:474328ms step_avg:169.95ms\nstep:2802/10000 train_loss:4.1296 train_time:474496ms step_avg:169.95ms\nstep:2803/10000 train_loss:4.4681 train_time:474665ms step_avg:169.95ms\nstep:2804/10000 train_loss:4.1663 train_time:474833ms step_avg:169.95ms\nstep:2805/10000 train_loss:4.0487 train_time:475006ms step_avg:169.95ms\nstep:2806/10000 train_loss:4.1546 train_time:475176ms step_avg:169.95ms\nstep:2807/10000 train_loss:4.0534 train_time:475351ms step_avg:169.95ms\nstep:2808/10000 train_loss:4.0928 train_time:475517ms step_avg:169.95ms\nstep:2809/10000 train_loss:4.2829 train_time:475684ms step_avg:169.95ms\nstep:2810/10000 train_loss:3.7395 train_time:475855ms step_avg:169.95ms\nstep:2811/10000 train_loss:4.2714 train_time:476026ms step_avg:169.95ms\nstep:2812/10000 train_loss:4.1172 train_time:476195ms step_avg:169.95ms\nstep:2813/10000 train_loss:4.1740 train_time:476368ms step_avg:169.95ms\nstep:2814/10000 train_loss:4.1803 train_time:476537ms step_avg:169.95ms\nstep:2815/10000 train_loss:4.1672 train_time:476707ms step_avg:169.95ms\nstep:2816/10000 train_loss:4.0491 train_time:476878ms step_avg:169.95ms\nstep:2817/10000 train_loss:4.1570 train_time:477048ms step_avg:169.95ms\nstep:2818/10000 train_loss:4.0524 train_time:477215ms step_avg:169.95ms\nstep:2819/10000 train_loss:4.1964 train_time:477384ms step_avg:169.95ms\nstep:2820/10000 train_loss:4.1484 train_time:477556ms step_avg:169.95ms\nstep:2821/10000 train_loss:4.3007 train_time:477725ms step_avg:169.95ms\nstep:2822/10000 train_loss:4.2848 train_time:477895ms step_avg:169.95ms\nstep:2823/10000 train_loss:4.2702 train_time:478067ms step_avg:169.95ms\nstep:2824/10000 train_loss:4.1699 train_time:478238ms step_avg:169.95ms\nstep:2825/10000 train_loss:4.2502 train_time:478409ms step_avg:169.95ms\nstep:2826/10000 train_loss:4.1300 train_time:478581ms step_avg:169.95ms\nstep:2827/10000 train_loss:4.1733 train_time:478752ms step_avg:169.95ms\nstep:2828/10000 train_loss:4.1826 train_time:478923ms step_avg:169.95ms\nstep:2829/10000 train_loss:4.0584 train_time:479092ms step_avg:169.95ms\nstep:2830/10000 train_loss:4.2210 train_time:479264ms step_avg:169.95ms\nstep:2831/10000 train_loss:4.1719 train_time:479431ms step_avg:169.95ms\nstep:2832/10000 train_loss:4.2393 train_time:479600ms step_avg:169.95ms\nstep:2833/10000 train_loss:4.1665 train_time:479767ms step_avg:169.95ms\nstep:2834/10000 train_loss:4.1796 train_time:479938ms step_avg:169.95ms\nstep:2835/10000 train_loss:4.5858 train_time:480109ms step_avg:169.95ms\nstep:2836/10000 train_loss:4.1767 train_time:480280ms step_avg:169.95ms\nstep:2837/10000 train_loss:4.1033 train_time:480447ms step_avg:169.95ms\nstep:2838/10000 train_loss:4.2578 train_time:480618ms step_avg:169.95ms\nstep:2839/10000 train_loss:4.2171 train_time:480788ms step_avg:169.95ms\nstep:2840/10000 train_loss:4.1498 train_time:480957ms step_avg:169.95ms\nstep:2841/10000 train_loss:4.2330 train_time:481127ms step_avg:169.95ms\nstep:2842/10000 train_loss:4.2416 train_time:481298ms step_avg:169.95ms\nstep:2843/10000 train_loss:4.1450 train_time:481466ms step_avg:169.95ms\nstep:2844/10000 train_loss:4.1584 train_time:481635ms step_avg:169.95ms\nstep:2845/10000 train_loss:4.2303 train_time:481804ms step_avg:169.95ms\nstep:2846/10000 train_loss:4.1881 train_time:481974ms step_avg:169.95ms\nstep:2847/10000 train_loss:4.3247 train_time:482145ms step_avg:169.95ms\nstep:2848/10000 train_loss:4.1940 train_time:482313ms step_avg:169.95ms\nstep:2849/10000 train_loss:4.1788 train_time:482485ms step_avg:169.95ms\nstep:2850/10000 train_loss:4.1366 train_time:482656ms step_avg:169.95ms\nstep:2851/10000 train_loss:4.2414 train_time:482828ms step_avg:169.95ms\nstep:2852/10000 train_loss:4.2291 train_time:482997ms step_avg:169.95ms\nstep:2853/10000 train_loss:4.1726 train_time:483167ms step_avg:169.95ms\nstep:2854/10000 train_loss:4.1477 train_time:483337ms step_avg:169.95ms\nstep:2855/10000 train_loss:4.1044 train_time:483509ms step_avg:169.95ms\nstep:2856/10000 train_loss:4.2940 train_time:483679ms step_avg:169.95ms\nstep:2857/10000 train_loss:4.2021 train_time:483848ms step_avg:169.95ms\nstep:2858/10000 train_loss:4.1935 train_time:484016ms step_avg:169.95ms\nstep:2859/10000 train_loss:4.3416 train_time:484186ms step_avg:169.95ms\nstep:2860/10000 train_loss:4.1559 train_time:484353ms step_avg:169.95ms\nstep:2861/10000 train_loss:4.1302 train_time:484524ms step_avg:169.95ms\nstep:2862/10000 train_loss:4.0208 train_time:484694ms step_avg:169.95ms\nstep:2863/10000 train_loss:4.0999 train_time:484864ms step_avg:169.95ms\nstep:2864/10000 train_loss:4.1696 train_time:485032ms step_avg:169.95ms\nstep:2865/10000 train_loss:4.1295 train_time:485204ms step_avg:169.95ms\nstep:2866/10000 train_loss:4.4254 train_time:485371ms step_avg:169.95ms\nstep:2867/10000 train_loss:4.3747 train_time:485543ms step_avg:169.95ms\nstep:2868/10000 train_loss:4.1873 train_time:485712ms step_avg:169.95ms\nstep:2869/10000 train_loss:3.9908 train_time:485882ms step_avg:169.95ms\nstep:2870/10000 train_loss:4.3020 train_time:486051ms step_avg:169.95ms\nstep:2871/10000 train_loss:4.1480 train_time:486222ms step_avg:169.95ms\nstep:2872/10000 train_loss:4.3256 train_time:486392ms step_avg:169.95ms\nstep:2873/10000 train_loss:4.1587 train_time:486563ms step_avg:169.95ms\nstep:2874/10000 train_loss:4.2940 train_time:486732ms step_avg:169.95ms\nstep:2875/10000 train_loss:4.1933 train_time:486901ms step_avg:169.95ms\nstep:2876/10000 train_loss:4.1840 train_time:487071ms step_avg:169.95ms\nstep:2877/10000 train_loss:4.2272 train_time:487242ms step_avg:169.95ms\nstep:2878/10000 train_loss:4.2637 train_time:487411ms step_avg:169.95ms\nstep:2879/10000 train_loss:4.2020 train_time:487581ms step_avg:169.95ms\nstep:2880/10000 train_loss:4.2322 train_time:487752ms step_avg:169.95ms\nstep:2881/10000 train_loss:4.0517 train_time:487923ms step_avg:169.95ms\nstep:2882/10000 train_loss:4.2137 train_time:488092ms step_avg:169.95ms\nstep:2883/10000 train_loss:4.0562 train_time:488263ms step_avg:169.95ms\nstep:2884/10000 train_loss:4.3443 train_time:488431ms step_avg:169.95ms\nstep:2885/10000 train_loss:4.3329 train_time:488601ms step_avg:169.95ms\nstep:2886/10000 train_loss:4.0809 train_time:488770ms step_avg:169.95ms\nstep:2887/10000 train_loss:4.1537 train_time:488943ms step_avg:169.95ms\nstep:2888/10000 train_loss:4.0389 train_time:489113ms step_avg:169.95ms\nstep:2889/10000 train_loss:4.2565 train_time:489285ms step_avg:169.95ms\nstep:2890/10000 train_loss:4.1921 train_time:489456ms step_avg:169.95ms\nstep:2891/10000 train_loss:4.3684 train_time:489626ms step_avg:169.95ms\nstep:2892/10000 train_loss:4.3168 train_time:489796ms step_avg:169.95ms\nstep:2893/10000 train_loss:4.1502 train_time:489968ms step_avg:169.95ms\nstep:2894/10000 train_loss:4.1684 train_time:490137ms step_avg:169.95ms\nstep:2895/10000 train_loss:4.3216 train_time:490311ms step_avg:169.95ms\nstep:2896/10000 train_loss:4.3569 train_time:490480ms step_avg:169.95ms\nstep:2897/10000 train_loss:4.2121 train_time:490652ms step_avg:169.95ms\nstep:2898/10000 train_loss:4.1939 train_time:490822ms step_avg:169.95ms\nstep:2899/10000 train_loss:3.9728 train_time:490992ms step_avg:169.95ms\nstep:2900/10000 train_loss:4.1499 train_time:491161ms step_avg:169.95ms\nstep:2901/10000 train_loss:4.1431 train_time:491335ms step_avg:169.95ms\nstep:2902/10000 train_loss:4.1883 train_time:491504ms step_avg:169.95ms\nstep:2903/10000 train_loss:4.2164 train_time:491673ms step_avg:169.95ms\nstep:2904/10000 train_loss:4.2286 train_time:491848ms step_avg:169.95ms\nstep:2905/10000 train_loss:4.1130 train_time:492012ms step_avg:169.95ms\nstep:2906/10000 train_loss:4.0716 train_time:492183ms step_avg:169.95ms\nstep:2907/10000 train_loss:4.0006 train_time:492354ms step_avg:169.95ms\nstep:2908/10000 train_loss:4.1446 train_time:492525ms step_avg:169.95ms\nstep:2909/10000 train_loss:4.0771 train_time:492692ms step_avg:169.95ms\nstep:2910/10000 train_loss:4.2333 train_time:492864ms step_avg:169.95ms\nstep:2911/10000 train_loss:4.2286 train_time:493035ms step_avg:169.95ms\nstep:2912/10000 train_loss:4.1064 train_time:493205ms step_avg:169.95ms\nstep:2913/10000 train_loss:4.2146 train_time:493373ms step_avg:169.95ms\nstep:2914/10000 train_loss:4.1653 train_time:493545ms step_avg:169.95ms\nstep:2915/10000 train_loss:4.1593 train_time:493712ms step_avg:169.95ms\nstep:2916/10000 train_loss:4.3014 train_time:493885ms step_avg:169.95ms\nstep:2917/10000 train_loss:4.1644 train_time:494057ms step_avg:169.95ms\nstep:2918/10000 train_loss:3.9664 train_time:494225ms step_avg:169.95ms\nstep:2919/10000 train_loss:4.0138 train_time:494393ms step_avg:169.95ms\nstep:2920/10000 train_loss:4.2919 train_time:494563ms step_avg:169.95ms\nstep:2921/10000 train_loss:4.1957 train_time:494731ms step_avg:169.95ms\nstep:2922/10000 train_loss:4.1087 train_time:494902ms step_avg:169.95ms\nstep:2923/10000 train_loss:4.2200 train_time:495070ms step_avg:169.95ms\nstep:2924/10000 train_loss:4.2950 train_time:495244ms step_avg:169.95ms\nstep:2925/10000 train_loss:4.1978 train_time:495413ms step_avg:169.95ms\nstep:2926/10000 train_loss:4.2346 train_time:495585ms step_avg:169.95ms\nstep:2927/10000 train_loss:4.2402 train_time:495753ms step_avg:169.95ms\nstep:2928/10000 train_loss:4.1407 train_time:495925ms step_avg:169.95ms\nstep:2929/10000 train_loss:4.2209 train_time:496095ms step_avg:169.95ms\nstep:2930/10000 train_loss:4.2103 train_time:496267ms step_avg:169.95ms\nstep:2931/10000 train_loss:4.2235 train_time:496434ms step_avg:169.95ms\nstep:2932/10000 train_loss:4.1817 train_time:496606ms step_avg:169.95ms\nstep:2933/10000 train_loss:4.1157 train_time:496774ms step_avg:169.95ms\nstep:2934/10000 train_loss:4.2870 train_time:496946ms step_avg:169.95ms\nstep:2935/10000 train_loss:4.3379 train_time:497113ms step_avg:169.95ms\nstep:2936/10000 train_loss:4.4098 train_time:497282ms step_avg:169.95ms\nstep:2937/10000 train_loss:4.0996 train_time:497451ms step_avg:169.95ms\nstep:2938/10000 train_loss:4.1161 train_time:497621ms step_avg:169.95ms\nstep:2939/10000 train_loss:4.2302 train_time:497789ms step_avg:169.95ms\nstep:2940/10000 train_loss:4.1298 train_time:497958ms step_avg:169.95ms\nstep:2941/10000 train_loss:4.2347 train_time:498131ms step_avg:169.95ms\nstep:2942/10000 train_loss:4.2134 train_time:498299ms step_avg:169.95ms\nstep:2943/10000 train_loss:4.1617 train_time:498468ms step_avg:169.95ms\nstep:2944/10000 train_loss:4.2309 train_time:498639ms step_avg:169.95ms\nstep:2945/10000 train_loss:4.2290 train_time:498812ms step_avg:169.95ms\nstep:2946/10000 train_loss:4.2042 train_time:498982ms step_avg:169.95ms\nstep:2947/10000 train_loss:4.1570 train_time:499152ms step_avg:169.95ms\nstep:2948/10000 train_loss:4.3319 train_time:499321ms step_avg:169.95ms\nstep:2949/10000 train_loss:4.0581 train_time:499492ms step_avg:169.95ms\nstep:2950/10000 train_loss:4.2620 train_time:499666ms step_avg:169.95ms\nstep:2951/10000 train_loss:4.0575 train_time:499834ms step_avg:169.95ms\nstep:2952/10000 train_loss:4.2207 train_time:500004ms step_avg:169.95ms\nstep:2953/10000 train_loss:4.1685 train_time:500173ms step_avg:169.95ms\nstep:2954/10000 train_loss:4.1863 train_time:500346ms step_avg:169.95ms\nstep:2955/10000 train_loss:4.2494 train_time:500512ms step_avg:169.95ms\nstep:2956/10000 train_loss:4.1897 train_time:500682ms step_avg:169.95ms\nstep:2957/10000 train_loss:4.1224 train_time:500854ms step_avg:169.95ms\nstep:2958/10000 train_loss:4.2016 train_time:501026ms step_avg:169.95ms\nstep:2959/10000 train_loss:4.1327 train_time:501197ms step_avg:169.96ms\nstep:2960/10000 train_loss:4.0803 train_time:501368ms step_avg:169.96ms\nstep:2961/10000 train_loss:4.1919 train_time:501538ms step_avg:169.96ms\nstep:2962/10000 train_loss:4.2261 train_time:501708ms step_avg:169.96ms\nstep:2963/10000 train_loss:4.0959 train_time:501877ms step_avg:169.95ms\nstep:2964/10000 train_loss:4.7103 train_time:502047ms step_avg:169.95ms\nstep:2965/10000 train_loss:4.2479 train_time:502217ms step_avg:169.96ms\nstep:2966/10000 train_loss:4.1987 train_time:502389ms step_avg:169.96ms\nstep:2967/10000 train_loss:4.2136 train_time:502558ms step_avg:169.96ms\nstep:2968/10000 train_loss:4.0968 train_time:502729ms step_avg:169.96ms\nstep:2969/10000 train_loss:4.2164 train_time:502899ms step_avg:169.96ms\nstep:2970/10000 train_loss:4.2229 train_time:503067ms step_avg:169.96ms\nstep:2971/10000 train_loss:4.2543 train_time:503238ms step_avg:169.96ms\nstep:2972/10000 train_loss:4.1858 train_time:503410ms step_avg:169.96ms\nstep:2973/10000 train_loss:4.3536 train_time:503576ms step_avg:169.95ms\nstep:2974/10000 train_loss:4.1000 train_time:503747ms step_avg:169.96ms\nstep:2975/10000 train_loss:4.1782 train_time:503917ms step_avg:169.96ms\nstep:2976/10000 train_loss:4.1493 train_time:504089ms step_avg:169.96ms\nstep:2977/10000 train_loss:4.2256 train_time:504260ms step_avg:169.96ms\nstep:2978/10000 train_loss:4.3426 train_time:504431ms step_avg:169.96ms\nstep:2979/10000 train_loss:4.1353 train_time:504600ms step_avg:169.96ms\nstep:2980/10000 train_loss:4.2062 train_time:504768ms step_avg:169.96ms\nstep:2981/10000 train_loss:4.0939 train_time:504938ms step_avg:169.96ms\nstep:2982/10000 train_loss:4.1253 train_time:505108ms step_avg:169.96ms\nstep:2983/10000 train_loss:3.9333 train_time:505281ms step_avg:169.96ms\nstep:2984/10000 train_loss:4.2412 train_time:505453ms step_avg:169.96ms\nstep:2985/10000 train_loss:4.1432 train_time:505623ms step_avg:169.96ms\nstep:2986/10000 train_loss:4.1578 train_time:505791ms step_avg:169.96ms\nstep:2987/10000 train_loss:4.0600 train_time:505961ms step_avg:169.96ms\nstep:2988/10000 train_loss:4.1823 train_time:506128ms step_avg:169.96ms\nstep:2989/10000 train_loss:4.2119 train_time:506301ms step_avg:169.96ms\nstep:2990/10000 train_loss:4.2770 train_time:506472ms step_avg:169.96ms\nstep:2991/10000 train_loss:4.1194 train_time:506640ms step_avg:169.96ms\nstep:2992/10000 train_loss:4.2060 train_time:506808ms step_avg:169.96ms\nstep:2993/10000 train_loss:4.1952 train_time:506977ms step_avg:169.96ms\nstep:2994/10000 train_loss:4.1447 train_time:507148ms step_avg:169.96ms\nstep:2995/10000 train_loss:4.1715 train_time:507317ms step_avg:169.96ms\nstep:2996/10000 train_loss:4.1242 train_time:507487ms step_avg:169.96ms\nstep:2997/10000 train_loss:4.2619 train_time:507657ms step_avg:169.96ms\nstep:2998/10000 train_loss:4.2248 train_time:507829ms step_avg:169.96ms\nstep:2999/10000 train_loss:4.3066 train_time:507998ms step_avg:169.96ms\nstep:3000/10000 train_loss:4.1474 train_time:508170ms step_avg:169.96ms\nstep:3001/10000 train_loss:4.0627 train_time:508341ms step_avg:169.96ms\nstep:3002/10000 train_loss:4.1077 train_time:508512ms step_avg:169.96ms\nstep:3003/10000 train_loss:4.2185 train_time:508683ms step_avg:169.96ms\nstep:3004/10000 train_loss:4.1890 train_time:508851ms step_avg:169.96ms\nstep:3005/10000 train_loss:4.1356 train_time:509020ms step_avg:169.96ms\nstep:3006/10000 train_loss:4.2351 train_time:509195ms step_avg:169.96ms\nstep:3007/10000 train_loss:4.0765 train_time:509358ms step_avg:169.96ms\nstep:3008/10000 train_loss:4.1483 train_time:509528ms step_avg:169.96ms\nstep:3009/10000 train_loss:4.2429 train_time:509698ms step_avg:169.96ms\nstep:3010/10000 train_loss:4.0794 train_time:509868ms step_avg:169.96ms\nstep:3011/10000 train_loss:4.1099 train_time:510035ms step_avg:169.96ms\nstep:3012/10000 train_loss:4.1583 train_time:510208ms step_avg:169.96ms\nstep:3013/10000 train_loss:4.1382 train_time:510378ms step_avg:169.96ms\nstep:3014/10000 train_loss:4.1398 train_time:510550ms step_avg:169.96ms\nstep:3015/10000 train_loss:4.1974 train_time:510722ms step_avg:169.96ms\nstep:3016/10000 train_loss:4.2029 train_time:510893ms step_avg:169.96ms\nstep:3017/10000 train_loss:4.1146 train_time:511063ms step_avg:169.96ms\nstep:3018/10000 train_loss:4.1072 train_time:511233ms step_avg:169.96ms\nstep:3019/10000 train_loss:4.2789 train_time:511405ms step_avg:169.96ms\nstep:3020/10000 train_loss:4.1508 train_time:511574ms step_avg:169.96ms\nstep:3021/10000 train_loss:4.3203 train_time:511745ms step_avg:169.96ms\nstep:3022/10000 train_loss:4.1604 train_time:511915ms step_avg:169.96ms\nstep:3023/10000 train_loss:4.0123 train_time:512085ms step_avg:169.96ms\nstep:3024/10000 train_loss:4.2403 train_time:512254ms step_avg:169.96ms\nstep:3025/10000 train_loss:4.2177 train_time:512426ms step_avg:169.96ms\nstep:3026/10000 train_loss:4.0670 train_time:512596ms step_avg:169.96ms\nstep:3027/10000 train_loss:4.2445 train_time:512778ms step_avg:169.96ms\nstep:3028/10000 train_loss:4.0872 train_time:512940ms step_avg:169.96ms\nstep:3029/10000 train_loss:4.0685 train_time:513109ms step_avg:169.96ms\nstep:3030/10000 train_loss:4.3351 train_time:513278ms step_avg:169.96ms\nstep:3031/10000 train_loss:4.2876 train_time:513446ms step_avg:169.96ms\nstep:3032/10000 train_loss:4.2846 train_time:513614ms step_avg:169.96ms\nstep:3033/10000 train_loss:4.1059 train_time:513787ms step_avg:169.96ms\nstep:3034/10000 train_loss:4.4191 train_time:513956ms step_avg:169.96ms\nstep:3035/10000 train_loss:3.8838 train_time:514126ms step_avg:169.96ms\nstep:3036/10000 train_loss:4.2128 train_time:514294ms step_avg:169.96ms\nstep:3037/10000 train_loss:4.1619 train_time:514467ms step_avg:169.96ms\nstep:3038/10000 train_loss:4.1612 train_time:514634ms step_avg:169.96ms\nstep:3039/10000 train_loss:4.1249 train_time:514808ms step_avg:169.96ms\nstep:3040/10000 train_loss:4.1310 train_time:514979ms step_avg:169.96ms\nstep:3041/10000 train_loss:4.1601 train_time:515152ms step_avg:169.96ms\nstep:3042/10000 train_loss:4.1031 train_time:515324ms step_avg:169.96ms\nstep:3043/10000 train_loss:4.2015 train_time:515494ms step_avg:169.96ms\nstep:3044/10000 train_loss:4.2359 train_time:515664ms step_avg:169.96ms\nstep:3045/10000 train_loss:4.1337 train_time:515833ms step_avg:169.96ms\nstep:3046/10000 train_loss:4.2790 train_time:516004ms step_avg:169.96ms\nstep:3047/10000 train_loss:4.2118 train_time:516175ms step_avg:169.96ms\nstep:3048/10000 train_loss:4.1833 train_time:516561ms step_avg:170.03ms\nstep:3049/10000 train_loss:4.1665 train_time:516724ms step_avg:170.03ms\nstep:3050/10000 train_loss:3.9856 train_time:516892ms step_avg:170.03ms\nstep:3051/10000 train_loss:4.1281 train_time:517059ms step_avg:170.03ms\nstep:3052/10000 train_loss:4.0703 train_time:517228ms step_avg:170.03ms\nstep:3053/10000 train_loss:4.0943 train_time:517395ms step_avg:170.03ms\nstep:3054/10000 train_loss:4.1498 train_time:517574ms step_avg:170.03ms\nstep:3055/10000 train_loss:4.2564 train_time:517746ms step_avg:170.03ms\nstep:3056/10000 train_loss:3.9682 train_time:517914ms step_avg:170.03ms\nstep:3057/10000 train_loss:4.1487 train_time:518081ms step_avg:170.03ms\nstep:3058/10000 train_loss:4.2497 train_time:518248ms step_avg:170.03ms\nstep:3059/10000 train_loss:4.1375 train_time:518419ms step_avg:170.03ms\nstep:3060/10000 train_loss:4.1372 train_time:518593ms step_avg:170.03ms\nstep:3061/10000 train_loss:4.1983 train_time:518763ms step_avg:170.03ms\nstep:3062/10000 train_loss:3.9143 train_time:518932ms step_avg:170.03ms\nstep:3063/10000 train_loss:4.1741 train_time:519106ms step_avg:170.03ms\nstep:3064/10000 train_loss:4.1427 train_time:519269ms step_avg:170.03ms\nstep:3065/10000 train_loss:4.1236 train_time:519443ms step_avg:170.03ms\nstep:3066/10000 train_loss:4.3161 train_time:519614ms step_avg:170.03ms\nstep:3067/10000 train_loss:4.1501 train_time:519785ms step_avg:170.03ms\nstep:3068/10000 train_loss:5.1184 train_time:519953ms step_avg:170.03ms\nstep:3069/10000 train_loss:4.0961 train_time:520122ms step_avg:170.03ms\nstep:3070/10000 train_loss:4.2181 train_time:520292ms step_avg:170.03ms\nstep:3071/10000 train_loss:4.1706 train_time:520464ms step_avg:170.03ms\nstep:3072/10000 train_loss:4.1079 train_time:520636ms step_avg:170.03ms\nstep:3073/10000 train_loss:4.1612 train_time:520810ms step_avg:170.03ms\nstep:3074/10000 train_loss:4.1358 train_time:520981ms step_avg:170.03ms\nstep:3075/10000 train_loss:4.3629 train_time:521151ms step_avg:170.03ms\nstep:3076/10000 train_loss:4.1989 train_time:521322ms step_avg:170.03ms\nstep:3077/10000 train_loss:4.2008 train_time:521491ms step_avg:170.03ms\nstep:3078/10000 train_loss:4.1552 train_time:521662ms step_avg:170.03ms\nstep:3079/10000 train_loss:4.2236 train_time:521830ms step_avg:170.03ms\nstep:3080/10000 train_loss:4.2102 train_time:522000ms step_avg:170.03ms\nstep:3081/10000 train_loss:4.0786 train_time:522169ms step_avg:170.03ms\nstep:3082/10000 train_loss:4.1852 train_time:522341ms step_avg:170.03ms\nstep:3083/10000 train_loss:4.1537 train_time:522515ms step_avg:170.03ms\nstep:3084/10000 train_loss:4.7357 train_time:522685ms step_avg:170.03ms\nstep:3085/10000 train_loss:4.0300 train_time:522853ms step_avg:170.03ms\nstep:3086/10000 train_loss:4.2544 train_time:523024ms step_avg:170.03ms\nstep:3087/10000 train_loss:3.9526 train_time:523191ms step_avg:170.03ms\nstep:3088/10000 train_loss:4.1917 train_time:523362ms step_avg:170.03ms\nstep:3089/10000 train_loss:4.2738 train_time:523531ms step_avg:170.03ms\nstep:3090/10000 train_loss:4.4542 train_time:523701ms step_avg:170.03ms\nstep:3091/10000 train_loss:4.1834 train_time:523871ms step_avg:170.03ms\nstep:3092/10000 train_loss:4.1544 train_time:524043ms step_avg:170.03ms\nstep:3093/10000 train_loss:4.1874 train_time:524212ms step_avg:170.03ms\nstep:3094/10000 train_loss:4.0200 train_time:524382ms step_avg:170.03ms\nstep:3095/10000 train_loss:4.0385 train_time:524553ms step_avg:170.03ms\nstep:3096/10000 train_loss:4.1964 train_time:524724ms step_avg:170.03ms\nstep:3097/10000 train_loss:4.1522 train_time:524893ms step_avg:170.03ms\nstep:3098/10000 train_loss:4.2575 train_time:525064ms step_avg:170.03ms\nstep:3099/10000 train_loss:4.1016 train_time:525234ms step_avg:170.03ms\nstep:3100/10000 train_loss:4.3032 train_time:525406ms step_avg:170.03ms\nstep:3101/10000 train_loss:4.0129 train_time:525576ms step_avg:170.03ms\nstep:3102/10000 train_loss:4.2037 train_time:525750ms step_avg:170.04ms\nstep:3103/10000 train_loss:4.4940 train_time:525923ms step_avg:170.04ms\nstep:3104/10000 train_loss:4.1154 train_time:526093ms step_avg:170.04ms\nstep:3105/10000 train_loss:3.9536 train_time:526263ms step_avg:170.04ms\nstep:3106/10000 train_loss:4.1646 train_time:526431ms step_avg:170.04ms\nstep:3107/10000 train_loss:4.0091 train_time:526602ms step_avg:170.04ms\nstep:3108/10000 train_loss:4.1881 train_time:526768ms step_avg:170.03ms\nstep:3109/10000 train_loss:4.1756 train_time:526942ms step_avg:170.04ms\nstep:3110/10000 train_loss:4.0229 train_time:527115ms step_avg:170.04ms\nstep:3111/10000 train_loss:4.1354 train_time:527287ms step_avg:170.04ms\nstep:3112/10000 train_loss:4.2012 train_time:527455ms step_avg:170.04ms\nstep:3113/10000 train_loss:4.1028 train_time:527623ms step_avg:170.04ms\nstep:3114/10000 train_loss:3.9892 train_time:527795ms step_avg:170.04ms\nstep:3115/10000 train_loss:4.2248 train_time:527967ms step_avg:170.04ms\nstep:3116/10000 train_loss:4.1593 train_time:528137ms step_avg:170.04ms\nstep:3117/10000 train_loss:4.0288 train_time:528309ms step_avg:170.04ms\nstep:3118/10000 train_loss:4.0489 train_time:528480ms step_avg:170.04ms\nstep:3119/10000 train_loss:4.2141 train_time:528648ms step_avg:170.04ms\nstep:3120/10000 train_loss:4.1356 train_time:528818ms step_avg:170.04ms\nstep:3121/10000 train_loss:4.2403 train_time:528988ms step_avg:170.04ms\nstep:3122/10000 train_loss:4.1124 train_time:529157ms step_avg:170.04ms\nstep:3123/10000 train_loss:4.1370 train_time:529330ms step_avg:170.04ms\nstep:3124/10000 train_loss:3.9991 train_time:529500ms step_avg:170.04ms\nstep:3125/10000 train_loss:4.1778 train_time:529669ms step_avg:170.04ms\nstep:3126/10000 train_loss:4.1674 train_time:529839ms step_avg:170.04ms\nstep:3127/10000 train_loss:4.1219 train_time:530008ms step_avg:170.04ms\nstep:3128/10000 train_loss:4.1449 train_time:530178ms step_avg:170.04ms\nstep:3129/10000 train_loss:4.1584 train_time:530349ms step_avg:170.04ms\nstep:3130/10000 train_loss:4.2296 train_time:530521ms step_avg:170.04ms\nstep:3131/10000 train_loss:3.9605 train_time:530689ms step_avg:170.04ms\nstep:3132/10000 train_loss:4.1488 train_time:530860ms step_avg:170.04ms\nstep:3133/10000 train_loss:4.0844 train_time:531027ms step_avg:170.04ms\nstep:3134/10000 train_loss:4.0952 train_time:531199ms step_avg:170.04ms\nstep:3135/10000 train_loss:4.0909 train_time:531372ms step_avg:170.04ms\nstep:3136/10000 train_loss:4.1245 train_time:531541ms step_avg:170.04ms\nstep:3137/10000 train_loss:4.2066 train_time:531712ms step_avg:170.04ms\nstep:3138/10000 train_loss:4.1618 train_time:531883ms step_avg:170.04ms\nstep:3139/10000 train_loss:4.0930 train_time:532053ms step_avg:170.04ms\nstep:3140/10000 train_loss:4.0887 train_time:532229ms step_avg:170.04ms\nstep:3141/10000 train_loss:4.0613 train_time:532398ms step_avg:170.04ms\nstep:3142/10000 train_loss:4.1038 train_time:532567ms step_avg:170.04ms\nstep:3143/10000 train_loss:4.0279 train_time:532729ms step_avg:170.04ms\nstep:3144/10000 train_loss:4.1980 train_time:532899ms step_avg:170.04ms\nstep:3145/10000 train_loss:4.1583 train_time:533067ms step_avg:170.04ms\nstep:3146/10000 train_loss:4.1281 train_time:533238ms step_avg:170.04ms\nstep:3147/10000 train_loss:4.1362 train_time:533411ms step_avg:170.04ms\nstep:3148/10000 train_loss:4.0765 train_time:533582ms step_avg:170.04ms\nstep:3149/10000 train_loss:3.9560 train_time:533752ms step_avg:170.04ms\nstep:3150/10000 train_loss:4.0267 train_time:533924ms step_avg:170.04ms\nstep:3151/10000 train_loss:4.1929 train_time:534091ms step_avg:170.04ms\nstep:3152/10000 train_loss:4.1475 train_time:534260ms step_avg:170.04ms\nstep:3153/10000 train_loss:4.2401 train_time:534430ms step_avg:170.04ms\nstep:3154/10000 train_loss:4.2969 train_time:534602ms step_avg:170.04ms\nstep:3155/10000 train_loss:4.1031 train_time:534774ms step_avg:170.04ms\nstep:3156/10000 train_loss:4.1441 train_time:534944ms step_avg:170.04ms\nstep:3157/10000 train_loss:4.1662 train_time:535114ms step_avg:170.04ms\nstep:3158/10000 train_loss:4.0760 train_time:535284ms step_avg:170.04ms\nstep:3159/10000 train_loss:4.0658 train_time:535455ms step_avg:170.04ms\nstep:3160/10000 train_loss:4.0974 train_time:535625ms step_avg:170.04ms\nstep:3161/10000 train_loss:4.1165 train_time:535796ms step_avg:170.04ms\nstep:3162/10000 train_loss:4.1465 train_time:535968ms step_avg:170.04ms\nstep:3163/10000 train_loss:4.1334 train_time:536137ms step_avg:170.04ms\nstep:3164/10000 train_loss:4.1153 train_time:536305ms step_avg:170.04ms\nstep:3165/10000 train_loss:4.5076 train_time:536473ms step_avg:170.04ms\nstep:3166/10000 train_loss:4.1907 train_time:536644ms step_avg:170.04ms\nstep:3167/10000 train_loss:4.0517 train_time:536813ms step_avg:170.04ms\nstep:3168/10000 train_loss:4.1764 train_time:536984ms step_avg:170.04ms\nstep:3169/10000 train_loss:4.1855 train_time:537154ms step_avg:170.04ms\nstep:3170/10000 train_loss:4.3103 train_time:537326ms step_avg:170.04ms\nstep:3171/10000 train_loss:4.3391 train_time:537496ms step_avg:170.04ms\nstep:3172/10000 train_loss:4.1938 train_time:537666ms step_avg:170.04ms\nstep:3173/10000 train_loss:4.0746 train_time:537836ms step_avg:170.04ms\nstep:3174/10000 train_loss:4.3324 train_time:538007ms step_avg:170.04ms\nstep:3175/10000 train_loss:4.3142 train_time:538176ms step_avg:170.04ms\nstep:3176/10000 train_loss:4.1797 train_time:538347ms step_avg:170.04ms\nstep:3177/10000 train_loss:4.2489 train_time:538516ms step_avg:170.04ms\nstep:3178/10000 train_loss:4.0943 train_time:538688ms step_avg:170.04ms\nstep:3179/10000 train_loss:4.0229 train_time:538858ms step_avg:170.04ms\nstep:3180/10000 train_loss:4.1199 train_time:539032ms step_avg:170.04ms\nstep:3181/10000 train_loss:4.1542 train_time:539201ms step_avg:170.04ms\nstep:3182/10000 train_loss:4.1623 train_time:539368ms step_avg:170.04ms\nstep:3183/10000 train_loss:4.1093 train_time:539538ms step_avg:170.04ms\nstep:3184/10000 train_loss:4.2051 train_time:539706ms step_avg:170.04ms\nstep:3185/10000 train_loss:5.1851 train_time:539876ms step_avg:170.04ms\nstep:3186/10000 train_loss:4.2362 train_time:540047ms step_avg:170.04ms\nstep:3187/10000 train_loss:4.1359 train_time:540217ms step_avg:170.04ms\nstep:3188/10000 train_loss:4.1818 train_time:540393ms step_avg:170.04ms\nstep:3189/10000 train_loss:4.1261 train_time:540556ms step_avg:170.04ms\nstep:3190/10000 train_loss:4.3436 train_time:540727ms step_avg:170.04ms\nstep:3191/10000 train_loss:4.5343 train_time:540895ms step_avg:170.04ms\nstep:3192/10000 train_loss:3.9678 train_time:541068ms step_avg:170.04ms\nstep:3193/10000 train_loss:4.1226 train_time:541235ms step_avg:170.04ms\nstep:3194/10000 train_loss:4.0915 train_time:541405ms step_avg:170.04ms\nstep:3195/10000 train_loss:4.2701 train_time:541573ms step_avg:170.04ms\nstep:3196/10000 train_loss:4.0390 train_time:541743ms step_avg:170.04ms\nstep:3197/10000 train_loss:4.1336 train_time:541914ms step_avg:170.04ms\nstep:3198/10000 train_loss:4.2302 train_time:542086ms step_avg:170.04ms\nstep:3199/10000 train_loss:4.2695 train_time:542255ms step_avg:170.04ms\nstep:3200/10000 train_loss:4.7347 train_time:542429ms step_avg:170.04ms\nstep:3201/10000 train_loss:4.0760 train_time:542599ms step_avg:170.04ms\nstep:3202/10000 train_loss:4.1413 train_time:542768ms step_avg:170.04ms\nstep:3203/10000 train_loss:4.2484 train_time:542937ms step_avg:170.04ms\nstep:3204/10000 train_loss:4.2673 train_time:543110ms step_avg:170.04ms\nstep:3205/10000 train_loss:4.1457 train_time:543281ms step_avg:170.04ms\nstep:3206/10000 train_loss:4.1413 train_time:543451ms step_avg:170.04ms\nstep:3207/10000 train_loss:4.0583 train_time:543621ms step_avg:170.04ms\nstep:3208/10000 train_loss:4.1470 train_time:543790ms step_avg:170.04ms\nstep:3209/10000 train_loss:4.2143 train_time:543958ms step_avg:170.04ms\nstep:3210/10000 train_loss:4.2382 train_time:544129ms step_avg:170.04ms\nstep:3211/10000 train_loss:4.3617 train_time:544297ms step_avg:170.04ms\nstep:3212/10000 train_loss:4.1836 train_time:544466ms step_avg:170.04ms\nstep:3213/10000 train_loss:4.3171 train_time:544636ms step_avg:170.04ms\nstep:3214/10000 train_loss:4.0726 train_time:544809ms step_avg:170.04ms\nstep:3215/10000 train_loss:4.1534 train_time:544979ms step_avg:170.04ms\nstep:3216/10000 train_loss:4.1647 train_time:545150ms step_avg:170.04ms\nstep:3217/10000 train_loss:4.0897 train_time:545319ms step_avg:170.04ms\nstep:3218/10000 train_loss:4.1272 train_time:545485ms step_avg:170.04ms\nstep:3219/10000 train_loss:4.1220 train_time:545656ms step_avg:170.04ms\nstep:3220/10000 train_loss:4.0888 train_time:545827ms step_avg:170.04ms\nstep:3221/10000 train_loss:4.0875 train_time:545997ms step_avg:170.04ms\nstep:3222/10000 train_loss:4.2116 train_time:546170ms step_avg:170.04ms\nstep:3223/10000 train_loss:4.1057 train_time:546337ms step_avg:170.04ms\nstep:3224/10000 train_loss:4.0189 train_time:546508ms step_avg:170.04ms\nstep:3225/10000 train_loss:4.0983 train_time:546680ms step_avg:170.04ms\nstep:3226/10000 train_loss:4.1077 train_time:546852ms step_avg:170.04ms\nstep:3227/10000 train_loss:4.2407 train_time:547025ms step_avg:170.04ms\nstep:3228/10000 train_loss:4.1347 train_time:547196ms step_avg:170.04ms\nstep:3229/10000 train_loss:4.2106 train_time:547366ms step_avg:170.04ms\nstep:3230/10000 train_loss:4.0957 train_time:547535ms step_avg:170.04ms\nstep:3231/10000 train_loss:4.0439 train_time:547703ms step_avg:170.04ms\nstep:3232/10000 train_loss:4.1770 train_time:547872ms step_avg:170.04ms\nstep:3233/10000 train_loss:4.2310 train_time:548044ms step_avg:170.04ms\nstep:3234/10000 train_loss:4.2226 train_time:548212ms step_avg:170.04ms\nstep:3235/10000 train_loss:4.0327 train_time:548385ms step_avg:170.04ms\nstep:3236/10000 train_loss:4.3975 train_time:548552ms step_avg:170.04ms\nstep:3237/10000 train_loss:4.1266 train_time:548721ms step_avg:170.04ms\nstep:3238/10000 train_loss:4.1344 train_time:548891ms step_avg:170.04ms\nstep:3239/10000 train_loss:4.1623 train_time:549064ms step_avg:170.04ms\nstep:3240/10000 train_loss:4.2407 train_time:549240ms step_avg:170.04ms\nstep:3241/10000 train_loss:4.2164 train_time:549404ms step_avg:170.04ms\nstep:3242/10000 train_loss:4.0355 train_time:549572ms step_avg:170.04ms\nstep:3243/10000 train_loss:4.5801 train_time:549742ms step_avg:170.04ms\nstep:3244/10000 train_loss:4.1017 train_time:549911ms step_avg:170.04ms\nstep:3245/10000 train_loss:4.1951 train_time:550084ms step_avg:170.04ms\nstep:3246/10000 train_loss:4.1343 train_time:550254ms step_avg:170.04ms\nstep:3247/10000 train_loss:4.0721 train_time:550427ms step_avg:170.04ms\nstep:3248/10000 train_loss:4.1680 train_time:550592ms step_avg:170.04ms\nstep:3249/10000 train_loss:4.1760 train_time:550763ms step_avg:170.04ms\nstep:3250/10000 train_loss:4.1811 train_time:550931ms step_avg:170.04ms\nstep:3251/10000 train_loss:4.1768 train_time:551102ms step_avg:170.04ms\nstep:3252/10000 train_loss:4.1446 train_time:551271ms step_avg:170.04ms\nstep:3253/10000 train_loss:4.3475 train_time:551443ms step_avg:170.04ms\nstep:3254/10000 train_loss:4.1407 train_time:551611ms step_avg:170.04ms\nstep:3255/10000 train_loss:4.1953 train_time:551781ms step_avg:170.04ms\nstep:3256/10000 train_loss:4.1649 train_time:551949ms step_avg:170.04ms\nstep:3257/10000 train_loss:4.1271 train_time:552120ms step_avg:170.04ms\nstep:3258/10000 train_loss:4.2040 train_time:552287ms step_avg:170.04ms\nstep:3259/10000 train_loss:4.1324 train_time:552456ms step_avg:170.04ms\nstep:3260/10000 train_loss:3.9956 train_time:552627ms step_avg:170.04ms\nstep:3261/10000 train_loss:4.3617 train_time:552797ms step_avg:170.04ms\nstep:3262/10000 train_loss:4.1556 train_time:552968ms step_avg:170.04ms\nstep:3263/10000 train_loss:4.3411 train_time:553137ms step_avg:170.04ms\nstep:3264/10000 train_loss:4.3378 train_time:553307ms step_avg:170.04ms\nstep:3265/10000 train_loss:4.1056 train_time:553476ms step_avg:170.04ms\nstep:3266/10000 train_loss:4.1315 train_time:553648ms step_avg:170.04ms\nstep:3267/10000 train_loss:4.0850 train_time:553816ms step_avg:170.04ms\nstep:3268/10000 train_loss:4.0987 train_time:553987ms step_avg:170.04ms\nstep:3269/10000 train_loss:4.1998 train_time:554155ms step_avg:170.04ms\nstep:3270/10000 train_loss:4.1869 train_time:554325ms step_avg:170.04ms\nstep:3271/10000 train_loss:4.0804 train_time:554493ms step_avg:170.04ms\nstep:3272/10000 train_loss:4.0857 train_time:554663ms step_avg:170.04ms\nstep:3273/10000 train_loss:4.0831 train_time:554831ms step_avg:170.04ms\nstep:3274/10000 train_loss:4.2898 train_time:555002ms step_avg:170.04ms\nstep:3275/10000 train_loss:4.0834 train_time:555172ms step_avg:170.04ms\nstep:3276/10000 train_loss:4.0820 train_time:555343ms step_avg:170.04ms\nstep:3277/10000 train_loss:4.1411 train_time:555513ms step_avg:170.04ms\nstep:3278/10000 train_loss:4.1131 train_time:555681ms step_avg:170.04ms\nstep:3279/10000 train_loss:4.1098 train_time:555851ms step_avg:170.04ms\nstep:3280/10000 train_loss:4.1377 train_time:556021ms step_avg:170.04ms\nstep:3281/10000 train_loss:4.0758 train_time:556190ms step_avg:170.04ms\nstep:3282/10000 train_loss:4.2973 train_time:556360ms step_avg:170.04ms\nstep:3283/10000 train_loss:4.1449 train_time:556530ms step_avg:170.04ms\nstep:3284/10000 train_loss:3.9580 train_time:556700ms step_avg:170.04ms\nstep:3285/10000 train_loss:4.0296 train_time:556874ms step_avg:170.04ms\nstep:3286/10000 train_loss:4.0770 train_time:557046ms step_avg:170.04ms\nstep:3287/10000 train_loss:4.1678 train_time:557212ms step_avg:170.04ms\nstep:3288/10000 train_loss:4.1139 train_time:557383ms step_avg:170.04ms\nstep:3289/10000 train_loss:4.0476 train_time:557554ms step_avg:170.04ms\nstep:3290/10000 train_loss:4.0970 train_time:557726ms step_avg:170.04ms\nstep:3291/10000 train_loss:4.1274 train_time:557896ms step_avg:170.04ms\nstep:3292/10000 train_loss:4.2117 train_time:558067ms step_avg:170.04ms\nstep:3293/10000 train_loss:4.0257 train_time:558237ms step_avg:170.04ms\nstep:3294/10000 train_loss:4.1888 train_time:558407ms step_avg:170.04ms\nstep:3295/10000 train_loss:4.1057 train_time:558575ms step_avg:170.04ms\nstep:3296/10000 train_loss:3.9229 train_time:558745ms step_avg:170.04ms\nstep:3297/10000 train_loss:4.0664 train_time:558916ms step_avg:170.04ms\nstep:3298/10000 train_loss:4.1384 train_time:559088ms step_avg:170.04ms\nstep:3299/10000 train_loss:4.0896 train_time:559256ms step_avg:170.04ms\nstep:3300/10000 train_loss:4.1333 train_time:559426ms step_avg:170.04ms\nstep:3301/10000 train_loss:4.0703 train_time:559595ms step_avg:170.04ms\nstep:3302/10000 train_loss:4.0650 train_time:559766ms step_avg:170.04ms\nstep:3303/10000 train_loss:4.1424 train_time:559937ms step_avg:170.04ms\nstep:3304/10000 train_loss:4.2670 train_time:560107ms step_avg:170.04ms\nstep:3305/10000 train_loss:4.1029 train_time:560276ms step_avg:170.04ms\nstep:3306/10000 train_loss:4.1524 train_time:560448ms step_avg:170.04ms\nstep:3307/10000 train_loss:4.0922 train_time:560615ms step_avg:170.04ms\nstep:3308/10000 train_loss:4.0316 train_time:560785ms step_avg:170.04ms\nstep:3309/10000 train_loss:3.9556 train_time:560954ms step_avg:170.04ms\nstep:3310/10000 train_loss:4.0769 train_time:561125ms step_avg:170.04ms\nstep:3311/10000 train_loss:4.1901 train_time:561293ms step_avg:170.04ms\nstep:3312/10000 train_loss:4.0840 train_time:561463ms step_avg:170.04ms\nstep:3313/10000 train_loss:4.0828 train_time:561632ms step_avg:170.04ms\nstep:3314/10000 train_loss:4.2852 train_time:561802ms step_avg:170.04ms\nstep:3315/10000 train_loss:4.0849 train_time:561970ms step_avg:170.04ms\nstep:3316/10000 train_loss:4.1098 train_time:562143ms step_avg:170.04ms\nstep:3317/10000 train_loss:4.0892 train_time:562314ms step_avg:170.04ms\nstep:3318/10000 train_loss:4.1767 train_time:562486ms step_avg:170.04ms\nstep:3319/10000 train_loss:4.1288 train_time:562656ms step_avg:170.04ms\nstep:3320/10000 train_loss:4.1742 train_time:562827ms step_avg:170.04ms\nstep:3321/10000 train_loss:4.0264 train_time:562997ms step_avg:170.04ms\nstep:3322/10000 train_loss:4.2732 train_time:563167ms step_avg:170.04ms\nstep:3323/10000 train_loss:4.0859 train_time:563336ms step_avg:170.04ms\nstep:3324/10000 train_loss:3.9936 train_time:563509ms step_avg:170.04ms\nstep:3325/10000 train_loss:4.1455 train_time:563678ms step_avg:170.04ms\nstep:3326/10000 train_loss:4.0768 train_time:563847ms step_avg:170.04ms\nstep:3327/10000 train_loss:4.1692 train_time:564017ms step_avg:170.04ms\nstep:3328/10000 train_loss:4.1912 train_time:564195ms step_avg:170.04ms\nstep:3329/10000 train_loss:4.1171 train_time:564360ms step_avg:170.04ms\nstep:3330/10000 train_loss:4.1378 train_time:564531ms step_avg:170.04ms\nstep:3331/10000 train_loss:4.3745 train_time:564702ms step_avg:170.04ms\nstep:3332/10000 train_loss:4.1676 train_time:564870ms step_avg:170.04ms\nstep:3333/10000 train_loss:4.0774 train_time:565040ms step_avg:170.04ms\nstep:3334/10000 train_loss:4.0781 train_time:565211ms step_avg:170.04ms\nstep:3335/10000 train_loss:4.1650 train_time:565382ms step_avg:170.04ms\nstep:3336/10000 train_loss:4.1606 train_time:565551ms step_avg:170.04ms\nstep:3337/10000 train_loss:4.1228 train_time:565721ms step_avg:170.04ms\nstep:3338/10000 train_loss:4.1852 train_time:565890ms step_avg:170.04ms\nstep:3339/10000 train_loss:4.2529 train_time:566063ms step_avg:170.04ms\nstep:3340/10000 train_loss:4.0386 train_time:566231ms step_avg:170.04ms\nstep:3341/10000 train_loss:4.2348 train_time:566401ms step_avg:170.04ms\nstep:3342/10000 train_loss:4.0802 train_time:566571ms step_avg:170.04ms\nstep:3343/10000 train_loss:4.0999 train_time:566742ms step_avg:170.04ms\nstep:3344/10000 train_loss:4.2652 train_time:566912ms step_avg:170.04ms\nstep:3345/10000 train_loss:4.0289 train_time:567082ms step_avg:170.04ms\nstep:3346/10000 train_loss:4.0235 train_time:567251ms step_avg:170.04ms\nstep:3347/10000 train_loss:4.0335 train_time:567422ms step_avg:170.04ms\nstep:3348/10000 train_loss:4.0046 train_time:567591ms step_avg:170.04ms\nstep:3349/10000 train_loss:4.1548 train_time:567759ms step_avg:170.04ms\nstep:3350/10000 train_loss:4.2937 train_time:567930ms step_avg:170.04ms\nstep:3351/10000 train_loss:4.0535 train_time:568099ms step_avg:170.04ms\nstep:3352/10000 train_loss:3.9526 train_time:568268ms step_avg:170.04ms\nstep:3353/10000 train_loss:3.9151 train_time:568439ms step_avg:170.04ms\nstep:3354/10000 train_loss:3.9997 train_time:568609ms step_avg:170.04ms\nstep:3355/10000 train_loss:4.1436 train_time:568780ms step_avg:170.04ms\nstep:3356/10000 train_loss:4.1617 train_time:568949ms step_avg:170.04ms\nstep:3357/10000 train_loss:4.0994 train_time:569119ms step_avg:170.04ms\nstep:3358/10000 train_loss:4.1371 train_time:569290ms step_avg:170.04ms\nstep:3359/10000 train_loss:4.0826 train_time:569459ms step_avg:170.04ms\nstep:3360/10000 train_loss:4.1024 train_time:569629ms step_avg:170.04ms\nstep:3361/10000 train_loss:4.0403 train_time:569797ms step_avg:170.04ms\nstep:3362/10000 train_loss:4.0032 train_time:569966ms step_avg:170.04ms\nstep:3363/10000 train_loss:4.1382 train_time:570135ms step_avg:170.04ms\nstep:3364/10000 train_loss:4.1507 train_time:570304ms step_avg:170.04ms\nstep:3365/10000 train_loss:4.1517 train_time:570477ms step_avg:170.04ms\nstep:3366/10000 train_loss:4.0398 train_time:570650ms step_avg:170.04ms\nstep:3367/10000 train_loss:4.2112 train_time:570821ms step_avg:170.04ms\nstep:3368/10000 train_loss:4.0817 train_time:570989ms step_avg:170.04ms\nstep:3369/10000 train_loss:4.0755 train_time:571160ms step_avg:170.04ms\nstep:3370/10000 train_loss:4.1166 train_time:571329ms step_avg:170.04ms\nstep:3371/10000 train_loss:4.1274 train_time:571498ms step_avg:170.04ms\nstep:3372/10000 train_loss:4.0602 train_time:571673ms step_avg:170.04ms\nstep:3373/10000 train_loss:4.1710 train_time:571837ms step_avg:170.04ms\nstep:3374/10000 train_loss:4.1168 train_time:572009ms step_avg:170.04ms\nstep:3375/10000 train_loss:3.9640 train_time:572178ms step_avg:170.04ms\nstep:3376/10000 train_loss:4.0590 train_time:572347ms step_avg:170.04ms\nstep:3377/10000 train_loss:4.0933 train_time:572516ms step_avg:170.04ms\nstep:3378/10000 train_loss:4.1293 train_time:572687ms step_avg:170.04ms\nstep:3379/10000 train_loss:4.2268 train_time:572857ms step_avg:170.04ms\nstep:3380/10000 train_loss:4.1202 train_time:573030ms step_avg:170.04ms\nstep:3381/10000 train_loss:3.8799 train_time:573201ms step_avg:170.04ms\nstep:3382/10000 train_loss:3.9809 train_time:573370ms step_avg:170.04ms\nstep:3383/10000 train_loss:4.1249 train_time:573537ms step_avg:170.04ms\nstep:3384/10000 train_loss:4.0998 train_time:573708ms step_avg:170.04ms\nstep:3385/10000 train_loss:4.1896 train_time:573877ms step_avg:170.04ms\nstep:3386/10000 train_loss:4.2187 train_time:574047ms step_avg:170.04ms\nstep:3387/10000 train_loss:4.0660 train_time:574217ms step_avg:170.04ms\nstep:3388/10000 train_loss:4.1412 train_time:574391ms step_avg:170.04ms\nstep:3389/10000 train_loss:4.1371 train_time:574562ms step_avg:170.04ms\nstep:3390/10000 train_loss:4.0841 train_time:574730ms step_avg:170.04ms\nstep:3391/10000 train_loss:4.1412 train_time:574898ms step_avg:170.04ms\nstep:3392/10000 train_loss:4.0947 train_time:575069ms step_avg:170.04ms\nstep:3393/10000 train_loss:4.0679 train_time:575240ms step_avg:170.04ms\nstep:3394/10000 train_loss:4.0529 train_time:575410ms step_avg:170.04ms\nstep:3395/10000 train_loss:4.1736 train_time:575579ms step_avg:170.04ms\nstep:3396/10000 train_loss:4.0025 train_time:575751ms step_avg:170.04ms\nstep:3397/10000 train_loss:4.1799 train_time:575920ms step_avg:170.04ms\nstep:3398/10000 train_loss:4.1666 train_time:576088ms step_avg:170.04ms\nstep:3399/10000 train_loss:4.2150 train_time:576256ms step_avg:170.04ms\nstep:3400/10000 train_loss:4.1657 train_time:576426ms step_avg:170.04ms\nstep:3401/10000 train_loss:4.1312 train_time:576595ms step_avg:170.04ms\nstep:3402/10000 train_loss:4.0450 train_time:576764ms step_avg:170.04ms\nstep:3403/10000 train_loss:4.1484 train_time:576934ms step_avg:170.04ms\nstep:3404/10000 train_loss:4.0309 train_time:577108ms step_avg:170.04ms\nstep:3405/10000 train_loss:4.1476 train_time:577279ms step_avg:170.04ms\nstep:3406/10000 train_loss:4.0116 train_time:577451ms step_avg:170.04ms\nstep:3407/10000 train_loss:4.4087 train_time:577621ms step_avg:170.04ms\nstep:3408/10000 train_loss:4.1566 train_time:577792ms step_avg:170.04ms\nstep:3409/10000 train_loss:4.2282 train_time:577963ms step_avg:170.04ms\nstep:3410/10000 train_loss:4.3254 train_time:578131ms step_avg:170.04ms\nstep:3411/10000 train_loss:4.1218 train_time:578301ms step_avg:170.04ms\nstep:3412/10000 train_loss:3.8999 train_time:578471ms step_avg:170.04ms\nstep:3413/10000 train_loss:4.2713 train_time:578641ms step_avg:170.04ms\nstep:3414/10000 train_loss:4.0312 train_time:578815ms step_avg:170.04ms\nstep:3415/10000 train_loss:4.1133 train_time:578983ms step_avg:170.04ms\nstep:3416/10000 train_loss:4.2247 train_time:579147ms step_avg:170.04ms\nstep:3417/10000 train_loss:4.0412 train_time:579318ms step_avg:170.04ms\nstep:3418/10000 train_loss:4.1242 train_time:579490ms step_avg:170.04ms\nstep:3419/10000 train_loss:4.1066 train_time:579660ms step_avg:170.04ms\nstep:3420/10000 train_loss:4.0900 train_time:579832ms step_avg:170.04ms\nstep:3421/10000 train_loss:4.1371 train_time:580003ms step_avg:170.04ms\nstep:3422/10000 train_loss:4.2117 train_time:580177ms step_avg:170.04ms\nstep:3423/10000 train_loss:4.1021 train_time:580344ms step_avg:170.04ms\nstep:3424/10000 train_loss:3.8724 train_time:580513ms step_avg:170.04ms\nstep:3425/10000 train_loss:4.1345 train_time:580684ms step_avg:170.04ms\nstep:3426/10000 train_loss:4.1264 train_time:580853ms step_avg:170.04ms\nstep:3427/10000 train_loss:4.0401 train_time:581024ms step_avg:170.04ms\nstep:3428/10000 train_loss:4.1776 train_time:581192ms step_avg:170.04ms\nstep:3429/10000 train_loss:4.1623 train_time:581362ms step_avg:170.04ms\nstep:3430/10000 train_loss:4.0966 train_time:581531ms step_avg:170.04ms\nstep:3431/10000 train_loss:4.1093 train_time:581701ms step_avg:170.04ms\nstep:3432/10000 train_loss:4.2434 train_time:581868ms step_avg:170.04ms\nstep:3433/10000 train_loss:4.1096 train_time:582040ms step_avg:170.04ms\nstep:3434/10000 train_loss:4.1330 train_time:582211ms step_avg:170.04ms\nstep:3435/10000 train_loss:4.1374 train_time:582379ms step_avg:170.04ms\nstep:3436/10000 train_loss:3.9776 train_time:582549ms step_avg:170.04ms\nstep:3437/10000 train_loss:4.1095 train_time:582719ms step_avg:170.04ms\nstep:3438/10000 train_loss:4.0929 train_time:582889ms step_avg:170.04ms\nstep:3439/10000 train_loss:3.9324 train_time:583060ms step_avg:170.04ms\nstep:3440/10000 train_loss:4.0904 train_time:583233ms step_avg:170.04ms\nstep:3441/10000 train_loss:4.4200 train_time:583401ms step_avg:170.04ms\nstep:3442/10000 train_loss:4.1687 train_time:583571ms step_avg:170.04ms\nstep:3443/10000 train_loss:3.9846 train_time:583741ms step_avg:170.04ms\nstep:3444/10000 train_loss:4.2181 train_time:583909ms step_avg:170.04ms\nstep:3445/10000 train_loss:4.2402 train_time:584080ms step_avg:170.04ms\nstep:3446/10000 train_loss:4.1602 train_time:584249ms step_avg:170.04ms\nstep:3447/10000 train_loss:4.1255 train_time:584419ms step_avg:170.04ms\nstep:3448/10000 train_loss:3.9548 train_time:584588ms step_avg:170.04ms\nstep:3449/10000 train_loss:4.1410 train_time:584760ms step_avg:170.04ms\nstep:3450/10000 train_loss:4.2137 train_time:584931ms step_avg:170.04ms\nstep:3451/10000 train_loss:3.9564 train_time:585101ms step_avg:170.04ms\nstep:3452/10000 train_loss:4.0567 train_time:585268ms step_avg:170.04ms\nstep:3453/10000 train_loss:4.0599 train_time:585439ms step_avg:170.04ms\nstep:3454/10000 train_loss:4.1144 train_time:585610ms step_avg:170.04ms\nstep:3455/10000 train_loss:4.0195 train_time:585781ms step_avg:170.04ms\nstep:3456/10000 train_loss:4.3468 train_time:585952ms step_avg:170.04ms\nstep:3457/10000 train_loss:4.0984 train_time:586123ms step_avg:170.04ms\nstep:3458/10000 train_loss:4.1233 train_time:586292ms step_avg:170.04ms\nstep:3459/10000 train_loss:4.0712 train_time:586461ms step_avg:170.04ms\nstep:3460/10000 train_loss:4.3354 train_time:586633ms step_avg:170.04ms\nstep:3461/10000 train_loss:4.0742 train_time:586804ms step_avg:170.04ms\nstep:3462/10000 train_loss:3.9919 train_time:586976ms step_avg:170.04ms\nstep:3463/10000 train_loss:4.2390 train_time:587146ms step_avg:170.04ms\nstep:3464/10000 train_loss:4.1258 train_time:587313ms step_avg:170.04ms\nstep:3465/10000 train_loss:4.2503 train_time:587483ms step_avg:170.04ms\nstep:3466/10000 train_loss:4.1456 train_time:587650ms step_avg:170.04ms\nstep:3467/10000 train_loss:3.9979 train_time:587818ms step_avg:170.04ms\nstep:3468/10000 train_loss:4.0718 train_time:587988ms step_avg:170.04ms\nstep:3469/10000 train_loss:4.4088 train_time:588159ms step_avg:170.04ms\nstep:3470/10000 train_loss:4.4385 train_time:588331ms step_avg:170.04ms\nstep:3471/10000 train_loss:4.0523 train_time:588502ms step_avg:170.04ms\nstep:3472/10000 train_loss:4.3107 train_time:588670ms step_avg:170.04ms\nstep:3473/10000 train_loss:4.0584 train_time:588841ms step_avg:170.04ms\nstep:3474/10000 train_loss:3.9970 train_time:589011ms step_avg:170.04ms\nstep:3475/10000 train_loss:4.2150 train_time:589178ms step_avg:170.04ms\nstep:3476/10000 train_loss:4.0163 train_time:589352ms step_avg:170.04ms\nstep:3477/10000 train_loss:4.0915 train_time:589523ms step_avg:170.04ms\nstep:3478/10000 train_loss:4.1738 train_time:589691ms step_avg:170.04ms\nstep:3479/10000 train_loss:4.0729 train_time:589861ms step_avg:170.04ms\nstep:3480/10000 train_loss:3.9383 train_time:590033ms step_avg:170.04ms\nstep:3481/10000 train_loss:4.0851 train_time:590201ms step_avg:170.04ms\nstep:3482/10000 train_loss:4.1568 train_time:590370ms step_avg:170.04ms\nstep:3483/10000 train_loss:4.1819 train_time:590541ms step_avg:170.04ms\nstep:3484/10000 train_loss:4.1676 train_time:590710ms step_avg:170.04ms\nstep:3485/10000 train_loss:4.1875 train_time:590878ms step_avg:170.04ms\nstep:3486/10000 train_loss:3.9895 train_time:591047ms step_avg:170.04ms\nstep:3487/10000 train_loss:4.2097 train_time:591216ms step_avg:170.04ms\nstep:3488/10000 train_loss:4.1094 train_time:591387ms step_avg:170.04ms\nstep:3489/10000 train_loss:4.0186 train_time:591558ms step_avg:170.04ms\nstep:3490/10000 train_loss:4.0029 train_time:591733ms step_avg:170.04ms\nstep:3491/10000 train_loss:4.0461 train_time:591907ms step_avg:170.04ms\nstep:3492/10000 train_loss:4.0384 train_time:592075ms step_avg:170.04ms\nstep:3493/10000 train_loss:4.2090 train_time:592245ms step_avg:170.04ms\nstep:3494/10000 train_loss:4.2116 train_time:592416ms step_avg:170.04ms\nstep:3495/10000 train_loss:4.1083 train_time:592586ms step_avg:170.04ms\nstep:3496/10000 train_loss:4.1220 train_time:592753ms step_avg:170.04ms\nstep:3497/10000 train_loss:4.1721 train_time:592926ms step_avg:170.04ms\nstep:3498/10000 train_loss:3.9988 train_time:593093ms step_avg:170.04ms\nstep:3499/10000 train_loss:4.1179 train_time:593263ms step_avg:170.04ms\nstep:3500/10000 train_loss:4.0496 train_time:593431ms step_avg:170.04ms\nstep:3501/10000 train_loss:4.1559 train_time:593600ms step_avg:170.04ms\nstep:3502/10000 train_loss:3.6136 train_time:593772ms step_avg:170.04ms\nstep:3503/10000 train_loss:4.0858 train_time:593944ms step_avg:170.04ms\nstep:3504/10000 train_loss:4.1741 train_time:594111ms step_avg:170.04ms\nstep:3505/10000 train_loss:4.1300 train_time:594285ms step_avg:170.04ms\nstep:3506/10000 train_loss:4.1775 train_time:594450ms step_avg:170.04ms\nstep:3507/10000 train_loss:4.1170 train_time:594622ms step_avg:170.04ms\nstep:3508/10000 train_loss:3.9254 train_time:594795ms step_avg:170.04ms\nstep:3509/10000 train_loss:4.1133 train_time:594964ms step_avg:170.04ms\nstep:3510/10000 train_loss:4.0267 train_time:595137ms step_avg:170.04ms\nstep:3511/10000 train_loss:3.8939 train_time:595309ms step_avg:170.04ms\nstep:3512/10000 train_loss:4.2075 train_time:595477ms step_avg:170.04ms\nstep:3513/10000 train_loss:4.3869 train_time:595648ms step_avg:170.04ms\nstep:3514/10000 train_loss:4.1909 train_time:595817ms step_avg:170.04ms\nstep:3515/10000 train_loss:4.0713 train_time:595986ms step_avg:170.04ms\nstep:3516/10000 train_loss:4.0511 train_time:596157ms step_avg:170.04ms\nstep:3517/10000 train_loss:4.0341 train_time:596329ms step_avg:170.04ms\nstep:3518/10000 train_loss:4.0612 train_time:596498ms step_avg:170.04ms\nstep:3519/10000 train_loss:4.1352 train_time:596666ms step_avg:170.04ms\nstep:3520/10000 train_loss:4.4060 train_time:596835ms step_avg:170.04ms\nstep:3521/10000 train_loss:4.1315 train_time:597008ms step_avg:170.04ms\nstep:3522/10000 train_loss:4.2063 train_time:597178ms step_avg:170.04ms\nstep:3523/10000 train_loss:3.9329 train_time:597347ms step_avg:170.04ms\nstep:3524/10000 train_loss:4.1455 train_time:597520ms step_avg:170.04ms\nstep:3525/10000 train_loss:4.0454 train_time:597691ms step_avg:170.04ms\nstep:3526/10000 train_loss:3.8850 train_time:597860ms step_avg:170.04ms\nstep:3527/10000 train_loss:4.0247 train_time:598029ms step_avg:170.04ms\nstep:3528/10000 train_loss:3.9494 train_time:598197ms step_avg:170.04ms\nstep:3529/10000 train_loss:3.9999 train_time:598368ms step_avg:170.04ms\nstep:3530/10000 train_loss:4.0596 train_time:598540ms step_avg:170.04ms\nstep:3531/10000 train_loss:4.1830 train_time:598714ms step_avg:170.04ms\nstep:3532/10000 train_loss:4.2539 train_time:598884ms step_avg:170.04ms\nstep:3533/10000 train_loss:4.0530 train_time:599053ms step_avg:170.04ms\nstep:3534/10000 train_loss:4.2482 train_time:599224ms step_avg:170.04ms\nstep:3535/10000 train_loss:4.0924 train_time:599392ms step_avg:170.04ms\nstep:3536/10000 train_loss:3.9732 train_time:599563ms step_avg:170.04ms\nstep:3537/10000 train_loss:4.1144 train_time:599734ms step_avg:170.04ms\nstep:3538/10000 train_loss:4.4026 train_time:599903ms step_avg:170.04ms\nstep:3539/10000 train_loss:3.9886 train_time:600072ms step_avg:170.04ms\nstep:3540/10000 train_loss:4.2028 train_time:600243ms step_avg:170.04ms\nstep:3541/10000 train_loss:3.9902 train_time:600410ms step_avg:170.04ms\nstep:3542/10000 train_loss:4.1374 train_time:600588ms step_avg:170.04ms\nstep:3543/10000 train_loss:4.0603 train_time:600749ms step_avg:170.04ms\nstep:3544/10000 train_loss:4.0695 train_time:600919ms step_avg:170.04ms\nstep:3545/10000 train_loss:3.9720 train_time:601089ms step_avg:170.04ms\nstep:3546/10000 train_loss:4.0287 train_time:601259ms step_avg:170.04ms\nstep:3547/10000 train_loss:4.0005 train_time:601430ms step_avg:170.04ms\nstep:3548/10000 train_loss:4.2371 train_time:601599ms step_avg:170.04ms\nstep:3549/10000 train_loss:4.0657 train_time:601768ms step_avg:170.04ms\nstep:3550/10000 train_loss:4.1568 train_time:601940ms step_avg:170.04ms\nstep:3551/10000 train_loss:4.1039 train_time:602112ms step_avg:170.04ms\nstep:3552/10000 train_loss:4.1113 train_time:602284ms step_avg:170.04ms\nstep:3553/10000 train_loss:3.9898 train_time:602454ms step_avg:170.04ms\nstep:3554/10000 train_loss:4.6007 train_time:602623ms step_avg:170.04ms\nstep:3555/10000 train_loss:4.2596 train_time:602793ms step_avg:170.04ms\nstep:3556/10000 train_loss:4.1700 train_time:602962ms step_avg:170.04ms\nstep:3557/10000 train_loss:3.9758 train_time:603132ms step_avg:170.04ms\nstep:3558/10000 train_loss:4.3534 train_time:603304ms step_avg:170.04ms\nstep:3559/10000 train_loss:4.0613 train_time:603475ms step_avg:170.04ms\nstep:3560/10000 train_loss:4.1276 train_time:603647ms step_avg:170.04ms\nstep:3561/10000 train_loss:4.1562 train_time:603816ms step_avg:170.04ms\nstep:3562/10000 train_loss:4.0823 train_time:603985ms step_avg:170.04ms\nstep:3563/10000 train_loss:3.9463 train_time:604154ms step_avg:170.04ms\nstep:3564/10000 train_loss:3.9507 train_time:604324ms step_avg:170.04ms\nstep:3565/10000 train_loss:3.9763 train_time:604492ms step_avg:170.04ms\nstep:3566/10000 train_loss:4.0905 train_time:604665ms step_avg:170.04ms\nstep:3567/10000 train_loss:3.8551 train_time:604828ms step_avg:170.04ms\nstep:3568/10000 train_loss:4.1218 train_time:605000ms step_avg:170.04ms\nstep:3569/10000 train_loss:4.0114 train_time:605169ms step_avg:170.04ms\nstep:3570/10000 train_loss:3.9101 train_time:605340ms step_avg:170.04ms\nstep:3571/10000 train_loss:3.9387 train_time:605510ms step_avg:170.04ms\nstep:3572/10000 train_loss:4.0154 train_time:605679ms step_avg:170.04ms\nstep:3573/10000 train_loss:4.1769 train_time:605846ms step_avg:170.04ms\nstep:3574/10000 train_loss:4.2258 train_time:606017ms step_avg:170.04ms\nstep:3575/10000 train_loss:4.2536 train_time:606189ms step_avg:170.04ms\nstep:3576/10000 train_loss:4.0869 train_time:606359ms step_avg:170.04ms\nstep:3577/10000 train_loss:3.9174 train_time:606533ms step_avg:170.04ms\nstep:3578/10000 train_loss:3.9876 train_time:606704ms step_avg:170.04ms\nstep:3579/10000 train_loss:4.0550 train_time:606875ms step_avg:170.04ms\nstep:3580/10000 train_loss:3.9126 train_time:607047ms step_avg:170.04ms\nstep:3581/10000 train_loss:3.9879 train_time:607216ms step_avg:170.04ms\nstep:3582/10000 train_loss:4.3209 train_time:607384ms step_avg:170.04ms\nstep:3583/10000 train_loss:4.0929 train_time:607554ms step_avg:170.04ms\nstep:3584/10000 train_loss:3.9038 train_time:607725ms step_avg:170.04ms\nstep:3585/10000 train_loss:4.2171 train_time:607893ms step_avg:170.04ms\nstep:3586/10000 train_loss:4.2546 train_time:608064ms step_avg:170.04ms\nstep:3587/10000 train_loss:4.0213 train_time:608232ms step_avg:170.04ms\nstep:3588/10000 train_loss:4.0074 train_time:608405ms step_avg:170.04ms\nstep:3589/10000 train_loss:3.9776 train_time:608575ms step_avg:170.04ms\nstep:3590/10000 train_loss:4.0591 train_time:608747ms step_avg:170.04ms\nstep:3591/10000 train_loss:4.0502 train_time:608920ms step_avg:170.04ms\nstep:3592/10000 train_loss:4.1895 train_time:609090ms step_avg:170.04ms\nstep:3593/10000 train_loss:4.0990 train_time:609259ms step_avg:170.04ms\nstep:3594/10000 train_loss:4.0720 train_time:609427ms step_avg:170.04ms\nstep:3595/10000 train_loss:4.1014 train_time:609596ms step_avg:170.04ms\nstep:3596/10000 train_loss:4.0759 train_time:609763ms step_avg:170.04ms\nstep:3597/10000 train_loss:4.1081 train_time:609933ms step_avg:170.04ms\nstep:3598/10000 train_loss:4.1316 train_time:610104ms step_avg:170.04ms\nstep:3599/10000 train_loss:4.0436 train_time:610274ms step_avg:170.04ms\nstep:3600/10000 train_loss:4.0891 train_time:610446ms step_avg:170.04ms\nstep:3601/10000 train_loss:4.6287 train_time:610617ms step_avg:170.04ms\nstep:3602/10000 train_loss:4.0310 train_time:610788ms step_avg:170.04ms\nstep:3603/10000 train_loss:4.0227 train_time:610956ms step_avg:170.04ms\nstep:3604/10000 train_loss:4.2835 train_time:611124ms step_avg:170.04ms\nstep:3605/10000 train_loss:4.0068 train_time:611294ms step_avg:170.04ms\nstep:3606/10000 train_loss:3.9700 train_time:611467ms step_avg:170.04ms\nstep:3607/10000 train_loss:4.0618 train_time:611637ms step_avg:170.04ms\nstep:3608/10000 train_loss:4.0838 train_time:611809ms step_avg:170.04ms\nstep:3609/10000 train_loss:4.0482 train_time:611980ms step_avg:170.04ms\nstep:3610/10000 train_loss:4.1910 train_time:612148ms step_avg:170.04ms\nstep:3611/10000 train_loss:4.0334 train_time:612318ms step_avg:170.04ms\nstep:3612/10000 train_loss:3.9916 train_time:612487ms step_avg:170.04ms\nstep:3613/10000 train_loss:4.1024 train_time:612657ms step_avg:170.04ms\nstep:3614/10000 train_loss:4.1970 train_time:612831ms step_avg:170.04ms\nstep:3615/10000 train_loss:4.4674 train_time:613002ms step_avg:170.04ms\nstep:3616/10000 train_loss:4.2372 train_time:613171ms step_avg:170.04ms\nstep:3617/10000 train_loss:4.0625 train_time:613342ms step_avg:170.04ms\nstep:3618/10000 train_loss:4.0735 train_time:613511ms step_avg:170.04ms\nstep:3619/10000 train_loss:4.0410 train_time:613683ms step_avg:170.04ms\nstep:3620/10000 train_loss:4.1686 train_time:613852ms step_avg:170.04ms\nstep:3621/10000 train_loss:4.0654 train_time:614025ms step_avg:170.04ms\nstep:3622/10000 train_loss:4.0545 train_time:614192ms step_avg:170.04ms\nstep:3623/10000 train_loss:4.1095 train_time:614363ms step_avg:170.04ms\nstep:3624/10000 train_loss:4.0709 train_time:614532ms step_avg:170.04ms\nstep:3625/10000 train_loss:4.0293 train_time:614704ms step_avg:170.04ms\nstep:3626/10000 train_loss:4.0539 train_time:614873ms step_avg:170.04ms\nstep:3627/10000 train_loss:3.9408 train_time:615042ms step_avg:170.04ms\nstep:3628/10000 train_loss:4.0183 train_time:615210ms step_avg:170.04ms\nstep:3629/10000 train_loss:4.0304 train_time:615381ms step_avg:170.04ms\nstep:3630/10000 train_loss:4.2724 train_time:615552ms step_avg:170.04ms\nstep:3631/10000 train_loss:4.0661 train_time:615722ms step_avg:170.04ms\nstep:3632/10000 train_loss:4.3139 train_time:615889ms step_avg:170.04ms\nstep:3633/10000 train_loss:4.0890 train_time:616059ms step_avg:170.04ms\nstep:3634/10000 train_loss:4.0662 train_time:616229ms step_avg:170.04ms\nstep:3635/10000 train_loss:4.1677 train_time:616398ms step_avg:170.04ms\nstep:3636/10000 train_loss:4.0296 train_time:616567ms step_avg:170.04ms\nstep:3637/10000 train_loss:4.0087 train_time:616738ms step_avg:170.04ms\nstep:3638/10000 train_loss:4.0870 train_time:616911ms step_avg:170.04ms\nstep:3639/10000 train_loss:4.0695 train_time:617079ms step_avg:170.04ms\nstep:3640/10000 train_loss:4.1668 train_time:617248ms step_avg:170.04ms\nstep:3641/10000 train_loss:4.0759 train_time:617417ms step_avg:170.04ms\nstep:3642/10000 train_loss:3.9980 train_time:617590ms step_avg:170.04ms\nstep:3643/10000 train_loss:4.1629 train_time:617760ms step_avg:170.04ms\nstep:3644/10000 train_loss:4.4041 train_time:617928ms step_avg:170.04ms\nstep:3645/10000 train_loss:4.1684 train_time:618099ms step_avg:170.04ms\nstep:3646/10000 train_loss:4.1058 train_time:618269ms step_avg:170.04ms\nstep:3647/10000 train_loss:4.0387 train_time:618437ms step_avg:170.04ms\nstep:3648/10000 train_loss:3.9245 train_time:618607ms step_avg:170.04ms\nstep:3649/10000 train_loss:4.0447 train_time:618778ms step_avg:170.04ms\nstep:3650/10000 train_loss:4.1435 train_time:618947ms step_avg:170.04ms\nstep:3651/10000 train_loss:3.9990 train_time:619116ms step_avg:170.04ms\nstep:3652/10000 train_loss:4.1966 train_time:619287ms step_avg:170.04ms\nstep:3653/10000 train_loss:4.0704 train_time:619456ms step_avg:170.04ms\nstep:3654/10000 train_loss:4.1508 train_time:619629ms step_avg:170.04ms\nstep:3655/10000 train_loss:3.9925 train_time:619797ms step_avg:170.04ms\nstep:3656/10000 train_loss:4.1162 train_time:619966ms step_avg:170.04ms\nstep:3657/10000 train_loss:4.1034 train_time:620134ms step_avg:170.04ms\nstep:3658/10000 train_loss:4.0646 train_time:620303ms step_avg:170.04ms\nstep:3659/10000 train_loss:4.1105 train_time:620471ms step_avg:170.04ms\nstep:3660/10000 train_loss:4.0963 train_time:620645ms step_avg:170.04ms\nstep:3661/10000 train_loss:3.9091 train_time:620815ms step_avg:170.04ms\nstep:3662/10000 train_loss:4.2806 train_time:620982ms step_avg:170.04ms\nstep:3663/10000 train_loss:4.1020 train_time:621151ms step_avg:170.04ms\nstep:3664/10000 train_loss:4.2740 train_time:621322ms step_avg:170.04ms\nstep:3665/10000 train_loss:4.0908 train_time:621491ms step_avg:170.04ms\nstep:3666/10000 train_loss:4.1155 train_time:621662ms step_avg:170.04ms\nstep:3667/10000 train_loss:3.9944 train_time:621832ms step_avg:170.04ms\nstep:3668/10000 train_loss:4.0167 train_time:622003ms step_avg:170.04ms\nstep:3669/10000 train_loss:4.2827 train_time:622172ms step_avg:170.04ms\nstep:3670/10000 train_loss:4.1711 train_time:622343ms step_avg:170.04ms\nstep:3671/10000 train_loss:4.1412 train_time:622512ms step_avg:170.04ms\nstep:3672/10000 train_loss:3.9962 train_time:622683ms step_avg:170.04ms\nstep:3673/10000 train_loss:4.5297 train_time:622851ms step_avg:170.04ms\nstep:3674/10000 train_loss:4.1690 train_time:623021ms step_avg:170.04ms\nstep:3675/10000 train_loss:4.0023 train_time:623190ms step_avg:170.04ms\nstep:3676/10000 train_loss:4.0710 train_time:623361ms step_avg:170.04ms\nstep:3677/10000 train_loss:4.0715 train_time:623532ms step_avg:170.04ms\nstep:3678/10000 train_loss:5.2969 train_time:623703ms step_avg:170.04ms\nstep:3679/10000 train_loss:4.0265 train_time:623873ms step_avg:170.04ms\nstep:3680/10000 train_loss:3.9967 train_time:624043ms step_avg:170.04ms\nstep:3681/10000 train_loss:3.7511 train_time:624210ms step_avg:170.04ms\nstep:3682/10000 train_loss:4.1156 train_time:624380ms step_avg:170.04ms\nstep:3683/10000 train_loss:3.8859 train_time:624548ms step_avg:170.04ms\nstep:3684/10000 train_loss:4.0525 train_time:624719ms step_avg:170.04ms\nstep:3685/10000 train_loss:4.1648 train_time:624890ms step_avg:170.04ms\nstep:3686/10000 train_loss:4.1047 train_time:625062ms step_avg:170.04ms\nstep:3687/10000 train_loss:4.1313 train_time:625230ms step_avg:170.04ms\nstep:3688/10000 train_loss:4.0089 train_time:625402ms step_avg:170.04ms\nstep:3689/10000 train_loss:4.0592 train_time:625569ms step_avg:170.04ms\nstep:3690/10000 train_loss:4.0639 train_time:625739ms step_avg:170.04ms\nstep:3691/10000 train_loss:3.9728 train_time:625908ms step_avg:170.04ms\nstep:3692/10000 train_loss:4.1183 train_time:626080ms step_avg:170.04ms\nstep:3693/10000 train_loss:4.0621 train_time:626250ms step_avg:170.04ms\nstep:3694/10000 train_loss:4.1420 train_time:626420ms step_avg:170.04ms\nstep:3695/10000 train_loss:4.0346 train_time:626588ms step_avg:170.04ms\nstep:3696/10000 train_loss:4.0933 train_time:626757ms step_avg:170.04ms\nstep:3697/10000 train_loss:4.0434 train_time:626929ms step_avg:170.04ms\nstep:3698/10000 train_loss:4.1291 train_time:627099ms step_avg:170.04ms\nstep:3699/10000 train_loss:3.9836 train_time:627274ms step_avg:170.04ms\nstep:3700/10000 train_loss:3.9819 train_time:627442ms step_avg:170.04ms\nstep:3701/10000 train_loss:4.0642 train_time:627613ms step_avg:170.04ms\nstep:3702/10000 train_loss:4.2081 train_time:627780ms step_avg:170.04ms\nstep:3703/10000 train_loss:3.9581 train_time:627953ms step_avg:170.04ms\nstep:3704/10000 train_loss:4.0833 train_time:628117ms step_avg:170.04ms\nstep:3705/10000 train_loss:4.0117 train_time:628287ms step_avg:170.04ms\nstep:3706/10000 train_loss:4.2024 train_time:628455ms step_avg:170.04ms\nstep:3707/10000 train_loss:4.0528 train_time:628626ms step_avg:170.04ms\nstep:3708/10000 train_loss:3.9940 train_time:628790ms step_avg:170.04ms\nstep:3709/10000 train_loss:3.8797 train_time:628962ms step_avg:170.04ms\nstep:3710/10000 train_loss:4.0800 train_time:629130ms step_avg:170.04ms\nstep:3711/10000 train_loss:3.9661 train_time:629298ms step_avg:170.03ms\nstep:3712/10000 train_loss:4.1790 train_time:629466ms step_avg:170.03ms\nstep:3713/10000 train_loss:4.2026 train_time:629636ms step_avg:170.03ms\nstep:3714/10000 train_loss:4.0868 train_time:629806ms step_avg:170.03ms\nstep:3715/10000 train_loss:3.9094 train_time:629975ms step_avg:170.03ms\nstep:3716/10000 train_loss:4.0568 train_time:630146ms step_avg:170.03ms\nstep:3717/10000 train_loss:3.9809 train_time:630315ms step_avg:170.03ms\nstep:3718/10000 train_loss:4.2598 train_time:630486ms step_avg:170.03ms\nstep:3719/10000 train_loss:4.1124 train_time:630654ms step_avg:170.03ms\nstep:3720/10000 train_loss:3.8962 train_time:630823ms step_avg:170.03ms\nstep:3721/10000 train_loss:4.0956 train_time:630993ms step_avg:170.03ms\nstep:3722/10000 train_loss:4.1391 train_time:631164ms step_avg:170.03ms\nstep:3723/10000 train_loss:4.1603 train_time:631333ms step_avg:170.03ms\nstep:3724/10000 train_loss:4.2456 train_time:631503ms step_avg:170.03ms\nstep:3725/10000 train_loss:4.2668 train_time:631674ms step_avg:170.03ms\nstep:3726/10000 train_loss:3.9144 train_time:631845ms step_avg:170.03ms\nstep:3727/10000 train_loss:4.1612 train_time:632013ms step_avg:170.03ms\nstep:3728/10000 train_loss:3.9812 train_time:632183ms step_avg:170.03ms\nstep:3729/10000 train_loss:4.0559 train_time:632353ms step_avg:170.03ms\nstep:3730/10000 train_loss:4.0361 train_time:632526ms step_avg:170.03ms\nstep:3731/10000 train_loss:4.1253 train_time:632694ms step_avg:170.03ms\nstep:3732/10000 train_loss:3.9549 train_time:632866ms step_avg:170.03ms\nstep:3733/10000 train_loss:4.0564 train_time:633036ms step_avg:170.03ms\nstep:3734/10000 train_loss:4.2238 train_time:633210ms step_avg:170.03ms\nstep:3735/10000 train_loss:4.3319 train_time:633373ms step_avg:170.03ms\nstep:3736/10000 train_loss:4.1046 train_time:633542ms step_avg:170.03ms\nstep:3737/10000 train_loss:4.0550 train_time:633711ms step_avg:170.03ms\nstep:3738/10000 train_loss:3.9840 train_time:633882ms step_avg:170.03ms\nstep:3739/10000 train_loss:3.9840 train_time:634050ms step_avg:170.03ms\nstep:3740/10000 train_loss:4.1150 train_time:634217ms step_avg:170.03ms\nstep:3741/10000 train_loss:4.0211 train_time:634387ms step_avg:170.03ms\nstep:3742/10000 train_loss:4.0882 train_time:634556ms step_avg:170.03ms\nstep:3743/10000 train_loss:4.2315 train_time:634727ms step_avg:170.03ms\nstep:3744/10000 train_loss:3.9481 train_time:634896ms step_avg:170.03ms\nstep:3745/10000 train_loss:4.2176 train_time:635068ms step_avg:170.03ms\nstep:3746/10000 train_loss:4.0108 train_time:635236ms step_avg:170.03ms\nstep:3747/10000 train_loss:3.9553 train_time:635408ms step_avg:170.03ms\nstep:3748/10000 train_loss:3.9232 train_time:635577ms step_avg:170.03ms\nstep:3749/10000 train_loss:4.2991 train_time:635751ms step_avg:170.03ms\nstep:3750/10000 train_loss:4.0575 train_time:635921ms step_avg:170.03ms\nstep:3751/10000 train_loss:4.3067 train_time:636090ms step_avg:170.03ms\nstep:3752/10000 train_loss:4.0431 train_time:636258ms step_avg:170.03ms\nstep:3753/10000 train_loss:4.0047 train_time:636428ms step_avg:170.03ms\nstep:3754/10000 train_loss:4.1499 train_time:636598ms step_avg:170.03ms\nstep:3755/10000 train_loss:4.1487 train_time:636772ms step_avg:170.03ms\nstep:3756/10000 train_loss:3.8692 train_time:636942ms step_avg:170.03ms\nstep:3757/10000 train_loss:4.0351 train_time:637115ms step_avg:170.03ms\nstep:3758/10000 train_loss:4.0615 train_time:637282ms step_avg:170.03ms\nstep:3759/10000 train_loss:4.0323 train_time:637452ms step_avg:170.03ms\nstep:3760/10000 train_loss:4.1117 train_time:637623ms step_avg:170.03ms\nstep:3761/10000 train_loss:4.1062 train_time:637794ms step_avg:170.03ms\nstep:3762/10000 train_loss:4.2440 train_time:637965ms step_avg:170.03ms\nstep:3763/10000 train_loss:4.0000 train_time:638135ms step_avg:170.03ms\nstep:3764/10000 train_loss:3.9662 train_time:638307ms step_avg:170.03ms\nstep:3765/10000 train_loss:4.1162 train_time:638475ms step_avg:170.03ms\nstep:3766/10000 train_loss:4.0392 train_time:638647ms step_avg:170.03ms\nstep:3767/10000 train_loss:3.9717 train_time:638820ms step_avg:170.03ms\nstep:3768/10000 train_loss:3.9338 train_time:638991ms step_avg:170.03ms\nstep:3769/10000 train_loss:4.0454 train_time:639162ms step_avg:170.04ms\nstep:3770/10000 train_loss:4.0975 train_time:639330ms step_avg:170.03ms\nstep:3771/10000 train_loss:4.6136 train_time:639502ms step_avg:170.04ms\nstep:3772/10000 train_loss:4.0840 train_time:639671ms step_avg:170.03ms\nstep:3773/10000 train_loss:4.0838 train_time:639841ms step_avg:170.03ms\nstep:3774/10000 train_loss:3.9133 train_time:640013ms step_avg:170.04ms\nstep:3775/10000 train_loss:3.9094 train_time:640185ms step_avg:170.04ms\nstep:3776/10000 train_loss:3.9785 train_time:640352ms step_avg:170.03ms\nstep:3777/10000 train_loss:4.2433 train_time:640521ms step_avg:170.03ms\nstep:3778/10000 train_loss:4.2148 train_time:640692ms step_avg:170.03ms\nstep:3779/10000 train_loss:4.3129 train_time:640863ms step_avg:170.04ms\nstep:3780/10000 train_loss:4.0164 train_time:641032ms step_avg:170.03ms\nstep:3781/10000 train_loss:4.0699 train_time:641201ms step_avg:170.03ms\nstep:3782/10000 train_loss:3.9483 train_time:641370ms step_avg:170.03ms\nstep:3783/10000 train_loss:4.0196 train_time:641540ms step_avg:170.03ms\nstep:3784/10000 train_loss:4.2740 train_time:641711ms step_avg:170.03ms\nstep:3785/10000 train_loss:4.1331 train_time:641884ms step_avg:170.04ms\nstep:3786/10000 train_loss:3.9703 train_time:642053ms step_avg:170.04ms\nstep:3787/10000 train_loss:4.2287 train_time:642222ms step_avg:170.04ms\nstep:3788/10000 train_loss:4.0652 train_time:642392ms step_avg:170.03ms\nstep:3789/10000 train_loss:3.9973 train_time:642561ms step_avg:170.03ms\nstep:3790/10000 train_loss:5.5217 train_time:642730ms step_avg:170.03ms\nstep:3791/10000 train_loss:3.9876 train_time:642902ms step_avg:170.03ms\nstep:3792/10000 train_loss:4.1422 train_time:643070ms step_avg:170.03ms\nstep:3793/10000 train_loss:4.1244 train_time:643242ms step_avg:170.03ms\nstep:3794/10000 train_loss:4.1231 train_time:643411ms step_avg:170.03ms\nstep:3795/10000 train_loss:4.3671 train_time:643580ms step_avg:170.03ms\nstep:3796/10000 train_loss:4.3020 train_time:643752ms step_avg:170.03ms\nstep:3797/10000 train_loss:4.1096 train_time:643925ms step_avg:170.04ms\nstep:3798/10000 train_loss:4.0338 train_time:644093ms step_avg:170.04ms\nstep:3799/10000 train_loss:4.0639 train_time:644263ms step_avg:170.04ms\nstep:3800/10000 train_loss:3.9096 train_time:644432ms step_avg:170.03ms\nstep:3801/10000 train_loss:4.0919 train_time:644603ms step_avg:170.04ms\nstep:3802/10000 train_loss:3.9422 train_time:644774ms step_avg:170.04ms\nstep:3803/10000 train_loss:4.0073 train_time:644946ms step_avg:170.04ms\nstep:3804/10000 train_loss:3.9946 train_time:645116ms step_avg:170.04ms\nstep:3805/10000 train_loss:3.9749 train_time:645288ms step_avg:170.04ms\nstep:3806/10000 train_loss:3.9996 train_time:645459ms step_avg:170.04ms\nstep:3807/10000 train_loss:3.9233 train_time:645634ms step_avg:170.04ms\nstep:3808/10000 train_loss:4.2759 train_time:645806ms step_avg:170.04ms\nstep:3809/10000 train_loss:4.0514 train_time:645974ms step_avg:170.04ms\nstep:3810/10000 train_loss:4.2150 train_time:646366ms step_avg:170.10ms\nstep:3811/10000 train_loss:4.0896 train_time:646531ms step_avg:170.09ms\nstep:3812/10000 train_loss:3.9508 train_time:646700ms step_avg:170.09ms\nstep:3813/10000 train_loss:4.0174 train_time:646869ms step_avg:170.09ms\nstep:3814/10000 train_loss:4.1234 train_time:647038ms step_avg:170.09ms\nstep:3815/10000 train_loss:4.0570 train_time:647206ms step_avg:170.09ms\nstep:3816/10000 train_loss:4.0238 train_time:647394ms step_avg:170.10ms\nstep:3817/10000 train_loss:4.0518 train_time:647565ms step_avg:170.10ms\nstep:3818/10000 train_loss:3.8127 train_time:647735ms step_avg:170.10ms\nstep:3819/10000 train_loss:3.9948 train_time:647904ms step_avg:170.10ms\nstep:3820/10000 train_loss:4.0730 train_time:648066ms step_avg:170.10ms\nstep:3821/10000 train_loss:4.0198 train_time:648242ms step_avg:170.10ms\nstep:3822/10000 train_loss:4.0079 train_time:648417ms step_avg:170.10ms\nstep:3823/10000 train_loss:3.9747 train_time:648589ms step_avg:170.10ms\nstep:3824/10000 train_loss:3.9448 train_time:648757ms step_avg:170.10ms\nstep:3825/10000 train_loss:4.1118 train_time:648921ms step_avg:170.10ms\nstep:3826/10000 train_loss:3.9866 train_time:649088ms step_avg:170.10ms\nstep:3827/10000 train_loss:4.0152 train_time:649260ms step_avg:170.10ms\nstep:3828/10000 train_loss:4.0245 train_time:649431ms step_avg:170.10ms\nstep:3829/10000 train_loss:4.0043 train_time:649603ms step_avg:170.10ms\nstep:3830/10000 train_loss:4.0099 train_time:649770ms step_avg:170.10ms\nstep:3831/10000 train_loss:4.0935 train_time:649941ms step_avg:170.10ms\nstep:3832/10000 train_loss:3.9594 train_time:650110ms step_avg:170.10ms\nstep:3833/10000 train_loss:4.5736 train_time:650279ms step_avg:170.10ms\nstep:3834/10000 train_loss:4.0059 train_time:650449ms step_avg:170.10ms\nstep:3835/10000 train_loss:3.9770 train_time:650618ms step_avg:170.10ms\nstep:3836/10000 train_loss:4.0415 train_time:650787ms step_avg:170.10ms\nstep:3837/10000 train_loss:4.0942 train_time:650957ms step_avg:170.10ms\nstep:3838/10000 train_loss:4.1595 train_time:651128ms step_avg:170.10ms\nstep:3839/10000 train_loss:4.0465 train_time:651299ms step_avg:170.10ms\nstep:3840/10000 train_loss:4.0191 train_time:651471ms step_avg:170.10ms\nstep:3841/10000 train_loss:4.2782 train_time:651642ms step_avg:170.10ms\nstep:3842/10000 train_loss:4.2192 train_time:651809ms step_avg:170.10ms\nstep:3843/10000 train_loss:3.9363 train_time:651977ms step_avg:170.10ms\nstep:3844/10000 train_loss:4.0384 train_time:652145ms step_avg:170.10ms\nstep:3845/10000 train_loss:4.2876 train_time:652317ms step_avg:170.10ms\nstep:3846/10000 train_loss:4.1595 train_time:652488ms step_avg:170.10ms\nstep:3847/10000 train_loss:4.0833 train_time:652660ms step_avg:170.10ms\nstep:3848/10000 train_loss:3.9073 train_time:652829ms step_avg:170.10ms\nstep:3849/10000 train_loss:4.2030 train_time:652999ms step_avg:170.10ms\nstep:3850/10000 train_loss:4.0312 train_time:653170ms step_avg:170.10ms\nstep:3851/10000 train_loss:4.0690 train_time:653340ms step_avg:170.10ms\nstep:3852/10000 train_loss:4.1991 train_time:653509ms step_avg:170.10ms\nstep:3853/10000 train_loss:4.0476 train_time:653679ms step_avg:170.10ms\nstep:3854/10000 train_loss:3.9100 train_time:653847ms step_avg:170.10ms\nstep:3855/10000 train_loss:4.0176 train_time:654017ms step_avg:170.10ms\nstep:3856/10000 train_loss:4.1422 train_time:654186ms step_avg:170.10ms\nstep:3857/10000 train_loss:4.0688 train_time:654354ms step_avg:170.09ms\nstep:3858/10000 train_loss:4.0117 train_time:654524ms step_avg:170.09ms\nstep:3859/10000 train_loss:3.9726 train_time:654693ms step_avg:170.09ms\nstep:3860/10000 train_loss:3.9899 train_time:654864ms step_avg:170.09ms\nstep:3861/10000 train_loss:4.0664 train_time:655035ms step_avg:170.09ms\nstep:3862/10000 train_loss:4.1142 train_time:655207ms step_avg:170.10ms\nstep:3863/10000 train_loss:4.1759 train_time:655378ms step_avg:170.10ms\nstep:3864/10000 train_loss:3.9742 train_time:655547ms step_avg:170.10ms\nstep:3865/10000 train_loss:3.9114 train_time:655715ms step_avg:170.09ms\nstep:3866/10000 train_loss:4.1625 train_time:655886ms step_avg:170.09ms\nstep:3867/10000 train_loss:4.0142 train_time:656053ms step_avg:170.09ms\nstep:3868/10000 train_loss:4.0209 train_time:656224ms step_avg:170.09ms\nstep:3869/10000 train_loss:4.0596 train_time:656391ms step_avg:170.09ms\nstep:3870/10000 train_loss:4.0321 train_time:656563ms step_avg:170.09ms\nstep:3871/10000 train_loss:4.0203 train_time:656735ms step_avg:170.09ms\nstep:3872/10000 train_loss:4.2173 train_time:656907ms step_avg:170.09ms\nstep:3873/10000 train_loss:4.0936 train_time:657076ms step_avg:170.09ms\nstep:3874/10000 train_loss:4.1034 train_time:657248ms step_avg:170.10ms\nstep:3875/10000 train_loss:3.9463 train_time:657417ms step_avg:170.09ms\nstep:3876/10000 train_loss:4.0752 train_time:657590ms step_avg:170.10ms\nstep:3877/10000 train_loss:4.0722 train_time:657762ms step_avg:170.10ms\nstep:3878/10000 train_loss:4.0752 train_time:657930ms step_avg:170.10ms\nstep:3879/10000 train_loss:4.1136 train_time:658101ms step_avg:170.10ms\nstep:3880/10000 train_loss:4.0727 train_time:658270ms step_avg:170.10ms\nstep:3881/10000 train_loss:3.9724 train_time:658442ms step_avg:170.10ms\nstep:3882/10000 train_loss:4.1012 train_time:658611ms step_avg:170.10ms\nstep:3883/10000 train_loss:3.9749 train_time:658781ms step_avg:170.10ms\nstep:3884/10000 train_loss:4.0837 train_time:658948ms step_avg:170.10ms\nstep:3885/10000 train_loss:4.0421 train_time:659117ms step_avg:170.09ms\nstep:3886/10000 train_loss:3.9791 train_time:659285ms step_avg:170.09ms\nstep:3887/10000 train_loss:3.9415 train_time:659456ms step_avg:170.09ms\nstep:3888/10000 train_loss:3.9298 train_time:659627ms step_avg:170.09ms\nstep:3889/10000 train_loss:4.1846 train_time:659797ms step_avg:170.09ms\nstep:3890/10000 train_loss:3.8876 train_time:659969ms step_avg:170.10ms\nstep:3891/10000 train_loss:4.0817 train_time:660140ms step_avg:170.10ms\nstep:3892/10000 train_loss:4.0738 train_time:660307ms step_avg:170.09ms\nstep:3893/10000 train_loss:4.0437 train_time:660476ms step_avg:170.09ms\nstep:3894/10000 train_loss:4.0704 train_time:660649ms step_avg:170.09ms\nstep:3895/10000 train_loss:3.9498 train_time:660818ms step_avg:170.09ms\nstep:3896/10000 train_loss:4.0367 train_time:660989ms step_avg:170.09ms\nstep:3897/10000 train_loss:3.9779 train_time:661160ms step_avg:170.10ms\nstep:3898/10000 train_loss:3.9078 train_time:661330ms step_avg:170.10ms\nstep:3899/10000 train_loss:3.8986 train_time:661500ms step_avg:170.10ms\nstep:3900/10000 train_loss:4.0837 train_time:661669ms step_avg:170.09ms\nstep:3901/10000 train_loss:4.0113 train_time:661840ms step_avg:170.10ms\nstep:3902/10000 train_loss:4.0438 train_time:662009ms step_avg:170.09ms\nstep:3903/10000 train_loss:4.1194 train_time:662179ms step_avg:170.09ms\nstep:3904/10000 train_loss:4.1724 train_time:662349ms step_avg:170.09ms\nstep:3905/10000 train_loss:3.9322 train_time:662520ms step_avg:170.10ms\nstep:3906/10000 train_loss:4.1105 train_time:662689ms step_avg:170.09ms\nstep:3907/10000 train_loss:4.0127 train_time:662860ms step_avg:170.09ms\nstep:3908/10000 train_loss:4.0156 train_time:663029ms step_avg:170.09ms\nstep:3909/10000 train_loss:4.0162 train_time:663202ms step_avg:170.10ms\nstep:3910/10000 train_loss:3.9939 train_time:663372ms step_avg:170.10ms\nstep:3911/10000 train_loss:4.3507 train_time:663542ms step_avg:170.10ms\nstep:3912/10000 train_loss:4.2168 train_time:663712ms step_avg:170.10ms\nstep:3913/10000 train_loss:3.9980 train_time:663881ms step_avg:170.10ms\nstep:3914/10000 train_loss:4.0279 train_time:664052ms step_avg:170.10ms\nstep:3915/10000 train_loss:4.1910 train_time:664222ms step_avg:170.10ms\nstep:3916/10000 train_loss:4.1054 train_time:664391ms step_avg:170.09ms\nstep:3917/10000 train_loss:3.9655 train_time:664563ms step_avg:170.10ms\nstep:3918/10000 train_loss:4.0934 train_time:664730ms step_avg:170.09ms\nstep:3919/10000 train_loss:4.0847 train_time:664900ms step_avg:170.09ms\nstep:3920/10000 train_loss:3.9551 train_time:665067ms step_avg:170.09ms\nstep:3921/10000 train_loss:4.1124 train_time:665238ms step_avg:170.09ms\nstep:3922/10000 train_loss:3.9089 train_time:665407ms step_avg:170.09ms\nstep:3923/10000 train_loss:3.9589 train_time:665577ms step_avg:170.09ms\nstep:3924/10000 train_loss:4.2417 train_time:665746ms step_avg:170.09ms\nstep:3925/10000 train_loss:4.0859 train_time:665919ms step_avg:170.09ms\nstep:3926/10000 train_loss:4.0244 train_time:666088ms step_avg:170.09ms\nstep:3927/10000 train_loss:4.0022 train_time:666257ms step_avg:170.09ms\nstep:3928/10000 train_loss:3.9552 train_time:666426ms step_avg:170.09ms\nstep:3929/10000 train_loss:4.1579 train_time:666595ms step_avg:170.09ms\nstep:3930/10000 train_loss:3.9874 train_time:666767ms step_avg:170.09ms\nstep:3931/10000 train_loss:4.0748 train_time:666935ms step_avg:170.09ms\nstep:3932/10000 train_loss:4.1143 train_time:667107ms step_avg:170.09ms\nstep:3933/10000 train_loss:4.0315 train_time:667275ms step_avg:170.09ms\nstep:3934/10000 train_loss:4.0177 train_time:667448ms step_avg:170.09ms\nstep:3935/10000 train_loss:4.0296 train_time:667616ms step_avg:170.09ms\nstep:3936/10000 train_loss:4.1077 train_time:667787ms step_avg:170.09ms\nstep:3937/10000 train_loss:4.0544 train_time:667955ms step_avg:170.09ms\nstep:3938/10000 train_loss:4.1365 train_time:668127ms step_avg:170.09ms\nstep:3939/10000 train_loss:3.8887 train_time:668300ms step_avg:170.09ms\nstep:3940/10000 train_loss:4.1417 train_time:668471ms step_avg:170.09ms\nstep:3941/10000 train_loss:4.0296 train_time:668640ms step_avg:170.09ms\nstep:3942/10000 train_loss:4.0110 train_time:668809ms step_avg:170.09ms\nstep:3943/10000 train_loss:3.9875 train_time:668982ms step_avg:170.09ms\nstep:3944/10000 train_loss:4.0483 train_time:669150ms step_avg:170.09ms\nstep:3945/10000 train_loss:4.1276 train_time:669319ms step_avg:170.09ms\nstep:3946/10000 train_loss:4.0106 train_time:669490ms step_avg:170.09ms\nstep:3947/10000 train_loss:3.9237 train_time:669665ms step_avg:170.10ms\nstep:3948/10000 train_loss:4.1335 train_time:669835ms step_avg:170.10ms\nstep:3949/10000 train_loss:4.0831 train_time:670005ms step_avg:170.10ms\nstep:3950/10000 train_loss:4.0262 train_time:670174ms step_avg:170.09ms\nstep:3951/10000 train_loss:4.4697 train_time:670346ms step_avg:170.10ms\nstep:3952/10000 train_loss:3.9506 train_time:670518ms step_avg:170.10ms\nstep:3953/10000 train_loss:4.1206 train_time:670688ms step_avg:170.10ms\nstep:3954/10000 train_loss:4.1297 train_time:670863ms step_avg:170.10ms\nstep:3955/10000 train_loss:3.9353 train_time:671032ms step_avg:170.10ms\nstep:3956/10000 train_loss:4.0258 train_time:671196ms step_avg:170.10ms\nstep:3957/10000 train_loss:4.1828 train_time:671371ms step_avg:170.10ms\nstep:3958/10000 train_loss:3.8994 train_time:671534ms step_avg:170.09ms\nstep:3959/10000 train_loss:4.1176 train_time:671702ms step_avg:170.09ms\nstep:3960/10000 train_loss:4.0380 train_time:671873ms step_avg:170.09ms\nstep:3961/10000 train_loss:4.0675 train_time:672045ms step_avg:170.10ms\nstep:3962/10000 train_loss:3.9361 train_time:672214ms step_avg:170.09ms\nstep:3963/10000 train_loss:3.8562 train_time:672385ms step_avg:170.09ms\nstep:3964/10000 train_loss:3.9749 train_time:672554ms step_avg:170.09ms\nstep:3965/10000 train_loss:3.9974 train_time:672724ms step_avg:170.09ms\nstep:3966/10000 train_loss:4.0946 train_time:672895ms step_avg:170.09ms\nstep:3967/10000 train_loss:3.9588 train_time:673067ms step_avg:170.10ms\nstep:3968/10000 train_loss:4.1347 train_time:673236ms step_avg:170.09ms\nstep:3969/10000 train_loss:3.9689 train_time:673404ms step_avg:170.09ms\nstep:3970/10000 train_loss:4.0220 train_time:673573ms step_avg:170.09ms\nstep:3971/10000 train_loss:4.0276 train_time:673742ms step_avg:170.09ms\nstep:3972/10000 train_loss:4.0170 train_time:673913ms step_avg:170.09ms\nstep:3973/10000 train_loss:4.0600 train_time:674086ms step_avg:170.09ms\nstep:3974/10000 train_loss:4.0456 train_time:674257ms step_avg:170.10ms\nstep:3975/10000 train_loss:3.9442 train_time:674429ms step_avg:170.10ms\nstep:3976/10000 train_loss:3.9687 train_time:674600ms step_avg:170.10ms\nstep:3977/10000 train_loss:4.0930 train_time:674769ms step_avg:170.10ms\nstep:3978/10000 train_loss:4.0015 train_time:674941ms step_avg:170.10ms\nstep:3979/10000 train_loss:4.0608 train_time:675111ms step_avg:170.10ms\nstep:3980/10000 train_loss:4.0680 train_time:675283ms step_avg:170.10ms\nstep:3981/10000 train_loss:3.9465 train_time:675454ms step_avg:170.10ms\nstep:3982/10000 train_loss:3.9562 train_time:675625ms step_avg:170.10ms\nstep:3983/10000 train_loss:4.3071 train_time:675793ms step_avg:170.10ms\nstep:3984/10000 train_loss:4.1226 train_time:675966ms step_avg:170.10ms\nstep:3985/10000 train_loss:4.1544 train_time:676135ms step_avg:170.10ms\nstep:3986/10000 train_loss:3.9972 train_time:676305ms step_avg:170.10ms\nstep:3987/10000 train_loss:4.1012 train_time:676473ms step_avg:170.10ms\nstep:3988/10000 train_loss:4.0806 train_time:676643ms step_avg:170.10ms\nstep:3989/10000 train_loss:3.9851 train_time:676813ms step_avg:170.10ms\nstep:3990/10000 train_loss:3.9991 train_time:676983ms step_avg:170.10ms\nstep:3991/10000 train_loss:3.9667 train_time:677151ms step_avg:170.10ms\nstep:3992/10000 train_loss:3.9732 train_time:677325ms step_avg:170.10ms\nstep:3993/10000 train_loss:3.8074 train_time:677492ms step_avg:170.10ms\nstep:3994/10000 train_loss:4.1340 train_time:677662ms step_avg:170.10ms\nstep:3995/10000 train_loss:4.0482 train_time:677832ms step_avg:170.10ms\nstep:3996/10000 train_loss:4.3692 train_time:678001ms step_avg:170.10ms\nstep:3997/10000 train_loss:3.9515 train_time:678170ms step_avg:170.10ms\nstep:3998/10000 train_loss:3.9485 train_time:678342ms step_avg:170.10ms\nstep:3999/10000 train_loss:4.0568 train_time:678512ms step_avg:170.10ms\nstep:4000/10000 train_loss:3.9910 train_time:678682ms step_avg:170.10ms\nstep:4001/10000 train_loss:4.0869 train_time:678852ms step_avg:170.10ms\nstep:4002/10000 train_loss:4.1765 train_time:679025ms step_avg:170.10ms\nstep:4003/10000 train_loss:4.0777 train_time:679193ms step_avg:170.10ms\nstep:4004/10000 train_loss:4.1001 train_time:679364ms step_avg:170.10ms\nstep:4005/10000 train_loss:4.0674 train_time:679535ms step_avg:170.10ms\nstep:4006/10000 train_loss:4.1055 train_time:679705ms step_avg:170.10ms\nstep:4007/10000 train_loss:4.1603 train_time:679874ms step_avg:170.10ms\nstep:4008/10000 train_loss:3.9833 train_time:680044ms step_avg:170.10ms\nstep:4009/10000 train_loss:4.2333 train_time:680214ms step_avg:170.10ms\nstep:4010/10000 train_loss:4.0804 train_time:680386ms step_avg:170.10ms\nstep:4011/10000 train_loss:4.1163 train_time:680557ms step_avg:170.10ms\nstep:4012/10000 train_loss:4.0591 train_time:680730ms step_avg:170.10ms\nstep:4013/10000 train_loss:4.0212 train_time:680899ms step_avg:170.10ms\nstep:4014/10000 train_loss:4.0565 train_time:681069ms step_avg:170.10ms\nstep:4015/10000 train_loss:4.0285 train_time:681237ms step_avg:170.10ms\nstep:4016/10000 train_loss:3.9280 train_time:681408ms step_avg:170.10ms\nstep:4017/10000 train_loss:4.0945 train_time:681579ms step_avg:170.10ms\nstep:4018/10000 train_loss:4.0347 train_time:681750ms step_avg:170.10ms\nstep:4019/10000 train_loss:4.0781 train_time:681921ms step_avg:170.10ms\nstep:4020/10000 train_loss:4.1058 train_time:682089ms step_avg:170.10ms\nstep:4021/10000 train_loss:3.9787 train_time:682258ms step_avg:170.10ms\nstep:4022/10000 train_loss:4.0380 train_time:682428ms step_avg:170.10ms\nstep:4023/10000 train_loss:4.2624 train_time:682600ms step_avg:170.10ms\nstep:4024/10000 train_loss:4.1609 train_time:682769ms step_avg:170.10ms\nstep:4025/10000 train_loss:4.0283 train_time:682941ms step_avg:170.10ms\nstep:4026/10000 train_loss:4.0002 train_time:683108ms step_avg:170.10ms\nstep:4027/10000 train_loss:4.2146 train_time:683276ms step_avg:170.10ms\nstep:4028/10000 train_loss:3.9287 train_time:683447ms step_avg:170.10ms\nstep:4029/10000 train_loss:3.9408 train_time:683615ms step_avg:170.10ms\nstep:4030/10000 train_loss:3.9970 train_time:683786ms step_avg:170.10ms\nstep:4031/10000 train_loss:4.1011 train_time:683955ms step_avg:170.10ms\nstep:4032/10000 train_loss:4.0187 train_time:684129ms step_avg:170.10ms\nstep:4033/10000 train_loss:3.9704 train_time:684298ms step_avg:170.10ms\nstep:4034/10000 train_loss:4.0005 train_time:684471ms step_avg:170.10ms\nstep:4035/10000 train_loss:3.9801 train_time:684640ms step_avg:170.10ms\nstep:4036/10000 train_loss:3.9863 train_time:684811ms step_avg:170.10ms\nstep:4037/10000 train_loss:4.2656 train_time:684979ms step_avg:170.10ms\nstep:4038/10000 train_loss:4.0028 train_time:685148ms step_avg:170.10ms\nstep:4039/10000 train_loss:4.0292 train_time:685317ms step_avg:170.10ms\nstep:4040/10000 train_loss:4.1065 train_time:685485ms step_avg:170.10ms\nstep:4041/10000 train_loss:3.9804 train_time:685656ms step_avg:170.10ms\nstep:4042/10000 train_loss:4.2892 train_time:685829ms step_avg:170.10ms\nstep:4043/10000 train_loss:4.0943 train_time:685999ms step_avg:170.10ms\nstep:4044/10000 train_loss:4.1895 train_time:686168ms step_avg:170.10ms\nstep:4045/10000 train_loss:4.0682 train_time:686339ms step_avg:170.10ms\nstep:4046/10000 train_loss:4.1830 train_time:686511ms step_avg:170.10ms\nstep:4047/10000 train_loss:4.1280 train_time:686677ms step_avg:170.10ms\nstep:4048/10000 train_loss:4.0914 train_time:686846ms step_avg:170.10ms\nstep:4049/10000 train_loss:4.1576 train_time:687014ms step_avg:170.10ms\nstep:4050/10000 train_loss:3.9502 train_time:687182ms step_avg:170.09ms\nstep:4051/10000 train_loss:4.0846 train_time:687354ms step_avg:170.09ms\nstep:4052/10000 train_loss:4.0467 train_time:687525ms step_avg:170.10ms\nstep:4053/10000 train_loss:4.0407 train_time:687694ms step_avg:170.09ms\nstep:4054/10000 train_loss:4.0035 train_time:687866ms step_avg:170.10ms\nstep:4055/10000 train_loss:4.3722 train_time:688036ms step_avg:170.10ms\nstep:4056/10000 train_loss:3.9175 train_time:688204ms step_avg:170.10ms\nstep:4057/10000 train_loss:4.1167 train_time:688373ms step_avg:170.09ms\nstep:4058/10000 train_loss:3.9900 train_time:688542ms step_avg:170.09ms\nstep:4059/10000 train_loss:4.0237 train_time:688712ms step_avg:170.09ms\nstep:4060/10000 train_loss:4.0625 train_time:688883ms step_avg:170.09ms\nstep:4061/10000 train_loss:4.0901 train_time:689052ms step_avg:170.09ms\nstep:4062/10000 train_loss:4.0584 train_time:689223ms step_avg:170.09ms\nstep:4063/10000 train_loss:4.1109 train_time:689391ms step_avg:170.09ms\nstep:4064/10000 train_loss:4.1866 train_time:689560ms step_avg:170.09ms\nstep:4065/10000 train_loss:3.9280 train_time:689731ms step_avg:170.09ms\nstep:4066/10000 train_loss:4.1297 train_time:689900ms step_avg:170.09ms\nstep:4067/10000 train_loss:3.9741 train_time:690070ms step_avg:170.09ms\nstep:4068/10000 train_loss:3.8725 train_time:690238ms step_avg:170.09ms\nstep:4069/10000 train_loss:4.2649 train_time:690406ms step_avg:170.09ms\nstep:4070/10000 train_loss:3.9577 train_time:690575ms step_avg:170.09ms\nstep:4071/10000 train_loss:3.9995 train_time:690747ms step_avg:170.09ms\nstep:4072/10000 train_loss:3.9945 train_time:690915ms step_avg:170.09ms\nstep:4073/10000 train_loss:4.0240 train_time:691086ms step_avg:170.09ms\nstep:4074/10000 train_loss:4.0156 train_time:691254ms step_avg:170.09ms\nstep:4075/10000 train_loss:4.0412 train_time:691423ms step_avg:170.09ms\nstep:4076/10000 train_loss:4.0236 train_time:691591ms step_avg:170.09ms\nstep:4077/10000 train_loss:4.0035 train_time:691763ms step_avg:170.09ms\nstep:4078/10000 train_loss:3.9811 train_time:691933ms step_avg:170.09ms\nstep:4079/10000 train_loss:4.0170 train_time:692104ms step_avg:170.09ms\nstep:4080/10000 train_loss:3.9202 train_time:692274ms step_avg:170.09ms\nstep:4081/10000 train_loss:3.9297 train_time:692444ms step_avg:170.09ms\nstep:4082/10000 train_loss:4.1532 train_time:692615ms step_avg:170.09ms\nstep:4083/10000 train_loss:4.0267 train_time:692786ms step_avg:170.09ms\nstep:4084/10000 train_loss:4.0778 train_time:692954ms step_avg:170.09ms\nstep:4085/10000 train_loss:4.3721 train_time:693123ms step_avg:170.09ms\nstep:4086/10000 train_loss:3.9229 train_time:693292ms step_avg:170.09ms\nstep:4087/10000 train_loss:4.0562 train_time:693464ms step_avg:170.09ms\nstep:4088/10000 train_loss:4.0105 train_time:693634ms step_avg:170.09ms\nstep:4089/10000 train_loss:4.0024 train_time:693808ms step_avg:170.09ms\nstep:4090/10000 train_loss:4.2730 train_time:693979ms step_avg:170.09ms\nstep:4091/10000 train_loss:4.0447 train_time:694150ms step_avg:170.09ms\nstep:4092/10000 train_loss:3.9227 train_time:694315ms step_avg:170.09ms\nstep:4093/10000 train_loss:3.9860 train_time:694484ms step_avg:170.09ms\nstep:4094/10000 train_loss:4.1378 train_time:694656ms step_avg:170.09ms\nstep:4095/10000 train_loss:4.0861 train_time:694823ms step_avg:170.09ms\nstep:4096/10000 train_loss:4.0203 train_time:694995ms step_avg:170.09ms\nstep:4097/10000 train_loss:4.0158 train_time:695166ms step_avg:170.09ms\nstep:4098/10000 train_loss:3.8728 train_time:695331ms step_avg:170.09ms\nstep:4099/10000 train_loss:4.1039 train_time:695502ms step_avg:170.09ms\nstep:4100/10000 train_loss:4.0544 train_time:695672ms step_avg:170.09ms\nstep:4101/10000 train_loss:4.1231 train_time:695844ms step_avg:170.09ms\nstep:4102/10000 train_loss:4.0364 train_time:696018ms step_avg:170.09ms\nstep:4103/10000 train_loss:4.0339 train_time:696186ms step_avg:170.09ms\nstep:4104/10000 train_loss:4.0141 train_time:696355ms step_avg:170.09ms\nstep:4105/10000 train_loss:4.0347 train_time:696526ms step_avg:170.09ms\nstep:4106/10000 train_loss:3.9884 train_time:696696ms step_avg:170.09ms\nstep:4107/10000 train_loss:4.0887 train_time:696866ms step_avg:170.09ms\nstep:4108/10000 train_loss:4.0650 train_time:697037ms step_avg:170.09ms\nstep:4109/10000 train_loss:3.9470 train_time:697206ms step_avg:170.09ms\nstep:4110/10000 train_loss:4.0332 train_time:697374ms step_avg:170.09ms\nstep:4111/10000 train_loss:4.1584 train_time:697543ms step_avg:170.09ms\nstep:4112/10000 train_loss:4.2007 train_time:697712ms step_avg:170.09ms\nstep:4113/10000 train_loss:3.9311 train_time:697884ms step_avg:170.09ms\nstep:4114/10000 train_loss:3.9945 train_time:698055ms step_avg:170.09ms\nstep:4115/10000 train_loss:4.0727 train_time:698226ms step_avg:170.09ms\nstep:4116/10000 train_loss:3.8699 train_time:698394ms step_avg:170.09ms\nstep:4117/10000 train_loss:4.1939 train_time:698564ms step_avg:170.09ms\nstep:4118/10000 train_loss:3.9169 train_time:698732ms step_avg:170.09ms\nstep:4119/10000 train_loss:4.0632 train_time:698906ms step_avg:170.09ms\nstep:4120/10000 train_loss:3.9020 train_time:699075ms step_avg:170.09ms\nstep:4121/10000 train_loss:4.0250 train_time:699249ms step_avg:170.09ms\nstep:4122/10000 train_loss:4.0106 train_time:699413ms step_avg:170.09ms\nstep:4123/10000 train_loss:3.9095 train_time:699581ms step_avg:170.09ms\nstep:4124/10000 train_loss:3.9138 train_time:699750ms step_avg:170.09ms\nstep:4125/10000 train_loss:4.0817 train_time:699922ms step_avg:170.09ms\nstep:4126/10000 train_loss:4.4302 train_time:700105ms step_avg:170.09ms\nstep:4127/10000 train_loss:3.9318 train_time:700263ms step_avg:170.09ms\nstep:4128/10000 train_loss:4.0170 train_time:700430ms step_avg:170.09ms\nstep:4129/10000 train_loss:3.9521 train_time:700600ms step_avg:170.09ms\nstep:4130/10000 train_loss:3.9612 train_time:700768ms step_avg:170.09ms\nstep:4131/10000 train_loss:4.0469 train_time:700939ms step_avg:170.09ms\nstep:4132/10000 train_loss:4.0463 train_time:701111ms step_avg:170.09ms\nstep:4133/10000 train_loss:3.8334 train_time:701282ms step_avg:170.09ms\nstep:4134/10000 train_loss:3.9507 train_time:701451ms step_avg:170.09ms\nstep:4135/10000 train_loss:4.0141 train_time:701619ms step_avg:170.09ms\nstep:4136/10000 train_loss:4.2037 train_time:701788ms step_avg:170.09ms\nstep:4137/10000 train_loss:4.0480 train_time:701959ms step_avg:170.09ms\nstep:4138/10000 train_loss:4.0389 train_time:702131ms step_avg:170.09ms\nstep:4139/10000 train_loss:4.1022 train_time:702305ms step_avg:170.09ms\nstep:4140/10000 train_loss:4.0341 train_time:702475ms step_avg:170.09ms\nstep:4141/10000 train_loss:3.9930 train_time:702644ms step_avg:170.09ms\nstep:4142/10000 train_loss:4.0496 train_time:702813ms step_avg:170.09ms\nstep:4143/10000 train_loss:4.1117 train_time:702985ms step_avg:170.09ms\nstep:4144/10000 train_loss:4.0089 train_time:703157ms step_avg:170.09ms\nstep:4145/10000 train_loss:4.0578 train_time:703324ms step_avg:170.09ms\nstep:4146/10000 train_loss:3.8944 train_time:703492ms step_avg:170.09ms\nstep:4147/10000 train_loss:4.1865 train_time:703663ms step_avg:170.09ms\nstep:4148/10000 train_loss:3.9547 train_time:703833ms step_avg:170.09ms\nstep:4149/10000 train_loss:3.9895 train_time:704006ms step_avg:170.09ms\nstep:4150/10000 train_loss:3.8714 train_time:704174ms step_avg:170.09ms\nstep:4151/10000 train_loss:4.0321 train_time:704345ms step_avg:170.09ms\nstep:4152/10000 train_loss:4.0581 train_time:704514ms step_avg:170.09ms\nstep:4153/10000 train_loss:3.9639 train_time:704684ms step_avg:170.09ms\nstep:4154/10000 train_loss:4.0928 train_time:704852ms step_avg:170.09ms\nstep:4155/10000 train_loss:4.1646 train_time:705022ms step_avg:170.09ms\nstep:4156/10000 train_loss:4.3474 train_time:705189ms step_avg:170.09ms\nstep:4157/10000 train_loss:3.9777 train_time:705361ms step_avg:170.09ms\nstep:4158/10000 train_loss:3.8789 train_time:705530ms step_avg:170.09ms\nstep:4159/10000 train_loss:4.1014 train_time:705699ms step_avg:170.09ms\nstep:4160/10000 train_loss:4.1518 train_time:705870ms step_avg:170.09ms\nstep:4161/10000 train_loss:4.0328 train_time:706042ms step_avg:170.09ms\nstep:4162/10000 train_loss:3.9287 train_time:706212ms step_avg:170.09ms\nstep:4163/10000 train_loss:3.9838 train_time:706382ms step_avg:170.09ms\nstep:4164/10000 train_loss:4.0471 train_time:706549ms step_avg:170.09ms\nstep:4165/10000 train_loss:3.9317 train_time:706718ms step_avg:170.09ms\nstep:4166/10000 train_loss:4.0293 train_time:706887ms step_avg:170.09ms\nstep:4167/10000 train_loss:4.0418 train_time:707058ms step_avg:170.09ms\nstep:4168/10000 train_loss:3.9621 train_time:707226ms step_avg:170.09ms\nstep:4169/10000 train_loss:4.0384 train_time:707397ms step_avg:170.09ms\nstep:4170/10000 train_loss:3.9148 train_time:707568ms step_avg:170.09ms\nstep:4171/10000 train_loss:4.0957 train_time:707739ms step_avg:170.09ms\nstep:4172/10000 train_loss:4.0046 train_time:707908ms step_avg:170.09ms\nstep:4173/10000 train_loss:4.2093 train_time:708078ms step_avg:170.09ms\nstep:4174/10000 train_loss:3.9502 train_time:708248ms step_avg:170.09ms\nstep:4175/10000 train_loss:3.9603 train_time:708417ms step_avg:170.09ms\nstep:4176/10000 train_loss:4.0647 train_time:708585ms step_avg:170.09ms\nstep:4177/10000 train_loss:4.0297 train_time:708757ms step_avg:170.09ms\nstep:4178/10000 train_loss:4.0558 train_time:708930ms step_avg:170.09ms\nstep:4179/10000 train_loss:4.2149 train_time:709101ms step_avg:170.09ms\nstep:4180/10000 train_loss:3.9786 train_time:709269ms step_avg:170.09ms\nstep:4181/10000 train_loss:3.9897 train_time:709439ms step_avg:170.09ms\nstep:4182/10000 train_loss:4.1888 train_time:709607ms step_avg:170.09ms\nstep:4183/10000 train_loss:4.1242 train_time:709777ms step_avg:170.09ms\nstep:4184/10000 train_loss:3.9851 train_time:709949ms step_avg:170.09ms\nstep:4185/10000 train_loss:3.9653 train_time:710117ms step_avg:170.09ms\nstep:4186/10000 train_loss:3.8560 train_time:710285ms step_avg:170.09ms\nstep:4187/10000 train_loss:3.9698 train_time:710455ms step_avg:170.09ms\nstep:4188/10000 train_loss:4.0886 train_time:710628ms step_avg:170.09ms\nstep:4189/10000 train_loss:4.0548 train_time:710796ms step_avg:170.09ms\nstep:4190/10000 train_loss:4.2199 train_time:710970ms step_avg:170.09ms\nstep:4191/10000 train_loss:3.9995 train_time:711141ms step_avg:170.09ms\nstep:4192/10000 train_loss:3.9953 train_time:711311ms step_avg:170.09ms\nstep:4193/10000 train_loss:4.0029 train_time:711479ms step_avg:170.09ms\nstep:4194/10000 train_loss:3.9153 train_time:711647ms step_avg:170.09ms\nstep:4195/10000 train_loss:4.0061 train_time:711818ms step_avg:170.09ms\nstep:4196/10000 train_loss:4.2290 train_time:711988ms step_avg:170.09ms\nstep:4197/10000 train_loss:3.9916 train_time:712159ms step_avg:170.09ms\nstep:4198/10000 train_loss:4.4807 train_time:712330ms step_avg:170.09ms\nstep:4199/10000 train_loss:4.0239 train_time:712500ms step_avg:170.09ms\nstep:4200/10000 train_loss:4.0409 train_time:712669ms step_avg:170.09ms\nstep:4201/10000 train_loss:3.8572 train_time:712840ms step_avg:170.09ms\nstep:4202/10000 train_loss:4.0563 train_time:713009ms step_avg:170.09ms\nstep:4203/10000 train_loss:4.0453 train_time:713177ms step_avg:170.09ms\nstep:4204/10000 train_loss:3.9823 train_time:713347ms step_avg:170.09ms\nstep:4205/10000 train_loss:4.0640 train_time:713516ms step_avg:170.09ms\nstep:4206/10000 train_loss:3.9600 train_time:713688ms step_avg:170.09ms\nstep:4207/10000 train_loss:3.9805 train_time:713857ms step_avg:170.09ms\nstep:4208/10000 train_loss:3.9307 train_time:714025ms step_avg:170.09ms\nstep:4209/10000 train_loss:4.0111 train_time:714195ms step_avg:170.09ms\nstep:4210/10000 train_loss:3.9332 train_time:714363ms step_avg:170.09ms\nstep:4211/10000 train_loss:4.0039 train_time:714531ms step_avg:170.09ms\nstep:4212/10000 train_loss:4.0580 train_time:714703ms step_avg:170.09ms\nstep:4213/10000 train_loss:4.0327 train_time:714870ms step_avg:170.09ms\nstep:4214/10000 train_loss:4.0420 train_time:715042ms step_avg:170.09ms\nstep:4215/10000 train_loss:4.0787 train_time:715212ms step_avg:170.09ms\nstep:4216/10000 train_loss:3.9450 train_time:715383ms step_avg:170.09ms\nstep:4217/10000 train_loss:3.9936 train_time:715553ms step_avg:170.09ms\nstep:4218/10000 train_loss:3.9922 train_time:715723ms step_avg:170.09ms\nstep:4219/10000 train_loss:4.0787 train_time:715896ms step_avg:170.09ms\nstep:4220/10000 train_loss:4.2681 train_time:716069ms step_avg:170.09ms\nstep:4221/10000 train_loss:4.0563 train_time:716233ms step_avg:170.09ms\nstep:4222/10000 train_loss:4.0051 train_time:716403ms step_avg:170.09ms\nstep:4223/10000 train_loss:3.9335 train_time:716572ms step_avg:170.09ms\nstep:4224/10000 train_loss:4.0776 train_time:716742ms step_avg:170.09ms\nstep:4225/10000 train_loss:4.0997 train_time:716911ms step_avg:170.09ms\nstep:4226/10000 train_loss:3.9495 train_time:717084ms step_avg:170.09ms\nstep:4227/10000 train_loss:3.9121 train_time:717252ms step_avg:170.09ms\nstep:4228/10000 train_loss:3.9532 train_time:717421ms step_avg:170.09ms\nstep:4229/10000 train_loss:3.9284 train_time:717590ms step_avg:170.09ms\nstep:4230/10000 train_loss:3.9814 train_time:717760ms step_avg:170.09ms\nstep:4231/10000 train_loss:4.0111 train_time:717931ms step_avg:170.09ms\nstep:4232/10000 train_loss:4.0526 train_time:718104ms step_avg:170.09ms\nstep:4233/10000 train_loss:3.9171 train_time:718275ms step_avg:170.09ms\nstep:4234/10000 train_loss:4.0628 train_time:718446ms step_avg:170.09ms\nstep:4235/10000 train_loss:3.9085 train_time:718614ms step_avg:170.09ms\nstep:4236/10000 train_loss:4.1129 train_time:718784ms step_avg:170.09ms\nstep:4237/10000 train_loss:3.9958 train_time:718955ms step_avg:170.09ms\nstep:4238/10000 train_loss:4.1848 train_time:719125ms step_avg:170.09ms\nstep:4239/10000 train_loss:3.9588 train_time:719295ms step_avg:170.09ms\nstep:4240/10000 train_loss:3.8469 train_time:719466ms step_avg:170.09ms\nstep:4241/10000 train_loss:3.9494 train_time:719634ms step_avg:170.09ms\nstep:4242/10000 train_loss:3.9852 train_time:719805ms step_avg:170.09ms\nstep:4243/10000 train_loss:3.9969 train_time:719976ms step_avg:170.09ms\nstep:4244/10000 train_loss:4.0086 train_time:720147ms step_avg:170.09ms\nstep:4245/10000 train_loss:4.0500 train_time:720316ms step_avg:170.09ms\nstep:4246/10000 train_loss:4.2003 train_time:720487ms step_avg:170.09ms\nstep:4247/10000 train_loss:4.0431 train_time:720656ms step_avg:170.09ms\nstep:4248/10000 train_loss:4.0006 train_time:720825ms step_avg:170.09ms\nstep:4249/10000 train_loss:3.9913 train_time:720995ms step_avg:170.09ms\nstep:4250/10000 train_loss:3.9107 train_time:721166ms step_avg:170.09ms\nstep:4251/10000 train_loss:3.8262 train_time:721336ms step_avg:170.09ms\nstep:4252/10000 train_loss:3.9676 train_time:721508ms step_avg:170.09ms\nstep:4253/10000 train_loss:4.1510 train_time:721677ms step_avg:170.09ms\nstep:4254/10000 train_loss:3.9537 train_time:721849ms step_avg:170.09ms\nstep:4255/10000 train_loss:4.0835 train_time:722020ms step_avg:170.09ms\nstep:4256/10000 train_loss:3.9381 train_time:722189ms step_avg:170.09ms\nstep:4257/10000 train_loss:3.9579 train_time:722360ms step_avg:170.09ms\nstep:4258/10000 train_loss:4.1279 train_time:722530ms step_avg:170.09ms\nstep:4259/10000 train_loss:4.0095 train_time:722699ms step_avg:170.09ms\nstep:4260/10000 train_loss:4.0531 train_time:722869ms step_avg:170.09ms\nstep:4261/10000 train_loss:3.9454 train_time:723042ms step_avg:170.09ms\nstep:4262/10000 train_loss:4.0319 train_time:723208ms step_avg:170.09ms\nstep:4263/10000 train_loss:3.9916 train_time:723378ms step_avg:170.09ms\nstep:4264/10000 train_loss:3.9617 train_time:723547ms step_avg:170.09ms\nstep:4265/10000 train_loss:4.0769 train_time:723716ms step_avg:170.09ms\nstep:4266/10000 train_loss:4.1632 train_time:723889ms step_avg:170.09ms\nstep:4267/10000 train_loss:4.1141 train_time:724059ms step_avg:170.09ms\nstep:4268/10000 train_loss:3.9121 train_time:724230ms step_avg:170.09ms\nstep:4269/10000 train_loss:4.0687 train_time:724403ms step_avg:170.09ms\nstep:4270/10000 train_loss:3.6935 train_time:724571ms step_avg:170.09ms\nstep:4271/10000 train_loss:4.1336 train_time:724742ms step_avg:170.09ms\nstep:4272/10000 train_loss:3.9199 train_time:724912ms step_avg:170.09ms\nstep:4273/10000 train_loss:4.0856 train_time:725084ms step_avg:170.09ms\nstep:4274/10000 train_loss:3.9192 train_time:725252ms step_avg:170.09ms\nstep:4275/10000 train_loss:3.9845 train_time:725424ms step_avg:170.09ms\nstep:4276/10000 train_loss:3.8747 train_time:725592ms step_avg:170.09ms\nstep:4277/10000 train_loss:4.0513 train_time:725762ms step_avg:170.09ms\nstep:4278/10000 train_loss:4.0230 train_time:725930ms step_avg:170.09ms\nstep:4279/10000 train_loss:3.9572 train_time:726103ms step_avg:170.09ms\nstep:4280/10000 train_loss:3.9534 train_time:726270ms step_avg:170.09ms\nstep:4281/10000 train_loss:3.9438 train_time:726440ms step_avg:170.09ms\nstep:4282/10000 train_loss:4.0168 train_time:726607ms step_avg:170.09ms\nstep:4283/10000 train_loss:4.2053 train_time:726776ms step_avg:170.09ms\nstep:4284/10000 train_loss:3.8722 train_time:726947ms step_avg:170.09ms\nstep:4285/10000 train_loss:4.0785 train_time:727116ms step_avg:170.09ms\nstep:4286/10000 train_loss:3.9658 train_time:727289ms step_avg:170.09ms\nstep:4287/10000 train_loss:4.0842 train_time:727460ms step_avg:170.09ms\nstep:4288/10000 train_loss:4.0657 train_time:727631ms step_avg:170.09ms\nstep:4289/10000 train_loss:4.1368 train_time:727802ms step_avg:170.09ms\nstep:4290/10000 train_loss:3.8982 train_time:727971ms step_avg:170.09ms\nstep:4291/10000 train_loss:3.9618 train_time:728143ms step_avg:170.09ms\nstep:4292/10000 train_loss:4.0379 train_time:728313ms step_avg:170.09ms\nstep:4293/10000 train_loss:3.9416 train_time:728485ms step_avg:170.09ms\nstep:4294/10000 train_loss:3.9632 train_time:728653ms step_avg:170.09ms\nstep:4295/10000 train_loss:3.9575 train_time:728826ms step_avg:170.09ms\nstep:4296/10000 train_loss:4.0039 train_time:728996ms step_avg:170.09ms\nstep:4297/10000 train_loss:3.9611 train_time:729165ms step_avg:170.09ms\nstep:4298/10000 train_loss:4.0392 train_time:729335ms step_avg:170.09ms\nstep:4299/10000 train_loss:4.0908 train_time:729504ms step_avg:170.09ms\nstep:4300/10000 train_loss:4.0565 train_time:729672ms step_avg:170.09ms\nstep:4301/10000 train_loss:3.7835 train_time:729842ms step_avg:170.09ms\nstep:4302/10000 train_loss:3.9098 train_time:730012ms step_avg:170.09ms\nstep:4303/10000 train_loss:3.9025 train_time:730184ms step_avg:170.09ms\nstep:4304/10000 train_loss:4.0344 train_time:730352ms step_avg:170.09ms\nstep:4305/10000 train_loss:4.1375 train_time:730523ms step_avg:170.09ms\nstep:4306/10000 train_loss:4.1205 train_time:730692ms step_avg:170.09ms\nstep:4307/10000 train_loss:3.9108 train_time:730862ms step_avg:170.09ms\nstep:4308/10000 train_loss:3.9417 train_time:731030ms step_avg:170.09ms\nstep:4309/10000 train_loss:4.0651 train_time:731199ms step_avg:170.09ms\nstep:4310/10000 train_loss:4.0209 train_time:731368ms step_avg:170.09ms\nstep:4311/10000 train_loss:4.0404 train_time:731536ms step_avg:170.09ms\nstep:4312/10000 train_loss:4.3161 train_time:731706ms step_avg:170.08ms\nstep:4313/10000 train_loss:4.1145 train_time:731875ms step_avg:170.08ms\nstep:4314/10000 train_loss:3.9916 train_time:732048ms step_avg:170.09ms\nstep:4315/10000 train_loss:4.0138 train_time:732216ms step_avg:170.08ms\nstep:4316/10000 train_loss:4.0734 train_time:732385ms step_avg:170.08ms\nstep:4317/10000 train_loss:3.8847 train_time:732552ms step_avg:170.08ms\nstep:4318/10000 train_loss:3.9825 train_time:732722ms step_avg:170.08ms\nstep:4319/10000 train_loss:4.0148 train_time:732891ms step_avg:170.08ms\nstep:4320/10000 train_loss:3.7671 train_time:733063ms step_avg:170.08ms\nstep:4321/10000 train_loss:3.9501 train_time:733230ms step_avg:170.08ms\nstep:4322/10000 train_loss:4.0576 train_time:733401ms step_avg:170.08ms\nstep:4323/10000 train_loss:4.0293 train_time:733573ms step_avg:170.08ms\nstep:4324/10000 train_loss:4.0713 train_time:733745ms step_avg:170.08ms\nstep:4325/10000 train_loss:4.0003 train_time:733914ms step_avg:170.08ms\nstep:4326/10000 train_loss:4.0640 train_time:734085ms step_avg:170.08ms\nstep:4327/10000 train_loss:3.9653 train_time:734253ms step_avg:170.08ms\nstep:4328/10000 train_loss:3.9962 train_time:734424ms step_avg:170.08ms\nstep:4329/10000 train_loss:4.0221 train_time:734593ms step_avg:170.08ms\nstep:4330/10000 train_loss:4.0181 train_time:734767ms step_avg:170.08ms\nstep:4331/10000 train_loss:3.9194 train_time:734937ms step_avg:170.08ms\nstep:4332/10000 train_loss:4.0771 train_time:735107ms step_avg:170.09ms\nstep:4333/10000 train_loss:3.9028 train_time:735275ms step_avg:170.08ms\nstep:4334/10000 train_loss:3.9768 train_time:735445ms step_avg:170.08ms\nstep:4335/10000 train_loss:3.9367 train_time:735614ms step_avg:170.08ms\nstep:4336/10000 train_loss:4.1667 train_time:735784ms step_avg:170.08ms\nstep:4337/10000 train_loss:3.8587 train_time:735953ms step_avg:170.08ms\nstep:4338/10000 train_loss:3.7761 train_time:736122ms step_avg:170.08ms\nstep:4339/10000 train_loss:3.9240 train_time:736291ms step_avg:170.08ms\nstep:4340/10000 train_loss:4.0511 train_time:736459ms step_avg:170.08ms\nstep:4341/10000 train_loss:3.9579 train_time:736626ms step_avg:170.08ms\nstep:4342/10000 train_loss:3.9233 train_time:736799ms step_avg:170.08ms\nstep:4343/10000 train_loss:3.8962 train_time:736971ms step_avg:170.08ms\nstep:4344/10000 train_loss:4.0563 train_time:737141ms step_avg:170.08ms\nstep:4345/10000 train_loss:4.0516 train_time:737311ms step_avg:170.08ms\nstep:4346/10000 train_loss:4.0537 train_time:737482ms step_avg:170.08ms\nstep:4347/10000 train_loss:3.9434 train_time:737650ms step_avg:170.08ms\nstep:4348/10000 train_loss:3.9903 train_time:737820ms step_avg:170.08ms\nstep:4349/10000 train_loss:4.0243 train_time:737991ms step_avg:170.08ms\nstep:4350/10000 train_loss:3.8968 train_time:738162ms step_avg:170.08ms\nstep:4351/10000 train_loss:3.9936 train_time:738330ms step_avg:170.08ms\nstep:4352/10000 train_loss:4.1925 train_time:738501ms step_avg:170.08ms\nstep:4353/10000 train_loss:4.0597 train_time:738669ms step_avg:170.08ms\nstep:4354/10000 train_loss:3.8617 train_time:738840ms step_avg:170.08ms\nstep:4355/10000 train_loss:4.0353 train_time:739010ms step_avg:170.08ms\nstep:4356/10000 train_loss:4.1081 train_time:739182ms step_avg:170.08ms\nstep:4357/10000 train_loss:4.0402 train_time:739351ms step_avg:170.08ms\nstep:4358/10000 train_loss:3.9967 train_time:739522ms step_avg:170.08ms\nstep:4359/10000 train_loss:3.8426 train_time:739691ms step_avg:170.08ms\nstep:4360/10000 train_loss:3.8912 train_time:739863ms step_avg:170.08ms\nstep:4361/10000 train_loss:3.9266 train_time:740031ms step_avg:170.08ms\nstep:4362/10000 train_loss:4.0398 train_time:740201ms step_avg:170.08ms\nstep:4363/10000 train_loss:3.9266 train_time:740370ms step_avg:170.08ms\nstep:4364/10000 train_loss:3.9393 train_time:740541ms step_avg:170.08ms\nstep:4365/10000 train_loss:3.9603 train_time:740709ms step_avg:170.08ms\nstep:4366/10000 train_loss:4.0512 train_time:740880ms step_avg:170.08ms\nstep:4367/10000 train_loss:3.9443 train_time:741052ms step_avg:170.08ms\nstep:4368/10000 train_loss:4.0507 train_time:741223ms step_avg:170.08ms\nstep:4369/10000 train_loss:4.0055 train_time:741393ms step_avg:170.08ms\nstep:4370/10000 train_loss:4.0232 train_time:741564ms step_avg:170.08ms\nstep:4371/10000 train_loss:3.9239 train_time:741734ms step_avg:170.08ms\nstep:4372/10000 train_loss:3.8484 train_time:741907ms step_avg:170.08ms\nstep:4373/10000 train_loss:3.9496 train_time:742075ms step_avg:170.08ms\nstep:4374/10000 train_loss:3.9131 train_time:742244ms step_avg:170.08ms\nstep:4375/10000 train_loss:3.9847 train_time:742413ms step_avg:170.08ms\nstep:4376/10000 train_loss:4.0554 train_time:742586ms step_avg:170.08ms\nstep:4377/10000 train_loss:3.9298 train_time:742754ms step_avg:170.08ms\nstep:4378/10000 train_loss:3.9067 train_time:742924ms step_avg:170.08ms\nstep:4379/10000 train_loss:3.9636 train_time:743094ms step_avg:170.08ms\nstep:4380/10000 train_loss:4.1669 train_time:743268ms step_avg:170.08ms\nstep:4381/10000 train_loss:4.0105 train_time:743435ms step_avg:170.08ms\nstep:4382/10000 train_loss:4.1506 train_time:743608ms step_avg:170.08ms\nstep:4383/10000 train_loss:3.9679 train_time:743780ms step_avg:170.08ms\nstep:4384/10000 train_loss:4.0158 train_time:743950ms step_avg:170.08ms\nstep:4385/10000 train_loss:4.1463 train_time:744121ms step_avg:170.08ms\nstep:4386/10000 train_loss:3.9421 train_time:744290ms step_avg:170.08ms\nstep:4387/10000 train_loss:3.9194 train_time:744459ms step_avg:170.08ms\nstep:4388/10000 train_loss:3.9768 train_time:744631ms step_avg:170.08ms\nstep:4389/10000 train_loss:4.0553 train_time:744801ms step_avg:170.08ms\nstep:4390/10000 train_loss:3.9349 train_time:744969ms step_avg:170.08ms\nstep:4391/10000 train_loss:4.0229 train_time:745140ms step_avg:170.08ms\nstep:4392/10000 train_loss:3.9983 train_time:745310ms step_avg:170.08ms\nstep:4393/10000 train_loss:4.0658 train_time:745480ms step_avg:170.08ms\nstep:4394/10000 train_loss:3.9614 train_time:745648ms step_avg:170.08ms\nstep:4395/10000 train_loss:3.9091 train_time:745816ms step_avg:170.08ms\nstep:4396/10000 train_loss:4.0456 train_time:745988ms step_avg:170.08ms\nstep:4397/10000 train_loss:4.0050 train_time:746158ms step_avg:170.08ms\nstep:4398/10000 train_loss:4.0154 train_time:746325ms step_avg:170.08ms\nstep:4399/10000 train_loss:4.0307 train_time:746495ms step_avg:170.08ms\nstep:4400/10000 train_loss:4.0966 train_time:746667ms step_avg:170.08ms\nstep:4401/10000 train_loss:3.8986 train_time:746837ms step_avg:170.08ms\nstep:4402/10000 train_loss:4.0920 train_time:747005ms step_avg:170.08ms\nstep:4403/10000 train_loss:3.9536 train_time:747182ms step_avg:170.08ms\nstep:4404/10000 train_loss:4.0531 train_time:747347ms step_avg:170.08ms\nstep:4405/10000 train_loss:3.9644 train_time:747516ms step_avg:170.08ms\nstep:4406/10000 train_loss:3.9484 train_time:747685ms step_avg:170.08ms\nstep:4407/10000 train_loss:3.8926 train_time:747854ms step_avg:170.08ms\nstep:4408/10000 train_loss:4.0113 train_time:748030ms step_avg:170.08ms\nstep:4409/10000 train_loss:4.0244 train_time:748196ms step_avg:170.08ms\nstep:4410/10000 train_loss:3.9979 train_time:748366ms step_avg:170.08ms\nstep:4411/10000 train_loss:3.9260 train_time:748536ms step_avg:170.08ms\nstep:4412/10000 train_loss:3.8564 train_time:748708ms step_avg:170.08ms\nstep:4413/10000 train_loss:4.0273 train_time:748879ms step_avg:170.08ms\nstep:4414/10000 train_loss:3.9405 train_time:749050ms step_avg:170.08ms\nstep:4415/10000 train_loss:4.1363 train_time:749220ms step_avg:170.08ms\nstep:4416/10000 train_loss:3.9569 train_time:749388ms step_avg:170.08ms\nstep:4417/10000 train_loss:4.0689 train_time:749558ms step_avg:170.08ms\nstep:4418/10000 train_loss:3.8868 train_time:749731ms step_avg:170.08ms\nstep:4419/10000 train_loss:4.2731 train_time:749901ms step_avg:170.08ms\nstep:4420/10000 train_loss:3.9927 train_time:750072ms step_avg:170.08ms\nstep:4421/10000 train_loss:4.0532 train_time:750244ms step_avg:170.08ms\nstep:4422/10000 train_loss:4.0200 train_time:750414ms step_avg:170.08ms\nstep:4423/10000 train_loss:4.0071 train_time:750586ms step_avg:170.09ms\nstep:4424/10000 train_loss:3.8855 train_time:750755ms step_avg:170.09ms\nstep:4425/10000 train_loss:4.2299 train_time:750926ms step_avg:170.09ms\nstep:4426/10000 train_loss:4.1140 train_time:751097ms step_avg:170.09ms\nstep:4427/10000 train_loss:4.2558 train_time:751267ms step_avg:170.09ms\nstep:4428/10000 train_loss:4.6151 train_time:751436ms step_avg:170.09ms\nstep:4429/10000 train_loss:4.0116 train_time:751610ms step_avg:170.09ms\nstep:4430/10000 train_loss:3.9426 train_time:751780ms step_avg:170.09ms\nstep:4431/10000 train_loss:4.0659 train_time:751948ms step_avg:170.09ms\nstep:4432/10000 train_loss:3.7898 train_time:752120ms step_avg:170.09ms\nstep:4433/10000 train_loss:4.0538 train_time:752291ms step_avg:170.09ms\nstep:4434/10000 train_loss:4.1817 train_time:752462ms step_avg:170.09ms\nstep:4435/10000 train_loss:4.0743 train_time:752631ms step_avg:170.09ms\nstep:4436/10000 train_loss:4.1118 train_time:752800ms step_avg:170.09ms\nstep:4437/10000 train_loss:4.0688 train_time:752969ms step_avg:170.09ms\nstep:4438/10000 train_loss:3.8892 train_time:753139ms step_avg:170.09ms\nstep:4439/10000 train_loss:4.1977 train_time:753306ms step_avg:170.08ms\nstep:4440/10000 train_loss:3.9238 train_time:753477ms step_avg:170.09ms\nstep:4441/10000 train_loss:3.9392 train_time:753646ms step_avg:170.08ms\nstep:4442/10000 train_loss:4.0209 train_time:753814ms step_avg:170.08ms\nstep:4443/10000 train_loss:4.1160 train_time:753984ms step_avg:170.08ms\nstep:4444/10000 train_loss:4.0376 train_time:754154ms step_avg:170.08ms\nstep:4445/10000 train_loss:3.9386 train_time:754328ms step_avg:170.09ms\nstep:4446/10000 train_loss:4.0423 train_time:754493ms step_avg:170.08ms\nstep:4447/10000 train_loss:3.8822 train_time:754664ms step_avg:170.08ms\nstep:4448/10000 train_loss:4.0016 train_time:754832ms step_avg:170.08ms\nstep:4449/10000 train_loss:3.9487 train_time:755004ms step_avg:170.08ms\nstep:4450/10000 train_loss:4.0108 train_time:755173ms step_avg:170.08ms\nstep:4451/10000 train_loss:3.9435 train_time:755345ms step_avg:170.08ms\nstep:4452/10000 train_loss:4.0229 train_time:755516ms step_avg:170.08ms\nstep:4453/10000 train_loss:3.8144 train_time:755687ms step_avg:170.08ms\nstep:4454/10000 train_loss:3.9876 train_time:755857ms step_avg:170.08ms\nstep:4455/10000 train_loss:3.8584 train_time:756028ms step_avg:170.09ms\nstep:4456/10000 train_loss:4.1177 train_time:756199ms step_avg:170.09ms\nstep:4457/10000 train_loss:4.0771 train_time:756369ms step_avg:170.09ms\nstep:4458/10000 train_loss:3.8885 train_time:756543ms step_avg:170.09ms\nstep:4459/10000 train_loss:4.0034 train_time:756714ms step_avg:170.09ms\nstep:4460/10000 train_loss:3.9835 train_time:756884ms step_avg:170.09ms\nstep:4461/10000 train_loss:3.9511 train_time:757054ms step_avg:170.09ms\nstep:4462/10000 train_loss:4.0841 train_time:757225ms step_avg:170.09ms\nstep:4463/10000 train_loss:4.0983 train_time:757393ms step_avg:170.09ms\nstep:4464/10000 train_loss:3.9122 train_time:757565ms step_avg:170.09ms\nstep:4465/10000 train_loss:4.0631 train_time:757734ms step_avg:170.09ms\nstep:4466/10000 train_loss:4.0717 train_time:757906ms step_avg:170.09ms\nstep:4467/10000 train_loss:4.2154 train_time:758073ms step_avg:170.09ms\nstep:4468/10000 train_loss:3.8454 train_time:758244ms step_avg:170.09ms\nstep:4469/10000 train_loss:3.9970 train_time:758417ms step_avg:170.09ms\nstep:4470/10000 train_loss:3.9005 train_time:758580ms step_avg:170.09ms\nstep:4471/10000 train_loss:3.9144 train_time:758750ms step_avg:170.09ms\nstep:4472/10000 train_loss:3.9923 train_time:758927ms step_avg:170.09ms\nstep:4473/10000 train_loss:4.0849 train_time:759091ms step_avg:170.09ms\nstep:4474/10000 train_loss:4.1561 train_time:759260ms step_avg:170.09ms\nstep:4475/10000 train_loss:3.8625 train_time:759430ms step_avg:170.09ms\nstep:4476/10000 train_loss:4.1396 train_time:759601ms step_avg:170.09ms\nstep:4477/10000 train_loss:3.9617 train_time:759771ms step_avg:170.09ms\nstep:4478/10000 train_loss:3.9855 train_time:759941ms step_avg:170.09ms\nstep:4479/10000 train_loss:4.0951 train_time:760111ms step_avg:170.09ms\nstep:4480/10000 train_loss:4.1676 train_time:760282ms step_avg:170.09ms\nstep:4481/10000 train_loss:4.0020 train_time:760451ms step_avg:170.09ms\nstep:4482/10000 train_loss:3.9822 train_time:760623ms step_avg:170.09ms\nstep:4483/10000 train_loss:3.9739 train_time:760793ms step_avg:170.09ms\nstep:4484/10000 train_loss:4.0168 train_time:760962ms step_avg:170.09ms\nstep:4485/10000 train_loss:4.0737 train_time:761130ms step_avg:170.08ms\nstep:4486/10000 train_loss:4.0516 train_time:761298ms step_avg:170.08ms\nstep:4487/10000 train_loss:3.9499 train_time:761466ms step_avg:170.08ms\nstep:4488/10000 train_loss:4.0450 train_time:761636ms step_avg:170.08ms\nstep:4489/10000 train_loss:3.9141 train_time:761807ms step_avg:170.08ms\nstep:4490/10000 train_loss:4.2340 train_time:761977ms step_avg:170.08ms\nstep:4491/10000 train_loss:4.0787 train_time:762149ms step_avg:170.08ms\nstep:4492/10000 train_loss:3.9490 train_time:762320ms step_avg:170.08ms\nstep:4493/10000 train_loss:4.0616 train_time:762490ms step_avg:170.08ms\nstep:4494/10000 train_loss:3.9499 train_time:762658ms step_avg:170.08ms\nstep:4495/10000 train_loss:3.9438 train_time:762829ms step_avg:170.08ms\nstep:4496/10000 train_loss:3.9974 train_time:762999ms step_avg:170.08ms\nstep:4497/10000 train_loss:4.0126 train_time:763169ms step_avg:170.08ms\nstep:4498/10000 train_loss:3.9526 train_time:763342ms step_avg:170.09ms\nstep:4499/10000 train_loss:3.9811 train_time:763512ms step_avg:170.09ms\nstep:4500/10000 train_loss:4.1314 train_time:763683ms step_avg:170.09ms\nstep:4501/10000 train_loss:3.9135 train_time:763853ms step_avg:170.09ms\nstep:4502/10000 train_loss:3.9579 train_time:764023ms step_avg:170.09ms\nstep:4503/10000 train_loss:4.0295 train_time:764190ms step_avg:170.08ms\nstep:4504/10000 train_loss:3.9259 train_time:764360ms step_avg:170.08ms\nstep:4505/10000 train_loss:4.0684 train_time:764531ms step_avg:170.08ms\nstep:4506/10000 train_loss:4.0136 train_time:764701ms step_avg:170.08ms\nstep:4507/10000 train_loss:3.9485 train_time:764872ms step_avg:170.08ms\nstep:4508/10000 train_loss:3.8126 train_time:765044ms step_avg:170.09ms\nstep:4509/10000 train_loss:4.0419 train_time:765211ms step_avg:170.08ms\nstep:4510/10000 train_loss:4.1042 train_time:765381ms step_avg:170.08ms\nstep:4511/10000 train_loss:4.1878 train_time:765551ms step_avg:170.08ms\nstep:4512/10000 train_loss:4.0423 train_time:765721ms step_avg:170.08ms\nstep:4513/10000 train_loss:3.9041 train_time:765892ms step_avg:170.08ms\nstep:4514/10000 train_loss:3.8817 train_time:766070ms step_avg:170.09ms\nstep:4515/10000 train_loss:3.9694 train_time:766239ms step_avg:170.09ms\nstep:4516/10000 train_loss:4.2537 train_time:766411ms step_avg:170.09ms\nstep:4517/10000 train_loss:3.9739 train_time:766580ms step_avg:170.09ms\nstep:4518/10000 train_loss:3.9318 train_time:766741ms step_avg:170.08ms\nstep:4519/10000 train_loss:4.1523 train_time:766911ms step_avg:170.08ms\nstep:4520/10000 train_loss:3.7900 train_time:767082ms step_avg:170.08ms\nstep:4521/10000 train_loss:3.8699 train_time:767252ms step_avg:170.08ms\nstep:4522/10000 train_loss:4.0384 train_time:767425ms step_avg:170.09ms\nstep:4523/10000 train_loss:3.9563 train_time:767592ms step_avg:170.08ms\nstep:4524/10000 train_loss:4.1038 train_time:767761ms step_avg:170.08ms\nstep:4525/10000 train_loss:4.0386 train_time:767932ms step_avg:170.08ms\nstep:4526/10000 train_loss:3.9030 train_time:768102ms step_avg:170.08ms\nstep:4527/10000 train_loss:4.0071 train_time:768271ms step_avg:170.08ms\nstep:4528/10000 train_loss:3.9535 train_time:768442ms step_avg:170.08ms\nstep:4529/10000 train_loss:3.9602 train_time:768609ms step_avg:170.08ms\nstep:4530/10000 train_loss:4.0524 train_time:768778ms step_avg:170.08ms\nstep:4531/10000 train_loss:3.8362 train_time:768947ms step_avg:170.08ms\nstep:4532/10000 train_loss:3.9539 train_time:769115ms step_avg:170.08ms\nstep:4533/10000 train_loss:4.1296 train_time:769287ms step_avg:170.08ms\nstep:4534/10000 train_loss:3.9752 train_time:769456ms step_avg:170.08ms\nstep:4535/10000 train_loss:3.8470 train_time:769630ms step_avg:170.08ms\nstep:4536/10000 train_loss:3.9300 train_time:769799ms step_avg:170.08ms\nstep:4537/10000 train_loss:4.0573 train_time:769968ms step_avg:170.08ms\nstep:4538/10000 train_loss:3.9923 train_time:770137ms step_avg:170.08ms\nstep:4539/10000 train_loss:3.9750 train_time:770309ms step_avg:170.08ms\nstep:4540/10000 train_loss:4.1063 train_time:770480ms step_avg:170.08ms\nstep:4541/10000 train_loss:3.9563 train_time:770650ms step_avg:170.08ms\nstep:4542/10000 train_loss:3.9278 train_time:770823ms step_avg:170.08ms\nstep:4543/10000 train_loss:3.8714 train_time:770993ms step_avg:170.08ms\nstep:4544/10000 train_loss:4.0246 train_time:771163ms step_avg:170.08ms\nstep:4545/10000 train_loss:4.0842 train_time:771331ms step_avg:170.08ms\nstep:4546/10000 train_loss:4.1937 train_time:771501ms step_avg:170.08ms\nstep:4547/10000 train_loss:3.9292 train_time:771671ms step_avg:170.08ms\nstep:4548/10000 train_loss:4.0585 train_time:771844ms step_avg:170.08ms\nstep:4549/10000 train_loss:4.0829 train_time:772013ms step_avg:170.08ms\nstep:4550/10000 train_loss:4.0801 train_time:772184ms step_avg:170.08ms\nstep:4551/10000 train_loss:4.0320 train_time:772357ms step_avg:170.09ms\nstep:4552/10000 train_loss:4.0808 train_time:772527ms step_avg:170.09ms\nstep:4553/10000 train_loss:3.9059 train_time:772699ms step_avg:170.09ms\nstep:4554/10000 train_loss:3.9080 train_time:772872ms step_avg:170.09ms\nstep:4555/10000 train_loss:3.8843 train_time:773043ms step_avg:170.09ms\nstep:4556/10000 train_loss:4.0126 train_time:773213ms step_avg:170.09ms\nstep:4557/10000 train_loss:4.0346 train_time:773384ms step_avg:170.09ms\nstep:4558/10000 train_loss:4.0432 train_time:773551ms step_avg:170.09ms\nstep:4559/10000 train_loss:3.9806 train_time:773720ms step_avg:170.09ms\nstep:4560/10000 train_loss:3.9188 train_time:773888ms step_avg:170.09ms\nstep:4561/10000 train_loss:3.8673 train_time:774059ms step_avg:170.09ms\nstep:4562/10000 train_loss:3.9525 train_time:774227ms step_avg:170.09ms\nstep:4563/10000 train_loss:3.9522 train_time:774400ms step_avg:170.09ms\nstep:4564/10000 train_loss:4.1973 train_time:774567ms step_avg:170.09ms\nstep:4565/10000 train_loss:3.9007 train_time:774740ms step_avg:170.09ms\nstep:4566/10000 train_loss:3.9805 train_time:774910ms step_avg:170.09ms\nstep:4567/10000 train_loss:4.0320 train_time:775080ms step_avg:170.09ms\nstep:4568/10000 train_loss:4.0163 train_time:775249ms step_avg:170.09ms\nstep:4569/10000 train_loss:3.8663 train_time:775419ms step_avg:170.09ms\nstep:4570/10000 train_loss:4.0220 train_time:775590ms step_avg:170.09ms\nstep:4571/10000 train_loss:3.8998 train_time:775763ms step_avg:170.09ms\nstep:4572/10000 train_loss:4.1243 train_time:776150ms step_avg:170.13ms\nstep:4573/10000 train_loss:3.9433 train_time:776314ms step_avg:170.13ms\nstep:4574/10000 train_loss:4.0746 train_time:776482ms step_avg:170.13ms\nstep:4575/10000 train_loss:3.9404 train_time:776650ms step_avg:170.13ms\nstep:4576/10000 train_loss:4.1272 train_time:776820ms step_avg:170.13ms\nstep:4577/10000 train_loss:4.0952 train_time:776988ms step_avg:170.13ms\nstep:4578/10000 train_loss:4.0213 train_time:777168ms step_avg:170.13ms\nstep:4579/10000 train_loss:3.9783 train_time:777338ms step_avg:170.13ms\nstep:4580/10000 train_loss:3.8417 train_time:777507ms step_avg:170.13ms\nstep:4581/10000 train_loss:4.0928 train_time:777677ms step_avg:170.13ms\nstep:4582/10000 train_loss:3.9420 train_time:777843ms step_avg:170.13ms\nstep:4583/10000 train_loss:3.9795 train_time:778012ms step_avg:170.13ms\nstep:4584/10000 train_loss:4.0898 train_time:778187ms step_avg:170.13ms\nstep:4585/10000 train_loss:3.9382 train_time:778358ms step_avg:170.13ms\nstep:4586/10000 train_loss:4.0270 train_time:778528ms step_avg:170.13ms\nstep:4587/10000 train_loss:3.8940 train_time:778696ms step_avg:170.13ms\nstep:4588/10000 train_loss:3.8998 train_time:778867ms step_avg:170.13ms\nstep:4589/10000 train_loss:4.0355 train_time:779036ms step_avg:170.13ms\nstep:4590/10000 train_loss:4.1546 train_time:779204ms step_avg:170.13ms\nstep:4591/10000 train_loss:3.9386 train_time:779375ms step_avg:170.13ms\nstep:4592/10000 train_loss:3.9436 train_time:779546ms step_avg:170.13ms\nstep:4593/10000 train_loss:4.0128 train_time:779715ms step_avg:170.13ms\nstep:4594/10000 train_loss:4.0813 train_time:779885ms step_avg:170.13ms\nstep:4595/10000 train_loss:4.2991 train_time:780054ms step_avg:170.13ms\nstep:4596/10000 train_loss:4.0238 train_time:780224ms step_avg:170.13ms\nstep:4597/10000 train_loss:3.9732 train_time:780392ms step_avg:170.13ms\nstep:4598/10000 train_loss:4.0150 train_time:780561ms step_avg:170.13ms\nstep:4599/10000 train_loss:4.1129 train_time:780731ms step_avg:170.13ms\nstep:4600/10000 train_loss:4.0789 train_time:780903ms step_avg:170.13ms\nstep:4601/10000 train_loss:3.9384 train_time:781073ms step_avg:170.13ms\nstep:4602/10000 train_loss:4.0576 train_time:781244ms step_avg:170.13ms\nstep:4603/10000 train_loss:4.1049 train_time:781413ms step_avg:170.13ms\nstep:4604/10000 train_loss:4.0394 train_time:781585ms step_avg:170.13ms\nstep:4605/10000 train_loss:3.9836 train_time:781756ms step_avg:170.13ms\nstep:4606/10000 train_loss:3.9158 train_time:781927ms step_avg:170.13ms\nstep:4607/10000 train_loss:3.9359 train_time:782097ms step_avg:170.13ms\nstep:4608/10000 train_loss:3.9519 train_time:782267ms step_avg:170.13ms\nstep:4609/10000 train_loss:3.8964 train_time:782437ms step_avg:170.13ms\nstep:4610/10000 train_loss:3.9035 train_time:782606ms step_avg:170.13ms\nstep:4611/10000 train_loss:4.0591 train_time:782775ms step_avg:170.13ms\nstep:4612/10000 train_loss:3.8939 train_time:782954ms step_avg:170.13ms\nstep:4613/10000 train_loss:4.0067 train_time:783114ms step_avg:170.13ms\nstep:4614/10000 train_loss:3.9016 train_time:783283ms step_avg:170.13ms\nstep:4615/10000 train_loss:3.9541 train_time:783453ms step_avg:170.13ms\nstep:4616/10000 train_loss:4.1622 train_time:783623ms step_avg:170.13ms\nstep:4617/10000 train_loss:3.7892 train_time:783794ms step_avg:170.13ms\nstep:4618/10000 train_loss:4.0883 train_time:783966ms step_avg:170.13ms\nstep:4619/10000 train_loss:4.1155 train_time:784135ms step_avg:170.13ms\nstep:4620/10000 train_loss:4.1561 train_time:784305ms step_avg:170.13ms\nstep:4621/10000 train_loss:4.2105 train_time:784473ms step_avg:170.13ms\nstep:4622/10000 train_loss:4.0233 train_time:784644ms step_avg:170.13ms\nstep:4623/10000 train_loss:3.8587 train_time:784812ms step_avg:170.13ms\nstep:4624/10000 train_loss:3.8423 train_time:784985ms step_avg:170.13ms\nstep:4625/10000 train_loss:4.0591 train_time:785156ms step_avg:170.13ms\nstep:4626/10000 train_loss:4.0562 train_time:785326ms step_avg:170.13ms\nstep:4627/10000 train_loss:3.8797 train_time:785494ms step_avg:170.13ms\nstep:4628/10000 train_loss:3.8574 train_time:785666ms step_avg:170.13ms\nstep:4629/10000 train_loss:4.0755 train_time:785836ms step_avg:170.13ms\nstep:4630/10000 train_loss:3.9457 train_time:786008ms step_avg:170.13ms\nstep:4631/10000 train_loss:3.9780 train_time:786176ms step_avg:170.13ms\nstep:4632/10000 train_loss:3.9986 train_time:786348ms step_avg:170.13ms\nstep:4633/10000 train_loss:3.9704 train_time:786535ms step_avg:170.14ms\nstep:4634/10000 train_loss:4.0838 train_time:786693ms step_avg:170.13ms\nstep:4635/10000 train_loss:4.0474 train_time:786861ms step_avg:170.13ms\nstep:4636/10000 train_loss:4.0717 train_time:787031ms step_avg:170.13ms\nstep:4637/10000 train_loss:4.0835 train_time:787203ms step_avg:170.13ms\nstep:4638/10000 train_loss:4.0551 train_time:787371ms step_avg:170.13ms\nstep:4639/10000 train_loss:3.8913 train_time:787543ms step_avg:170.13ms\nstep:4640/10000 train_loss:3.7928 train_time:787712ms step_avg:170.13ms\nstep:4641/10000 train_loss:4.0637 train_time:787883ms step_avg:170.13ms\nstep:4642/10000 train_loss:3.9107 train_time:788054ms step_avg:170.13ms\nstep:4643/10000 train_loss:4.3296 train_time:788229ms step_avg:170.13ms\nstep:4644/10000 train_loss:3.8638 train_time:788398ms step_avg:170.13ms\nstep:4645/10000 train_loss:4.0386 train_time:788568ms step_avg:170.13ms\nstep:4646/10000 train_loss:3.8765 train_time:788739ms step_avg:170.13ms\nstep:4647/10000 train_loss:4.0617 train_time:788909ms step_avg:170.13ms\nstep:4648/10000 train_loss:3.8619 train_time:789078ms step_avg:170.13ms\nstep:4649/10000 train_loss:3.9014 train_time:789249ms step_avg:170.13ms\nstep:4650/10000 train_loss:3.8803 train_time:789422ms step_avg:170.13ms\nstep:4651/10000 train_loss:3.9203 train_time:789590ms step_avg:170.13ms\nstep:4652/10000 train_loss:4.0438 train_time:789762ms step_avg:170.13ms\nstep:4653/10000 train_loss:4.0044 train_time:789931ms step_avg:170.13ms\nstep:4654/10000 train_loss:3.7024 train_time:790100ms step_avg:170.13ms\nstep:4655/10000 train_loss:3.8985 train_time:790276ms step_avg:170.13ms\nstep:4656/10000 train_loss:3.8377 train_time:790444ms step_avg:170.13ms\nstep:4657/10000 train_loss:3.8595 train_time:790611ms step_avg:170.13ms\nstep:4658/10000 train_loss:4.0189 train_time:790782ms step_avg:170.13ms\nstep:4659/10000 train_loss:3.9981 train_time:790951ms step_avg:170.13ms\nstep:4660/10000 train_loss:3.9839 train_time:791119ms step_avg:170.13ms\nstep:4661/10000 train_loss:4.0462 train_time:791286ms step_avg:170.13ms\nstep:4662/10000 train_loss:3.9800 train_time:791456ms step_avg:170.13ms\nstep:4663/10000 train_loss:3.8329 train_time:791627ms step_avg:170.13ms\nstep:4664/10000 train_loss:4.0362 train_time:791798ms step_avg:170.13ms\nstep:4665/10000 train_loss:4.0115 train_time:791966ms step_avg:170.13ms\nstep:4666/10000 train_loss:3.8447 train_time:792135ms step_avg:170.13ms\nstep:4667/10000 train_loss:4.1399 train_time:792304ms step_avg:170.13ms\nstep:4668/10000 train_loss:3.9899 train_time:792472ms step_avg:170.13ms\nstep:4669/10000 train_loss:3.9086 train_time:792645ms step_avg:170.13ms\nstep:4670/10000 train_loss:3.8588 train_time:792814ms step_avg:170.13ms\nstep:4671/10000 train_loss:4.0285 train_time:792985ms step_avg:170.13ms\nstep:4672/10000 train_loss:3.9432 train_time:793153ms step_avg:170.13ms\nstep:4673/10000 train_loss:3.9962 train_time:793323ms step_avg:170.13ms\nstep:4674/10000 train_loss:3.9283 train_time:793491ms step_avg:170.13ms\nstep:4675/10000 train_loss:3.9839 train_time:793665ms step_avg:170.13ms\nstep:4676/10000 train_loss:3.9491 train_time:793836ms step_avg:170.13ms\nstep:4677/10000 train_loss:4.0521 train_time:794008ms step_avg:170.13ms\nstep:4678/10000 train_loss:3.9153 train_time:794180ms step_avg:170.13ms\nstep:4679/10000 train_loss:3.9389 train_time:794351ms step_avg:170.13ms\nstep:4680/10000 train_loss:4.1339 train_time:794519ms step_avg:170.13ms\nstep:4681/10000 train_loss:3.9716 train_time:794689ms step_avg:170.13ms\nstep:4682/10000 train_loss:4.2391 train_time:794859ms step_avg:170.13ms\nstep:4683/10000 train_loss:3.9799 train_time:795029ms step_avg:170.13ms\nstep:4684/10000 train_loss:3.9411 train_time:795199ms step_avg:170.13ms\nstep:4685/10000 train_loss:4.0128 train_time:795369ms step_avg:170.13ms\nstep:4686/10000 train_loss:3.9490 train_time:795544ms step_avg:170.13ms\nstep:4687/10000 train_loss:3.9213 train_time:795705ms step_avg:170.13ms\nstep:4688/10000 train_loss:4.0358 train_time:795874ms step_avg:170.13ms\nstep:4689/10000 train_loss:3.8994 train_time:796043ms step_avg:170.13ms\nstep:4690/10000 train_loss:3.9624 train_time:796212ms step_avg:170.13ms\nstep:4691/10000 train_loss:3.9618 train_time:796382ms step_avg:170.13ms\nstep:4692/10000 train_loss:4.0052 train_time:796552ms step_avg:170.13ms\nstep:4693/10000 train_loss:3.9951 train_time:796723ms step_avg:170.13ms\nstep:4694/10000 train_loss:3.9075 train_time:796892ms step_avg:170.13ms\nstep:4695/10000 train_loss:3.8461 train_time:797061ms step_avg:170.13ms\nstep:4696/10000 train_loss:3.8848 train_time:797230ms step_avg:170.13ms\nstep:4697/10000 train_loss:3.8067 train_time:797400ms step_avg:170.13ms\nstep:4698/10000 train_loss:3.9442 train_time:797568ms step_avg:170.13ms\nstep:4699/10000 train_loss:4.1501 train_time:797737ms step_avg:170.13ms\nstep:4700/10000 train_loss:3.9796 train_time:797909ms step_avg:170.13ms\nstep:4701/10000 train_loss:3.9994 train_time:798087ms step_avg:170.13ms\nstep:4702/10000 train_loss:4.1044 train_time:798248ms step_avg:170.13ms\nstep:4703/10000 train_loss:3.9860 train_time:798419ms step_avg:170.13ms\nstep:4704/10000 train_loss:4.3148 train_time:798589ms step_avg:170.13ms\nstep:4705/10000 train_loss:4.0794 train_time:798758ms step_avg:170.13ms\nstep:4706/10000 train_loss:3.9408 train_time:798928ms step_avg:170.13ms\nstep:4707/10000 train_loss:3.9074 train_time:799098ms step_avg:170.13ms\nstep:4708/10000 train_loss:3.9334 train_time:799269ms step_avg:170.13ms\nstep:4709/10000 train_loss:3.9775 train_time:799437ms step_avg:170.13ms\nstep:4710/10000 train_loss:3.9736 train_time:799607ms step_avg:170.13ms\nstep:4711/10000 train_loss:3.9475 train_time:799776ms step_avg:170.13ms\nstep:4712/10000 train_loss:3.8872 train_time:799948ms step_avg:170.13ms\nstep:4713/10000 train_loss:3.9288 train_time:800117ms step_avg:170.13ms\nstep:4714/10000 train_loss:4.0188 train_time:800288ms step_avg:170.13ms\nstep:4715/10000 train_loss:4.0695 train_time:800459ms step_avg:170.13ms\nstep:4716/10000 train_loss:3.8844 train_time:800629ms step_avg:170.13ms\nstep:4717/10000 train_loss:4.0167 train_time:800799ms step_avg:170.13ms\nstep:4718/10000 train_loss:3.9907 train_time:800970ms step_avg:170.13ms\nstep:4719/10000 train_loss:4.0220 train_time:801141ms step_avg:170.13ms\nstep:4720/10000 train_loss:3.9910 train_time:801310ms step_avg:170.13ms\nstep:4721/10000 train_loss:3.9427 train_time:801482ms step_avg:170.13ms\nstep:4722/10000 train_loss:4.0323 train_time:801652ms step_avg:170.13ms\nstep:4723/10000 train_loss:3.9136 train_time:801823ms step_avg:170.13ms\nstep:4724/10000 train_loss:4.0314 train_time:801991ms step_avg:170.13ms\nstep:4725/10000 train_loss:3.8939 train_time:802163ms step_avg:170.13ms\nstep:4726/10000 train_loss:3.9682 train_time:802331ms step_avg:170.13ms\nstep:4727/10000 train_loss:4.0880 train_time:802501ms step_avg:170.13ms\nstep:4728/10000 train_loss:3.9578 train_time:802670ms step_avg:170.13ms\nstep:4729/10000 train_loss:3.9232 train_time:802840ms step_avg:170.13ms\nstep:4730/10000 train_loss:3.9883 train_time:803009ms step_avg:170.13ms\nstep:4731/10000 train_loss:3.9882 train_time:803178ms step_avg:170.13ms\nstep:4732/10000 train_loss:3.8250 train_time:803348ms step_avg:170.13ms\nstep:4733/10000 train_loss:3.9237 train_time:803519ms step_avg:170.13ms\nstep:4734/10000 train_loss:3.9069 train_time:803688ms step_avg:170.13ms\nstep:4735/10000 train_loss:4.0006 train_time:803859ms step_avg:170.13ms\nstep:4736/10000 train_loss:3.9569 train_time:804032ms step_avg:170.13ms\nstep:4737/10000 train_loss:3.8849 train_time:804201ms step_avg:170.13ms\nstep:4738/10000 train_loss:3.9677 train_time:804373ms step_avg:170.13ms\nstep:4739/10000 train_loss:4.0578 train_time:804543ms step_avg:170.13ms\nstep:4740/10000 train_loss:4.0916 train_time:804713ms step_avg:170.13ms\nstep:4741/10000 train_loss:4.0133 train_time:804885ms step_avg:170.13ms\nstep:4742/10000 train_loss:3.8943 train_time:805053ms step_avg:170.13ms\nstep:4743/10000 train_loss:3.9461 train_time:805222ms step_avg:170.13ms\nstep:4744/10000 train_loss:4.2156 train_time:805390ms step_avg:170.13ms\nstep:4745/10000 train_loss:3.8872 train_time:805559ms step_avg:170.13ms\nstep:4746/10000 train_loss:4.0511 train_time:805730ms step_avg:170.13ms\nstep:4747/10000 train_loss:3.9030 train_time:805900ms step_avg:170.13ms\nstep:4748/10000 train_loss:4.0696 train_time:806072ms step_avg:170.13ms\nstep:4749/10000 train_loss:3.8283 train_time:806245ms step_avg:170.13ms\nstep:4750/10000 train_loss:4.1130 train_time:806414ms step_avg:170.13ms\nstep:4751/10000 train_loss:4.0893 train_time:806584ms step_avg:170.13ms\nstep:4752/10000 train_loss:3.9394 train_time:806756ms step_avg:170.13ms\nstep:4753/10000 train_loss:3.8965 train_time:806927ms step_avg:170.13ms\nstep:4754/10000 train_loss:3.8898 train_time:807097ms step_avg:170.13ms\nstep:4755/10000 train_loss:3.9812 train_time:807265ms step_avg:170.13ms\nstep:4756/10000 train_loss:3.8182 train_time:807434ms step_avg:170.13ms\nstep:4757/10000 train_loss:3.9518 train_time:807603ms step_avg:170.13ms\nstep:4758/10000 train_loss:3.9613 train_time:807772ms step_avg:170.13ms\nstep:4759/10000 train_loss:4.0074 train_time:807943ms step_avg:170.13ms\nstep:4760/10000 train_loss:3.9127 train_time:808112ms step_avg:170.13ms\nstep:4761/10000 train_loss:3.9290 train_time:808281ms step_avg:170.13ms\nstep:4762/10000 train_loss:3.9974 train_time:808450ms step_avg:170.13ms\nstep:4763/10000 train_loss:4.1934 train_time:808619ms step_avg:170.13ms\nstep:4764/10000 train_loss:3.9598 train_time:808789ms step_avg:170.13ms\nstep:4765/10000 train_loss:3.8369 train_time:808961ms step_avg:170.13ms\nstep:4766/10000 train_loss:3.8371 train_time:809133ms step_avg:170.13ms\nstep:4767/10000 train_loss:4.1526 train_time:809303ms step_avg:170.13ms\nstep:4768/10000 train_loss:4.0023 train_time:809471ms step_avg:170.13ms\nstep:4769/10000 train_loss:3.9976 train_time:809641ms step_avg:170.13ms\nstep:4770/10000 train_loss:4.0150 train_time:809810ms step_avg:170.13ms\nstep:4771/10000 train_loss:3.8420 train_time:809981ms step_avg:170.13ms\nstep:4772/10000 train_loss:3.9762 train_time:810150ms step_avg:170.13ms\nstep:4773/10000 train_loss:4.1226 train_time:810321ms step_avg:170.13ms\nstep:4774/10000 train_loss:4.1365 train_time:810490ms step_avg:170.13ms\nstep:4775/10000 train_loss:4.1421 train_time:810660ms step_avg:170.13ms\nstep:4776/10000 train_loss:4.0686 train_time:810830ms step_avg:170.13ms\nstep:4777/10000 train_loss:3.9531 train_time:811001ms step_avg:170.13ms\nstep:4778/10000 train_loss:3.9572 train_time:811171ms step_avg:170.13ms\nstep:4779/10000 train_loss:3.9228 train_time:811344ms step_avg:170.13ms\nstep:4780/10000 train_loss:3.8879 train_time:811513ms step_avg:170.13ms\nstep:4781/10000 train_loss:4.4609 train_time:811683ms step_avg:170.13ms\nstep:4782/10000 train_loss:3.8434 train_time:811855ms step_avg:170.13ms\nstep:4783/10000 train_loss:3.9131 train_time:812022ms step_avg:170.13ms\nstep:4784/10000 train_loss:3.9222 train_time:812194ms step_avg:170.13ms\nstep:4785/10000 train_loss:3.9342 train_time:812363ms step_avg:170.13ms\nstep:4786/10000 train_loss:4.1508 train_time:812530ms step_avg:170.13ms\nstep:4787/10000 train_loss:3.9010 train_time:812704ms step_avg:170.13ms\nstep:4788/10000 train_loss:3.9064 train_time:812869ms step_avg:170.13ms\nstep:4789/10000 train_loss:3.8041 train_time:813039ms step_avg:170.13ms\nstep:4790/10000 train_loss:3.9259 train_time:813211ms step_avg:170.13ms\nstep:4791/10000 train_loss:3.9491 train_time:813381ms step_avg:170.13ms\nstep:4792/10000 train_loss:3.9639 train_time:813551ms step_avg:170.13ms\nstep:4793/10000 train_loss:3.7680 train_time:813722ms step_avg:170.13ms\nstep:4794/10000 train_loss:3.9277 train_time:813889ms step_avg:170.13ms\nstep:4795/10000 train_loss:4.0630 train_time:814062ms step_avg:170.13ms\nstep:4796/10000 train_loss:3.9622 train_time:814234ms step_avg:170.13ms\nstep:4797/10000 train_loss:3.9730 train_time:814404ms step_avg:170.13ms\nstep:4798/10000 train_loss:3.8400 train_time:814572ms step_avg:170.13ms\nstep:4799/10000 train_loss:3.8584 train_time:814749ms step_avg:170.13ms\nstep:4800/10000 train_loss:3.9058 train_time:814912ms step_avg:170.13ms\nstep:4801/10000 train_loss:3.8085 train_time:815082ms step_avg:170.13ms\nstep:4802/10000 train_loss:3.8162 train_time:815252ms step_avg:170.13ms\nstep:4803/10000 train_loss:3.8815 train_time:815424ms step_avg:170.13ms\nstep:4804/10000 train_loss:4.3142 train_time:815595ms step_avg:170.13ms\nstep:4805/10000 train_loss:3.9559 train_time:815766ms step_avg:170.13ms\nstep:4806/10000 train_loss:3.9005 train_time:815937ms step_avg:170.13ms\nstep:4807/10000 train_loss:4.0070 train_time:816108ms step_avg:170.13ms\nstep:4808/10000 train_loss:3.8777 train_time:816279ms step_avg:170.13ms\nstep:4809/10000 train_loss:3.9072 train_time:816450ms step_avg:170.13ms\nstep:4810/10000 train_loss:3.9617 train_time:816619ms step_avg:170.13ms\nstep:4811/10000 train_loss:3.8736 train_time:816792ms step_avg:170.13ms\nstep:4812/10000 train_loss:3.9561 train_time:816962ms step_avg:170.13ms\nstep:4813/10000 train_loss:3.8542 train_time:817128ms step_avg:170.13ms\nstep:4814/10000 train_loss:4.0686 train_time:817298ms step_avg:170.13ms\nstep:4815/10000 train_loss:3.9810 train_time:817467ms step_avg:170.13ms\nstep:4816/10000 train_loss:3.9386 train_time:817638ms step_avg:170.13ms\nstep:4817/10000 train_loss:3.7711 train_time:817809ms step_avg:170.13ms\nstep:4818/10000 train_loss:4.0365 train_time:817977ms step_avg:170.13ms\nstep:4819/10000 train_loss:3.8715 train_time:818146ms step_avg:170.13ms\nstep:4820/10000 train_loss:4.0455 train_time:818315ms step_avg:170.13ms\nstep:4821/10000 train_loss:3.9177 train_time:818483ms step_avg:170.13ms\nstep:4822/10000 train_loss:3.9872 train_time:818650ms step_avg:170.13ms\nstep:4823/10000 train_loss:3.8828 train_time:818825ms step_avg:170.13ms\nstep:4824/10000 train_loss:4.0321 train_time:818996ms step_avg:170.13ms\nstep:4825/10000 train_loss:4.0486 train_time:819165ms step_avg:170.13ms\nstep:4826/10000 train_loss:3.9314 train_time:819334ms step_avg:170.13ms\nstep:4827/10000 train_loss:3.8507 train_time:819506ms step_avg:170.13ms\nstep:4828/10000 train_loss:3.9880 train_time:819676ms step_avg:170.13ms\nstep:4829/10000 train_loss:4.1280 train_time:819844ms step_avg:170.13ms\nstep:4830/10000 train_loss:3.9635 train_time:820013ms step_avg:170.13ms\nstep:4831/10000 train_loss:4.0443 train_time:820185ms step_avg:170.13ms\nstep:4832/10000 train_loss:3.9082 train_time:820355ms step_avg:170.13ms\nstep:4833/10000 train_loss:4.0903 train_time:820526ms step_avg:170.13ms\nstep:4834/10000 train_loss:4.0121 train_time:820696ms step_avg:170.13ms\nstep:4835/10000 train_loss:3.8744 train_time:820865ms step_avg:170.13ms\nstep:4836/10000 train_loss:3.9449 train_time:821037ms step_avg:170.13ms\nstep:4837/10000 train_loss:3.7994 train_time:821203ms step_avg:170.13ms\nstep:4838/10000 train_loss:4.0749 train_time:821372ms step_avg:170.13ms\nstep:4839/10000 train_loss:4.1342 train_time:821545ms step_avg:170.13ms\nstep:4840/10000 train_loss:3.8890 train_time:821712ms step_avg:170.13ms\nstep:4841/10000 train_loss:3.9060 train_time:821882ms step_avg:170.13ms\nstep:4842/10000 train_loss:3.9484 train_time:822050ms step_avg:170.13ms\nstep:4843/10000 train_loss:3.8965 train_time:822220ms step_avg:170.13ms\nstep:4844/10000 train_loss:3.9754 train_time:822389ms step_avg:170.13ms\nstep:4845/10000 train_loss:3.8912 train_time:822560ms step_avg:170.13ms\nstep:4846/10000 train_loss:3.7977 train_time:822729ms step_avg:170.13ms\nstep:4847/10000 train_loss:3.8799 train_time:822901ms step_avg:170.13ms\nstep:4848/10000 train_loss:4.2127 train_time:823070ms step_avg:170.13ms\nstep:4849/10000 train_loss:3.8735 train_time:823242ms step_avg:170.13ms\nstep:4850/10000 train_loss:3.8582 train_time:823409ms step_avg:170.13ms\nstep:4851/10000 train_loss:3.9974 train_time:823579ms step_avg:170.13ms\nstep:4852/10000 train_loss:3.8775 train_time:823749ms step_avg:170.13ms\nstep:4853/10000 train_loss:3.9748 train_time:823922ms step_avg:170.13ms\nstep:4854/10000 train_loss:3.9560 train_time:824091ms step_avg:170.13ms\nstep:4855/10000 train_loss:3.9436 train_time:824262ms step_avg:170.13ms\nstep:4856/10000 train_loss:4.1391 train_time:824432ms step_avg:170.13ms\nstep:4857/10000 train_loss:3.8582 train_time:824601ms step_avg:170.13ms\nstep:4858/10000 train_loss:3.9753 train_time:824772ms step_avg:170.13ms\nstep:4859/10000 train_loss:3.8421 train_time:824945ms step_avg:170.13ms\nstep:4860/10000 train_loss:3.9872 train_time:825113ms step_avg:170.13ms\nstep:4861/10000 train_loss:3.9153 train_time:825286ms step_avg:170.13ms\nstep:4862/10000 train_loss:4.0086 train_time:825460ms step_avg:170.13ms\nstep:4863/10000 train_loss:4.0291 train_time:825628ms step_avg:170.13ms\nstep:4864/10000 train_loss:4.0046 train_time:825795ms step_avg:170.13ms\nstep:4865/10000 train_loss:3.9548 train_time:825967ms step_avg:170.13ms\nstep:4866/10000 train_loss:3.9516 train_time:826139ms step_avg:170.13ms\nstep:4867/10000 train_loss:3.9908 train_time:826306ms step_avg:170.13ms\nstep:4868/10000 train_loss:3.9810 train_time:826478ms step_avg:170.13ms\nstep:4869/10000 train_loss:3.9115 train_time:826646ms step_avg:170.13ms\nstep:4870/10000 train_loss:3.9638 train_time:826817ms step_avg:170.13ms\nstep:4871/10000 train_loss:3.8929 train_time:826986ms step_avg:170.13ms\nstep:4872/10000 train_loss:3.9788 train_time:827157ms step_avg:170.13ms\nstep:4873/10000 train_loss:4.1259 train_time:827329ms step_avg:170.13ms\nstep:4874/10000 train_loss:3.8972 train_time:827501ms step_avg:170.13ms\nstep:4875/10000 train_loss:3.7800 train_time:827672ms step_avg:170.13ms\nstep:4876/10000 train_loss:4.0200 train_time:827841ms step_avg:170.13ms\nstep:4877/10000 train_loss:3.9285 train_time:828010ms step_avg:170.13ms\nstep:4878/10000 train_loss:3.9348 train_time:828180ms step_avg:170.13ms\nstep:4879/10000 train_loss:3.9970 train_time:828351ms step_avg:170.13ms\nstep:4880/10000 train_loss:3.9416 train_time:828522ms step_avg:170.13ms\nstep:4881/10000 train_loss:3.8491 train_time:828696ms step_avg:170.13ms\nstep:4882/10000 train_loss:3.9944 train_time:828863ms step_avg:170.13ms\nstep:4883/10000 train_loss:3.9769 train_time:829035ms step_avg:170.13ms\nstep:4884/10000 train_loss:3.9920 train_time:829206ms step_avg:170.13ms\nstep:4885/10000 train_loss:3.9222 train_time:829376ms step_avg:170.13ms\nstep:4886/10000 train_loss:3.8270 train_time:829546ms step_avg:170.13ms\nstep:4887/10000 train_loss:3.9982 train_time:829714ms step_avg:170.13ms\nstep:4888/10000 train_loss:3.9823 train_time:829884ms step_avg:170.13ms\nstep:4889/10000 train_loss:4.0422 train_time:830052ms step_avg:170.13ms\nstep:4890/10000 train_loss:4.0410 train_time:830221ms step_avg:170.13ms\nstep:4891/10000 train_loss:3.8827 train_time:830389ms step_avg:170.13ms\nstep:4892/10000 train_loss:4.0600 train_time:830560ms step_avg:170.13ms\nstep:4893/10000 train_loss:3.9743 train_time:830729ms step_avg:170.13ms\nstep:4894/10000 train_loss:4.0793 train_time:830900ms step_avg:170.13ms\nstep:4895/10000 train_loss:3.8815 train_time:831070ms step_avg:170.13ms\nstep:4896/10000 train_loss:4.0039 train_time:831239ms step_avg:170.13ms\nstep:4897/10000 train_loss:3.9555 train_time:831406ms step_avg:170.13ms\nstep:4898/10000 train_loss:4.0279 train_time:831579ms step_avg:170.13ms\nstep:4899/10000 train_loss:4.0559 train_time:831748ms step_avg:170.13ms\nstep:4900/10000 train_loss:3.9558 train_time:831920ms step_avg:170.13ms\nstep:4901/10000 train_loss:4.1436 train_time:832092ms step_avg:170.13ms\nstep:4902/10000 train_loss:3.8377 train_time:832262ms step_avg:170.13ms\nstep:4903/10000 train_loss:3.9736 train_time:832431ms step_avg:170.13ms\nstep:4904/10000 train_loss:3.8902 train_time:832603ms step_avg:170.13ms\nstep:4905/10000 train_loss:4.1826 train_time:832775ms step_avg:170.13ms\nstep:4906/10000 train_loss:3.8664 train_time:832944ms step_avg:170.13ms\nstep:4907/10000 train_loss:4.1249 train_time:833114ms step_avg:170.13ms\nstep:4908/10000 train_loss:4.0070 train_time:833281ms step_avg:170.13ms\nstep:4909/10000 train_loss:3.9970 train_time:833450ms step_avg:170.13ms\nstep:4910/10000 train_loss:3.9360 train_time:833621ms step_avg:170.13ms\nstep:4911/10000 train_loss:3.9692 train_time:833790ms step_avg:170.13ms\nstep:4912/10000 train_loss:3.7978 train_time:833963ms step_avg:170.13ms\nstep:4913/10000 train_loss:3.9341 train_time:834131ms step_avg:170.13ms\nstep:4914/10000 train_loss:3.9082 train_time:834302ms step_avg:170.13ms\nstep:4915/10000 train_loss:4.0006 train_time:834470ms step_avg:170.13ms\nstep:4916/10000 train_loss:4.1550 train_time:834640ms step_avg:170.13ms\nstep:4917/10000 train_loss:4.1537 train_time:834808ms step_avg:170.13ms\nstep:4918/10000 train_loss:3.8355 train_time:834979ms step_avg:170.13ms\nstep:4919/10000 train_loss:3.9263 train_time:835150ms step_avg:170.13ms\nstep:4920/10000 train_loss:3.8982 train_time:835321ms step_avg:170.13ms\nstep:4921/10000 train_loss:3.9250 train_time:835491ms step_avg:170.13ms\nstep:4922/10000 train_loss:4.0185 train_time:835660ms step_avg:170.13ms\nstep:4923/10000 train_loss:4.0261 train_time:835827ms step_avg:170.13ms\nstep:4924/10000 train_loss:3.9039 train_time:835996ms step_avg:170.13ms\nstep:4925/10000 train_loss:3.7903 train_time:836163ms step_avg:170.12ms\nstep:4926/10000 train_loss:3.9291 train_time:836333ms step_avg:170.12ms\nstep:4927/10000 train_loss:3.9480 train_time:836503ms step_avg:170.12ms\nstep:4928/10000 train_loss:4.0763 train_time:836672ms step_avg:170.12ms\nstep:4929/10000 train_loss:3.7306 train_time:836843ms step_avg:170.12ms\nstep:4930/10000 train_loss:4.0018 train_time:837012ms step_avg:170.12ms\nstep:4931/10000 train_loss:3.7877 train_time:837183ms step_avg:170.12ms\nstep:4932/10000 train_loss:3.8012 train_time:837352ms step_avg:170.12ms\nstep:4933/10000 train_loss:3.9680 train_time:837524ms step_avg:170.12ms\nstep:4934/10000 train_loss:4.1561 train_time:837691ms step_avg:170.12ms\nstep:4935/10000 train_loss:3.9904 train_time:837864ms step_avg:170.12ms\nstep:4936/10000 train_loss:3.9219 train_time:838034ms step_avg:170.12ms\nstep:4937/10000 train_loss:3.8645 train_time:838203ms step_avg:170.12ms\nstep:4938/10000 train_loss:4.0540 train_time:838375ms step_avg:170.12ms\nstep:4939/10000 train_loss:4.1134 train_time:838547ms step_avg:170.13ms\nstep:4940/10000 train_loss:3.9296 train_time:838716ms step_avg:170.13ms\nstep:4941/10000 train_loss:3.9170 train_time:838888ms step_avg:170.13ms\nstep:4942/10000 train_loss:3.9791 train_time:839061ms step_avg:170.13ms\nstep:4943/10000 train_loss:3.7126 train_time:839230ms step_avg:170.13ms\nstep:4944/10000 train_loss:3.9891 train_time:839404ms step_avg:170.13ms\nstep:4945/10000 train_loss:3.8506 train_time:839572ms step_avg:170.13ms\nstep:4946/10000 train_loss:4.1404 train_time:839741ms step_avg:170.13ms\nstep:4947/10000 train_loss:3.8502 train_time:839912ms step_avg:170.13ms\nstep:4948/10000 train_loss:3.9052 train_time:840082ms step_avg:170.13ms\nstep:4949/10000 train_loss:3.8189 train_time:840249ms step_avg:170.13ms\nstep:4950/10000 train_loss:3.9264 train_time:840421ms step_avg:170.13ms\nstep:4951/10000 train_loss:3.9535 train_time:840588ms step_avg:170.13ms\nstep:4952/10000 train_loss:3.9195 train_time:840759ms step_avg:170.13ms\nstep:4953/10000 train_loss:3.9571 train_time:840930ms step_avg:170.13ms\nstep:4954/10000 train_loss:3.8761 train_time:841100ms step_avg:170.13ms\nstep:4955/10000 train_loss:4.0231 train_time:841269ms step_avg:170.13ms\nstep:4956/10000 train_loss:4.1398 train_time:841440ms step_avg:170.13ms\nstep:4957/10000 train_loss:3.9565 train_time:841610ms step_avg:170.13ms\nstep:4958/10000 train_loss:3.8108 train_time:841783ms step_avg:170.13ms\nstep:4959/10000 train_loss:4.0978 train_time:841954ms step_avg:170.13ms\nstep:4960/10000 train_loss:3.8949 train_time:842122ms step_avg:170.13ms\nstep:4961/10000 train_loss:3.7935 train_time:842290ms step_avg:170.13ms\nstep:4962/10000 train_loss:3.9392 train_time:842463ms step_avg:170.13ms\nstep:4963/10000 train_loss:4.0552 train_time:842632ms step_avg:170.13ms\nstep:4964/10000 train_loss:3.7838 train_time:842803ms step_avg:170.13ms\nstep:4965/10000 train_loss:3.8748 train_time:842971ms step_avg:170.13ms\nstep:4966/10000 train_loss:3.8932 train_time:843141ms step_avg:170.13ms\nstep:4967/10000 train_loss:4.0240 train_time:843310ms step_avg:170.13ms\nstep:4968/10000 train_loss:4.1117 train_time:843477ms step_avg:170.12ms\nstep:4969/10000 train_loss:3.9676 train_time:843647ms step_avg:170.12ms\nstep:4970/10000 train_loss:3.9476 train_time:843819ms step_avg:170.12ms\nstep:4971/10000 train_loss:3.9450 train_time:843989ms step_avg:170.12ms\nstep:4972/10000 train_loss:3.7483 train_time:844159ms step_avg:170.12ms\nstep:4973/10000 train_loss:3.8755 train_time:844327ms step_avg:170.12ms\nstep:4974/10000 train_loss:3.9880 train_time:844496ms step_avg:170.12ms\nstep:4975/10000 train_loss:3.9591 train_time:844666ms step_avg:170.12ms\nstep:4976/10000 train_loss:3.9918 train_time:844836ms step_avg:170.12ms\nstep:4977/10000 train_loss:3.9762 train_time:845008ms step_avg:170.12ms\nstep:4978/10000 train_loss:3.8617 train_time:845178ms step_avg:170.12ms\nstep:4979/10000 train_loss:3.8663 train_time:845347ms step_avg:170.12ms\nstep:4980/10000 train_loss:4.2130 train_time:845517ms step_avg:170.12ms\nstep:4981/10000 train_loss:4.1539 train_time:845687ms step_avg:170.12ms\nstep:4982/10000 train_loss:3.9540 train_time:845856ms step_avg:170.12ms\nstep:4983/10000 train_loss:4.0561 train_time:846024ms step_avg:170.12ms\nstep:4984/10000 train_loss:3.8623 train_time:846194ms step_avg:170.12ms\nstep:4985/10000 train_loss:3.9147 train_time:846367ms step_avg:170.12ms\nstep:4986/10000 train_loss:3.9709 train_time:846536ms step_avg:170.12ms\nstep:4987/10000 train_loss:4.0678 train_time:846706ms step_avg:170.12ms\nstep:4988/10000 train_loss:3.9373 train_time:846876ms step_avg:170.12ms\nstep:4989/10000 train_loss:3.9071 train_time:847046ms step_avg:170.12ms\nstep:4990/10000 train_loss:3.9740 train_time:847215ms step_avg:170.12ms\nstep:4991/10000 train_loss:3.9676 train_time:847384ms step_avg:170.12ms\nstep:4992/10000 train_loss:4.0554 train_time:847552ms step_avg:170.12ms\nstep:4993/10000 train_loss:3.9707 train_time:847724ms step_avg:170.12ms\nstep:4994/10000 train_loss:3.8122 train_time:847895ms step_avg:170.12ms\nstep:4995/10000 train_loss:3.9843 train_time:848075ms step_avg:170.13ms\nstep:4996/10000 train_loss:3.8875 train_time:848238ms step_avg:170.12ms\nstep:4997/10000 train_loss:3.9503 train_time:848410ms step_avg:170.12ms\nstep:4998/10000 train_loss:3.9690 train_time:848581ms step_avg:170.12ms\nstep:4999/10000 train_loss:3.9535 train_time:848749ms step_avg:170.12ms\nstep:5000/10000 train_loss:3.8503 train_time:848918ms step_avg:170.12ms\nstep:5001/10000 train_loss:3.9578 train_time:849090ms step_avg:170.12ms\nstep:5002/10000 train_loss:4.0167 train_time:849260ms step_avg:170.12ms\nstep:5003/10000 train_loss:3.9525 train_time:849429ms step_avg:170.12ms\nstep:5004/10000 train_loss:3.8751 train_time:849598ms step_avg:170.12ms\nstep:5005/10000 train_loss:3.9445 train_time:849766ms step_avg:170.12ms\nstep:5006/10000 train_loss:3.9734 train_time:849937ms step_avg:170.12ms\nstep:5007/10000 train_loss:3.9851 train_time:850109ms step_avg:170.12ms\nstep:5008/10000 train_loss:3.7408 train_time:850281ms step_avg:170.12ms\nstep:5009/10000 train_loss:3.9063 train_time:850446ms step_avg:170.12ms\nstep:5010/10000 train_loss:3.8652 train_time:850615ms step_avg:170.12ms\nstep:5011/10000 train_loss:3.8184 train_time:850786ms step_avg:170.12ms\nstep:5012/10000 train_loss:4.0107 train_time:850955ms step_avg:170.12ms\nstep:5013/10000 train_loss:3.8647 train_time:851127ms step_avg:170.12ms\nstep:5014/10000 train_loss:4.0828 train_time:851297ms step_avg:170.12ms\nstep:5015/10000 train_loss:3.8466 train_time:851465ms step_avg:170.12ms\nstep:5016/10000 train_loss:4.0918 train_time:851633ms step_avg:170.12ms\nstep:5017/10000 train_loss:3.8418 train_time:851803ms step_avg:170.12ms\nstep:5018/10000 train_loss:3.9718 train_time:851972ms step_avg:170.12ms\nstep:5019/10000 train_loss:3.9443 train_time:852142ms step_avg:170.12ms\nstep:5020/10000 train_loss:3.8649 train_time:852311ms step_avg:170.12ms\nstep:5021/10000 train_loss:3.9206 train_time:852480ms step_avg:170.12ms\nstep:5022/10000 train_loss:3.8606 train_time:852647ms step_avg:170.12ms\nstep:5023/10000 train_loss:4.0111 train_time:852817ms step_avg:170.12ms\nstep:5024/10000 train_loss:4.0396 train_time:852986ms step_avg:170.12ms\nstep:5025/10000 train_loss:3.9641 train_time:853156ms step_avg:170.12ms\nstep:5026/10000 train_loss:4.0147 train_time:853324ms step_avg:170.12ms\nstep:5027/10000 train_loss:3.9413 train_time:853493ms step_avg:170.12ms\nstep:5028/10000 train_loss:3.7863 train_time:853665ms step_avg:170.12ms\nstep:5029/10000 train_loss:4.0522 train_time:853835ms step_avg:170.12ms\nstep:5030/10000 train_loss:4.0755 train_time:854003ms step_avg:170.12ms\nstep:5031/10000 train_loss:3.9864 train_time:854173ms step_avg:170.12ms\nstep:5032/10000 train_loss:3.9797 train_time:854345ms step_avg:170.12ms\nstep:5033/10000 train_loss:3.8270 train_time:854514ms step_avg:170.12ms\nstep:5034/10000 train_loss:3.8923 train_time:854684ms step_avg:170.12ms\nstep:5035/10000 train_loss:3.9853 train_time:854854ms step_avg:170.12ms\nstep:5036/10000 train_loss:4.0922 train_time:855026ms step_avg:170.12ms\nstep:5037/10000 train_loss:3.9335 train_time:855194ms step_avg:170.12ms\nstep:5038/10000 train_loss:3.8898 train_time:855366ms step_avg:170.12ms\nstep:5039/10000 train_loss:3.8604 train_time:855535ms step_avg:170.12ms\nstep:5040/10000 train_loss:3.8993 train_time:855709ms step_avg:170.12ms\nstep:5041/10000 train_loss:4.0819 train_time:855881ms step_avg:170.12ms\nstep:5042/10000 train_loss:3.9298 train_time:856056ms step_avg:170.12ms\nstep:5043/10000 train_loss:4.7526 train_time:856220ms step_avg:170.12ms\nstep:5044/10000 train_loss:3.9625 train_time:856390ms step_avg:170.12ms\nstep:5045/10000 train_loss:3.9681 train_time:856561ms step_avg:170.12ms\nstep:5046/10000 train_loss:3.9330 train_time:856727ms step_avg:170.12ms\nstep:5047/10000 train_loss:3.9526 train_time:856899ms step_avg:170.12ms\nstep:5048/10000 train_loss:4.0927 train_time:857069ms step_avg:170.12ms\nstep:5049/10000 train_loss:3.9218 train_time:857240ms step_avg:170.12ms\nstep:5050/10000 train_loss:3.7723 train_time:857410ms step_avg:170.12ms\nstep:5051/10000 train_loss:4.0473 train_time:857581ms step_avg:170.12ms\nstep:5052/10000 train_loss:3.8004 train_time:857748ms step_avg:170.12ms\nstep:5053/10000 train_loss:3.9620 train_time:857916ms step_avg:170.12ms\nstep:5054/10000 train_loss:3.8525 train_time:858084ms step_avg:170.12ms\nstep:5055/10000 train_loss:3.9431 train_time:858254ms step_avg:170.12ms\nstep:5056/10000 train_loss:4.0133 train_time:858425ms step_avg:170.12ms\nstep:5057/10000 train_loss:4.0557 train_time:858593ms step_avg:170.12ms\nstep:5058/10000 train_loss:3.9764 train_time:858765ms step_avg:170.12ms\nstep:5059/10000 train_loss:4.1230 train_time:858933ms step_avg:170.12ms\nstep:5060/10000 train_loss:3.9247 train_time:859103ms step_avg:170.12ms\nstep:5061/10000 train_loss:3.9848 train_time:859273ms step_avg:170.12ms\nstep:5062/10000 train_loss:3.7726 train_time:859443ms step_avg:170.12ms\nstep:5063/10000 train_loss:3.9808 train_time:859612ms step_avg:170.12ms\nstep:5064/10000 train_loss:3.9351 train_time:859782ms step_avg:170.12ms\nstep:5065/10000 train_loss:4.1095 train_time:859951ms step_avg:170.12ms\nstep:5066/10000 train_loss:4.0177 train_time:860122ms step_avg:170.12ms\nstep:5067/10000 train_loss:3.9197 train_time:860293ms step_avg:170.12ms\nstep:5068/10000 train_loss:3.8576 train_time:860465ms step_avg:170.12ms\nstep:5069/10000 train_loss:3.8806 train_time:860633ms step_avg:170.12ms\nstep:5070/10000 train_loss:3.9939 train_time:860804ms step_avg:170.12ms\nstep:5071/10000 train_loss:4.0539 train_time:860973ms step_avg:170.12ms\nstep:5072/10000 train_loss:3.8681 train_time:861144ms step_avg:170.12ms\nstep:5073/10000 train_loss:3.8096 train_time:861313ms step_avg:170.12ms\nstep:5074/10000 train_loss:3.9980 train_time:861485ms step_avg:170.12ms\nstep:5075/10000 train_loss:3.8316 train_time:861656ms step_avg:170.12ms\nstep:5076/10000 train_loss:3.9748 train_time:861827ms step_avg:170.12ms\nstep:5077/10000 train_loss:4.0120 train_time:861997ms step_avg:170.12ms\nstep:5078/10000 train_loss:3.9366 train_time:862166ms step_avg:170.12ms\nstep:5079/10000 train_loss:3.7440 train_time:862337ms step_avg:170.12ms\nstep:5080/10000 train_loss:3.8600 train_time:862506ms step_avg:170.12ms\nstep:5081/10000 train_loss:3.9548 train_time:862676ms step_avg:170.12ms\nstep:5082/10000 train_loss:3.9798 train_time:862848ms step_avg:170.12ms\nstep:5083/10000 train_loss:3.8731 train_time:863018ms step_avg:170.12ms\nstep:5084/10000 train_loss:3.9502 train_time:863188ms step_avg:170.12ms\nstep:5085/10000 train_loss:3.8540 train_time:863359ms step_avg:170.12ms\nstep:5086/10000 train_loss:3.8386 train_time:863529ms step_avg:170.12ms\nstep:5087/10000 train_loss:3.9898 train_time:863699ms step_avg:170.12ms\nstep:5088/10000 train_loss:3.9425 train_time:863870ms step_avg:170.12ms\nstep:5089/10000 train_loss:3.8726 train_time:864041ms step_avg:170.12ms\nstep:5090/10000 train_loss:3.6864 train_time:864209ms step_avg:170.12ms\nstep:5091/10000 train_loss:3.9220 train_time:864379ms step_avg:170.12ms\nstep:5092/10000 train_loss:3.9584 train_time:864550ms step_avg:170.12ms\nstep:5093/10000 train_loss:4.1172 train_time:864722ms step_avg:170.12ms\nstep:5094/10000 train_loss:3.9473 train_time:864892ms step_avg:170.12ms\nstep:5095/10000 train_loss:3.9012 train_time:865064ms step_avg:170.12ms\nstep:5096/10000 train_loss:3.8712 train_time:865232ms step_avg:170.12ms\nstep:5097/10000 train_loss:3.8982 train_time:865400ms step_avg:170.12ms\nstep:5098/10000 train_loss:3.9176 train_time:865569ms step_avg:170.12ms\nstep:5099/10000 train_loss:3.9652 train_time:865739ms step_avg:170.12ms\nstep:5100/10000 train_loss:4.0059 train_time:865907ms step_avg:170.12ms\nstep:5101/10000 train_loss:3.9405 train_time:866076ms step_avg:170.12ms\nstep:5102/10000 train_loss:3.9891 train_time:866245ms step_avg:170.12ms\nstep:5103/10000 train_loss:4.0725 train_time:866415ms step_avg:170.12ms\nstep:5104/10000 train_loss:3.9077 train_time:866588ms step_avg:170.12ms\nstep:5105/10000 train_loss:3.9688 train_time:866758ms step_avg:170.12ms\nstep:5106/10000 train_loss:3.9016 train_time:866924ms step_avg:170.12ms\nstep:5107/10000 train_loss:3.9115 train_time:867095ms step_avg:170.12ms\nstep:5108/10000 train_loss:3.9917 train_time:867267ms step_avg:170.12ms\nstep:5109/10000 train_loss:3.9720 train_time:867437ms step_avg:170.12ms\nstep:5110/10000 train_loss:4.0118 train_time:867605ms step_avg:170.12ms\nstep:5111/10000 train_loss:3.8098 train_time:867776ms step_avg:170.12ms\nstep:5112/10000 train_loss:3.9550 train_time:867949ms step_avg:170.12ms\nstep:5113/10000 train_loss:3.8401 train_time:868121ms step_avg:170.12ms\nstep:5114/10000 train_loss:4.0382 train_time:868290ms step_avg:170.12ms\nstep:5115/10000 train_loss:3.9926 train_time:868463ms step_avg:170.12ms\nstep:5116/10000 train_loss:3.8810 train_time:868632ms step_avg:170.12ms\nstep:5117/10000 train_loss:4.0304 train_time:868802ms step_avg:170.12ms\nstep:5118/10000 train_loss:3.9958 train_time:868970ms step_avg:170.12ms\nstep:5119/10000 train_loss:3.9076 train_time:869140ms step_avg:170.12ms\nstep:5120/10000 train_loss:3.8187 train_time:869308ms step_avg:170.12ms\nstep:5121/10000 train_loss:3.8695 train_time:869477ms step_avg:170.12ms\nstep:5122/10000 train_loss:3.9820 train_time:869646ms step_avg:170.12ms\nstep:5123/10000 train_loss:3.9815 train_time:869818ms step_avg:170.12ms\nstep:5124/10000 train_loss:3.9671 train_time:869989ms step_avg:170.12ms\nstep:5125/10000 train_loss:3.9178 train_time:870161ms step_avg:170.12ms\nstep:5126/10000 train_loss:3.8012 train_time:870331ms step_avg:170.12ms\nstep:5127/10000 train_loss:3.7498 train_time:870500ms step_avg:170.12ms\nstep:5128/10000 train_loss:3.8724 train_time:870670ms step_avg:170.12ms\nstep:5129/10000 train_loss:4.0551 train_time:870842ms step_avg:170.12ms\nstep:5130/10000 train_loss:3.9496 train_time:871012ms step_avg:170.12ms\nstep:5131/10000 train_loss:3.9680 train_time:871186ms step_avg:170.12ms\nstep:5132/10000 train_loss:4.0650 train_time:871357ms step_avg:170.12ms\nstep:5133/10000 train_loss:3.9404 train_time:871524ms step_avg:170.12ms\nstep:5134/10000 train_loss:4.0103 train_time:871692ms step_avg:170.12ms\nstep:5135/10000 train_loss:3.9000 train_time:871862ms step_avg:170.12ms\nstep:5136/10000 train_loss:3.9083 train_time:872032ms step_avg:170.12ms\nstep:5137/10000 train_loss:4.0268 train_time:872202ms step_avg:170.12ms\nstep:5138/10000 train_loss:4.0255 train_time:872372ms step_avg:170.12ms\nstep:5139/10000 train_loss:3.8761 train_time:872543ms step_avg:170.12ms\nstep:5140/10000 train_loss:4.0086 train_time:872713ms step_avg:170.12ms\nstep:5141/10000 train_loss:3.9222 train_time:872884ms step_avg:170.12ms\nstep:5142/10000 train_loss:3.9231 train_time:873052ms step_avg:170.12ms\nstep:5143/10000 train_loss:3.8244 train_time:873221ms step_avg:170.12ms\nstep:5144/10000 train_loss:3.8728 train_time:873389ms step_avg:170.12ms\nstep:5145/10000 train_loss:3.9209 train_time:873559ms step_avg:170.12ms\nstep:5146/10000 train_loss:3.9660 train_time:873727ms step_avg:170.12ms\nstep:5147/10000 train_loss:3.8092 train_time:873896ms step_avg:170.12ms\nstep:5148/10000 train_loss:3.9728 train_time:874067ms step_avg:170.12ms\nstep:5149/10000 train_loss:3.8301 train_time:874237ms step_avg:170.12ms\nstep:5150/10000 train_loss:3.9259 train_time:874409ms step_avg:170.12ms\nstep:5151/10000 train_loss:4.0196 train_time:874580ms step_avg:170.12ms\nstep:5152/10000 train_loss:3.9905 train_time:874748ms step_avg:170.12ms\nstep:5153/10000 train_loss:4.0049 train_time:874918ms step_avg:170.12ms\nstep:5154/10000 train_loss:3.9161 train_time:875086ms step_avg:170.12ms\nstep:5155/10000 train_loss:3.8966 train_time:875257ms step_avg:170.12ms\nstep:5156/10000 train_loss:3.9247 train_time:875428ms step_avg:170.12ms\nstep:5157/10000 train_loss:4.1058 train_time:875598ms step_avg:170.12ms\nstep:5158/10000 train_loss:4.0125 train_time:875769ms step_avg:170.12ms\nstep:5159/10000 train_loss:4.1831 train_time:875941ms step_avg:170.12ms\nstep:5160/10000 train_loss:3.9469 train_time:876111ms step_avg:170.12ms\nstep:5161/10000 train_loss:3.8480 train_time:876281ms step_avg:170.12ms\nstep:5162/10000 train_loss:3.8898 train_time:876451ms step_avg:170.12ms\nstep:5163/10000 train_loss:3.9251 train_time:876621ms step_avg:170.12ms\nstep:5164/10000 train_loss:4.0831 train_time:876791ms step_avg:170.12ms\nstep:5165/10000 train_loss:3.8239 train_time:876964ms step_avg:170.12ms\nstep:5166/10000 train_loss:3.9506 train_time:877132ms step_avg:170.12ms\nstep:5167/10000 train_loss:3.7430 train_time:877303ms step_avg:170.12ms\nstep:5168/10000 train_loss:3.9258 train_time:877471ms step_avg:170.12ms\nstep:5169/10000 train_loss:4.0054 train_time:877639ms step_avg:170.12ms\nstep:5170/10000 train_loss:4.0225 train_time:877807ms step_avg:170.12ms\nstep:5171/10000 train_loss:3.9627 train_time:877977ms step_avg:170.12ms\nstep:5172/10000 train_loss:3.6938 train_time:878144ms step_avg:170.12ms\nstep:5173/10000 train_loss:3.7897 train_time:878313ms step_avg:170.12ms\nstep:5174/10000 train_loss:3.9501 train_time:878483ms step_avg:170.12ms\nstep:5175/10000 train_loss:3.8690 train_time:878650ms step_avg:170.12ms\nstep:5176/10000 train_loss:3.9309 train_time:878821ms step_avg:170.12ms\nstep:5177/10000 train_loss:4.0176 train_time:878992ms step_avg:170.12ms\nstep:5178/10000 train_loss:4.0415 train_time:879161ms step_avg:170.12ms\nstep:5179/10000 train_loss:3.9031 train_time:879331ms step_avg:170.12ms\nstep:5180/10000 train_loss:3.9475 train_time:879505ms step_avg:170.12ms\nstep:5181/10000 train_loss:4.1210 train_time:879673ms step_avg:170.12ms\nstep:5182/10000 train_loss:4.0711 train_time:879845ms step_avg:170.12ms\nstep:5183/10000 train_loss:3.9048 train_time:880016ms step_avg:170.12ms\nstep:5184/10000 train_loss:3.9823 train_time:880192ms step_avg:170.12ms\nstep:5185/10000 train_loss:4.0832 train_time:880358ms step_avg:170.12ms\nstep:5186/10000 train_loss:4.0442 train_time:880529ms step_avg:170.12ms\nstep:5187/10000 train_loss:3.9127 train_time:880699ms step_avg:170.12ms\nstep:5188/10000 train_loss:4.0023 train_time:880870ms step_avg:170.12ms\nstep:5189/10000 train_loss:3.8759 train_time:881041ms step_avg:170.12ms\nstep:5190/10000 train_loss:3.8575 train_time:881211ms step_avg:170.12ms\nstep:5191/10000 train_loss:4.0473 train_time:881383ms step_avg:170.12ms\nstep:5192/10000 train_loss:3.9727 train_time:881552ms step_avg:170.12ms\nstep:5193/10000 train_loss:4.0134 train_time:881721ms step_avg:170.12ms\nstep:5194/10000 train_loss:3.9268 train_time:881891ms step_avg:170.12ms\nstep:5195/10000 train_loss:4.0571 train_time:882060ms step_avg:170.12ms\nstep:5196/10000 train_loss:4.0310 train_time:882227ms step_avg:170.12ms\nstep:5197/10000 train_loss:3.9215 train_time:882398ms step_avg:170.12ms\nstep:5198/10000 train_loss:3.8157 train_time:882569ms step_avg:170.12ms\nstep:5199/10000 train_loss:4.1112 train_time:882739ms step_avg:170.12ms\nstep:5200/10000 train_loss:3.9345 train_time:882909ms step_avg:170.12ms\nstep:5201/10000 train_loss:3.8306 train_time:883081ms step_avg:170.12ms\nstep:5202/10000 train_loss:3.9158 train_time:883250ms step_avg:170.12ms\nstep:5203/10000 train_loss:3.8820 train_time:883421ms step_avg:170.12ms\nstep:5204/10000 train_loss:4.0588 train_time:883589ms step_avg:170.12ms\nstep:5205/10000 train_loss:4.0250 train_time:883759ms step_avg:170.12ms\nstep:5206/10000 train_loss:3.9452 train_time:883931ms step_avg:170.12ms\nstep:5207/10000 train_loss:3.8826 train_time:884101ms step_avg:170.12ms\nstep:5208/10000 train_loss:3.9815 train_time:884271ms step_avg:170.12ms\nstep:5209/10000 train_loss:3.8267 train_time:884441ms step_avg:170.12ms\nstep:5210/10000 train_loss:3.9818 train_time:884609ms step_avg:170.12ms\nstep:5211/10000 train_loss:3.8633 train_time:884779ms step_avg:170.12ms\nstep:5212/10000 train_loss:3.9727 train_time:884951ms step_avg:170.12ms\nstep:5213/10000 train_loss:4.0525 train_time:885123ms step_avg:170.12ms\nstep:5214/10000 train_loss:3.9932 train_time:885294ms step_avg:170.12ms\nstep:5215/10000 train_loss:4.0894 train_time:885468ms step_avg:170.12ms\nstep:5216/10000 train_loss:3.8970 train_time:885638ms step_avg:170.12ms\nstep:5217/10000 train_loss:3.8729 train_time:885808ms step_avg:170.12ms\nstep:5218/10000 train_loss:4.1016 train_time:885977ms step_avg:170.12ms\nstep:5219/10000 train_loss:3.9307 train_time:886146ms step_avg:170.12ms\nstep:5220/10000 train_loss:4.0346 train_time:886315ms step_avg:170.12ms\nstep:5221/10000 train_loss:3.9181 train_time:886483ms step_avg:170.12ms\nstep:5222/10000 train_loss:3.9140 train_time:886651ms step_avg:170.12ms\nstep:5223/10000 train_loss:3.8999 train_time:886824ms step_avg:170.12ms\nstep:5224/10000 train_loss:4.0824 train_time:886993ms step_avg:170.12ms\nstep:5225/10000 train_loss:3.9658 train_time:887165ms step_avg:170.12ms\nstep:5226/10000 train_loss:4.1213 train_time:887336ms step_avg:170.12ms\nstep:5227/10000 train_loss:3.9249 train_time:887505ms step_avg:170.12ms\nstep:5228/10000 train_loss:3.8709 train_time:887673ms step_avg:170.12ms\nstep:5229/10000 train_loss:3.8996 train_time:887842ms step_avg:170.12ms\nstep:5230/10000 train_loss:3.8611 train_time:888012ms step_avg:170.12ms\nstep:5231/10000 train_loss:3.8945 train_time:888184ms step_avg:170.12ms\nstep:5232/10000 train_loss:3.8736 train_time:888351ms step_avg:170.12ms\nstep:5233/10000 train_loss:3.9474 train_time:888522ms step_avg:170.12ms\nstep:5234/10000 train_loss:4.0034 train_time:888692ms step_avg:170.12ms\nstep:5235/10000 train_loss:3.9256 train_time:888862ms step_avg:170.12ms\nstep:5236/10000 train_loss:3.8701 train_time:889032ms step_avg:170.12ms\nstep:5237/10000 train_loss:3.9470 train_time:889205ms step_avg:170.12ms\nstep:5238/10000 train_loss:4.0194 train_time:889373ms step_avg:170.12ms\nstep:5239/10000 train_loss:3.8377 train_time:889544ms step_avg:170.12ms\nstep:5240/10000 train_loss:3.8940 train_time:889713ms step_avg:170.12ms\nstep:5241/10000 train_loss:3.8879 train_time:889885ms step_avg:170.12ms\nstep:5242/10000 train_loss:3.9139 train_time:890049ms step_avg:170.12ms\nstep:5243/10000 train_loss:3.9579 train_time:890219ms step_avg:170.12ms\nstep:5244/10000 train_loss:3.8508 train_time:890388ms step_avg:170.12ms\nstep:5245/10000 train_loss:3.9215 train_time:890560ms step_avg:170.12ms\nstep:5246/10000 train_loss:3.8260 train_time:890731ms step_avg:170.12ms\nstep:5247/10000 train_loss:4.0032 train_time:890901ms step_avg:170.12ms\nstep:5248/10000 train_loss:4.0237 train_time:891069ms step_avg:170.12ms\nstep:5249/10000 train_loss:4.2050 train_time:891240ms step_avg:170.12ms\nstep:5250/10000 train_loss:3.8881 train_time:891408ms step_avg:170.12ms\nstep:5251/10000 train_loss:3.7805 train_time:891578ms step_avg:170.12ms\nstep:5252/10000 train_loss:3.9382 train_time:891749ms step_avg:170.12ms\nstep:5253/10000 train_loss:3.9291 train_time:891920ms step_avg:170.12ms\nstep:5254/10000 train_loss:3.8403 train_time:892089ms step_avg:170.12ms\nstep:5255/10000 train_loss:4.0185 train_time:892257ms step_avg:170.12ms\nstep:5256/10000 train_loss:3.9939 train_time:892428ms step_avg:170.12ms\nstep:5257/10000 train_loss:3.8585 train_time:892598ms step_avg:170.12ms\nstep:5258/10000 train_loss:3.9822 train_time:892770ms step_avg:170.12ms\nstep:5259/10000 train_loss:3.9584 train_time:892941ms step_avg:170.12ms\nstep:5260/10000 train_loss:3.9802 train_time:893110ms step_avg:170.12ms\nstep:5261/10000 train_loss:3.8887 train_time:893280ms step_avg:170.12ms\nstep:5262/10000 train_loss:3.9190 train_time:893449ms step_avg:170.12ms\nstep:5263/10000 train_loss:3.9019 train_time:893620ms step_avg:170.12ms\nstep:5264/10000 train_loss:3.9807 train_time:893788ms step_avg:170.12ms\nstep:5265/10000 train_loss:3.7987 train_time:893960ms step_avg:170.12ms\nstep:5266/10000 train_loss:3.6538 train_time:894132ms step_avg:170.12ms\nstep:5267/10000 train_loss:3.8325 train_time:894301ms step_avg:170.12ms\nstep:5268/10000 train_loss:3.8211 train_time:894469ms step_avg:170.12ms\nstep:5269/10000 train_loss:3.8387 train_time:894641ms step_avg:170.12ms\nstep:5270/10000 train_loss:3.8720 train_time:894810ms step_avg:170.12ms\nstep:5271/10000 train_loss:3.9727 train_time:894983ms step_avg:170.12ms\nstep:5272/10000 train_loss:4.0650 train_time:895151ms step_avg:170.12ms\nstep:5273/10000 train_loss:3.8332 train_time:895321ms step_avg:170.12ms\nstep:5274/10000 train_loss:3.9416 train_time:895490ms step_avg:170.12ms\nstep:5275/10000 train_loss:4.0301 train_time:895662ms step_avg:170.12ms\nstep:5276/10000 train_loss:4.0005 train_time:895830ms step_avg:170.12ms\nstep:5277/10000 train_loss:4.0310 train_time:896001ms step_avg:170.12ms\nstep:5278/10000 train_loss:3.9645 train_time:896170ms step_avg:170.12ms\nstep:5279/10000 train_loss:3.8028 train_time:896340ms step_avg:170.12ms\nstep:5280/10000 train_loss:3.9224 train_time:896510ms step_avg:170.12ms\nstep:5281/10000 train_loss:3.9980 train_time:896680ms step_avg:170.12ms\nstep:5282/10000 train_loss:3.8193 train_time:896849ms step_avg:170.12ms\nstep:5283/10000 train_loss:3.9094 train_time:897020ms step_avg:170.12ms\nstep:5284/10000 train_loss:4.0103 train_time:897187ms step_avg:170.12ms\nstep:5285/10000 train_loss:3.8537 train_time:897357ms step_avg:170.12ms\nstep:5286/10000 train_loss:3.9760 train_time:897526ms step_avg:170.11ms\nstep:5287/10000 train_loss:3.8287 train_time:897695ms step_avg:170.11ms\nstep:5288/10000 train_loss:3.9724 train_time:897865ms step_avg:170.11ms\nstep:5289/10000 train_loss:3.9879 train_time:898035ms step_avg:170.11ms\nstep:5290/10000 train_loss:4.0229 train_time:898205ms step_avg:170.11ms\nstep:5291/10000 train_loss:3.9194 train_time:898375ms step_avg:170.11ms\nstep:5292/10000 train_loss:3.8894 train_time:898548ms step_avg:170.12ms\nstep:5293/10000 train_loss:4.0615 train_time:898717ms step_avg:170.11ms\nstep:5294/10000 train_loss:3.9151 train_time:898890ms step_avg:170.12ms\nstep:5295/10000 train_loss:3.6779 train_time:899062ms step_avg:170.12ms\nstep:5296/10000 train_loss:4.1109 train_time:899232ms step_avg:170.12ms\nstep:5297/10000 train_loss:4.0179 train_time:899401ms step_avg:170.12ms\nstep:5298/10000 train_loss:3.8973 train_time:899569ms step_avg:170.12ms\nstep:5299/10000 train_loss:3.9668 train_time:899740ms step_avg:170.12ms\nstep:5300/10000 train_loss:4.0054 train_time:899911ms step_avg:170.12ms\nstep:5301/10000 train_loss:3.7229 train_time:900083ms step_avg:170.12ms\nstep:5302/10000 train_loss:3.9296 train_time:900251ms step_avg:170.12ms\nstep:5303/10000 train_loss:3.8185 train_time:900420ms step_avg:170.12ms\nstep:5304/10000 train_loss:4.0216 train_time:900596ms step_avg:170.12ms\nstep:5305/10000 train_loss:3.8984 train_time:900757ms step_avg:170.11ms\nstep:5306/10000 train_loss:4.0083 train_time:900928ms step_avg:170.11ms\nstep:5307/10000 train_loss:3.9099 train_time:901098ms step_avg:170.11ms\nstep:5308/10000 train_loss:4.0689 train_time:901267ms step_avg:170.11ms\nstep:5309/10000 train_loss:3.9987 train_time:901438ms step_avg:170.11ms\nstep:5310/10000 train_loss:4.0661 train_time:901607ms step_avg:170.11ms\nstep:5311/10000 train_loss:3.8532 train_time:901778ms step_avg:170.11ms\nstep:5312/10000 train_loss:3.8380 train_time:901949ms step_avg:170.11ms\nstep:5313/10000 train_loss:3.8613 train_time:902118ms step_avg:170.11ms\nstep:5314/10000 train_loss:3.8540 train_time:902288ms step_avg:170.11ms\nstep:5315/10000 train_loss:3.9654 train_time:902458ms step_avg:170.11ms\nstep:5316/10000 train_loss:3.8835 train_time:902625ms step_avg:170.11ms\nstep:5317/10000 train_loss:3.8828 train_time:902794ms step_avg:170.11ms\nstep:5318/10000 train_loss:3.7519 train_time:902965ms step_avg:170.11ms\nstep:5319/10000 train_loss:3.9658 train_time:903138ms step_avg:170.11ms\nstep:5320/10000 train_loss:3.9699 train_time:903307ms step_avg:170.11ms\nstep:5321/10000 train_loss:3.9105 train_time:903474ms step_avg:170.11ms\nstep:5322/10000 train_loss:4.0264 train_time:903642ms step_avg:170.11ms\nstep:5323/10000 train_loss:3.9823 train_time:903811ms step_avg:170.11ms\nstep:5324/10000 train_loss:2.8577 train_time:903983ms step_avg:170.11ms\nstep:5325/10000 train_loss:3.9077 train_time:904153ms step_avg:170.11ms\nstep:5326/10000 train_loss:3.9311 train_time:904325ms step_avg:170.11ms\nstep:5327/10000 train_loss:3.8559 train_time:904494ms step_avg:170.11ms\nstep:5328/10000 train_loss:3.9374 train_time:904665ms step_avg:170.11ms\nstep:5329/10000 train_loss:4.2154 train_time:904835ms step_avg:170.11ms\nstep:5330/10000 train_loss:4.0182 train_time:905006ms step_avg:170.11ms\nstep:5331/10000 train_loss:3.9152 train_time:905178ms step_avg:170.11ms\nstep:5332/10000 train_loss:3.9212 train_time:905351ms step_avg:170.11ms\nstep:5333/10000 train_loss:3.7166 train_time:905523ms step_avg:170.12ms\nstep:5334/10000 train_loss:3.8656 train_time:905906ms step_avg:170.16ms\nstep:5335/10000 train_loss:3.9019 train_time:906071ms step_avg:170.15ms\nstep:5336/10000 train_loss:3.9484 train_time:906238ms step_avg:170.15ms\nstep:5337/10000 train_loss:3.9672 train_time:906407ms step_avg:170.15ms\nstep:5338/10000 train_loss:3.8871 train_time:906576ms step_avg:170.15ms\nstep:5339/10000 train_loss:3.8859 train_time:906746ms step_avg:170.15ms\nstep:5340/10000 train_loss:4.3065 train_time:906924ms step_avg:170.15ms\nstep:5341/10000 train_loss:4.0446 train_time:907091ms step_avg:170.15ms\nstep:5342/10000 train_loss:3.8618 train_time:907260ms step_avg:170.15ms\nstep:5343/10000 train_loss:3.9010 train_time:907429ms step_avg:170.15ms\nstep:5344/10000 train_loss:4.0073 train_time:907597ms step_avg:170.15ms\nstep:5345/10000 train_loss:4.0100 train_time:907766ms step_avg:170.15ms\nstep:5346/10000 train_loss:3.9662 train_time:907936ms step_avg:170.15ms\nstep:5347/10000 train_loss:3.8773 train_time:908109ms step_avg:170.15ms\nstep:5348/10000 train_loss:3.9162 train_time:908279ms step_avg:170.15ms\nstep:5349/10000 train_loss:3.8918 train_time:908447ms step_avg:170.15ms\nstep:5350/10000 train_loss:3.8948 train_time:908616ms step_avg:170.15ms\nstep:5351/10000 train_loss:3.9428 train_time:908790ms step_avg:170.15ms\nstep:5352/10000 train_loss:3.9046 train_time:908962ms step_avg:170.15ms\nstep:5353/10000 train_loss:4.3959 train_time:909131ms step_avg:170.15ms\nstep:5354/10000 train_loss:3.9719 train_time:909305ms step_avg:170.15ms\nstep:5355/10000 train_loss:3.9532 train_time:909473ms step_avg:170.15ms\nstep:5356/10000 train_loss:3.8814 train_time:909643ms step_avg:170.15ms\nstep:5357/10000 train_loss:3.9621 train_time:909812ms step_avg:170.15ms\nstep:5358/10000 train_loss:3.8784 train_time:909986ms step_avg:170.15ms\nstep:5359/10000 train_loss:3.9055 train_time:910163ms step_avg:170.16ms\nstep:5360/10000 train_loss:4.1346 train_time:910327ms step_avg:170.15ms\nstep:5361/10000 train_loss:3.8756 train_time:910498ms step_avg:170.15ms\nstep:5362/10000 train_loss:3.8005 train_time:910670ms step_avg:170.16ms\nstep:5363/10000 train_loss:4.0990 train_time:910842ms step_avg:170.16ms\nstep:5364/10000 train_loss:4.0371 train_time:911011ms step_avg:170.16ms\nstep:5365/10000 train_loss:3.9094 train_time:911182ms step_avg:170.16ms\nstep:5366/10000 train_loss:3.8583 train_time:911350ms step_avg:170.16ms\nstep:5367/10000 train_loss:3.7846 train_time:911521ms step_avg:170.16ms\nstep:5368/10000 train_loss:3.9020 train_time:911688ms step_avg:170.15ms\nstep:5369/10000 train_loss:3.8773 train_time:911859ms step_avg:170.15ms\nstep:5370/10000 train_loss:3.9711 train_time:912030ms step_avg:170.15ms\nstep:5371/10000 train_loss:3.5189 train_time:912201ms step_avg:170.15ms\nstep:5372/10000 train_loss:3.8702 train_time:912369ms step_avg:170.15ms\nstep:5373/10000 train_loss:3.7839 train_time:912538ms step_avg:170.15ms\nstep:5374/10000 train_loss:3.9209 train_time:912708ms step_avg:170.15ms\nstep:5375/10000 train_loss:3.9375 train_time:912877ms step_avg:170.15ms\nstep:5376/10000 train_loss:4.1815 train_time:913047ms step_avg:170.15ms\nstep:5377/10000 train_loss:3.8606 train_time:913220ms step_avg:170.15ms\nstep:5378/10000 train_loss:4.0563 train_time:913391ms step_avg:170.15ms\nstep:5379/10000 train_loss:3.9071 train_time:913559ms step_avg:170.15ms\nstep:5380/10000 train_loss:3.9523 train_time:913730ms step_avg:170.15ms\nstep:5381/10000 train_loss:4.3596 train_time:913898ms step_avg:170.15ms\nstep:5382/10000 train_loss:3.9236 train_time:914069ms step_avg:170.15ms\nstep:5383/10000 train_loss:3.8842 train_time:914239ms step_avg:170.15ms\nstep:5384/10000 train_loss:3.9477 train_time:914410ms step_avg:170.15ms\nstep:5385/10000 train_loss:3.9863 train_time:914581ms step_avg:170.15ms\nstep:5386/10000 train_loss:3.8945 train_time:914750ms step_avg:170.15ms\nstep:5387/10000 train_loss:3.9391 train_time:914921ms step_avg:170.15ms\nstep:5388/10000 train_loss:4.0034 train_time:915090ms step_avg:170.15ms\nstep:5389/10000 train_loss:3.9043 train_time:915262ms step_avg:170.15ms\nstep:5390/10000 train_loss:3.8915 train_time:915432ms step_avg:170.15ms\nstep:5391/10000 train_loss:3.8736 train_time:915603ms step_avg:170.15ms\nstep:5392/10000 train_loss:3.9514 train_time:915771ms step_avg:170.15ms\nstep:5393/10000 train_loss:3.7888 train_time:915941ms step_avg:170.15ms\nstep:5394/10000 train_loss:3.8455 train_time:916111ms step_avg:170.15ms\nstep:5395/10000 train_loss:4.0187 train_time:916284ms step_avg:170.15ms\nstep:5396/10000 train_loss:3.8674 train_time:916454ms step_avg:170.15ms\nstep:5397/10000 train_loss:3.9114 train_time:916626ms step_avg:170.16ms\nstep:5398/10000 train_loss:3.8169 train_time:916799ms step_avg:170.16ms\nstep:5399/10000 train_loss:4.0010 train_time:916971ms step_avg:170.16ms\nstep:5400/10000 train_loss:3.7671 train_time:917142ms step_avg:170.16ms\nstep:5401/10000 train_loss:3.8463 train_time:917310ms step_avg:170.16ms\nstep:5402/10000 train_loss:4.0975 train_time:917481ms step_avg:170.16ms\nstep:5403/10000 train_loss:3.9463 train_time:917650ms step_avg:170.16ms\nstep:5404/10000 train_loss:3.8841 train_time:917818ms step_avg:170.16ms\nstep:5405/10000 train_loss:3.9567 train_time:917988ms step_avg:170.16ms\nstep:5406/10000 train_loss:3.8209 train_time:918159ms step_avg:170.16ms\nstep:5407/10000 train_loss:3.9275 train_time:918328ms step_avg:170.16ms\nstep:5408/10000 train_loss:3.8474 train_time:918499ms step_avg:170.16ms\nstep:5409/10000 train_loss:3.8228 train_time:918670ms step_avg:170.16ms\nstep:5410/10000 train_loss:3.9235 train_time:918841ms step_avg:170.16ms\nstep:5411/10000 train_loss:3.8559 train_time:919011ms step_avg:170.16ms\nstep:5412/10000 train_loss:3.9709 train_time:919182ms step_avg:170.16ms\nstep:5413/10000 train_loss:3.8326 train_time:919353ms step_avg:170.16ms\nstep:5414/10000 train_loss:4.0192 train_time:919523ms step_avg:170.16ms\nstep:5415/10000 train_loss:3.9784 train_time:919691ms step_avg:170.16ms\nstep:5416/10000 train_loss:3.8127 train_time:919860ms step_avg:170.16ms\nstep:5417/10000 train_loss:4.0415 train_time:920030ms step_avg:170.16ms\nstep:5418/10000 train_loss:3.9572 train_time:920203ms step_avg:170.16ms\nstep:5419/10000 train_loss:3.9962 train_time:920369ms step_avg:170.16ms\nstep:5420/10000 train_loss:3.8723 train_time:920539ms step_avg:170.16ms\nstep:5421/10000 train_loss:3.7856 train_time:920709ms step_avg:170.16ms\nstep:5422/10000 train_loss:3.8784 train_time:920881ms step_avg:170.16ms\nstep:5423/10000 train_loss:3.9062 train_time:921050ms step_avg:170.16ms\nstep:5424/10000 train_loss:4.0011 train_time:921219ms step_avg:170.16ms\nstep:5425/10000 train_loss:4.0276 train_time:921390ms step_avg:170.16ms\nstep:5426/10000 train_loss:3.8763 train_time:921559ms step_avg:170.15ms\nstep:5427/10000 train_loss:3.8375 train_time:921731ms step_avg:170.16ms\nstep:5428/10000 train_loss:3.8211 train_time:921903ms step_avg:170.16ms\nstep:5429/10000 train_loss:3.8283 train_time:922075ms step_avg:170.16ms\nstep:5430/10000 train_loss:3.9876 train_time:922245ms step_avg:170.16ms\nstep:5431/10000 train_loss:3.8832 train_time:922414ms step_avg:170.16ms\nstep:5432/10000 train_loss:3.7533 train_time:922582ms step_avg:170.16ms\nstep:5433/10000 train_loss:3.8678 train_time:922749ms step_avg:170.15ms\nstep:5434/10000 train_loss:3.8692 train_time:922923ms step_avg:170.16ms\nstep:5435/10000 train_loss:3.8910 train_time:923092ms step_avg:170.16ms\nstep:5436/10000 train_loss:4.1013 train_time:923266ms step_avg:170.16ms\nstep:5437/10000 train_loss:3.9820 train_time:923434ms step_avg:170.16ms\nstep:5438/10000 train_loss:3.9467 train_time:923601ms step_avg:170.16ms\nstep:5439/10000 train_loss:3.8587 train_time:923772ms step_avg:170.16ms\nstep:5440/10000 train_loss:4.0405 train_time:923943ms step_avg:170.16ms\nstep:5441/10000 train_loss:4.0946 train_time:924112ms step_avg:170.16ms\nstep:5442/10000 train_loss:4.0667 train_time:924286ms step_avg:170.16ms\nstep:5443/10000 train_loss:3.8879 train_time:924456ms step_avg:170.16ms\nstep:5444/10000 train_loss:3.7557 train_time:924626ms step_avg:170.16ms\nstep:5445/10000 train_loss:3.9445 train_time:924794ms step_avg:170.16ms\nstep:5446/10000 train_loss:4.0809 train_time:924968ms step_avg:170.16ms\nstep:5447/10000 train_loss:4.0072 train_time:925139ms step_avg:170.16ms\nstep:5448/10000 train_loss:4.0467 train_time:925309ms step_avg:170.16ms\nstep:5449/10000 train_loss:3.8844 train_time:925480ms step_avg:170.16ms\nstep:5450/10000 train_loss:3.8396 train_time:925649ms step_avg:170.16ms\nstep:5451/10000 train_loss:3.9160 train_time:925819ms step_avg:170.16ms\nstep:5452/10000 train_loss:3.9076 train_time:925986ms step_avg:170.16ms\nstep:5453/10000 train_loss:3.9815 train_time:926158ms step_avg:170.16ms\nstep:5454/10000 train_loss:3.8639 train_time:926329ms step_avg:170.16ms\nstep:5455/10000 train_loss:3.7198 train_time:926499ms step_avg:170.16ms\nstep:5456/10000 train_loss:3.8177 train_time:926668ms step_avg:170.16ms\nstep:5457/10000 train_loss:3.9020 train_time:926839ms step_avg:170.16ms\nstep:5458/10000 train_loss:3.8745 train_time:927010ms step_avg:170.16ms\nstep:5459/10000 train_loss:3.9277 train_time:927178ms step_avg:170.16ms\nstep:5460/10000 train_loss:3.9457 train_time:927351ms step_avg:170.16ms\nstep:5461/10000 train_loss:3.9529 train_time:927522ms step_avg:170.16ms\nstep:5462/10000 train_loss:3.8099 train_time:927690ms step_avg:170.16ms\nstep:5463/10000 train_loss:3.9717 train_time:927859ms step_avg:170.16ms\nstep:5464/10000 train_loss:3.8376 train_time:928031ms step_avg:170.16ms\nstep:5465/10000 train_loss:4.0161 train_time:928200ms step_avg:170.16ms\nstep:5466/10000 train_loss:3.7610 train_time:928370ms step_avg:170.16ms\nstep:5467/10000 train_loss:3.7793 train_time:928541ms step_avg:170.16ms\nstep:5468/10000 train_loss:3.8936 train_time:928710ms step_avg:170.16ms\nstep:5469/10000 train_loss:3.9821 train_time:928881ms step_avg:170.16ms\nstep:5470/10000 train_loss:3.7877 train_time:929050ms step_avg:170.16ms\nstep:5471/10000 train_loss:3.9149 train_time:929219ms step_avg:170.16ms\nstep:5472/10000 train_loss:3.8993 train_time:929388ms step_avg:170.16ms\nstep:5473/10000 train_loss:3.9114 train_time:929560ms step_avg:170.16ms\nstep:5474/10000 train_loss:3.8708 train_time:929732ms step_avg:170.16ms\nstep:5475/10000 train_loss:3.8350 train_time:929902ms step_avg:170.16ms\nstep:5476/10000 train_loss:3.9932 train_time:930072ms step_avg:170.16ms\nstep:5477/10000 train_loss:3.8846 train_time:930242ms step_avg:170.16ms\nstep:5478/10000 train_loss:3.9287 train_time:930410ms step_avg:170.16ms\nstep:5479/10000 train_loss:3.8921 train_time:930581ms step_avg:170.16ms\nstep:5480/10000 train_loss:3.9800 train_time:930753ms step_avg:170.16ms\nstep:5481/10000 train_loss:4.0758 train_time:930923ms step_avg:170.16ms\nstep:5482/10000 train_loss:3.9068 train_time:931094ms step_avg:170.16ms\nstep:5483/10000 train_loss:3.9331 train_time:931263ms step_avg:170.16ms\nstep:5484/10000 train_loss:3.9488 train_time:931433ms step_avg:170.16ms\nstep:5485/10000 train_loss:3.8960 train_time:931602ms step_avg:170.16ms\nstep:5486/10000 train_loss:4.0891 train_time:931771ms step_avg:170.16ms\nstep:5487/10000 train_loss:3.9191 train_time:931943ms step_avg:170.16ms\nstep:5488/10000 train_loss:3.7953 train_time:932113ms step_avg:170.16ms\nstep:5489/10000 train_loss:3.8052 train_time:932285ms step_avg:170.16ms\nstep:5490/10000 train_loss:3.8995 train_time:932452ms step_avg:170.16ms\nstep:5491/10000 train_loss:3.8444 train_time:932623ms step_avg:170.16ms\nstep:5492/10000 train_loss:4.1572 train_time:932793ms step_avg:170.16ms\nstep:5493/10000 train_loss:3.8557 train_time:932964ms step_avg:170.16ms\nstep:5494/10000 train_loss:3.7763 train_time:933133ms step_avg:170.16ms\nstep:5495/10000 train_loss:4.4172 train_time:933306ms step_avg:170.16ms\nstep:5496/10000 train_loss:3.9250 train_time:933474ms step_avg:170.16ms\nstep:5497/10000 train_loss:3.8776 train_time:933642ms step_avg:170.16ms\nstep:5498/10000 train_loss:3.7824 train_time:933812ms step_avg:170.16ms\nstep:5499/10000 train_loss:3.8688 train_time:933987ms step_avg:170.16ms\nstep:5500/10000 train_loss:3.9424 train_time:934158ms step_avg:170.16ms\nstep:5501/10000 train_loss:3.8478 train_time:934327ms step_avg:170.16ms\nstep:5502/10000 train_loss:4.0128 train_time:934496ms step_avg:170.16ms\nstep:5503/10000 train_loss:3.9989 train_time:934667ms step_avg:170.16ms\nstep:5504/10000 train_loss:3.9331 train_time:934836ms step_avg:170.16ms\nstep:5505/10000 train_loss:3.8095 train_time:935005ms step_avg:170.16ms\nstep:5506/10000 train_loss:3.9151 train_time:935176ms step_avg:170.16ms\nstep:5507/10000 train_loss:3.9117 train_time:935345ms step_avg:170.16ms\nstep:5508/10000 train_loss:3.7646 train_time:935513ms step_avg:170.16ms\nstep:5509/10000 train_loss:3.8757 train_time:935682ms step_avg:170.15ms\nstep:5510/10000 train_loss:3.9896 train_time:935851ms step_avg:170.15ms\nstep:5511/10000 train_loss:3.8682 train_time:936021ms step_avg:170.15ms\nstep:5512/10000 train_loss:4.1466 train_time:936192ms step_avg:170.15ms\nstep:5513/10000 train_loss:3.9757 train_time:936367ms step_avg:170.16ms\nstep:5514/10000 train_loss:3.8174 train_time:936535ms step_avg:170.16ms\nstep:5515/10000 train_loss:3.9110 train_time:936704ms step_avg:170.16ms\nstep:5516/10000 train_loss:3.9152 train_time:936873ms step_avg:170.15ms\nstep:5517/10000 train_loss:3.8910 train_time:937044ms step_avg:170.16ms\nstep:5518/10000 train_loss:3.8972 train_time:937215ms step_avg:170.16ms\nstep:5519/10000 train_loss:4.1203 train_time:937386ms step_avg:170.16ms\nstep:5520/10000 train_loss:4.0477 train_time:937557ms step_avg:170.16ms\nstep:5521/10000 train_loss:3.9229 train_time:937729ms step_avg:170.16ms\nstep:5522/10000 train_loss:3.8505 train_time:937901ms step_avg:170.16ms\nstep:5523/10000 train_loss:3.8494 train_time:938068ms step_avg:170.16ms\nstep:5524/10000 train_loss:4.1026 train_time:938238ms step_avg:170.16ms\nstep:5525/10000 train_loss:3.7852 train_time:938408ms step_avg:170.16ms\nstep:5526/10000 train_loss:3.8774 train_time:938578ms step_avg:170.16ms\nstep:5527/10000 train_loss:3.8818 train_time:938747ms step_avg:170.16ms\nstep:5528/10000 train_loss:3.7349 train_time:938922ms step_avg:170.16ms\nstep:5529/10000 train_loss:3.8308 train_time:939088ms step_avg:170.16ms\nstep:5530/10000 train_loss:4.0741 train_time:939258ms step_avg:170.16ms\nstep:5531/10000 train_loss:3.7298 train_time:939432ms step_avg:170.16ms\nstep:5532/10000 train_loss:3.9053 train_time:939602ms step_avg:170.16ms\nstep:5533/10000 train_loss:3.8198 train_time:939769ms step_avg:170.16ms\nstep:5534/10000 train_loss:3.8153 train_time:939941ms step_avg:170.16ms\nstep:5535/10000 train_loss:3.6920 train_time:940113ms step_avg:170.16ms\nstep:5536/10000 train_loss:4.3519 train_time:940283ms step_avg:170.16ms\nstep:5537/10000 train_loss:3.8934 train_time:940451ms step_avg:170.16ms\nstep:5538/10000 train_loss:3.8316 train_time:940621ms step_avg:170.16ms\nstep:5539/10000 train_loss:3.9174 train_time:940789ms step_avg:170.16ms\nstep:5540/10000 train_loss:3.8332 train_time:940961ms step_avg:170.16ms\nstep:5541/10000 train_loss:3.9691 train_time:941132ms step_avg:170.16ms\nstep:5542/10000 train_loss:3.9112 train_time:941301ms step_avg:170.16ms\nstep:5543/10000 train_loss:3.8178 train_time:941469ms step_avg:170.16ms\nstep:5544/10000 train_loss:3.9733 train_time:941641ms step_avg:170.16ms\nstep:5545/10000 train_loss:3.6343 train_time:941807ms step_avg:170.15ms\nstep:5546/10000 train_loss:3.8773 train_time:941978ms step_avg:170.15ms\nstep:5547/10000 train_loss:4.1573 train_time:942146ms step_avg:170.15ms\nstep:5548/10000 train_loss:4.0391 train_time:942316ms step_avg:170.15ms\nstep:5549/10000 train_loss:3.8224 train_time:942491ms step_avg:170.16ms\nstep:5550/10000 train_loss:3.8508 train_time:942663ms step_avg:170.16ms\nstep:5551/10000 train_loss:3.8757 train_time:942832ms step_avg:170.16ms\nstep:5552/10000 train_loss:3.9420 train_time:942998ms step_avg:170.15ms\nstep:5553/10000 train_loss:3.9051 train_time:943169ms step_avg:170.15ms\nstep:5554/10000 train_loss:3.8003 train_time:943336ms step_avg:170.15ms\nstep:5555/10000 train_loss:3.7572 train_time:943507ms step_avg:170.15ms\nstep:5556/10000 train_loss:3.9687 train_time:943676ms step_avg:170.15ms\nstep:5557/10000 train_loss:3.9771 train_time:943852ms step_avg:170.16ms\nstep:5558/10000 train_loss:3.9376 train_time:944018ms step_avg:170.15ms\nstep:5559/10000 train_loss:3.9742 train_time:944191ms step_avg:170.16ms\nstep:5560/10000 train_loss:3.7747 train_time:944362ms step_avg:170.16ms\nstep:5561/10000 train_loss:3.9487 train_time:944531ms step_avg:170.16ms\nstep:5562/10000 train_loss:3.9675 train_time:944702ms step_avg:170.16ms\nstep:5563/10000 train_loss:3.8535 train_time:944870ms step_avg:170.15ms\nstep:5564/10000 train_loss:3.9185 train_time:945042ms step_avg:170.16ms\nstep:5565/10000 train_loss:3.8138 train_time:945210ms step_avg:170.15ms\nstep:5566/10000 train_loss:3.9943 train_time:945380ms step_avg:170.15ms\nstep:5567/10000 train_loss:3.9246 train_time:945549ms step_avg:170.15ms\nstep:5568/10000 train_loss:3.8807 train_time:945721ms step_avg:170.15ms\nstep:5569/10000 train_loss:3.8205 train_time:945891ms step_avg:170.15ms\nstep:5570/10000 train_loss:4.0649 train_time:946063ms step_avg:170.16ms\nstep:5571/10000 train_loss:3.9857 train_time:946231ms step_avg:170.15ms\nstep:5572/10000 train_loss:4.1686 train_time:946401ms step_avg:170.15ms\nstep:5573/10000 train_loss:3.8750 train_time:946570ms step_avg:170.15ms\nstep:5574/10000 train_loss:3.9503 train_time:946739ms step_avg:170.15ms\nstep:5575/10000 train_loss:3.8252 train_time:946910ms step_avg:170.15ms\nstep:5576/10000 train_loss:3.8733 train_time:947083ms step_avg:170.15ms\nstep:5577/10000 train_loss:4.0234 train_time:947256ms step_avg:170.16ms\nstep:5578/10000 train_loss:3.9779 train_time:947422ms step_avg:170.15ms\nstep:5579/10000 train_loss:3.8812 train_time:947591ms step_avg:170.15ms\nstep:5580/10000 train_loss:3.8628 train_time:947762ms step_avg:170.15ms\nstep:5581/10000 train_loss:3.8519 train_time:947931ms step_avg:170.15ms\nstep:5582/10000 train_loss:3.9430 train_time:948100ms step_avg:170.15ms\nstep:5583/10000 train_loss:3.9590 train_time:948272ms step_avg:170.15ms\nstep:5584/10000 train_loss:3.8565 train_time:948442ms step_avg:170.15ms\nstep:5585/10000 train_loss:3.9513 train_time:948611ms step_avg:170.15ms\nstep:5586/10000 train_loss:3.7403 train_time:948782ms step_avg:170.15ms\nstep:5587/10000 train_loss:3.8891 train_time:948949ms step_avg:170.15ms\nstep:5588/10000 train_loss:4.1159 train_time:949121ms step_avg:170.15ms\nstep:5589/10000 train_loss:4.0565 train_time:949291ms step_avg:170.15ms\nstep:5590/10000 train_loss:4.0101 train_time:949461ms step_avg:170.15ms\nstep:5591/10000 train_loss:3.8623 train_time:949629ms step_avg:170.15ms\nstep:5592/10000 train_loss:4.0156 train_time:949800ms step_avg:170.15ms\nstep:5593/10000 train_loss:3.9370 train_time:949970ms step_avg:170.15ms\nstep:5594/10000 train_loss:3.8897 train_time:950141ms step_avg:170.15ms\nstep:5595/10000 train_loss:4.0277 train_time:950309ms step_avg:170.15ms\nstep:5596/10000 train_loss:3.9997 train_time:950479ms step_avg:170.15ms\nstep:5597/10000 train_loss:3.8961 train_time:950650ms step_avg:170.15ms\nstep:5598/10000 train_loss:3.8036 train_time:950821ms step_avg:170.15ms\nstep:5599/10000 train_loss:3.8984 train_time:950989ms step_avg:170.15ms\nstep:5600/10000 train_loss:3.8573 train_time:951162ms step_avg:170.15ms\nstep:5601/10000 train_loss:3.9794 train_time:951332ms step_avg:170.15ms\nstep:5602/10000 train_loss:3.7353 train_time:951504ms step_avg:170.15ms\nstep:5603/10000 train_loss:3.9729 train_time:951671ms step_avg:170.15ms\nstep:5604/10000 train_loss:3.8632 train_time:951841ms step_avg:170.15ms\nstep:5605/10000 train_loss:4.0023 train_time:952014ms step_avg:170.15ms\nstep:5606/10000 train_loss:3.7312 train_time:952186ms step_avg:170.15ms\nstep:5607/10000 train_loss:3.9300 train_time:952354ms step_avg:170.15ms\nstep:5608/10000 train_loss:3.9294 train_time:952525ms step_avg:170.15ms\nstep:5609/10000 train_loss:4.0442 train_time:952694ms step_avg:170.15ms\nstep:5610/10000 train_loss:4.0894 train_time:952866ms step_avg:170.15ms\nstep:5611/10000 train_loss:3.6836 train_time:953035ms step_avg:170.15ms\nstep:5612/10000 train_loss:3.8795 train_time:953207ms step_avg:170.15ms\nstep:5613/10000 train_loss:3.9267 train_time:953378ms step_avg:170.16ms\nstep:5614/10000 train_loss:4.1189 train_time:953550ms step_avg:170.16ms\nstep:5615/10000 train_loss:3.8974 train_time:953721ms step_avg:170.16ms\nstep:5616/10000 train_loss:3.8935 train_time:953891ms step_avg:170.16ms\nstep:5617/10000 train_loss:3.7215 train_time:954062ms step_avg:170.16ms\nstep:5618/10000 train_loss:3.9702 train_time:954229ms step_avg:170.16ms\nstep:5619/10000 train_loss:3.9644 train_time:954400ms step_avg:170.16ms\nstep:5620/10000 train_loss:3.8801 train_time:954569ms step_avg:170.15ms\nstep:5621/10000 train_loss:3.8731 train_time:954741ms step_avg:170.16ms\nstep:5622/10000 train_loss:3.8772 train_time:954910ms step_avg:170.15ms\nstep:5623/10000 train_loss:3.8990 train_time:955083ms step_avg:170.16ms\nstep:5624/10000 train_loss:4.0959 train_time:955252ms step_avg:170.16ms\nstep:5625/10000 train_loss:3.9372 train_time:955424ms step_avg:170.16ms\nstep:5626/10000 train_loss:3.9694 train_time:955595ms step_avg:170.16ms\nstep:5627/10000 train_loss:3.9250 train_time:955768ms step_avg:170.16ms\nstep:5628/10000 train_loss:3.8376 train_time:955936ms step_avg:170.16ms\nstep:5629/10000 train_loss:3.8659 train_time:956106ms step_avg:170.16ms\nstep:5630/10000 train_loss:3.8391 train_time:956274ms step_avg:170.16ms\nstep:5631/10000 train_loss:3.8577 train_time:956443ms step_avg:170.16ms\nstep:5632/10000 train_loss:3.9074 train_time:956613ms step_avg:170.16ms\nstep:5633/10000 train_loss:3.9123 train_time:956787ms step_avg:170.16ms\nstep:5634/10000 train_loss:3.7117 train_time:956957ms step_avg:170.16ms\nstep:5635/10000 train_loss:3.9345 train_time:957126ms step_avg:170.16ms\nstep:5636/10000 train_loss:3.9127 train_time:957296ms step_avg:170.16ms\nstep:5637/10000 train_loss:4.0896 train_time:957466ms step_avg:170.16ms\nstep:5638/10000 train_loss:3.8473 train_time:957637ms step_avg:170.16ms\nstep:5639/10000 train_loss:3.8288 train_time:957812ms step_avg:170.16ms\nstep:5640/10000 train_loss:3.7826 train_time:957984ms step_avg:170.16ms\nstep:5641/10000 train_loss:3.9978 train_time:958153ms step_avg:170.16ms\nstep:5642/10000 train_loss:3.9307 train_time:958325ms step_avg:170.16ms\nstep:5643/10000 train_loss:3.8865 train_time:958492ms step_avg:170.16ms\nstep:5644/10000 train_loss:3.8580 train_time:958660ms step_avg:170.16ms\nstep:5645/10000 train_loss:3.9806 train_time:958828ms step_avg:170.16ms\nstep:5646/10000 train_loss:4.2388 train_time:959000ms step_avg:170.16ms\nstep:5647/10000 train_loss:3.8782 train_time:959169ms step_avg:170.16ms\nstep:5648/10000 train_loss:3.8492 train_time:959340ms step_avg:170.16ms\nstep:5649/10000 train_loss:3.9227 train_time:959511ms step_avg:170.16ms\nstep:5650/10000 train_loss:4.0446 train_time:959683ms step_avg:170.16ms\nstep:5651/10000 train_loss:3.9015 train_time:959851ms step_avg:170.16ms\nstep:5652/10000 train_loss:3.8483 train_time:960022ms step_avg:170.16ms\nstep:5653/10000 train_loss:3.9274 train_time:960192ms step_avg:170.16ms\nstep:5654/10000 train_loss:4.0569 train_time:960362ms step_avg:170.16ms\nstep:5655/10000 train_loss:3.8834 train_time:960530ms step_avg:170.16ms\nstep:5656/10000 train_loss:3.9629 train_time:960701ms step_avg:170.16ms\nstep:5657/10000 train_loss:3.9847 train_time:960870ms step_avg:170.16ms\nstep:5658/10000 train_loss:4.0503 train_time:961040ms step_avg:170.16ms\nstep:5659/10000 train_loss:3.8736 train_time:961210ms step_avg:170.16ms\nstep:5660/10000 train_loss:3.8565 train_time:961381ms step_avg:170.16ms\nstep:5661/10000 train_loss:3.9217 train_time:961551ms step_avg:170.16ms\nstep:5662/10000 train_loss:3.8524 train_time:961722ms step_avg:170.16ms\nstep:5663/10000 train_loss:3.9789 train_time:961889ms step_avg:170.16ms\nstep:5664/10000 train_loss:3.9372 train_time:962060ms step_avg:170.16ms\nstep:5665/10000 train_loss:3.8999 train_time:962231ms step_avg:170.16ms\nstep:5666/10000 train_loss:3.9307 train_time:962403ms step_avg:170.16ms\nstep:5667/10000 train_loss:3.9550 train_time:962572ms step_avg:170.16ms\nstep:5668/10000 train_loss:3.9383 train_time:962746ms step_avg:170.16ms\nstep:5669/10000 train_loss:3.7566 train_time:962915ms step_avg:170.16ms\nstep:5670/10000 train_loss:3.7158 train_time:963084ms step_avg:170.16ms\nstep:5671/10000 train_loss:3.8878 train_time:963253ms step_avg:170.16ms\nstep:5672/10000 train_loss:4.0004 train_time:963422ms step_avg:170.16ms\nstep:5673/10000 train_loss:3.9353 train_time:963591ms step_avg:170.16ms\nstep:5674/10000 train_loss:3.3646 train_time:963764ms step_avg:170.16ms\nstep:5675/10000 train_loss:3.9278 train_time:963931ms step_avg:170.16ms\nstep:5676/10000 train_loss:3.8053 train_time:964101ms step_avg:170.16ms\nstep:5677/10000 train_loss:3.9722 train_time:964272ms step_avg:170.16ms\nstep:5678/10000 train_loss:3.9293 train_time:964440ms step_avg:170.16ms\nstep:5679/10000 train_loss:3.9102 train_time:964610ms step_avg:170.16ms\nstep:5680/10000 train_loss:3.8665 train_time:964784ms step_avg:170.16ms\nstep:5681/10000 train_loss:4.1041 train_time:964954ms step_avg:170.16ms\nstep:5682/10000 train_loss:3.9803 train_time:965124ms step_avg:170.16ms\nstep:5683/10000 train_loss:3.9898 train_time:965291ms step_avg:170.16ms\nstep:5684/10000 train_loss:3.9542 train_time:965461ms step_avg:170.16ms\nstep:5685/10000 train_loss:3.9833 train_time:965631ms step_avg:170.16ms\nstep:5686/10000 train_loss:3.8827 train_time:965804ms step_avg:170.16ms\nstep:5687/10000 train_loss:3.8191 train_time:965975ms step_avg:170.16ms\nstep:5688/10000 train_loss:4.0236 train_time:966148ms step_avg:170.16ms\nstep:5689/10000 train_loss:4.1257 train_time:966319ms step_avg:170.16ms\nstep:5690/10000 train_loss:3.9475 train_time:966486ms step_avg:170.16ms\nstep:5691/10000 train_loss:3.9494 train_time:966657ms step_avg:170.16ms\nstep:5692/10000 train_loss:3.9669 train_time:966829ms step_avg:170.16ms\nstep:5693/10000 train_loss:3.8322 train_time:967002ms step_avg:170.16ms\nstep:5694/10000 train_loss:3.9443 train_time:967171ms step_avg:170.16ms\nstep:5695/10000 train_loss:3.9057 train_time:967342ms step_avg:170.16ms\nstep:5696/10000 train_loss:3.9293 train_time:967509ms step_avg:170.16ms\nstep:5697/10000 train_loss:3.8613 train_time:967680ms step_avg:170.16ms\nstep:5698/10000 train_loss:3.7891 train_time:967850ms step_avg:170.16ms\nstep:5699/10000 train_loss:3.8968 train_time:968023ms step_avg:170.16ms\nstep:5700/10000 train_loss:3.9499 train_time:968192ms step_avg:170.16ms\nstep:5701/10000 train_loss:3.9431 train_time:968364ms step_avg:170.16ms\nstep:5702/10000 train_loss:3.8075 train_time:968532ms step_avg:170.16ms\nstep:5703/10000 train_loss:4.1484 train_time:968703ms step_avg:170.16ms\nstep:5704/10000 train_loss:3.9376 train_time:968871ms step_avg:170.16ms\nstep:5705/10000 train_loss:3.9183 train_time:969042ms step_avg:170.16ms\nstep:5706/10000 train_loss:4.0309 train_time:969210ms step_avg:170.16ms\nstep:5707/10000 train_loss:3.8038 train_time:969383ms step_avg:170.16ms\nstep:5708/10000 train_loss:3.9081 train_time:969553ms step_avg:170.16ms\nstep:5709/10000 train_loss:3.8183 train_time:969723ms step_avg:170.16ms\nstep:5710/10000 train_loss:3.7488 train_time:969893ms step_avg:170.16ms\nstep:5711/10000 train_loss:4.0817 train_time:970064ms step_avg:170.16ms\nstep:5712/10000 train_loss:3.7747 train_time:970232ms step_avg:170.16ms\nstep:5713/10000 train_loss:3.9680 train_time:970404ms step_avg:170.16ms\nstep:5714/10000 train_loss:3.9419 train_time:970574ms step_avg:170.16ms\nstep:5715/10000 train_loss:3.8750 train_time:970744ms step_avg:170.16ms\nstep:5716/10000 train_loss:3.9130 train_time:970914ms step_avg:170.16ms\nstep:5717/10000 train_loss:4.0422 train_time:971085ms step_avg:170.16ms\nstep:5718/10000 train_loss:3.9944 train_time:971255ms step_avg:170.16ms\nstep:5719/10000 train_loss:3.8067 train_time:971427ms step_avg:170.16ms\nstep:5720/10000 train_loss:3.8121 train_time:971597ms step_avg:170.16ms\nstep:5721/10000 train_loss:3.8805 train_time:971766ms step_avg:170.16ms\nstep:5722/10000 train_loss:3.8992 train_time:971935ms step_avg:170.16ms\nstep:5723/10000 train_loss:3.9334 train_time:972105ms step_avg:170.16ms\nstep:5724/10000 train_loss:3.9459 train_time:972273ms step_avg:170.16ms\nstep:5725/10000 train_loss:4.0194 train_time:972445ms step_avg:170.16ms\nstep:5726/10000 train_loss:3.8942 train_time:972614ms step_avg:170.16ms\nstep:5727/10000 train_loss:3.8173 train_time:972784ms step_avg:170.16ms\nstep:5728/10000 train_loss:3.9267 train_time:972951ms step_avg:170.16ms\nstep:5729/10000 train_loss:3.9209 train_time:973122ms step_avg:170.16ms\nstep:5730/10000 train_loss:3.8267 train_time:973291ms step_avg:170.16ms\nstep:5731/10000 train_loss:3.8909 train_time:973462ms step_avg:170.16ms\nstep:5732/10000 train_loss:3.8431 train_time:973634ms step_avg:170.16ms\nstep:5733/10000 train_loss:3.8388 train_time:973807ms step_avg:170.16ms\nstep:5734/10000 train_loss:3.9509 train_time:973978ms step_avg:170.16ms\nstep:5735/10000 train_loss:3.8362 train_time:974148ms step_avg:170.16ms\nstep:5736/10000 train_loss:3.8785 train_time:974318ms step_avg:170.16ms\nstep:5737/10000 train_loss:3.8547 train_time:974485ms step_avg:170.16ms\nstep:5738/10000 train_loss:3.8526 train_time:974654ms step_avg:170.16ms\nstep:5739/10000 train_loss:3.9072 train_time:974826ms step_avg:170.16ms\nstep:5740/10000 train_loss:3.9425 train_time:974993ms step_avg:170.16ms\nstep:5741/10000 train_loss:3.8659 train_time:975163ms step_avg:170.16ms\nstep:5742/10000 train_loss:4.3390 train_time:975333ms step_avg:170.16ms\nstep:5743/10000 train_loss:3.8463 train_time:975504ms step_avg:170.16ms\nstep:5744/10000 train_loss:3.7665 train_time:975673ms step_avg:170.16ms\nstep:5745/10000 train_loss:3.9585 train_time:975846ms step_avg:170.16ms\nstep:5746/10000 train_loss:3.7312 train_time:976016ms step_avg:170.16ms\nstep:5747/10000 train_loss:3.8609 train_time:976184ms step_avg:170.16ms\nstep:5748/10000 train_loss:3.9093 train_time:976353ms step_avg:170.16ms\nstep:5749/10000 train_loss:3.8742 train_time:976525ms step_avg:170.16ms\nstep:5750/10000 train_loss:3.8364 train_time:976693ms step_avg:170.16ms\nstep:5751/10000 train_loss:3.7618 train_time:976865ms step_avg:170.16ms\nstep:5752/10000 train_loss:3.7654 train_time:977035ms step_avg:170.16ms\nstep:5753/10000 train_loss:3.8697 train_time:977208ms step_avg:170.16ms\nstep:5754/10000 train_loss:3.8476 train_time:977377ms step_avg:170.16ms\nstep:5755/10000 train_loss:3.6969 train_time:977544ms step_avg:170.16ms\nstep:5756/10000 train_loss:3.7885 train_time:977716ms step_avg:170.16ms\nstep:5757/10000 train_loss:3.7221 train_time:977890ms step_avg:170.16ms\nstep:5758/10000 train_loss:3.8679 train_time:978059ms step_avg:170.16ms\nstep:5759/10000 train_loss:3.8070 train_time:978228ms step_avg:170.16ms\nstep:5760/10000 train_loss:3.9931 train_time:978398ms step_avg:170.16ms\nstep:5761/10000 train_loss:3.9303 train_time:978565ms step_avg:170.16ms\nstep:5762/10000 train_loss:3.8141 train_time:978734ms step_avg:170.16ms\nstep:5763/10000 train_loss:3.9226 train_time:978908ms step_avg:170.16ms\nstep:5764/10000 train_loss:3.9331 train_time:979080ms step_avg:170.16ms\nstep:5765/10000 train_loss:3.9093 train_time:979249ms step_avg:170.16ms\nstep:5766/10000 train_loss:3.9653 train_time:979421ms step_avg:170.16ms\nstep:5767/10000 train_loss:3.8982 train_time:979592ms step_avg:170.16ms\nstep:5768/10000 train_loss:3.8984 train_time:979763ms step_avg:170.16ms\nstep:5769/10000 train_loss:3.9726 train_time:979933ms step_avg:170.16ms\nstep:5770/10000 train_loss:3.8169 train_time:980106ms step_avg:170.16ms\nstep:5771/10000 train_loss:3.8467 train_time:980274ms step_avg:170.16ms\nstep:5772/10000 train_loss:3.7509 train_time:980444ms step_avg:170.16ms\nstep:5773/10000 train_loss:3.7644 train_time:980612ms step_avg:170.16ms\nstep:5774/10000 train_loss:3.9539 train_time:980784ms step_avg:170.16ms\nstep:5775/10000 train_loss:3.9726 train_time:980952ms step_avg:170.16ms\nstep:5776/10000 train_loss:3.9384 train_time:981124ms step_avg:170.16ms\nstep:5777/10000 train_loss:3.8042 train_time:981294ms step_avg:170.16ms\nstep:5778/10000 train_loss:3.8857 train_time:981467ms step_avg:170.16ms\nstep:5779/10000 train_loss:3.8655 train_time:981635ms step_avg:170.16ms\nstep:5780/10000 train_loss:3.9994 train_time:981807ms step_avg:170.16ms\nstep:5781/10000 train_loss:4.0182 train_time:981977ms step_avg:170.16ms\nstep:5782/10000 train_loss:3.8655 train_time:982149ms step_avg:170.16ms\nstep:5783/10000 train_loss:3.7516 train_time:982319ms step_avg:170.16ms\nstep:5784/10000 train_loss:4.0217 train_time:982490ms step_avg:170.16ms\nstep:5785/10000 train_loss:3.9577 train_time:982659ms step_avg:170.16ms\nstep:5786/10000 train_loss:4.0718 train_time:982828ms step_avg:170.16ms\nstep:5787/10000 train_loss:3.9539 train_time:982998ms step_avg:170.16ms\nstep:5788/10000 train_loss:4.0343 train_time:983167ms step_avg:170.16ms\nstep:5789/10000 train_loss:3.8317 train_time:983338ms step_avg:170.16ms\nstep:5790/10000 train_loss:3.7919 train_time:983510ms step_avg:170.16ms\nstep:5791/10000 train_loss:3.9532 train_time:983681ms step_avg:170.16ms\nstep:5792/10000 train_loss:3.9129 train_time:983851ms step_avg:170.16ms\nstep:5793/10000 train_loss:3.9265 train_time:984021ms step_avg:170.16ms\nstep:5794/10000 train_loss:3.6913 train_time:984189ms step_avg:170.16ms\nstep:5795/10000 train_loss:3.9376 train_time:984367ms step_avg:170.16ms\nstep:5796/10000 train_loss:3.8074 train_time:984535ms step_avg:170.16ms\nstep:5797/10000 train_loss:3.9077 train_time:984709ms step_avg:170.16ms\nstep:5798/10000 train_loss:4.0375 train_time:984876ms step_avg:170.16ms\nstep:5799/10000 train_loss:3.9318 train_time:985047ms step_avg:170.16ms\nstep:5800/10000 train_loss:3.8493 train_time:985215ms step_avg:170.16ms\nstep:5801/10000 train_loss:3.9243 train_time:985387ms step_avg:170.16ms\nstep:5802/10000 train_loss:3.8951 train_time:985556ms step_avg:170.16ms\nstep:5803/10000 train_loss:3.6954 train_time:985727ms step_avg:170.16ms\nstep:5804/10000 train_loss:3.8875 train_time:985897ms step_avg:170.16ms\nstep:5805/10000 train_loss:3.9439 train_time:986069ms step_avg:170.16ms\nstep:5806/10000 train_loss:3.8765 train_time:986240ms step_avg:170.16ms\nstep:5807/10000 train_loss:3.9680 train_time:986409ms step_avg:170.16ms\nstep:5808/10000 train_loss:3.9947 train_time:986578ms step_avg:170.16ms\nstep:5809/10000 train_loss:3.8815 train_time:986745ms step_avg:170.16ms\nstep:5810/10000 train_loss:3.7887 train_time:986915ms step_avg:170.16ms\nstep:5811/10000 train_loss:3.6958 train_time:987085ms step_avg:170.16ms\nstep:5812/10000 train_loss:4.0097 train_time:987254ms step_avg:170.16ms\nstep:5813/10000 train_loss:3.9045 train_time:987425ms step_avg:170.16ms\nstep:5814/10000 train_loss:3.7310 train_time:987594ms step_avg:170.16ms\nstep:5815/10000 train_loss:4.0278 train_time:987767ms step_avg:170.16ms\nstep:5816/10000 train_loss:3.8208 train_time:987937ms step_avg:170.16ms\nstep:5817/10000 train_loss:3.9654 train_time:988108ms step_avg:170.16ms\nstep:5818/10000 train_loss:3.8627 train_time:988279ms step_avg:170.16ms\nstep:5819/10000 train_loss:3.7084 train_time:988451ms step_avg:170.16ms\nstep:5820/10000 train_loss:3.9154 train_time:988621ms step_avg:170.16ms\nstep:5821/10000 train_loss:3.8646 train_time:988792ms step_avg:170.16ms\nstep:5822/10000 train_loss:3.8585 train_time:988966ms step_avg:170.16ms\nstep:5823/10000 train_loss:3.9647 train_time:989136ms step_avg:170.16ms\nstep:5824/10000 train_loss:3.8194 train_time:989308ms step_avg:170.16ms\nstep:5825/10000 train_loss:3.8907 train_time:989476ms step_avg:170.16ms\nstep:5826/10000 train_loss:3.7447 train_time:989643ms step_avg:170.16ms\nstep:5827/10000 train_loss:3.8725 train_time:989813ms step_avg:170.16ms\nstep:5828/10000 train_loss:3.8632 train_time:989986ms step_avg:170.16ms\nstep:5829/10000 train_loss:3.9997 train_time:990156ms step_avg:170.16ms\nstep:5830/10000 train_loss:3.9734 train_time:990330ms step_avg:170.16ms\nstep:5831/10000 train_loss:3.9292 train_time:990502ms step_avg:170.16ms\nstep:5832/10000 train_loss:3.9168 train_time:990671ms step_avg:170.16ms\nstep:5833/10000 train_loss:3.9901 train_time:990841ms step_avg:170.16ms\nstep:5834/10000 train_loss:3.9742 train_time:991012ms step_avg:170.16ms\nstep:5835/10000 train_loss:3.7912 train_time:991182ms step_avg:170.16ms\nstep:5836/10000 train_loss:3.8186 train_time:991352ms step_avg:170.16ms\nstep:5837/10000 train_loss:3.8311 train_time:991524ms step_avg:170.16ms\nstep:5838/10000 train_loss:3.8383 train_time:991694ms step_avg:170.16ms\nstep:5839/10000 train_loss:3.9572 train_time:991865ms step_avg:170.16ms\nstep:5840/10000 train_loss:3.7513 train_time:992034ms step_avg:170.16ms\nstep:5841/10000 train_loss:3.8289 train_time:992205ms step_avg:170.16ms\nstep:5842/10000 train_loss:3.9117 train_time:992374ms step_avg:170.16ms\nstep:5843/10000 train_loss:3.9990 train_time:992546ms step_avg:170.16ms\nstep:5844/10000 train_loss:3.9643 train_time:992714ms step_avg:170.16ms\nstep:5845/10000 train_loss:4.3010 train_time:992888ms step_avg:170.16ms\nstep:5846/10000 train_loss:3.8360 train_time:993064ms step_avg:170.16ms\nstep:5847/10000 train_loss:3.8601 train_time:993225ms step_avg:170.16ms\nstep:5848/10000 train_loss:3.8727 train_time:993396ms step_avg:170.16ms\nstep:5849/10000 train_loss:3.8247 train_time:993569ms step_avg:170.16ms\nstep:5850/10000 train_loss:3.9016 train_time:993737ms step_avg:170.16ms\nstep:5851/10000 train_loss:3.9016 train_time:993908ms step_avg:170.16ms\nstep:5852/10000 train_loss:3.9041 train_time:994076ms step_avg:170.16ms\nstep:5853/10000 train_loss:3.7822 train_time:994246ms step_avg:170.16ms\nstep:5854/10000 train_loss:3.8502 train_time:994417ms step_avg:170.16ms\nstep:5855/10000 train_loss:3.8220 train_time:994590ms step_avg:170.16ms\nstep:5856/10000 train_loss:4.0451 train_time:994760ms step_avg:170.16ms\nstep:5857/10000 train_loss:3.8276 train_time:994937ms step_avg:170.16ms\nstep:5858/10000 train_loss:3.8429 train_time:995099ms step_avg:170.16ms\nstep:5859/10000 train_loss:3.9185 train_time:995269ms step_avg:170.16ms\nstep:5860/10000 train_loss:3.8620 train_time:995440ms step_avg:170.16ms\nstep:5861/10000 train_loss:3.7810 train_time:995610ms step_avg:170.16ms\nstep:5862/10000 train_loss:3.7827 train_time:995783ms step_avg:170.16ms\nstep:5863/10000 train_loss:3.7857 train_time:995954ms step_avg:170.16ms\nstep:5864/10000 train_loss:4.0052 train_time:996125ms step_avg:170.16ms\nstep:5865/10000 train_loss:3.8728 train_time:996294ms step_avg:170.16ms\nstep:5866/10000 train_loss:3.9728 train_time:996464ms step_avg:170.16ms\nstep:5867/10000 train_loss:4.0091 train_time:996635ms step_avg:170.16ms\nstep:5868/10000 train_loss:3.9117 train_time:996806ms step_avg:170.16ms\nstep:5869/10000 train_loss:3.9258 train_time:996976ms step_avg:170.16ms\nstep:5870/10000 train_loss:3.9404 train_time:997148ms step_avg:170.16ms\nstep:5871/10000 train_loss:3.8841 train_time:997318ms step_avg:170.16ms\nstep:5872/10000 train_loss:3.9219 train_time:997487ms step_avg:170.16ms\nstep:5873/10000 train_loss:3.9952 train_time:997658ms step_avg:170.16ms\nstep:5874/10000 train_loss:3.8310 train_time:997829ms step_avg:170.16ms\nstep:5875/10000 train_loss:3.8701 train_time:997999ms step_avg:170.16ms\nstep:5876/10000 train_loss:3.8524 train_time:998171ms step_avg:170.16ms\nstep:5877/10000 train_loss:3.9092 train_time:998341ms step_avg:170.16ms\nstep:5878/10000 train_loss:3.8580 train_time:998510ms step_avg:170.16ms\nstep:5879/10000 train_loss:3.9349 train_time:998680ms step_avg:170.16ms\nstep:5880/10000 train_loss:4.0266 train_time:998851ms step_avg:170.16ms\nstep:5881/10000 train_loss:3.8383 train_time:999021ms step_avg:170.16ms\nstep:5882/10000 train_loss:3.8992 train_time:999189ms step_avg:170.16ms\nstep:5883/10000 train_loss:3.8259 train_time:999361ms step_avg:170.16ms\nstep:5884/10000 train_loss:3.8691 train_time:999531ms step_avg:170.16ms\nstep:5885/10000 train_loss:3.9309 train_time:999701ms step_avg:170.16ms\nstep:5886/10000 train_loss:3.7585 train_time:999870ms step_avg:170.16ms\nstep:5887/10000 train_loss:3.8742 train_time:1000043ms step_avg:170.16ms\nstep:5888/10000 train_loss:3.8233 train_time:1000213ms step_avg:170.16ms\nstep:5889/10000 train_loss:3.8300 train_time:1000384ms step_avg:170.16ms\nstep:5890/10000 train_loss:3.7736 train_time:1000552ms step_avg:170.16ms\nstep:5891/10000 train_loss:3.9063 train_time:1000721ms step_avg:170.16ms\nstep:5892/10000 train_loss:3.8891 train_time:1000890ms step_avg:170.16ms\nstep:5893/10000 train_loss:3.8458 train_time:1001061ms step_avg:170.16ms\nstep:5894/10000 train_loss:3.9424 train_time:1001230ms step_avg:170.16ms\nstep:5895/10000 train_loss:3.6573 train_time:1001400ms step_avg:170.16ms\nstep:5896/10000 train_loss:3.8189 train_time:1001571ms step_avg:170.16ms\nstep:5897/10000 train_loss:3.9070 train_time:1001742ms step_avg:170.16ms\nstep:5898/10000 train_loss:3.8530 train_time:1001913ms step_avg:170.16ms\nstep:5899/10000 train_loss:3.8545 train_time:1002086ms step_avg:170.16ms\nstep:5900/10000 train_loss:3.9494 train_time:1002255ms step_avg:170.16ms\nstep:5901/10000 train_loss:3.8514 train_time:1002424ms step_avg:170.16ms\nstep:5902/10000 train_loss:3.9769 train_time:1002592ms step_avg:170.16ms\nstep:5903/10000 train_loss:3.9417 train_time:1002764ms step_avg:170.16ms\nstep:5904/10000 train_loss:3.9861 train_time:1002934ms step_avg:170.16ms\nstep:5905/10000 train_loss:3.9048 train_time:1003106ms step_avg:170.16ms\nstep:5906/10000 train_loss:3.8108 train_time:1003276ms step_avg:170.16ms\nstep:5907/10000 train_loss:3.8355 train_time:1003448ms step_avg:170.16ms\nstep:5908/10000 train_loss:3.8988 train_time:1003617ms step_avg:170.16ms\nstep:5909/10000 train_loss:3.8417 train_time:1003787ms step_avg:170.16ms\nstep:5910/10000 train_loss:3.9033 train_time:1003958ms step_avg:170.16ms\nstep:5911/10000 train_loss:3.8582 train_time:1004131ms step_avg:170.16ms\nstep:5912/10000 train_loss:3.7009 train_time:1004301ms step_avg:170.16ms\nstep:5913/10000 train_loss:3.7028 train_time:1004471ms step_avg:170.16ms\nstep:5914/10000 train_loss:4.0240 train_time:1004642ms step_avg:170.16ms\nstep:5915/10000 train_loss:3.8440 train_time:1004810ms step_avg:170.16ms\nstep:5916/10000 train_loss:3.8195 train_time:1004980ms step_avg:170.16ms\nstep:5917/10000 train_loss:3.7937 train_time:1005149ms step_avg:170.16ms\nstep:5918/10000 train_loss:3.8218 train_time:1005319ms step_avg:170.16ms\nstep:5919/10000 train_loss:4.0035 train_time:1005489ms step_avg:170.16ms\nstep:5920/10000 train_loss:3.7946 train_time:1005661ms step_avg:170.16ms\nstep:5921/10000 train_loss:3.6508 train_time:1005829ms step_avg:170.16ms\nstep:5922/10000 train_loss:3.7977 train_time:1005998ms step_avg:170.16ms\nstep:5923/10000 train_loss:3.7754 train_time:1006171ms step_avg:170.16ms\nstep:5924/10000 train_loss:3.9133 train_time:1006343ms step_avg:170.16ms\nstep:5925/10000 train_loss:3.8926 train_time:1006513ms step_avg:170.16ms\nstep:5926/10000 train_loss:3.8522 train_time:1006684ms step_avg:170.16ms\nstep:5927/10000 train_loss:3.9617 train_time:1006853ms step_avg:170.16ms\nstep:5928/10000 train_loss:3.9719 train_time:1007025ms step_avg:170.16ms\nstep:5929/10000 train_loss:3.8184 train_time:1007194ms step_avg:170.16ms\nstep:5930/10000 train_loss:3.8487 train_time:1007366ms step_avg:170.16ms\nstep:5931/10000 train_loss:3.7910 train_time:1007536ms step_avg:170.16ms\nstep:5932/10000 train_loss:3.8460 train_time:1007710ms step_avg:170.16ms\nstep:5933/10000 train_loss:3.8648 train_time:1007879ms step_avg:170.16ms\nstep:5934/10000 train_loss:4.0240 train_time:1008048ms step_avg:170.16ms\nstep:5935/10000 train_loss:3.9680 train_time:1008220ms step_avg:170.16ms\nstep:5936/10000 train_loss:4.0172 train_time:1008390ms step_avg:170.16ms\nstep:5937/10000 train_loss:3.8511 train_time:1008563ms step_avg:170.16ms\nstep:5938/10000 train_loss:3.7391 train_time:1008735ms step_avg:170.16ms\nstep:5939/10000 train_loss:3.9033 train_time:1008905ms step_avg:170.16ms\nstep:5940/10000 train_loss:3.9598 train_time:1009073ms step_avg:170.16ms\nstep:5941/10000 train_loss:3.8098 train_time:1009245ms step_avg:170.16ms\nstep:5942/10000 train_loss:3.8763 train_time:1009413ms step_avg:170.16ms\nstep:5943/10000 train_loss:3.8895 train_time:1009585ms step_avg:170.16ms\nstep:5944/10000 train_loss:3.8557 train_time:1009754ms step_avg:170.16ms\nstep:5945/10000 train_loss:3.9409 train_time:1009922ms step_avg:170.16ms\nstep:5946/10000 train_loss:3.9596 train_time:1010091ms step_avg:170.16ms\nstep:5947/10000 train_loss:3.8009 train_time:1010261ms step_avg:170.16ms\nstep:5948/10000 train_loss:3.8424 train_time:1010431ms step_avg:170.16ms\nstep:5949/10000 train_loss:3.9661 train_time:1010604ms step_avg:170.16ms\nstep:5950/10000 train_loss:3.8614 train_time:1010777ms step_avg:170.16ms\nstep:5951/10000 train_loss:3.9108 train_time:1010947ms step_avg:170.16ms\nstep:5952/10000 train_loss:3.8140 train_time:1011118ms step_avg:170.16ms\nstep:5953/10000 train_loss:3.9029 train_time:1011286ms step_avg:170.16ms\nstep:5954/10000 train_loss:3.7530 train_time:1011455ms step_avg:170.16ms\nstep:5955/10000 train_loss:3.9693 train_time:1011627ms step_avg:170.16ms\nstep:5956/10000 train_loss:3.8194 train_time:1011800ms step_avg:170.16ms\nstep:5957/10000 train_loss:3.8751 train_time:1011970ms step_avg:170.16ms\nstep:5958/10000 train_loss:3.8767 train_time:1012139ms step_avg:170.16ms\nstep:5959/10000 train_loss:3.9238 train_time:1012309ms step_avg:170.16ms\nstep:5960/10000 train_loss:3.8691 train_time:1012478ms step_avg:170.16ms\nstep:5961/10000 train_loss:4.0417 train_time:1012649ms step_avg:170.16ms\nstep:5962/10000 train_loss:3.6831 train_time:1012820ms step_avg:170.16ms\nstep:5963/10000 train_loss:3.9078 train_time:1012988ms step_avg:170.16ms\nstep:5964/10000 train_loss:3.7811 train_time:1013160ms step_avg:170.16ms\nstep:5965/10000 train_loss:3.9353 train_time:1013329ms step_avg:170.16ms\nstep:5966/10000 train_loss:3.6067 train_time:1013497ms step_avg:170.16ms\nstep:5967/10000 train_loss:4.2999 train_time:1013669ms step_avg:170.16ms\nstep:5968/10000 train_loss:3.9198 train_time:1013842ms step_avg:170.16ms\nstep:5969/10000 train_loss:3.9008 train_time:1014009ms step_avg:170.16ms\nstep:5970/10000 train_loss:3.7206 train_time:1014179ms step_avg:170.16ms\nstep:5971/10000 train_loss:4.0421 train_time:1014348ms step_avg:170.16ms\nstep:5972/10000 train_loss:3.8383 train_time:1014517ms step_avg:170.16ms\nstep:5973/10000 train_loss:3.8090 train_time:1014685ms step_avg:170.16ms\nstep:5974/10000 train_loss:4.0739 train_time:1014857ms step_avg:170.16ms\nstep:5975/10000 train_loss:4.0708 train_time:1015027ms step_avg:170.16ms\nstep:5976/10000 train_loss:3.7824 train_time:1015198ms step_avg:170.16ms\nstep:5977/10000 train_loss:3.9829 train_time:1015367ms step_avg:170.16ms\nstep:5978/10000 train_loss:3.9343 train_time:1015539ms step_avg:170.16ms\nstep:5979/10000 train_loss:3.9266 train_time:1015708ms step_avg:170.16ms\nstep:5980/10000 train_loss:3.8164 train_time:1015879ms step_avg:170.16ms\nstep:5981/10000 train_loss:3.7176 train_time:1016050ms step_avg:170.16ms\nstep:5982/10000 train_loss:3.8406 train_time:1016219ms step_avg:170.16ms\nstep:5983/10000 train_loss:3.8983 train_time:1016389ms step_avg:170.16ms\nstep:5984/10000 train_loss:4.0162 train_time:1016561ms step_avg:170.16ms\nstep:5985/10000 train_loss:3.8580 train_time:1016729ms step_avg:170.16ms\nstep:5986/10000 train_loss:4.0560 train_time:1016901ms step_avg:170.16ms\nstep:5987/10000 train_loss:3.8714 train_time:1017070ms step_avg:170.16ms\nstep:5988/10000 train_loss:3.8767 train_time:1017242ms step_avg:170.16ms\nstep:5989/10000 train_loss:3.7781 train_time:1017412ms step_avg:170.16ms\nstep:5990/10000 train_loss:3.9075 train_time:1017584ms step_avg:170.16ms\nstep:5991/10000 train_loss:3.8773 train_time:1017755ms step_avg:170.16ms\nstep:5992/10000 train_loss:3.6257 train_time:1017931ms step_avg:170.17ms\nstep:5993/10000 train_loss:3.9827 train_time:1018100ms step_avg:170.17ms\nstep:5994/10000 train_loss:3.8662 train_time:1018267ms step_avg:170.16ms\nstep:5995/10000 train_loss:3.8070 train_time:1018437ms step_avg:170.16ms\nstep:5996/10000 train_loss:3.9150 train_time:1018605ms step_avg:170.16ms\nstep:5997/10000 train_loss:3.8116 train_time:1018774ms step_avg:170.16ms\nstep:5998/10000 train_loss:3.8295 train_time:1018944ms step_avg:170.16ms\nstep:5999/10000 train_loss:3.9348 train_time:1019114ms step_avg:170.16ms\nstep:6000/10000 train_loss:3.8773 train_time:1019286ms step_avg:170.16ms\nstep:6001/10000 train_loss:4.0139 train_time:1019455ms step_avg:170.16ms\nstep:6002/10000 train_loss:3.9178 train_time:1019623ms step_avg:170.16ms\nstep:6003/10000 train_loss:3.8895 train_time:1019793ms step_avg:170.16ms\nstep:6004/10000 train_loss:3.9161 train_time:1019962ms step_avg:170.16ms\nstep:6005/10000 train_loss:3.7127 train_time:1020131ms step_avg:170.16ms\nstep:6006/10000 train_loss:3.8742 train_time:1020303ms step_avg:170.16ms\nstep:6007/10000 train_loss:3.8244 train_time:1020473ms step_avg:170.16ms\nstep:6008/10000 train_loss:3.9037 train_time:1020642ms step_avg:170.16ms\nstep:6009/10000 train_loss:3.7914 train_time:1020812ms step_avg:170.16ms\nstep:6010/10000 train_loss:3.8456 train_time:1020985ms step_avg:170.16ms\nstep:6011/10000 train_loss:3.8640 train_time:1021153ms step_avg:170.16ms\nstep:6012/10000 train_loss:3.8313 train_time:1021328ms step_avg:170.16ms\nstep:6013/10000 train_loss:4.0105 train_time:1021497ms step_avg:170.16ms\nstep:6014/10000 train_loss:3.8011 train_time:1021666ms step_avg:170.16ms\nstep:6015/10000 train_loss:3.9307 train_time:1021834ms step_avg:170.16ms\nstep:6016/10000 train_loss:3.6611 train_time:1022005ms step_avg:170.16ms\nstep:6017/10000 train_loss:3.5415 train_time:1022176ms step_avg:170.16ms\nstep:6018/10000 train_loss:3.7774 train_time:1022346ms step_avg:170.16ms\nstep:6019/10000 train_loss:4.1310 train_time:1022518ms step_avg:170.16ms\nstep:6020/10000 train_loss:3.9242 train_time:1022689ms step_avg:170.16ms\nstep:6021/10000 train_loss:3.7719 train_time:1022860ms step_avg:170.16ms\nstep:6022/10000 train_loss:3.8709 train_time:1023031ms step_avg:170.16ms\nstep:6023/10000 train_loss:3.9025 train_time:1023204ms step_avg:170.17ms\nstep:6024/10000 train_loss:3.8174 train_time:1023375ms step_avg:170.17ms\nstep:6025/10000 train_loss:3.8846 train_time:1023546ms step_avg:170.17ms\nstep:6026/10000 train_loss:3.9144 train_time:1023714ms step_avg:170.17ms\nstep:6027/10000 train_loss:3.5454 train_time:1023881ms step_avg:170.16ms\nstep:6028/10000 train_loss:3.8314 train_time:1024051ms step_avg:170.16ms\nstep:6029/10000 train_loss:3.8269 train_time:1024224ms step_avg:170.17ms\nstep:6030/10000 train_loss:3.8708 train_time:1024395ms step_avg:170.17ms\nstep:6031/10000 train_loss:3.7540 train_time:1024569ms step_avg:170.17ms\nstep:6032/10000 train_loss:3.7935 train_time:1024739ms step_avg:170.17ms\nstep:6033/10000 train_loss:3.8409 train_time:1024907ms step_avg:170.17ms\nstep:6034/10000 train_loss:3.7958 train_time:1025078ms step_avg:170.17ms\nstep:6035/10000 train_loss:3.7678 train_time:1025247ms step_avg:170.17ms\nstep:6036/10000 train_loss:3.8725 train_time:1025418ms step_avg:170.17ms\nstep:6037/10000 train_loss:4.0468 train_time:1025587ms step_avg:170.17ms\nstep:6038/10000 train_loss:3.7914 train_time:1025758ms step_avg:170.17ms\nstep:6039/10000 train_loss:3.8078 train_time:1025925ms step_avg:170.16ms\nstep:6040/10000 train_loss:3.8101 train_time:1026099ms step_avg:170.17ms\nstep:6041/10000 train_loss:3.8023 train_time:1026268ms step_avg:170.17ms\nstep:6042/10000 train_loss:3.8296 train_time:1026436ms step_avg:170.17ms\nstep:6043/10000 train_loss:3.7851 train_time:1026606ms step_avg:170.17ms\nstep:6044/10000 train_loss:4.0975 train_time:1026779ms step_avg:170.17ms\nstep:6045/10000 train_loss:3.9434 train_time:1026950ms step_avg:170.17ms\nstep:6046/10000 train_loss:3.8469 train_time:1027124ms step_avg:170.17ms\nstep:6047/10000 train_loss:3.7915 train_time:1027292ms step_avg:170.17ms\nstep:6048/10000 train_loss:3.8324 train_time:1027463ms step_avg:170.17ms\nstep:6049/10000 train_loss:3.9892 train_time:1027630ms step_avg:170.17ms\nstep:6050/10000 train_loss:3.8955 train_time:1027799ms step_avg:170.17ms\nstep:6051/10000 train_loss:3.7928 train_time:1027970ms step_avg:170.17ms\nstep:6052/10000 train_loss:3.8522 train_time:1028143ms step_avg:170.17ms\nstep:6053/10000 train_loss:3.8192 train_time:1028310ms step_avg:170.17ms\nstep:6054/10000 train_loss:3.7881 train_time:1028483ms step_avg:170.17ms\nstep:6055/10000 train_loss:3.9480 train_time:1028650ms step_avg:170.17ms\nstep:6056/10000 train_loss:3.9478 train_time:1028821ms step_avg:170.17ms\nstep:6057/10000 train_loss:3.8390 train_time:1028993ms step_avg:170.17ms\nstep:6058/10000 train_loss:3.9390 train_time:1029161ms step_avg:170.17ms\nstep:6059/10000 train_loss:3.9070 train_time:1029330ms step_avg:170.17ms\nstep:6060/10000 train_loss:3.9909 train_time:1029500ms step_avg:170.17ms\nstep:6061/10000 train_loss:3.9500 train_time:1029670ms step_avg:170.17ms\nstep:6062/10000 train_loss:3.8219 train_time:1029844ms step_avg:170.17ms\nstep:6063/10000 train_loss:3.8110 train_time:1030014ms step_avg:170.17ms\nstep:6064/10000 train_loss:3.8579 train_time:1030185ms step_avg:170.17ms\nstep:6065/10000 train_loss:3.8716 train_time:1030354ms step_avg:170.17ms\nstep:6066/10000 train_loss:3.8657 train_time:1030526ms step_avg:170.17ms\nstep:6067/10000 train_loss:3.9025 train_time:1030699ms step_avg:170.17ms\nstep:6068/10000 train_loss:3.6716 train_time:1030865ms step_avg:170.17ms\nstep:6069/10000 train_loss:3.9577 train_time:1031036ms step_avg:170.17ms\nstep:6070/10000 train_loss:3.8531 train_time:1031208ms step_avg:170.17ms\nstep:6071/10000 train_loss:3.9490 train_time:1031376ms step_avg:170.17ms\nstep:6072/10000 train_loss:3.8087 train_time:1031545ms step_avg:170.17ms\nstep:6073/10000 train_loss:3.9840 train_time:1031716ms step_avg:170.17ms\nstep:6074/10000 train_loss:3.8247 train_time:1031888ms step_avg:170.17ms\nstep:6075/10000 train_loss:3.8410 train_time:1032058ms step_avg:170.17ms\nstep:6076/10000 train_loss:3.8298 train_time:1032230ms step_avg:170.17ms\nstep:6077/10000 train_loss:4.0383 train_time:1032401ms step_avg:170.17ms\nstep:6078/10000 train_loss:3.9143 train_time:1032570ms step_avg:170.17ms\nstep:6079/10000 train_loss:3.8633 train_time:1032743ms step_avg:170.17ms\nstep:6080/10000 train_loss:3.9289 train_time:1032914ms step_avg:170.17ms\nstep:6081/10000 train_loss:3.8897 train_time:1033087ms step_avg:170.17ms\nstep:6082/10000 train_loss:3.7829 train_time:1033256ms step_avg:170.17ms\nstep:6083/10000 train_loss:3.9038 train_time:1033426ms step_avg:170.17ms\nstep:6084/10000 train_loss:3.8153 train_time:1033596ms step_avg:170.17ms\nstep:6085/10000 train_loss:3.8708 train_time:1033769ms step_avg:170.17ms\nstep:6086/10000 train_loss:3.9510 train_time:1033940ms step_avg:170.17ms\nstep:6087/10000 train_loss:3.8178 train_time:1034108ms step_avg:170.17ms\nstep:6088/10000 train_loss:4.0654 train_time:1034277ms step_avg:170.17ms\nstep:6089/10000 train_loss:3.9979 train_time:1034445ms step_avg:170.17ms\nstep:6090/10000 train_loss:3.9555 train_time:1034615ms step_avg:170.17ms\nstep:6091/10000 train_loss:3.9669 train_time:1034786ms step_avg:170.17ms\nstep:6092/10000 train_loss:3.7811 train_time:1034957ms step_avg:170.17ms\nstep:6093/10000 train_loss:3.8159 train_time:1035129ms step_avg:170.17ms\nstep:6094/10000 train_loss:3.8835 train_time:1035300ms step_avg:170.17ms\nstep:6095/10000 train_loss:3.8499 train_time:1035470ms step_avg:170.17ms\nstep:6096/10000 train_loss:3.8423 train_time:1035855ms step_avg:170.20ms\nstep:6097/10000 train_loss:3.7806 train_time:1036019ms step_avg:170.20ms\nstep:6098/10000 train_loss:3.8393 train_time:1036186ms step_avg:170.20ms\nstep:6099/10000 train_loss:3.9011 train_time:1036354ms step_avg:170.20ms\nstep:6100/10000 train_loss:3.8594 train_time:1036521ms step_avg:170.20ms\nstep:6101/10000 train_loss:3.8477 train_time:1036689ms step_avg:170.20ms\nstep:6102/10000 train_loss:3.9536 train_time:1036870ms step_avg:170.20ms\nstep:6103/10000 train_loss:3.7658 train_time:1037042ms step_avg:170.20ms\nstep:6104/10000 train_loss:3.8695 train_time:1037211ms step_avg:170.20ms\nstep:6105/10000 train_loss:3.8414 train_time:1037379ms step_avg:170.20ms\nstep:6106/10000 train_loss:3.8228 train_time:1037550ms step_avg:170.20ms\nstep:6107/10000 train_loss:3.7477 train_time:1037723ms step_avg:170.20ms\nstep:6108/10000 train_loss:3.9421 train_time:1037896ms step_avg:170.20ms\nstep:6109/10000 train_loss:3.9858 train_time:1038066ms step_avg:170.20ms\nstep:6110/10000 train_loss:3.8947 train_time:1038237ms step_avg:170.20ms\nstep:6111/10000 train_loss:3.9359 train_time:1038406ms step_avg:170.20ms\nstep:6112/10000 train_loss:3.8211 train_time:1038574ms step_avg:170.20ms\nstep:6113/10000 train_loss:3.8773 train_time:1038747ms step_avg:170.20ms\nstep:6114/10000 train_loss:3.8954 train_time:1038921ms step_avg:170.20ms\nstep:6115/10000 train_loss:3.8978 train_time:1039090ms step_avg:170.20ms\nstep:6116/10000 train_loss:3.8254 train_time:1039259ms step_avg:170.20ms\nstep:6117/10000 train_loss:3.8871 train_time:1039427ms step_avg:170.20ms\nstep:6118/10000 train_loss:3.8632 train_time:1039597ms step_avg:170.20ms\nstep:6119/10000 train_loss:3.9392 train_time:1039766ms step_avg:170.20ms\nstep:6120/10000 train_loss:3.8289 train_time:1039937ms step_avg:170.20ms\nstep:6121/10000 train_loss:3.8801 train_time:1040110ms step_avg:170.20ms\nstep:6122/10000 train_loss:3.8570 train_time:1040278ms step_avg:170.20ms\nstep:6123/10000 train_loss:3.8334 train_time:1040447ms step_avg:170.20ms\nstep:6124/10000 train_loss:3.8592 train_time:1040617ms step_avg:170.20ms\nstep:6125/10000 train_loss:3.8762 train_time:1040787ms step_avg:170.20ms\nstep:6126/10000 train_loss:3.9970 train_time:1040958ms step_avg:170.20ms\nstep:6127/10000 train_loss:4.0066 train_time:1041135ms step_avg:170.20ms\nstep:6128/10000 train_loss:3.8342 train_time:1041303ms step_avg:170.20ms\nstep:6129/10000 train_loss:3.8237 train_time:1041471ms step_avg:170.20ms\nstep:6130/10000 train_loss:3.9862 train_time:1041641ms step_avg:170.20ms\nstep:6131/10000 train_loss:3.9325 train_time:1041812ms step_avg:170.20ms\nstep:6132/10000 train_loss:3.8255 train_time:1041985ms step_avg:170.20ms\nstep:6133/10000 train_loss:3.8600 train_time:1042152ms step_avg:170.20ms\nstep:6134/10000 train_loss:3.8115 train_time:1042323ms step_avg:170.20ms\nstep:6135/10000 train_loss:3.8303 train_time:1042492ms step_avg:170.20ms\nstep:6136/10000 train_loss:4.0055 train_time:1042663ms step_avg:170.20ms\nstep:6137/10000 train_loss:3.8638 train_time:1042832ms step_avg:170.20ms\nstep:6138/10000 train_loss:3.7978 train_time:1043001ms step_avg:170.20ms\nstep:6139/10000 train_loss:3.9859 train_time:1043171ms step_avg:170.20ms\nstep:6140/10000 train_loss:3.8402 train_time:1043343ms step_avg:170.20ms\nstep:6141/10000 train_loss:3.9312 train_time:1043512ms step_avg:170.20ms\nstep:6142/10000 train_loss:3.9446 train_time:1043685ms step_avg:170.20ms\nstep:6143/10000 train_loss:3.8116 train_time:1043855ms step_avg:170.20ms\nstep:6144/10000 train_loss:3.9115 train_time:1044027ms step_avg:170.20ms\nstep:6145/10000 train_loss:3.8140 train_time:1044198ms step_avg:170.20ms\nstep:6146/10000 train_loss:3.8896 train_time:1044369ms step_avg:170.20ms\nstep:6147/10000 train_loss:3.8111 train_time:1044541ms step_avg:170.20ms\nstep:6148/10000 train_loss:3.7934 train_time:1044711ms step_avg:170.20ms\nstep:6149/10000 train_loss:3.9233 train_time:1044882ms step_avg:170.20ms\nstep:6150/10000 train_loss:3.8797 train_time:1045052ms step_avg:170.20ms\nstep:6151/10000 train_loss:3.8594 train_time:1045224ms step_avg:170.20ms\nstep:6152/10000 train_loss:3.7269 train_time:1045392ms step_avg:170.20ms\nstep:6153/10000 train_loss:3.7799 train_time:1045562ms step_avg:170.20ms\nstep:6154/10000 train_loss:3.8541 train_time:1045729ms step_avg:170.20ms\nstep:6155/10000 train_loss:3.8541 train_time:1045899ms step_avg:170.20ms\nstep:6156/10000 train_loss:3.6400 train_time:1046069ms step_avg:170.20ms\nstep:6157/10000 train_loss:4.0881 train_time:1046240ms step_avg:170.20ms\nstep:6158/10000 train_loss:3.7866 train_time:1046410ms step_avg:170.20ms\nstep:6159/10000 train_loss:3.8945 train_time:1046581ms step_avg:170.20ms\nstep:6160/10000 train_loss:3.7897 train_time:1046749ms step_avg:170.20ms\nstep:6161/10000 train_loss:3.7398 train_time:1046918ms step_avg:170.20ms\nstep:6162/10000 train_loss:3.8791 train_time:1047086ms step_avg:170.20ms\nstep:6163/10000 train_loss:3.9163 train_time:1047257ms step_avg:170.20ms\nstep:6164/10000 train_loss:3.7859 train_time:1047428ms step_avg:170.20ms\nstep:6165/10000 train_loss:3.8902 train_time:1047597ms step_avg:170.20ms\nstep:6166/10000 train_loss:4.0347 train_time:1047766ms step_avg:170.20ms\nstep:6167/10000 train_loss:3.8839 train_time:1047935ms step_avg:170.20ms\nstep:6168/10000 train_loss:3.6998 train_time:1048104ms step_avg:170.20ms\nstep:6169/10000 train_loss:3.8487 train_time:1048272ms step_avg:170.20ms\nstep:6170/10000 train_loss:3.9527 train_time:1048444ms step_avg:170.20ms\nstep:6171/10000 train_loss:4.2616 train_time:1048613ms step_avg:170.20ms\nstep:6172/10000 train_loss:4.0043 train_time:1048784ms step_avg:170.20ms\nstep:6173/10000 train_loss:3.7447 train_time:1048952ms step_avg:170.20ms\nstep:6174/10000 train_loss:3.8691 train_time:1049127ms step_avg:170.20ms\nstep:6175/10000 train_loss:3.8184 train_time:1049291ms step_avg:170.20ms\nstep:6176/10000 train_loss:3.7783 train_time:1049463ms step_avg:170.20ms\nstep:6177/10000 train_loss:3.8251 train_time:1049631ms step_avg:170.20ms\nstep:6178/10000 train_loss:3.8420 train_time:1049800ms step_avg:170.20ms\nstep:6179/10000 train_loss:3.9614 train_time:1049972ms step_avg:170.20ms\nstep:6180/10000 train_loss:3.6421 train_time:1050142ms step_avg:170.20ms\nstep:6181/10000 train_loss:4.0010 train_time:1050310ms step_avg:170.20ms\nstep:6182/10000 train_loss:3.8044 train_time:1050480ms step_avg:170.20ms\nstep:6183/10000 train_loss:3.9239 train_time:1050651ms step_avg:170.20ms\nstep:6184/10000 train_loss:3.8505 train_time:1050820ms step_avg:170.20ms\nstep:6185/10000 train_loss:3.7477 train_time:1050990ms step_avg:170.20ms\nstep:6186/10000 train_loss:3.8028 train_time:1051163ms step_avg:170.20ms\nstep:6187/10000 train_loss:3.9446 train_time:1051332ms step_avg:170.20ms\nstep:6188/10000 train_loss:3.8736 train_time:1051501ms step_avg:170.20ms\nstep:6189/10000 train_loss:3.8540 train_time:1051671ms step_avg:170.20ms\nstep:6190/10000 train_loss:3.8970 train_time:1051842ms step_avg:170.20ms\nstep:6191/10000 train_loss:3.9330 train_time:1052013ms step_avg:170.20ms\nstep:6192/10000 train_loss:3.6444 train_time:1052189ms step_avg:170.20ms\nstep:6193/10000 train_loss:3.8438 train_time:1052360ms step_avg:170.20ms\nstep:6194/10000 train_loss:3.8572 train_time:1052528ms step_avg:170.20ms\nstep:6195/10000 train_loss:3.9265 train_time:1052696ms step_avg:170.20ms\nstep:6196/10000 train_loss:4.0593 train_time:1052869ms step_avg:170.20ms\nstep:6197/10000 train_loss:4.0832 train_time:1053041ms step_avg:170.20ms\nstep:6198/10000 train_loss:3.8140 train_time:1053211ms step_avg:170.20ms\nstep:6199/10000 train_loss:3.8117 train_time:1053383ms step_avg:170.20ms\nstep:6200/10000 train_loss:3.9777 train_time:1053551ms step_avg:170.20ms\nstep:6201/10000 train_loss:3.8078 train_time:1053722ms step_avg:170.20ms\nstep:6202/10000 train_loss:3.7921 train_time:1053894ms step_avg:170.20ms\nstep:6203/10000 train_loss:3.8960 train_time:1054070ms step_avg:170.20ms\nstep:6204/10000 train_loss:3.8974 train_time:1054237ms step_avg:170.20ms\nstep:6205/10000 train_loss:3.9555 train_time:1054410ms step_avg:170.20ms\nstep:6206/10000 train_loss:3.8081 train_time:1054579ms step_avg:170.20ms\nstep:6207/10000 train_loss:3.9676 train_time:1054748ms step_avg:170.20ms\nstep:6208/10000 train_loss:3.9907 train_time:1054919ms step_avg:170.20ms\nstep:6209/10000 train_loss:3.9000 train_time:1055093ms step_avg:170.20ms\nstep:6210/10000 train_loss:3.9354 train_time:1055264ms step_avg:170.20ms\nstep:6211/10000 train_loss:3.8912 train_time:1055430ms step_avg:170.20ms\nstep:6212/10000 train_loss:3.9175 train_time:1055601ms step_avg:170.20ms\nstep:6213/10000 train_loss:3.9411 train_time:1055771ms step_avg:170.20ms\nstep:6214/10000 train_loss:3.8452 train_time:1055944ms step_avg:170.20ms\nstep:6215/10000 train_loss:3.9681 train_time:1056114ms step_avg:170.20ms\nstep:6216/10000 train_loss:3.8141 train_time:1056288ms step_avg:170.20ms\nstep:6217/10000 train_loss:3.9721 train_time:1056457ms step_avg:170.20ms\nstep:6218/10000 train_loss:3.7999 train_time:1056623ms step_avg:170.20ms\nstep:6219/10000 train_loss:3.9230 train_time:1056793ms step_avg:170.20ms\nstep:6220/10000 train_loss:3.9966 train_time:1056968ms step_avg:170.20ms\nstep:6221/10000 train_loss:3.7876 train_time:1057137ms step_avg:170.20ms\nstep:6222/10000 train_loss:3.7849 train_time:1057310ms step_avg:170.20ms\nstep:6223/10000 train_loss:3.9276 train_time:1057481ms step_avg:170.20ms\nstep:6224/10000 train_loss:3.9288 train_time:1057651ms step_avg:170.20ms\nstep:6225/10000 train_loss:3.8985 train_time:1057821ms step_avg:170.20ms\nstep:6226/10000 train_loss:3.7660 train_time:1057991ms step_avg:170.20ms\nstep:6227/10000 train_loss:3.8308 train_time:1058161ms step_avg:170.20ms\nstep:6228/10000 train_loss:3.8434 train_time:1058330ms step_avg:170.20ms\nstep:6229/10000 train_loss:3.8634 train_time:1058500ms step_avg:170.20ms\nstep:6230/10000 train_loss:3.8973 train_time:1058668ms step_avg:170.20ms\nstep:6231/10000 train_loss:3.8266 train_time:1058840ms step_avg:170.20ms\nstep:6232/10000 train_loss:3.7852 train_time:1059009ms step_avg:170.20ms\nstep:6233/10000 train_loss:3.9048 train_time:1059181ms step_avg:170.20ms\nstep:6234/10000 train_loss:3.9097 train_time:1059351ms step_avg:170.20ms\nstep:6235/10000 train_loss:3.8309 train_time:1059524ms step_avg:170.20ms\nstep:6236/10000 train_loss:3.8536 train_time:1059694ms step_avg:170.20ms\nstep:6237/10000 train_loss:3.9620 train_time:1059865ms step_avg:170.20ms\nstep:6238/10000 train_loss:3.8509 train_time:1060034ms step_avg:170.20ms\nstep:6239/10000 train_loss:3.9705 train_time:1060201ms step_avg:170.20ms\nstep:6240/10000 train_loss:3.8098 train_time:1060370ms step_avg:170.20ms\nstep:6241/10000 train_loss:3.8802 train_time:1060541ms step_avg:170.20ms\nstep:6242/10000 train_loss:3.9154 train_time:1060709ms step_avg:170.20ms\nstep:6243/10000 train_loss:3.9389 train_time:1060882ms step_avg:170.20ms\nstep:6244/10000 train_loss:3.8750 train_time:1061050ms step_avg:170.20ms\nstep:6245/10000 train_loss:3.6661 train_time:1061220ms step_avg:170.20ms\nstep:6246/10000 train_loss:3.8407 train_time:1061389ms step_avg:170.20ms\nstep:6247/10000 train_loss:3.8583 train_time:1061561ms step_avg:170.20ms\nstep:6248/10000 train_loss:3.8300 train_time:1061730ms step_avg:170.20ms\nstep:6249/10000 train_loss:3.7022 train_time:1061904ms step_avg:170.20ms\nstep:6250/10000 train_loss:3.7901 train_time:1062069ms step_avg:170.20ms\nstep:6251/10000 train_loss:3.8660 train_time:1062239ms step_avg:170.20ms\nstep:6252/10000 train_loss:3.9436 train_time:1062409ms step_avg:170.20ms\nstep:6253/10000 train_loss:4.0503 train_time:1062580ms step_avg:170.20ms\nstep:6254/10000 train_loss:3.9755 train_time:1062751ms step_avg:170.20ms\nstep:6255/10000 train_loss:3.8783 train_time:1062920ms step_avg:170.20ms\nstep:6256/10000 train_loss:3.8148 train_time:1063089ms step_avg:170.20ms\nstep:6257/10000 train_loss:3.7951 train_time:1063261ms step_avg:170.20ms\nstep:6258/10000 train_loss:3.8306 train_time:1063432ms step_avg:170.20ms\nstep:6259/10000 train_loss:3.8573 train_time:1063599ms step_avg:170.20ms\nstep:6260/10000 train_loss:3.9260 train_time:1063769ms step_avg:170.20ms\nstep:6261/10000 train_loss:3.8547 train_time:1063941ms step_avg:170.20ms\nstep:6262/10000 train_loss:3.7771 train_time:1064109ms step_avg:170.20ms\nstep:6263/10000 train_loss:4.0076 train_time:1064278ms step_avg:170.20ms\nstep:6264/10000 train_loss:3.7795 train_time:1064446ms step_avg:170.20ms\nstep:6265/10000 train_loss:3.8744 train_time:1064617ms step_avg:170.20ms\nstep:6266/10000 train_loss:3.9378 train_time:1064784ms step_avg:170.20ms\nstep:6267/10000 train_loss:3.7447 train_time:1064954ms step_avg:170.20ms\nstep:6268/10000 train_loss:3.9062 train_time:1065126ms step_avg:170.20ms\nstep:6269/10000 train_loss:3.8912 train_time:1065295ms step_avg:170.20ms\nstep:6270/10000 train_loss:3.7330 train_time:1065467ms step_avg:170.20ms\nstep:6271/10000 train_loss:3.8763 train_time:1065637ms step_avg:170.20ms\nstep:6272/10000 train_loss:3.7051 train_time:1065809ms step_avg:170.20ms\nstep:6273/10000 train_loss:3.8883 train_time:1065981ms step_avg:170.20ms\nstep:6274/10000 train_loss:3.8387 train_time:1066151ms step_avg:170.20ms\nstep:6275/10000 train_loss:3.8584 train_time:1066320ms step_avg:170.20ms\nstep:6276/10000 train_loss:3.7860 train_time:1066489ms step_avg:170.20ms\nstep:6277/10000 train_loss:3.7931 train_time:1066659ms step_avg:170.20ms\nstep:6278/10000 train_loss:3.9228 train_time:1066828ms step_avg:170.20ms\nstep:6279/10000 train_loss:3.8631 train_time:1066999ms step_avg:170.20ms\nstep:6280/10000 train_loss:3.7572 train_time:1067166ms step_avg:170.20ms\nstep:6281/10000 train_loss:3.7617 train_time:1067336ms step_avg:170.20ms\nstep:6282/10000 train_loss:3.8355 train_time:1067508ms step_avg:170.20ms\nstep:6283/10000 train_loss:3.8295 train_time:1067676ms step_avg:170.20ms\nstep:6284/10000 train_loss:3.7647 train_time:1067847ms step_avg:170.20ms\nstep:6285/10000 train_loss:3.8373 train_time:1068018ms step_avg:170.20ms\nstep:6286/10000 train_loss:3.8089 train_time:1068186ms step_avg:170.20ms\nstep:6287/10000 train_loss:3.8552 train_time:1068358ms step_avg:170.20ms\nstep:6288/10000 train_loss:3.7864 train_time:1068529ms step_avg:170.20ms\nstep:6289/10000 train_loss:3.9176 train_time:1068699ms step_avg:170.20ms\nstep:6290/10000 train_loss:3.9149 train_time:1068870ms step_avg:170.20ms\nstep:6291/10000 train_loss:3.8350 train_time:1069039ms step_avg:170.20ms\nstep:6292/10000 train_loss:3.8360 train_time:1069208ms step_avg:170.20ms\nstep:6293/10000 train_loss:3.8275 train_time:1069376ms step_avg:170.20ms\nstep:6294/10000 train_loss:3.9352 train_time:1069547ms step_avg:170.20ms\nstep:6295/10000 train_loss:3.6949 train_time:1069718ms step_avg:170.20ms\nstep:6296/10000 train_loss:3.6639 train_time:1069888ms step_avg:170.20ms\nstep:6297/10000 train_loss:3.8971 train_time:1070062ms step_avg:170.20ms\nstep:6298/10000 train_loss:3.7397 train_time:1070231ms step_avg:170.20ms\nstep:6299/10000 train_loss:3.8030 train_time:1070403ms step_avg:170.20ms\nstep:6300/10000 train_loss:3.8454 train_time:1070570ms step_avg:170.20ms\nstep:6301/10000 train_loss:3.8337 train_time:1070740ms step_avg:170.20ms\nstep:6302/10000 train_loss:3.7699 train_time:1070909ms step_avg:170.20ms\nstep:6303/10000 train_loss:3.7444 train_time:1071081ms step_avg:170.20ms\nstep:6304/10000 train_loss:3.8544 train_time:1071251ms step_avg:170.20ms\nstep:6305/10000 train_loss:3.8305 train_time:1071421ms step_avg:170.20ms\nstep:6306/10000 train_loss:3.8851 train_time:1071589ms step_avg:170.20ms\nstep:6307/10000 train_loss:3.8208 train_time:1071761ms step_avg:170.20ms\nstep:6308/10000 train_loss:3.8315 train_time:1071930ms step_avg:170.20ms\nstep:6309/10000 train_loss:3.9060 train_time:1072103ms step_avg:170.20ms\nstep:6310/10000 train_loss:3.7599 train_time:1072274ms step_avg:170.20ms\nstep:6311/10000 train_loss:3.8510 train_time:1072448ms step_avg:170.20ms\nstep:6312/10000 train_loss:3.8497 train_time:1072620ms step_avg:170.20ms\nstep:6313/10000 train_loss:3.9828 train_time:1072793ms step_avg:170.20ms\nstep:6314/10000 train_loss:3.8914 train_time:1072958ms step_avg:170.20ms\nstep:6315/10000 train_loss:3.8128 train_time:1073127ms step_avg:170.20ms\nstep:6316/10000 train_loss:3.8236 train_time:1073304ms step_avg:170.20ms\nstep:6317/10000 train_loss:3.8452 train_time:1073468ms step_avg:170.20ms\nstep:6318/10000 train_loss:3.8085 train_time:1073641ms step_avg:170.20ms\nstep:6319/10000 train_loss:3.8935 train_time:1073808ms step_avg:170.20ms\nstep:6320/10000 train_loss:3.8168 train_time:1073980ms step_avg:170.20ms\nstep:6321/10000 train_loss:4.1874 train_time:1074148ms step_avg:170.20ms\nstep:6322/10000 train_loss:3.8689 train_time:1074319ms step_avg:170.20ms\nstep:6323/10000 train_loss:3.7816 train_time:1074490ms step_avg:170.20ms\nstep:6324/10000 train_loss:3.9970 train_time:1074661ms step_avg:170.20ms\nstep:6325/10000 train_loss:3.8964 train_time:1074831ms step_avg:170.20ms\nstep:6326/10000 train_loss:3.8114 train_time:1075003ms step_avg:170.20ms\nstep:6327/10000 train_loss:3.7027 train_time:1075174ms step_avg:170.20ms\nstep:6328/10000 train_loss:3.8756 train_time:1075346ms step_avg:170.20ms\nstep:6329/10000 train_loss:4.0582 train_time:1075516ms step_avg:170.20ms\nstep:6330/10000 train_loss:3.7934 train_time:1075689ms step_avg:170.20ms\nstep:6331/10000 train_loss:3.8260 train_time:1075859ms step_avg:170.20ms\nstep:6332/10000 train_loss:3.9249 train_time:1076029ms step_avg:170.20ms\nstep:6333/10000 train_loss:3.8627 train_time:1076202ms step_avg:170.20ms\nstep:6334/10000 train_loss:3.7789 train_time:1076370ms step_avg:170.20ms\nstep:6335/10000 train_loss:3.7375 train_time:1076542ms step_avg:170.20ms\nstep:6336/10000 train_loss:3.7052 train_time:1076710ms step_avg:170.20ms\nstep:6337/10000 train_loss:3.8655 train_time:1076881ms step_avg:170.20ms\nstep:6338/10000 train_loss:3.9942 train_time:1077052ms step_avg:170.20ms\nstep:6339/10000 train_loss:3.8933 train_time:1077223ms step_avg:170.20ms\nstep:6340/10000 train_loss:3.8157 train_time:1077392ms step_avg:170.20ms\nstep:6341/10000 train_loss:4.0115 train_time:1077563ms step_avg:170.20ms\nstep:6342/10000 train_loss:3.7965 train_time:1077732ms step_avg:170.20ms\nstep:6343/10000 train_loss:4.0107 train_time:1077905ms step_avg:170.20ms\nstep:6344/10000 train_loss:3.7909 train_time:1078079ms step_avg:170.21ms\nstep:6345/10000 train_loss:3.8171 train_time:1078246ms step_avg:170.20ms\nstep:6346/10000 train_loss:3.9421 train_time:1078416ms step_avg:170.20ms\nstep:6347/10000 train_loss:3.8836 train_time:1078585ms step_avg:170.20ms\nstep:6348/10000 train_loss:3.9638 train_time:1078755ms step_avg:170.20ms\nstep:6349/10000 train_loss:3.8191 train_time:1078924ms step_avg:170.20ms\nstep:6350/10000 train_loss:3.7871 train_time:1079094ms step_avg:170.20ms\nstep:6351/10000 train_loss:3.8742 train_time:1079266ms step_avg:170.20ms\nstep:6352/10000 train_loss:3.8078 train_time:1079433ms step_avg:170.20ms\nstep:6353/10000 train_loss:3.9258 train_time:1079605ms step_avg:170.20ms\nstep:6354/10000 train_loss:3.9286 train_time:1079775ms step_avg:170.20ms\nstep:6355/10000 train_loss:4.1350 train_time:1079947ms step_avg:170.20ms\nstep:6356/10000 train_loss:3.7652 train_time:1080117ms step_avg:170.20ms\nstep:6357/10000 train_loss:3.7981 train_time:1080286ms step_avg:170.20ms\nstep:6358/10000 train_loss:3.7085 train_time:1080459ms step_avg:170.20ms\nstep:6359/10000 train_loss:3.7604 train_time:1080628ms step_avg:170.20ms\nstep:6360/10000 train_loss:3.8948 train_time:1080798ms step_avg:170.20ms\nstep:6361/10000 train_loss:3.7970 train_time:1080967ms step_avg:170.20ms\nstep:6362/10000 train_loss:3.9108 train_time:1081138ms step_avg:170.20ms\nstep:6363/10000 train_loss:3.8882 train_time:1081309ms step_avg:170.20ms\nstep:6364/10000 train_loss:3.8299 train_time:1081480ms step_avg:170.20ms\nstep:6365/10000 train_loss:3.7685 train_time:1081649ms step_avg:170.20ms\nstep:6366/10000 train_loss:3.8180 train_time:1081821ms step_avg:170.20ms\nstep:6367/10000 train_loss:4.0331 train_time:1081988ms step_avg:170.20ms\nstep:6368/10000 train_loss:3.7869 train_time:1082159ms step_avg:170.20ms\nstep:6369/10000 train_loss:3.8975 train_time:1082329ms step_avg:170.20ms\nstep:6370/10000 train_loss:3.9848 train_time:1082502ms step_avg:170.20ms\nstep:6371/10000 train_loss:3.8821 train_time:1082670ms step_avg:170.20ms\nstep:6372/10000 train_loss:3.7683 train_time:1082839ms step_avg:170.20ms\nstep:6373/10000 train_loss:3.7724 train_time:1083012ms step_avg:170.20ms\nstep:6374/10000 train_loss:3.8047 train_time:1083183ms step_avg:170.20ms\nstep:6375/10000 train_loss:3.8634 train_time:1083355ms step_avg:170.20ms\nstep:6376/10000 train_loss:4.1638 train_time:1083526ms step_avg:170.21ms\nstep:6377/10000 train_loss:3.5304 train_time:1083695ms step_avg:170.20ms\nstep:6378/10000 train_loss:3.8093 train_time:1083867ms step_avg:170.21ms\nstep:6379/10000 train_loss:3.9236 train_time:1084038ms step_avg:170.21ms\nstep:6380/10000 train_loss:3.6953 train_time:1084207ms step_avg:170.21ms\nstep:6381/10000 train_loss:3.7735 train_time:1084376ms step_avg:170.20ms\nstep:6382/10000 train_loss:3.8962 train_time:1084544ms step_avg:170.20ms\nstep:6383/10000 train_loss:3.8459 train_time:1084713ms step_avg:170.20ms\nstep:6384/10000 train_loss:3.7471 train_time:1084886ms step_avg:170.20ms\nstep:6385/10000 train_loss:3.7906 train_time:1085057ms step_avg:170.20ms\nstep:6386/10000 train_loss:3.8786 train_time:1085225ms step_avg:170.20ms\nstep:6387/10000 train_loss:3.6365 train_time:1085393ms step_avg:170.20ms\nstep:6388/10000 train_loss:3.8685 train_time:1085563ms step_avg:170.20ms\nstep:6389/10000 train_loss:3.9473 train_time:1085733ms step_avg:170.20ms\nstep:6390/10000 train_loss:3.8452 train_time:1085907ms step_avg:170.20ms\nstep:6391/10000 train_loss:3.8469 train_time:1086081ms step_avg:170.21ms\nstep:6392/10000 train_loss:3.8096 train_time:1086251ms step_avg:170.21ms\nstep:6393/10000 train_loss:3.7941 train_time:1086423ms step_avg:170.21ms\nstep:6394/10000 train_loss:3.9888 train_time:1086590ms step_avg:170.21ms\nstep:6395/10000 train_loss:3.9749 train_time:1086762ms step_avg:170.21ms\nstep:6396/10000 train_loss:3.6683 train_time:1086931ms step_avg:170.21ms\nstep:6397/10000 train_loss:3.8738 train_time:1087102ms step_avg:170.21ms\nstep:6398/10000 train_loss:3.7492 train_time:1087270ms step_avg:170.21ms\nstep:6399/10000 train_loss:3.8495 train_time:1087442ms step_avg:170.21ms\nstep:6400/10000 train_loss:3.8230 train_time:1087608ms step_avg:170.20ms\nstep:6401/10000 train_loss:3.8672 train_time:1087779ms step_avg:170.20ms\nstep:6402/10000 train_loss:3.8738 train_time:1087948ms step_avg:170.20ms\nstep:6403/10000 train_loss:3.8642 train_time:1088120ms step_avg:170.20ms\nstep:6404/10000 train_loss:3.8048 train_time:1088289ms step_avg:170.20ms\nstep:6405/10000 train_loss:3.7290 train_time:1088463ms step_avg:170.21ms\nstep:6406/10000 train_loss:3.9079 train_time:1088631ms step_avg:170.21ms\nstep:6407/10000 train_loss:4.2756 train_time:1088802ms step_avg:170.21ms\nstep:6408/10000 train_loss:3.8627 train_time:1088971ms step_avg:170.20ms\nstep:6409/10000 train_loss:3.8200 train_time:1089144ms step_avg:170.21ms\nstep:6410/10000 train_loss:3.9613 train_time:1089311ms step_avg:170.20ms\nstep:6411/10000 train_loss:3.7671 train_time:1089482ms step_avg:170.21ms\nstep:6412/10000 train_loss:3.8133 train_time:1089652ms step_avg:170.20ms\nstep:6413/10000 train_loss:3.8006 train_time:1089819ms step_avg:170.20ms\nstep:6414/10000 train_loss:3.8003 train_time:1089990ms step_avg:170.20ms\nstep:6415/10000 train_loss:3.7919 train_time:1090161ms step_avg:170.20ms\nstep:6416/10000 train_loss:3.7556 train_time:1090329ms step_avg:170.20ms\nstep:6417/10000 train_loss:3.8380 train_time:1090498ms step_avg:170.20ms\nstep:6418/10000 train_loss:4.0936 train_time:1090669ms step_avg:170.20ms\nstep:6419/10000 train_loss:3.9637 train_time:1090839ms step_avg:170.20ms\nstep:6420/10000 train_loss:3.8161 train_time:1091010ms step_avg:170.20ms\nstep:6421/10000 train_loss:3.8957 train_time:1091181ms step_avg:170.20ms\nstep:6422/10000 train_loss:4.0109 train_time:1091350ms step_avg:170.20ms\nstep:6423/10000 train_loss:3.8487 train_time:1091518ms step_avg:170.20ms\nstep:6424/10000 train_loss:3.7210 train_time:1091687ms step_avg:170.20ms\nstep:6425/10000 train_loss:3.6906 train_time:1091856ms step_avg:170.20ms\nstep:6426/10000 train_loss:3.8212 train_time:1092026ms step_avg:170.20ms\nstep:6427/10000 train_loss:3.7953 train_time:1092198ms step_avg:170.20ms\nstep:6428/10000 train_loss:3.8363 train_time:1092368ms step_avg:170.20ms\nstep:6429/10000 train_loss:3.8599 train_time:1092537ms step_avg:170.20ms\nstep:6430/10000 train_loss:3.8401 train_time:1092709ms step_avg:170.20ms\nstep:6431/10000 train_loss:3.7715 train_time:1092879ms step_avg:170.20ms\nstep:6432/10000 train_loss:3.7286 train_time:1093051ms step_avg:170.20ms\nstep:6433/10000 train_loss:3.9481 train_time:1093221ms step_avg:170.20ms\nstep:6434/10000 train_loss:3.7011 train_time:1093390ms step_avg:170.20ms\nstep:6435/10000 train_loss:3.8895 train_time:1093562ms step_avg:170.20ms\nstep:6436/10000 train_loss:3.7882 train_time:1093730ms step_avg:170.20ms\nstep:6437/10000 train_loss:3.8085 train_time:1093899ms step_avg:170.20ms\nstep:6438/10000 train_loss:4.0431 train_time:1094069ms step_avg:170.20ms\nstep:6439/10000 train_loss:3.9622 train_time:1094240ms step_avg:170.20ms\nstep:6440/10000 train_loss:3.8232 train_time:1094407ms step_avg:170.20ms\nstep:6441/10000 train_loss:3.9640 train_time:1094578ms step_avg:170.20ms\nstep:6442/10000 train_loss:3.8018 train_time:1094751ms step_avg:170.20ms\nstep:6443/10000 train_loss:3.8517 train_time:1094922ms step_avg:170.20ms\nstep:6444/10000 train_loss:3.6750 train_time:1095091ms step_avg:170.20ms\nstep:6445/10000 train_loss:3.7124 train_time:1095259ms step_avg:170.20ms\nstep:6446/10000 train_loss:3.8370 train_time:1095432ms step_avg:170.20ms\nstep:6447/10000 train_loss:3.6251 train_time:1095603ms step_avg:170.20ms\nstep:6448/10000 train_loss:3.8395 train_time:1095771ms step_avg:170.20ms\nstep:6449/10000 train_loss:3.7937 train_time:1095938ms step_avg:170.20ms\nstep:6450/10000 train_loss:3.7817 train_time:1096110ms step_avg:170.20ms\nstep:6451/10000 train_loss:3.8198 train_time:1096280ms step_avg:170.20ms\nstep:6452/10000 train_loss:3.8191 train_time:1096450ms step_avg:170.20ms\nstep:6453/10000 train_loss:3.8762 train_time:1096625ms step_avg:170.20ms\nstep:6454/10000 train_loss:4.3801 train_time:1096796ms step_avg:170.20ms\nstep:6455/10000 train_loss:3.8907 train_time:1096966ms step_avg:170.20ms\nstep:6456/10000 train_loss:3.8185 train_time:1097134ms step_avg:170.20ms\nstep:6457/10000 train_loss:3.8089 train_time:1097302ms step_avg:170.20ms\nstep:6458/10000 train_loss:4.0259 train_time:1097471ms step_avg:170.20ms\nstep:6459/10000 train_loss:3.8734 train_time:1097641ms step_avg:170.20ms\nstep:6460/10000 train_loss:3.9051 train_time:1097811ms step_avg:170.20ms\nstep:6461/10000 train_loss:3.8440 train_time:1097982ms step_avg:170.20ms\nstep:6462/10000 train_loss:3.9696 train_time:1098150ms step_avg:170.20ms\nstep:6463/10000 train_loss:3.9321 train_time:1098321ms step_avg:170.20ms\nstep:6464/10000 train_loss:3.8246 train_time:1098490ms step_avg:170.20ms\nstep:6465/10000 train_loss:3.7819 train_time:1098660ms step_avg:170.20ms\nstep:6466/10000 train_loss:3.8374 train_time:1098829ms step_avg:170.20ms\nstep:6467/10000 train_loss:3.9849 train_time:1099004ms step_avg:170.20ms\nstep:6468/10000 train_loss:3.8773 train_time:1099173ms step_avg:170.20ms\nstep:6469/10000 train_loss:3.7592 train_time:1099340ms step_avg:170.20ms\nstep:6470/10000 train_loss:3.8763 train_time:1099510ms step_avg:170.20ms\nstep:6471/10000 train_loss:3.8594 train_time:1099680ms step_avg:170.20ms\nstep:6472/10000 train_loss:3.8347 train_time:1099850ms step_avg:170.20ms\nstep:6473/10000 train_loss:3.8558 train_time:1100021ms step_avg:170.20ms\nstep:6474/10000 train_loss:3.8447 train_time:1100191ms step_avg:170.20ms\nstep:6475/10000 train_loss:3.7222 train_time:1100361ms step_avg:170.20ms\nstep:6476/10000 train_loss:3.9219 train_time:1100529ms step_avg:170.20ms\nstep:6477/10000 train_loss:3.7570 train_time:1100699ms step_avg:170.20ms\nstep:6478/10000 train_loss:3.7903 train_time:1100867ms step_avg:170.20ms\nstep:6479/10000 train_loss:3.8075 train_time:1101038ms step_avg:170.20ms\nstep:6480/10000 train_loss:3.9465 train_time:1101207ms step_avg:170.20ms\nstep:6481/10000 train_loss:3.8462 train_time:1101378ms step_avg:170.20ms\nstep:6482/10000 train_loss:3.8680 train_time:1101547ms step_avg:170.20ms\nstep:6483/10000 train_loss:3.8358 train_time:1101717ms step_avg:170.20ms\nstep:6484/10000 train_loss:3.8103 train_time:1101886ms step_avg:170.20ms\nstep:6485/10000 train_loss:3.7236 train_time:1102056ms step_avg:170.20ms\nstep:6486/10000 train_loss:3.8675 train_time:1102228ms step_avg:170.20ms\nstep:6487/10000 train_loss:3.7443 train_time:1102400ms step_avg:170.20ms\nstep:6488/10000 train_loss:4.1136 train_time:1102569ms step_avg:170.20ms\nstep:6489/10000 train_loss:3.9578 train_time:1102742ms step_avg:170.20ms\nstep:6490/10000 train_loss:3.7386 train_time:1102913ms step_avg:170.20ms\nstep:6491/10000 train_loss:3.8300 train_time:1103085ms step_avg:170.20ms\nstep:6492/10000 train_loss:3.7307 train_time:1103255ms step_avg:170.20ms\nstep:6493/10000 train_loss:3.7034 train_time:1103427ms step_avg:170.20ms\nstep:6494/10000 train_loss:4.0023 train_time:1103598ms step_avg:170.20ms\nstep:6495/10000 train_loss:3.9167 train_time:1103768ms step_avg:170.20ms\nstep:6496/10000 train_loss:4.0783 train_time:1103940ms step_avg:170.20ms\nstep:6497/10000 train_loss:3.8240 train_time:1104110ms step_avg:170.20ms\nstep:6498/10000 train_loss:3.8611 train_time:1104282ms step_avg:170.20ms\nstep:6499/10000 train_loss:3.8643 train_time:1104452ms step_avg:170.20ms\nstep:6500/10000 train_loss:3.7885 train_time:1104622ms step_avg:170.20ms\nstep:6501/10000 train_loss:3.7385 train_time:1104792ms step_avg:170.20ms\nstep:6502/10000 train_loss:3.7127 train_time:1104962ms step_avg:170.20ms\nstep:6503/10000 train_loss:3.9397 train_time:1105130ms step_avg:170.20ms\nstep:6504/10000 train_loss:3.8822 train_time:1105301ms step_avg:170.20ms\nstep:6505/10000 train_loss:3.7174 train_time:1105468ms step_avg:170.20ms\nstep:6506/10000 train_loss:3.7998 train_time:1105640ms step_avg:170.20ms\nstep:6507/10000 train_loss:3.8114 train_time:1105810ms step_avg:170.20ms\nstep:6508/10000 train_loss:3.8801 train_time:1105981ms step_avg:170.20ms\nstep:6509/10000 train_loss:3.7994 train_time:1106150ms step_avg:170.20ms\nstep:6510/10000 train_loss:3.8052 train_time:1106321ms step_avg:170.20ms\nstep:6511/10000 train_loss:3.7938 train_time:1106488ms step_avg:170.20ms\nstep:6512/10000 train_loss:3.9462 train_time:1106658ms step_avg:170.20ms\nstep:6513/10000 train_loss:3.5528 train_time:1106828ms step_avg:170.20ms\nstep:6514/10000 train_loss:3.8359 train_time:1106999ms step_avg:170.20ms\nstep:6515/10000 train_loss:3.8018 train_time:1107169ms step_avg:170.20ms\nstep:6516/10000 train_loss:3.7912 train_time:1107340ms step_avg:170.20ms\nstep:6517/10000 train_loss:3.9106 train_time:1107509ms step_avg:170.20ms\nstep:6518/10000 train_loss:3.8307 train_time:1107681ms step_avg:170.20ms\nstep:6519/10000 train_loss:3.8741 train_time:1107856ms step_avg:170.20ms\nstep:6520/10000 train_loss:3.8210 train_time:1108026ms step_avg:170.20ms\nstep:6521/10000 train_loss:3.8883 train_time:1108192ms step_avg:170.20ms\nstep:6522/10000 train_loss:3.7759 train_time:1108361ms step_avg:170.20ms\nstep:6523/10000 train_loss:3.9163 train_time:1108532ms step_avg:170.20ms\nstep:6524/10000 train_loss:3.8366 train_time:1108699ms step_avg:170.20ms\nstep:6525/10000 train_loss:3.7185 train_time:1108869ms step_avg:170.20ms\nstep:6526/10000 train_loss:3.8777 train_time:1109041ms step_avg:170.20ms\nstep:6527/10000 train_loss:3.6670 train_time:1109212ms step_avg:170.20ms\nstep:6528/10000 train_loss:3.7134 train_time:1109384ms step_avg:170.20ms\nstep:6529/10000 train_loss:3.7787 train_time:1109554ms step_avg:170.20ms\nstep:6530/10000 train_loss:3.7846 train_time:1109728ms step_avg:170.20ms\nstep:6531/10000 train_loss:3.8493 train_time:1109900ms step_avg:170.20ms\nstep:6532/10000 train_loss:3.7832 train_time:1110071ms step_avg:170.20ms\nstep:6533/10000 train_loss:3.9454 train_time:1110242ms step_avg:170.20ms\nstep:6534/10000 train_loss:3.8231 train_time:1110410ms step_avg:170.20ms\nstep:6535/10000 train_loss:3.8331 train_time:1110580ms step_avg:170.20ms\nstep:6536/10000 train_loss:3.8032 train_time:1110750ms step_avg:170.20ms\nstep:6537/10000 train_loss:3.9882 train_time:1110921ms step_avg:170.20ms\nstep:6538/10000 train_loss:3.6874 train_time:1111090ms step_avg:170.20ms\nstep:6539/10000 train_loss:3.8232 train_time:1111261ms step_avg:170.20ms\nstep:6540/10000 train_loss:3.8267 train_time:1111429ms step_avg:170.20ms\nstep:6541/10000 train_loss:3.8324 train_time:1111599ms step_avg:170.20ms\nstep:6542/10000 train_loss:3.6869 train_time:1111770ms step_avg:170.20ms\nstep:6543/10000 train_loss:3.9135 train_time:1111941ms step_avg:170.20ms\nstep:6544/10000 train_loss:3.8699 train_time:1112111ms step_avg:170.20ms\nstep:6545/10000 train_loss:3.7613 train_time:1112285ms step_avg:170.20ms\nstep:6546/10000 train_loss:3.8519 train_time:1112456ms step_avg:170.20ms\nstep:6547/10000 train_loss:4.0054 train_time:1112627ms step_avg:170.20ms\nstep:6548/10000 train_loss:4.1373 train_time:1112795ms step_avg:170.20ms\nstep:6549/10000 train_loss:3.8712 train_time:1112966ms step_avg:170.20ms\nstep:6550/10000 train_loss:3.8354 train_time:1113137ms step_avg:170.20ms\nstep:6551/10000 train_loss:4.0077 train_time:1113307ms step_avg:170.20ms\nstep:6552/10000 train_loss:3.7684 train_time:1113480ms step_avg:170.20ms\nstep:6553/10000 train_loss:3.6740 train_time:1113649ms step_avg:170.20ms\nstep:6554/10000 train_loss:3.7751 train_time:1113820ms step_avg:170.20ms\nstep:6555/10000 train_loss:3.8878 train_time:1113989ms step_avg:170.20ms\nstep:6556/10000 train_loss:3.7612 train_time:1114160ms step_avg:170.20ms\nstep:6557/10000 train_loss:3.9757 train_time:1114328ms step_avg:170.20ms\nstep:6558/10000 train_loss:3.8682 train_time:1114501ms step_avg:170.20ms\nstep:6559/10000 train_loss:3.8689 train_time:1114671ms step_avg:170.20ms\nstep:6560/10000 train_loss:3.7842 train_time:1114841ms step_avg:170.20ms\nstep:6561/10000 train_loss:3.7995 train_time:1115012ms step_avg:170.20ms\nstep:6562/10000 train_loss:3.9301 train_time:1115186ms step_avg:170.21ms\nstep:6563/10000 train_loss:3.6560 train_time:1115355ms step_avg:170.21ms\nstep:6564/10000 train_loss:3.9181 train_time:1115525ms step_avg:170.21ms\nstep:6565/10000 train_loss:4.0297 train_time:1115696ms step_avg:170.21ms\nstep:6566/10000 train_loss:3.9240 train_time:1115867ms step_avg:170.21ms\nstep:6567/10000 train_loss:3.8667 train_time:1116039ms step_avg:170.21ms\nstep:6568/10000 train_loss:3.8205 train_time:1116211ms step_avg:170.21ms\nstep:6569/10000 train_loss:3.6703 train_time:1116380ms step_avg:170.21ms\nstep:6570/10000 train_loss:4.1009 train_time:1116549ms step_avg:170.21ms\nstep:6571/10000 train_loss:4.0084 train_time:1116721ms step_avg:170.21ms\nstep:6572/10000 train_loss:3.8757 train_time:1116890ms step_avg:170.21ms\nstep:6573/10000 train_loss:3.7071 train_time:1117061ms step_avg:170.21ms\nstep:6574/10000 train_loss:3.9512 train_time:1117231ms step_avg:170.21ms\nstep:6575/10000 train_loss:3.8090 train_time:1117402ms step_avg:170.21ms\nstep:6576/10000 train_loss:3.7303 train_time:1117569ms step_avg:170.21ms\nstep:6577/10000 train_loss:3.7298 train_time:1117739ms step_avg:170.21ms\nstep:6578/10000 train_loss:3.8337 train_time:1117909ms step_avg:170.21ms\nstep:6579/10000 train_loss:3.8572 train_time:1118081ms step_avg:170.21ms\nstep:6580/10000 train_loss:3.7785 train_time:1118249ms step_avg:170.21ms\nstep:6581/10000 train_loss:3.8164 train_time:1118421ms step_avg:170.21ms\nstep:6582/10000 train_loss:3.9046 train_time:1118591ms step_avg:170.21ms\nstep:6583/10000 train_loss:3.9169 train_time:1118763ms step_avg:170.21ms\nstep:6584/10000 train_loss:3.8509 train_time:1118931ms step_avg:170.21ms\nstep:6585/10000 train_loss:3.7644 train_time:1119102ms step_avg:170.21ms\nstep:6586/10000 train_loss:3.8081 train_time:1119272ms step_avg:170.21ms\nstep:6587/10000 train_loss:3.7882 train_time:1119444ms step_avg:170.21ms\nstep:6588/10000 train_loss:3.7411 train_time:1119615ms step_avg:170.21ms\nstep:6589/10000 train_loss:3.9329 train_time:1119786ms step_avg:170.21ms\nstep:6590/10000 train_loss:3.8278 train_time:1119958ms step_avg:170.21ms\nstep:6591/10000 train_loss:3.9869 train_time:1120129ms step_avg:170.21ms\nstep:6592/10000 train_loss:3.8354 train_time:1120300ms step_avg:170.21ms\nstep:6593/10000 train_loss:3.7819 train_time:1120471ms step_avg:170.21ms\nstep:6594/10000 train_loss:3.8425 train_time:1120641ms step_avg:170.21ms\nstep:6595/10000 train_loss:3.7950 train_time:1120809ms step_avg:170.21ms\nstep:6596/10000 train_loss:3.9071 train_time:1120981ms step_avg:170.21ms\nstep:6597/10000 train_loss:3.7949 train_time:1121152ms step_avg:170.21ms\nstep:6598/10000 train_loss:3.6424 train_time:1121322ms step_avg:170.21ms\nstep:6599/10000 train_loss:3.8438 train_time:1121491ms step_avg:170.21ms\nstep:6600/10000 train_loss:3.7952 train_time:1121663ms step_avg:170.21ms\nstep:6601/10000 train_loss:3.7303 train_time:1121833ms step_avg:170.21ms\nstep:6602/10000 train_loss:3.8959 train_time:1122007ms step_avg:170.21ms\nstep:6603/10000 train_loss:3.8149 train_time:1122177ms step_avg:170.21ms\nstep:6604/10000 train_loss:3.8673 train_time:1122345ms step_avg:170.21ms\nstep:6605/10000 train_loss:3.8180 train_time:1122514ms step_avg:170.21ms\nstep:6606/10000 train_loss:3.7346 train_time:1122684ms step_avg:170.21ms\nstep:6607/10000 train_loss:3.8129 train_time:1122854ms step_avg:170.21ms\nstep:6608/10000 train_loss:3.8549 train_time:1123025ms step_avg:170.21ms\nstep:6609/10000 train_loss:4.0278 train_time:1123195ms step_avg:170.21ms\nstep:6610/10000 train_loss:3.8138 train_time:1123367ms step_avg:170.21ms\nstep:6611/10000 train_loss:3.8765 train_time:1123536ms step_avg:170.21ms\nstep:6612/10000 train_loss:3.8084 train_time:1123708ms step_avg:170.21ms\nstep:6613/10000 train_loss:3.9065 train_time:1123878ms step_avg:170.21ms\nstep:6614/10000 train_loss:3.7924 train_time:1124050ms step_avg:170.21ms\nstep:6615/10000 train_loss:3.8142 train_time:1124221ms step_avg:170.21ms\nstep:6616/10000 train_loss:3.9120 train_time:1124389ms step_avg:170.21ms\nstep:6617/10000 train_loss:3.6843 train_time:1124561ms step_avg:170.21ms\nstep:6618/10000 train_loss:3.8514 train_time:1124730ms step_avg:170.21ms\nstep:6619/10000 train_loss:3.9258 train_time:1124900ms step_avg:170.21ms\nstep:6620/10000 train_loss:3.7278 train_time:1125070ms step_avg:170.21ms\nstep:6621/10000 train_loss:3.7906 train_time:1125242ms step_avg:170.21ms\nstep:6622/10000 train_loss:3.8225 train_time:1125410ms step_avg:170.21ms\nstep:6623/10000 train_loss:3.8372 train_time:1125579ms step_avg:170.21ms\nstep:6624/10000 train_loss:3.5831 train_time:1125750ms step_avg:170.21ms\nstep:6625/10000 train_loss:3.8335 train_time:1125921ms step_avg:170.21ms\nstep:6626/10000 train_loss:3.9471 train_time:1126091ms step_avg:170.21ms\nstep:6627/10000 train_loss:3.7618 train_time:1126263ms step_avg:170.21ms\nstep:6628/10000 train_loss:3.8564 train_time:1126432ms step_avg:170.21ms\nstep:6629/10000 train_loss:3.7796 train_time:1126604ms step_avg:170.21ms\nstep:6630/10000 train_loss:3.9015 train_time:1126774ms step_avg:170.21ms\nstep:6631/10000 train_loss:3.7955 train_time:1126948ms step_avg:170.21ms\nstep:6632/10000 train_loss:3.8846 train_time:1127119ms step_avg:170.21ms\nstep:6633/10000 train_loss:3.8543 train_time:1127289ms step_avg:170.21ms\nstep:6634/10000 train_loss:3.7294 train_time:1127460ms step_avg:170.21ms\nstep:6635/10000 train_loss:3.9991 train_time:1127629ms step_avg:170.21ms\nstep:6636/10000 train_loss:3.8075 train_time:1127800ms step_avg:170.21ms\nstep:6637/10000 train_loss:3.8010 train_time:1127968ms step_avg:170.21ms\nstep:6638/10000 train_loss:3.8915 train_time:1128139ms step_avg:170.21ms\nstep:6639/10000 train_loss:3.8655 train_time:1128309ms step_avg:170.21ms\nstep:6640/10000 train_loss:4.0385 train_time:1128478ms step_avg:170.21ms\nstep:6641/10000 train_loss:3.9231 train_time:1128646ms step_avg:170.21ms\nstep:6642/10000 train_loss:3.7866 train_time:1128818ms step_avg:170.21ms\nstep:6643/10000 train_loss:3.9170 train_time:1128989ms step_avg:170.21ms\nstep:6644/10000 train_loss:3.8655 train_time:1129159ms step_avg:170.21ms\nstep:6645/10000 train_loss:3.8872 train_time:1129328ms step_avg:170.21ms\nstep:6646/10000 train_loss:3.7712 train_time:1129500ms step_avg:170.21ms\nstep:6647/10000 train_loss:3.7903 train_time:1129669ms step_avg:170.21ms\nstep:6648/10000 train_loss:3.8774 train_time:1129841ms step_avg:170.21ms\nstep:6649/10000 train_loss:3.9499 train_time:1130010ms step_avg:170.21ms\nstep:6650/10000 train_loss:3.8255 train_time:1130179ms step_avg:170.21ms\nstep:6651/10000 train_loss:3.9156 train_time:1130349ms step_avg:170.21ms\nstep:6652/10000 train_loss:3.8800 train_time:1130523ms step_avg:170.21ms\nstep:6653/10000 train_loss:3.8637 train_time:1130693ms step_avg:170.21ms\nstep:6654/10000 train_loss:3.8587 train_time:1130867ms step_avg:170.21ms\nstep:6655/10000 train_loss:4.0230 train_time:1131035ms step_avg:170.21ms\nstep:6656/10000 train_loss:3.8983 train_time:1131203ms step_avg:170.21ms\nstep:6657/10000 train_loss:3.7939 train_time:1131373ms step_avg:170.21ms\nstep:6658/10000 train_loss:3.7916 train_time:1131542ms step_avg:170.21ms\nstep:6659/10000 train_loss:3.8804 train_time:1131712ms step_avg:170.21ms\nstep:6660/10000 train_loss:3.7589 train_time:1131883ms step_avg:170.21ms\nstep:6661/10000 train_loss:3.9531 train_time:1132052ms step_avg:170.21ms\nstep:6662/10000 train_loss:3.8639 train_time:1132225ms step_avg:170.21ms\nstep:6663/10000 train_loss:3.9622 train_time:1132395ms step_avg:170.21ms\nstep:6664/10000 train_loss:3.8272 train_time:1132565ms step_avg:170.21ms\nstep:6665/10000 train_loss:3.7032 train_time:1132734ms step_avg:170.21ms\nstep:6666/10000 train_loss:3.8864 train_time:1132906ms step_avg:170.21ms\nstep:6667/10000 train_loss:3.9280 train_time:1133080ms step_avg:170.21ms\nstep:6668/10000 train_loss:3.7264 train_time:1133248ms step_avg:170.21ms\nstep:6669/10000 train_loss:3.8276 train_time:1133418ms step_avg:170.21ms\nstep:6670/10000 train_loss:3.9013 train_time:1133586ms step_avg:170.21ms\nstep:6671/10000 train_loss:3.8648 train_time:1133757ms step_avg:170.21ms\nstep:6672/10000 train_loss:3.8001 train_time:1133929ms step_avg:170.21ms\nstep:6673/10000 train_loss:3.7934 train_time:1134102ms step_avg:170.21ms\nstep:6674/10000 train_loss:3.9044 train_time:1134272ms step_avg:170.21ms\nstep:6675/10000 train_loss:3.8300 train_time:1134444ms step_avg:170.21ms\nstep:6676/10000 train_loss:3.8558 train_time:1134614ms step_avg:170.21ms\nstep:6677/10000 train_loss:3.8172 train_time:1134784ms step_avg:170.21ms\nstep:6678/10000 train_loss:4.0008 train_time:1134952ms step_avg:170.21ms\nstep:6679/10000 train_loss:3.8859 train_time:1135123ms step_avg:170.21ms\nstep:6680/10000 train_loss:3.6595 train_time:1135291ms step_avg:170.21ms\nstep:6681/10000 train_loss:3.7234 train_time:1135457ms step_avg:170.21ms\nstep:6682/10000 train_loss:3.7568 train_time:1135631ms step_avg:170.21ms\nstep:6683/10000 train_loss:3.7397 train_time:1135800ms step_avg:170.21ms\nstep:6684/10000 train_loss:3.8840 train_time:1135971ms step_avg:170.21ms\nstep:6685/10000 train_loss:3.8944 train_time:1136146ms step_avg:170.21ms\nstep:6686/10000 train_loss:3.8055 train_time:1136311ms step_avg:170.21ms\nstep:6687/10000 train_loss:3.9010 train_time:1136480ms step_avg:170.21ms\nstep:6688/10000 train_loss:3.6912 train_time:1136650ms step_avg:170.21ms\nstep:6689/10000 train_loss:3.7913 train_time:1136819ms step_avg:170.21ms\nstep:6690/10000 train_loss:3.7693 train_time:1136991ms step_avg:170.21ms\nstep:6691/10000 train_loss:3.8819 train_time:1137161ms step_avg:170.21ms\nstep:6692/10000 train_loss:3.9775 train_time:1137330ms step_avg:170.21ms\nstep:6693/10000 train_loss:3.8226 train_time:1137502ms step_avg:170.21ms\nstep:6694/10000 train_loss:3.7404 train_time:1137671ms step_avg:170.21ms\nstep:6695/10000 train_loss:3.7263 train_time:1137841ms step_avg:170.21ms\nstep:6696/10000 train_loss:3.8485 train_time:1138013ms step_avg:170.21ms\nstep:6697/10000 train_loss:3.8021 train_time:1138181ms step_avg:170.21ms\nstep:6698/10000 train_loss:3.9192 train_time:1138352ms step_avg:170.21ms\nstep:6699/10000 train_loss:3.9517 train_time:1138523ms step_avg:170.21ms\nstep:6700/10000 train_loss:3.8262 train_time:1138692ms step_avg:170.21ms\nstep:6701/10000 train_loss:3.9979 train_time:1138862ms step_avg:170.21ms\nstep:6702/10000 train_loss:4.0751 train_time:1139034ms step_avg:170.21ms\nstep:6703/10000 train_loss:3.6802 train_time:1139204ms step_avg:170.21ms\nstep:6704/10000 train_loss:3.6999 train_time:1139369ms step_avg:170.21ms\nstep:6705/10000 train_loss:3.8447 train_time:1139538ms step_avg:170.21ms\nstep:6706/10000 train_loss:3.8251 train_time:1139706ms step_avg:170.21ms\nstep:6707/10000 train_loss:3.8965 train_time:1139876ms step_avg:170.21ms\nstep:6708/10000 train_loss:3.7821 train_time:1140048ms step_avg:170.21ms\nstep:6709/10000 train_loss:3.7232 train_time:1140218ms step_avg:170.21ms\nstep:6710/10000 train_loss:3.7834 train_time:1140386ms step_avg:170.21ms\nstep:6711/10000 train_loss:3.8524 train_time:1140556ms step_avg:170.21ms\nstep:6712/10000 train_loss:4.0951 train_time:1140725ms step_avg:170.21ms\nstep:6713/10000 train_loss:3.8920 train_time:1140894ms step_avg:170.21ms\nstep:6714/10000 train_loss:3.7065 train_time:1141075ms step_avg:170.21ms\nstep:6715/10000 train_loss:3.7820 train_time:1141235ms step_avg:170.21ms\nstep:6716/10000 train_loss:3.8547 train_time:1141404ms step_avg:170.21ms\nstep:6717/10000 train_loss:3.7864 train_time:1141576ms step_avg:170.21ms\nstep:6718/10000 train_loss:3.8867 train_time:1141748ms step_avg:170.21ms\nstep:6719/10000 train_loss:3.7587 train_time:1141919ms step_avg:170.21ms\nstep:6720/10000 train_loss:3.8142 train_time:1142089ms step_avg:170.21ms\nstep:6721/10000 train_loss:3.9379 train_time:1142260ms step_avg:170.21ms\nstep:6722/10000 train_loss:3.7995 train_time:1142430ms step_avg:170.21ms\nstep:6723/10000 train_loss:4.0343 train_time:1142601ms step_avg:170.21ms\nstep:6724/10000 train_loss:3.9335 train_time:1142772ms step_avg:170.21ms\nstep:6725/10000 train_loss:3.9926 train_time:1142945ms step_avg:170.21ms\nstep:6726/10000 train_loss:3.8156 train_time:1143116ms step_avg:170.21ms\nstep:6727/10000 train_loss:3.8524 train_time:1143289ms step_avg:170.21ms\nstep:6728/10000 train_loss:3.8882 train_time:1143452ms step_avg:170.21ms\nstep:6729/10000 train_loss:3.9430 train_time:1143624ms step_avg:170.21ms\nstep:6730/10000 train_loss:3.8056 train_time:1143793ms step_avg:170.21ms\nstep:6731/10000 train_loss:3.7340 train_time:1143963ms step_avg:170.21ms\nstep:6732/10000 train_loss:3.7797 train_time:1144137ms step_avg:170.21ms\nstep:6733/10000 train_loss:3.7976 train_time:1144308ms step_avg:170.21ms\nstep:6734/10000 train_loss:3.6482 train_time:1144478ms step_avg:170.21ms\nstep:6735/10000 train_loss:3.7625 train_time:1144645ms step_avg:170.21ms\nstep:6736/10000 train_loss:3.8720 train_time:1144814ms step_avg:170.21ms\nstep:6737/10000 train_loss:3.8584 train_time:1144984ms step_avg:170.21ms\nstep:6738/10000 train_loss:3.8049 train_time:1145151ms step_avg:170.21ms\nstep:6739/10000 train_loss:3.7738 train_time:1145320ms step_avg:170.21ms\nstep:6740/10000 train_loss:3.7131 train_time:1145489ms step_avg:170.21ms\nstep:6741/10000 train_loss:3.8055 train_time:1145660ms step_avg:170.21ms\nstep:6742/10000 train_loss:3.8424 train_time:1145830ms step_avg:170.21ms\nstep:6743/10000 train_loss:3.7825 train_time:1146002ms step_avg:170.21ms\nstep:6744/10000 train_loss:3.8698 train_time:1146167ms step_avg:170.21ms\nstep:6745/10000 train_loss:3.7930 train_time:1146337ms step_avg:170.21ms\nstep:6746/10000 train_loss:3.9755 train_time:1146505ms step_avg:170.21ms\nstep:6747/10000 train_loss:3.6816 train_time:1146677ms step_avg:170.21ms\nstep:6748/10000 train_loss:3.7606 train_time:1146848ms step_avg:170.21ms\nstep:6749/10000 train_loss:3.8002 train_time:1147022ms step_avg:170.21ms\nstep:6750/10000 train_loss:3.8816 train_time:1147192ms step_avg:170.21ms\nstep:6751/10000 train_loss:3.8888 train_time:1147359ms step_avg:170.21ms\nstep:6752/10000 train_loss:3.6446 train_time:1147527ms step_avg:170.21ms\nstep:6753/10000 train_loss:3.8678 train_time:1147699ms step_avg:170.21ms\nstep:6754/10000 train_loss:3.7996 train_time:1147871ms step_avg:170.21ms\nstep:6755/10000 train_loss:3.7814 train_time:1148057ms step_avg:170.21ms\nstep:6756/10000 train_loss:4.0559 train_time:1148213ms step_avg:170.21ms\nstep:6757/10000 train_loss:3.8408 train_time:1148383ms step_avg:170.21ms\nstep:6758/10000 train_loss:3.8768 train_time:1148553ms step_avg:170.21ms\nstep:6759/10000 train_loss:3.7377 train_time:1148726ms step_avg:170.21ms\nstep:6760/10000 train_loss:3.8683 train_time:1148895ms step_avg:170.21ms\nstep:6761/10000 train_loss:3.8837 train_time:1149066ms step_avg:170.21ms\nstep:6762/10000 train_loss:3.7619 train_time:1149233ms step_avg:170.21ms\nstep:6763/10000 train_loss:3.7627 train_time:1149402ms step_avg:170.21ms\nstep:6764/10000 train_loss:3.8548 train_time:1149572ms step_avg:170.21ms\nstep:6765/10000 train_loss:3.7950 train_time:1149746ms step_avg:170.21ms\nstep:6766/10000 train_loss:3.8829 train_time:1149917ms step_avg:170.21ms\nstep:6767/10000 train_loss:3.8355 train_time:1150088ms step_avg:170.21ms\nstep:6768/10000 train_loss:3.7857 train_time:1150260ms step_avg:170.21ms\nstep:6769/10000 train_loss:3.9581 train_time:1150430ms step_avg:170.21ms\nstep:6770/10000 train_loss:3.7590 train_time:1150602ms step_avg:170.21ms\nstep:6771/10000 train_loss:3.8186 train_time:1150771ms step_avg:170.21ms\nstep:6772/10000 train_loss:3.7238 train_time:1150943ms step_avg:170.21ms\nstep:6773/10000 train_loss:3.8114 train_time:1151116ms step_avg:170.21ms\nstep:6774/10000 train_loss:3.7155 train_time:1151280ms step_avg:170.21ms\nstep:6775/10000 train_loss:3.7976 train_time:1151448ms step_avg:170.21ms\nstep:6776/10000 train_loss:3.8390 train_time:1151621ms step_avg:170.21ms\nstep:6777/10000 train_loss:3.6411 train_time:1151791ms step_avg:170.21ms\nstep:6778/10000 train_loss:3.7943 train_time:1151964ms step_avg:170.21ms\nstep:6779/10000 train_loss:3.7874 train_time:1152135ms step_avg:170.21ms\nstep:6780/10000 train_loss:3.7950 train_time:1152303ms step_avg:170.21ms\nstep:6781/10000 train_loss:3.7790 train_time:1152472ms step_avg:170.21ms\nstep:6782/10000 train_loss:3.7999 train_time:1152643ms step_avg:170.21ms\nstep:6783/10000 train_loss:3.8443 train_time:1152815ms step_avg:170.21ms\nstep:6784/10000 train_loss:3.8548 train_time:1152988ms step_avg:170.21ms\nstep:6785/10000 train_loss:3.7984 train_time:1153160ms step_avg:170.21ms\nstep:6786/10000 train_loss:3.7361 train_time:1153327ms step_avg:170.21ms\nstep:6787/10000 train_loss:3.9344 train_time:1153499ms step_avg:170.21ms\nstep:6788/10000 train_loss:3.8604 train_time:1153670ms step_avg:170.21ms\nstep:6789/10000 train_loss:3.7485 train_time:1153841ms step_avg:170.21ms\nstep:6790/10000 train_loss:4.1644 train_time:1154011ms step_avg:170.21ms\nstep:6791/10000 train_loss:3.8474 train_time:1154183ms step_avg:170.21ms\nstep:6792/10000 train_loss:3.9002 train_time:1154354ms step_avg:170.21ms\nstep:6793/10000 train_loss:3.8440 train_time:1154526ms step_avg:170.21ms\nstep:6794/10000 train_loss:3.8916 train_time:1154695ms step_avg:170.21ms\nstep:6795/10000 train_loss:3.8123 train_time:1154866ms step_avg:170.21ms\nstep:6796/10000 train_loss:3.8404 train_time:1155037ms step_avg:170.21ms\nstep:6797/10000 train_loss:3.8427 train_time:1155207ms step_avg:170.21ms\nstep:6798/10000 train_loss:3.8626 train_time:1155378ms step_avg:170.21ms\nstep:6799/10000 train_loss:3.7325 train_time:1155550ms step_avg:170.21ms\nstep:6800/10000 train_loss:3.7861 train_time:1155721ms step_avg:170.21ms\nstep:6801/10000 train_loss:3.8218 train_time:1155888ms step_avg:170.21ms\nstep:6802/10000 train_loss:3.7846 train_time:1156058ms step_avg:170.21ms\nstep:6803/10000 train_loss:3.9599 train_time:1156227ms step_avg:170.21ms\nstep:6804/10000 train_loss:3.7132 train_time:1156399ms step_avg:170.21ms\nstep:6805/10000 train_loss:3.8759 train_time:1156567ms step_avg:170.21ms\nstep:6806/10000 train_loss:3.7458 train_time:1156738ms step_avg:170.21ms\nstep:6807/10000 train_loss:3.7872 train_time:1156905ms step_avg:170.21ms\nstep:6808/10000 train_loss:3.7513 train_time:1157077ms step_avg:170.21ms\nstep:6809/10000 train_loss:3.7755 train_time:1157249ms step_avg:170.21ms\nstep:6810/10000 train_loss:3.8129 train_time:1157421ms step_avg:170.21ms\nstep:6811/10000 train_loss:3.8391 train_time:1157588ms step_avg:170.21ms\nstep:6812/10000 train_loss:3.8012 train_time:1157757ms step_avg:170.21ms\nstep:6813/10000 train_loss:3.8258 train_time:1157925ms step_avg:170.21ms\nstep:6814/10000 train_loss:3.7629 train_time:1158093ms step_avg:170.21ms\nstep:6815/10000 train_loss:3.7928 train_time:1158265ms step_avg:170.21ms\nstep:6816/10000 train_loss:3.8103 train_time:1158435ms step_avg:170.21ms\nstep:6817/10000 train_loss:4.0411 train_time:1158606ms step_avg:170.21ms\nstep:6818/10000 train_loss:3.8320 train_time:1158777ms step_avg:170.21ms\nstep:6819/10000 train_loss:3.8635 train_time:1158947ms step_avg:170.21ms\nstep:6820/10000 train_loss:3.8283 train_time:1159119ms step_avg:170.21ms\nstep:6821/10000 train_loss:3.8926 train_time:1159290ms step_avg:170.21ms\nstep:6822/10000 train_loss:3.7662 train_time:1159460ms step_avg:170.21ms\nstep:6823/10000 train_loss:3.8813 train_time:1159632ms step_avg:170.21ms\nstep:6824/10000 train_loss:3.8700 train_time:1159804ms step_avg:170.21ms\nstep:6825/10000 train_loss:3.8113 train_time:1159973ms step_avg:170.21ms\nstep:6826/10000 train_loss:3.9897 train_time:1160143ms step_avg:170.21ms\nstep:6827/10000 train_loss:3.7625 train_time:1160313ms step_avg:170.21ms\nstep:6828/10000 train_loss:3.7697 train_time:1160484ms step_avg:170.21ms\nstep:6829/10000 train_loss:3.8138 train_time:1160654ms step_avg:170.21ms\nstep:6830/10000 train_loss:3.7375 train_time:1160825ms step_avg:170.21ms\nstep:6831/10000 train_loss:3.7597 train_time:1160997ms step_avg:170.21ms\nstep:6832/10000 train_loss:3.9157 train_time:1161168ms step_avg:170.21ms\nstep:6833/10000 train_loss:3.8792 train_time:1161338ms step_avg:170.21ms\nstep:6834/10000 train_loss:3.9621 train_time:1161507ms step_avg:170.21ms\nstep:6835/10000 train_loss:3.8604 train_time:1161678ms step_avg:170.21ms\nstep:6836/10000 train_loss:3.6834 train_time:1161847ms step_avg:170.21ms\nstep:6837/10000 train_loss:3.8728 train_time:1162018ms step_avg:170.21ms\nstep:6838/10000 train_loss:3.8271 train_time:1162187ms step_avg:170.21ms\nstep:6839/10000 train_loss:3.5936 train_time:1162357ms step_avg:170.21ms\nstep:6840/10000 train_loss:3.8146 train_time:1162528ms step_avg:170.21ms\nstep:6841/10000 train_loss:3.8754 train_time:1162697ms step_avg:170.21ms\nstep:6842/10000 train_loss:3.5492 train_time:1162868ms step_avg:170.21ms\nstep:6843/10000 train_loss:3.9232 train_time:1163040ms step_avg:170.21ms\nstep:6844/10000 train_loss:3.7589 train_time:1163210ms step_avg:170.21ms\nstep:6845/10000 train_loss:3.6411 train_time:1163380ms step_avg:170.21ms\nstep:6846/10000 train_loss:3.9216 train_time:1163548ms step_avg:170.21ms\nstep:6847/10000 train_loss:3.8621 train_time:1163718ms step_avg:170.21ms\nstep:6848/10000 train_loss:3.7472 train_time:1163888ms step_avg:170.21ms\nstep:6849/10000 train_loss:3.7188 train_time:1164061ms step_avg:170.21ms\nstep:6850/10000 train_loss:3.8597 train_time:1164232ms step_avg:170.21ms\nstep:6851/10000 train_loss:3.8980 train_time:1164407ms step_avg:170.21ms\nstep:6852/10000 train_loss:4.2444 train_time:1164575ms step_avg:170.21ms\nstep:6853/10000 train_loss:3.8691 train_time:1164743ms step_avg:170.21ms\nstep:6854/10000 train_loss:3.8489 train_time:1164912ms step_avg:170.21ms\nstep:6855/10000 train_loss:4.1962 train_time:1165086ms step_avg:170.21ms\nstep:6856/10000 train_loss:3.8133 train_time:1165257ms step_avg:170.21ms\nstep:6857/10000 train_loss:3.6659 train_time:1165424ms step_avg:170.21ms\nstep:6858/10000 train_loss:3.8149 train_time:1165810ms step_avg:170.24ms\nstep:6859/10000 train_loss:3.8144 train_time:1165975ms step_avg:170.24ms\nstep:6860/10000 train_loss:3.8091 train_time:1166145ms step_avg:170.24ms\nstep:6861/10000 train_loss:3.6562 train_time:1166313ms step_avg:170.24ms\nstep:6862/10000 train_loss:4.0488 train_time:1166481ms step_avg:170.24ms\nstep:6863/10000 train_loss:3.7310 train_time:1166648ms step_avg:170.24ms\nstep:6864/10000 train_loss:3.8687 train_time:1166829ms step_avg:170.24ms\nstep:6865/10000 train_loss:3.7696 train_time:1166998ms step_avg:170.24ms\nstep:6866/10000 train_loss:3.8254 train_time:1167165ms step_avg:170.24ms\nstep:6867/10000 train_loss:3.9364 train_time:1167334ms step_avg:170.24ms\nstep:6868/10000 train_loss:3.8797 train_time:1167506ms step_avg:170.24ms\nstep:6869/10000 train_loss:3.8045 train_time:1167676ms step_avg:170.24ms\nstep:6870/10000 train_loss:3.8390 train_time:1167849ms step_avg:170.24ms\nstep:6871/10000 train_loss:3.8410 train_time:1168023ms step_avg:170.24ms\nstep:6872/10000 train_loss:3.8378 train_time:1168190ms step_avg:170.24ms\nstep:6873/10000 train_loss:3.7802 train_time:1168359ms step_avg:170.24ms\nstep:6874/10000 train_loss:3.8986 train_time:1168525ms step_avg:170.24ms\nstep:6875/10000 train_loss:4.0408 train_time:1168695ms step_avg:170.24ms\nstep:6876/10000 train_loss:3.7732 train_time:1168870ms step_avg:170.24ms\nstep:6877/10000 train_loss:3.9848 train_time:1169045ms step_avg:170.24ms\nstep:6878/10000 train_loss:3.9704 train_time:1169213ms step_avg:170.24ms\nstep:6879/10000 train_loss:3.7754 train_time:1169384ms step_avg:170.24ms\nstep:6880/10000 train_loss:3.6623 train_time:1169554ms step_avg:170.24ms\nstep:6881/10000 train_loss:3.7924 train_time:1169721ms step_avg:170.24ms\nstep:6882/10000 train_loss:3.9859 train_time:1169892ms step_avg:170.24ms\nstep:6883/10000 train_loss:3.6640 train_time:1170066ms step_avg:170.24ms\nstep:6884/10000 train_loss:3.7723 train_time:1170236ms step_avg:170.24ms\nstep:6885/10000 train_loss:3.8512 train_time:1170407ms step_avg:170.24ms\nstep:6886/10000 train_loss:3.8402 train_time:1170578ms step_avg:170.24ms\nstep:6887/10000 train_loss:3.8822 train_time:1170749ms step_avg:170.24ms\nstep:6888/10000 train_loss:3.8017 train_time:1170920ms step_avg:170.24ms\nstep:6889/10000 train_loss:3.8151 train_time:1171090ms step_avg:170.24ms\nstep:6890/10000 train_loss:3.8108 train_time:1171262ms step_avg:170.24ms\nstep:6891/10000 train_loss:3.8050 train_time:1171432ms step_avg:170.24ms\nstep:6892/10000 train_loss:3.7509 train_time:1171603ms step_avg:170.24ms\nstep:6893/10000 train_loss:3.9453 train_time:1171772ms step_avg:170.24ms\nstep:6894/10000 train_loss:3.8022 train_time:1171943ms step_avg:170.24ms\nstep:6895/10000 train_loss:3.7586 train_time:1172111ms step_avg:170.24ms\nstep:6896/10000 train_loss:3.8843 train_time:1172283ms step_avg:170.24ms\nstep:6897/10000 train_loss:3.8740 train_time:1172453ms step_avg:170.24ms\nstep:6898/10000 train_loss:3.4117 train_time:1172625ms step_avg:170.24ms\nstep:6899/10000 train_loss:3.7557 train_time:1172794ms step_avg:170.24ms\nstep:6900/10000 train_loss:3.7250 train_time:1172966ms step_avg:170.24ms\nstep:6901/10000 train_loss:3.7908 train_time:1173134ms step_avg:170.24ms\nstep:6902/10000 train_loss:3.7213 train_time:1173303ms step_avg:170.24ms\nstep:6903/10000 train_loss:3.9154 train_time:1173472ms step_avg:170.24ms\nstep:6904/10000 train_loss:3.8434 train_time:1173644ms step_avg:170.24ms\nstep:6905/10000 train_loss:3.8906 train_time:1173812ms step_avg:170.24ms\nstep:6906/10000 train_loss:3.8512 train_time:1173983ms step_avg:170.24ms\nstep:6907/10000 train_loss:3.8869 train_time:1174150ms step_avg:170.24ms\nstep:6908/10000 train_loss:3.9318 train_time:1174320ms step_avg:170.24ms\nstep:6909/10000 train_loss:3.9659 train_time:1174491ms step_avg:170.24ms\nstep:6910/10000 train_loss:3.7963 train_time:1174664ms step_avg:170.24ms\nstep:6911/10000 train_loss:3.7410 train_time:1174837ms step_avg:170.24ms\nstep:6912/10000 train_loss:3.7885 train_time:1175010ms step_avg:170.24ms\nstep:6913/10000 train_loss:3.7225 train_time:1175182ms step_avg:170.24ms\nstep:6914/10000 train_loss:3.8637 train_time:1175349ms step_avg:170.24ms\nstep:6915/10000 train_loss:4.0096 train_time:1175520ms step_avg:170.24ms\nstep:6916/10000 train_loss:4.0170 train_time:1175690ms step_avg:170.24ms\nstep:6917/10000 train_loss:3.8365 train_time:1175862ms step_avg:170.24ms\nstep:6918/10000 train_loss:3.8022 train_time:1176033ms step_avg:170.24ms\nstep:6919/10000 train_loss:3.7396 train_time:1176199ms step_avg:170.24ms\nstep:6920/10000 train_loss:3.8002 train_time:1176369ms step_avg:170.24ms\nstep:6921/10000 train_loss:3.8664 train_time:1176540ms step_avg:170.24ms\nstep:6922/10000 train_loss:3.7748 train_time:1176709ms step_avg:170.24ms\nstep:6923/10000 train_loss:3.7863 train_time:1176882ms step_avg:170.24ms\nstep:6924/10000 train_loss:3.8569 train_time:1177053ms step_avg:170.24ms\nstep:6925/10000 train_loss:3.8966 train_time:1177222ms step_avg:170.24ms\nstep:6926/10000 train_loss:3.7993 train_time:1177393ms step_avg:170.24ms\nstep:6927/10000 train_loss:3.7581 train_time:1177567ms step_avg:170.24ms\nstep:6928/10000 train_loss:3.8289 train_time:1177736ms step_avg:170.24ms\nstep:6929/10000 train_loss:3.7860 train_time:1177907ms step_avg:170.24ms\nstep:6930/10000 train_loss:3.9510 train_time:1178079ms step_avg:170.24ms\nstep:6931/10000 train_loss:3.8748 train_time:1178248ms step_avg:170.24ms\nstep:6932/10000 train_loss:3.8557 train_time:1178417ms step_avg:170.24ms\nstep:6933/10000 train_loss:4.0364 train_time:1178585ms step_avg:170.24ms\nstep:6934/10000 train_loss:3.7382 train_time:1178755ms step_avg:170.24ms\nstep:6935/10000 train_loss:3.7349 train_time:1178925ms step_avg:170.24ms\nstep:6936/10000 train_loss:4.2289 train_time:1179099ms step_avg:170.24ms\nstep:6937/10000 train_loss:3.7616 train_time:1179272ms step_avg:170.24ms\nstep:6938/10000 train_loss:3.8101 train_time:1179440ms step_avg:170.24ms\nstep:6939/10000 train_loss:4.0469 train_time:1179610ms step_avg:170.24ms\nstep:6940/10000 train_loss:3.8118 train_time:1179784ms step_avg:170.24ms\nstep:6941/10000 train_loss:3.8563 train_time:1179952ms step_avg:170.24ms\nstep:6942/10000 train_loss:3.8041 train_time:1180125ms step_avg:170.24ms\nstep:6943/10000 train_loss:3.7753 train_time:1180292ms step_avg:170.24ms\nstep:6944/10000 train_loss:3.9426 train_time:1180464ms step_avg:170.24ms\nstep:6945/10000 train_loss:3.8082 train_time:1180634ms step_avg:170.24ms\nstep:6946/10000 train_loss:3.7742 train_time:1180804ms step_avg:170.24ms\nstep:6947/10000 train_loss:3.9237 train_time:1180976ms step_avg:170.24ms\nstep:6948/10000 train_loss:3.7300 train_time:1181147ms step_avg:170.24ms\nstep:6949/10000 train_loss:3.9078 train_time:1181318ms step_avg:170.24ms\nstep:6950/10000 train_loss:3.7434 train_time:1181487ms step_avg:170.24ms\nstep:6951/10000 train_loss:3.7978 train_time:1181657ms step_avg:170.24ms\nstep:6952/10000 train_loss:3.7651 train_time:1181828ms step_avg:170.24ms\nstep:6953/10000 train_loss:3.7368 train_time:1182002ms step_avg:170.24ms\nstep:6954/10000 train_loss:3.9219 train_time:1182169ms step_avg:170.24ms\nstep:6955/10000 train_loss:4.6620 train_time:1182341ms step_avg:170.24ms\nstep:6956/10000 train_loss:3.7953 train_time:1182509ms step_avg:170.24ms\nstep:6957/10000 train_loss:3.8412 train_time:1182677ms step_avg:170.24ms\nstep:6958/10000 train_loss:3.8098 train_time:1182847ms step_avg:170.24ms\nstep:6959/10000 train_loss:3.7858 train_time:1183020ms step_avg:170.24ms\nstep:6960/10000 train_loss:3.7712 train_time:1183190ms step_avg:170.24ms\nstep:6961/10000 train_loss:4.0812 train_time:1183355ms step_avg:170.24ms\nstep:6962/10000 train_loss:3.9256 train_time:1183528ms step_avg:170.24ms\nstep:6963/10000 train_loss:3.7601 train_time:1183699ms step_avg:170.24ms\nstep:6964/10000 train_loss:3.9371 train_time:1183869ms step_avg:170.24ms\nstep:6965/10000 train_loss:4.0583 train_time:1184040ms step_avg:170.24ms\nstep:6966/10000 train_loss:3.8185 train_time:1184209ms step_avg:170.24ms\nstep:6967/10000 train_loss:3.8179 train_time:1184381ms step_avg:170.24ms\nstep:6968/10000 train_loss:3.7817 train_time:1184549ms step_avg:170.24ms\nstep:6969/10000 train_loss:3.6167 train_time:1184719ms step_avg:170.24ms\nstep:6970/10000 train_loss:3.9096 train_time:1184889ms step_avg:170.24ms\nstep:6971/10000 train_loss:3.7063 train_time:1185070ms step_avg:170.24ms\nstep:6972/10000 train_loss:3.8680 train_time:1185227ms step_avg:170.24ms\nstep:6973/10000 train_loss:3.7719 train_time:1185400ms step_avg:170.24ms\nstep:6974/10000 train_loss:3.9089 train_time:1185569ms step_avg:170.24ms\nstep:6975/10000 train_loss:3.7732 train_time:1185739ms step_avg:170.24ms\nstep:6976/10000 train_loss:3.8797 train_time:1185908ms step_avg:170.24ms\nstep:6977/10000 train_loss:3.8020 train_time:1186079ms step_avg:170.24ms\nstep:6978/10000 train_loss:3.8938 train_time:1186246ms step_avg:170.24ms\nstep:6979/10000 train_loss:3.6970 train_time:1186419ms step_avg:170.24ms\nstep:6980/10000 train_loss:3.7113 train_time:1186589ms step_avg:170.24ms\nstep:6981/10000 train_loss:3.6623 train_time:1186760ms step_avg:170.24ms\nstep:6982/10000 train_loss:3.9016 train_time:1186930ms step_avg:170.24ms\nstep:6983/10000 train_loss:3.7492 train_time:1187115ms step_avg:170.24ms\nstep:6984/10000 train_loss:3.5797 train_time:1187279ms step_avg:170.24ms\nstep:6985/10000 train_loss:3.9354 train_time:1187447ms step_avg:170.24ms\nstep:6986/10000 train_loss:3.9252 train_time:1187616ms step_avg:170.24ms\nstep:6987/10000 train_loss:3.7815 train_time:1187785ms step_avg:170.24ms\nstep:6988/10000 train_loss:3.5230 train_time:1187956ms step_avg:170.24ms\nstep:6989/10000 train_loss:3.6280 train_time:1188128ms step_avg:170.24ms\nstep:6990/10000 train_loss:3.7521 train_time:1188295ms step_avg:170.24ms\nstep:6991/10000 train_loss:3.8755 train_time:1188464ms step_avg:170.24ms\nstep:6992/10000 train_loss:3.8263 train_time:1188634ms step_avg:170.24ms\nstep:6993/10000 train_loss:3.8224 train_time:1188801ms step_avg:170.24ms\nstep:6994/10000 train_loss:3.7430 train_time:1188971ms step_avg:170.24ms\nstep:6995/10000 train_loss:3.7370 train_time:1189141ms step_avg:170.24ms\nstep:6996/10000 train_loss:3.8300 train_time:1189310ms step_avg:170.24ms\nstep:6997/10000 train_loss:3.8783 train_time:1189482ms step_avg:170.24ms\nstep:6998/10000 train_loss:3.8540 train_time:1189653ms step_avg:170.24ms\nstep:6999/10000 train_loss:3.7928 train_time:1189824ms step_avg:170.24ms\nstep:7000/10000 train_loss:3.8254 train_time:1189994ms step_avg:170.24ms\nstep:7001/10000 train_loss:3.8688 train_time:1190165ms step_avg:170.24ms\nstep:7002/10000 train_loss:3.8914 train_time:1190336ms step_avg:170.24ms\nstep:7003/10000 train_loss:3.8591 train_time:1190507ms step_avg:170.24ms\nstep:7004/10000 train_loss:3.7816 train_time:1190678ms step_avg:170.24ms\nstep:7005/10000 train_loss:3.7613 train_time:1190848ms step_avg:170.24ms\nstep:7006/10000 train_loss:3.8410 train_time:1191021ms step_avg:170.24ms\nstep:7007/10000 train_loss:3.8491 train_time:1191191ms step_avg:170.24ms\nstep:7008/10000 train_loss:4.0703 train_time:1191360ms step_avg:170.24ms\nstep:7009/10000 train_loss:3.7860 train_time:1191529ms step_avg:170.24ms\nstep:7010/10000 train_loss:3.8713 train_time:1191698ms step_avg:170.24ms\nstep:7011/10000 train_loss:4.1050 train_time:1191867ms step_avg:170.24ms\nstep:7012/10000 train_loss:3.7671 train_time:1192038ms step_avg:170.24ms\nstep:7013/10000 train_loss:3.8330 train_time:1192208ms step_avg:170.24ms\nstep:7014/10000 train_loss:3.7935 train_time:1192380ms step_avg:170.24ms\nstep:7015/10000 train_loss:4.0106 train_time:1192549ms step_avg:170.24ms\nstep:7016/10000 train_loss:3.8580 train_time:1192719ms step_avg:170.24ms\nstep:7017/10000 train_loss:3.9003 train_time:1192889ms step_avg:170.24ms\nstep:7018/10000 train_loss:3.8043 train_time:1193063ms step_avg:170.24ms\nstep:7019/10000 train_loss:3.7698 train_time:1193229ms step_avg:170.24ms\nstep:7020/10000 train_loss:3.8188 train_time:1193400ms step_avg:170.24ms\nstep:7021/10000 train_loss:3.7988 train_time:1193572ms step_avg:170.24ms\nstep:7022/10000 train_loss:3.8211 train_time:1193743ms step_avg:170.24ms\nstep:7023/10000 train_loss:3.9678 train_time:1193913ms step_avg:170.24ms\nstep:7024/10000 train_loss:3.4848 train_time:1194084ms step_avg:170.24ms\nstep:7025/10000 train_loss:3.9270 train_time:1194254ms step_avg:170.24ms\nstep:7026/10000 train_loss:3.9606 train_time:1194426ms step_avg:170.24ms\nstep:7027/10000 train_loss:3.8854 train_time:1194596ms step_avg:170.24ms\nstep:7028/10000 train_loss:3.6131 train_time:1194768ms step_avg:170.24ms\nstep:7029/10000 train_loss:3.8238 train_time:1194940ms step_avg:170.24ms\nstep:7030/10000 train_loss:3.8689 train_time:1195108ms step_avg:170.24ms\nstep:7031/10000 train_loss:3.9465 train_time:1195281ms step_avg:170.24ms\nstep:7032/10000 train_loss:3.7014 train_time:1195449ms step_avg:170.24ms\nstep:7033/10000 train_loss:3.9125 train_time:1195620ms step_avg:170.24ms\nstep:7034/10000 train_loss:3.9044 train_time:1195789ms step_avg:170.24ms\nstep:7035/10000 train_loss:3.7305 train_time:1195961ms step_avg:170.24ms\nstep:7036/10000 train_loss:3.6242 train_time:1196133ms step_avg:170.24ms\nstep:7037/10000 train_loss:3.8023 train_time:1196302ms step_avg:170.24ms\nstep:7038/10000 train_loss:3.8321 train_time:1196472ms step_avg:170.24ms\nstep:7039/10000 train_loss:3.7649 train_time:1196645ms step_avg:170.24ms\nstep:7040/10000 train_loss:3.7895 train_time:1196814ms step_avg:170.24ms\nstep:7041/10000 train_loss:3.7797 train_time:1196982ms step_avg:170.24ms\nstep:7042/10000 train_loss:3.7882 train_time:1197151ms step_avg:170.24ms\nstep:7043/10000 train_loss:3.7149 train_time:1197321ms step_avg:170.24ms\nstep:7044/10000 train_loss:3.6937 train_time:1197490ms step_avg:170.24ms\nstep:7045/10000 train_loss:3.7707 train_time:1197666ms step_avg:170.24ms\nstep:7046/10000 train_loss:3.9169 train_time:1197840ms step_avg:170.24ms\nstep:7047/10000 train_loss:3.8120 train_time:1198013ms step_avg:170.24ms\nstep:7048/10000 train_loss:3.6563 train_time:1198181ms step_avg:170.24ms\nstep:7049/10000 train_loss:3.6659 train_time:1198349ms step_avg:170.24ms\nstep:7050/10000 train_loss:3.7917 train_time:1198518ms step_avg:170.24ms\nstep:7051/10000 train_loss:3.8850 train_time:1198691ms step_avg:170.24ms\nstep:7052/10000 train_loss:4.7217 train_time:1198863ms step_avg:170.24ms\nstep:7053/10000 train_loss:3.8037 train_time:1199035ms step_avg:170.24ms\nstep:7054/10000 train_loss:3.8350 train_time:1199207ms step_avg:170.25ms\nstep:7055/10000 train_loss:3.7968 train_time:1199374ms step_avg:170.24ms\nstep:7056/10000 train_loss:3.7867 train_time:1199545ms step_avg:170.24ms\nstep:7057/10000 train_loss:3.8401 train_time:1199715ms step_avg:170.24ms\nstep:7058/10000 train_loss:3.8010 train_time:1199886ms step_avg:170.24ms\nstep:7059/10000 train_loss:3.9194 train_time:1200059ms step_avg:170.25ms\nstep:7060/10000 train_loss:3.8033 train_time:1200226ms step_avg:170.24ms\nstep:7061/10000 train_loss:3.9618 train_time:1200397ms step_avg:170.24ms\nstep:7062/10000 train_loss:3.9237 train_time:1200565ms step_avg:170.24ms\nstep:7063/10000 train_loss:3.7346 train_time:1200738ms step_avg:170.25ms\nstep:7064/10000 train_loss:3.8341 train_time:1200908ms step_avg:170.24ms\nstep:7065/10000 train_loss:3.8166 train_time:1201081ms step_avg:170.25ms\nstep:7066/10000 train_loss:3.8092 train_time:1201247ms step_avg:170.24ms\nstep:7067/10000 train_loss:3.7372 train_time:1201418ms step_avg:170.24ms\nstep:7068/10000 train_loss:3.8562 train_time:1201589ms step_avg:170.24ms\nstep:7069/10000 train_loss:3.8325 train_time:1201761ms step_avg:170.25ms\nstep:7070/10000 train_loss:3.7605 train_time:1201932ms step_avg:170.25ms\nstep:7071/10000 train_loss:3.8372 train_time:1202103ms step_avg:170.25ms\nstep:7072/10000 train_loss:3.7588 train_time:1202272ms step_avg:170.25ms\nstep:7073/10000 train_loss:3.7707 train_time:1202445ms step_avg:170.25ms\nstep:7074/10000 train_loss:3.8398 train_time:1202614ms step_avg:170.25ms\nstep:7075/10000 train_loss:3.8753 train_time:1202781ms step_avg:170.25ms\nstep:7076/10000 train_loss:3.7798 train_time:1202949ms step_avg:170.24ms\nstep:7077/10000 train_loss:3.7391 train_time:1203124ms step_avg:170.25ms\nstep:7078/10000 train_loss:3.8338 train_time:1203295ms step_avg:170.25ms\nstep:7079/10000 train_loss:3.7793 train_time:1203461ms step_avg:170.24ms\nstep:7080/10000 train_loss:3.8331 train_time:1203632ms step_avg:170.24ms\nstep:7081/10000 train_loss:3.7586 train_time:1203802ms step_avg:170.24ms\nstep:7082/10000 train_loss:3.7615 train_time:1203972ms step_avg:170.24ms\nstep:7083/10000 train_loss:3.7032 train_time:1204142ms step_avg:170.24ms\nstep:7084/10000 train_loss:4.0764 train_time:1204309ms step_avg:170.24ms\nstep:7085/10000 train_loss:3.8519 train_time:1204480ms step_avg:170.24ms\nstep:7086/10000 train_loss:3.8490 train_time:1204649ms step_avg:170.24ms\nstep:7087/10000 train_loss:3.8199 train_time:1204822ms step_avg:170.24ms\nstep:7088/10000 train_loss:3.8004 train_time:1204993ms step_avg:170.24ms\nstep:7089/10000 train_loss:3.8030 train_time:1205161ms step_avg:170.24ms\nstep:7090/10000 train_loss:3.8250 train_time:1205329ms step_avg:170.24ms\nstep:7091/10000 train_loss:3.8042 train_time:1205499ms step_avg:170.24ms\nstep:7092/10000 train_loss:3.8780 train_time:1205668ms step_avg:170.24ms\nstep:7093/10000 train_loss:3.9284 train_time:1205838ms step_avg:170.24ms\nstep:7094/10000 train_loss:3.8217 train_time:1206008ms step_avg:170.24ms\nstep:7095/10000 train_loss:4.1845 train_time:1206177ms step_avg:170.24ms\nstep:7096/10000 train_loss:4.2270 train_time:1206346ms step_avg:170.24ms\nstep:7097/10000 train_loss:3.7670 train_time:1206516ms step_avg:170.24ms\nstep:7098/10000 train_loss:3.7911 train_time:1206687ms step_avg:170.24ms\nstep:7099/10000 train_loss:3.8520 train_time:1206859ms step_avg:170.24ms\nstep:7100/10000 train_loss:3.8250 train_time:1207032ms step_avg:170.24ms\nstep:7101/10000 train_loss:3.8660 train_time:1207204ms step_avg:170.24ms\nstep:7102/10000 train_loss:3.8119 train_time:1207371ms step_avg:170.24ms\nstep:7103/10000 train_loss:3.9553 train_time:1207541ms step_avg:170.24ms\nstep:7104/10000 train_loss:3.7032 train_time:1207712ms step_avg:170.24ms\nstep:7105/10000 train_loss:3.7875 train_time:1207891ms step_avg:170.25ms\nstep:7106/10000 train_loss:3.8684 train_time:1208062ms step_avg:170.25ms\nstep:7107/10000 train_loss:3.7061 train_time:1208233ms step_avg:170.25ms\nstep:7108/10000 train_loss:3.9846 train_time:1208405ms step_avg:170.25ms\nstep:7109/10000 train_loss:4.1477 train_time:1208576ms step_avg:170.25ms\nstep:7110/10000 train_loss:3.7561 train_time:1208746ms step_avg:170.25ms\nstep:7111/10000 train_loss:3.7206 train_time:1208917ms step_avg:170.25ms\nstep:7112/10000 train_loss:3.6248 train_time:1209087ms step_avg:170.25ms\nstep:7113/10000 train_loss:3.8751 train_time:1209259ms step_avg:170.25ms\nstep:7114/10000 train_loss:3.9239 train_time:1209427ms step_avg:170.25ms\nstep:7115/10000 train_loss:3.8892 train_time:1209597ms step_avg:170.25ms\nstep:7116/10000 train_loss:3.8353 train_time:1209768ms step_avg:170.25ms\nstep:7117/10000 train_loss:3.8976 train_time:1209940ms step_avg:170.25ms\nstep:7118/10000 train_loss:4.0391 train_time:1210109ms step_avg:170.25ms\nstep:7119/10000 train_loss:3.7778 train_time:1210282ms step_avg:170.25ms\nstep:7120/10000 train_loss:3.9250 train_time:1210453ms step_avg:170.25ms\nstep:7121/10000 train_loss:3.7925 train_time:1210622ms step_avg:170.25ms\nstep:7122/10000 train_loss:3.7733 train_time:1210790ms step_avg:170.25ms\nstep:7123/10000 train_loss:4.0380 train_time:1210961ms step_avg:170.25ms\nstep:7124/10000 train_loss:3.9886 train_time:1211131ms step_avg:170.25ms\nstep:7125/10000 train_loss:3.9900 train_time:1211300ms step_avg:170.25ms\nstep:7126/10000 train_loss:3.8178 train_time:1211472ms step_avg:170.25ms\nstep:7127/10000 train_loss:3.6963 train_time:1211644ms step_avg:170.25ms\nstep:7128/10000 train_loss:3.8335 train_time:1211812ms step_avg:170.25ms\nstep:7129/10000 train_loss:3.8727 train_time:1211981ms step_avg:170.25ms\nstep:7130/10000 train_loss:3.7534 train_time:1212151ms step_avg:170.25ms\nstep:7131/10000 train_loss:3.9946 train_time:1212320ms step_avg:170.25ms\nstep:7132/10000 train_loss:4.0863 train_time:1212491ms step_avg:170.25ms\nstep:7133/10000 train_loss:3.9718 train_time:1212660ms step_avg:170.25ms\nstep:7134/10000 train_loss:3.7924 train_time:1212831ms step_avg:170.25ms\nstep:7135/10000 train_loss:3.8303 train_time:1213005ms step_avg:170.25ms\nstep:7136/10000 train_loss:3.9024 train_time:1213175ms step_avg:170.25ms\nstep:7137/10000 train_loss:3.8953 train_time:1213348ms step_avg:170.25ms\nstep:7138/10000 train_loss:3.8108 train_time:1213520ms step_avg:170.25ms\nstep:7139/10000 train_loss:3.7190 train_time:1213690ms step_avg:170.25ms\nstep:7140/10000 train_loss:3.8979 train_time:1213861ms step_avg:170.25ms\nstep:7141/10000 train_loss:3.7832 train_time:1214029ms step_avg:170.25ms\nstep:7142/10000 train_loss:3.7183 train_time:1214200ms step_avg:170.25ms\nstep:7143/10000 train_loss:3.7273 train_time:1214370ms step_avg:170.25ms\nstep:7144/10000 train_loss:3.7052 train_time:1214544ms step_avg:170.25ms\nstep:7145/10000 train_loss:3.8516 train_time:1214715ms step_avg:170.25ms\nstep:7146/10000 train_loss:3.9553 train_time:1214887ms step_avg:170.25ms\nstep:7147/10000 train_loss:3.8511 train_time:1215058ms step_avg:170.25ms\nstep:7148/10000 train_loss:3.7700 train_time:1215229ms step_avg:170.25ms\nstep:7149/10000 train_loss:3.8493 train_time:1215399ms step_avg:170.25ms\nstep:7150/10000 train_loss:3.9121 train_time:1215566ms step_avg:170.25ms\nstep:7151/10000 train_loss:3.8597 train_time:1215737ms step_avg:170.25ms\nstep:7152/10000 train_loss:3.8897 train_time:1215907ms step_avg:170.25ms\nstep:7153/10000 train_loss:3.8983 train_time:1216075ms step_avg:170.25ms\nstep:7154/10000 train_loss:3.6539 train_time:1216249ms step_avg:170.25ms\nstep:7155/10000 train_loss:3.7319 train_time:1216422ms step_avg:170.25ms\nstep:7156/10000 train_loss:3.7837 train_time:1216592ms step_avg:170.25ms\nstep:7157/10000 train_loss:3.8360 train_time:1216762ms step_avg:170.25ms\nstep:7158/10000 train_loss:3.8404 train_time:1216932ms step_avg:170.25ms\nstep:7159/10000 train_loss:3.9695 train_time:1217103ms step_avg:170.25ms\nstep:7160/10000 train_loss:3.9502 train_time:1217274ms step_avg:170.25ms\nstep:7161/10000 train_loss:3.6963 train_time:1217447ms step_avg:170.25ms\nstep:7162/10000 train_loss:3.8186 train_time:1217617ms step_avg:170.25ms\nstep:7163/10000 train_loss:3.8906 train_time:1217787ms step_avg:170.25ms\nstep:7164/10000 train_loss:3.8111 train_time:1217957ms step_avg:170.25ms\nstep:7165/10000 train_loss:4.1117 train_time:1218125ms step_avg:170.25ms\nstep:7166/10000 train_loss:3.8184 train_time:1218294ms step_avg:170.25ms\nstep:7167/10000 train_loss:3.8354 train_time:1218466ms step_avg:170.25ms\nstep:7168/10000 train_loss:3.7531 train_time:1218636ms step_avg:170.25ms\nstep:7169/10000 train_loss:3.6439 train_time:1218808ms step_avg:170.25ms\nstep:7170/10000 train_loss:3.9227 train_time:1218981ms step_avg:170.25ms\nstep:7171/10000 train_loss:3.7649 train_time:1219149ms step_avg:170.25ms\nstep:7172/10000 train_loss:3.9119 train_time:1219318ms step_avg:170.25ms\nstep:7173/10000 train_loss:3.7866 train_time:1219488ms step_avg:170.25ms\nstep:7174/10000 train_loss:3.9399 train_time:1219656ms step_avg:170.25ms\nstep:7175/10000 train_loss:3.7222 train_time:1219828ms step_avg:170.25ms\nstep:7176/10000 train_loss:3.7842 train_time:1219999ms step_avg:170.25ms\nstep:7177/10000 train_loss:3.7524 train_time:1220167ms step_avg:170.25ms\nstep:7178/10000 train_loss:3.8546 train_time:1220337ms step_avg:170.25ms\nstep:7179/10000 train_loss:3.8030 train_time:1220507ms step_avg:170.25ms\nstep:7180/10000 train_loss:3.7521 train_time:1220676ms step_avg:170.25ms\nstep:7181/10000 train_loss:3.7950 train_time:1220849ms step_avg:170.25ms\nstep:7182/10000 train_loss:3.7331 train_time:1221022ms step_avg:170.25ms\nstep:7183/10000 train_loss:3.9486 train_time:1221190ms step_avg:170.25ms\nstep:7184/10000 train_loss:3.8723 train_time:1221359ms step_avg:170.25ms\nstep:7185/10000 train_loss:3.7800 train_time:1221529ms step_avg:170.25ms\nstep:7186/10000 train_loss:3.9019 train_time:1221698ms step_avg:170.25ms\nstep:7187/10000 train_loss:3.7820 train_time:1221868ms step_avg:170.25ms\nstep:7188/10000 train_loss:3.7540 train_time:1222042ms step_avg:170.25ms\nstep:7189/10000 train_loss:3.7189 train_time:1222211ms step_avg:170.25ms\nstep:7190/10000 train_loss:3.8871 train_time:1222381ms step_avg:170.25ms\nstep:7191/10000 train_loss:3.9008 train_time:1222549ms step_avg:170.25ms\nstep:7192/10000 train_loss:3.7615 train_time:1222720ms step_avg:170.25ms\nstep:7193/10000 train_loss:3.8351 train_time:1222889ms step_avg:170.25ms\nstep:7194/10000 train_loss:3.8218 train_time:1223062ms step_avg:170.25ms\nstep:7195/10000 train_loss:3.8007 train_time:1223230ms step_avg:170.25ms\nstep:7196/10000 train_loss:3.7971 train_time:1223400ms step_avg:170.25ms\nstep:7197/10000 train_loss:3.7591 train_time:1223568ms step_avg:170.25ms\nstep:7198/10000 train_loss:3.8983 train_time:1223739ms step_avg:170.25ms\nstep:7199/10000 train_loss:3.8390 train_time:1223909ms step_avg:170.25ms\nstep:7200/10000 train_loss:3.7915 train_time:1224083ms step_avg:170.25ms\nstep:7201/10000 train_loss:3.8934 train_time:1224251ms step_avg:170.25ms\nstep:7202/10000 train_loss:3.8678 train_time:1224422ms step_avg:170.25ms\nstep:7203/10000 train_loss:4.2318 train_time:1224591ms step_avg:170.25ms\nstep:7204/10000 train_loss:3.8882 train_time:1224762ms step_avg:170.25ms\nstep:7205/10000 train_loss:3.7931 train_time:1224930ms step_avg:170.25ms\nstep:7206/10000 train_loss:3.8545 train_time:1225101ms step_avg:170.25ms\nstep:7207/10000 train_loss:3.8022 train_time:1225271ms step_avg:170.25ms\nstep:7208/10000 train_loss:3.7884 train_time:1225441ms step_avg:170.25ms\nstep:7209/10000 train_loss:3.8063 train_time:1225612ms step_avg:170.25ms\nstep:7210/10000 train_loss:3.8522 train_time:1225784ms step_avg:170.25ms\nstep:7211/10000 train_loss:3.9038 train_time:1225954ms step_avg:170.25ms\nstep:7212/10000 train_loss:3.8540 train_time:1226125ms step_avg:170.25ms\nstep:7213/10000 train_loss:3.8221 train_time:1226293ms step_avg:170.25ms\nstep:7214/10000 train_loss:3.7544 train_time:1226462ms step_avg:170.25ms\nstep:7215/10000 train_loss:4.0349 train_time:1226632ms step_avg:170.25ms\nstep:7216/10000 train_loss:3.7949 train_time:1226803ms step_avg:170.25ms\nstep:7217/10000 train_loss:3.7914 train_time:1226972ms step_avg:170.25ms\nstep:7218/10000 train_loss:3.7777 train_time:1227142ms step_avg:170.25ms\nstep:7219/10000 train_loss:3.7517 train_time:1227314ms step_avg:170.25ms\nstep:7220/10000 train_loss:3.8286 train_time:1227484ms step_avg:170.25ms\nstep:7221/10000 train_loss:3.7743 train_time:1227655ms step_avg:170.25ms\nstep:7222/10000 train_loss:3.9762 train_time:1227824ms step_avg:170.25ms\nstep:7223/10000 train_loss:3.9419 train_time:1227994ms step_avg:170.25ms\nstep:7224/10000 train_loss:3.7841 train_time:1228167ms step_avg:170.25ms\nstep:7225/10000 train_loss:3.8110 train_time:1228339ms step_avg:170.25ms\nstep:7226/10000 train_loss:3.7154 train_time:1228509ms step_avg:170.25ms\nstep:7227/10000 train_loss:3.8230 train_time:1228678ms step_avg:170.25ms\nstep:7228/10000 train_loss:3.7114 train_time:1228846ms step_avg:170.25ms\nstep:7229/10000 train_loss:3.7915 train_time:1229016ms step_avg:170.25ms\nstep:7230/10000 train_loss:3.7917 train_time:1229188ms step_avg:170.25ms\nstep:7231/10000 train_loss:4.0321 train_time:1229360ms step_avg:170.25ms\nstep:7232/10000 train_loss:3.9201 train_time:1229530ms step_avg:170.25ms\nstep:7233/10000 train_loss:3.7620 train_time:1229702ms step_avg:170.25ms\nstep:7234/10000 train_loss:3.8040 train_time:1229872ms step_avg:170.25ms\nstep:7235/10000 train_loss:3.7610 train_time:1230043ms step_avg:170.25ms\nstep:7236/10000 train_loss:3.8414 train_time:1230213ms step_avg:170.25ms\nstep:7237/10000 train_loss:3.7745 train_time:1230386ms step_avg:170.25ms\nstep:7238/10000 train_loss:3.8967 train_time:1230557ms step_avg:170.25ms\nstep:7239/10000 train_loss:3.8374 train_time:1230727ms step_avg:170.25ms\nstep:7240/10000 train_loss:3.8285 train_time:1230897ms step_avg:170.25ms\nstep:7241/10000 train_loss:3.8426 train_time:1231066ms step_avg:170.25ms\nstep:7242/10000 train_loss:3.9231 train_time:1231239ms step_avg:170.25ms\nstep:7243/10000 train_loss:3.7477 train_time:1231408ms step_avg:170.25ms\nstep:7244/10000 train_loss:3.9556 train_time:1231578ms step_avg:170.25ms\nstep:7245/10000 train_loss:3.8294 train_time:1231747ms step_avg:170.25ms\nstep:7246/10000 train_loss:3.7413 train_time:1231920ms step_avg:170.25ms\nstep:7247/10000 train_loss:3.7523 train_time:1232089ms step_avg:170.25ms\nstep:7248/10000 train_loss:3.7638 train_time:1232260ms step_avg:170.25ms\nstep:7249/10000 train_loss:3.8759 train_time:1232428ms step_avg:170.25ms\nstep:7250/10000 train_loss:3.8760 train_time:1232600ms step_avg:170.25ms\nstep:7251/10000 train_loss:3.9332 train_time:1232769ms step_avg:170.25ms\nstep:7252/10000 train_loss:3.8777 train_time:1232941ms step_avg:170.25ms\nstep:7253/10000 train_loss:3.8797 train_time:1233110ms step_avg:170.25ms\nstep:7254/10000 train_loss:3.7357 train_time:1233283ms step_avg:170.25ms\nstep:7255/10000 train_loss:3.8578 train_time:1233451ms step_avg:170.25ms\nstep:7256/10000 train_loss:3.9838 train_time:1233620ms step_avg:170.25ms\nstep:7257/10000 train_loss:3.7564 train_time:1233789ms step_avg:170.25ms\nstep:7258/10000 train_loss:3.8133 train_time:1233960ms step_avg:170.25ms\nstep:7259/10000 train_loss:3.8474 train_time:1234131ms step_avg:170.25ms\nstep:7260/10000 train_loss:3.8569 train_time:1234305ms step_avg:170.25ms\nstep:7261/10000 train_loss:3.7589 train_time:1234479ms step_avg:170.25ms\nstep:7262/10000 train_loss:3.7876 train_time:1234647ms step_avg:170.25ms\nstep:7263/10000 train_loss:3.7212 train_time:1234818ms step_avg:170.25ms\nstep:7264/10000 train_loss:3.8827 train_time:1234988ms step_avg:170.25ms\nstep:7265/10000 train_loss:3.8051 train_time:1235156ms step_avg:170.25ms\nstep:7266/10000 train_loss:3.6600 train_time:1235327ms step_avg:170.25ms\nstep:7267/10000 train_loss:3.8509 train_time:1235499ms step_avg:170.25ms\nstep:7268/10000 train_loss:3.8994 train_time:1235667ms step_avg:170.25ms\nstep:7269/10000 train_loss:3.8122 train_time:1235839ms step_avg:170.25ms\nstep:7270/10000 train_loss:3.8547 train_time:1236010ms step_avg:170.25ms\nstep:7271/10000 train_loss:3.7980 train_time:1236180ms step_avg:170.25ms\nstep:7272/10000 train_loss:3.8631 train_time:1236349ms step_avg:170.25ms\nstep:7273/10000 train_loss:3.8332 train_time:1236521ms step_avg:170.25ms\nstep:7274/10000 train_loss:3.6383 train_time:1236689ms step_avg:170.25ms\nstep:7275/10000 train_loss:3.7617 train_time:1236861ms step_avg:170.25ms\nstep:7276/10000 train_loss:3.9117 train_time:1237032ms step_avg:170.25ms\nstep:7277/10000 train_loss:3.7992 train_time:1237204ms step_avg:170.25ms\nstep:7278/10000 train_loss:3.8152 train_time:1237374ms step_avg:170.25ms\nstep:7279/10000 train_loss:3.8902 train_time:1237547ms step_avg:170.25ms\nstep:7280/10000 train_loss:3.8622 train_time:1237717ms step_avg:170.25ms\nstep:7281/10000 train_loss:3.8167 train_time:1237889ms step_avg:170.25ms\nstep:7282/10000 train_loss:3.7883 train_time:1238060ms step_avg:170.25ms\nstep:7283/10000 train_loss:3.7712 train_time:1238227ms step_avg:170.25ms\nstep:7284/10000 train_loss:3.8963 train_time:1238396ms step_avg:170.25ms\nstep:7285/10000 train_loss:3.7136 train_time:1238564ms step_avg:170.25ms\nstep:7286/10000 train_loss:3.9022 train_time:1238733ms step_avg:170.25ms\nstep:7287/10000 train_loss:3.8145 train_time:1238901ms step_avg:170.25ms\nstep:7288/10000 train_loss:3.8273 train_time:1239071ms step_avg:170.25ms\nstep:7289/10000 train_loss:3.8618 train_time:1239240ms step_avg:170.25ms\nstep:7290/10000 train_loss:3.7960 train_time:1239407ms step_avg:170.25ms\nstep:7291/10000 train_loss:3.8607 train_time:1239578ms step_avg:170.25ms\nstep:7292/10000 train_loss:3.8292 train_time:1239747ms step_avg:170.25ms\nstep:7293/10000 train_loss:3.8490 train_time:1239918ms step_avg:170.25ms\nstep:7294/10000 train_loss:3.7685 train_time:1240090ms step_avg:170.25ms\nstep:7295/10000 train_loss:3.8559 train_time:1240261ms step_avg:170.25ms\nstep:7296/10000 train_loss:3.7616 train_time:1240431ms step_avg:170.25ms\nstep:7297/10000 train_loss:3.6793 train_time:1240602ms step_avg:170.25ms\nstep:7298/10000 train_loss:3.8118 train_time:1240771ms step_avg:170.25ms\nstep:7299/10000 train_loss:3.7779 train_time:1240942ms step_avg:170.25ms\nstep:7300/10000 train_loss:3.7734 train_time:1241110ms step_avg:170.25ms\nstep:7301/10000 train_loss:3.9299 train_time:1241281ms step_avg:170.25ms\nstep:7302/10000 train_loss:3.7758 train_time:1241452ms step_avg:170.25ms\nstep:7303/10000 train_loss:3.7530 train_time:1241625ms step_avg:170.25ms\nstep:7304/10000 train_loss:3.6471 train_time:1241794ms step_avg:170.25ms\nstep:7305/10000 train_loss:3.7798 train_time:1241964ms step_avg:170.25ms\nstep:7306/10000 train_loss:3.7660 train_time:1242137ms step_avg:170.25ms\nstep:7307/10000 train_loss:3.8881 train_time:1242304ms step_avg:170.25ms\nstep:7308/10000 train_loss:3.8194 train_time:1242477ms step_avg:170.25ms\nstep:7309/10000 train_loss:4.0084 train_time:1242645ms step_avg:170.25ms\nstep:7310/10000 train_loss:3.8960 train_time:1242819ms step_avg:170.25ms\nstep:7311/10000 train_loss:3.7111 train_time:1242988ms step_avg:170.25ms\nstep:7312/10000 train_loss:3.9476 train_time:1243160ms step_avg:170.25ms\nstep:7313/10000 train_loss:3.8864 train_time:1243330ms step_avg:170.25ms\nstep:7314/10000 train_loss:3.6456 train_time:1243505ms step_avg:170.25ms\nstep:7315/10000 train_loss:3.9116 train_time:1243674ms step_avg:170.25ms\nstep:7316/10000 train_loss:3.8930 train_time:1243846ms step_avg:170.25ms\nstep:7317/10000 train_loss:3.9840 train_time:1244017ms step_avg:170.25ms\nstep:7318/10000 train_loss:4.2061 train_time:1244189ms step_avg:170.25ms\nstep:7319/10000 train_loss:3.6756 train_time:1244360ms step_avg:170.25ms\nstep:7320/10000 train_loss:3.8796 train_time:1244528ms step_avg:170.25ms\nstep:7321/10000 train_loss:3.7775 train_time:1244698ms step_avg:170.25ms\nstep:7322/10000 train_loss:3.8024 train_time:1244866ms step_avg:170.25ms\nstep:7323/10000 train_loss:3.8340 train_time:1245036ms step_avg:170.25ms\nstep:7324/10000 train_loss:3.7329 train_time:1245206ms step_avg:170.25ms\nstep:7325/10000 train_loss:3.8359 train_time:1245375ms step_avg:170.25ms\nstep:7326/10000 train_loss:3.4930 train_time:1245544ms step_avg:170.25ms\nstep:7327/10000 train_loss:3.7877 train_time:1245720ms step_avg:170.25ms\nstep:7328/10000 train_loss:3.8031 train_time:1245887ms step_avg:170.25ms\nstep:7329/10000 train_loss:3.8844 train_time:1246058ms step_avg:170.25ms\nstep:7330/10000 train_loss:3.8207 train_time:1246231ms step_avg:170.25ms\nstep:7331/10000 train_loss:3.9351 train_time:1246395ms step_avg:170.25ms\nstep:7332/10000 train_loss:3.8877 train_time:1246565ms step_avg:170.25ms\nstep:7333/10000 train_loss:3.7275 train_time:1246735ms step_avg:170.25ms\nstep:7334/10000 train_loss:3.6685 train_time:1246907ms step_avg:170.25ms\nstep:7335/10000 train_loss:3.8198 train_time:1247079ms step_avg:170.25ms\nstep:7336/10000 train_loss:3.7030 train_time:1247250ms step_avg:170.25ms\nstep:7337/10000 train_loss:3.8355 train_time:1247419ms step_avg:170.25ms\nstep:7338/10000 train_loss:3.8371 train_time:1247590ms step_avg:170.25ms\nstep:7339/10000 train_loss:3.7892 train_time:1247761ms step_avg:170.25ms\nstep:7340/10000 train_loss:3.8367 train_time:1247929ms step_avg:170.25ms\nstep:7341/10000 train_loss:3.7290 train_time:1248102ms step_avg:170.25ms\nstep:7342/10000 train_loss:3.9042 train_time:1248272ms step_avg:170.25ms\nstep:7343/10000 train_loss:3.8147 train_time:1248441ms step_avg:170.25ms\nstep:7344/10000 train_loss:3.7113 train_time:1248610ms step_avg:170.25ms\nstep:7345/10000 train_loss:3.8777 train_time:1248783ms step_avg:170.25ms\nstep:7346/10000 train_loss:3.8344 train_time:1248953ms step_avg:170.25ms\nstep:7347/10000 train_loss:3.9213 train_time:1249126ms step_avg:170.25ms\nstep:7348/10000 train_loss:4.0701 train_time:1249295ms step_avg:170.25ms\nstep:7349/10000 train_loss:3.8969 train_time:1249463ms step_avg:170.25ms\nstep:7350/10000 train_loss:3.7088 train_time:1249631ms step_avg:170.25ms\nstep:7351/10000 train_loss:3.7440 train_time:1249805ms step_avg:170.25ms\nstep:7352/10000 train_loss:3.8658 train_time:1249975ms step_avg:170.25ms\nstep:7353/10000 train_loss:3.8818 train_time:1250146ms step_avg:170.25ms\nstep:7354/10000 train_loss:3.8885 train_time:1250318ms step_avg:170.25ms\nstep:7355/10000 train_loss:3.6679 train_time:1250487ms step_avg:170.25ms\nstep:7356/10000 train_loss:3.8522 train_time:1250655ms step_avg:170.25ms\nstep:7357/10000 train_loss:3.8479 train_time:1250826ms step_avg:170.25ms\nstep:7358/10000 train_loss:3.8437 train_time:1250995ms step_avg:170.25ms\nstep:7359/10000 train_loss:3.8291 train_time:1251161ms step_avg:170.25ms\nstep:7360/10000 train_loss:3.9556 train_time:1251332ms step_avg:170.25ms\nstep:7361/10000 train_loss:3.7776 train_time:1251502ms step_avg:170.25ms\nstep:7362/10000 train_loss:3.7800 train_time:1251670ms step_avg:170.25ms\nstep:7363/10000 train_loss:3.8101 train_time:1251842ms step_avg:170.25ms\nstep:7364/10000 train_loss:3.8373 train_time:1252011ms step_avg:170.25ms\nstep:7365/10000 train_loss:3.8554 train_time:1252186ms step_avg:170.25ms\nstep:7366/10000 train_loss:3.7630 train_time:1252360ms step_avg:170.25ms\nstep:7367/10000 train_loss:3.7447 train_time:1252528ms step_avg:170.25ms\nstep:7368/10000 train_loss:3.6937 train_time:1252699ms step_avg:170.25ms\nstep:7369/10000 train_loss:3.7979 train_time:1252869ms step_avg:170.25ms\nstep:7370/10000 train_loss:3.6290 train_time:1253040ms step_avg:170.25ms\nstep:7371/10000 train_loss:3.7314 train_time:1253209ms step_avg:170.25ms\nstep:7372/10000 train_loss:3.8589 train_time:1253383ms step_avg:170.25ms\nstep:7373/10000 train_loss:3.7812 train_time:1253553ms step_avg:170.25ms\nstep:7374/10000 train_loss:3.8515 train_time:1253724ms step_avg:170.25ms\nstep:7375/10000 train_loss:3.8925 train_time:1253895ms step_avg:170.25ms\nstep:7376/10000 train_loss:3.7220 train_time:1254065ms step_avg:170.25ms\nstep:7377/10000 train_loss:3.7926 train_time:1254233ms step_avg:170.25ms\nstep:7378/10000 train_loss:3.8868 train_time:1254402ms step_avg:170.25ms\nstep:7379/10000 train_loss:3.7341 train_time:1254570ms step_avg:170.25ms\nstep:7380/10000 train_loss:3.8204 train_time:1254740ms step_avg:170.25ms\nstep:7381/10000 train_loss:3.7720 train_time:1254907ms step_avg:170.25ms\nstep:7382/10000 train_loss:3.7633 train_time:1255079ms step_avg:170.25ms\nstep:7383/10000 train_loss:3.7931 train_time:1255250ms step_avg:170.25ms\nstep:7384/10000 train_loss:3.8883 train_time:1255419ms step_avg:170.25ms\nstep:7385/10000 train_loss:3.7821 train_time:1255587ms step_avg:170.25ms\nstep:7386/10000 train_loss:3.8780 train_time:1255760ms step_avg:170.25ms\nstep:7387/10000 train_loss:3.5782 train_time:1255928ms step_avg:170.25ms\nstep:7388/10000 train_loss:3.6997 train_time:1256099ms step_avg:170.25ms\nstep:7389/10000 train_loss:3.8534 train_time:1256269ms step_avg:170.25ms\nstep:7390/10000 train_loss:3.6104 train_time:1256440ms step_avg:170.25ms\nstep:7391/10000 train_loss:3.8038 train_time:1256607ms step_avg:170.25ms\nstep:7392/10000 train_loss:3.7848 train_time:1256778ms step_avg:170.25ms\nstep:7393/10000 train_loss:3.7347 train_time:1256949ms step_avg:170.25ms\nstep:7394/10000 train_loss:3.7748 train_time:1257119ms step_avg:170.25ms\nstep:7395/10000 train_loss:3.7974 train_time:1257288ms step_avg:170.25ms\nstep:7396/10000 train_loss:3.7926 train_time:1257459ms step_avg:170.25ms\nstep:7397/10000 train_loss:3.8321 train_time:1257630ms step_avg:170.25ms\nstep:7398/10000 train_loss:3.7529 train_time:1257801ms step_avg:170.25ms\nstep:7399/10000 train_loss:3.7788 train_time:1257970ms step_avg:170.25ms\nstep:7400/10000 train_loss:3.7856 train_time:1258143ms step_avg:170.25ms\nstep:7401/10000 train_loss:3.8899 train_time:1258312ms step_avg:170.25ms\nstep:7402/10000 train_loss:3.8031 train_time:1258481ms step_avg:170.25ms\nstep:7403/10000 train_loss:3.7556 train_time:1258650ms step_avg:170.25ms\nstep:7404/10000 train_loss:3.6633 train_time:1258820ms step_avg:170.25ms\nstep:7405/10000 train_loss:3.8238 train_time:1258988ms step_avg:170.25ms\nstep:7406/10000 train_loss:3.8194 train_time:1259160ms step_avg:170.25ms\nstep:7407/10000 train_loss:3.8639 train_time:1259329ms step_avg:170.25ms\nstep:7408/10000 train_loss:3.9278 train_time:1259500ms step_avg:170.25ms\nstep:7409/10000 train_loss:3.8402 train_time:1259666ms step_avg:170.25ms\nstep:7410/10000 train_loss:3.8075 train_time:1259837ms step_avg:170.25ms\nstep:7411/10000 train_loss:3.7630 train_time:1260004ms step_avg:170.25ms\nstep:7412/10000 train_loss:3.7032 train_time:1260175ms step_avg:170.25ms\nstep:7413/10000 train_loss:3.8921 train_time:1260347ms step_avg:170.25ms\nstep:7414/10000 train_loss:3.8626 train_time:1260520ms step_avg:170.25ms\nstep:7415/10000 train_loss:3.7152 train_time:1260689ms step_avg:170.25ms\nstep:7416/10000 train_loss:3.7788 train_time:1260860ms step_avg:170.25ms\nstep:7417/10000 train_loss:3.7312 train_time:1261031ms step_avg:170.25ms\nstep:7418/10000 train_loss:3.8009 train_time:1261203ms step_avg:170.25ms\nstep:7419/10000 train_loss:3.6989 train_time:1261369ms step_avg:170.25ms\nstep:7420/10000 train_loss:3.7551 train_time:1261538ms step_avg:170.25ms\nstep:7421/10000 train_loss:3.8157 train_time:1261707ms step_avg:170.25ms\nstep:7422/10000 train_loss:3.8364 train_time:1261878ms step_avg:170.25ms\nstep:7423/10000 train_loss:3.7338 train_time:1262046ms step_avg:170.25ms\nstep:7424/10000 train_loss:3.9310 train_time:1262218ms step_avg:170.25ms\nstep:7425/10000 train_loss:3.5178 train_time:1262388ms step_avg:170.25ms\nstep:7426/10000 train_loss:3.8158 train_time:1262559ms step_avg:170.25ms\nstep:7427/10000 train_loss:3.8094 train_time:1262729ms step_avg:170.25ms\nstep:7428/10000 train_loss:3.8867 train_time:1262900ms step_avg:170.25ms\nstep:7429/10000 train_loss:3.8736 train_time:1263070ms step_avg:170.25ms\nstep:7430/10000 train_loss:3.8292 train_time:1263240ms step_avg:170.25ms\nstep:7431/10000 train_loss:3.7967 train_time:1263408ms step_avg:170.25ms\nstep:7432/10000 train_loss:3.7914 train_time:1263579ms step_avg:170.25ms\nstep:7433/10000 train_loss:3.7855 train_time:1263749ms step_avg:170.25ms\nstep:7434/10000 train_loss:3.7688 train_time:1263921ms step_avg:170.25ms\nstep:7435/10000 train_loss:3.8654 train_time:1264091ms step_avg:170.25ms\nstep:7436/10000 train_loss:3.7339 train_time:1264260ms step_avg:170.25ms\nstep:7437/10000 train_loss:3.8715 train_time:1264430ms step_avg:170.25ms\nstep:7438/10000 train_loss:3.7520 train_time:1264599ms step_avg:170.25ms\nstep:7439/10000 train_loss:3.7081 train_time:1264768ms step_avg:170.25ms\nstep:7440/10000 train_loss:3.7982 train_time:1264942ms step_avg:170.25ms\nstep:7441/10000 train_loss:3.9413 train_time:1265110ms step_avg:170.25ms\nstep:7442/10000 train_loss:3.7399 train_time:1265279ms step_avg:170.25ms\nstep:7443/10000 train_loss:3.7798 train_time:1265447ms step_avg:170.25ms\nstep:7444/10000 train_loss:3.9942 train_time:1265618ms step_avg:170.25ms\nstep:7445/10000 train_loss:3.7409 train_time:1265788ms step_avg:170.25ms\nstep:7446/10000 train_loss:3.8065 train_time:1265958ms step_avg:170.25ms\nstep:7447/10000 train_loss:3.8002 train_time:1266127ms step_avg:170.25ms\nstep:7448/10000 train_loss:3.8678 train_time:1266296ms step_avg:170.25ms\nstep:7449/10000 train_loss:3.8116 train_time:1266465ms step_avg:170.25ms\nstep:7450/10000 train_loss:4.0349 train_time:1266637ms step_avg:170.25ms\nstep:7451/10000 train_loss:3.8486 train_time:1266807ms step_avg:170.25ms\nstep:7452/10000 train_loss:3.7980 train_time:1266980ms step_avg:170.25ms\nstep:7453/10000 train_loss:3.7391 train_time:1267149ms step_avg:170.25ms\nstep:7454/10000 train_loss:3.7528 train_time:1267318ms step_avg:170.25ms\nstep:7455/10000 train_loss:3.8453 train_time:1267487ms step_avg:170.25ms\nstep:7456/10000 train_loss:3.8348 train_time:1267659ms step_avg:170.25ms\nstep:7457/10000 train_loss:3.7812 train_time:1267829ms step_avg:170.25ms\nstep:7458/10000 train_loss:3.8448 train_time:1268002ms step_avg:170.25ms\nstep:7459/10000 train_loss:3.6717 train_time:1268168ms step_avg:170.25ms\nstep:7460/10000 train_loss:3.7610 train_time:1268339ms step_avg:170.25ms\nstep:7461/10000 train_loss:3.7575 train_time:1268508ms step_avg:170.25ms\nstep:7462/10000 train_loss:3.7405 train_time:1268679ms step_avg:170.25ms\nstep:7463/10000 train_loss:3.7297 train_time:1268848ms step_avg:170.25ms\nstep:7464/10000 train_loss:3.7415 train_time:1269020ms step_avg:170.25ms\nstep:7465/10000 train_loss:3.9808 train_time:1269188ms step_avg:170.25ms\nstep:7466/10000 train_loss:3.7943 train_time:1269358ms step_avg:170.25ms\nstep:7467/10000 train_loss:3.7869 train_time:1269528ms step_avg:170.25ms\nstep:7468/10000 train_loss:3.8028 train_time:1269700ms step_avg:170.25ms\nstep:7469/10000 train_loss:3.9873 train_time:1269870ms step_avg:170.25ms\nstep:7470/10000 train_loss:3.8598 train_time:1270042ms step_avg:170.25ms\nstep:7471/10000 train_loss:3.8280 train_time:1270213ms step_avg:170.25ms\nstep:7472/10000 train_loss:3.8443 train_time:1270385ms step_avg:170.25ms\nstep:7473/10000 train_loss:3.7465 train_time:1270555ms step_avg:170.25ms\nstep:7474/10000 train_loss:3.7389 train_time:1270724ms step_avg:170.25ms\nstep:7475/10000 train_loss:4.0729 train_time:1270893ms step_avg:170.25ms\nstep:7476/10000 train_loss:3.6144 train_time:1271064ms step_avg:170.25ms\nstep:7477/10000 train_loss:3.7464 train_time:1271235ms step_avg:170.25ms\nstep:7478/10000 train_loss:3.9516 train_time:1271406ms step_avg:170.25ms\nstep:7479/10000 train_loss:3.7976 train_time:1271576ms step_avg:170.25ms\nstep:7480/10000 train_loss:3.7242 train_time:1271747ms step_avg:170.25ms\nstep:7481/10000 train_loss:3.7231 train_time:1271916ms step_avg:170.25ms\nstep:7482/10000 train_loss:3.7851 train_time:1272088ms step_avg:170.25ms\nstep:7483/10000 train_loss:3.7805 train_time:1272258ms step_avg:170.25ms\nstep:7484/10000 train_loss:3.6764 train_time:1272426ms step_avg:170.25ms\nstep:7485/10000 train_loss:4.0055 train_time:1272596ms step_avg:170.25ms\nstep:7486/10000 train_loss:3.8498 train_time:1272766ms step_avg:170.25ms\nstep:7487/10000 train_loss:3.7727 train_time:1272939ms step_avg:170.25ms\nstep:7488/10000 train_loss:3.6911 train_time:1273108ms step_avg:170.25ms\nstep:7489/10000 train_loss:3.9661 train_time:1273276ms step_avg:170.25ms\nstep:7490/10000 train_loss:3.8791 train_time:1273446ms step_avg:170.25ms\nstep:7491/10000 train_loss:3.8210 train_time:1273616ms step_avg:170.25ms\nstep:7492/10000 train_loss:3.8372 train_time:1273785ms step_avg:170.25ms\nstep:7493/10000 train_loss:3.6221 train_time:1273956ms step_avg:170.25ms\nstep:7494/10000 train_loss:3.7261 train_time:1274126ms step_avg:170.25ms\nstep:7495/10000 train_loss:3.8109 train_time:1274295ms step_avg:170.25ms\nstep:7496/10000 train_loss:3.8289 train_time:1274466ms step_avg:170.25ms\nstep:7497/10000 train_loss:3.8749 train_time:1274635ms step_avg:170.25ms\nstep:7498/10000 train_loss:3.8305 train_time:1274805ms step_avg:170.25ms\nstep:7499/10000 train_loss:3.7623 train_time:1274977ms step_avg:170.25ms\nstep:7500/10000 train_loss:3.7807 train_time:1275146ms step_avg:170.25ms\nstep:7501/10000 train_loss:3.8126 train_time:1275318ms step_avg:170.25ms\nstep:7502/10000 train_loss:3.7744 train_time:1275489ms step_avg:170.25ms\nstep:7503/10000 train_loss:3.8824 train_time:1275659ms step_avg:170.25ms\nstep:7504/10000 train_loss:3.7755 train_time:1275828ms step_avg:170.25ms\nstep:7505/10000 train_loss:3.8951 train_time:1276000ms step_avg:170.25ms\nstep:7506/10000 train_loss:3.7337 train_time:1276171ms step_avg:170.25ms\nstep:7507/10000 train_loss:3.5810 train_time:1276340ms step_avg:170.25ms\nstep:7508/10000 train_loss:3.7168 train_time:1276510ms step_avg:170.25ms\nstep:7509/10000 train_loss:3.8735 train_time:1276682ms step_avg:170.25ms\nstep:7510/10000 train_loss:3.7498 train_time:1276851ms step_avg:170.25ms\nstep:7511/10000 train_loss:3.8071 train_time:1277021ms step_avg:170.25ms\nstep:7512/10000 train_loss:3.8254 train_time:1277191ms step_avg:170.25ms\nstep:7513/10000 train_loss:3.6933 train_time:1277362ms step_avg:170.25ms\nstep:7514/10000 train_loss:3.7801 train_time:1277531ms step_avg:170.25ms\nstep:7515/10000 train_loss:3.8302 train_time:1277702ms step_avg:170.25ms\nstep:7516/10000 train_loss:3.8234 train_time:1277872ms step_avg:170.25ms\nstep:7517/10000 train_loss:3.9340 train_time:1278043ms step_avg:170.25ms\nstep:7518/10000 train_loss:4.1271 train_time:1278211ms step_avg:170.25ms\nstep:7519/10000 train_loss:3.7548 train_time:1278384ms step_avg:170.25ms\nstep:7520/10000 train_loss:3.7538 train_time:1278553ms step_avg:170.25ms\nstep:7521/10000 train_loss:3.6910 train_time:1278723ms step_avg:170.25ms\nstep:7522/10000 train_loss:3.8713 train_time:1278896ms step_avg:170.25ms\nstep:7523/10000 train_loss:3.6832 train_time:1279067ms step_avg:170.25ms\nstep:7524/10000 train_loss:3.7928 train_time:1279239ms step_avg:170.25ms\nstep:7525/10000 train_loss:3.9264 train_time:1279409ms step_avg:170.25ms\nstep:7526/10000 train_loss:3.8153 train_time:1279580ms step_avg:170.25ms\nstep:7527/10000 train_loss:3.8456 train_time:1279750ms step_avg:170.25ms\nstep:7528/10000 train_loss:3.9014 train_time:1279918ms step_avg:170.25ms\nstep:7529/10000 train_loss:3.8077 train_time:1280089ms step_avg:170.25ms\nstep:7530/10000 train_loss:3.9168 train_time:1280261ms step_avg:170.25ms\nstep:7531/10000 train_loss:3.9227 train_time:1280429ms step_avg:170.25ms\nstep:7532/10000 train_loss:3.7693 train_time:1280601ms step_avg:170.25ms\nstep:7533/10000 train_loss:4.1709 train_time:1280770ms step_avg:170.25ms\nstep:7534/10000 train_loss:3.8119 train_time:1280938ms step_avg:170.25ms\nstep:7535/10000 train_loss:3.8244 train_time:1281109ms step_avg:170.25ms\nstep:7536/10000 train_loss:3.8384 train_time:1281277ms step_avg:170.25ms\nstep:7537/10000 train_loss:3.8316 train_time:1281448ms step_avg:170.25ms\nstep:7538/10000 train_loss:3.7200 train_time:1281620ms step_avg:170.25ms\nstep:7539/10000 train_loss:3.7922 train_time:1281789ms step_avg:170.25ms\nstep:7540/10000 train_loss:3.7450 train_time:1281959ms step_avg:170.25ms\nstep:7541/10000 train_loss:3.7887 train_time:1282132ms step_avg:170.25ms\nstep:7542/10000 train_loss:3.7080 train_time:1282303ms step_avg:170.25ms\nstep:7543/10000 train_loss:3.7215 train_time:1282474ms step_avg:170.25ms\nstep:7544/10000 train_loss:3.8170 train_time:1282644ms step_avg:170.25ms\nstep:7545/10000 train_loss:3.8524 train_time:1282816ms step_avg:170.25ms\nstep:7546/10000 train_loss:3.7492 train_time:1282987ms step_avg:170.25ms\nstep:7547/10000 train_loss:3.7808 train_time:1283157ms step_avg:170.25ms\nstep:7548/10000 train_loss:3.8164 train_time:1283326ms step_avg:170.25ms\nstep:7549/10000 train_loss:3.8186 train_time:1283496ms step_avg:170.25ms\nstep:7550/10000 train_loss:3.7821 train_time:1283667ms step_avg:170.25ms\nstep:7551/10000 train_loss:3.7692 train_time:1283837ms step_avg:170.25ms\nstep:7552/10000 train_loss:3.5524 train_time:1284009ms step_avg:170.25ms\nstep:7553/10000 train_loss:3.8987 train_time:1284183ms step_avg:170.25ms\nstep:7554/10000 train_loss:4.0296 train_time:1284352ms step_avg:170.25ms\nstep:7555/10000 train_loss:3.6531 train_time:1284523ms step_avg:170.25ms\nstep:7556/10000 train_loss:3.9876 train_time:1284692ms step_avg:170.25ms\nstep:7557/10000 train_loss:3.7951 train_time:1284861ms step_avg:170.25ms\nstep:7558/10000 train_loss:3.8978 train_time:1285030ms step_avg:170.25ms\nstep:7559/10000 train_loss:3.8270 train_time:1285200ms step_avg:170.25ms\nstep:7560/10000 train_loss:3.6981 train_time:1285369ms step_avg:170.25ms\nstep:7561/10000 train_loss:3.8258 train_time:1285538ms step_avg:170.25ms\nstep:7562/10000 train_loss:3.7241 train_time:1285708ms step_avg:170.25ms\nstep:7563/10000 train_loss:3.8739 train_time:1285879ms step_avg:170.25ms\nstep:7564/10000 train_loss:3.7439 train_time:1286047ms step_avg:170.25ms\nstep:7565/10000 train_loss:3.8060 train_time:1286216ms step_avg:170.25ms\nstep:7566/10000 train_loss:3.7459 train_time:1286385ms step_avg:170.25ms\nstep:7567/10000 train_loss:3.7616 train_time:1286556ms step_avg:170.25ms\nstep:7568/10000 train_loss:3.7710 train_time:1286724ms step_avg:170.25ms\nstep:7569/10000 train_loss:3.7725 train_time:1286894ms step_avg:170.25ms\nstep:7570/10000 train_loss:3.7195 train_time:1287066ms step_avg:170.25ms\nstep:7571/10000 train_loss:3.7492 train_time:1287238ms step_avg:170.25ms\nstep:7572/10000 train_loss:3.8210 train_time:1287408ms step_avg:170.25ms\nstep:7573/10000 train_loss:3.8252 train_time:1287580ms step_avg:170.25ms\nstep:7574/10000 train_loss:3.7104 train_time:1287750ms step_avg:170.25ms\nstep:7575/10000 train_loss:3.7875 train_time:1287919ms step_avg:170.25ms\nstep:7576/10000 train_loss:3.6812 train_time:1288087ms step_avg:170.25ms\nstep:7577/10000 train_loss:3.8688 train_time:1288256ms step_avg:170.25ms\nstep:7578/10000 train_loss:3.7814 train_time:1288434ms step_avg:170.25ms\nstep:7579/10000 train_loss:3.7917 train_time:1288607ms step_avg:170.25ms\nstep:7580/10000 train_loss:3.9436 train_time:1288770ms step_avg:170.25ms\nstep:7581/10000 train_loss:3.7310 train_time:1288942ms step_avg:170.25ms\nstep:7582/10000 train_loss:3.9043 train_time:1289111ms step_avg:170.25ms\nstep:7583/10000 train_loss:3.8223 train_time:1289280ms step_avg:170.25ms\nstep:7584/10000 train_loss:3.7086 train_time:1289447ms step_avg:170.25ms\nstep:7585/10000 train_loss:3.6229 train_time:1289619ms step_avg:170.25ms\nstep:7586/10000 train_loss:3.8093 train_time:1289789ms step_avg:170.25ms\nstep:7587/10000 train_loss:3.7443 train_time:1289962ms step_avg:170.25ms\nstep:7588/10000 train_loss:3.8074 train_time:1290128ms step_avg:170.25ms\nstep:7589/10000 train_loss:3.7990 train_time:1290300ms step_avg:170.25ms\nstep:7590/10000 train_loss:3.9214 train_time:1290468ms step_avg:170.25ms\nstep:7591/10000 train_loss:3.7994 train_time:1290638ms step_avg:170.25ms\nstep:7592/10000 train_loss:3.8639 train_time:1290809ms step_avg:170.25ms\nstep:7593/10000 train_loss:3.7908 train_time:1290980ms step_avg:170.25ms\nstep:7594/10000 train_loss:3.8229 train_time:1291150ms step_avg:170.25ms\nstep:7595/10000 train_loss:3.8068 train_time:1291321ms step_avg:170.25ms\nstep:7596/10000 train_loss:3.6576 train_time:1291491ms step_avg:170.25ms\nstep:7597/10000 train_loss:3.7450 train_time:1291658ms step_avg:170.25ms\nstep:7598/10000 train_loss:3.8157 train_time:1291829ms step_avg:170.25ms\nstep:7599/10000 train_loss:3.6818 train_time:1292000ms step_avg:170.25ms\nstep:7600/10000 train_loss:3.8108 train_time:1292168ms step_avg:170.25ms\nstep:7601/10000 train_loss:3.7757 train_time:1292338ms step_avg:170.25ms\nstep:7602/10000 train_loss:3.9832 train_time:1292508ms step_avg:170.25ms\nstep:7603/10000 train_loss:3.7011 train_time:1292675ms step_avg:170.25ms\nstep:7604/10000 train_loss:3.8053 train_time:1292846ms step_avg:170.25ms\nstep:7605/10000 train_loss:3.9484 train_time:1293021ms step_avg:170.25ms\nstep:7606/10000 train_loss:3.8300 train_time:1293190ms step_avg:170.25ms\nstep:7607/10000 train_loss:3.9091 train_time:1293361ms step_avg:170.25ms\nstep:7608/10000 train_loss:3.8664 train_time:1293530ms step_avg:170.25ms\nstep:7609/10000 train_loss:3.7663 train_time:1293698ms step_avg:170.25ms\nstep:7610/10000 train_loss:3.8116 train_time:1293869ms step_avg:170.25ms\nstep:7611/10000 train_loss:3.6979 train_time:1294044ms step_avg:170.25ms\nstep:7612/10000 train_loss:3.6564 train_time:1294215ms step_avg:170.25ms\nstep:7613/10000 train_loss:3.6345 train_time:1294383ms step_avg:170.25ms\nstep:7614/10000 train_loss:3.9248 train_time:1294553ms step_avg:170.25ms\nstep:7615/10000 train_loss:3.7525 train_time:1294723ms step_avg:170.25ms\nstep:7616/10000 train_loss:3.8796 train_time:1294893ms step_avg:170.25ms\nstep:7617/10000 train_loss:3.8139 train_time:1295061ms step_avg:170.25ms\nstep:7618/10000 train_loss:3.7596 train_time:1295231ms step_avg:170.25ms\nstep:7619/10000 train_loss:3.7682 train_time:1295402ms step_avg:170.25ms\nstep:7620/10000 train_loss:4.0608 train_time:1295793ms step_avg:170.27ms\nstep:7621/10000 train_loss:3.8504 train_time:1295957ms step_avg:170.27ms\nstep:7622/10000 train_loss:3.7719 train_time:1296123ms step_avg:170.27ms\nstep:7623/10000 train_loss:3.8173 train_time:1296290ms step_avg:170.27ms\nstep:7624/10000 train_loss:3.7690 train_time:1296459ms step_avg:170.27ms\nstep:7625/10000 train_loss:3.6497 train_time:1296627ms step_avg:170.27ms\nstep:7626/10000 train_loss:3.9205 train_time:1296809ms step_avg:170.27ms\nstep:7627/10000 train_loss:3.7704 train_time:1296981ms step_avg:170.27ms\nstep:7628/10000 train_loss:3.7527 train_time:1297151ms step_avg:170.27ms\nstep:7629/10000 train_loss:3.8623 train_time:1297321ms step_avg:170.27ms\nstep:7630/10000 train_loss:3.7456 train_time:1297488ms step_avg:170.27ms\nstep:7631/10000 train_loss:3.9039 train_time:1297659ms step_avg:170.27ms\nstep:7632/10000 train_loss:3.7651 train_time:1297832ms step_avg:170.27ms\nstep:7633/10000 train_loss:3.9183 train_time:1298004ms step_avg:170.27ms\nstep:7634/10000 train_loss:4.0185 train_time:1298172ms step_avg:170.27ms\nstep:7635/10000 train_loss:3.7427 train_time:1298343ms step_avg:170.27ms\nstep:7636/10000 train_loss:3.7753 train_time:1298512ms step_avg:170.27ms\nstep:7637/10000 train_loss:3.8570 train_time:1298684ms step_avg:170.27ms\nstep:7638/10000 train_loss:3.7531 train_time:1298853ms step_avg:170.27ms\nstep:7639/10000 train_loss:3.7567 train_time:1299023ms step_avg:170.27ms\nstep:7640/10000 train_loss:3.7852 train_time:1299193ms step_avg:170.27ms\nstep:7641/10000 train_loss:3.7949 train_time:1299363ms step_avg:170.27ms\nstep:7642/10000 train_loss:3.7460 train_time:1299531ms step_avg:170.27ms\nstep:7643/10000 train_loss:3.7419 train_time:1299701ms step_avg:170.27ms\nstep:7644/10000 train_loss:3.8836 train_time:1299871ms step_avg:170.27ms\nstep:7645/10000 train_loss:3.8053 train_time:1300045ms step_avg:170.27ms\nstep:7646/10000 train_loss:3.8330 train_time:1300215ms step_avg:170.27ms\nstep:7647/10000 train_loss:3.6699 train_time:1300386ms step_avg:170.27ms\nstep:7648/10000 train_loss:3.7993 train_time:1300555ms step_avg:170.27ms\nstep:7649/10000 train_loss:3.8034 train_time:1300722ms step_avg:170.27ms\nstep:7650/10000 train_loss:3.7241 train_time:1300891ms step_avg:170.27ms\nstep:7651/10000 train_loss:3.8250 train_time:1301063ms step_avg:170.27ms\nstep:7652/10000 train_loss:3.8121 train_time:1301236ms step_avg:170.27ms\nstep:7653/10000 train_loss:3.7859 train_time:1301407ms step_avg:170.27ms\nstep:7654/10000 train_loss:3.7735 train_time:1301577ms step_avg:170.27ms\nstep:7655/10000 train_loss:3.7738 train_time:1301745ms step_avg:170.27ms\nstep:7656/10000 train_loss:3.8566 train_time:1301914ms step_avg:170.27ms\nstep:7657/10000 train_loss:3.7520 train_time:1302084ms step_avg:170.27ms\nstep:7658/10000 train_loss:3.8654 train_time:1302255ms step_avg:170.27ms\nstep:7659/10000 train_loss:3.7369 train_time:1302426ms step_avg:170.27ms\nstep:7660/10000 train_loss:3.7708 train_time:1302599ms step_avg:170.27ms\nstep:7661/10000 train_loss:3.7036 train_time:1302768ms step_avg:170.27ms\nstep:7662/10000 train_loss:3.8019 train_time:1302937ms step_avg:170.27ms\nstep:7663/10000 train_loss:3.6660 train_time:1303107ms step_avg:170.27ms\nstep:7664/10000 train_loss:3.9028 train_time:1303276ms step_avg:170.27ms\nstep:7665/10000 train_loss:3.8271 train_time:1303446ms step_avg:170.27ms\nstep:7666/10000 train_loss:3.6621 train_time:1303617ms step_avg:170.27ms\nstep:7667/10000 train_loss:3.6887 train_time:1303785ms step_avg:170.27ms\nstep:7668/10000 train_loss:3.6819 train_time:1303955ms step_avg:170.27ms\nstep:7669/10000 train_loss:3.7618 train_time:1304126ms step_avg:170.27ms\nstep:7670/10000 train_loss:3.5822 train_time:1304296ms step_avg:170.27ms\nstep:7671/10000 train_loss:3.9018 train_time:1304468ms step_avg:170.27ms\nstep:7672/10000 train_loss:4.2404 train_time:1304639ms step_avg:170.27ms\nstep:7673/10000 train_loss:3.7662 train_time:1304806ms step_avg:170.27ms\nstep:7674/10000 train_loss:3.7523 train_time:1304976ms step_avg:170.27ms\nstep:7675/10000 train_loss:3.7787 train_time:1305146ms step_avg:170.27ms\nstep:7676/10000 train_loss:5.2728 train_time:1305316ms step_avg:170.27ms\nstep:7677/10000 train_loss:3.7979 train_time:1305486ms step_avg:170.27ms\nstep:7678/10000 train_loss:3.9879 train_time:1305657ms step_avg:170.27ms\nstep:7679/10000 train_loss:3.7211 train_time:1305826ms step_avg:170.27ms\nstep:7680/10000 train_loss:3.7167 train_time:1305995ms step_avg:170.27ms\nstep:7681/10000 train_loss:3.6508 train_time:1306168ms step_avg:170.27ms\nstep:7682/10000 train_loss:3.9435 train_time:1306339ms step_avg:170.27ms\nstep:7683/10000 train_loss:3.7957 train_time:1306508ms step_avg:170.27ms\nstep:7684/10000 train_loss:3.5754 train_time:1306679ms step_avg:170.27ms\nstep:7685/10000 train_loss:3.8554 train_time:1306851ms step_avg:170.27ms\nstep:7686/10000 train_loss:3.8894 train_time:1307020ms step_avg:170.27ms\nstep:7687/10000 train_loss:3.7523 train_time:1307191ms step_avg:170.27ms\nstep:7688/10000 train_loss:3.8173 train_time:1307360ms step_avg:170.27ms\nstep:7689/10000 train_loss:3.7525 train_time:1307528ms step_avg:170.27ms\nstep:7690/10000 train_loss:3.7710 train_time:1307696ms step_avg:170.27ms\nstep:7691/10000 train_loss:3.7682 train_time:1307868ms step_avg:170.27ms\nstep:7692/10000 train_loss:3.8712 train_time:1308040ms step_avg:170.27ms\nstep:7693/10000 train_loss:3.8031 train_time:1308208ms step_avg:170.27ms\nstep:7694/10000 train_loss:3.9685 train_time:1308380ms step_avg:170.27ms\nstep:7695/10000 train_loss:3.8334 train_time:1308551ms step_avg:170.27ms\nstep:7696/10000 train_loss:3.8317 train_time:1308722ms step_avg:170.27ms\nstep:7697/10000 train_loss:3.7706 train_time:1308891ms step_avg:170.27ms\nstep:7698/10000 train_loss:3.8069 train_time:1309059ms step_avg:170.27ms\nstep:7699/10000 train_loss:3.7853 train_time:1309234ms step_avg:170.27ms\nstep:7700/10000 train_loss:3.7178 train_time:1309404ms step_avg:170.27ms\nstep:7701/10000 train_loss:3.7494 train_time:1309574ms step_avg:170.27ms\nstep:7702/10000 train_loss:3.7195 train_time:1309745ms step_avg:170.27ms\nstep:7703/10000 train_loss:3.8870 train_time:1309918ms step_avg:170.27ms\nstep:7704/10000 train_loss:3.7101 train_time:1310087ms step_avg:170.27ms\nstep:7705/10000 train_loss:3.8337 train_time:1310258ms step_avg:170.27ms\nstep:7706/10000 train_loss:3.9029 train_time:1310428ms step_avg:170.27ms\nstep:7707/10000 train_loss:3.7085 train_time:1310600ms step_avg:170.27ms\nstep:7708/10000 train_loss:3.8228 train_time:1310768ms step_avg:170.27ms\nstep:7709/10000 train_loss:3.6982 train_time:1310937ms step_avg:170.27ms\nstep:7710/10000 train_loss:3.8709 train_time:1311106ms step_avg:170.27ms\nstep:7711/10000 train_loss:3.8198 train_time:1311276ms step_avg:170.27ms\nstep:7712/10000 train_loss:3.7360 train_time:1311447ms step_avg:170.27ms\nstep:7713/10000 train_loss:3.8641 train_time:1311616ms step_avg:170.27ms\nstep:7714/10000 train_loss:3.7009 train_time:1311786ms step_avg:170.27ms\nstep:7715/10000 train_loss:3.7795 train_time:1311958ms step_avg:170.27ms\nstep:7716/10000 train_loss:3.7844 train_time:1312126ms step_avg:170.27ms\nstep:7717/10000 train_loss:3.8186 train_time:1312298ms step_avg:170.27ms\nstep:7718/10000 train_loss:3.7521 train_time:1312468ms step_avg:170.27ms\nstep:7719/10000 train_loss:3.8169 train_time:1312641ms step_avg:170.27ms\nstep:7720/10000 train_loss:3.8429 train_time:1312812ms step_avg:170.27ms\nstep:7721/10000 train_loss:3.7688 train_time:1312986ms step_avg:170.27ms\nstep:7722/10000 train_loss:3.6618 train_time:1313156ms step_avg:170.27ms\nstep:7723/10000 train_loss:3.7464 train_time:1313325ms step_avg:170.27ms\nstep:7724/10000 train_loss:3.9055 train_time:1313496ms step_avg:170.27ms\nstep:7725/10000 train_loss:3.8112 train_time:1313667ms step_avg:170.27ms\nstep:7726/10000 train_loss:3.7370 train_time:1313840ms step_avg:170.27ms\nstep:7727/10000 train_loss:3.8458 train_time:1314008ms step_avg:170.27ms\nstep:7728/10000 train_loss:3.6914 train_time:1314181ms step_avg:170.27ms\nstep:7729/10000 train_loss:4.1108 train_time:1314349ms step_avg:170.27ms\nstep:7730/10000 train_loss:3.6723 train_time:1314519ms step_avg:170.27ms\nstep:7731/10000 train_loss:3.9962 train_time:1314687ms step_avg:170.27ms\nstep:7732/10000 train_loss:3.7961 train_time:1314859ms step_avg:170.27ms\nstep:7733/10000 train_loss:3.8650 train_time:1315027ms step_avg:170.27ms\nstep:7734/10000 train_loss:3.7252 train_time:1315197ms step_avg:170.27ms\nstep:7735/10000 train_loss:3.8093 train_time:1315365ms step_avg:170.27ms\nstep:7736/10000 train_loss:3.9416 train_time:1315534ms step_avg:170.27ms\nstep:7737/10000 train_loss:3.8635 train_time:1315707ms step_avg:170.27ms\nstep:7738/10000 train_loss:3.8545 train_time:1315879ms step_avg:170.27ms\nstep:7739/10000 train_loss:3.9905 train_time:1316048ms step_avg:170.27ms\nstep:7740/10000 train_loss:3.7869 train_time:1316219ms step_avg:170.27ms\nstep:7741/10000 train_loss:3.8279 train_time:1316388ms step_avg:170.27ms\nstep:7742/10000 train_loss:3.7381 train_time:1316556ms step_avg:170.27ms\nstep:7743/10000 train_loss:3.7128 train_time:1316725ms step_avg:170.27ms\nstep:7744/10000 train_loss:3.6384 train_time:1316896ms step_avg:170.27ms\nstep:7745/10000 train_loss:3.6884 train_time:1317064ms step_avg:170.27ms\nstep:7746/10000 train_loss:3.8632 train_time:1317236ms step_avg:170.27ms\nstep:7747/10000 train_loss:3.6411 train_time:1317406ms step_avg:170.27ms\nstep:7748/10000 train_loss:5.5906 train_time:1317575ms step_avg:170.27ms\nstep:7749/10000 train_loss:3.8816 train_time:1317747ms step_avg:170.27ms\nstep:7750/10000 train_loss:3.7910 train_time:1317919ms step_avg:170.27ms\nstep:7751/10000 train_loss:3.7180 train_time:1318088ms step_avg:170.27ms\nstep:7752/10000 train_loss:3.8475 train_time:1318259ms step_avg:170.27ms\nstep:7753/10000 train_loss:3.6714 train_time:1318429ms step_avg:170.27ms\nstep:7754/10000 train_loss:3.8009 train_time:1318596ms step_avg:170.27ms\nstep:7755/10000 train_loss:3.7778 train_time:1318765ms step_avg:170.27ms\nstep:7756/10000 train_loss:3.7968 train_time:1318937ms step_avg:170.27ms\nstep:7757/10000 train_loss:3.8263 train_time:1319106ms step_avg:170.27ms\nstep:7758/10000 train_loss:3.8536 train_time:1319278ms step_avg:170.27ms\nstep:7759/10000 train_loss:3.7955 train_time:1319450ms step_avg:170.27ms\nstep:7760/10000 train_loss:4.0518 train_time:1319621ms step_avg:170.27ms\nstep:7761/10000 train_loss:3.7599 train_time:1319789ms step_avg:170.27ms\nstep:7762/10000 train_loss:3.7882 train_time:1319960ms step_avg:170.27ms\nstep:7763/10000 train_loss:3.8478 train_time:1320129ms step_avg:170.27ms\nstep:7764/10000 train_loss:3.8177 train_time:1320300ms step_avg:170.27ms\nstep:7765/10000 train_loss:3.7653 train_time:1320470ms step_avg:170.27ms\nstep:7766/10000 train_loss:3.8114 train_time:1320642ms step_avg:170.27ms\nstep:7767/10000 train_loss:3.7451 train_time:1320813ms step_avg:170.27ms\nstep:7768/10000 train_loss:3.7959 train_time:1320984ms step_avg:170.27ms\nstep:7769/10000 train_loss:3.7560 train_time:1321153ms step_avg:170.27ms\nstep:7770/10000 train_loss:3.7510 train_time:1321324ms step_avg:170.27ms\nstep:7771/10000 train_loss:3.8187 train_time:1321494ms step_avg:170.27ms\nstep:7772/10000 train_loss:3.7949 train_time:1321664ms step_avg:170.27ms\nstep:7773/10000 train_loss:3.7238 train_time:1321835ms step_avg:170.27ms\nstep:7774/10000 train_loss:3.8441 train_time:1322007ms step_avg:170.27ms\nstep:7775/10000 train_loss:3.7816 train_time:1322178ms step_avg:170.27ms\nstep:7776/10000 train_loss:3.8352 train_time:1322346ms step_avg:170.27ms\nstep:7777/10000 train_loss:3.7200 train_time:1322518ms step_avg:170.27ms\nstep:7778/10000 train_loss:3.8713 train_time:1322688ms step_avg:170.27ms\nstep:7779/10000 train_loss:3.8468 train_time:1322859ms step_avg:170.27ms\nstep:7780/10000 train_loss:3.6975 train_time:1323028ms step_avg:170.27ms\nstep:7781/10000 train_loss:3.9051 train_time:1323197ms step_avg:170.27ms\nstep:7782/10000 train_loss:3.7032 train_time:1323367ms step_avg:170.27ms\nstep:7783/10000 train_loss:3.7794 train_time:1323539ms step_avg:170.27ms\nstep:7784/10000 train_loss:3.8425 train_time:1323707ms step_avg:170.27ms\nstep:7785/10000 train_loss:3.8394 train_time:1323878ms step_avg:170.27ms\nstep:7786/10000 train_loss:3.7285 train_time:1324048ms step_avg:170.27ms\nstep:7787/10000 train_loss:3.7460 train_time:1324219ms step_avg:170.27ms\nstep:7788/10000 train_loss:3.6261 train_time:1324389ms step_avg:170.27ms\nstep:7789/10000 train_loss:3.7598 train_time:1324558ms step_avg:170.27ms\nstep:7790/10000 train_loss:3.8201 train_time:1324727ms step_avg:170.27ms\nstep:7791/10000 train_loss:3.8160 train_time:1324900ms step_avg:170.27ms\nstep:7792/10000 train_loss:3.9755 train_time:1325069ms step_avg:170.27ms\nstep:7793/10000 train_loss:3.9110 train_time:1325238ms step_avg:170.27ms\nstep:7794/10000 train_loss:3.6576 train_time:1325406ms step_avg:170.27ms\nstep:7795/10000 train_loss:3.7250 train_time:1325575ms step_avg:170.27ms\nstep:7796/10000 train_loss:3.6399 train_time:1325744ms step_avg:170.27ms\nstep:7797/10000 train_loss:3.8352 train_time:1325914ms step_avg:170.27ms\nstep:7798/10000 train_loss:3.9072 train_time:1326084ms step_avg:170.27ms\nstep:7799/10000 train_loss:3.6635 train_time:1326255ms step_avg:170.27ms\nstep:7800/10000 train_loss:3.8522 train_time:1326426ms step_avg:170.27ms\nstep:7801/10000 train_loss:3.7912 train_time:1326597ms step_avg:170.27ms\nstep:7802/10000 train_loss:3.7948 train_time:1326768ms step_avg:170.27ms\nstep:7803/10000 train_loss:3.8007 train_time:1326939ms step_avg:170.27ms\nstep:7804/10000 train_loss:3.8093 train_time:1327108ms step_avg:170.27ms\nstep:7805/10000 train_loss:3.9127 train_time:1327278ms step_avg:170.27ms\nstep:7806/10000 train_loss:3.7903 train_time:1327450ms step_avg:170.27ms\nstep:7807/10000 train_loss:3.7057 train_time:1327620ms step_avg:170.27ms\nstep:7808/10000 train_loss:3.7287 train_time:1327790ms step_avg:170.27ms\nstep:7809/10000 train_loss:3.6998 train_time:1327960ms step_avg:170.27ms\nstep:7810/10000 train_loss:3.7959 train_time:1328128ms step_avg:170.27ms\nstep:7811/10000 train_loss:3.7895 train_time:1328299ms step_avg:170.27ms\nstep:7812/10000 train_loss:3.8917 train_time:1328468ms step_avg:170.27ms\nstep:7813/10000 train_loss:3.9118 train_time:1328642ms step_avg:170.27ms\nstep:7814/10000 train_loss:3.6666 train_time:1328810ms step_avg:170.27ms\nstep:7815/10000 train_loss:3.6660 train_time:1328981ms step_avg:170.27ms\nstep:7816/10000 train_loss:3.8173 train_time:1329150ms step_avg:170.27ms\nstep:7817/10000 train_loss:3.8175 train_time:1329317ms step_avg:170.27ms\nstep:7818/10000 train_loss:3.7846 train_time:1329487ms step_avg:170.27ms\nstep:7819/10000 train_loss:3.7898 train_time:1329658ms step_avg:170.27ms\nstep:7820/10000 train_loss:3.7481 train_time:1329829ms step_avg:170.27ms\nstep:7821/10000 train_loss:3.8302 train_time:1330002ms step_avg:170.27ms\nstep:7822/10000 train_loss:3.6243 train_time:1330172ms step_avg:170.27ms\nstep:7823/10000 train_loss:3.8012 train_time:1330344ms step_avg:170.27ms\nstep:7824/10000 train_loss:3.7999 train_time:1330516ms step_avg:170.27ms\nstep:7825/10000 train_loss:3.9012 train_time:1330684ms step_avg:170.27ms\nstep:7826/10000 train_loss:3.8274 train_time:1330853ms step_avg:170.27ms\nstep:7827/10000 train_loss:3.7366 train_time:1331024ms step_avg:170.27ms\nstep:7828/10000 train_loss:3.7203 train_time:1331195ms step_avg:170.27ms\nstep:7829/10000 train_loss:3.7941 train_time:1331366ms step_avg:170.27ms\nstep:7830/10000 train_loss:3.8347 train_time:1331536ms step_avg:170.27ms\nstep:7831/10000 train_loss:3.8736 train_time:1331704ms step_avg:170.27ms\nstep:7832/10000 train_loss:3.7096 train_time:1331874ms step_avg:170.27ms\nstep:7833/10000 train_loss:3.8247 train_time:1332046ms step_avg:170.27ms\nstep:7834/10000 train_loss:3.7800 train_time:1332216ms step_avg:170.27ms\nstep:7835/10000 train_loss:3.6959 train_time:1332387ms step_avg:170.27ms\nstep:7836/10000 train_loss:3.6929 train_time:1332558ms step_avg:170.27ms\nstep:7837/10000 train_loss:3.7113 train_time:1332728ms step_avg:170.27ms\nstep:7838/10000 train_loss:3.6784 train_time:1332899ms step_avg:170.27ms\nstep:7839/10000 train_loss:3.8805 train_time:1333070ms step_avg:170.27ms\nstep:7840/10000 train_loss:3.8247 train_time:1333241ms step_avg:170.27ms\nstep:7841/10000 train_loss:3.9320 train_time:1333412ms step_avg:170.27ms\nstep:7842/10000 train_loss:3.7481 train_time:1333582ms step_avg:170.27ms\nstep:7843/10000 train_loss:3.6920 train_time:1333751ms step_avg:170.27ms\nstep:7844/10000 train_loss:3.8582 train_time:1333922ms step_avg:170.27ms\nstep:7845/10000 train_loss:3.9196 train_time:1334092ms step_avg:170.27ms\nstep:7846/10000 train_loss:3.9421 train_time:1334262ms step_avg:170.27ms\nstep:7847/10000 train_loss:3.8356 train_time:1334432ms step_avg:170.27ms\nstep:7848/10000 train_loss:3.6334 train_time:1334602ms step_avg:170.27ms\nstep:7849/10000 train_loss:3.7749 train_time:1334774ms step_avg:170.27ms\nstep:7850/10000 train_loss:3.8573 train_time:1334944ms step_avg:170.27ms\nstep:7851/10000 train_loss:3.7978 train_time:1335115ms step_avg:170.27ms\nstep:7852/10000 train_loss:3.8492 train_time:1335286ms step_avg:170.27ms\nstep:7853/10000 train_loss:3.7519 train_time:1335458ms step_avg:170.27ms\nstep:7854/10000 train_loss:3.8745 train_time:1335626ms step_avg:170.27ms\nstep:7855/10000 train_loss:3.8149 train_time:1335799ms step_avg:170.27ms\nstep:7856/10000 train_loss:3.8129 train_time:1335967ms step_avg:170.27ms\nstep:7857/10000 train_loss:3.9837 train_time:1336139ms step_avg:170.27ms\nstep:7858/10000 train_loss:3.7005 train_time:1336308ms step_avg:170.27ms\nstep:7859/10000 train_loss:3.9551 train_time:1336481ms step_avg:170.27ms\nstep:7860/10000 train_loss:3.6448 train_time:1336648ms step_avg:170.27ms\nstep:7861/10000 train_loss:3.8066 train_time:1336820ms step_avg:170.27ms\nstep:7862/10000 train_loss:3.8050 train_time:1336990ms step_avg:170.27ms\nstep:7863/10000 train_loss:3.8423 train_time:1337160ms step_avg:170.27ms\nstep:7864/10000 train_loss:3.9658 train_time:1337330ms step_avg:170.27ms\nstep:7865/10000 train_loss:3.8305 train_time:1337498ms step_avg:170.27ms\nstep:7866/10000 train_loss:3.8385 train_time:1337668ms step_avg:170.27ms\nstep:7867/10000 train_loss:3.7736 train_time:1337838ms step_avg:170.27ms\nstep:7868/10000 train_loss:3.8131 train_time:1338010ms step_avg:170.27ms\nstep:7869/10000 train_loss:3.7263 train_time:1338182ms step_avg:170.27ms\nstep:7870/10000 train_loss:3.8109 train_time:1338349ms step_avg:170.27ms\nstep:7871/10000 train_loss:3.7810 train_time:1338519ms step_avg:170.27ms\nstep:7872/10000 train_loss:3.7150 train_time:1338687ms step_avg:170.27ms\nstep:7873/10000 train_loss:3.8688 train_time:1338859ms step_avg:170.27ms\nstep:7874/10000 train_loss:3.6928 train_time:1339030ms step_avg:170.27ms\nstep:7875/10000 train_loss:3.7886 train_time:1339202ms step_avg:170.27ms\nstep:7876/10000 train_loss:3.7435 train_time:1339368ms step_avg:170.27ms\nstep:7877/10000 train_loss:3.8396 train_time:1339539ms step_avg:170.27ms\nstep:7878/10000 train_loss:4.0926 train_time:1339708ms step_avg:170.27ms\nstep:7879/10000 train_loss:3.7573 train_time:1339878ms step_avg:170.27ms\nstep:7880/10000 train_loss:3.8506 train_time:1340045ms step_avg:170.27ms\nstep:7881/10000 train_loss:3.7731 train_time:1340218ms step_avg:170.27ms\nstep:7882/10000 train_loss:3.7610 train_time:1340387ms step_avg:170.27ms\nstep:7883/10000 train_loss:3.7876 train_time:1340557ms step_avg:170.27ms\nstep:7884/10000 train_loss:3.9658 train_time:1340729ms step_avg:170.27ms\nstep:7885/10000 train_loss:3.9152 train_time:1340902ms step_avg:170.27ms\nstep:7886/10000 train_loss:3.7495 train_time:1341072ms step_avg:170.27ms\nstep:7887/10000 train_loss:3.8123 train_time:1341242ms step_avg:170.27ms\nstep:7888/10000 train_loss:3.6051 train_time:1341412ms step_avg:170.27ms\nstep:7889/10000 train_loss:3.8190 train_time:1341581ms step_avg:170.27ms\nstep:7890/10000 train_loss:3.8130 train_time:1341750ms step_avg:170.27ms\nstep:7891/10000 train_loss:3.7814 train_time:1341925ms step_avg:170.27ms\nstep:7892/10000 train_loss:3.7875 train_time:1342095ms step_avg:170.27ms\nstep:7893/10000 train_loss:3.8270 train_time:1342265ms step_avg:170.27ms\nstep:7894/10000 train_loss:3.7777 train_time:1342434ms step_avg:170.27ms\nstep:7895/10000 train_loss:3.8619 train_time:1342604ms step_avg:170.27ms\nstep:7896/10000 train_loss:3.8677 train_time:1342774ms step_avg:170.27ms\nstep:7897/10000 train_loss:3.6053 train_time:1342943ms step_avg:170.27ms\nstep:7898/10000 train_loss:3.6831 train_time:1343114ms step_avg:170.27ms\nstep:7899/10000 train_loss:3.8794 train_time:1343287ms step_avg:170.27ms\nstep:7900/10000 train_loss:3.7534 train_time:1343456ms step_avg:170.27ms\nstep:7901/10000 train_loss:3.7085 train_time:1343625ms step_avg:170.27ms\nstep:7902/10000 train_loss:3.6518 train_time:1343798ms step_avg:170.27ms\nstep:7903/10000 train_loss:3.8263 train_time:1343969ms step_avg:170.27ms\nstep:7904/10000 train_loss:3.9642 train_time:1344140ms step_avg:170.27ms\nstep:7905/10000 train_loss:3.8945 train_time:1344309ms step_avg:170.27ms\nstep:7906/10000 train_loss:3.7599 train_time:1344479ms step_avg:170.27ms\nstep:7907/10000 train_loss:3.6818 train_time:1344649ms step_avg:170.27ms\nstep:7908/10000 train_loss:3.7959 train_time:1344821ms step_avg:170.27ms\nstep:7909/10000 train_loss:3.8269 train_time:1344990ms step_avg:170.27ms\nstep:7910/10000 train_loss:3.7684 train_time:1345161ms step_avg:170.27ms\nstep:7911/10000 train_loss:3.5677 train_time:1345331ms step_avg:170.27ms\nstep:7912/10000 train_loss:3.8601 train_time:1345501ms step_avg:170.27ms\nstep:7913/10000 train_loss:3.6309 train_time:1345670ms step_avg:170.27ms\nstep:7914/10000 train_loss:3.6814 train_time:1345845ms step_avg:170.27ms\nstep:7915/10000 train_loss:3.7689 train_time:1346015ms step_avg:170.27ms\nstep:7916/10000 train_loss:3.7934 train_time:1346187ms step_avg:170.27ms\nstep:7917/10000 train_loss:3.7781 train_time:1346359ms step_avg:170.27ms\nstep:7918/10000 train_loss:3.8455 train_time:1346526ms step_avg:170.27ms\nstep:7919/10000 train_loss:3.8368 train_time:1346697ms step_avg:170.27ms\nstep:7920/10000 train_loss:3.7684 train_time:1346863ms step_avg:170.27ms\nstep:7921/10000 train_loss:3.8180 train_time:1347034ms step_avg:170.27ms\nstep:7922/10000 train_loss:3.7240 train_time:1347204ms step_avg:170.27ms\nstep:7923/10000 train_loss:3.6533 train_time:1347374ms step_avg:170.27ms\nstep:7924/10000 train_loss:3.8541 train_time:1347548ms step_avg:170.27ms\nstep:7925/10000 train_loss:3.7678 train_time:1347719ms step_avg:170.27ms\nstep:7926/10000 train_loss:4.0935 train_time:1347886ms step_avg:170.27ms\nstep:7927/10000 train_loss:3.8701 train_time:1348057ms step_avg:170.27ms\nstep:7928/10000 train_loss:3.8445 train_time:1348229ms step_avg:170.27ms\nstep:7929/10000 train_loss:3.7682 train_time:1348400ms step_avg:170.27ms\nstep:7930/10000 train_loss:3.8070 train_time:1348569ms step_avg:170.27ms\nstep:7931/10000 train_loss:3.7813 train_time:1348743ms step_avg:170.27ms\nstep:7932/10000 train_loss:3.8709 train_time:1348914ms step_avg:170.27ms\nstep:7933/10000 train_loss:3.7274 train_time:1349087ms step_avg:170.27ms\nstep:7934/10000 train_loss:3.7876 train_time:1349258ms step_avg:170.27ms\nstep:7935/10000 train_loss:3.7924 train_time:1349427ms step_avg:170.27ms\nstep:7936/10000 train_loss:3.7631 train_time:1349598ms step_avg:170.27ms\nstep:7937/10000 train_loss:3.8027 train_time:1349768ms step_avg:170.27ms\nstep:7938/10000 train_loss:3.7698 train_time:1349938ms step_avg:170.27ms\nstep:7939/10000 train_loss:3.8284 train_time:1350109ms step_avg:170.27ms\nstep:7940/10000 train_loss:3.7162 train_time:1350281ms step_avg:170.28ms\nstep:7941/10000 train_loss:3.8279 train_time:1350449ms step_avg:170.27ms\nstep:7942/10000 train_loss:3.8158 train_time:1350620ms step_avg:170.27ms\nstep:7943/10000 train_loss:3.7620 train_time:1350790ms step_avg:170.27ms\nstep:7944/10000 train_loss:3.8976 train_time:1350960ms step_avg:170.27ms\nstep:7945/10000 train_loss:3.7766 train_time:1351131ms step_avg:170.27ms\nstep:7946/10000 train_loss:3.7622 train_time:1351305ms step_avg:170.28ms\nstep:7947/10000 train_loss:3.8236 train_time:1351476ms step_avg:170.28ms\nstep:7948/10000 train_loss:3.8962 train_time:1351646ms step_avg:170.28ms\nstep:7949/10000 train_loss:3.9109 train_time:1351822ms step_avg:170.28ms\nstep:7950/10000 train_loss:3.7393 train_time:1351987ms step_avg:170.28ms\nstep:7951/10000 train_loss:3.8392 train_time:1352157ms step_avg:170.28ms\nstep:7952/10000 train_loss:4.0453 train_time:1352325ms step_avg:170.28ms\nstep:7953/10000 train_loss:3.7589 train_time:1352495ms step_avg:170.28ms\nstep:7954/10000 train_loss:3.7536 train_time:1352666ms step_avg:170.28ms\nstep:7955/10000 train_loss:3.5350 train_time:1352836ms step_avg:170.28ms\nstep:7956/10000 train_loss:3.8568 train_time:1353006ms step_avg:170.28ms\nstep:7957/10000 train_loss:3.7531 train_time:1353177ms step_avg:170.28ms\nstep:7958/10000 train_loss:3.8186 train_time:1353346ms step_avg:170.28ms\nstep:7959/10000 train_loss:3.8876 train_time:1353515ms step_avg:170.27ms\nstep:7960/10000 train_loss:3.7479 train_time:1353683ms step_avg:170.27ms\nstep:7961/10000 train_loss:3.7844 train_time:1353854ms step_avg:170.27ms\nstep:7962/10000 train_loss:3.8232 train_time:1354024ms step_avg:170.27ms\nstep:7963/10000 train_loss:3.7330 train_time:1354193ms step_avg:170.27ms\nstep:7964/10000 train_loss:3.8464 train_time:1354364ms step_avg:170.27ms\nstep:7965/10000 train_loss:3.8154 train_time:1354533ms step_avg:170.27ms\nstep:7966/10000 train_loss:3.7639 train_time:1354710ms step_avg:170.28ms\nstep:7967/10000 train_loss:3.7110 train_time:1354873ms step_avg:170.27ms\nstep:7968/10000 train_loss:3.7592 train_time:1355045ms step_avg:170.27ms\nstep:7969/10000 train_loss:3.8444 train_time:1355217ms step_avg:170.27ms\nstep:7970/10000 train_loss:3.8543 train_time:1355387ms step_avg:170.27ms\nstep:7971/10000 train_loss:3.7575 train_time:1355558ms step_avg:170.27ms\nstep:7972/10000 train_loss:3.8825 train_time:1355728ms step_avg:170.27ms\nstep:7973/10000 train_loss:3.8574 train_time:1355900ms step_avg:170.27ms\nstep:7974/10000 train_loss:3.8812 train_time:1356070ms step_avg:170.27ms\nstep:7975/10000 train_loss:3.6810 train_time:1356242ms step_avg:170.28ms\nstep:7976/10000 train_loss:3.8177 train_time:1356410ms step_avg:170.27ms\nstep:7977/10000 train_loss:3.6117 train_time:1356579ms step_avg:170.27ms\nstep:7978/10000 train_loss:3.6331 train_time:1356749ms step_avg:170.27ms\nstep:7979/10000 train_loss:3.8113 train_time:1356919ms step_avg:170.27ms\nstep:7980/10000 train_loss:3.6570 train_time:1357088ms step_avg:170.27ms\nstep:7981/10000 train_loss:4.0022 train_time:1357257ms step_avg:170.27ms\nstep:7982/10000 train_loss:3.6777 train_time:1357426ms step_avg:170.27ms\nstep:7983/10000 train_loss:3.8650 train_time:1357597ms step_avg:170.27ms\nstep:7984/10000 train_loss:3.8313 train_time:1357765ms step_avg:170.27ms\nstep:7985/10000 train_loss:3.8174 train_time:1357935ms step_avg:170.27ms\nstep:7986/10000 train_loss:3.8616 train_time:1358106ms step_avg:170.27ms\nstep:7987/10000 train_loss:3.8181 train_time:1358275ms step_avg:170.27ms\nstep:7988/10000 train_loss:3.8165 train_time:1358443ms step_avg:170.27ms\nstep:7989/10000 train_loss:3.8285 train_time:1358613ms step_avg:170.27ms\nstep:7990/10000 train_loss:3.6591 train_time:1358784ms step_avg:170.27ms\nstep:7991/10000 train_loss:3.8776 train_time:1358955ms step_avg:170.27ms\nstep:7992/10000 train_loss:3.8207 train_time:1359126ms step_avg:170.27ms\nstep:7993/10000 train_loss:3.8845 train_time:1359295ms step_avg:170.27ms\nstep:7994/10000 train_loss:3.7862 train_time:1359464ms step_avg:170.27ms\nstep:7995/10000 train_loss:3.7247 train_time:1359635ms step_avg:170.27ms\nstep:7996/10000 train_loss:4.4124 train_time:1359803ms step_avg:170.27ms\nstep:7997/10000 train_loss:3.7470 train_time:1359971ms step_avg:170.27ms\nstep:7998/10000 train_loss:3.8387 train_time:1360141ms step_avg:170.27ms\nstep:7999/10000 train_loss:3.9056 train_time:1360311ms step_avg:170.27ms\nstep:8000/10000 train_loss:3.7720 train_time:1360482ms step_avg:170.27ms\nstep:8001/10000 train_loss:3.6868 train_time:1360652ms step_avg:170.27ms\nstep:8002/10000 train_loss:3.6872 train_time:1360826ms step_avg:170.27ms\nstep:8003/10000 train_loss:3.9457 train_time:1360996ms step_avg:170.27ms\nstep:8004/10000 train_loss:3.8142 train_time:1361165ms step_avg:170.27ms\nstep:8005/10000 train_loss:4.0550 train_time:1361335ms step_avg:170.27ms\nstep:8006/10000 train_loss:3.7989 train_time:1361505ms step_avg:170.27ms\nstep:8007/10000 train_loss:3.7605 train_time:1361674ms step_avg:170.27ms\nstep:8008/10000 train_loss:3.7588 train_time:1361843ms step_avg:170.27ms\nstep:8009/10000 train_loss:4.0862 train_time:1362016ms step_avg:170.27ms\nstep:8010/10000 train_loss:3.8341 train_time:1362186ms step_avg:170.27ms\nstep:8011/10000 train_loss:3.7591 train_time:1362355ms step_avg:170.27ms\nstep:8012/10000 train_loss:3.8629 train_time:1362526ms step_avg:170.27ms\nstep:8013/10000 train_loss:3.7689 train_time:1362696ms step_avg:170.27ms\nstep:8014/10000 train_loss:3.7299 train_time:1362866ms step_avg:170.27ms\nstep:8015/10000 train_loss:4.2546 train_time:1363036ms step_avg:170.27ms\nstep:8016/10000 train_loss:3.8121 train_time:1363205ms step_avg:170.27ms\nstep:8017/10000 train_loss:3.7859 train_time:1363375ms step_avg:170.27ms\nstep:8018/10000 train_loss:3.8581 train_time:1363547ms step_avg:170.27ms\nstep:8019/10000 train_loss:3.5695 train_time:1363718ms step_avg:170.27ms\nstep:8020/10000 train_loss:3.7496 train_time:1363885ms step_avg:170.27ms\nstep:8021/10000 train_loss:3.6138 train_time:1364054ms step_avg:170.27ms\nstep:8022/10000 train_loss:3.8348 train_time:1364224ms step_avg:170.27ms\nstep:8023/10000 train_loss:3.8242 train_time:1364392ms step_avg:170.27ms\nstep:8024/10000 train_loss:3.7839 train_time:1364563ms step_avg:170.27ms\nstep:8025/10000 train_loss:3.8670 train_time:1364733ms step_avg:170.27ms\nstep:8026/10000 train_loss:3.8950 train_time:1364904ms step_avg:170.27ms\nstep:8027/10000 train_loss:3.8118 train_time:1365075ms step_avg:170.27ms\nstep:8028/10000 train_loss:3.6340 train_time:1365245ms step_avg:170.27ms\nstep:8029/10000 train_loss:3.9089 train_time:1365415ms step_avg:170.27ms\nstep:8030/10000 train_loss:3.8148 train_time:1365584ms step_avg:170.27ms\nstep:8031/10000 train_loss:3.7616 train_time:1365754ms step_avg:170.27ms\nstep:8032/10000 train_loss:3.7941 train_time:1365925ms step_avg:170.27ms\nstep:8033/10000 train_loss:3.8173 train_time:1366097ms step_avg:170.27ms\nstep:8034/10000 train_loss:3.8675 train_time:1366266ms step_avg:170.27ms\nstep:8035/10000 train_loss:4.7298 train_time:1366435ms step_avg:170.27ms\nstep:8036/10000 train_loss:3.8108 train_time:1366605ms step_avg:170.27ms\nstep:8037/10000 train_loss:3.8466 train_time:1366774ms step_avg:170.27ms\nstep:8038/10000 train_loss:3.7178 train_time:1366943ms step_avg:170.27ms\nstep:8039/10000 train_loss:3.7018 train_time:1367114ms step_avg:170.27ms\nstep:8040/10000 train_loss:3.7701 train_time:1367284ms step_avg:170.27ms\nstep:8041/10000 train_loss:3.8149 train_time:1367456ms step_avg:170.27ms\nstep:8042/10000 train_loss:3.7546 train_time:1367624ms step_avg:170.27ms\nstep:8043/10000 train_loss:3.8109 train_time:1367794ms step_avg:170.27ms\nstep:8044/10000 train_loss:3.7141 train_time:1367962ms step_avg:170.27ms\nstep:8045/10000 train_loss:3.7744 train_time:1368133ms step_avg:170.27ms\nstep:8046/10000 train_loss:3.7614 train_time:1368302ms step_avg:170.27ms\nstep:8047/10000 train_loss:3.8336 train_time:1368471ms step_avg:170.27ms\nstep:8048/10000 train_loss:3.7539 train_time:1368643ms step_avg:170.27ms\nstep:8049/10000 train_loss:3.8344 train_time:1368813ms step_avg:170.27ms\nstep:8050/10000 train_loss:3.8188 train_time:1368984ms step_avg:170.27ms\nstep:8051/10000 train_loss:3.6765 train_time:1369157ms step_avg:170.27ms\nstep:8052/10000 train_loss:3.6342 train_time:1369326ms step_avg:170.27ms\nstep:8053/10000 train_loss:3.7328 train_time:1369495ms step_avg:170.27ms\nstep:8054/10000 train_loss:3.7238 train_time:1369662ms step_avg:170.27ms\nstep:8055/10000 train_loss:3.8124 train_time:1369833ms step_avg:170.27ms\nstep:8056/10000 train_loss:3.7061 train_time:1370007ms step_avg:170.27ms\nstep:8057/10000 train_loss:3.9809 train_time:1370172ms step_avg:170.27ms\nstep:8058/10000 train_loss:3.8149 train_time:1370344ms step_avg:170.27ms\nstep:8059/10000 train_loss:3.9074 train_time:1370513ms step_avg:170.27ms\nstep:8060/10000 train_loss:3.7048 train_time:1370687ms step_avg:170.27ms\nstep:8061/10000 train_loss:3.7401 train_time:1370856ms step_avg:170.27ms\nstep:8062/10000 train_loss:3.7741 train_time:1371024ms step_avg:170.27ms\nstep:8063/10000 train_loss:3.9454 train_time:1371193ms step_avg:170.27ms\nstep:8064/10000 train_loss:3.7601 train_time:1371364ms step_avg:170.27ms\nstep:8065/10000 train_loss:3.8444 train_time:1371533ms step_avg:170.27ms\nstep:8066/10000 train_loss:3.7979 train_time:1371708ms step_avg:170.27ms\nstep:8067/10000 train_loss:3.7309 train_time:1371879ms step_avg:170.27ms\nstep:8068/10000 train_loss:3.7643 train_time:1372049ms step_avg:170.27ms\nstep:8069/10000 train_loss:3.9650 train_time:1372217ms step_avg:170.27ms\nstep:8070/10000 train_loss:3.7234 train_time:1372387ms step_avg:170.27ms\nstep:8071/10000 train_loss:3.8874 train_time:1372556ms step_avg:170.27ms\nstep:8072/10000 train_loss:3.7334 train_time:1372727ms step_avg:170.27ms\nstep:8073/10000 train_loss:3.8520 train_time:1372899ms step_avg:170.27ms\nstep:8074/10000 train_loss:3.7767 train_time:1373070ms step_avg:170.27ms\nstep:8075/10000 train_loss:3.6965 train_time:1373240ms step_avg:170.27ms\nstep:8076/10000 train_loss:3.7095 train_time:1373410ms step_avg:170.27ms\nstep:8077/10000 train_loss:3.9435 train_time:1373581ms step_avg:170.27ms\nstep:8078/10000 train_loss:3.7617 train_time:1373750ms step_avg:170.27ms\nstep:8079/10000 train_loss:3.7663 train_time:1373924ms step_avg:170.27ms\nstep:8080/10000 train_loss:3.6395 train_time:1374095ms step_avg:170.27ms\nstep:8081/10000 train_loss:3.7691 train_time:1374267ms step_avg:170.27ms\nstep:8082/10000 train_loss:3.9168 train_time:1374440ms step_avg:170.27ms\nstep:8083/10000 train_loss:3.8240 train_time:1374612ms step_avg:170.27ms\nstep:8084/10000 train_loss:3.9287 train_time:1374782ms step_avg:170.27ms\nstep:8085/10000 train_loss:3.8183 train_time:1374951ms step_avg:170.27ms\nstep:8086/10000 train_loss:3.7388 train_time:1375121ms step_avg:170.27ms\nstep:8087/10000 train_loss:3.8170 train_time:1375292ms step_avg:170.27ms\nstep:8088/10000 train_loss:3.8331 train_time:1375465ms step_avg:170.27ms\nstep:8089/10000 train_loss:3.6944 train_time:1375637ms step_avg:170.27ms\nstep:8090/10000 train_loss:3.9043 train_time:1375808ms step_avg:170.27ms\nstep:8091/10000 train_loss:3.8294 train_time:1375977ms step_avg:170.27ms\nstep:8092/10000 train_loss:3.7771 train_time:1376148ms step_avg:170.27ms\nstep:8093/10000 train_loss:3.7925 train_time:1376316ms step_avg:170.27ms\nstep:8094/10000 train_loss:3.8999 train_time:1376486ms step_avg:170.27ms\nstep:8095/10000 train_loss:3.7582 train_time:1376658ms step_avg:170.27ms\nstep:8096/10000 train_loss:3.7880 train_time:1376827ms step_avg:170.27ms\nstep:8097/10000 train_loss:3.9058 train_time:1376998ms step_avg:170.27ms\nstep:8098/10000 train_loss:3.6791 train_time:1377169ms step_avg:170.27ms\nstep:8099/10000 train_loss:3.7722 train_time:1377339ms step_avg:170.27ms\nstep:8100/10000 train_loss:3.7531 train_time:1377510ms step_avg:170.27ms\nstep:8101/10000 train_loss:3.7792 train_time:1377682ms step_avg:170.27ms\nstep:8102/10000 train_loss:3.8725 train_time:1377852ms step_avg:170.27ms\nstep:8103/10000 train_loss:3.7961 train_time:1378025ms step_avg:170.27ms\nstep:8104/10000 train_loss:3.7129 train_time:1378197ms step_avg:170.27ms\nstep:8105/10000 train_loss:3.7435 train_time:1378369ms step_avg:170.27ms\nstep:8106/10000 train_loss:3.7781 train_time:1378537ms step_avg:170.27ms\nstep:8107/10000 train_loss:3.8410 train_time:1378707ms step_avg:170.27ms\nstep:8108/10000 train_loss:3.7872 train_time:1378879ms step_avg:170.27ms\nstep:8109/10000 train_loss:3.6769 train_time:1379049ms step_avg:170.27ms\nstep:8110/10000 train_loss:3.8677 train_time:1379221ms step_avg:170.27ms\nstep:8111/10000 train_loss:3.7899 train_time:1379389ms step_avg:170.27ms\nstep:8112/10000 train_loss:3.8577 train_time:1379563ms step_avg:170.27ms\nstep:8113/10000 train_loss:3.6364 train_time:1379732ms step_avg:170.27ms\nstep:8114/10000 train_loss:3.9281 train_time:1379903ms step_avg:170.27ms\nstep:8115/10000 train_loss:3.8058 train_time:1380074ms step_avg:170.27ms\nstep:8116/10000 train_loss:3.6757 train_time:1380246ms step_avg:170.27ms\nstep:8117/10000 train_loss:3.9067 train_time:1380416ms step_avg:170.27ms\nstep:8118/10000 train_loss:3.7088 train_time:1380588ms step_avg:170.27ms\nstep:8119/10000 train_loss:3.7155 train_time:1380758ms step_avg:170.27ms\nstep:8120/10000 train_loss:3.7360 train_time:1380927ms step_avg:170.27ms\nstep:8121/10000 train_loss:3.8854 train_time:1381099ms step_avg:170.27ms\nstep:8122/10000 train_loss:3.8031 train_time:1381267ms step_avg:170.27ms\nstep:8123/10000 train_loss:3.8468 train_time:1381437ms step_avg:170.27ms\nstep:8124/10000 train_loss:3.8230 train_time:1381605ms step_avg:170.27ms\nstep:8125/10000 train_loss:3.7936 train_time:1381777ms step_avg:170.27ms\nstep:8126/10000 train_loss:3.6427 train_time:1381947ms step_avg:170.27ms\nstep:8127/10000 train_loss:3.8053 train_time:1382118ms step_avg:170.27ms\nstep:8128/10000 train_loss:3.8621 train_time:1382288ms step_avg:170.27ms\nstep:8129/10000 train_loss:3.7556 train_time:1382461ms step_avg:170.27ms\nstep:8130/10000 train_loss:3.7661 train_time:1382629ms step_avg:170.27ms\nstep:8131/10000 train_loss:3.7369 train_time:1382801ms step_avg:170.27ms\nstep:8132/10000 train_loss:3.5417 train_time:1382971ms step_avg:170.27ms\nstep:8133/10000 train_loss:3.7655 train_time:1383145ms step_avg:170.28ms\nstep:8134/10000 train_loss:3.6890 train_time:1383314ms step_avg:170.28ms\nstep:8135/10000 train_loss:3.8351 train_time:1383485ms step_avg:170.28ms\nstep:8136/10000 train_loss:3.7798 train_time:1383658ms step_avg:170.28ms\nstep:8137/10000 train_loss:3.7843 train_time:1383829ms step_avg:170.28ms\nstep:8138/10000 train_loss:3.7650 train_time:1384001ms step_avg:170.28ms\nstep:8139/10000 train_loss:3.6329 train_time:1384172ms step_avg:170.28ms\nstep:8140/10000 train_loss:3.7177 train_time:1384342ms step_avg:170.28ms\nstep:8141/10000 train_loss:3.8494 train_time:1384511ms step_avg:170.28ms\nstep:8142/10000 train_loss:3.7744 train_time:1384683ms step_avg:170.28ms\nstep:8143/10000 train_loss:3.7874 train_time:1384853ms step_avg:170.28ms\nstep:8144/10000 train_loss:3.8155 train_time:1385028ms step_avg:170.28ms\nstep:8145/10000 train_loss:3.7587 train_time:1385200ms step_avg:170.28ms\nstep:8146/10000 train_loss:3.7837 train_time:1385369ms step_avg:170.28ms\nstep:8147/10000 train_loss:3.8285 train_time:1385540ms step_avg:170.28ms\nstep:8148/10000 train_loss:3.7572 train_time:1385708ms step_avg:170.28ms\nstep:8149/10000 train_loss:3.8038 train_time:1385878ms step_avg:170.28ms\nstep:8150/10000 train_loss:3.6845 train_time:1386047ms step_avg:170.28ms\nstep:8151/10000 train_loss:3.7724 train_time:1386219ms step_avg:170.28ms\nstep:8152/10000 train_loss:3.7935 train_time:1386387ms step_avg:170.28ms\nstep:8153/10000 train_loss:3.8448 train_time:1386557ms step_avg:170.28ms\nstep:8154/10000 train_loss:3.8355 train_time:1386726ms step_avg:170.28ms\nstep:8155/10000 train_loss:3.8455 train_time:1386898ms step_avg:170.28ms\nstep:8156/10000 train_loss:3.8102 train_time:1387066ms step_avg:170.28ms\nstep:8157/10000 train_loss:3.8166 train_time:1387237ms step_avg:170.28ms\nstep:8158/10000 train_loss:3.8252 train_time:1387406ms step_avg:170.28ms\nstep:8159/10000 train_loss:3.5083 train_time:1387577ms step_avg:170.28ms\nstep:8160/10000 train_loss:3.8369 train_time:1387746ms step_avg:170.28ms\nstep:8161/10000 train_loss:3.9569 train_time:1387915ms step_avg:170.28ms\nstep:8162/10000 train_loss:3.8275 train_time:1388088ms step_avg:170.28ms\nstep:8163/10000 train_loss:3.6864 train_time:1388261ms step_avg:170.28ms\nstep:8164/10000 train_loss:4.0552 train_time:1388430ms step_avg:170.28ms\nstep:8165/10000 train_loss:3.6865 train_time:1388600ms step_avg:170.28ms\nstep:8166/10000 train_loss:3.6771 train_time:1388770ms step_avg:170.28ms\nstep:8167/10000 train_loss:3.7087 train_time:1388940ms step_avg:170.28ms\nstep:8168/10000 train_loss:3.8061 train_time:1389110ms step_avg:170.28ms\nstep:8169/10000 train_loss:4.0299 train_time:1389282ms step_avg:170.28ms\nstep:8170/10000 train_loss:3.9444 train_time:1389452ms step_avg:170.28ms\nstep:8171/10000 train_loss:3.7208 train_time:1389623ms step_avg:170.28ms\nstep:8172/10000 train_loss:3.7488 train_time:1389794ms step_avg:170.28ms\nstep:8173/10000 train_loss:3.7175 train_time:1389966ms step_avg:170.28ms\nstep:8174/10000 train_loss:3.7789 train_time:1390137ms step_avg:170.28ms\nstep:8175/10000 train_loss:3.7483 train_time:1390305ms step_avg:170.28ms\nstep:8176/10000 train_loss:3.7006 train_time:1390474ms step_avg:170.28ms\nstep:8177/10000 train_loss:3.7741 train_time:1390643ms step_avg:170.28ms\nstep:8178/10000 train_loss:3.8067 train_time:1390810ms step_avg:170.28ms\nstep:8179/10000 train_loss:3.7938 train_time:1390978ms step_avg:170.28ms\nstep:8180/10000 train_loss:3.7717 train_time:1391149ms step_avg:170.28ms\nstep:8181/10000 train_loss:3.8328 train_time:1391320ms step_avg:170.28ms\nstep:8182/10000 train_loss:3.7508 train_time:1391488ms step_avg:170.28ms\nstep:8183/10000 train_loss:3.8966 train_time:1391661ms step_avg:170.28ms\nstep:8184/10000 train_loss:3.6997 train_time:1391832ms step_avg:170.28ms\nstep:8185/10000 train_loss:3.5520 train_time:1392000ms step_avg:170.28ms\nstep:8186/10000 train_loss:3.6400 train_time:1392172ms step_avg:170.28ms\nstep:8187/10000 train_loss:3.8247 train_time:1392345ms step_avg:170.28ms\nstep:8188/10000 train_loss:3.6805 train_time:1392514ms step_avg:170.28ms\nstep:8189/10000 train_loss:3.9414 train_time:1392682ms step_avg:170.28ms\nstep:8190/10000 train_loss:3.7908 train_time:1392852ms step_avg:170.28ms\nstep:8191/10000 train_loss:3.7850 train_time:1393022ms step_avg:170.28ms\nstep:8192/10000 train_loss:3.6995 train_time:1393193ms step_avg:170.28ms\nstep:8193/10000 train_loss:3.7832 train_time:1393363ms step_avg:170.28ms\nstep:8194/10000 train_loss:3.8074 train_time:1393532ms step_avg:170.28ms\nstep:8195/10000 train_loss:3.8147 train_time:1393702ms step_avg:170.28ms\nstep:8196/10000 train_loss:3.7371 train_time:1393875ms step_avg:170.28ms\nstep:8197/10000 train_loss:3.8435 train_time:1394046ms step_avg:170.28ms\nstep:8198/10000 train_loss:3.6511 train_time:1394216ms step_avg:170.28ms\nstep:8199/10000 train_loss:3.7480 train_time:1394386ms step_avg:170.28ms\nstep:8200/10000 train_loss:3.8106 train_time:1394557ms step_avg:170.28ms\nstep:8201/10000 train_loss:3.7100 train_time:1394725ms step_avg:170.28ms\nstep:8202/10000 train_loss:3.7396 train_time:1394897ms step_avg:170.28ms\nstep:8203/10000 train_loss:3.9377 train_time:1395068ms step_avg:170.28ms\nstep:8204/10000 train_loss:3.7636 train_time:1395239ms step_avg:170.28ms\nstep:8205/10000 train_loss:3.8228 train_time:1395412ms step_avg:170.28ms\nstep:8206/10000 train_loss:3.7805 train_time:1395578ms step_avg:170.28ms\nstep:8207/10000 train_loss:3.8141 train_time:1395748ms step_avg:170.28ms\nstep:8208/10000 train_loss:3.8069 train_time:1395918ms step_avg:170.28ms\nstep:8209/10000 train_loss:3.7564 train_time:1396087ms step_avg:170.28ms\nstep:8210/10000 train_loss:3.7384 train_time:1396258ms step_avg:170.28ms\nstep:8211/10000 train_loss:3.7687 train_time:1396429ms step_avg:170.28ms\nstep:8212/10000 train_loss:3.7060 train_time:1396600ms step_avg:170.28ms\nstep:8213/10000 train_loss:3.7393 train_time:1396770ms step_avg:170.28ms\nstep:8214/10000 train_loss:3.7639 train_time:1396940ms step_avg:170.28ms\nstep:8215/10000 train_loss:3.7155 train_time:1397110ms step_avg:170.28ms\nstep:8216/10000 train_loss:3.8904 train_time:1397282ms step_avg:170.28ms\nstep:8217/10000 train_loss:3.8157 train_time:1397454ms step_avg:170.28ms\nstep:8218/10000 train_loss:3.9269 train_time:1397625ms step_avg:170.28ms\nstep:8219/10000 train_loss:3.7623 train_time:1397797ms step_avg:170.28ms\nstep:8220/10000 train_loss:3.7685 train_time:1397966ms step_avg:170.28ms\nstep:8221/10000 train_loss:3.8061 train_time:1398136ms step_avg:170.28ms\nstep:8222/10000 train_loss:3.7986 train_time:1398305ms step_avg:170.28ms\nstep:8223/10000 train_loss:3.7402 train_time:1398476ms step_avg:170.28ms\nstep:8224/10000 train_loss:3.8621 train_time:1398647ms step_avg:170.28ms\nstep:8225/10000 train_loss:3.7721 train_time:1398816ms step_avg:170.28ms\nstep:8226/10000 train_loss:3.7487 train_time:1398986ms step_avg:170.28ms\nstep:8227/10000 train_loss:3.8494 train_time:1399159ms step_avg:170.28ms\nstep:8228/10000 train_loss:3.8055 train_time:1399327ms step_avg:170.28ms\nstep:8229/10000 train_loss:3.8163 train_time:1399498ms step_avg:170.28ms\nstep:8230/10000 train_loss:3.7374 train_time:1399668ms step_avg:170.28ms\nstep:8231/10000 train_loss:3.9692 train_time:1399839ms step_avg:170.28ms\nstep:8232/10000 train_loss:3.7868 train_time:1400006ms step_avg:170.28ms\nstep:8233/10000 train_loss:3.8100 train_time:1400180ms step_avg:170.28ms\nstep:8234/10000 train_loss:3.6036 train_time:1400346ms step_avg:170.28ms\nstep:8235/10000 train_loss:3.9048 train_time:1400515ms step_avg:170.28ms\nstep:8236/10000 train_loss:3.8482 train_time:1400683ms step_avg:170.28ms\nstep:8237/10000 train_loss:3.7372 train_time:1400853ms step_avg:170.28ms\nstep:8238/10000 train_loss:3.7050 train_time:1401023ms step_avg:170.28ms\nstep:8239/10000 train_loss:3.8127 train_time:1401193ms step_avg:170.28ms\nstep:8240/10000 train_loss:3.7786 train_time:1401366ms step_avg:170.28ms\nstep:8241/10000 train_loss:3.7526 train_time:1401537ms step_avg:170.28ms\nstep:8242/10000 train_loss:3.8937 train_time:1401708ms step_avg:170.28ms\nstep:8243/10000 train_loss:3.7818 train_time:1401878ms step_avg:170.28ms\nstep:8244/10000 train_loss:3.7008 train_time:1402047ms step_avg:170.28ms\nstep:8245/10000 train_loss:3.6513 train_time:1402216ms step_avg:170.28ms\nstep:8246/10000 train_loss:3.8907 train_time:1402385ms step_avg:170.28ms\nstep:8247/10000 train_loss:3.4882 train_time:1402553ms step_avg:170.27ms\nstep:8248/10000 train_loss:3.6722 train_time:1402721ms step_avg:170.27ms\nstep:8249/10000 train_loss:3.8440 train_time:1402893ms step_avg:170.27ms\nstep:8250/10000 train_loss:3.7699 train_time:1403066ms step_avg:170.27ms\nstep:8251/10000 train_loss:4.0061 train_time:1403238ms step_avg:170.28ms\nstep:8252/10000 train_loss:3.8544 train_time:1403408ms step_avg:170.28ms\nstep:8253/10000 train_loss:3.6700 train_time:1403579ms step_avg:170.28ms\nstep:8254/10000 train_loss:3.8148 train_time:1403749ms step_avg:170.28ms\nstep:8255/10000 train_loss:3.7635 train_time:1403920ms step_avg:170.28ms\nstep:8256/10000 train_loss:3.9474 train_time:1404088ms step_avg:170.28ms\nstep:8257/10000 train_loss:3.7943 train_time:1404260ms step_avg:170.28ms\nstep:8258/10000 train_loss:3.7989 train_time:1404430ms step_avg:170.28ms\nstep:8259/10000 train_loss:3.7059 train_time:1404601ms step_avg:170.28ms\nstep:8260/10000 train_loss:3.8952 train_time:1404770ms step_avg:170.28ms\nstep:8261/10000 train_loss:3.5459 train_time:1404941ms step_avg:170.28ms\nstep:8262/10000 train_loss:3.7585 train_time:1405109ms step_avg:170.27ms\nstep:8263/10000 train_loss:3.6179 train_time:1405279ms step_avg:170.27ms\nstep:8264/10000 train_loss:3.8220 train_time:1405448ms step_avg:170.27ms\nstep:8265/10000 train_loss:3.6384 train_time:1405621ms step_avg:170.28ms\nstep:8266/10000 train_loss:3.6812 train_time:1405791ms step_avg:170.28ms\nstep:8267/10000 train_loss:3.7528 train_time:1405964ms step_avg:170.28ms\nstep:8268/10000 train_loss:3.9352 train_time:1406132ms step_avg:170.28ms\nstep:8269/10000 train_loss:3.7888 train_time:1406301ms step_avg:170.27ms\nstep:8270/10000 train_loss:3.9750 train_time:1406469ms step_avg:170.27ms\nstep:8271/10000 train_loss:3.8490 train_time:1406641ms step_avg:170.27ms\nstep:8272/10000 train_loss:3.6918 train_time:1406810ms step_avg:170.27ms\nstep:8273/10000 train_loss:3.8302 train_time:1406980ms step_avg:170.27ms\nstep:8274/10000 train_loss:3.8428 train_time:1407150ms step_avg:170.27ms\nstep:8275/10000 train_loss:3.7210 train_time:1407322ms step_avg:170.27ms\nstep:8276/10000 train_loss:3.8111 train_time:1407490ms step_avg:170.27ms\nstep:8277/10000 train_loss:3.7353 train_time:1407661ms step_avg:170.27ms\nstep:8278/10000 train_loss:3.7789 train_time:1407828ms step_avg:170.27ms\nstep:8279/10000 train_loss:3.7617 train_time:1407998ms step_avg:170.27ms\nstep:8280/10000 train_loss:3.6152 train_time:1408168ms step_avg:170.27ms\nstep:8281/10000 train_loss:3.8349 train_time:1408338ms step_avg:170.27ms\nstep:8282/10000 train_loss:3.8153 train_time:1408508ms step_avg:170.27ms\nstep:8283/10000 train_loss:3.8580 train_time:1408677ms step_avg:170.27ms\nstep:8284/10000 train_loss:3.8082 train_time:1408848ms step_avg:170.27ms\nstep:8285/10000 train_loss:3.7377 train_time:1409017ms step_avg:170.27ms\nstep:8286/10000 train_loss:3.7396 train_time:1409187ms step_avg:170.27ms\nstep:8287/10000 train_loss:3.7764 train_time:1409357ms step_avg:170.27ms\nstep:8288/10000 train_loss:3.8731 train_time:1409528ms step_avg:170.27ms\nstep:8289/10000 train_loss:3.7489 train_time:1409698ms step_avg:170.27ms\nstep:8290/10000 train_loss:3.7903 train_time:1409865ms step_avg:170.27ms\nstep:8291/10000 train_loss:3.5393 train_time:1410036ms step_avg:170.27ms\nstep:8292/10000 train_loss:3.9764 train_time:1410206ms step_avg:170.27ms\nstep:8293/10000 train_loss:3.8080 train_time:1410378ms step_avg:170.27ms\nstep:8294/10000 train_loss:3.9067 train_time:1410551ms step_avg:170.27ms\nstep:8295/10000 train_loss:3.8022 train_time:1410724ms step_avg:170.27ms\nstep:8296/10000 train_loss:3.9435 train_time:1410893ms step_avg:170.27ms\nstep:8297/10000 train_loss:3.8150 train_time:1411064ms step_avg:170.27ms\nstep:8298/10000 train_loss:3.9169 train_time:1411235ms step_avg:170.27ms\nstep:8299/10000 train_loss:3.7014 train_time:1411406ms step_avg:170.27ms\nstep:8300/10000 train_loss:3.7948 train_time:1411576ms step_avg:170.27ms\nstep:8301/10000 train_loss:3.7697 train_time:1411749ms step_avg:170.27ms\nstep:8302/10000 train_loss:3.7746 train_time:1411919ms step_avg:170.27ms\nstep:8303/10000 train_loss:4.0874 train_time:1412087ms step_avg:170.27ms\nstep:8304/10000 train_loss:3.9251 train_time:1412258ms step_avg:170.27ms\nstep:8305/10000 train_loss:3.7460 train_time:1412429ms step_avg:170.27ms\nstep:8306/10000 train_loss:3.8052 train_time:1412598ms step_avg:170.27ms\nstep:8307/10000 train_loss:3.7725 train_time:1412765ms step_avg:170.27ms\nstep:8308/10000 train_loss:3.9398 train_time:1412939ms step_avg:170.27ms\nstep:8309/10000 train_loss:3.7399 train_time:1413103ms step_avg:170.27ms\nstep:8310/10000 train_loss:3.7045 train_time:1413275ms step_avg:170.27ms\nstep:8311/10000 train_loss:3.7204 train_time:1413446ms step_avg:170.27ms\nstep:8312/10000 train_loss:4.6089 train_time:1413617ms step_avg:170.27ms\nstep:8313/10000 train_loss:3.7477 train_time:1413788ms step_avg:170.27ms\nstep:8314/10000 train_loss:4.2144 train_time:1413963ms step_avg:170.27ms\nstep:8315/10000 train_loss:3.7395 train_time:1414131ms step_avg:170.27ms\nstep:8316/10000 train_loss:3.7715 train_time:1414300ms step_avg:170.27ms\nstep:8317/10000 train_loss:3.6732 train_time:1414470ms step_avg:170.27ms\nstep:8318/10000 train_loss:3.6419 train_time:1414643ms step_avg:170.27ms\nstep:8319/10000 train_loss:3.7878 train_time:1414812ms step_avg:170.27ms\nstep:8320/10000 train_loss:3.8037 train_time:1414986ms step_avg:170.28ms\nstep:8321/10000 train_loss:3.8234 train_time:1415152ms step_avg:170.27ms\nstep:8322/10000 train_loss:3.8180 train_time:1415321ms step_avg:170.27ms\nstep:8323/10000 train_loss:3.8582 train_time:1415491ms step_avg:170.27ms\nstep:8324/10000 train_loss:3.7771 train_time:1415663ms step_avg:170.27ms\nstep:8325/10000 train_loss:3.8404 train_time:1415834ms step_avg:170.27ms\nstep:8326/10000 train_loss:3.7199 train_time:1416002ms step_avg:170.27ms\nstep:8327/10000 train_loss:3.7817 train_time:1416172ms step_avg:170.27ms\nstep:8328/10000 train_loss:3.7831 train_time:1416346ms step_avg:170.27ms\nstep:8329/10000 train_loss:3.6750 train_time:1416519ms step_avg:170.28ms\nstep:8330/10000 train_loss:3.6806 train_time:1416690ms step_avg:170.28ms\nstep:8331/10000 train_loss:3.7131 train_time:1416859ms step_avg:170.28ms\nstep:8332/10000 train_loss:3.7131 train_time:1417029ms step_avg:170.28ms\nstep:8333/10000 train_loss:3.7103 train_time:1417200ms step_avg:170.28ms\nstep:8334/10000 train_loss:3.7695 train_time:1417371ms step_avg:170.28ms\nstep:8335/10000 train_loss:3.7968 train_time:1417544ms step_avg:170.28ms\nstep:8336/10000 train_loss:3.8045 train_time:1417715ms step_avg:170.28ms\nstep:8337/10000 train_loss:3.6833 train_time:1417886ms step_avg:170.28ms\nstep:8338/10000 train_loss:4.0800 train_time:1418058ms step_avg:170.28ms\nstep:8339/10000 train_loss:3.8059 train_time:1418227ms step_avg:170.28ms\nstep:8340/10000 train_loss:3.7108 train_time:1418398ms step_avg:170.28ms\nstep:8341/10000 train_loss:3.8703 train_time:1418567ms step_avg:170.28ms\nstep:8342/10000 train_loss:3.6633 train_time:1418738ms step_avg:170.28ms\nstep:8343/10000 train_loss:3.8529 train_time:1418908ms step_avg:170.28ms\nstep:8344/10000 train_loss:3.8120 train_time:1419080ms step_avg:170.28ms\nstep:8345/10000 train_loss:3.9195 train_time:1419249ms step_avg:170.28ms\nstep:8346/10000 train_loss:3.9867 train_time:1419421ms step_avg:170.28ms\nstep:8347/10000 train_loss:3.8093 train_time:1419590ms step_avg:170.28ms\nstep:8348/10000 train_loss:3.7695 train_time:1419760ms step_avg:170.28ms\nstep:8349/10000 train_loss:3.7286 train_time:1419930ms step_avg:170.28ms\nstep:8350/10000 train_loss:3.7077 train_time:1420100ms step_avg:170.28ms\nstep:8351/10000 train_loss:3.7606 train_time:1420268ms step_avg:170.28ms\nstep:8352/10000 train_loss:3.7227 train_time:1420438ms step_avg:170.28ms\nstep:8353/10000 train_loss:3.7728 train_time:1420607ms step_avg:170.28ms\nstep:8354/10000 train_loss:3.6721 train_time:1420778ms step_avg:170.28ms\nstep:8355/10000 train_loss:3.7652 train_time:1420947ms step_avg:170.28ms\nstep:8356/10000 train_loss:3.7710 train_time:1421120ms step_avg:170.28ms\nstep:8357/10000 train_loss:3.7681 train_time:1421289ms step_avg:170.28ms\nstep:8358/10000 train_loss:3.9093 train_time:1421460ms step_avg:170.28ms\nstep:8359/10000 train_loss:4.0699 train_time:1421629ms step_avg:170.28ms\nstep:8360/10000 train_loss:3.7346 train_time:1421802ms step_avg:170.28ms\nstep:8361/10000 train_loss:3.8029 train_time:1421973ms step_avg:170.28ms\nstep:8362/10000 train_loss:3.8749 train_time:1422145ms step_avg:170.28ms\nstep:8363/10000 train_loss:3.7229 train_time:1422315ms step_avg:170.28ms\nstep:8364/10000 train_loss:3.7078 train_time:1422486ms step_avg:170.28ms\nstep:8365/10000 train_loss:3.7969 train_time:1422658ms step_avg:170.28ms\nstep:8366/10000 train_loss:3.8339 train_time:1422827ms step_avg:170.28ms\nstep:8367/10000 train_loss:3.6087 train_time:1422997ms step_avg:170.28ms\nstep:8368/10000 train_loss:3.6832 train_time:1423167ms step_avg:170.28ms\nstep:8369/10000 train_loss:3.7865 train_time:1423337ms step_avg:170.28ms\nstep:8370/10000 train_loss:3.8918 train_time:1423505ms step_avg:170.28ms\nstep:8371/10000 train_loss:3.8720 train_time:1423676ms step_avg:170.28ms\nstep:8372/10000 train_loss:3.7153 train_time:1423847ms step_avg:170.28ms\nstep:8373/10000 train_loss:3.6991 train_time:1424021ms step_avg:170.28ms\nstep:8374/10000 train_loss:3.8139 train_time:1424189ms step_avg:170.28ms\nstep:8375/10000 train_loss:3.5791 train_time:1424358ms step_avg:170.28ms\nstep:8376/10000 train_loss:3.7016 train_time:1424528ms step_avg:170.28ms\nstep:8377/10000 train_loss:3.8190 train_time:1424698ms step_avg:170.28ms\nstep:8378/10000 train_loss:3.8023 train_time:1424869ms step_avg:170.28ms\nstep:8379/10000 train_loss:3.7681 train_time:1425042ms step_avg:170.28ms\nstep:8380/10000 train_loss:3.7862 train_time:1425212ms step_avg:170.28ms\nstep:8381/10000 train_loss:3.6133 train_time:1425384ms step_avg:170.28ms\nstep:8382/10000 train_loss:3.7913 train_time:1425780ms step_avg:170.30ms\nstep:8383/10000 train_loss:3.7538 train_time:1425940ms step_avg:170.30ms\nstep:8384/10000 train_loss:3.7604 train_time:1426108ms step_avg:170.30ms\nstep:8385/10000 train_loss:3.7240 train_time:1426277ms step_avg:170.30ms\nstep:8386/10000 train_loss:3.6997 train_time:1426445ms step_avg:170.30ms\nstep:8387/10000 train_loss:3.7278 train_time:1426614ms step_avg:170.30ms\nstep:8388/10000 train_loss:3.8347 train_time:1426794ms step_avg:170.30ms\nstep:8389/10000 train_loss:3.7211 train_time:1426970ms step_avg:170.30ms\nstep:8390/10000 train_loss:3.8047 train_time:1427139ms step_avg:170.30ms\nstep:8391/10000 train_loss:3.7465 train_time:1427305ms step_avg:170.30ms\nstep:8392/10000 train_loss:3.6707 train_time:1427474ms step_avg:170.30ms\nstep:8393/10000 train_loss:3.6674 train_time:1427644ms step_avg:170.30ms\nstep:8394/10000 train_loss:3.8469 train_time:1427814ms step_avg:170.30ms\nstep:8395/10000 train_loss:3.7513 train_time:1427988ms step_avg:170.30ms\nstep:8396/10000 train_loss:3.5127 train_time:1428160ms step_avg:170.30ms\nstep:8397/10000 train_loss:4.0907 train_time:1428330ms step_avg:170.30ms\nstep:8398/10000 train_loss:3.6896 train_time:1428498ms step_avg:170.30ms\nstep:8399/10000 train_loss:3.9992 train_time:1428667ms step_avg:170.30ms\nstep:8400/10000 train_loss:3.7482 train_time:1428838ms step_avg:170.30ms\nstep:8401/10000 train_loss:3.8793 train_time:1429008ms step_avg:170.30ms\nstep:8402/10000 train_loss:3.5970 train_time:1429178ms step_avg:170.30ms\nstep:8403/10000 train_loss:3.8567 train_time:1429348ms step_avg:170.30ms\nstep:8404/10000 train_loss:3.9383 train_time:1429518ms step_avg:170.30ms\nstep:8405/10000 train_loss:3.6660 train_time:1429687ms step_avg:170.30ms\nstep:8406/10000 train_loss:3.8210 train_time:1429861ms step_avg:170.30ms\nstep:8407/10000 train_loss:3.8246 train_time:1430032ms step_avg:170.30ms\nstep:8408/10000 train_loss:3.8026 train_time:1430205ms step_avg:170.30ms\nstep:8409/10000 train_loss:3.8305 train_time:1430374ms step_avg:170.30ms\nstep:8410/10000 train_loss:3.7717 train_time:1430543ms step_avg:170.30ms\nstep:8411/10000 train_loss:3.7388 train_time:1430715ms step_avg:170.30ms\nstep:8412/10000 train_loss:3.6389 train_time:1430887ms step_avg:170.30ms\nstep:8413/10000 train_loss:3.7554 train_time:1431059ms step_avg:170.30ms\nstep:8414/10000 train_loss:3.8021 train_time:1431230ms step_avg:170.30ms\nstep:8415/10000 train_loss:3.7004 train_time:1431402ms step_avg:170.30ms\nstep:8416/10000 train_loss:3.7470 train_time:1431570ms step_avg:170.30ms\nstep:8417/10000 train_loss:3.9019 train_time:1431742ms step_avg:170.30ms\nstep:8418/10000 train_loss:3.6987 train_time:1431912ms step_avg:170.30ms\nstep:8419/10000 train_loss:3.7900 train_time:1432083ms step_avg:170.30ms\nstep:8420/10000 train_loss:3.7278 train_time:1432250ms step_avg:170.30ms\nstep:8421/10000 train_loss:3.7730 train_time:1432422ms step_avg:170.30ms\nstep:8422/10000 train_loss:3.7876 train_time:1432591ms step_avg:170.30ms\nstep:8423/10000 train_loss:3.9571 train_time:1432763ms step_avg:170.30ms\nstep:8424/10000 train_loss:3.8414 train_time:1432934ms step_avg:170.30ms\nstep:8425/10000 train_loss:3.7364 train_time:1433107ms step_avg:170.30ms\nstep:8426/10000 train_loss:3.8179 train_time:1433275ms step_avg:170.30ms\nstep:8427/10000 train_loss:3.8450 train_time:1433444ms step_avg:170.30ms\nstep:8428/10000 train_loss:3.6890 train_time:1433615ms step_avg:170.30ms\nstep:8429/10000 train_loss:3.7882 train_time:1433784ms step_avg:170.30ms\nstep:8430/10000 train_loss:3.9189 train_time:1433956ms step_avg:170.30ms\nstep:8431/10000 train_loss:3.9016 train_time:1434126ms step_avg:170.30ms\nstep:8432/10000 train_loss:3.6378 train_time:1434297ms step_avg:170.30ms\nstep:8433/10000 train_loss:3.6138 train_time:1434466ms step_avg:170.30ms\nstep:8434/10000 train_loss:3.8590 train_time:1434637ms step_avg:170.30ms\nstep:8435/10000 train_loss:3.7438 train_time:1434809ms step_avg:170.30ms\nstep:8436/10000 train_loss:3.9058 train_time:1434979ms step_avg:170.30ms\nstep:8437/10000 train_loss:3.7315 train_time:1435150ms step_avg:170.30ms\nstep:8438/10000 train_loss:3.7814 train_time:1435322ms step_avg:170.30ms\nstep:8439/10000 train_loss:3.6499 train_time:1435491ms step_avg:170.30ms\nstep:8440/10000 train_loss:3.6707 train_time:1435663ms step_avg:170.30ms\nstep:8441/10000 train_loss:3.7964 train_time:1435831ms step_avg:170.30ms\nstep:8442/10000 train_loss:3.9413 train_time:1436002ms step_avg:170.30ms\nstep:8443/10000 train_loss:3.8056 train_time:1436171ms step_avg:170.30ms\nstep:8444/10000 train_loss:3.9985 train_time:1436345ms step_avg:170.30ms\nstep:8445/10000 train_loss:3.8504 train_time:1436514ms step_avg:170.30ms\nstep:8446/10000 train_loss:3.7517 train_time:1436684ms step_avg:170.30ms\nstep:8447/10000 train_loss:3.6590 train_time:1436855ms step_avg:170.30ms\nstep:8448/10000 train_loss:3.8714 train_time:1437027ms step_avg:170.30ms\nstep:8449/10000 train_loss:3.8267 train_time:1437206ms step_avg:170.31ms\nstep:8450/10000 train_loss:3.9480 train_time:1437370ms step_avg:170.30ms\nstep:8451/10000 train_loss:3.6890 train_time:1437543ms step_avg:170.30ms\nstep:8452/10000 train_loss:3.7982 train_time:1437713ms step_avg:170.30ms\nstep:8453/10000 train_loss:3.7590 train_time:1437884ms step_avg:170.30ms\nstep:8454/10000 train_loss:4.0194 train_time:1438052ms step_avg:170.30ms\nstep:8455/10000 train_loss:3.6894 train_time:1438222ms step_avg:170.30ms\nstep:8456/10000 train_loss:3.8036 train_time:1438392ms step_avg:170.30ms\nstep:8457/10000 train_loss:3.7755 train_time:1438560ms step_avg:170.30ms\nstep:8458/10000 train_loss:3.6479 train_time:1438732ms step_avg:170.30ms\nstep:8459/10000 train_loss:4.0660 train_time:1438904ms step_avg:170.30ms\nstep:8460/10000 train_loss:3.6354 train_time:1439073ms step_avg:170.30ms\nstep:8461/10000 train_loss:3.8036 train_time:1439247ms step_avg:170.30ms\nstep:8462/10000 train_loss:3.7336 train_time:1439417ms step_avg:170.30ms\nstep:8463/10000 train_loss:3.6444 train_time:1439585ms step_avg:170.30ms\nstep:8464/10000 train_loss:4.0797 train_time:1439757ms step_avg:170.30ms\nstep:8465/10000 train_loss:3.7002 train_time:1439927ms step_avg:170.30ms\nstep:8466/10000 train_loss:3.6111 train_time:1440099ms step_avg:170.30ms\nstep:8467/10000 train_loss:3.8160 train_time:1440268ms step_avg:170.30ms\nstep:8468/10000 train_loss:3.8194 train_time:1440438ms step_avg:170.30ms\nstep:8469/10000 train_loss:3.8066 train_time:1440606ms step_avg:170.30ms\nstep:8470/10000 train_loss:3.7660 train_time:1440775ms step_avg:170.30ms\nstep:8471/10000 train_loss:3.7250 train_time:1440944ms step_avg:170.30ms\nstep:8472/10000 train_loss:3.7102 train_time:1441113ms step_avg:170.30ms\nstep:8473/10000 train_loss:3.6622 train_time:1441284ms step_avg:170.30ms\nstep:8474/10000 train_loss:3.8330 train_time:1441456ms step_avg:170.30ms\nstep:8475/10000 train_loss:3.6761 train_time:1441629ms step_avg:170.30ms\nstep:8476/10000 train_loss:3.8426 train_time:1441802ms step_avg:170.30ms\nstep:8477/10000 train_loss:3.8517 train_time:1441973ms step_avg:170.31ms\nstep:8478/10000 train_loss:3.7419 train_time:1442142ms step_avg:170.30ms\nstep:8479/10000 train_loss:3.7671 train_time:1442311ms step_avg:170.30ms\nstep:8480/10000 train_loss:3.8058 train_time:1442481ms step_avg:170.30ms\nstep:8481/10000 train_loss:3.7110 train_time:1442653ms step_avg:170.30ms\nstep:8482/10000 train_loss:3.9151 train_time:1442825ms step_avg:170.31ms\nstep:8483/10000 train_loss:3.6834 train_time:1442994ms step_avg:170.31ms\nstep:8484/10000 train_loss:3.7588 train_time:1443168ms step_avg:170.31ms\nstep:8485/10000 train_loss:3.7431 train_time:1443336ms step_avg:170.31ms\nstep:8486/10000 train_loss:3.8768 train_time:1443509ms step_avg:170.31ms\nstep:8487/10000 train_loss:4.0594 train_time:1443680ms step_avg:170.31ms\nstep:8488/10000 train_loss:3.8279 train_time:1443847ms step_avg:170.31ms\nstep:8489/10000 train_loss:3.6812 train_time:1444018ms step_avg:170.31ms\nstep:8490/10000 train_loss:3.8826 train_time:1444188ms step_avg:170.31ms\nstep:8491/10000 train_loss:3.6954 train_time:1444360ms step_avg:170.31ms\nstep:8492/10000 train_loss:3.6905 train_time:1444528ms step_avg:170.31ms\nstep:8493/10000 train_loss:4.3956 train_time:1444698ms step_avg:170.31ms\nstep:8494/10000 train_loss:3.8177 train_time:1444867ms step_avg:170.30ms\nstep:8495/10000 train_loss:3.7294 train_time:1445036ms step_avg:170.30ms\nstep:8496/10000 train_loss:3.7348 train_time:1445206ms step_avg:170.30ms\nstep:8497/10000 train_loss:3.6582 train_time:1445377ms step_avg:170.30ms\nstep:8498/10000 train_loss:3.6853 train_time:1445546ms step_avg:170.30ms\nstep:8499/10000 train_loss:3.8171 train_time:1445717ms step_avg:170.30ms\nstep:8500/10000 train_loss:3.4678 train_time:1445885ms step_avg:170.30ms\nstep:8501/10000 train_loss:3.8213 train_time:1446054ms step_avg:170.30ms\nstep:8502/10000 train_loss:3.6992 train_time:1446222ms step_avg:170.30ms\nstep:8503/10000 train_loss:3.7003 train_time:1446392ms step_avg:170.30ms\nstep:8504/10000 train_loss:3.6602 train_time:1446563ms step_avg:170.30ms\nstep:8505/10000 train_loss:3.7935 train_time:1446732ms step_avg:170.30ms\nstep:8506/10000 train_loss:3.8194 train_time:1446902ms step_avg:170.30ms\nstep:8507/10000 train_loss:3.8235 train_time:1447072ms step_avg:170.30ms\nstep:8508/10000 train_loss:3.7969 train_time:1447243ms step_avg:170.30ms\nstep:8509/10000 train_loss:3.7225 train_time:1447412ms step_avg:170.30ms\nstep:8510/10000 train_loss:3.7705 train_time:1447584ms step_avg:170.30ms\nstep:8511/10000 train_loss:3.7750 train_time:1447755ms step_avg:170.30ms\nstep:8512/10000 train_loss:3.7671 train_time:1447926ms step_avg:170.30ms\nstep:8513/10000 train_loss:3.7582 train_time:1448099ms step_avg:170.30ms\nstep:8514/10000 train_loss:3.7888 train_time:1448268ms step_avg:170.30ms\nstep:8515/10000 train_loss:3.6763 train_time:1448438ms step_avg:170.30ms\nstep:8516/10000 train_loss:3.8419 train_time:1448608ms step_avg:170.30ms\nstep:8517/10000 train_loss:3.6272 train_time:1448779ms step_avg:170.30ms\nstep:8518/10000 train_loss:3.7689 train_time:1448949ms step_avg:170.30ms\nstep:8519/10000 train_loss:3.8723 train_time:1449119ms step_avg:170.30ms\nstep:8520/10000 train_loss:3.7358 train_time:1449288ms step_avg:170.30ms\nstep:8521/10000 train_loss:3.6462 train_time:1449459ms step_avg:170.30ms\nstep:8522/10000 train_loss:3.7803 train_time:1449628ms step_avg:170.30ms\nstep:8523/10000 train_loss:3.6686 train_time:1449799ms step_avg:170.30ms\nstep:8524/10000 train_loss:3.8839 train_time:1449968ms step_avg:170.30ms\nstep:8525/10000 train_loss:3.7662 train_time:1450139ms step_avg:170.30ms\nstep:8526/10000 train_loss:3.8208 train_time:1450309ms step_avg:170.30ms\nstep:8527/10000 train_loss:4.0138 train_time:1450480ms step_avg:170.30ms\nstep:8528/10000 train_loss:3.8414 train_time:1450649ms step_avg:170.30ms\nstep:8529/10000 train_loss:3.8636 train_time:1450818ms step_avg:170.30ms\nstep:8530/10000 train_loss:3.6322 train_time:1450988ms step_avg:170.30ms\nstep:8531/10000 train_loss:3.9181 train_time:1451157ms step_avg:170.30ms\nstep:8532/10000 train_loss:3.7356 train_time:1451326ms step_avg:170.30ms\nstep:8533/10000 train_loss:3.6885 train_time:1451499ms step_avg:170.30ms\nstep:8534/10000 train_loss:3.6985 train_time:1451667ms step_avg:170.30ms\nstep:8535/10000 train_loss:3.7533 train_time:1451839ms step_avg:170.30ms\nstep:8536/10000 train_loss:3.6773 train_time:1452009ms step_avg:170.30ms\nstep:8537/10000 train_loss:3.7873 train_time:1452178ms step_avg:170.30ms\nstep:8538/10000 train_loss:3.8294 train_time:1452347ms step_avg:170.30ms\nstep:8539/10000 train_loss:3.8404 train_time:1452517ms step_avg:170.30ms\nstep:8540/10000 train_loss:3.7158 train_time:1452687ms step_avg:170.30ms\nstep:8541/10000 train_loss:3.8388 train_time:1452859ms step_avg:170.30ms\nstep:8542/10000 train_loss:3.6868 train_time:1453030ms step_avg:170.30ms\nstep:8543/10000 train_loss:3.7332 train_time:1453198ms step_avg:170.30ms\nstep:8544/10000 train_loss:3.8617 train_time:1453365ms step_avg:170.30ms\nstep:8545/10000 train_loss:3.9911 train_time:1453537ms step_avg:170.30ms\nstep:8546/10000 train_loss:3.6394 train_time:1453708ms step_avg:170.30ms\nstep:8547/10000 train_loss:3.7920 train_time:1453881ms step_avg:170.30ms\nstep:8548/10000 train_loss:3.7446 train_time:1454052ms step_avg:170.30ms\nstep:8549/10000 train_loss:3.7230 train_time:1454221ms step_avg:170.30ms\nstep:8550/10000 train_loss:3.7094 train_time:1454391ms step_avg:170.30ms\nstep:8551/10000 train_loss:3.8564 train_time:1454562ms step_avg:170.30ms\nstep:8552/10000 train_loss:3.6725 train_time:1454731ms step_avg:170.30ms\nstep:8553/10000 train_loss:3.8951 train_time:1454899ms step_avg:170.30ms\nstep:8554/10000 train_loss:3.7337 train_time:1455070ms step_avg:170.30ms\nstep:8555/10000 train_loss:3.6631 train_time:1455244ms step_avg:170.30ms\nstep:8556/10000 train_loss:3.7713 train_time:1455415ms step_avg:170.30ms\nstep:8557/10000 train_loss:3.7445 train_time:1455585ms step_avg:170.30ms\nstep:8558/10000 train_loss:3.8454 train_time:1455755ms step_avg:170.30ms\nstep:8559/10000 train_loss:3.7132 train_time:1455923ms step_avg:170.30ms\nstep:8560/10000 train_loss:3.8545 train_time:1456094ms step_avg:170.30ms\nstep:8561/10000 train_loss:3.6778 train_time:1456265ms step_avg:170.30ms\nstep:8562/10000 train_loss:3.9144 train_time:1456434ms step_avg:170.30ms\nstep:8563/10000 train_loss:3.7304 train_time:1456603ms step_avg:170.30ms\nstep:8564/10000 train_loss:3.8498 train_time:1456773ms step_avg:170.30ms\nstep:8565/10000 train_loss:3.7521 train_time:1456945ms step_avg:170.30ms\nstep:8566/10000 train_loss:3.8147 train_time:1457116ms step_avg:170.30ms\nstep:8567/10000 train_loss:3.7164 train_time:1457287ms step_avg:170.30ms\nstep:8568/10000 train_loss:3.8182 train_time:1457459ms step_avg:170.30ms\nstep:8569/10000 train_loss:3.5225 train_time:1457628ms step_avg:170.30ms\nstep:8570/10000 train_loss:3.9079 train_time:1457797ms step_avg:170.30ms\nstep:8571/10000 train_loss:3.8156 train_time:1457966ms step_avg:170.30ms\nstep:8572/10000 train_loss:3.7809 train_time:1458135ms step_avg:170.30ms\nstep:8573/10000 train_loss:3.7421 train_time:1458303ms step_avg:170.30ms\nstep:8574/10000 train_loss:3.7878 train_time:1458473ms step_avg:170.30ms\nstep:8575/10000 train_loss:3.7568 train_time:1458643ms step_avg:170.30ms\nstep:8576/10000 train_loss:3.7748 train_time:1458812ms step_avg:170.30ms\nstep:8577/10000 train_loss:3.8127 train_time:1458983ms step_avg:170.30ms\nstep:8578/10000 train_loss:3.6402 train_time:1459153ms step_avg:170.30ms\nstep:8579/10000 train_loss:3.8402 train_time:1459323ms step_avg:170.30ms\nstep:8580/10000 train_loss:3.8609 train_time:1459493ms step_avg:170.30ms\nstep:8581/10000 train_loss:4.0067 train_time:1459664ms step_avg:170.30ms\nstep:8582/10000 train_loss:3.9315 train_time:1459833ms step_avg:170.30ms\nstep:8583/10000 train_loss:3.8843 train_time:1460006ms step_avg:170.30ms\nstep:8584/10000 train_loss:4.1501 train_time:1460177ms step_avg:170.30ms\nstep:8585/10000 train_loss:3.8417 train_time:1460349ms step_avg:170.30ms\nstep:8586/10000 train_loss:3.7060 train_time:1460521ms step_avg:170.30ms\nstep:8587/10000 train_loss:3.7374 train_time:1460690ms step_avg:170.30ms\nstep:8588/10000 train_loss:3.6835 train_time:1460866ms step_avg:170.30ms\nstep:8589/10000 train_loss:3.8688 train_time:1461033ms step_avg:170.30ms\nstep:8590/10000 train_loss:3.7370 train_time:1461204ms step_avg:170.30ms\nstep:8591/10000 train_loss:3.7679 train_time:1461373ms step_avg:170.30ms\nstep:8592/10000 train_loss:3.7322 train_time:1461544ms step_avg:170.30ms\nstep:8593/10000 train_loss:3.7436 train_time:1461713ms step_avg:170.30ms\nstep:8594/10000 train_loss:3.7989 train_time:1461883ms step_avg:170.30ms\nstep:8595/10000 train_loss:3.6492 train_time:1462052ms step_avg:170.30ms\nstep:8596/10000 train_loss:3.7502 train_time:1462225ms step_avg:170.30ms\nstep:8597/10000 train_loss:3.7438 train_time:1462394ms step_avg:170.30ms\nstep:8598/10000 train_loss:3.7724 train_time:1462565ms step_avg:170.30ms\nstep:8599/10000 train_loss:3.6575 train_time:1462735ms step_avg:170.30ms\nstep:8600/10000 train_loss:3.8784 train_time:1462904ms step_avg:170.30ms\nstep:8601/10000 train_loss:3.9945 train_time:1463077ms step_avg:170.30ms\nstep:8602/10000 train_loss:3.7477 train_time:1463248ms step_avg:170.30ms\nstep:8603/10000 train_loss:3.7829 train_time:1463418ms step_avg:170.30ms\nstep:8604/10000 train_loss:3.8097 train_time:1463586ms step_avg:170.30ms\nstep:8605/10000 train_loss:3.8260 train_time:1463757ms step_avg:170.30ms\nstep:8606/10000 train_loss:3.7056 train_time:1463928ms step_avg:170.30ms\nstep:8607/10000 train_loss:3.7737 train_time:1464100ms step_avg:170.30ms\nstep:8608/10000 train_loss:4.2242 train_time:1464267ms step_avg:170.30ms\nstep:8609/10000 train_loss:3.7813 train_time:1464438ms step_avg:170.30ms\nstep:8610/10000 train_loss:4.0284 train_time:1464606ms step_avg:170.30ms\nstep:8611/10000 train_loss:3.6776 train_time:1464778ms step_avg:170.30ms\nstep:8612/10000 train_loss:3.7266 train_time:1464948ms step_avg:170.30ms\nstep:8613/10000 train_loss:3.7569 train_time:1465120ms step_avg:170.30ms\nstep:8614/10000 train_loss:3.9404 train_time:1465288ms step_avg:170.30ms\nstep:8615/10000 train_loss:3.7437 train_time:1465460ms step_avg:170.30ms\nstep:8616/10000 train_loss:3.7323 train_time:1465630ms step_avg:170.30ms\nstep:8617/10000 train_loss:3.7232 train_time:1465798ms step_avg:170.30ms\nstep:8618/10000 train_loss:3.7142 train_time:1465967ms step_avg:170.30ms\nstep:8619/10000 train_loss:3.7858 train_time:1466139ms step_avg:170.30ms\nstep:8620/10000 train_loss:3.7556 train_time:1466308ms step_avg:170.30ms\nstep:8621/10000 train_loss:3.7879 train_time:1466477ms step_avg:170.30ms\nstep:8622/10000 train_loss:3.8502 train_time:1466647ms step_avg:170.30ms\nstep:8623/10000 train_loss:4.0083 train_time:1466818ms step_avg:170.30ms\nstep:8624/10000 train_loss:3.8059 train_time:1466987ms step_avg:170.30ms\nstep:8625/10000 train_loss:3.7589 train_time:1467158ms step_avg:170.30ms\nstep:8626/10000 train_loss:3.8237 train_time:1467327ms step_avg:170.30ms\nstep:8627/10000 train_loss:3.7641 train_time:1467496ms step_avg:170.30ms\nstep:8628/10000 train_loss:3.7874 train_time:1467666ms step_avg:170.30ms\nstep:8629/10000 train_loss:4.7064 train_time:1467837ms step_avg:170.30ms\nstep:8630/10000 train_loss:3.8024 train_time:1468011ms step_avg:170.30ms\nstep:8631/10000 train_loss:3.6550 train_time:1468180ms step_avg:170.30ms\nstep:8632/10000 train_loss:3.6952 train_time:1468351ms step_avg:170.30ms\nstep:8633/10000 train_loss:3.7263 train_time:1468522ms step_avg:170.30ms\nstep:8634/10000 train_loss:3.7936 train_time:1468690ms step_avg:170.30ms\nstep:8635/10000 train_loss:3.8837 train_time:1468861ms step_avg:170.30ms\nstep:8636/10000 train_loss:3.9225 train_time:1469031ms step_avg:170.30ms\nstep:8637/10000 train_loss:3.9746 train_time:1469204ms step_avg:170.30ms\nstep:8638/10000 train_loss:3.5929 train_time:1469375ms step_avg:170.30ms\nstep:8639/10000 train_loss:3.9962 train_time:1469547ms step_avg:170.30ms\nstep:8640/10000 train_loss:3.5580 train_time:1469718ms step_avg:170.30ms\nstep:8641/10000 train_loss:3.8170 train_time:1469885ms step_avg:170.30ms\nstep:8642/10000 train_loss:3.7507 train_time:1470054ms step_avg:170.30ms\nstep:8643/10000 train_loss:3.7496 train_time:1470226ms step_avg:170.30ms\nstep:8644/10000 train_loss:3.7023 train_time:1470399ms step_avg:170.30ms\nstep:8645/10000 train_loss:3.7798 train_time:1470566ms step_avg:170.30ms\nstep:8646/10000 train_loss:3.6722 train_time:1470736ms step_avg:170.30ms\nstep:8647/10000 train_loss:3.6308 train_time:1470908ms step_avg:170.30ms\nstep:8648/10000 train_loss:3.8450 train_time:1471078ms step_avg:170.30ms\nstep:8649/10000 train_loss:3.7691 train_time:1471253ms step_avg:170.30ms\nstep:8650/10000 train_loss:3.7901 train_time:1471421ms step_avg:170.30ms\nstep:8651/10000 train_loss:3.8286 train_time:1471596ms step_avg:170.30ms\nstep:8652/10000 train_loss:3.7708 train_time:1471765ms step_avg:170.30ms\nstep:8653/10000 train_loss:3.6848 train_time:1471932ms step_avg:170.30ms\nstep:8654/10000 train_loss:3.6378 train_time:1472104ms step_avg:170.30ms\nstep:8655/10000 train_loss:3.6408 train_time:1472273ms step_avg:170.30ms\nstep:8656/10000 train_loss:3.9102 train_time:1472443ms step_avg:170.30ms\nstep:8657/10000 train_loss:3.6680 train_time:1472614ms step_avg:170.30ms\nstep:8658/10000 train_loss:3.7624 train_time:1472786ms step_avg:170.30ms\nstep:8659/10000 train_loss:4.0513 train_time:1472963ms step_avg:170.30ms\nstep:8660/10000 train_loss:3.8630 train_time:1473132ms step_avg:170.30ms\nstep:8661/10000 train_loss:3.7686 train_time:1473301ms step_avg:170.30ms\nstep:8662/10000 train_loss:3.8574 train_time:1473467ms step_avg:170.30ms\nstep:8663/10000 train_loss:3.8033 train_time:1473632ms step_avg:170.30ms\nstep:8664/10000 train_loss:3.7766 train_time:1473807ms step_avg:170.30ms\nstep:8665/10000 train_loss:3.6336 train_time:1473977ms step_avg:170.30ms\nstep:8666/10000 train_loss:3.7438 train_time:1474148ms step_avg:170.30ms\nstep:8667/10000 train_loss:3.8676 train_time:1474316ms step_avg:170.30ms\nstep:8668/10000 train_loss:3.7344 train_time:1474488ms step_avg:170.30ms\nstep:8669/10000 train_loss:3.6689 train_time:1474658ms step_avg:170.30ms\nstep:8670/10000 train_loss:3.7461 train_time:1474825ms step_avg:170.30ms\nstep:8671/10000 train_loss:3.9008 train_time:1474996ms step_avg:170.30ms\nstep:8672/10000 train_loss:3.7559 train_time:1475163ms step_avg:170.30ms\nstep:8673/10000 train_loss:3.7680 train_time:1475332ms step_avg:170.30ms\nstep:8674/10000 train_loss:3.9191 train_time:1475502ms step_avg:170.30ms\nstep:8675/10000 train_loss:3.9511 train_time:1475673ms step_avg:170.30ms\nstep:8676/10000 train_loss:3.8286 train_time:1475846ms step_avg:170.30ms\nstep:8677/10000 train_loss:3.7847 train_time:1476017ms step_avg:170.30ms\nstep:8678/10000 train_loss:3.7486 train_time:1476187ms step_avg:170.30ms\nstep:8679/10000 train_loss:3.7624 train_time:1476358ms step_avg:170.30ms\nstep:8680/10000 train_loss:3.8415 train_time:1476528ms step_avg:170.30ms\nstep:8681/10000 train_loss:4.1128 train_time:1476698ms step_avg:170.30ms\nstep:8682/10000 train_loss:3.6773 train_time:1476869ms step_avg:170.30ms\nstep:8683/10000 train_loss:3.7837 train_time:1477041ms step_avg:170.30ms\nstep:8684/10000 train_loss:3.8135 train_time:1477210ms step_avg:170.30ms\nstep:8685/10000 train_loss:3.8939 train_time:1477379ms step_avg:170.30ms\nstep:8686/10000 train_loss:3.8781 train_time:1477550ms step_avg:170.30ms\nstep:8687/10000 train_loss:3.7909 train_time:1477721ms step_avg:170.30ms\nstep:8688/10000 train_loss:3.6694 train_time:1477891ms step_avg:170.30ms\nstep:8689/10000 train_loss:3.7055 train_time:1478063ms step_avg:170.30ms\nstep:8690/10000 train_loss:3.8193 train_time:1478233ms step_avg:170.30ms\nstep:8691/10000 train_loss:3.9106 train_time:1478401ms step_avg:170.30ms\nstep:8692/10000 train_loss:3.7377 train_time:1478573ms step_avg:170.30ms\nstep:8693/10000 train_loss:3.9087 train_time:1478745ms step_avg:170.30ms\nstep:8694/10000 train_loss:3.6467 train_time:1478917ms step_avg:170.30ms\nstep:8695/10000 train_loss:3.9143 train_time:1479086ms step_avg:170.30ms\nstep:8696/10000 train_loss:3.8210 train_time:1479258ms step_avg:170.30ms\nstep:8697/10000 train_loss:3.7171 train_time:1479426ms step_avg:170.30ms\nstep:8698/10000 train_loss:4.0908 train_time:1479597ms step_avg:170.30ms\nstep:8699/10000 train_loss:3.8325 train_time:1479768ms step_avg:170.30ms\nstep:8700/10000 train_loss:3.9014 train_time:1479938ms step_avg:170.30ms\nstep:8701/10000 train_loss:3.6941 train_time:1480108ms step_avg:170.30ms\nstep:8702/10000 train_loss:3.7930 train_time:1480280ms step_avg:170.30ms\nstep:8703/10000 train_loss:3.7770 train_time:1480450ms step_avg:170.30ms\nstep:8704/10000 train_loss:3.7411 train_time:1480624ms step_avg:170.30ms\nstep:8705/10000 train_loss:3.8601 train_time:1480792ms step_avg:170.30ms\nstep:8706/10000 train_loss:3.7692 train_time:1480961ms step_avg:170.30ms\nstep:8707/10000 train_loss:3.6336 train_time:1481132ms step_avg:170.30ms\nstep:8708/10000 train_loss:3.7021 train_time:1481305ms step_avg:170.30ms\nstep:8709/10000 train_loss:3.6910 train_time:1481474ms step_avg:170.30ms\nstep:8710/10000 train_loss:3.8705 train_time:1481645ms step_avg:170.30ms\nstep:8711/10000 train_loss:3.8428 train_time:1481817ms step_avg:170.30ms\nstep:8712/10000 train_loss:3.6915 train_time:1481985ms step_avg:170.30ms\nstep:8713/10000 train_loss:3.8642 train_time:1482155ms step_avg:170.30ms\nstep:8714/10000 train_loss:4.1675 train_time:1482325ms step_avg:170.30ms\nstep:8715/10000 train_loss:3.7244 train_time:1482494ms step_avg:170.30ms\nstep:8716/10000 train_loss:3.7760 train_time:1482664ms step_avg:170.30ms\nstep:8717/10000 train_loss:3.7379 train_time:1482834ms step_avg:170.30ms\nstep:8718/10000 train_loss:3.8163 train_time:1483004ms step_avg:170.30ms\nstep:8719/10000 train_loss:3.6897 train_time:1483173ms step_avg:170.30ms\nstep:8720/10000 train_loss:3.7482 train_time:1483346ms step_avg:170.30ms\nstep:8721/10000 train_loss:3.8098 train_time:1483516ms step_avg:170.30ms\nstep:8722/10000 train_loss:3.9440 train_time:1483686ms step_avg:170.30ms\nstep:8723/10000 train_loss:3.7880 train_time:1483855ms step_avg:170.30ms\nstep:8724/10000 train_loss:3.7066 train_time:1484026ms step_avg:170.30ms\nstep:8725/10000 train_loss:3.8421 train_time:1484195ms step_avg:170.30ms\nstep:8726/10000 train_loss:3.7955 train_time:1484364ms step_avg:170.30ms\nstep:8727/10000 train_loss:3.7525 train_time:1484533ms step_avg:170.30ms\nstep:8728/10000 train_loss:3.9181 train_time:1484701ms step_avg:170.30ms\nstep:8729/10000 train_loss:3.7277 train_time:1484871ms step_avg:170.30ms\nstep:8730/10000 train_loss:4.0523 train_time:1485043ms step_avg:170.30ms\nstep:8731/10000 train_loss:3.6976 train_time:1485213ms step_avg:170.30ms\nstep:8732/10000 train_loss:3.7339 train_time:1485382ms step_avg:170.30ms\nstep:8733/10000 train_loss:3.7054 train_time:1485551ms step_avg:170.30ms\nstep:8734/10000 train_loss:3.7489 train_time:1485723ms step_avg:170.30ms\nstep:8735/10000 train_loss:3.8419 train_time:1485895ms step_avg:170.30ms\nstep:8736/10000 train_loss:3.7210 train_time:1486068ms step_avg:170.30ms\nstep:8737/10000 train_loss:3.7069 train_time:1486237ms step_avg:170.30ms\nstep:8738/10000 train_loss:3.6175 train_time:1486408ms step_avg:170.30ms\nstep:8739/10000 train_loss:3.8729 train_time:1486581ms step_avg:170.30ms\nstep:8740/10000 train_loss:3.8371 train_time:1486750ms step_avg:170.30ms\nstep:8741/10000 train_loss:3.8932 train_time:1486920ms step_avg:170.30ms\nstep:8742/10000 train_loss:4.2033 train_time:1487088ms step_avg:170.30ms\nstep:8743/10000 train_loss:3.8091 train_time:1487258ms step_avg:170.30ms\nstep:8744/10000 train_loss:3.7683 train_time:1487428ms step_avg:170.30ms\nstep:8745/10000 train_loss:3.6890 train_time:1487600ms step_avg:170.30ms\nstep:8746/10000 train_loss:3.8917 train_time:1487768ms step_avg:170.30ms\nstep:8747/10000 train_loss:3.8101 train_time:1487941ms step_avg:170.30ms\nstep:8748/10000 train_loss:3.7982 train_time:1488111ms step_avg:170.30ms\nstep:8749/10000 train_loss:3.8972 train_time:1488281ms step_avg:170.30ms\nstep:8750/10000 train_loss:3.7519 train_time:1488453ms step_avg:170.30ms\nstep:8751/10000 train_loss:3.6352 train_time:1488624ms step_avg:170.30ms\nstep:8752/10000 train_loss:3.6704 train_time:1488794ms step_avg:170.30ms\nstep:8753/10000 train_loss:3.7413 train_time:1488966ms step_avg:170.30ms\nstep:8754/10000 train_loss:3.7179 train_time:1489135ms step_avg:170.30ms\nstep:8755/10000 train_loss:3.7008 train_time:1489304ms step_avg:170.30ms\nstep:8756/10000 train_loss:3.8071 train_time:1489474ms step_avg:170.30ms\nstep:8757/10000 train_loss:3.6104 train_time:1489645ms step_avg:170.30ms\nstep:8758/10000 train_loss:3.7910 train_time:1489814ms step_avg:170.30ms\nstep:8759/10000 train_loss:3.7776 train_time:1489982ms step_avg:170.30ms\nstep:8760/10000 train_loss:4.0072 train_time:1490151ms step_avg:170.30ms\nstep:8761/10000 train_loss:3.8699 train_time:1490322ms step_avg:170.30ms\nstep:8762/10000 train_loss:3.8444 train_time:1490494ms step_avg:170.30ms\nstep:8763/10000 train_loss:3.8421 train_time:1490665ms step_avg:170.30ms\nstep:8764/10000 train_loss:3.6344 train_time:1490836ms step_avg:170.30ms\nstep:8765/10000 train_loss:3.7485 train_time:1491008ms step_avg:170.30ms\nstep:8766/10000 train_loss:3.5284 train_time:1491177ms step_avg:170.30ms\nstep:8767/10000 train_loss:3.8246 train_time:1491348ms step_avg:170.30ms\nstep:8768/10000 train_loss:3.6935 train_time:1491520ms step_avg:170.30ms\nstep:8769/10000 train_loss:3.7065 train_time:1491690ms step_avg:170.30ms\nstep:8770/10000 train_loss:3.7032 train_time:1491858ms step_avg:170.30ms\nstep:8771/10000 train_loss:3.6805 train_time:1492029ms step_avg:170.30ms\nstep:8772/10000 train_loss:3.5953 train_time:1492199ms step_avg:170.30ms\nstep:8773/10000 train_loss:3.7644 train_time:1492369ms step_avg:170.30ms\nstep:8774/10000 train_loss:3.7733 train_time:1492540ms step_avg:170.30ms\nstep:8775/10000 train_loss:3.7093 train_time:1492710ms step_avg:170.30ms\nstep:8776/10000 train_loss:3.6849 train_time:1492879ms step_avg:170.30ms\nstep:8777/10000 train_loss:3.7058 train_time:1493048ms step_avg:170.30ms\nstep:8778/10000 train_loss:3.6608 train_time:1493217ms step_avg:170.30ms\nstep:8779/10000 train_loss:3.6653 train_time:1493386ms step_avg:170.30ms\nstep:8780/10000 train_loss:3.8175 train_time:1493557ms step_avg:170.30ms\nstep:8781/10000 train_loss:3.7595 train_time:1493727ms step_avg:170.30ms\nstep:8782/10000 train_loss:3.8432 train_time:1493898ms step_avg:170.30ms\nstep:8783/10000 train_loss:3.7449 train_time:1494067ms step_avg:170.30ms\nstep:8784/10000 train_loss:3.7782 train_time:1494236ms step_avg:170.30ms\nstep:8785/10000 train_loss:3.5993 train_time:1494409ms step_avg:170.30ms\nstep:8786/10000 train_loss:3.8605 train_time:1494584ms step_avg:170.30ms\nstep:8787/10000 train_loss:3.7132 train_time:1494748ms step_avg:170.30ms\nstep:8788/10000 train_loss:3.9800 train_time:1494918ms step_avg:170.30ms\nstep:8789/10000 train_loss:3.7880 train_time:1495087ms step_avg:170.30ms\nstep:8790/10000 train_loss:3.8735 train_time:1495258ms step_avg:170.30ms\nstep:8791/10000 train_loss:3.7452 train_time:1495427ms step_avg:170.30ms\nstep:8792/10000 train_loss:3.7621 train_time:1495599ms step_avg:170.30ms\nstep:8793/10000 train_loss:3.8899 train_time:1495767ms step_avg:170.30ms\nstep:8794/10000 train_loss:3.8268 train_time:1495937ms step_avg:170.30ms\nstep:8795/10000 train_loss:3.8254 train_time:1496109ms step_avg:170.30ms\nstep:8796/10000 train_loss:3.8444 train_time:1496281ms step_avg:170.30ms\nstep:8797/10000 train_loss:3.8206 train_time:1496450ms step_avg:170.30ms\nstep:8798/10000 train_loss:3.8363 train_time:1496622ms step_avg:170.30ms\nstep:8799/10000 train_loss:3.9358 train_time:1496791ms step_avg:170.30ms\nstep:8800/10000 train_loss:3.7660 train_time:1496962ms step_avg:170.30ms\nstep:8801/10000 train_loss:3.7183 train_time:1497133ms step_avg:170.30ms\nstep:8802/10000 train_loss:3.8349 train_time:1497303ms step_avg:170.30ms\nstep:8803/10000 train_loss:3.7359 train_time:1497473ms step_avg:170.30ms\nstep:8804/10000 train_loss:3.6720 train_time:1497650ms step_avg:170.30ms\nstep:8805/10000 train_loss:3.8727 train_time:1497816ms step_avg:170.30ms\nstep:8806/10000 train_loss:3.8039 train_time:1497986ms step_avg:170.30ms\nstep:8807/10000 train_loss:3.7997 train_time:1498158ms step_avg:170.30ms\nstep:8808/10000 train_loss:3.7818 train_time:1498327ms step_avg:170.30ms\nstep:8809/10000 train_loss:3.7897 train_time:1498498ms step_avg:170.30ms\nstep:8810/10000 train_loss:3.9467 train_time:1498666ms step_avg:170.30ms\nstep:8811/10000 train_loss:3.7240 train_time:1498836ms step_avg:170.30ms\nstep:8812/10000 train_loss:3.7351 train_time:1499008ms step_avg:170.30ms\nstep:8813/10000 train_loss:3.8194 train_time:1499178ms step_avg:170.30ms\nstep:8814/10000 train_loss:3.7581 train_time:1499352ms step_avg:170.30ms\nstep:8815/10000 train_loss:3.7349 train_time:1499520ms step_avg:170.30ms\nstep:8816/10000 train_loss:3.7278 train_time:1499690ms step_avg:170.30ms\nstep:8817/10000 train_loss:3.7135 train_time:1499861ms step_avg:170.30ms\nstep:8818/10000 train_loss:3.8286 train_time:1500031ms step_avg:170.30ms\nstep:8819/10000 train_loss:3.6712 train_time:1500202ms step_avg:170.30ms\nstep:8820/10000 train_loss:3.7834 train_time:1500370ms step_avg:170.30ms\nstep:8821/10000 train_loss:3.8613 train_time:1500541ms step_avg:170.30ms\nstep:8822/10000 train_loss:3.7902 train_time:1500711ms step_avg:170.30ms\nstep:8823/10000 train_loss:3.7395 train_time:1500881ms step_avg:170.30ms\nstep:8824/10000 train_loss:3.7420 train_time:1501052ms step_avg:170.30ms\nstep:8825/10000 train_loss:3.7633 train_time:1501223ms step_avg:170.30ms\nstep:8826/10000 train_loss:3.6725 train_time:1501393ms step_avg:170.30ms\nstep:8827/10000 train_loss:3.7143 train_time:1501564ms step_avg:170.30ms\nstep:8828/10000 train_loss:3.7171 train_time:1501736ms step_avg:170.30ms\nstep:8829/10000 train_loss:3.8906 train_time:1501904ms step_avg:170.30ms\nstep:8830/10000 train_loss:3.9268 train_time:1502074ms step_avg:170.30ms\nstep:8831/10000 train_loss:4.9887 train_time:1502242ms step_avg:170.30ms\nstep:8832/10000 train_loss:4.2127 train_time:1502411ms step_avg:170.30ms\nstep:8833/10000 train_loss:3.7800 train_time:1502580ms step_avg:170.30ms\nstep:8834/10000 train_loss:3.8841 train_time:1502750ms step_avg:170.30ms\nstep:8835/10000 train_loss:3.8286 train_time:1502920ms step_avg:170.30ms\nstep:8836/10000 train_loss:3.6376 train_time:1503090ms step_avg:170.30ms\nstep:8837/10000 train_loss:3.7586 train_time:1503262ms step_avg:170.30ms\nstep:8838/10000 train_loss:3.6506 train_time:1503431ms step_avg:170.30ms\nstep:8839/10000 train_loss:3.7808 train_time:1503604ms step_avg:170.30ms\nstep:8840/10000 train_loss:3.7681 train_time:1503775ms step_avg:170.30ms\nstep:8841/10000 train_loss:3.8378 train_time:1503946ms step_avg:170.30ms\nstep:8842/10000 train_loss:3.8176 train_time:1504114ms step_avg:170.30ms\nstep:8843/10000 train_loss:3.7687 train_time:1504286ms step_avg:170.30ms\nstep:8844/10000 train_loss:3.7580 train_time:1504456ms step_avg:170.30ms\nstep:8845/10000 train_loss:3.8771 train_time:1504626ms step_avg:170.30ms\nstep:8846/10000 train_loss:3.7413 train_time:1504796ms step_avg:170.30ms\nstep:8847/10000 train_loss:3.7700 train_time:1504970ms step_avg:170.30ms\nstep:8848/10000 train_loss:3.8103 train_time:1505141ms step_avg:170.30ms\nstep:8849/10000 train_loss:3.8150 train_time:1505308ms step_avg:170.30ms\nstep:8850/10000 train_loss:3.7324 train_time:1505479ms step_avg:170.30ms\nstep:8851/10000 train_loss:3.8454 train_time:1505647ms step_avg:170.30ms\nstep:8852/10000 train_loss:3.6873 train_time:1505817ms step_avg:170.30ms\nstep:8853/10000 train_loss:3.9108 train_time:1505986ms step_avg:170.30ms\nstep:8854/10000 train_loss:3.7223 train_time:1506158ms step_avg:170.30ms\nstep:8855/10000 train_loss:3.8684 train_time:1506327ms step_avg:170.30ms\nstep:8856/10000 train_loss:3.7684 train_time:1506498ms step_avg:170.30ms\nstep:8857/10000 train_loss:3.8150 train_time:1506668ms step_avg:170.30ms\nstep:8858/10000 train_loss:3.7942 train_time:1506838ms step_avg:170.30ms\nstep:8859/10000 train_loss:3.7231 train_time:1507008ms step_avg:170.30ms\nstep:8860/10000 train_loss:3.7806 train_time:1507181ms step_avg:170.30ms\nstep:8861/10000 train_loss:3.7301 train_time:1507349ms step_avg:170.30ms\nstep:8862/10000 train_loss:3.5909 train_time:1507520ms step_avg:170.30ms\nstep:8863/10000 train_loss:3.8148 train_time:1507690ms step_avg:170.30ms\nstep:8864/10000 train_loss:3.7742 train_time:1507860ms step_avg:170.30ms\nstep:8865/10000 train_loss:3.7297 train_time:1508030ms step_avg:170.30ms\nstep:8866/10000 train_loss:3.6051 train_time:1508202ms step_avg:170.30ms\nstep:8867/10000 train_loss:3.8115 train_time:1508373ms step_avg:170.30ms\nstep:8868/10000 train_loss:3.8152 train_time:1508544ms step_avg:170.30ms\nstep:8869/10000 train_loss:3.7720 train_time:1508714ms step_avg:170.30ms\nstep:8870/10000 train_loss:3.9224 train_time:1508885ms step_avg:170.30ms\nstep:8871/10000 train_loss:3.9136 train_time:1509056ms step_avg:170.30ms\nstep:8872/10000 train_loss:3.6704 train_time:1509227ms step_avg:170.30ms\nstep:8873/10000 train_loss:3.8713 train_time:1509402ms step_avg:170.30ms\nstep:8874/10000 train_loss:3.7890 train_time:1509570ms step_avg:170.30ms\nstep:8875/10000 train_loss:3.6452 train_time:1509742ms step_avg:170.30ms\nstep:8876/10000 train_loss:3.7236 train_time:1509912ms step_avg:170.30ms\nstep:8877/10000 train_loss:3.7967 train_time:1510087ms step_avg:170.30ms\nstep:8878/10000 train_loss:3.8819 train_time:1510256ms step_avg:170.30ms\nstep:8879/10000 train_loss:3.7347 train_time:1510428ms step_avg:170.30ms\nstep:8880/10000 train_loss:3.8625 train_time:1510597ms step_avg:170.30ms\nstep:8881/10000 train_loss:3.6525 train_time:1510765ms step_avg:170.30ms\nstep:8882/10000 train_loss:3.8920 train_time:1510934ms step_avg:170.30ms\nstep:8883/10000 train_loss:3.8086 train_time:1511107ms step_avg:170.30ms\nstep:8884/10000 train_loss:3.8261 train_time:1511279ms step_avg:170.30ms\nstep:8885/10000 train_loss:3.4710 train_time:1511449ms step_avg:170.30ms\nstep:8886/10000 train_loss:4.1359 train_time:1511623ms step_avg:170.30ms\nstep:8887/10000 train_loss:3.8455 train_time:1511795ms step_avg:170.30ms\nstep:8888/10000 train_loss:3.7689 train_time:1511964ms step_avg:170.30ms\nstep:8889/10000 train_loss:3.6857 train_time:1512134ms step_avg:170.30ms\nstep:8890/10000 train_loss:3.8472 train_time:1512305ms step_avg:170.30ms\nstep:8891/10000 train_loss:3.7858 train_time:1512475ms step_avg:170.30ms\nstep:8892/10000 train_loss:3.7788 train_time:1512647ms step_avg:170.30ms\nstep:8893/10000 train_loss:3.6742 train_time:1512819ms step_avg:170.30ms\nstep:8894/10000 train_loss:3.7940 train_time:1512990ms step_avg:170.31ms\nstep:8895/10000 train_loss:3.5809 train_time:1513160ms step_avg:170.31ms\nstep:8896/10000 train_loss:3.6846 train_time:1513329ms step_avg:170.30ms\nstep:8897/10000 train_loss:3.8008 train_time:1513498ms step_avg:170.30ms\nstep:8898/10000 train_loss:3.8023 train_time:1513669ms step_avg:170.30ms\nstep:8899/10000 train_loss:3.9611 train_time:1513841ms step_avg:170.30ms\nstep:8900/10000 train_loss:3.9579 train_time:1514011ms step_avg:170.30ms\nstep:8901/10000 train_loss:3.6938 train_time:1514185ms step_avg:170.31ms\nstep:8902/10000 train_loss:3.6702 train_time:1514356ms step_avg:170.31ms\nstep:8903/10000 train_loss:3.8012 train_time:1514526ms step_avg:170.31ms\nstep:8904/10000 train_loss:3.8655 train_time:1514697ms step_avg:170.31ms\nstep:8905/10000 train_loss:3.7855 train_time:1514867ms step_avg:170.31ms\nstep:8906/10000 train_loss:3.7434 train_time:1515037ms step_avg:170.31ms\nstep:8907/10000 train_loss:3.7365 train_time:1515207ms step_avg:170.31ms\nstep:8908/10000 train_loss:3.8905 train_time:1515379ms step_avg:170.31ms\nstep:8909/10000 train_loss:3.6882 train_time:1515546ms step_avg:170.31ms\nstep:8910/10000 train_loss:3.7326 train_time:1515717ms step_avg:170.31ms\nstep:8911/10000 train_loss:3.8590 train_time:1515884ms step_avg:170.30ms\nstep:8912/10000 train_loss:3.7137 train_time:1516053ms step_avg:170.30ms\nstep:8913/10000 train_loss:3.7140 train_time:1516223ms step_avg:170.30ms\nstep:8914/10000 train_loss:4.0733 train_time:1516392ms step_avg:170.30ms\nstep:8915/10000 train_loss:3.7176 train_time:1516564ms step_avg:170.30ms\nstep:8916/10000 train_loss:3.8717 train_time:1516735ms step_avg:170.30ms\nstep:8917/10000 train_loss:3.6883 train_time:1516906ms step_avg:170.30ms\nstep:8918/10000 train_loss:3.8240 train_time:1517078ms step_avg:170.31ms\nstep:8919/10000 train_loss:3.7545 train_time:1517246ms step_avg:170.30ms\nstep:8920/10000 train_loss:3.6267 train_time:1517416ms step_avg:170.30ms\nstep:8921/10000 train_loss:3.7389 train_time:1517584ms step_avg:170.30ms\nstep:8922/10000 train_loss:3.7442 train_time:1517754ms step_avg:170.30ms\nstep:8923/10000 train_loss:3.7477 train_time:1517926ms step_avg:170.30ms\nstep:8924/10000 train_loss:3.8677 train_time:1518098ms step_avg:170.30ms\nstep:8925/10000 train_loss:3.6506 train_time:1518267ms step_avg:170.30ms\nstep:8926/10000 train_loss:3.7839 train_time:1518438ms step_avg:170.30ms\nstep:8927/10000 train_loss:3.7413 train_time:1518607ms step_avg:170.30ms\nstep:8928/10000 train_loss:3.8327 train_time:1518776ms step_avg:170.30ms\nstep:8929/10000 train_loss:4.1296 train_time:1518948ms step_avg:170.30ms\nstep:8930/10000 train_loss:3.9960 train_time:1519117ms step_avg:170.30ms\nstep:8931/10000 train_loss:3.6152 train_time:1519285ms step_avg:170.30ms\nstep:8932/10000 train_loss:3.6282 train_time:1519458ms step_avg:170.30ms\nstep:8933/10000 train_loss:3.6654 train_time:1519626ms step_avg:170.30ms\nstep:8934/10000 train_loss:3.6963 train_time:1519797ms step_avg:170.30ms\nstep:8935/10000 train_loss:3.7449 train_time:1519968ms step_avg:170.30ms\nstep:8936/10000 train_loss:3.9665 train_time:1520140ms step_avg:170.30ms\nstep:8937/10000 train_loss:3.8253 train_time:1520307ms step_avg:170.30ms\nstep:8938/10000 train_loss:3.7775 train_time:1520479ms step_avg:170.30ms\nstep:8939/10000 train_loss:3.7259 train_time:1520648ms step_avg:170.30ms\nstep:8940/10000 train_loss:3.6780 train_time:1520817ms step_avg:170.30ms\nstep:8941/10000 train_loss:3.8793 train_time:1520988ms step_avg:170.30ms\nstep:8942/10000 train_loss:3.6613 train_time:1521161ms step_avg:170.30ms\nstep:8943/10000 train_loss:3.7942 train_time:1521329ms step_avg:170.30ms\nstep:8944/10000 train_loss:4.1246 train_time:1521501ms step_avg:170.30ms\nstep:8945/10000 train_loss:3.9246 train_time:1521672ms step_avg:170.30ms\nstep:8946/10000 train_loss:3.6986 train_time:1521840ms step_avg:170.30ms\nstep:8947/10000 train_loss:3.6438 train_time:1522009ms step_avg:170.30ms\nstep:8948/10000 train_loss:3.8388 train_time:1522181ms step_avg:170.30ms\nstep:8949/10000 train_loss:3.6554 train_time:1522347ms step_avg:170.30ms\nstep:8950/10000 train_loss:3.7314 train_time:1522519ms step_avg:170.30ms\nstep:8951/10000 train_loss:3.7578 train_time:1522689ms step_avg:170.30ms\nstep:8952/10000 train_loss:3.7485 train_time:1522859ms step_avg:170.30ms\nstep:8953/10000 train_loss:3.7805 train_time:1523027ms step_avg:170.30ms\nstep:8954/10000 train_loss:3.8249 train_time:1523198ms step_avg:170.30ms\nstep:8955/10000 train_loss:3.8046 train_time:1523367ms step_avg:170.30ms\nstep:8956/10000 train_loss:3.7980 train_time:1523538ms step_avg:170.30ms\nstep:8957/10000 train_loss:3.9194 train_time:1523708ms step_avg:170.30ms\nstep:8958/10000 train_loss:3.7685 train_time:1523877ms step_avg:170.30ms\nstep:8959/10000 train_loss:3.7658 train_time:1524046ms step_avg:170.30ms\nstep:8960/10000 train_loss:3.6316 train_time:1524216ms step_avg:170.30ms\nstep:8961/10000 train_loss:3.5775 train_time:1524386ms step_avg:170.30ms\nstep:8962/10000 train_loss:3.8046 train_time:1524557ms step_avg:170.30ms\nstep:8963/10000 train_loss:3.7613 train_time:1524725ms step_avg:170.30ms\nstep:8964/10000 train_loss:3.5841 train_time:1524895ms step_avg:170.30ms\nstep:8965/10000 train_loss:3.9454 train_time:1525062ms step_avg:170.30ms\nstep:8966/10000 train_loss:3.9338 train_time:1525231ms step_avg:170.30ms\nstep:8967/10000 train_loss:3.7258 train_time:1525401ms step_avg:170.30ms\nstep:8968/10000 train_loss:3.8186 train_time:1525573ms step_avg:170.30ms\nstep:8969/10000 train_loss:3.8171 train_time:1525744ms step_avg:170.30ms\nstep:8970/10000 train_loss:3.7470 train_time:1525914ms step_avg:170.30ms\nstep:8971/10000 train_loss:3.6352 train_time:1526082ms step_avg:170.30ms\nstep:8972/10000 train_loss:3.9033 train_time:1526255ms step_avg:170.30ms\nstep:8973/10000 train_loss:3.6317 train_time:1526418ms step_avg:170.30ms\nstep:8974/10000 train_loss:3.8049 train_time:1526586ms step_avg:170.30ms\nstep:8975/10000 train_loss:3.7794 train_time:1526758ms step_avg:170.30ms\nstep:8976/10000 train_loss:3.6280 train_time:1526926ms step_avg:170.30ms\nstep:8977/10000 train_loss:3.6860 train_time:1527096ms step_avg:170.30ms\nstep:8978/10000 train_loss:3.8087 train_time:1527264ms step_avg:170.30ms\nstep:8979/10000 train_loss:3.6293 train_time:1527433ms step_avg:170.30ms\nstep:8980/10000 train_loss:3.7238 train_time:1527608ms step_avg:170.30ms\nstep:8981/10000 train_loss:3.7559 train_time:1527782ms step_avg:170.30ms\nstep:8982/10000 train_loss:3.6174 train_time:1527953ms step_avg:170.30ms\nstep:8983/10000 train_loss:3.7448 train_time:1528127ms step_avg:170.30ms\nstep:8984/10000 train_loss:3.8614 train_time:1528296ms step_avg:170.30ms\nstep:8985/10000 train_loss:3.6694 train_time:1528465ms step_avg:170.30ms\nstep:8986/10000 train_loss:3.7618 train_time:1528633ms step_avg:170.30ms\nstep:8987/10000 train_loss:3.7966 train_time:1528805ms step_avg:170.30ms\nstep:8988/10000 train_loss:3.8412 train_time:1528977ms step_avg:170.30ms\nstep:8989/10000 train_loss:3.6174 train_time:1529146ms step_avg:170.30ms\nstep:8990/10000 train_loss:3.7925 train_time:1529318ms step_avg:170.30ms\nstep:8991/10000 train_loss:3.8582 train_time:1529487ms step_avg:170.30ms\nstep:8992/10000 train_loss:3.7275 train_time:1529660ms step_avg:170.30ms\nstep:8993/10000 train_loss:3.7679 train_time:1529828ms step_avg:170.30ms\nstep:8994/10000 train_loss:3.6934 train_time:1529999ms step_avg:170.30ms\nstep:8995/10000 train_loss:3.9994 train_time:1530168ms step_avg:170.30ms\nstep:8996/10000 train_loss:3.7537 train_time:1530340ms step_avg:170.30ms\nstep:8997/10000 train_loss:3.7078 train_time:1530510ms step_avg:170.30ms\nstep:8998/10000 train_loss:3.7108 train_time:1530683ms step_avg:170.30ms\nstep:8999/10000 train_loss:3.7204 train_time:1530855ms step_avg:170.30ms\nstep:9000/10000 train_loss:3.7826 train_time:1531022ms step_avg:170.30ms\nstep:9001/10000 train_loss:3.7145 train_time:1531192ms step_avg:170.30ms\nstep:9002/10000 train_loss:3.6482 train_time:1531362ms step_avg:170.30ms\nstep:9003/10000 train_loss:3.8103 train_time:1531533ms step_avg:170.30ms\nstep:9004/10000 train_loss:3.6761 train_time:1531702ms step_avg:170.30ms\nstep:9005/10000 train_loss:3.8412 train_time:1531874ms step_avg:170.30ms\nstep:9006/10000 train_loss:3.7366 train_time:1532045ms step_avg:170.30ms\nstep:9007/10000 train_loss:3.8092 train_time:1532217ms step_avg:170.30ms\nstep:9008/10000 train_loss:3.6527 train_time:1532384ms step_avg:170.30ms\nstep:9009/10000 train_loss:3.7788 train_time:1532554ms step_avg:170.30ms\nstep:9010/10000 train_loss:3.7656 train_time:1532723ms step_avg:170.30ms\nstep:9011/10000 train_loss:3.7182 train_time:1532895ms step_avg:170.30ms\nstep:9012/10000 train_loss:3.8878 train_time:1533065ms step_avg:170.30ms\nstep:9013/10000 train_loss:3.7501 train_time:1533236ms step_avg:170.30ms\nstep:9014/10000 train_loss:3.7272 train_time:1533405ms step_avg:170.30ms\nstep:9015/10000 train_loss:3.7959 train_time:1533577ms step_avg:170.30ms\nstep:9016/10000 train_loss:3.6321 train_time:1533747ms step_avg:170.30ms\nstep:9017/10000 train_loss:3.7898 train_time:1533920ms step_avg:170.30ms\nstep:9018/10000 train_loss:3.7318 train_time:1534089ms step_avg:170.30ms\nstep:9019/10000 train_loss:3.8543 train_time:1534259ms step_avg:170.30ms\nstep:9020/10000 train_loss:3.6276 train_time:1534429ms step_avg:170.30ms\nstep:9021/10000 train_loss:3.6349 train_time:1534598ms step_avg:170.30ms\nstep:9022/10000 train_loss:3.8606 train_time:1534768ms step_avg:170.30ms\nstep:9023/10000 train_loss:3.6402 train_time:1534939ms step_avg:170.30ms\nstep:9024/10000 train_loss:3.6877 train_time:1535109ms step_avg:170.30ms\nstep:9025/10000 train_loss:3.8275 train_time:1535279ms step_avg:170.30ms\nstep:9026/10000 train_loss:3.7640 train_time:1535449ms step_avg:170.30ms\nstep:9027/10000 train_loss:3.6680 train_time:1535621ms step_avg:170.30ms\nstep:9028/10000 train_loss:3.6933 train_time:1535790ms step_avg:170.30ms\nstep:9029/10000 train_loss:3.9498 train_time:1535960ms step_avg:170.30ms\nstep:9030/10000 train_loss:3.8664 train_time:1536132ms step_avg:170.30ms\nstep:9031/10000 train_loss:3.8948 train_time:1536312ms step_avg:170.30ms\nstep:9032/10000 train_loss:3.7037 train_time:1536483ms step_avg:170.30ms\nstep:9033/10000 train_loss:3.6961 train_time:1536646ms step_avg:170.30ms\nstep:9034/10000 train_loss:3.7086 train_time:1536817ms step_avg:170.30ms\nstep:9035/10000 train_loss:3.6547 train_time:1536983ms step_avg:170.30ms\nstep:9036/10000 train_loss:3.7716 train_time:1537160ms step_avg:170.30ms\nstep:9037/10000 train_loss:3.7292 train_time:1537325ms step_avg:170.30ms\nstep:9038/10000 train_loss:3.6196 train_time:1537497ms step_avg:170.30ms\nstep:9039/10000 train_loss:3.8052 train_time:1537668ms step_avg:170.30ms\nstep:9040/10000 train_loss:3.6908 train_time:1537841ms step_avg:170.30ms\nstep:9041/10000 train_loss:3.8122 train_time:1538010ms step_avg:170.30ms\nstep:9042/10000 train_loss:3.9064 train_time:1538182ms step_avg:170.30ms\nstep:9043/10000 train_loss:3.8272 train_time:1538353ms step_avg:170.30ms\nstep:9044/10000 train_loss:3.9339 train_time:1538526ms step_avg:170.30ms\nstep:9045/10000 train_loss:3.6316 train_time:1538696ms step_avg:170.30ms\nstep:9046/10000 train_loss:3.7056 train_time:1538867ms step_avg:170.30ms\nstep:9047/10000 train_loss:3.8131 train_time:1539037ms step_avg:170.30ms\nstep:9048/10000 train_loss:3.8440 train_time:1539205ms step_avg:170.30ms\nstep:9049/10000 train_loss:3.8271 train_time:1539375ms step_avg:170.30ms\nstep:9050/10000 train_loss:4.8913 train_time:1539543ms step_avg:170.30ms\nstep:9051/10000 train_loss:3.6289 train_time:1539714ms step_avg:170.30ms\nstep:9052/10000 train_loss:4.1243 train_time:1539885ms step_avg:170.30ms\nstep:9053/10000 train_loss:3.6825 train_time:1540054ms step_avg:170.30ms\nstep:9054/10000 train_loss:3.8848 train_time:1540228ms step_avg:170.30ms\nstep:9055/10000 train_loss:3.7617 train_time:1540401ms step_avg:170.30ms\nstep:9056/10000 train_loss:3.8095 train_time:1540571ms step_avg:170.30ms\nstep:9057/10000 train_loss:3.7158 train_time:1540743ms step_avg:170.30ms\nstep:9058/10000 train_loss:3.7812 train_time:1540913ms step_avg:170.30ms\nstep:9059/10000 train_loss:3.7980 train_time:1541081ms step_avg:170.30ms\nstep:9060/10000 train_loss:3.8758 train_time:1541252ms step_avg:170.30ms\nstep:9061/10000 train_loss:3.7991 train_time:1541424ms step_avg:170.30ms\nstep:9062/10000 train_loss:3.7294 train_time:1541596ms step_avg:170.30ms\nstep:9063/10000 train_loss:3.6083 train_time:1541766ms step_avg:170.30ms\nstep:9064/10000 train_loss:3.7980 train_time:1541935ms step_avg:170.30ms\nstep:9065/10000 train_loss:3.8570 train_time:1542109ms step_avg:170.30ms\nstep:9066/10000 train_loss:3.7255 train_time:1542280ms step_avg:170.30ms\nstep:9067/10000 train_loss:3.7544 train_time:1542449ms step_avg:170.30ms\nstep:9068/10000 train_loss:3.7971 train_time:1542619ms step_avg:170.30ms\nstep:9069/10000 train_loss:3.6911 train_time:1542789ms step_avg:170.30ms\nstep:9070/10000 train_loss:3.7537 train_time:1542959ms step_avg:170.30ms\nstep:9071/10000 train_loss:3.7555 train_time:1543131ms step_avg:170.30ms\nstep:9072/10000 train_loss:3.5219 train_time:1543302ms step_avg:170.30ms\nstep:9073/10000 train_loss:3.8289 train_time:1543470ms step_avg:170.30ms\nstep:9074/10000 train_loss:3.7801 train_time:1543640ms step_avg:170.30ms\nstep:9075/10000 train_loss:3.7935 train_time:1543808ms step_avg:170.30ms\nstep:9076/10000 train_loss:3.6880 train_time:1543976ms step_avg:170.30ms\nstep:9077/10000 train_loss:3.7158 train_time:1544147ms step_avg:170.30ms\nstep:9078/10000 train_loss:3.8208 train_time:1544320ms step_avg:170.30ms\nstep:9079/10000 train_loss:3.8171 train_time:1544489ms step_avg:170.30ms\nstep:9080/10000 train_loss:3.8785 train_time:1544661ms step_avg:170.30ms\nstep:9081/10000 train_loss:3.7169 train_time:1544830ms step_avg:170.30ms\nstep:9082/10000 train_loss:3.7181 train_time:1544999ms step_avg:170.30ms\nstep:9083/10000 train_loss:3.7906 train_time:1545166ms step_avg:170.30ms\nstep:9084/10000 train_loss:3.6095 train_time:1545336ms step_avg:170.30ms\nstep:9085/10000 train_loss:3.8151 train_time:1545506ms step_avg:170.30ms\nstep:9086/10000 train_loss:3.6803 train_time:1545678ms step_avg:170.30ms\nstep:9087/10000 train_loss:3.7941 train_time:1545847ms step_avg:170.30ms\nstep:9088/10000 train_loss:3.6840 train_time:1546017ms step_avg:170.30ms\nstep:9089/10000 train_loss:3.8223 train_time:1546193ms step_avg:170.30ms\nstep:9090/10000 train_loss:3.9033 train_time:1546359ms step_avg:170.30ms\nstep:9091/10000 train_loss:3.7340 train_time:1546526ms step_avg:170.30ms\nstep:9092/10000 train_loss:3.6514 train_time:1546697ms step_avg:170.30ms\nstep:9093/10000 train_loss:3.6833 train_time:1546866ms step_avg:170.30ms\nstep:9094/10000 train_loss:3.7500 train_time:1547038ms step_avg:170.30ms\nstep:9095/10000 train_loss:3.7347 train_time:1547205ms step_avg:170.30ms\nstep:9096/10000 train_loss:3.6260 train_time:1547374ms step_avg:170.30ms\nstep:9097/10000 train_loss:3.6669 train_time:1547544ms step_avg:170.30ms\nstep:9098/10000 train_loss:3.8177 train_time:1547717ms step_avg:170.30ms\nstep:9099/10000 train_loss:3.7624 train_time:1547887ms step_avg:170.30ms\nstep:9100/10000 train_loss:4.0271 train_time:1548056ms step_avg:170.30ms\nstep:9101/10000 train_loss:3.6480 train_time:1548225ms step_avg:170.30ms\nstep:9102/10000 train_loss:3.7298 train_time:1548395ms step_avg:170.30ms\nstep:9103/10000 train_loss:3.7453 train_time:1548566ms step_avg:170.30ms\nstep:9104/10000 train_loss:3.6782 train_time:1548737ms step_avg:170.30ms\nstep:9105/10000 train_loss:3.8887 train_time:1548907ms step_avg:170.30ms\nstep:9106/10000 train_loss:3.8483 train_time:1549077ms step_avg:170.30ms\nstep:9107/10000 train_loss:3.7812 train_time:1549246ms step_avg:170.30ms\nstep:9108/10000 train_loss:3.6952 train_time:1549417ms step_avg:170.30ms\nstep:9109/10000 train_loss:3.7550 train_time:1549586ms step_avg:170.30ms\nstep:9110/10000 train_loss:3.8281 train_time:1549754ms step_avg:170.30ms\nstep:9111/10000 train_loss:3.6756 train_time:1549924ms step_avg:170.30ms\nstep:9112/10000 train_loss:3.7766 train_time:1550097ms step_avg:170.30ms\nstep:9113/10000 train_loss:3.7542 train_time:1550266ms step_avg:170.30ms\nstep:9114/10000 train_loss:3.7434 train_time:1550437ms step_avg:170.30ms\nstep:9115/10000 train_loss:3.7943 train_time:1550608ms step_avg:170.30ms\nstep:9116/10000 train_loss:3.7245 train_time:1550779ms step_avg:170.30ms\nstep:9117/10000 train_loss:3.7300 train_time:1550946ms step_avg:170.30ms\nstep:9118/10000 train_loss:3.8552 train_time:1551120ms step_avg:170.30ms\nstep:9119/10000 train_loss:3.9507 train_time:1551287ms step_avg:170.30ms\nstep:9120/10000 train_loss:3.7394 train_time:1551459ms step_avg:170.30ms\nstep:9121/10000 train_loss:3.7324 train_time:1551628ms step_avg:170.30ms\nstep:9122/10000 train_loss:3.9134 train_time:1551799ms step_avg:170.30ms\nstep:9123/10000 train_loss:3.7331 train_time:1551968ms step_avg:170.30ms\nstep:9124/10000 train_loss:3.6189 train_time:1552138ms step_avg:170.30ms\nstep:9125/10000 train_loss:3.7004 train_time:1552308ms step_avg:170.30ms\nstep:9126/10000 train_loss:3.7168 train_time:1552480ms step_avg:170.30ms\nstep:9127/10000 train_loss:3.7659 train_time:1552650ms step_avg:170.30ms\nstep:9128/10000 train_loss:3.7327 train_time:1552820ms step_avg:170.30ms\nstep:9129/10000 train_loss:3.6837 train_time:1552988ms step_avg:170.30ms\nstep:9130/10000 train_loss:3.6492 train_time:1553157ms step_avg:170.30ms\nstep:9131/10000 train_loss:3.7829 train_time:1553325ms step_avg:170.30ms\nstep:9132/10000 train_loss:3.8638 train_time:1553495ms step_avg:170.30ms\nstep:9133/10000 train_loss:3.8000 train_time:1553664ms step_avg:170.30ms\nstep:9134/10000 train_loss:3.8531 train_time:1553836ms step_avg:170.30ms\nstep:9135/10000 train_loss:3.7277 train_time:1554007ms step_avg:170.30ms\nstep:9136/10000 train_loss:3.6522 train_time:1554178ms step_avg:170.30ms\nstep:9137/10000 train_loss:3.8807 train_time:1554347ms step_avg:170.30ms\nstep:9138/10000 train_loss:3.6917 train_time:1554518ms step_avg:170.30ms\nstep:9139/10000 train_loss:3.8161 train_time:1554688ms step_avg:170.30ms\nstep:9140/10000 train_loss:3.7106 train_time:1554861ms step_avg:170.30ms\nstep:9141/10000 train_loss:3.6686 train_time:1555030ms step_avg:170.30ms\nstep:9142/10000 train_loss:3.6245 train_time:1555199ms step_avg:170.30ms\nstep:9143/10000 train_loss:3.7615 train_time:1555369ms step_avg:170.30ms\nstep:9144/10000 train_loss:3.8564 train_time:1555758ms step_avg:170.33ms\nstep:9145/10000 train_loss:3.8630 train_time:1555923ms step_avg:170.33ms\nstep:9146/10000 train_loss:3.7258 train_time:1556092ms step_avg:170.33ms\nstep:9147/10000 train_loss:3.7743 train_time:1556259ms step_avg:170.32ms\nstep:9148/10000 train_loss:3.8429 train_time:1556427ms step_avg:170.32ms\nstep:9149/10000 train_loss:3.8180 train_time:1556598ms step_avg:170.32ms\nstep:9150/10000 train_loss:3.8697 train_time:1556782ms step_avg:170.33ms\nstep:9151/10000 train_loss:3.6960 train_time:1556950ms step_avg:170.33ms\nstep:9152/10000 train_loss:3.6689 train_time:1557122ms step_avg:170.33ms\nstep:9153/10000 train_loss:3.7706 train_time:1557291ms step_avg:170.33ms\nstep:9154/10000 train_loss:3.7437 train_time:1557462ms step_avg:170.33ms\nstep:9155/10000 train_loss:3.7698 train_time:1557633ms step_avg:170.33ms\nstep:9156/10000 train_loss:3.8097 train_time:1557809ms step_avg:170.33ms\nstep:9157/10000 train_loss:3.7657 train_time:1557981ms step_avg:170.33ms\nstep:9158/10000 train_loss:3.7542 train_time:1558151ms step_avg:170.33ms\nstep:9159/10000 train_loss:3.7697 train_time:1558319ms step_avg:170.33ms\nstep:9160/10000 train_loss:3.7455 train_time:1558488ms step_avg:170.33ms\nstep:9161/10000 train_loss:3.8699 train_time:1558662ms step_avg:170.33ms\nstep:9162/10000 train_loss:3.7606 train_time:1558834ms step_avg:170.33ms\nstep:9163/10000 train_loss:3.7476 train_time:1559007ms step_avg:170.33ms\nstep:9164/10000 train_loss:3.7359 train_time:1559177ms step_avg:170.33ms\nstep:9165/10000 train_loss:3.7882 train_time:1559346ms step_avg:170.33ms\nstep:9166/10000 train_loss:3.7639 train_time:1559516ms step_avg:170.33ms\nstep:9167/10000 train_loss:3.8325 train_time:1559686ms step_avg:170.33ms\nstep:9168/10000 train_loss:3.9402 train_time:1559858ms step_avg:170.33ms\nstep:9169/10000 train_loss:3.7616 train_time:1560028ms step_avg:170.33ms\nstep:9170/10000 train_loss:3.7124 train_time:1560198ms step_avg:170.33ms\nstep:9171/10000 train_loss:3.8178 train_time:1560365ms step_avg:170.33ms\nstep:9172/10000 train_loss:3.7111 train_time:1560536ms step_avg:170.33ms\nstep:9173/10000 train_loss:3.7962 train_time:1560704ms step_avg:170.33ms\nstep:9174/10000 train_loss:3.8547 train_time:1560877ms step_avg:170.33ms\nstep:9175/10000 train_loss:3.8295 train_time:1561047ms step_avg:170.33ms\nstep:9176/10000 train_loss:3.7582 train_time:1561216ms step_avg:170.33ms\nstep:9177/10000 train_loss:3.8011 train_time:1561386ms step_avg:170.33ms\nstep:9178/10000 train_loss:3.8417 train_time:1561558ms step_avg:170.33ms\nstep:9179/10000 train_loss:3.7890 train_time:1561725ms step_avg:170.33ms\nstep:9180/10000 train_loss:3.7069 train_time:1561895ms step_avg:170.33ms\nstep:9181/10000 train_loss:3.7274 train_time:1562066ms step_avg:170.33ms\nstep:9182/10000 train_loss:3.7029 train_time:1562236ms step_avg:170.33ms\nstep:9183/10000 train_loss:3.7064 train_time:1562406ms step_avg:170.33ms\nstep:9184/10000 train_loss:3.7439 train_time:1562578ms step_avg:170.33ms\nstep:9185/10000 train_loss:3.7290 train_time:1562750ms step_avg:170.33ms\nstep:9186/10000 train_loss:4.0347 train_time:1562924ms step_avg:170.33ms\nstep:9187/10000 train_loss:3.8026 train_time:1563094ms step_avg:170.33ms\nstep:9188/10000 train_loss:3.8035 train_time:1563265ms step_avg:170.33ms\nstep:9189/10000 train_loss:3.7400 train_time:1563435ms step_avg:170.33ms\nstep:9190/10000 train_loss:3.7108 train_time:1563604ms step_avg:170.33ms\nstep:9191/10000 train_loss:3.7311 train_time:1563776ms step_avg:170.33ms\nstep:9192/10000 train_loss:3.8640 train_time:1563948ms step_avg:170.33ms\nstep:9193/10000 train_loss:3.6883 train_time:1564119ms step_avg:170.33ms\nstep:9194/10000 train_loss:3.6809 train_time:1564287ms step_avg:170.33ms\nstep:9195/10000 train_loss:3.8333 train_time:1564456ms step_avg:170.33ms\nstep:9196/10000 train_loss:3.7890 train_time:1564626ms step_avg:170.33ms\nstep:9197/10000 train_loss:3.8717 train_time:1564797ms step_avg:170.33ms\nstep:9198/10000 train_loss:3.8360 train_time:1564965ms step_avg:170.33ms\nstep:9199/10000 train_loss:3.8127 train_time:1565136ms step_avg:170.33ms\nstep:9200/10000 train_loss:3.7348 train_time:1565306ms step_avg:170.33ms\nstep:9201/10000 train_loss:3.8231 train_time:1565477ms step_avg:170.33ms\nstep:9202/10000 train_loss:3.8062 train_time:1565645ms step_avg:170.33ms\nstep:9203/10000 train_loss:3.7262 train_time:1565816ms step_avg:170.33ms\nstep:9204/10000 train_loss:3.8680 train_time:1565987ms step_avg:170.33ms\nstep:9205/10000 train_loss:3.7789 train_time:1566157ms step_avg:170.33ms\nstep:9206/10000 train_loss:3.7316 train_time:1566327ms step_avg:170.33ms\nstep:9207/10000 train_loss:3.7646 train_time:1566499ms step_avg:170.33ms\nstep:9208/10000 train_loss:3.8609 train_time:1566669ms step_avg:170.33ms\nstep:9209/10000 train_loss:3.7833 train_time:1566841ms step_avg:170.33ms\nstep:9210/10000 train_loss:4.1263 train_time:1567010ms step_avg:170.33ms\nstep:9211/10000 train_loss:3.6132 train_time:1567179ms step_avg:170.33ms\nstep:9212/10000 train_loss:3.6285 train_time:1567350ms step_avg:170.33ms\nstep:9213/10000 train_loss:3.7437 train_time:1567517ms step_avg:170.33ms\nstep:9214/10000 train_loss:3.6840 train_time:1567686ms step_avg:170.33ms\nstep:9215/10000 train_loss:3.9074 train_time:1567859ms step_avg:170.33ms\nstep:9216/10000 train_loss:3.7213 train_time:1568027ms step_avg:170.33ms\nstep:9217/10000 train_loss:3.6556 train_time:1568197ms step_avg:170.33ms\nstep:9218/10000 train_loss:3.7049 train_time:1568368ms step_avg:170.33ms\nstep:9219/10000 train_loss:3.9179 train_time:1568538ms step_avg:170.33ms\nstep:9220/10000 train_loss:3.7886 train_time:1568708ms step_avg:170.33ms\nstep:9221/10000 train_loss:3.8083 train_time:1568880ms step_avg:170.33ms\nstep:9222/10000 train_loss:4.0215 train_time:1569051ms step_avg:170.33ms\nstep:9223/10000 train_loss:3.6817 train_time:1569226ms step_avg:170.33ms\nstep:9224/10000 train_loss:3.5933 train_time:1569393ms step_avg:170.33ms\nstep:9225/10000 train_loss:3.7813 train_time:1569565ms step_avg:170.33ms\nstep:9226/10000 train_loss:3.7423 train_time:1569737ms step_avg:170.33ms\nstep:9227/10000 train_loss:3.7713 train_time:1569907ms step_avg:170.33ms\nstep:9228/10000 train_loss:3.7352 train_time:1570078ms step_avg:170.33ms\nstep:9229/10000 train_loss:3.7603 train_time:1570243ms step_avg:170.33ms\nstep:9230/10000 train_loss:3.7300 train_time:1570413ms step_avg:170.33ms\nstep:9231/10000 train_loss:3.5597 train_time:1570583ms step_avg:170.33ms\nstep:9232/10000 train_loss:3.8925 train_time:1570753ms step_avg:170.33ms\nstep:9233/10000 train_loss:3.6827 train_time:1570924ms step_avg:170.33ms\nstep:9234/10000 train_loss:3.7374 train_time:1571096ms step_avg:170.33ms\nstep:9235/10000 train_loss:3.6701 train_time:1571265ms step_avg:170.33ms\nstep:9236/10000 train_loss:3.7393 train_time:1571433ms step_avg:170.33ms\nstep:9237/10000 train_loss:3.7696 train_time:1571603ms step_avg:170.33ms\nstep:9238/10000 train_loss:3.6824 train_time:1571772ms step_avg:170.33ms\nstep:9239/10000 train_loss:3.6962 train_time:1571944ms step_avg:170.33ms\nstep:9240/10000 train_loss:3.6882 train_time:1572114ms step_avg:170.33ms\nstep:9241/10000 train_loss:3.8566 train_time:1572284ms step_avg:170.33ms\nstep:9242/10000 train_loss:3.8182 train_time:1572450ms step_avg:170.33ms\nstep:9243/10000 train_loss:3.6891 train_time:1572622ms step_avg:170.33ms\nstep:9244/10000 train_loss:3.7486 train_time:1572792ms step_avg:170.33ms\nstep:9245/10000 train_loss:3.7435 train_time:1572965ms step_avg:170.33ms\nstep:9246/10000 train_loss:3.6829 train_time:1573138ms step_avg:170.33ms\nstep:9247/10000 train_loss:3.7350 train_time:1573308ms step_avg:170.33ms\nstep:9248/10000 train_loss:3.5962 train_time:1573478ms step_avg:170.33ms\nstep:9249/10000 train_loss:3.7473 train_time:1573647ms step_avg:170.33ms\nstep:9250/10000 train_loss:3.7205 train_time:1573817ms step_avg:170.33ms\nstep:9251/10000 train_loss:3.7418 train_time:1573987ms step_avg:170.33ms\nstep:9252/10000 train_loss:3.7928 train_time:1574159ms step_avg:170.33ms\nstep:9253/10000 train_loss:3.7483 train_time:1574330ms step_avg:170.33ms\nstep:9254/10000 train_loss:3.8681 train_time:1574503ms step_avg:170.33ms\nstep:9255/10000 train_loss:3.7712 train_time:1574670ms step_avg:170.33ms\nstep:9256/10000 train_loss:3.6432 train_time:1574841ms step_avg:170.33ms\nstep:9257/10000 train_loss:3.8877 train_time:1575011ms step_avg:170.33ms\nstep:9258/10000 train_loss:3.7843 train_time:1575183ms step_avg:170.33ms\nstep:9259/10000 train_loss:3.9338 train_time:1575355ms step_avg:170.33ms\nstep:9260/10000 train_loss:3.8272 train_time:1575524ms step_avg:170.33ms\nstep:9261/10000 train_loss:3.6776 train_time:1575695ms step_avg:170.33ms\nstep:9262/10000 train_loss:3.6661 train_time:1575862ms step_avg:170.33ms\nstep:9263/10000 train_loss:3.6491 train_time:1576030ms step_avg:170.33ms\nstep:9264/10000 train_loss:3.7957 train_time:1576200ms step_avg:170.33ms\nstep:9265/10000 train_loss:3.7272 train_time:1576370ms step_avg:170.33ms\nstep:9266/10000 train_loss:3.7697 train_time:1576541ms step_avg:170.33ms\nstep:9267/10000 train_loss:3.8396 train_time:1576712ms step_avg:170.33ms\nstep:9268/10000 train_loss:3.7259 train_time:1576883ms step_avg:170.33ms\nstep:9269/10000 train_loss:3.6789 train_time:1577052ms step_avg:170.33ms\nstep:9270/10000 train_loss:3.8842 train_time:1577224ms step_avg:170.33ms\nstep:9271/10000 train_loss:3.8343 train_time:1577395ms step_avg:170.33ms\nstep:9272/10000 train_loss:3.7473 train_time:1577564ms step_avg:170.33ms\nstep:9273/10000 train_loss:3.7491 train_time:1577736ms step_avg:170.33ms\nstep:9274/10000 train_loss:3.7383 train_time:1577905ms step_avg:170.33ms\nstep:9275/10000 train_loss:3.8376 train_time:1578076ms step_avg:170.33ms\nstep:9276/10000 train_loss:3.6453 train_time:1578245ms step_avg:170.33ms\nstep:9277/10000 train_loss:3.7500 train_time:1578414ms step_avg:170.33ms\nstep:9278/10000 train_loss:3.6346 train_time:1578586ms step_avg:170.33ms\nstep:9279/10000 train_loss:3.6665 train_time:1578757ms step_avg:170.33ms\nstep:9280/10000 train_loss:3.9615 train_time:1578925ms step_avg:170.33ms\nstep:9281/10000 train_loss:3.8039 train_time:1579097ms step_avg:170.33ms\nstep:9282/10000 train_loss:3.7366 train_time:1579266ms step_avg:170.33ms\nstep:9283/10000 train_loss:3.7901 train_time:1579435ms step_avg:170.33ms\nstep:9284/10000 train_loss:3.8312 train_time:1579604ms step_avg:170.33ms\nstep:9285/10000 train_loss:3.8705 train_time:1579775ms step_avg:170.33ms\nstep:9286/10000 train_loss:3.6251 train_time:1579946ms step_avg:170.33ms\nstep:9287/10000 train_loss:3.7879 train_time:1580116ms step_avg:170.33ms\nstep:9288/10000 train_loss:3.6748 train_time:1580285ms step_avg:170.33ms\nstep:9289/10000 train_loss:3.8285 train_time:1580458ms step_avg:170.33ms\nstep:9290/10000 train_loss:3.7194 train_time:1580626ms step_avg:170.33ms\nstep:9291/10000 train_loss:3.8256 train_time:1580797ms step_avg:170.33ms\nstep:9292/10000 train_loss:3.6467 train_time:1580966ms step_avg:170.33ms\nstep:9293/10000 train_loss:3.7331 train_time:1581138ms step_avg:170.33ms\nstep:9294/10000 train_loss:3.7717 train_time:1581305ms step_avg:170.33ms\nstep:9295/10000 train_loss:3.7407 train_time:1581477ms step_avg:170.33ms\nstep:9296/10000 train_loss:3.6965 train_time:1581646ms step_avg:170.33ms\nstep:9297/10000 train_loss:3.7792 train_time:1581816ms step_avg:170.33ms\nstep:9298/10000 train_loss:3.5525 train_time:1581987ms step_avg:170.33ms\nstep:9299/10000 train_loss:3.7456 train_time:1582156ms step_avg:170.33ms\nstep:9300/10000 train_loss:3.7446 train_time:1582327ms step_avg:170.33ms\nstep:9301/10000 train_loss:4.1682 train_time:1582497ms step_avg:170.33ms\nstep:9302/10000 train_loss:3.7291 train_time:1582663ms step_avg:170.33ms\nstep:9303/10000 train_loss:3.8253 train_time:1582831ms step_avg:170.33ms\nstep:9304/10000 train_loss:3.7014 train_time:1583000ms step_avg:170.32ms\nstep:9305/10000 train_loss:3.7997 train_time:1583172ms step_avg:170.33ms\nstep:9306/10000 train_loss:3.8135 train_time:1583344ms step_avg:170.33ms\nstep:9307/10000 train_loss:3.7496 train_time:1583513ms step_avg:170.33ms\nstep:9308/10000 train_loss:3.9710 train_time:1583682ms step_avg:170.33ms\nstep:9309/10000 train_loss:3.6832 train_time:1583852ms step_avg:170.32ms\nstep:9310/10000 train_loss:3.6908 train_time:1584023ms step_avg:170.33ms\nstep:9311/10000 train_loss:3.6783 train_time:1584193ms step_avg:170.32ms\nstep:9312/10000 train_loss:3.9296 train_time:1584361ms step_avg:170.32ms\nstep:9313/10000 train_loss:3.7985 train_time:1584531ms step_avg:170.32ms\nstep:9314/10000 train_loss:3.7278 train_time:1584704ms step_avg:170.32ms\nstep:9315/10000 train_loss:3.6996 train_time:1584875ms step_avg:170.33ms\nstep:9316/10000 train_loss:3.5527 train_time:1585047ms step_avg:170.33ms\nstep:9317/10000 train_loss:3.6377 train_time:1585225ms step_avg:170.33ms\nstep:9318/10000 train_loss:3.6650 train_time:1585394ms step_avg:170.33ms\nstep:9319/10000 train_loss:3.9122 train_time:1585568ms step_avg:170.33ms\nstep:9320/10000 train_loss:3.7730 train_time:1585737ms step_avg:170.33ms\nstep:9321/10000 train_loss:3.6671 train_time:1585908ms step_avg:170.33ms\nstep:9322/10000 train_loss:3.7819 train_time:1586078ms step_avg:170.33ms\nstep:9323/10000 train_loss:3.7334 train_time:1586243ms step_avg:170.33ms\nstep:9324/10000 train_loss:3.6355 train_time:1586411ms step_avg:170.33ms\nstep:9325/10000 train_loss:3.7810 train_time:1586578ms step_avg:170.33ms\nstep:9326/10000 train_loss:3.8055 train_time:1586747ms step_avg:170.32ms\nstep:9327/10000 train_loss:3.9339 train_time:1586918ms step_avg:170.32ms\nstep:9328/10000 train_loss:3.7871 train_time:1587088ms step_avg:170.32ms\nstep:9329/10000 train_loss:3.7115 train_time:1587263ms step_avg:170.33ms\nstep:9330/10000 train_loss:3.5829 train_time:1587436ms step_avg:170.33ms\nstep:9331/10000 train_loss:3.7262 train_time:1587598ms step_avg:170.32ms\nstep:9332/10000 train_loss:3.7565 train_time:1587768ms step_avg:170.32ms\nstep:9333/10000 train_loss:3.8136 train_time:1587940ms step_avg:170.32ms\nstep:9334/10000 train_loss:3.7047 train_time:1588108ms step_avg:170.32ms\nstep:9335/10000 train_loss:3.6833 train_time:1588279ms step_avg:170.32ms\nstep:9336/10000 train_loss:3.6918 train_time:1588450ms step_avg:170.32ms\nstep:9337/10000 train_loss:3.7118 train_time:1588624ms step_avg:170.33ms\nstep:9338/10000 train_loss:3.7425 train_time:1588793ms step_avg:170.33ms\nstep:9339/10000 train_loss:4.3457 train_time:1588965ms step_avg:170.33ms\nstep:9340/10000 train_loss:3.8811 train_time:1589139ms step_avg:170.33ms\nstep:9341/10000 train_loss:3.7893 train_time:1589309ms step_avg:170.33ms\nstep:9342/10000 train_loss:3.6659 train_time:1589480ms step_avg:170.33ms\nstep:9343/10000 train_loss:3.7835 train_time:1589649ms step_avg:170.33ms\nstep:9344/10000 train_loss:3.7691 train_time:1589820ms step_avg:170.33ms\nstep:9345/10000 train_loss:3.7874 train_time:1589990ms step_avg:170.33ms\nstep:9346/10000 train_loss:3.8272 train_time:1590163ms step_avg:170.33ms\nstep:9347/10000 train_loss:3.8083 train_time:1590333ms step_avg:170.33ms\nstep:9348/10000 train_loss:3.6591 train_time:1590503ms step_avg:170.33ms\nstep:9349/10000 train_loss:3.7539 train_time:1590672ms step_avg:170.33ms\nstep:9350/10000 train_loss:3.7914 train_time:1590841ms step_avg:170.33ms\nstep:9351/10000 train_loss:3.7483 train_time:1591011ms step_avg:170.33ms\nstep:9352/10000 train_loss:3.8486 train_time:1591181ms step_avg:170.33ms\nstep:9353/10000 train_loss:3.8325 train_time:1591351ms step_avg:170.33ms\nstep:9354/10000 train_loss:3.7259 train_time:1591521ms step_avg:170.33ms\nstep:9355/10000 train_loss:3.8330 train_time:1591692ms step_avg:170.33ms\nstep:9356/10000 train_loss:4.0744 train_time:1591861ms step_avg:170.33ms\nstep:9357/10000 train_loss:3.7857 train_time:1592029ms step_avg:170.33ms\nstep:9358/10000 train_loss:3.8133 train_time:1592199ms step_avg:170.33ms\nstep:9359/10000 train_loss:3.7153 train_time:1592368ms step_avg:170.32ms\nstep:9360/10000 train_loss:3.7169 train_time:1592542ms step_avg:170.33ms\nstep:9361/10000 train_loss:3.7429 train_time:1592708ms step_avg:170.32ms\nstep:9362/10000 train_loss:3.6616 train_time:1592878ms step_avg:170.32ms\nstep:9363/10000 train_loss:3.6738 train_time:1593047ms step_avg:170.32ms\nstep:9364/10000 train_loss:3.7565 train_time:1593216ms step_avg:170.32ms\nstep:9365/10000 train_loss:3.6041 train_time:1593383ms step_avg:170.32ms\nstep:9366/10000 train_loss:3.7877 train_time:1593554ms step_avg:170.32ms\nstep:9367/10000 train_loss:3.7285 train_time:1593723ms step_avg:170.32ms\nstep:9368/10000 train_loss:3.7423 train_time:1593894ms step_avg:170.32ms\nstep:9369/10000 train_loss:4.0104 train_time:1594067ms step_avg:170.32ms\nstep:9370/10000 train_loss:3.7482 train_time:1594239ms step_avg:170.32ms\nstep:9371/10000 train_loss:3.7066 train_time:1594406ms step_avg:170.32ms\nstep:9372/10000 train_loss:3.8422 train_time:1594577ms step_avg:170.32ms\nstep:9373/10000 train_loss:3.7544 train_time:1594746ms step_avg:170.32ms\nstep:9374/10000 train_loss:3.7357 train_time:1594917ms step_avg:170.32ms\nstep:9375/10000 train_loss:3.7645 train_time:1595085ms step_avg:170.32ms\nstep:9376/10000 train_loss:3.8681 train_time:1595256ms step_avg:170.32ms\nstep:9377/10000 train_loss:3.6621 train_time:1595425ms step_avg:170.32ms\nstep:9378/10000 train_loss:3.7955 train_time:1595595ms step_avg:170.32ms\nstep:9379/10000 train_loss:3.6441 train_time:1595767ms step_avg:170.32ms\nstep:9380/10000 train_loss:3.9491 train_time:1595941ms step_avg:170.32ms\nstep:9381/10000 train_loss:3.5947 train_time:1596112ms step_avg:170.32ms\nstep:9382/10000 train_loss:3.8315 train_time:1596282ms step_avg:170.32ms\nstep:9383/10000 train_loss:3.6424 train_time:1596450ms step_avg:170.32ms\nstep:9384/10000 train_loss:3.6294 train_time:1596620ms step_avg:170.32ms\nstep:9385/10000 train_loss:3.7168 train_time:1596791ms step_avg:170.32ms\nstep:9386/10000 train_loss:3.7324 train_time:1596963ms step_avg:170.32ms\nstep:9387/10000 train_loss:3.7598 train_time:1597135ms step_avg:170.32ms\nstep:9388/10000 train_loss:3.6793 train_time:1597305ms step_avg:170.32ms\nstep:9389/10000 train_loss:3.6904 train_time:1597476ms step_avg:170.32ms\nstep:9390/10000 train_loss:3.7042 train_time:1597648ms step_avg:170.32ms\nstep:9391/10000 train_loss:3.7607 train_time:1597827ms step_avg:170.33ms\nstep:9392/10000 train_loss:3.5648 train_time:1597988ms step_avg:170.32ms\nstep:9393/10000 train_loss:4.2468 train_time:1598160ms step_avg:170.33ms\nstep:9394/10000 train_loss:3.7736 train_time:1598327ms step_avg:170.32ms\nstep:9395/10000 train_loss:3.6997 train_time:1598498ms step_avg:170.32ms\nstep:9396/10000 train_loss:3.8501 train_time:1598666ms step_avg:170.32ms\nstep:9397/10000 train_loss:3.7774 train_time:1598838ms step_avg:170.32ms\nstep:9398/10000 train_loss:3.7316 train_time:1599009ms step_avg:170.32ms\nstep:9399/10000 train_loss:3.7760 train_time:1599183ms step_avg:170.33ms\nstep:9400/10000 train_loss:3.7059 train_time:1599353ms step_avg:170.33ms\nstep:9401/10000 train_loss:3.8765 train_time:1599525ms step_avg:170.33ms\nstep:9402/10000 train_loss:3.8618 train_time:1599696ms step_avg:170.33ms\nstep:9403/10000 train_loss:3.8121 train_time:1599866ms step_avg:170.33ms\nstep:9404/10000 train_loss:3.7901 train_time:1600036ms step_avg:170.33ms\nstep:9405/10000 train_loss:3.7864 train_time:1600204ms step_avg:170.33ms\nstep:9406/10000 train_loss:3.7785 train_time:1600375ms step_avg:170.33ms\nstep:9407/10000 train_loss:3.4793 train_time:1600542ms step_avg:170.32ms\nstep:9408/10000 train_loss:3.9494 train_time:1600713ms step_avg:170.32ms\nstep:9409/10000 train_loss:3.6690 train_time:1600887ms step_avg:170.33ms\nstep:9410/10000 train_loss:3.8472 train_time:1601059ms step_avg:170.33ms\nstep:9411/10000 train_loss:3.8523 train_time:1601227ms step_avg:170.33ms\nstep:9412/10000 train_loss:3.9345 train_time:1601396ms step_avg:170.33ms\nstep:9413/10000 train_loss:3.8062 train_time:1601567ms step_avg:170.33ms\nstep:9414/10000 train_loss:3.8082 train_time:1601738ms step_avg:170.33ms\nstep:9415/10000 train_loss:3.9359 train_time:1601906ms step_avg:170.32ms\nstep:9416/10000 train_loss:3.6273 train_time:1602077ms step_avg:170.33ms\nstep:9417/10000 train_loss:4.0131 train_time:1602248ms step_avg:170.33ms\nstep:9418/10000 train_loss:3.8542 train_time:1602417ms step_avg:170.32ms\nstep:9419/10000 train_loss:3.7558 train_time:1602585ms step_avg:170.32ms\nstep:9420/10000 train_loss:3.7043 train_time:1602755ms step_avg:170.32ms\nstep:9421/10000 train_loss:3.7519 train_time:1602927ms step_avg:170.32ms\nstep:9422/10000 train_loss:3.5418 train_time:1603100ms step_avg:170.33ms\nstep:9423/10000 train_loss:3.8107 train_time:1603269ms step_avg:170.32ms\nstep:9424/10000 train_loss:3.7438 train_time:1603440ms step_avg:170.33ms\nstep:9425/10000 train_loss:3.8189 train_time:1603610ms step_avg:170.32ms\nstep:9426/10000 train_loss:3.6799 train_time:1603781ms step_avg:170.33ms\nstep:9427/10000 train_loss:3.8044 train_time:1603949ms step_avg:170.32ms\nstep:9428/10000 train_loss:3.7284 train_time:1604119ms step_avg:170.32ms\nstep:9429/10000 train_loss:3.7367 train_time:1604286ms step_avg:170.32ms\nstep:9430/10000 train_loss:3.6583 train_time:1604456ms step_avg:170.32ms\nstep:9431/10000 train_loss:3.8449 train_time:1604627ms step_avg:170.32ms\nstep:9432/10000 train_loss:3.6930 train_time:1604798ms step_avg:170.32ms\nstep:9433/10000 train_loss:3.6295 train_time:1604969ms step_avg:170.32ms\nstep:9434/10000 train_loss:3.8677 train_time:1605140ms step_avg:170.32ms\nstep:9435/10000 train_loss:3.8712 train_time:1605307ms step_avg:170.32ms\nstep:9436/10000 train_loss:3.9009 train_time:1605479ms step_avg:170.32ms\nstep:9437/10000 train_loss:3.9094 train_time:1605649ms step_avg:170.32ms\nstep:9438/10000 train_loss:3.6858 train_time:1605820ms step_avg:170.32ms\nstep:9439/10000 train_loss:3.6197 train_time:1605989ms step_avg:170.32ms\nstep:9440/10000 train_loss:4.0083 train_time:1606159ms step_avg:170.32ms\nstep:9441/10000 train_loss:3.7038 train_time:1606328ms step_avg:170.32ms\nstep:9442/10000 train_loss:3.6803 train_time:1606500ms step_avg:170.32ms\nstep:9443/10000 train_loss:3.7106 train_time:1606670ms step_avg:170.32ms\nstep:9444/10000 train_loss:3.2456 train_time:1606839ms step_avg:170.32ms\nstep:9445/10000 train_loss:3.8420 train_time:1607010ms step_avg:170.32ms\nstep:9446/10000 train_loss:3.8194 train_time:1607180ms step_avg:170.32ms\nstep:9447/10000 train_loss:3.7221 train_time:1607350ms step_avg:170.32ms\nstep:9448/10000 train_loss:4.1560 train_time:1607523ms step_avg:170.32ms\nstep:9449/10000 train_loss:3.9007 train_time:1607694ms step_avg:170.32ms\nstep:9450/10000 train_loss:3.6952 train_time:1607866ms step_avg:170.32ms\nstep:9451/10000 train_loss:3.7870 train_time:1608036ms step_avg:170.32ms\nstep:9452/10000 train_loss:4.0554 train_time:1608206ms step_avg:170.32ms\nstep:9453/10000 train_loss:3.7634 train_time:1608375ms step_avg:170.32ms\nstep:9454/10000 train_loss:3.8068 train_time:1608543ms step_avg:170.32ms\nstep:9455/10000 train_loss:3.6796 train_time:1608713ms step_avg:170.32ms\nstep:9456/10000 train_loss:3.8564 train_time:1608881ms step_avg:170.32ms\nstep:9457/10000 train_loss:3.7280 train_time:1609051ms step_avg:170.32ms\nstep:9458/10000 train_loss:3.6654 train_time:1609220ms step_avg:170.32ms\nstep:9459/10000 train_loss:3.8734 train_time:1609392ms step_avg:170.32ms\nstep:9460/10000 train_loss:3.7784 train_time:1609562ms step_avg:170.32ms\nstep:9461/10000 train_loss:3.7722 train_time:1609732ms step_avg:170.32ms\nstep:9462/10000 train_loss:3.6911 train_time:1609902ms step_avg:170.32ms\nstep:9463/10000 train_loss:3.8316 train_time:1610075ms step_avg:170.32ms\nstep:9464/10000 train_loss:3.7786 train_time:1610242ms step_avg:170.32ms\nstep:9465/10000 train_loss:3.7003 train_time:1610412ms step_avg:170.32ms\nstep:9466/10000 train_loss:3.7689 train_time:1610583ms step_avg:170.32ms\nstep:9467/10000 train_loss:3.7881 train_time:1610753ms step_avg:170.32ms\nstep:9468/10000 train_loss:3.7705 train_time:1610924ms step_avg:170.32ms\nstep:9469/10000 train_loss:3.8201 train_time:1611096ms step_avg:170.32ms\nstep:9470/10000 train_loss:3.7484 train_time:1611266ms step_avg:170.32ms\nstep:9471/10000 train_loss:3.6914 train_time:1611435ms step_avg:170.32ms\nstep:9472/10000 train_loss:3.7575 train_time:1611605ms step_avg:170.32ms\nstep:9473/10000 train_loss:3.7341 train_time:1611775ms step_avg:170.32ms\nstep:9474/10000 train_loss:3.7326 train_time:1611943ms step_avg:170.32ms\nstep:9475/10000 train_loss:3.7854 train_time:1612111ms step_avg:170.32ms\nstep:9476/10000 train_loss:3.6819 train_time:1612283ms step_avg:170.32ms\nstep:9477/10000 train_loss:3.7071 train_time:1612452ms step_avg:170.32ms\nstep:9478/10000 train_loss:4.1628 train_time:1612623ms step_avg:170.32ms\nstep:9479/10000 train_loss:3.5315 train_time:1612793ms step_avg:170.32ms\nstep:9480/10000 train_loss:3.8698 train_time:1612967ms step_avg:170.32ms\nstep:9481/10000 train_loss:3.6306 train_time:1613138ms step_avg:170.32ms\nstep:9482/10000 train_loss:3.7917 train_time:1613309ms step_avg:170.32ms\nstep:9483/10000 train_loss:3.5432 train_time:1613479ms step_avg:170.32ms\nstep:9484/10000 train_loss:3.7281 train_time:1613649ms step_avg:170.32ms\nstep:9485/10000 train_loss:3.8073 train_time:1613821ms step_avg:170.32ms\nstep:9486/10000 train_loss:3.9576 train_time:1613991ms step_avg:170.32ms\nstep:9487/10000 train_loss:3.8375 train_time:1614169ms step_avg:170.32ms\nstep:9488/10000 train_loss:3.7355 train_time:1614337ms step_avg:170.32ms\nstep:9489/10000 train_loss:3.6116 train_time:1614507ms step_avg:170.32ms\nstep:9490/10000 train_loss:3.7940 train_time:1614677ms step_avg:170.32ms\nstep:9491/10000 train_loss:4.0219 train_time:1614854ms step_avg:170.33ms\nstep:9492/10000 train_loss:4.0884 train_time:1615018ms step_avg:170.32ms\nstep:9493/10000 train_loss:3.8831 train_time:1615188ms step_avg:170.32ms\nstep:9494/10000 train_loss:3.7582 train_time:1615358ms step_avg:170.32ms\nstep:9495/10000 train_loss:3.8006 train_time:1615526ms step_avg:170.32ms\nstep:9496/10000 train_loss:3.7418 train_time:1615697ms step_avg:170.32ms\nstep:9497/10000 train_loss:3.9214 train_time:1615867ms step_avg:170.32ms\nstep:9498/10000 train_loss:3.7985 train_time:1616038ms step_avg:170.32ms\nstep:9499/10000 train_loss:3.8581 train_time:1616208ms step_avg:170.32ms\nstep:9500/10000 train_loss:3.7184 train_time:1616379ms step_avg:170.32ms\nstep:9501/10000 train_loss:3.6776 train_time:1616549ms step_avg:170.32ms\nstep:9502/10000 train_loss:3.6879 train_time:1616722ms step_avg:170.32ms\nstep:9503/10000 train_loss:3.6571 train_time:1616892ms step_avg:170.32ms\nstep:9504/10000 train_loss:3.7255 train_time:1617063ms step_avg:170.32ms\nstep:9505/10000 train_loss:3.7660 train_time:1617231ms step_avg:170.32ms\nstep:9506/10000 train_loss:3.9030 train_time:1617400ms step_avg:170.32ms\nstep:9507/10000 train_loss:3.7346 train_time:1617569ms step_avg:170.32ms\nstep:9508/10000 train_loss:3.7840 train_time:1617738ms step_avg:170.32ms\nstep:9509/10000 train_loss:3.7711 train_time:1617906ms step_avg:170.32ms\nstep:9510/10000 train_loss:3.7540 train_time:1618079ms step_avg:170.32ms\nstep:9511/10000 train_loss:3.7614 train_time:1618250ms step_avg:170.32ms\nstep:9512/10000 train_loss:3.6417 train_time:1618424ms step_avg:170.32ms\nstep:9513/10000 train_loss:3.6400 train_time:1618596ms step_avg:170.32ms\nstep:9514/10000 train_loss:4.3044 train_time:1618766ms step_avg:170.32ms\nstep:9515/10000 train_loss:3.7103 train_time:1618936ms step_avg:170.32ms\nstep:9516/10000 train_loss:3.8005 train_time:1619107ms step_avg:170.32ms\nstep:9517/10000 train_loss:3.7387 train_time:1619279ms step_avg:170.32ms\nstep:9518/10000 train_loss:3.7158 train_time:1619447ms step_avg:170.32ms\nstep:9519/10000 train_loss:3.8236 train_time:1619616ms step_avg:170.32ms\nstep:9520/10000 train_loss:3.6901 train_time:1619785ms step_avg:170.32ms\nstep:9521/10000 train_loss:3.6857 train_time:1619958ms step_avg:170.32ms\nstep:9522/10000 train_loss:3.6510 train_time:1620127ms step_avg:170.32ms\nstep:9523/10000 train_loss:3.7674 train_time:1620296ms step_avg:170.32ms\nstep:9524/10000 train_loss:3.7396 train_time:1620467ms step_avg:170.32ms\nstep:9525/10000 train_loss:3.7168 train_time:1620637ms step_avg:170.32ms\nstep:9526/10000 train_loss:3.8260 train_time:1620805ms step_avg:170.32ms\nstep:9527/10000 train_loss:3.8884 train_time:1620979ms step_avg:170.32ms\nstep:9528/10000 train_loss:3.7755 train_time:1621147ms step_avg:170.32ms\nstep:9529/10000 train_loss:3.7891 train_time:1621316ms step_avg:170.32ms\nstep:9530/10000 train_loss:3.7752 train_time:1621485ms step_avg:170.32ms\nstep:9531/10000 train_loss:3.7676 train_time:1621657ms step_avg:170.32ms\nstep:9532/10000 train_loss:4.0506 train_time:1621825ms step_avg:170.32ms\nstep:9533/10000 train_loss:3.7304 train_time:1621997ms step_avg:170.32ms\nstep:9534/10000 train_loss:3.7861 train_time:1622168ms step_avg:170.32ms\nstep:9535/10000 train_loss:3.6873 train_time:1622338ms step_avg:170.32ms\nstep:9536/10000 train_loss:3.7474 train_time:1622509ms step_avg:170.32ms\nstep:9537/10000 train_loss:3.7609 train_time:1622679ms step_avg:170.32ms\nstep:9538/10000 train_loss:3.7915 train_time:1622847ms step_avg:170.32ms\nstep:9539/10000 train_loss:3.7163 train_time:1623020ms step_avg:170.32ms\nstep:9540/10000 train_loss:3.7929 train_time:1623190ms step_avg:170.32ms\nstep:9541/10000 train_loss:3.7106 train_time:1623361ms step_avg:170.32ms\nstep:9542/10000 train_loss:3.6838 train_time:1623529ms step_avg:170.32ms\nstep:9543/10000 train_loss:3.6892 train_time:1623698ms step_avg:170.32ms\nstep:9544/10000 train_loss:3.7970 train_time:1623868ms step_avg:170.32ms\nstep:9545/10000 train_loss:3.6845 train_time:1624039ms step_avg:170.32ms\nstep:9546/10000 train_loss:3.7522 train_time:1624208ms step_avg:170.32ms\nstep:9547/10000 train_loss:3.7824 train_time:1624378ms step_avg:170.32ms\nstep:9548/10000 train_loss:3.7256 train_time:1624547ms step_avg:170.32ms\nstep:9549/10000 train_loss:3.7781 train_time:1624716ms step_avg:170.32ms\nstep:9550/10000 train_loss:3.7830 train_time:1624886ms step_avg:170.32ms\nstep:9551/10000 train_loss:3.7461 train_time:1625058ms step_avg:170.32ms\nstep:9552/10000 train_loss:3.7636 train_time:1625228ms step_avg:170.32ms\nstep:9553/10000 train_loss:3.7571 train_time:1625399ms step_avg:170.32ms\nstep:9554/10000 train_loss:3.7496 train_time:1625569ms step_avg:170.32ms\nstep:9555/10000 train_loss:3.7518 train_time:1625740ms step_avg:170.32ms\nstep:9556/10000 train_loss:3.5986 train_time:1625908ms step_avg:170.32ms\nstep:9557/10000 train_loss:3.8450 train_time:1626086ms step_avg:170.32ms\nstep:9558/10000 train_loss:3.6627 train_time:1626253ms step_avg:170.32ms\nstep:9559/10000 train_loss:3.6780 train_time:1626424ms step_avg:170.32ms\nstep:9560/10000 train_loss:3.7953 train_time:1626593ms step_avg:170.32ms\nstep:9561/10000 train_loss:3.8292 train_time:1626763ms step_avg:170.32ms\nstep:9562/10000 train_loss:3.7748 train_time:1626934ms step_avg:170.32ms\nstep:9563/10000 train_loss:3.6807 train_time:1627105ms step_avg:170.32ms\nstep:9564/10000 train_loss:3.8000 train_time:1627276ms step_avg:170.32ms\nstep:9565/10000 train_loss:3.6499 train_time:1627445ms step_avg:170.32ms\nstep:9566/10000 train_loss:3.7420 train_time:1627617ms step_avg:170.32ms\nstep:9567/10000 train_loss:3.7549 train_time:1627788ms step_avg:170.32ms\nstep:9568/10000 train_loss:3.7626 train_time:1627955ms step_avg:170.32ms\nstep:9569/10000 train_loss:3.6938 train_time:1628122ms step_avg:170.32ms\nstep:9570/10000 train_loss:3.7325 train_time:1628294ms step_avg:170.32ms\nstep:9571/10000 train_loss:3.7208 train_time:1628464ms step_avg:170.32ms\nstep:9572/10000 train_loss:3.7713 train_time:1628632ms step_avg:170.32ms\nstep:9573/10000 train_loss:3.7482 train_time:1628802ms step_avg:170.32ms\nstep:9574/10000 train_loss:3.7500 train_time:1628974ms step_avg:170.32ms\nstep:9575/10000 train_loss:3.6984 train_time:1629143ms step_avg:170.32ms\nstep:9576/10000 train_loss:3.8174 train_time:1629313ms step_avg:170.32ms\nstep:9577/10000 train_loss:3.7310 train_time:1629486ms step_avg:170.32ms\nstep:9578/10000 train_loss:3.7538 train_time:1629656ms step_avg:170.32ms\nstep:9579/10000 train_loss:3.7536 train_time:1629826ms step_avg:170.32ms\nstep:9580/10000 train_loss:3.7605 train_time:1629996ms step_avg:170.32ms\nstep:9581/10000 train_loss:3.8047 train_time:1630169ms step_avg:170.32ms\nstep:9582/10000 train_loss:3.7610 train_time:1630338ms step_avg:170.32ms\nstep:9583/10000 train_loss:3.7186 train_time:1630507ms step_avg:170.32ms\nstep:9584/10000 train_loss:3.7428 train_time:1630681ms step_avg:170.32ms\nstep:9585/10000 train_loss:3.9083 train_time:1630850ms step_avg:170.32ms\nstep:9586/10000 train_loss:3.6217 train_time:1631021ms step_avg:170.32ms\nstep:9587/10000 train_loss:3.8226 train_time:1631189ms step_avg:170.32ms\nstep:9588/10000 train_loss:3.8472 train_time:1631360ms step_avg:170.32ms\nstep:9589/10000 train_loss:3.7767 train_time:1631527ms step_avg:170.32ms\nstep:9590/10000 train_loss:3.8261 train_time:1631699ms step_avg:170.32ms\nstep:9591/10000 train_loss:3.7117 train_time:1631870ms step_avg:170.32ms\nstep:9592/10000 train_loss:3.7946 train_time:1632042ms step_avg:170.32ms\nstep:9593/10000 train_loss:3.5931 train_time:1632210ms step_avg:170.32ms\nstep:9594/10000 train_loss:3.7167 train_time:1632379ms step_avg:170.32ms\nstep:9595/10000 train_loss:3.7323 train_time:1632549ms step_avg:170.32ms\nstep:9596/10000 train_loss:3.8146 train_time:1632717ms step_avg:170.32ms\nstep:9597/10000 train_loss:3.8149 train_time:1632888ms step_avg:170.32ms\nstep:9598/10000 train_loss:3.7778 train_time:1633059ms step_avg:170.32ms\nstep:9599/10000 train_loss:3.7914 train_time:1633229ms step_avg:170.32ms\nstep:9600/10000 train_loss:3.8773 train_time:1633397ms step_avg:170.32ms\nstep:9601/10000 train_loss:3.8711 train_time:1633571ms step_avg:170.32ms\nstep:9602/10000 train_loss:3.9057 train_time:1633738ms step_avg:170.32ms\nstep:9603/10000 train_loss:3.8159 train_time:1633908ms step_avg:170.32ms\nstep:9604/10000 train_loss:3.4519 train_time:1634080ms step_avg:170.32ms\nstep:9605/10000 train_loss:3.7007 train_time:1634251ms step_avg:170.32ms\nstep:9606/10000 train_loss:3.9244 train_time:1634423ms step_avg:170.32ms\nstep:9607/10000 train_loss:3.8056 train_time:1634594ms step_avg:170.32ms\nstep:9608/10000 train_loss:3.7730 train_time:1634766ms step_avg:170.32ms\nstep:9609/10000 train_loss:3.7129 train_time:1634937ms step_avg:170.32ms\nstep:9610/10000 train_loss:3.8486 train_time:1635106ms step_avg:170.32ms\nstep:9611/10000 train_loss:3.7642 train_time:1635276ms step_avg:170.32ms\nstep:9612/10000 train_loss:3.7156 train_time:1635445ms step_avg:170.32ms\nstep:9613/10000 train_loss:3.9398 train_time:1635617ms step_avg:170.32ms\nstep:9614/10000 train_loss:3.8319 train_time:1635789ms step_avg:170.32ms\nstep:9615/10000 train_loss:3.7307 train_time:1635959ms step_avg:170.32ms\nstep:9616/10000 train_loss:3.5710 train_time:1636132ms step_avg:170.32ms\nstep:9617/10000 train_loss:3.8236 train_time:1636299ms step_avg:170.32ms\nstep:9618/10000 train_loss:3.7259 train_time:1636467ms step_avg:170.32ms\nstep:9619/10000 train_loss:3.7509 train_time:1636638ms step_avg:170.32ms\nstep:9620/10000 train_loss:3.5351 train_time:1636808ms step_avg:170.32ms\nstep:9621/10000 train_loss:3.6518 train_time:1636981ms step_avg:170.32ms\nstep:9622/10000 train_loss:3.6922 train_time:1637152ms step_avg:170.32ms\nstep:9623/10000 train_loss:3.8009 train_time:1637324ms step_avg:170.32ms\nstep:9624/10000 train_loss:3.6938 train_time:1637495ms step_avg:170.32ms\nstep:9625/10000 train_loss:3.7647 train_time:1637666ms step_avg:170.32ms\nstep:9626/10000 train_loss:3.8205 train_time:1637835ms step_avg:170.32ms\nstep:9627/10000 train_loss:3.7249 train_time:1638005ms step_avg:170.32ms\nstep:9628/10000 train_loss:3.7450 train_time:1638175ms step_avg:170.32ms\nstep:9629/10000 train_loss:3.6808 train_time:1638344ms step_avg:170.32ms\nstep:9630/10000 train_loss:3.6600 train_time:1638513ms step_avg:170.32ms\nstep:9631/10000 train_loss:3.6593 train_time:1638682ms step_avg:170.32ms\nstep:9632/10000 train_loss:3.7616 train_time:1638851ms step_avg:170.32ms\nstep:9633/10000 train_loss:3.7107 train_time:1639020ms step_avg:170.32ms\nstep:9634/10000 train_loss:3.7967 train_time:1639191ms step_avg:170.32ms\nstep:9635/10000 train_loss:3.6984 train_time:1639362ms step_avg:170.32ms\nstep:9636/10000 train_loss:3.7425 train_time:1639532ms step_avg:170.32ms\nstep:9637/10000 train_loss:3.7249 train_time:1639707ms step_avg:170.32ms\nstep:9638/10000 train_loss:3.9895 train_time:1639877ms step_avg:170.32ms\nstep:9639/10000 train_loss:3.8221 train_time:1640046ms step_avg:170.32ms\nstep:9640/10000 train_loss:3.7200 train_time:1640218ms step_avg:170.32ms\nstep:9641/10000 train_loss:3.9446 train_time:1640385ms step_avg:170.32ms\nstep:9642/10000 train_loss:3.7593 train_time:1640554ms step_avg:170.32ms\nstep:9643/10000 train_loss:3.7898 train_time:1640721ms step_avg:170.32ms\nstep:9644/10000 train_loss:3.7376 train_time:1640891ms step_avg:170.32ms\nstep:9645/10000 train_loss:3.7167 train_time:1641064ms step_avg:170.32ms\nstep:9646/10000 train_loss:3.8650 train_time:1641236ms step_avg:170.32ms\nstep:9647/10000 train_loss:3.7011 train_time:1641407ms step_avg:170.32ms\nstep:9648/10000 train_loss:3.8934 train_time:1641578ms step_avg:170.32ms\nstep:9649/10000 train_loss:3.7800 train_time:1641745ms step_avg:170.32ms\nstep:9650/10000 train_loss:3.7347 train_time:1641916ms step_avg:170.32ms\nstep:9651/10000 train_loss:3.6249 train_time:1642087ms step_avg:170.32ms\nstep:9652/10000 train_loss:3.7329 train_time:1642258ms step_avg:170.32ms\nstep:9653/10000 train_loss:3.5892 train_time:1642428ms step_avg:170.32ms\nstep:9654/10000 train_loss:3.6967 train_time:1642600ms step_avg:170.32ms\nstep:9655/10000 train_loss:3.7417 train_time:1642769ms step_avg:170.32ms\nstep:9656/10000 train_loss:3.8281 train_time:1642942ms step_avg:170.32ms\nstep:9657/10000 train_loss:3.7115 train_time:1643112ms step_avg:170.32ms\nstep:9658/10000 train_loss:3.8010 train_time:1643283ms step_avg:170.32ms\nstep:9659/10000 train_loss:3.7300 train_time:1643452ms step_avg:170.32ms\nstep:9660/10000 train_loss:3.6680 train_time:1643622ms step_avg:170.32ms\nstep:9661/10000 train_loss:3.6498 train_time:1643795ms step_avg:170.32ms\nstep:9662/10000 train_loss:3.6304 train_time:1643967ms step_avg:170.32ms\nstep:9663/10000 train_loss:3.7995 train_time:1644141ms step_avg:170.32ms\nstep:9664/10000 train_loss:3.9498 train_time:1644310ms step_avg:170.32ms\nstep:9665/10000 train_loss:3.7442 train_time:1644488ms step_avg:170.33ms\nstep:9666/10000 train_loss:4.0145 train_time:1644651ms step_avg:170.32ms\nstep:9667/10000 train_loss:3.7417 train_time:1644829ms step_avg:170.33ms\nstep:9668/10000 train_loss:3.7735 train_time:1644996ms step_avg:170.32ms\nstep:9669/10000 train_loss:3.7899 train_time:1645165ms step_avg:170.32ms\nstep:9670/10000 train_loss:3.7027 train_time:1645334ms step_avg:170.32ms\nstep:9671/10000 train_loss:3.5599 train_time:1645506ms step_avg:170.32ms\nstep:9672/10000 train_loss:3.7993 train_time:1645677ms step_avg:170.32ms\nstep:9673/10000 train_loss:3.6414 train_time:1645846ms step_avg:170.32ms\nstep:9674/10000 train_loss:3.7677 train_time:1646017ms step_avg:170.32ms\nstep:9675/10000 train_loss:3.6981 train_time:1646190ms step_avg:170.32ms\nstep:9676/10000 train_loss:3.6986 train_time:1646360ms step_avg:170.32ms\nstep:9677/10000 train_loss:3.8247 train_time:1646531ms step_avg:170.32ms\nstep:9678/10000 train_loss:3.7137 train_time:1646704ms step_avg:170.33ms\nstep:9679/10000 train_loss:3.7986 train_time:1646873ms step_avg:170.33ms\nstep:9680/10000 train_loss:3.9041 train_time:1647045ms step_avg:170.33ms\nstep:9681/10000 train_loss:4.1586 train_time:1647215ms step_avg:170.33ms\nstep:9682/10000 train_loss:3.7764 train_time:1647392ms step_avg:170.33ms\nstep:9683/10000 train_loss:3.7962 train_time:1647561ms step_avg:170.33ms\nstep:9684/10000 train_loss:3.7622 train_time:1647731ms step_avg:170.33ms\nstep:9685/10000 train_loss:3.8081 train_time:1647899ms step_avg:170.33ms\nstep:9686/10000 train_loss:3.6290 train_time:1648063ms step_avg:170.32ms\nstep:9687/10000 train_loss:3.8247 train_time:1648234ms step_avg:170.32ms\nstep:9688/10000 train_loss:3.8240 train_time:1648404ms step_avg:170.32ms\nstep:9689/10000 train_loss:3.6964 train_time:1648574ms step_avg:170.32ms\nstep:9690/10000 train_loss:3.5651 train_time:1648742ms step_avg:170.32ms\nstep:9691/10000 train_loss:3.6807 train_time:1648913ms step_avg:170.32ms\nstep:9692/10000 train_loss:3.7698 train_time:1649087ms step_avg:170.32ms\nstep:9693/10000 train_loss:3.8240 train_time:1649260ms step_avg:170.33ms\nstep:9694/10000 train_loss:3.8059 train_time:1649429ms step_avg:170.33ms\nstep:9695/10000 train_loss:3.6421 train_time:1649606ms step_avg:170.33ms\nstep:9696/10000 train_loss:3.6697 train_time:1649776ms step_avg:170.33ms\nstep:9697/10000 train_loss:3.7997 train_time:1649944ms step_avg:170.33ms\nstep:9698/10000 train_loss:3.6961 train_time:1650108ms step_avg:170.32ms\nstep:9699/10000 train_loss:3.8639 train_time:1650279ms step_avg:170.33ms\nstep:9700/10000 train_loss:3.7855 train_time:1650450ms step_avg:170.33ms\nstep:9701/10000 train_loss:3.7840 train_time:1650622ms step_avg:170.33ms\nstep:9702/10000 train_loss:3.7584 train_time:1650790ms step_avg:170.33ms\nstep:9703/10000 train_loss:3.6804 train_time:1650963ms step_avg:170.33ms\nstep:9704/10000 train_loss:3.7028 train_time:1651132ms step_avg:170.33ms\nstep:9705/10000 train_loss:3.7231 train_time:1651303ms step_avg:170.33ms\nstep:9706/10000 train_loss:3.8243 train_time:1651474ms step_avg:170.33ms\nstep:9707/10000 train_loss:3.8325 train_time:1651645ms step_avg:170.33ms\nstep:9708/10000 train_loss:4.2369 train_time:1651814ms step_avg:170.33ms\nstep:9709/10000 train_loss:3.9034 train_time:1651985ms step_avg:170.33ms\nstep:9710/10000 train_loss:3.8992 train_time:1652154ms step_avg:170.33ms\nstep:9711/10000 train_loss:3.7266 train_time:1652326ms step_avg:170.33ms\nstep:9712/10000 train_loss:3.6187 train_time:1652496ms step_avg:170.33ms\nstep:9713/10000 train_loss:4.4104 train_time:1652667ms step_avg:170.33ms\nstep:9714/10000 train_loss:3.7105 train_time:1652837ms step_avg:170.33ms\nstep:9715/10000 train_loss:3.7830 train_time:1653008ms step_avg:170.33ms\nstep:9716/10000 train_loss:3.8365 train_time:1653179ms step_avg:170.33ms\nstep:9717/10000 train_loss:3.8824 train_time:1653349ms step_avg:170.33ms\nstep:9718/10000 train_loss:3.7930 train_time:1653519ms step_avg:170.33ms\nstep:9719/10000 train_loss:3.7667 train_time:1653691ms step_avg:170.33ms\nstep:9720/10000 train_loss:3.8725 train_time:1653861ms step_avg:170.33ms\nstep:9721/10000 train_loss:3.8257 train_time:1654030ms step_avg:170.33ms\nstep:9722/10000 train_loss:3.7931 train_time:1654201ms step_avg:170.33ms\nstep:9723/10000 train_loss:3.7169 train_time:1654369ms step_avg:170.33ms\nstep:9724/10000 train_loss:3.6130 train_time:1654541ms step_avg:170.33ms\nstep:9725/10000 train_loss:3.8209 train_time:1654711ms step_avg:170.33ms\nstep:9726/10000 train_loss:3.7374 train_time:1654883ms step_avg:170.33ms\nstep:9727/10000 train_loss:3.6588 train_time:1655051ms step_avg:170.33ms\nstep:9728/10000 train_loss:3.8033 train_time:1655223ms step_avg:170.33ms\nstep:9729/10000 train_loss:3.6687 train_time:1655395ms step_avg:170.33ms\nstep:9730/10000 train_loss:3.6825 train_time:1655564ms step_avg:170.33ms\nstep:9731/10000 train_loss:3.7216 train_time:1655733ms step_avg:170.33ms\nstep:9732/10000 train_loss:4.2061 train_time:1655906ms step_avg:170.33ms\nstep:9733/10000 train_loss:3.8607 train_time:1656076ms step_avg:170.33ms\nstep:9734/10000 train_loss:3.7819 train_time:1656246ms step_avg:170.33ms\nstep:9735/10000 train_loss:3.6685 train_time:1656419ms step_avg:170.33ms\nstep:9736/10000 train_loss:3.6927 train_time:1656589ms step_avg:170.33ms\nstep:9737/10000 train_loss:3.7133 train_time:1656761ms step_avg:170.33ms\nstep:9738/10000 train_loss:3.7188 train_time:1656929ms step_avg:170.33ms\nstep:9739/10000 train_loss:3.7683 train_time:1657100ms step_avg:170.33ms\nstep:9740/10000 train_loss:3.7264 train_time:1657269ms step_avg:170.33ms\nstep:9741/10000 train_loss:3.6448 train_time:1657439ms step_avg:170.33ms\nstep:9742/10000 train_loss:3.7514 train_time:1657613ms step_avg:170.33ms\nstep:9743/10000 train_loss:3.7425 train_time:1657782ms step_avg:170.33ms\nstep:9744/10000 train_loss:3.7619 train_time:1657955ms step_avg:170.33ms\nstep:9745/10000 train_loss:3.8258 train_time:1658124ms step_avg:170.33ms\nstep:9746/10000 train_loss:3.8013 train_time:1658294ms step_avg:170.33ms\nstep:9747/10000 train_loss:3.7122 train_time:1658464ms step_avg:170.33ms\nstep:9748/10000 train_loss:4.2772 train_time:1658636ms step_avg:170.33ms\nstep:9749/10000 train_loss:3.9295 train_time:1658805ms step_avg:170.33ms\nstep:9750/10000 train_loss:3.7586 train_time:1658976ms step_avg:170.33ms\nstep:9751/10000 train_loss:3.7227 train_time:1659145ms step_avg:170.33ms\nstep:9752/10000 train_loss:3.6360 train_time:1659317ms step_avg:170.33ms\nstep:9753/10000 train_loss:3.9591 train_time:1659485ms step_avg:170.33ms\nstep:9754/10000 train_loss:3.6584 train_time:1659657ms step_avg:170.33ms\nstep:9755/10000 train_loss:3.6103 train_time:1659826ms step_avg:170.33ms\nstep:9756/10000 train_loss:3.7680 train_time:1659995ms step_avg:170.33ms\nstep:9757/10000 train_loss:3.8573 train_time:1660164ms step_avg:170.33ms\nstep:9758/10000 train_loss:3.6215 train_time:1660335ms step_avg:170.33ms\nstep:9759/10000 train_loss:3.7706 train_time:1660503ms step_avg:170.33ms\nstep:9760/10000 train_loss:3.8008 train_time:1660671ms step_avg:170.33ms\nstep:9761/10000 train_loss:3.7787 train_time:1660841ms step_avg:170.33ms\nstep:9762/10000 train_loss:3.7631 train_time:1661010ms step_avg:170.33ms\nstep:9763/10000 train_loss:3.8275 train_time:1661182ms step_avg:170.33ms\nstep:9764/10000 train_loss:3.7636 train_time:1661351ms step_avg:170.33ms\nstep:9765/10000 train_loss:3.6589 train_time:1661521ms step_avg:170.33ms\nstep:9766/10000 train_loss:3.6759 train_time:1661693ms step_avg:170.33ms\nstep:9767/10000 train_loss:3.7571 train_time:1661866ms step_avg:170.33ms\nstep:9768/10000 train_loss:3.7361 train_time:1662037ms step_avg:170.33ms\nstep:9769/10000 train_loss:3.8864 train_time:1662207ms step_avg:170.33ms\nstep:9770/10000 train_loss:3.7124 train_time:1662377ms step_avg:170.33ms\nstep:9771/10000 train_loss:3.7357 train_time:1662544ms step_avg:170.33ms\nstep:9772/10000 train_loss:3.7342 train_time:1662713ms step_avg:170.33ms\nstep:9773/10000 train_loss:3.8030 train_time:1662883ms step_avg:170.33ms\nstep:9774/10000 train_loss:3.7735 train_time:1663053ms step_avg:170.32ms\nstep:9775/10000 train_loss:3.6961 train_time:1663227ms step_avg:170.33ms\nstep:9776/10000 train_loss:3.6926 train_time:1663400ms step_avg:170.33ms\nstep:9777/10000 train_loss:3.6498 train_time:1663568ms step_avg:170.33ms\nstep:9778/10000 train_loss:3.7035 train_time:1663737ms step_avg:170.33ms\nstep:9779/10000 train_loss:3.8897 train_time:1663907ms step_avg:170.33ms\nstep:9780/10000 train_loss:3.7982 train_time:1664076ms step_avg:170.33ms\nstep:9781/10000 train_loss:3.6250 train_time:1664244ms step_avg:170.32ms\nstep:9782/10000 train_loss:3.8087 train_time:1664417ms step_avg:170.33ms\nstep:9783/10000 train_loss:3.7141 train_time:1664586ms step_avg:170.32ms\nstep:9784/10000 train_loss:3.6780 train_time:1664756ms step_avg:170.32ms\nstep:9785/10000 train_loss:3.9026 train_time:1664924ms step_avg:170.32ms\nstep:9786/10000 train_loss:3.6906 train_time:1665095ms step_avg:170.32ms\nstep:9787/10000 train_loss:3.6726 train_time:1665266ms step_avg:170.32ms\nstep:9788/10000 train_loss:3.7391 train_time:1665439ms step_avg:170.33ms\nstep:9789/10000 train_loss:3.7208 train_time:1665609ms step_avg:170.33ms\nstep:9790/10000 train_loss:3.7703 train_time:1665781ms step_avg:170.33ms\nstep:9791/10000 train_loss:3.7837 train_time:1665950ms step_avg:170.33ms\nstep:9792/10000 train_loss:3.8541 train_time:1666120ms step_avg:170.33ms\nstep:9793/10000 train_loss:3.6885 train_time:1666289ms step_avg:170.32ms\nstep:9794/10000 train_loss:3.6269 train_time:1666461ms step_avg:170.33ms\nstep:9795/10000 train_loss:3.7720 train_time:1666632ms step_avg:170.33ms\nstep:9796/10000 train_loss:3.8345 train_time:1666801ms step_avg:170.33ms\nstep:9797/10000 train_loss:3.6704 train_time:1666971ms step_avg:170.33ms\nstep:9798/10000 train_loss:3.6337 train_time:1667144ms step_avg:170.33ms\nstep:9799/10000 train_loss:3.6218 train_time:1667318ms step_avg:170.33ms\nstep:9800/10000 train_loss:3.7257 train_time:1667486ms step_avg:170.33ms\nstep:9801/10000 train_loss:3.6569 train_time:1667657ms step_avg:170.33ms\nstep:9802/10000 train_loss:3.8276 train_time:1667827ms step_avg:170.33ms\nstep:9803/10000 train_loss:3.7830 train_time:1667998ms step_avg:170.33ms\nstep:9804/10000 train_loss:3.8652 train_time:1668165ms step_avg:170.33ms\nstep:9805/10000 train_loss:3.8455 train_time:1668336ms step_avg:170.33ms\nstep:9806/10000 train_loss:3.6457 train_time:1668503ms step_avg:170.32ms\nstep:9807/10000 train_loss:3.7323 train_time:1668676ms step_avg:170.33ms\nstep:9808/10000 train_loss:3.8050 train_time:1668845ms step_avg:170.33ms\nstep:9809/10000 train_loss:3.7900 train_time:1669016ms step_avg:170.33ms\nstep:9810/10000 train_loss:3.7272 train_time:1669185ms step_avg:170.32ms\nstep:9811/10000 train_loss:3.7235 train_time:1669359ms step_avg:170.33ms\nstep:9812/10000 train_loss:3.7825 train_time:1669530ms step_avg:170.33ms\nstep:9813/10000 train_loss:3.7397 train_time:1669701ms step_avg:170.33ms\nstep:9814/10000 train_loss:3.8950 train_time:1669871ms step_avg:170.33ms\nstep:9815/10000 train_loss:3.7483 train_time:1670043ms step_avg:170.33ms\nstep:9816/10000 train_loss:3.8168 train_time:1670215ms step_avg:170.33ms\nstep:9817/10000 train_loss:3.8698 train_time:1670384ms step_avg:170.33ms\nstep:9818/10000 train_loss:3.7225 train_time:1670553ms step_avg:170.33ms\nstep:9819/10000 train_loss:3.8338 train_time:1670722ms step_avg:170.33ms\nstep:9820/10000 train_loss:3.7075 train_time:1670892ms step_avg:170.33ms\nstep:9821/10000 train_loss:3.8415 train_time:1671061ms step_avg:170.33ms\nstep:9822/10000 train_loss:3.8051 train_time:1671233ms step_avg:170.33ms\nstep:9823/10000 train_loss:3.7548 train_time:1671404ms step_avg:170.33ms\nstep:9824/10000 train_loss:3.7018 train_time:1671574ms step_avg:170.33ms\nstep:9825/10000 train_loss:3.7389 train_time:1671745ms step_avg:170.33ms\nstep:9826/10000 train_loss:3.7042 train_time:1671917ms step_avg:170.33ms\nstep:9827/10000 train_loss:3.7650 train_time:1672084ms step_avg:170.33ms\nstep:9828/10000 train_loss:3.8852 train_time:1672254ms step_avg:170.33ms\nstep:9829/10000 train_loss:3.6740 train_time:1672425ms step_avg:170.33ms\nstep:9830/10000 train_loss:3.8009 train_time:1672596ms step_avg:170.33ms\nstep:9831/10000 train_loss:3.8452 train_time:1672765ms step_avg:170.33ms\nstep:9832/10000 train_loss:3.7053 train_time:1672936ms step_avg:170.33ms\nstep:9833/10000 train_loss:3.9780 train_time:1673106ms step_avg:170.33ms\nstep:9834/10000 train_loss:3.6968 train_time:1673277ms step_avg:170.33ms\nstep:9835/10000 train_loss:3.7958 train_time:1673446ms step_avg:170.33ms\nstep:9836/10000 train_loss:3.5433 train_time:1673616ms step_avg:170.33ms\nstep:9837/10000 train_loss:3.7387 train_time:1673785ms step_avg:170.33ms\nstep:9838/10000 train_loss:3.8450 train_time:1673956ms step_avg:170.33ms\nstep:9839/10000 train_loss:3.8675 train_time:1674124ms step_avg:170.32ms\nstep:9840/10000 train_loss:3.7575 train_time:1674294ms step_avg:170.32ms\nstep:9841/10000 train_loss:3.7451 train_time:1674464ms step_avg:170.32ms\nstep:9842/10000 train_loss:3.7544 train_time:1674633ms step_avg:170.32ms\nstep:9843/10000 train_loss:3.8548 train_time:1674807ms step_avg:170.33ms\nstep:9844/10000 train_loss:3.7210 train_time:1674981ms step_avg:170.33ms\nstep:9845/10000 train_loss:4.2411 train_time:1675153ms step_avg:170.33ms\nstep:9846/10000 train_loss:3.6803 train_time:1675324ms step_avg:170.33ms\nstep:9847/10000 train_loss:3.6726 train_time:1675495ms step_avg:170.33ms\nstep:9848/10000 train_loss:3.7075 train_time:1675665ms step_avg:170.33ms\nstep:9849/10000 train_loss:3.7930 train_time:1675835ms step_avg:170.33ms\nstep:9850/10000 train_loss:3.6605 train_time:1676008ms step_avg:170.33ms\nstep:9851/10000 train_loss:3.7296 train_time:1676179ms step_avg:170.33ms\nstep:9852/10000 train_loss:3.8839 train_time:1676347ms step_avg:170.33ms\nstep:9853/10000 train_loss:3.6612 train_time:1676517ms step_avg:170.33ms\nstep:9854/10000 train_loss:3.7306 train_time:1676685ms step_avg:170.33ms\nstep:9855/10000 train_loss:3.6941 train_time:1676856ms step_avg:170.33ms\nstep:9856/10000 train_loss:4.6412 train_time:1677026ms step_avg:170.33ms\nstep:9857/10000 train_loss:3.9163 train_time:1677198ms step_avg:170.33ms\nstep:9858/10000 train_loss:4.2956 train_time:1677369ms step_avg:170.33ms\nstep:9859/10000 train_loss:3.8141 train_time:1677543ms step_avg:170.33ms\nstep:9860/10000 train_loss:3.8446 train_time:1677711ms step_avg:170.33ms\nstep:9861/10000 train_loss:3.9463 train_time:1677882ms step_avg:170.33ms\nstep:9862/10000 train_loss:3.8089 train_time:1678052ms step_avg:170.33ms\nstep:9863/10000 train_loss:3.7690 train_time:1678224ms step_avg:170.33ms\nstep:9864/10000 train_loss:3.7505 train_time:1678395ms step_avg:170.33ms\nstep:9865/10000 train_loss:3.7733 train_time:1678565ms step_avg:170.33ms\nstep:9866/10000 train_loss:3.6322 train_time:1678734ms step_avg:170.33ms\nstep:9867/10000 train_loss:3.7174 train_time:1678906ms step_avg:170.33ms\nstep:9868/10000 train_loss:3.7050 train_time:1679077ms step_avg:170.33ms\nstep:9869/10000 train_loss:3.6510 train_time:1679246ms step_avg:170.33ms\nstep:9870/10000 train_loss:3.8047 train_time:1679413ms step_avg:170.33ms\nstep:9871/10000 train_loss:3.7447 train_time:1679586ms step_avg:170.33ms\nstep:9872/10000 train_loss:3.7840 train_time:1679757ms step_avg:170.33ms\nstep:9873/10000 train_loss:4.0111 train_time:1679926ms step_avg:170.33ms\nstep:9874/10000 train_loss:3.7468 train_time:1680099ms step_avg:170.33ms\nstep:9875/10000 train_loss:3.8251 train_time:1680267ms step_avg:170.33ms\nstep:9876/10000 train_loss:3.7280 train_time:1680436ms step_avg:170.33ms\nstep:9877/10000 train_loss:3.7259 train_time:1680608ms step_avg:170.33ms\nstep:9878/10000 train_loss:3.9216 train_time:1680785ms step_avg:170.33ms\nstep:9879/10000 train_loss:3.8228 train_time:1680953ms step_avg:170.33ms\nstep:9880/10000 train_loss:4.0548 train_time:1681121ms step_avg:170.33ms\nstep:9881/10000 train_loss:3.7540 train_time:1681290ms step_avg:170.33ms\nstep:9882/10000 train_loss:4.1932 train_time:1681460ms step_avg:170.33ms\nstep:9883/10000 train_loss:3.8094 train_time:1681631ms step_avg:170.33ms\nstep:9884/10000 train_loss:3.6518 train_time:1681802ms step_avg:170.33ms\nstep:9885/10000 train_loss:3.6038 train_time:1681973ms step_avg:170.33ms\nstep:9886/10000 train_loss:3.8548 train_time:1682145ms step_avg:170.33ms\nstep:9887/10000 train_loss:3.7394 train_time:1682316ms step_avg:170.33ms\nstep:9888/10000 train_loss:3.8088 train_time:1682484ms step_avg:170.33ms\nstep:9889/10000 train_loss:3.8042 train_time:1682655ms step_avg:170.33ms\nstep:9890/10000 train_loss:3.7070 train_time:1682823ms step_avg:170.33ms\nstep:9891/10000 train_loss:3.7591 train_time:1682993ms step_avg:170.33ms\nstep:9892/10000 train_loss:3.6877 train_time:1683164ms step_avg:170.33ms\nstep:9893/10000 train_loss:3.5998 train_time:1683335ms step_avg:170.33ms\nstep:9894/10000 train_loss:3.8150 train_time:1683503ms step_avg:170.33ms\nstep:9895/10000 train_loss:3.7209 train_time:1683672ms step_avg:170.33ms\nstep:9896/10000 train_loss:3.8217 train_time:1683842ms step_avg:170.33ms\nstep:9897/10000 train_loss:3.7088 train_time:1684012ms step_avg:170.33ms\nstep:9898/10000 train_loss:3.8589 train_time:1684183ms step_avg:170.33ms\nstep:9899/10000 train_loss:3.7865 train_time:1684351ms step_avg:170.33ms\nstep:9900/10000 train_loss:3.8547 train_time:1684522ms step_avg:170.33ms\nstep:9901/10000 train_loss:3.7867 train_time:1684693ms step_avg:170.33ms\nstep:9902/10000 train_loss:3.6528 train_time:1684864ms step_avg:170.33ms\nstep:9903/10000 train_loss:3.8190 train_time:1685032ms step_avg:170.33ms\nstep:9904/10000 train_loss:3.8344 train_time:1685203ms step_avg:170.33ms\nstep:9905/10000 train_loss:3.7166 train_time:1685374ms step_avg:170.33ms\nstep:9906/10000 train_loss:3.7895 train_time:1685752ms step_avg:170.35ms\nstep:9907/10000 train_loss:3.7718 train_time:1685917ms step_avg:170.35ms\nstep:9908/10000 train_loss:3.7473 train_time:1686086ms step_avg:170.35ms\nstep:9909/10000 train_loss:3.7430 train_time:1686255ms step_avg:170.35ms\nstep:9910/10000 train_loss:3.7663 train_time:1686423ms step_avg:170.35ms\nstep:9911/10000 train_loss:3.8311 train_time:1686592ms step_avg:170.35ms\nstep:9912/10000 train_loss:3.7280 train_time:1686776ms step_avg:170.35ms\nstep:9913/10000 train_loss:3.7116 train_time:1686946ms step_avg:170.35ms\nstep:9914/10000 train_loss:3.7187 train_time:1687115ms step_avg:170.35ms\nstep:9915/10000 train_loss:3.8091 train_time:1687285ms step_avg:170.35ms\nstep:9916/10000 train_loss:3.8833 train_time:1687455ms step_avg:170.35ms\nstep:9917/10000 train_loss:3.8340 train_time:1687625ms step_avg:170.35ms\nstep:9918/10000 train_loss:3.6558 train_time:1687799ms step_avg:170.35ms\nstep:9919/10000 train_loss:3.7227 train_time:1687968ms step_avg:170.35ms\nstep:9920/10000 train_loss:3.9830 train_time:1688139ms step_avg:170.35ms\nstep:9921/10000 train_loss:3.7289 train_time:1688307ms step_avg:170.35ms\nstep:9922/10000 train_loss:3.7439 train_time:1688477ms step_avg:170.35ms\nstep:9923/10000 train_loss:3.7743 train_time:1688648ms step_avg:170.35ms\nstep:9924/10000 train_loss:3.6545 train_time:1688822ms step_avg:170.35ms\nstep:9925/10000 train_loss:3.6880 train_time:1688992ms step_avg:170.35ms\nstep:9926/10000 train_loss:3.7294 train_time:1689164ms step_avg:170.35ms\nstep:9927/10000 train_loss:3.7543 train_time:1689334ms step_avg:170.35ms\nstep:9928/10000 train_loss:3.7985 train_time:1689503ms step_avg:170.35ms\nstep:9929/10000 train_loss:3.6957 train_time:1689674ms step_avg:170.35ms\nstep:9930/10000 train_loss:3.7896 train_time:1689846ms step_avg:170.35ms\nstep:9931/10000 train_loss:3.6983 train_time:1690018ms step_avg:170.35ms\nstep:9932/10000 train_loss:3.7758 train_time:1690185ms step_avg:170.35ms\nstep:9933/10000 train_loss:3.9020 train_time:1690354ms step_avg:170.35ms\nstep:9934/10000 train_loss:3.7334 train_time:1690523ms step_avg:170.35ms\nstep:9935/10000 train_loss:3.7107 train_time:1690692ms step_avg:170.35ms\nstep:9936/10000 train_loss:3.6314 train_time:1690864ms step_avg:170.35ms\nstep:9937/10000 train_loss:3.7688 train_time:1691033ms step_avg:170.35ms\nstep:9938/10000 train_loss:3.8657 train_time:1691202ms step_avg:170.35ms\nstep:9939/10000 train_loss:3.8102 train_time:1691371ms step_avg:170.35ms\nstep:9940/10000 train_loss:3.5896 train_time:1691538ms step_avg:170.35ms\nstep:9941/10000 train_loss:3.9838 train_time:1691707ms step_avg:170.35ms\nstep:9942/10000 train_loss:3.7822 train_time:1691880ms step_avg:170.35ms\nstep:9943/10000 train_loss:3.8050 train_time:1692049ms step_avg:170.35ms\nstep:9944/10000 train_loss:3.7592 train_time:1692221ms step_avg:170.35ms\nstep:9945/10000 train_loss:3.8450 train_time:1692391ms step_avg:170.35ms\nstep:9946/10000 train_loss:3.7811 train_time:1692560ms step_avg:170.35ms\nstep:9947/10000 train_loss:3.7589 train_time:1692729ms step_avg:170.35ms\nstep:9948/10000 train_loss:3.7848 train_time:1692901ms step_avg:170.35ms\nstep:9949/10000 train_loss:3.6981 train_time:1693072ms step_avg:170.35ms\nstep:9950/10000 train_loss:3.8924 train_time:1693241ms step_avg:170.35ms\nstep:9951/10000 train_loss:3.9503 train_time:1693408ms step_avg:170.35ms\nstep:9952/10000 train_loss:3.7777 train_time:1693578ms step_avg:170.35ms\nstep:9953/10000 train_loss:3.7072 train_time:1693746ms step_avg:170.35ms\nstep:9954/10000 train_loss:3.4322 train_time:1693919ms step_avg:170.35ms\nstep:9955/10000 train_loss:3.8317 train_time:1694089ms step_avg:170.35ms\nstep:9956/10000 train_loss:3.7134 train_time:1694261ms step_avg:170.35ms\nstep:9957/10000 train_loss:3.8188 train_time:1694433ms step_avg:170.35ms\nstep:9958/10000 train_loss:3.7148 train_time:1694602ms step_avg:170.35ms\nstep:9959/10000 train_loss:3.7386 train_time:1694772ms step_avg:170.35ms\nstep:9960/10000 train_loss:3.7600 train_time:1694942ms step_avg:170.35ms\nstep:9961/10000 train_loss:3.8426 train_time:1695111ms step_avg:170.35ms\nstep:9962/10000 train_loss:3.8211 train_time:1695282ms step_avg:170.35ms\nstep:9963/10000 train_loss:3.8648 train_time:1695452ms step_avg:170.35ms\nstep:9964/10000 train_loss:3.6470 train_time:1695622ms step_avg:170.35ms\nstep:9965/10000 train_loss:4.2323 train_time:1695792ms step_avg:170.35ms\nstep:9966/10000 train_loss:3.7622 train_time:1695963ms step_avg:170.35ms\nstep:9967/10000 train_loss:3.8018 train_time:1696137ms step_avg:170.35ms\nstep:9968/10000 train_loss:3.8100 train_time:1696307ms step_avg:170.35ms\nstep:9969/10000 train_loss:3.7668 train_time:1696481ms step_avg:170.35ms\nstep:9970/10000 train_loss:3.7633 train_time:1696651ms step_avg:170.35ms\nstep:9971/10000 train_loss:3.7342 train_time:1696820ms step_avg:170.35ms\nstep:9972/10000 train_loss:3.7988 train_time:1696990ms step_avg:170.35ms\nstep:9973/10000 train_loss:3.8611 train_time:1697164ms step_avg:170.35ms\nstep:9974/10000 train_loss:3.8090 train_time:1697334ms step_avg:170.35ms\nstep:9975/10000 train_loss:3.6823 train_time:1697505ms step_avg:170.35ms\nstep:9976/10000 train_loss:3.7143 train_time:1697678ms step_avg:170.35ms\nstep:9977/10000 train_loss:3.8442 train_time:1697846ms step_avg:170.35ms\nstep:9978/10000 train_loss:3.8396 train_time:1698016ms step_avg:170.35ms\nstep:9979/10000 train_loss:3.6383 train_time:1698185ms step_avg:170.35ms\nstep:9980/10000 train_loss:3.7853 train_time:1698356ms step_avg:170.35ms\nstep:9981/10000 train_loss:3.8430 train_time:1698525ms step_avg:170.35ms\nstep:9982/10000 train_loss:3.8469 train_time:1698697ms step_avg:170.35ms\nstep:9983/10000 train_loss:3.7074 train_time:1698866ms step_avg:170.35ms\nstep:9984/10000 train_loss:3.8505 train_time:1699036ms step_avg:170.35ms\nstep:9985/10000 train_loss:3.8038 train_time:1699206ms step_avg:170.35ms\nstep:9986/10000 train_loss:3.7642 train_time:1699379ms step_avg:170.35ms\nstep:9987/10000 train_loss:3.8177 train_time:1699548ms step_avg:170.35ms\nstep:9988/10000 train_loss:3.9404 train_time:1699718ms step_avg:170.35ms\nstep:9989/10000 train_loss:3.7990 train_time:1699890ms step_avg:170.35ms\nstep:9990/10000 train_loss:3.8310 train_time:1700065ms step_avg:170.35ms\nstep:9991/10000 train_loss:3.8574 train_time:1700235ms step_avg:170.35ms\nstep:9992/10000 train_loss:3.7876 train_time:1700407ms step_avg:170.35ms\nstep:9993/10000 train_loss:3.9443 train_time:1700576ms step_avg:170.35ms\nstep:9994/10000 train_loss:3.5581 train_time:1700747ms step_avg:170.35ms\nstep:9995/10000 train_loss:3.6214 train_time:1700916ms step_avg:170.35ms\nstep:9996/10000 train_loss:3.7184 train_time:1701087ms step_avg:170.35ms\nstep:9997/10000 train_loss:3.7943 train_time:1701257ms step_avg:170.35ms\nstep:9998/10000 train_loss:3.6423 train_time:1701424ms step_avg:170.35ms\nstep:9999/10000 train_loss:3.7478 train_time:1701597ms step_avg:170.35ms\nstep:10000/10000 train_loss:3.8501 train_time:1701766ms step_avg:170.35ms\npeak memory consumption: 37806 MiB\n(Current Step: 23, Remaining Steps: 27)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    }
  ],
  "info": {
    "model_stats": {
      "total_cost": 1.84932,
      "task_cost": 1.84932,
      "tokens_sent": 581312,
      "tokens_received": 8782,
      "api_calls": 23
    },
    "exit_status": "",
    "submission": null,
    "score": [],
    "summarizer": {}
  }
}