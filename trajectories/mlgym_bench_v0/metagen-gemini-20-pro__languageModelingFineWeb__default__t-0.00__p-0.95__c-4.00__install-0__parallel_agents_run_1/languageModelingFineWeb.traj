{
  "environment": "mlgym_main",
  "trajectory": [
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, let's start by exploring the workspace to see what files we have. This will help us understand the structure, identify any existing scripts (like a baseline or evaluation script), and locate the data.\n```bash\nls -a\n```",
      "thought": "DISCUSSION\nOkay, let's start by exploring the workspace to see what files we have. This will help us understand the structure, identify any existing scripts (like a baseline or evaluation script), and locate the data.\n",
      "action": "ls -a\n",
      "execution_time": 0.3007160406559706,
      "observation": "./\n../\nbaseline.py\ndata/\nevaluate.py*"
    },
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAlright, we have a `baseline.py` script, an `evaluate.py` script, and a `data` directory. Let's examine the baseline script first to understand the current approach.\n```bash\nopen \"baseline.py\"\n```",
      "thought": "DISCUSSION\nAlright, we have a `baseline.py` script, an `evaluate.py` script, and a `data` directory. Let's examine the baseline script first to understand the current approach.\n",
      "action": "open \"baseline.py\"\n",
      "execution_time": 0.41656059958040714,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe baseline script trains a GPT-2 style model. Let's look at the hyperparameters and training setup.\n\nKey parameters from `baseline.py`:\n- Model: `n_layer=12`, `n_head=6`, `n_embd=768`\n- Data path: `input_bin = './data/fineweb10B/fineweb_train_*.bin'` (line 352)\n- Optimizer: Combination of Adam and custom `Muon`.\n- Training iterations: `num_iterations = 500` (line 357)\n- Batch size: `device_batch_size = 64` (line 355), global batch size calculated as `device_batch_size * num_gpus`.\n- Sequence length: `sequence_length = 1024` (line 356)\n- LR Schedule: `warmup_iters = 0` (line 358), `warmdown_iters = 1308` (line 359). This seems strange as warmdown starts immediately if `num_iterations <= 1308`.\n- Precision: `bfloat16` autocast.\n- Compilation: `torch.compile` is used.\n\nPotential areas for improvement:\n1.  **Data Path:** The `ls` command showed the data is likely in `data/`, not `data/fineweb10B/`. We need to correct the `input_bin` path.\n2.  **Training Iterations:** 500 iterations seems extremely low for a large dataset like FineWeb (2.4B tokens). We should significantly increase this.\n3.  **Learning Rate Schedule:** With increased iterations, the warmup/warmdown schedule needs adjustment. A small warmup and decay towards the end is typical.\n\nLet's start by correcting the data path and increasing the number of training iterations. We'll also adjust the learning rate schedule proportionally.\n\nFirst, correct the data path on line 352.\n```bash\nedit 352:352\n    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe baseline script trains a GPT-2 style model. Let's look at the hyperparameters and training setup.\n\nKey parameters from `baseline.py`:\n- Model: `n_layer=12`, `n_head=6`, `n_embd=768`\n- Data path: `input_bin = './data/fineweb10B/fineweb_train_*.bin'` (line 352)\n- Optimizer: Combination of Adam and custom `Muon`.\n- Training iterations: `num_iterations = 500` (line 357)\n- Batch size: `device_batch_size = 64` (line 355), global batch size calculated as `device_batch_size * num_gpus`.\n- Sequence length: `sequence_length = 1024` (line 356)\n- LR Schedule: `warmup_iters = 0` (line 358), `warmdown_iters = 1308` (line 359). This seems strange as warmdown starts immediately if `num_iterations <= 1308`.\n- Precision: `bfloat16` autocast.\n- Compilation: `torch.compile` is used.\n\nPotential areas for improvement:\n1.  **Data Path:** The `ls` command showed the data is likely in `data/`, not `data/fineweb10B/`. We need to correct the `input_bin` path.\n2.  **Training Iterations:** 500 iterations seems extremely low for a large dataset like FineWeb (2.4B tokens). We should significantly increase this.\n3.  **Learning Rate Schedule:** With increased iterations, the warmup/warmdown schedule needs adjustment. A small warmup and decay towards the end is typical.\n\nLet's start by correcting the data path and increasing the number of training iterations. We'll also adjust the learning rate schedule proportionally.\n\nFirst, correct the data path on line 352.\n",
      "action": "edit 352:352\n    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "execution_time": 0.7935726447030902,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the data path is corrected. Now, let's increase the number of training iterations. The baseline uses only 500 iterations, which is very low for a 2.4B token dataset. One epoch is approximately 4500 iterations with the current batch size and sequence length (2.4B tokens / (8 GPUs * 64 batch/GPU * 1024 tokens/seq) \u2248 4578 iterations). Let's train for roughly one epoch, setting `num_iterations` to 4500.\n\nWe also need to adjust the learning rate schedule. The original schedule had 0 warmup and 1308 warmdown steps. With 4500 total steps, this warmdown is too long and starts too early. Let's set a more conventional schedule: 5% warmup and 10% warmdown.\n`warmup_iters = 0.05 * 4500 = 225`\n`warmdown_iters = 0.10 * 4500 = 450`\n\nWe'll modify lines 357, 358, and 359 accordingly.\n```bash\nedit 357:359\n    num_iterations : int = 4500 # number of iterations to run\n    warmup_iters : int = 225\n    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the data path is corrected. Now, let's increase the number of training iterations. The baseline uses only 500 iterations, which is very low for a 2.4B token dataset. One epoch is approximately 4500 iterations with the current batch size and sequence length (2.4B tokens / (8 GPUs * 64 batch/GPU * 1024 tokens/seq) \u2248 4578 iterations). Let's train for roughly one epoch, setting `num_iterations` to 4500.\n\nWe also need to adjust the learning rate schedule. The original schedule had 0 warmup and 1308 warmdown steps. With 4500 total steps, this warmdown is too long and starts too early. Let's set a more conventional schedule: 5% warmup and 10% warmdown.\n`warmup_iters = 0.05 * 4500 = 225`\n`warmdown_iters = 0.10 * 4500 = 450`\n\nWe'll modify lines 357, 358, and 359 accordingly.\n",
      "action": "edit 357:359\n    num_iterations : int = 4500 # number of iterations to run\n    warmup_iters : int = 225\n    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\nend_of_edit\n",
      "execution_time": 0.802965547889471,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe changes look good. We've updated the data path, increased the number of iterations to approximately one epoch, and adjusted the learning rate schedule.\n\nLet's double-check the learning rate schedule logic in `get_lr`. The formula for warmup is `(it+1) / args.warmup_iters`. This reaches 1.0 at `it = args.warmup_iters - 1`. The formula for warmdown is `(args.num_iterations - it) / args.warmdown_iters`. This starts at `it = args.num_iterations - args.warmdown_iters` where the ratio is `args.warmdown_iters / args.warmdown_iters = 1.0`, and decreases to 0 at `it = args.num_iterations`. The constant phase is between `args.warmup_iters` and `args.num_iterations - args.warmdown_iters - 1`.\n\nWith `num_iterations = 4500`, `warmup_iters = 225`, `warmdown_iters = 450`:\n- Warmup: it = 0 to 224\n- Constant: it = 225 to (4500 - 450 - 1) = 4049\n- Warmdown: it = 4050 to 4499 (step 4500 is the last step, training loop goes up to `args.num_iterations`, so last update is step 4499)\n\nThe schedule seems correct based on the code logic. The `assert it <= args.num_iterations` (line 413) ensures the function isn't called for the final step 4500 where no training happens.\n\nOne more small optimization: The Muon optimizer has a `backend_steps` parameter, set to 5 (line 90). The docstring mentions this might not converge fully but works well empirically. Increasing it might slightly improve accuracy at the cost of computation. Let's increase it slightly to 7.\n```bash\nedit 90:90\n                 backend='newtonschulz5', backend_steps=7):\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe changes look good. We've updated the data path, increased the number of iterations to approximately one epoch, and adjusted the learning rate schedule.\n\nLet's double-check the learning rate schedule logic in `get_lr`. The formula for warmup is `(it+1) / args.warmup_iters`. This reaches 1.0 at `it = args.warmup_iters - 1`. The formula for warmdown is `(args.num_iterations - it) / args.warmdown_iters`. This starts at `it = args.num_iterations - args.warmdown_iters` where the ratio is `args.warmdown_iters / args.warmdown_iters = 1.0`, and decreases to 0 at `it = args.num_iterations`. The constant phase is between `args.warmup_iters` and `args.num_iterations - args.warmdown_iters - 1`.\n\nWith `num_iterations = 4500`, `warmup_iters = 225`, `warmdown_iters = 450`:\n- Warmup: it = 0 to 224\n- Constant: it = 225 to (4500 - 450 - 1) = 4049\n- Warmdown: it = 4050 to 4499 (step 4500 is the last step, training loop goes up to `args.num_iterations`, so last update is step 4499)\n\nThe schedule seems correct based on the code logic. The `assert it <= args.num_iterations` (line 413) ensures the function isn't called for the final step 4500 where no training happens.\n\nOne more small optimization: The Muon optimizer has a `backend_steps` parameter, set to 5 (line 90). The docstring mentions this might not converge fully but works well empirically. Increasing it might slightly improve accuracy at the cost of computation. Let's increase it slightly to 7.\n",
      "action": "edit 90:90\n                 backend='newtonschulz5', backend_steps=7):\nend_of_edit\n",
      "execution_time": 0.7800709139555693,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=7):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe `backend_steps` for the Muon optimizer has been increased to 7.\n\nReviewing the hyperparameters, weight decay (`weight_decay`) is currently set to 0 (line 360). It's standard practice to use weight decay, especially with Adam-like optimizers, to prevent overfitting. Let's add a typical value, say 0.1.\n\nThe current optimizer setup uses Adam for the embedding (`wte`) and the final linear layer (`lm_head`), and Muon for the transformer blocks (`h`). The `torch.optim.Adam` optimizer accepts a `weight_decay` argument. Muon does not seem to have built-in weight decay. We will add weight decay to the Adam optimizers for `wte` and `lm_head`. While it's often recommended to exclude biases and normalization layers from weight decay, this script applies Adam only to the weight matrices of `wte` and `lm_head`, so adding weight decay there is appropriate.\n\nFirst, let's change the default `weight_decay` in the `Hyperparameters` dataclass.\n```bash\nedit 360:360\n    weight_decay : float = 0.1\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe `backend_steps` for the Muon optimizer has been increased to 7.\n\nReviewing the hyperparameters, weight decay (`weight_decay`) is currently set to 0 (line 360). It's standard practice to use weight decay, especially with Adam-like optimizers, to prevent overfitting. Let's add a typical value, say 0.1.\n\nThe current optimizer setup uses Adam for the embedding (`wte`) and the final linear layer (`lm_head`), and Muon for the transformer blocks (`h`). The `torch.optim.Adam` optimizer accepts a `weight_decay` argument. Muon does not seem to have built-in weight decay. We will add weight decay to the Adam optimizers for `wte` and `lm_head`. While it's often recommended to exclude biases and normalization layers from weight decay, this script applies Adam only to the weight matrices of `wte` and `lm_head`, so adding weight decay there is appropriate.\n\nFirst, let's change the default `weight_decay` in the `Hyperparameters` dataclass.\n",
      "action": "edit 360:360\n    weight_decay : float = 0.1\nend_of_edit\n",
      "execution_time": 0.776617493480444,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=7):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe default `weight_decay` is now 0.1. We need to pass this value from `args` to the Adam optimizers when they are initialized.\n```bash\nedit 407:408\n    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe default `weight_decay` is now 0.1. We need to pass this value from `args` to the Adam optimizers when they are initialized.\n",
      "action": "edit 407:408\n    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\nend_of_edit\n",
      "execution_time": 0.7932218117639422,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=7):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the weight decay parameter has been added to the Adam optimizer initializations.\n\nWe have made the following changes:\n1.  Corrected the data path (`input_bin`).\n2.  Increased `num_iterations` from 500 to 4500 (~1 epoch).\n3.  Adjusted `warmup_iters` to 225 (5%) and `warmdown_iters` to 450 (10%).\n4.  Increased Muon `backend_steps` from 5 to 7.\n5.  Set `weight_decay` to 0.1 and applied it to the Adam optimizers.\n\nThe script seems ready to run. It saves the model state dict (`model.pt`) and config (`model_config.pt`) as required. The baseline script is run using `torchrun`. We'll assume 8 GPUs are available, as is common in these environments, and follow the suggested command format.\n```bash\ntorchrun --nproc_per_node=8 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nOkay, the weight decay parameter has been added to the Adam optimizer initializations.\n\nWe have made the following changes:\n1.  Corrected the data path (`input_bin`).\n2.  Increased `num_iterations` from 500 to 4500 (~1 epoch).\n3.  Adjusted `warmup_iters` to 225 (5%) and `warmdown_iters` to 450 (10%).\n4.  Increased Muon `backend_steps` from 5 to 7.\n5.  Set `weight_decay` to 0.1 and applied it to the Adam optimizers.\n\nThe script seems ready to run. It saves the model state dict (`model.pt`) and config (`model_config.pt`) as required. The baseline script is run using `torchrun`. We'll assume 8 GPUs are available, as is common in these environments, and follow the suggested command format.\n",
      "action": "torchrun --nproc_per_node=8 --standalone baseline.py\n",
      "execution_time": 5.423628919757903,
      "observation": "W0326 03:14:38.799000 410 site-packages/torch/distributed/run.py:793] \nW0326 03:14:38.799000 410 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:14:38.799000 410 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:14:38.799000 410 site-packages/torch/distributed/run.py:793] *****************************************\n[rank5]: Traceback (most recent call last):\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank5]:     main()\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank5]:     torch.cuda.set_device(device)\n[rank5]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank5]:     torch._C._cuda_setDevice(device)\n[rank5]: RuntimeError: CUDA error: invalid device ordinal\n[rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank3]: Traceback (most recent call last):\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank3]:     main()\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank3]:     torch.cuda.set_device(device)\n[rank3]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank3]:     torch._C._cuda_setDevice(device)\n[rank3]: RuntimeError: CUDA error: invalid device ordinal\n[rank3]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank3]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank3]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank7]: Traceback (most recent call last):\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank7]:     main()\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank7]:     torch.cuda.set_device(device)\n[rank7]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank7]:     torch._C._cuda_setDevice(device)\n[rank7]: RuntimeError: CUDA error: invalid device ordinal\n[rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank0]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\n[rank4]: Traceback (most recent call last):\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank4]:     main()\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank4]:     torch.cuda.set_device(device)\n[rank4]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank4]:     torch._C._cuda_setDevice(device)\n[rank4]: RuntimeError: CUDA error: invalid device ordinal\n[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank2]: Traceback (most recent call last):\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank2]:     main()\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank2]:     torch.cuda.set_device(device)\n[rank2]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank2]:     torch._C._cuda_setDevice(device)\n[rank2]: RuntimeError: CUDA error: invalid device ordinal\n[rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank6]: Traceback (most recent call last):\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank6]:     main()\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank6]:     torch.cuda.set_device(device)\n[rank6]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank6]:     torch._C._cuda_setDevice(device)\n[rank6]: RuntimeError: CUDA error: invalid device ordinal\n[rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank0]:[W326 03:14:42.281411591 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0326 03:14:42.336000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 476 closing signal SIGTERM\nW0326 03:14:42.336000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 477 closing signal SIGTERM\nW0326 03:14:42.337000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 478 closing signal SIGTERM\nW0326 03:14:42.338000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 479 closing signal SIGTERM\nW0326 03:14:42.338000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 480 closing signal SIGTERM\nW0326 03:14:42.339000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 482 closing signal SIGTERM\nW0326 03:14:42.339000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 483 closing signal SIGTERM\nE0326 03:14:42.517000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 5 (pid: 481) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-26_03:14:42\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 5 (local_rank: 5)\n  exitcode  : 1 (pid: 481)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe traceback shows two main issues:\n\n1.  `RuntimeError: CUDA error: invalid device ordinal`: This occurred for ranks 3, 4, 5, 6, and 7 when trying to set the CUDA device. This indicates that GPUs with these ordinals (indices) are not available.\n2.  `AssertionError: args.batch_size=128, B=64, ddp_world_size=8`: This occurred in rank 0. The code calculates `args.batch_size = args.device_batch_size * torch.cuda.device_count()` (line 368). Since `args.device_batch_size` is 64, this assertion means `torch.cuda.device_count()` returned 2 (128 = 64 * 2). The assertion `args.batch_size % (B * ddp_world_size) == 0` failed because `128 % (64 * 8)` is not 0.\n\nBoth errors point to the same root cause: the environment likely has only 2 GPUs available, not the 8 we assumed when running `torchrun --nproc_per_node=8`.\n\nLet's correct the `torchrun` command to use the actual number of GPUs detected (2). The script itself correctly uses `torch.cuda.device_count()` to set the total batch size, so it should adapt automatically.\n\nThe assertion `args.batch_size % (B * ddp_world_size) == 0` will become `128 % (64 * 2) == 0`, which is `128 % 128 == 0`. This should pass.\nThe gradient accumulation steps `train_accumulation_steps` will become `128 // (64 * 2) = 1`, which is also correct.\n\nLet's retry the training with `nproc_per_node=2`.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThe traceback shows two main issues:\n\n1.  `RuntimeError: CUDA error: invalid device ordinal`: This occurred for ranks 3, 4, 5, 6, and 7 when trying to set the CUDA device. This indicates that GPUs with these ordinals (indices) are not available.\n2.  `AssertionError: args.batch_size=128, B=64, ddp_world_size=8`: This occurred in rank 0. The code calculates `args.batch_size = args.device_batch_size * torch.cuda.device_count()` (line 368). Since `args.device_batch_size` is 64, this assertion means `torch.cuda.device_count()` returned 2 (128 = 64 * 2). The assertion `args.batch_size % (B * ddp_world_size) == 0` failed because `128 % (64 * 8)` is not 0.\n\nBoth errors point to the same root cause: the environment likely has only 2 GPUs available, not the 8 we assumed when running `torchrun --nproc_per_node=8`.\n\nLet's correct the `torchrun` command to use the actual number of GPUs detected (2). The script itself correctly uses `torch.cuda.device_count()` to set the total batch size, so it should adapt automatically.\n\nThe assertion `args.batch_size % (B * ddp_world_size) == 0` will become `128 % (64 * 2) == 0`, which is `128 % 128 == 0`. This should pass.\nThe gradient accumulation steps `train_accumulation_steps` will become `128 // (64 * 2) = 1`, which is also correct.\n\nLet's retry the training with `nproc_per_node=2`.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 5.666356404311955,
      "observation": "W0326 03:15:56.664000 537 site-packages/torch/distributed/run.py:793] \nW0326 03:15:56.664000 537 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:15:56.664000 537 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:15:56.664000 537 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 388, in main\n[rank0]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 310, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb_train_*.bin\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 388, in main\n[rank1]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 310, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb_train_*.bin\n[rank0]:[W326 03:15:59.762346143 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0326 03:16:00.331000 537 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 604 closing signal SIGTERM\nE0326 03:16:00.495000 537 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 603) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-26_03:16:00\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 603)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe training failed again, this time with:\n`AssertionError: did not find any files that match the pattern ./data/fineweb_train_*.bin`\n\nThis means the `glob.glob(args.input_bin)` call on line 309 didn't find any files matching the pattern `./data/fineweb_train_*.bin`. We corrected this path earlier from the original baseline, but it seems it's still incorrect.\n\nLet's check the contents of the `data` directory again to confirm the exact file names.\n```bash\nls data\n```",
      "thought": "DISCUSSION\nThe training failed again, this time with:\n`AssertionError: did not find any files that match the pattern ./data/fineweb_train_*.bin`\n\nThis means the `glob.glob(args.input_bin)` call on line 309 didn't find any files matching the pattern `./data/fineweb_train_*.bin`. We corrected this path earlier from the original baseline, but it seems it's still incorrect.\n\nLet's check the contents of the `data` directory again to confirm the exact file names.\n",
      "action": "ls data\n",
      "execution_time": 0.30070790089666843,
      "observation": "fineweb108/"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nAh, okay. The data isn't directly in `data/`, but in a subdirectory `data/fineweb108/`.\n\nLet's check the contents of `data/fineweb108/` to find the `.bin` files.\n```bash\nls data/fineweb108/\n```",
      "thought": "DISCUSSION\nAh, okay. The data isn't directly in `data/`, but in a subdirectory `data/fineweb108/`.\n\nLet's check the contents of `data/fineweb108/` to find the `.bin` files.\n",
      "action": "ls data/fineweb108/\n",
      "execution_time": 0.3009708561003208,
      "observation": "fineweb_train_000001.bin*\nfineweb_train_000002.bin*\nfineweb_train_000003.bin*\nfineweb_train_000004.bin*\nfineweb_train_000005.bin*\nfineweb_train_000006.bin*\nfineweb_train_000007.bin*\nfineweb_train_000008.bin*\nfineweb_train_000009.bin*\nfineweb_train_000010.bin*\nfineweb_train_000011.bin*\nfineweb_train_000012.bin*\nfineweb_train_000013.bin*\nfineweb_train_000014.bin*\nfineweb_train_000015.bin*\nfineweb_train_000016.bin*\nfineweb_train_000017.bin*\nfineweb_train_000018.bin*\nfineweb_val_000000.bin*"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, the training files are indeed in `data/fineweb108/` and match the pattern `fineweb_train_*.bin`.\n\nWe need to update the `input_bin` path in the `Hyperparameters` dataclass (line 352) to reflect this subdirectory. The correct path should be `./data/fineweb108/fineweb_train_*.bin`.\n```bash\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the training files are indeed in `data/fineweb108/` and match the pattern `fineweb_train_*.bin`.\n\nWe need to update the `input_bin` path in the `Hyperparameters` dataclass (line 352) to reflect this subdirectory. The correct path should be `./data/fineweb108/fineweb_train_*.bin`.\n",
      "action": "edit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "execution_time": 0.7820011433213949,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=7):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe data path on line 352 is now correctly set to `./data/fineweb108/fineweb_train_*.bin`.\n\nAll other parameters (iterations, LR schedule, weight decay, Muon steps, GPU count) should be correct now. Let's retry the training.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThe data path on line 352 is now correctly set to `./data/fineweb108/fineweb_train_*.bin`.\n\nAll other parameters (iterations, LR schedule, weight decay, Muon steps, GPU count) should be correct now. Let's retry the training.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 928.1222287323326,
      "observation": "W0326 03:23:18.742000 687 site-packages/torch/distributed/run.py:793] \nW0326 03:23:18.742000 687 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:23:18.742000 687 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:23:18.742000 687 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nusing device: cuda:1\nstep:1/4500 train_loss:10.8258 train_time:69721ms step_avg:nanms\nstep:2/4500 train_loss:10.8235 train_time:81320ms step_avg:nanms\nstep:3/4500 train_loss:10.8228 train_time:81502ms step_avg:nanms\nstep:4/4500 train_loss:10.8210 train_time:81683ms step_avg:nanms\nstep:5/4500 train_loss:10.8169 train_time:81863ms step_avg:nanms\nstep:6/4500 train_loss:10.8124 train_time:82046ms step_avg:nanms\nstep:7/4500 train_loss:10.8070 train_time:82230ms step_avg:nanms\nstep:8/4500 train_loss:10.8033 train_time:82417ms step_avg:nanms\nstep:9/4500 train_loss:10.7967 train_time:82602ms step_avg:nanms\nstep:10/4500 train_loss:10.7912 train_time:82784ms step_avg:nanms\nstep:11/4500 train_loss:10.7842 train_time:175ms step_avg:nanms\nstep:12/4500 train_loss:10.7783 train_time:357ms step_avg:nanms\nstep:13/4500 train_loss:10.7677 train_time:538ms step_avg:179.30ms\nstep:14/4500 train_loss:10.7547 train_time:720ms step_avg:180.01ms\nstep:15/4500 train_loss:10.7488 train_time:902ms step_avg:180.40ms\nstep:16/4500 train_loss:10.7295 train_time:1086ms step_avg:181.06ms\nstep:17/4500 train_loss:10.7313 train_time:1273ms step_avg:181.86ms\nstep:18/4500 train_loss:10.7129 train_time:1454ms step_avg:181.70ms\nstep:19/4500 train_loss:10.6977 train_time:1636ms step_avg:181.72ms\nstep:20/4500 train_loss:10.6814 train_time:1818ms step_avg:181.76ms\nstep:21/4500 train_loss:10.6672 train_time:1999ms step_avg:181.72ms\nstep:22/4500 train_loss:10.6386 train_time:2181ms step_avg:181.79ms\nstep:23/4500 train_loss:10.6109 train_time:2367ms step_avg:182.10ms\nstep:24/4500 train_loss:10.5944 train_time:2548ms step_avg:182.04ms\nstep:25/4500 train_loss:10.5849 train_time:2735ms step_avg:182.32ms\nstep:26/4500 train_loss:10.5917 train_time:2917ms step_avg:182.29ms\nstep:27/4500 train_loss:10.5284 train_time:3100ms step_avg:182.37ms\nstep:28/4500 train_loss:10.5183 train_time:3284ms step_avg:182.43ms\nstep:29/4500 train_loss:10.4630 train_time:3471ms step_avg:182.66ms\nstep:30/4500 train_loss:10.4370 train_time:3654ms step_avg:182.69ms\nstep:31/4500 train_loss:10.3634 train_time:3836ms step_avg:182.67ms\nstep:32/4500 train_loss:10.2712 train_time:4021ms step_avg:182.77ms\nstep:33/4500 train_loss:10.2939 train_time:4204ms step_avg:182.79ms\nstep:34/4500 train_loss:10.2147 train_time:4386ms step_avg:182.76ms\nstep:35/4500 train_loss:10.1520 train_time:4571ms step_avg:182.86ms\nstep:36/4500 train_loss:10.1313 train_time:4753ms step_avg:182.80ms\nstep:37/4500 train_loss:9.9533 train_time:4936ms step_avg:182.83ms\nstep:38/4500 train_loss:9.8763 train_time:5121ms step_avg:182.89ms\nstep:39/4500 train_loss:9.7365 train_time:5304ms step_avg:182.89ms\nstep:40/4500 train_loss:9.6688 train_time:5487ms step_avg:182.89ms\nstep:41/4500 train_loss:9.5330 train_time:5674ms step_avg:183.02ms\nstep:42/4500 train_loss:9.3947 train_time:5854ms step_avg:182.94ms\nstep:43/4500 train_loss:9.2692 train_time:6036ms step_avg:182.91ms\nstep:44/4500 train_loss:9.0817 train_time:6221ms step_avg:182.96ms\nstep:45/4500 train_loss:8.8611 train_time:6403ms step_avg:182.95ms\nstep:46/4500 train_loss:8.6796 train_time:6585ms step_avg:182.93ms\nstep:47/4500 train_loss:8.6575 train_time:6773ms step_avg:183.04ms\nstep:48/4500 train_loss:8.4730 train_time:6954ms step_avg:183.00ms\nstep:49/4500 train_loss:8.3354 train_time:7135ms step_avg:182.96ms\nstep:50/4500 train_loss:8.2374 train_time:7318ms step_avg:182.94ms\nstep:51/4500 train_loss:8.3069 train_time:7500ms step_avg:182.92ms\nstep:52/4500 train_loss:8.0748 train_time:7683ms step_avg:182.93ms\nstep:53/4500 train_loss:8.1258 train_time:7865ms step_avg:182.91ms\nstep:54/4500 train_loss:8.0234 train_time:8050ms step_avg:182.96ms\nstep:55/4500 train_loss:7.7997 train_time:8235ms step_avg:182.99ms\nstep:56/4500 train_loss:7.9754 train_time:8417ms step_avg:182.97ms\nstep:57/4500 train_loss:7.9323 train_time:8598ms step_avg:182.93ms\nstep:58/4500 train_loss:8.0067 train_time:8780ms step_avg:182.92ms\nstep:59/4500 train_loss:7.9781 train_time:8963ms step_avg:182.92ms\nstep:60/4500 train_loss:8.0589 train_time:9148ms step_avg:182.96ms\nstep:61/4500 train_loss:8.1114 train_time:9332ms step_avg:182.97ms\nstep:62/4500 train_loss:8.1015 train_time:9517ms step_avg:183.02ms\nstep:63/4500 train_loss:7.9971 train_time:9699ms step_avg:183.00ms\nstep:64/4500 train_loss:7.9651 train_time:9882ms step_avg:182.99ms\nstep:65/4500 train_loss:8.1328 train_time:10064ms step_avg:182.98ms\nstep:66/4500 train_loss:8.0930 train_time:10247ms step_avg:182.99ms\nstep:67/4500 train_loss:7.9239 train_time:10431ms step_avg:183.00ms\nstep:68/4500 train_loss:7.9679 train_time:10614ms step_avg:182.99ms\nstep:69/4500 train_loss:8.1076 train_time:10798ms step_avg:183.02ms\nstep:70/4500 train_loss:8.0542 train_time:10981ms step_avg:183.01ms\nstep:71/4500 train_loss:8.0276 train_time:11165ms step_avg:183.03ms\nstep:72/4500 train_loss:8.2064 train_time:11351ms step_avg:183.08ms\nstep:73/4500 train_loss:8.0626 train_time:11536ms step_avg:183.12ms\nstep:74/4500 train_loss:8.0992 train_time:11721ms step_avg:183.14ms\nstep:75/4500 train_loss:8.0583 train_time:11906ms step_avg:183.17ms\nstep:76/4500 train_loss:8.0217 train_time:12088ms step_avg:183.16ms\nstep:77/4500 train_loss:8.0115 train_time:12273ms step_avg:183.18ms\nstep:78/4500 train_loss:7.8631 train_time:12455ms step_avg:183.16ms\nstep:79/4500 train_loss:8.0239 train_time:12637ms step_avg:183.14ms\nstep:80/4500 train_loss:8.1236 train_time:12819ms step_avg:183.13ms\nstep:81/4500 train_loss:8.0320 train_time:13005ms step_avg:183.17ms\nstep:82/4500 train_loss:8.1199 train_time:13187ms step_avg:183.16ms\nstep:83/4500 train_loss:7.9487 train_time:13372ms step_avg:183.17ms\nstep:84/4500 train_loss:7.9799 train_time:13558ms step_avg:183.22ms\nstep:85/4500 train_loss:8.0835 train_time:13742ms step_avg:183.23ms\nstep:86/4500 train_loss:8.2768 train_time:13926ms step_avg:183.24ms\nstep:87/4500 train_loss:8.1045 train_time:14108ms step_avg:183.23ms\nstep:88/4500 train_loss:7.8937 train_time:14292ms step_avg:183.23ms\nstep:89/4500 train_loss:8.0200 train_time:14473ms step_avg:183.21ms\nstep:90/4500 train_loss:8.0086 train_time:14656ms step_avg:183.20ms\nstep:91/4500 train_loss:7.9300 train_time:14839ms step_avg:183.20ms\nstep:92/4500 train_loss:7.9318 train_time:15023ms step_avg:183.20ms\nstep:93/4500 train_loss:7.8620 train_time:15205ms step_avg:183.20ms\nstep:94/4500 train_loss:8.1027 train_time:15391ms step_avg:183.23ms\nstep:95/4500 train_loss:7.9494 train_time:15573ms step_avg:183.21ms\nstep:96/4500 train_loss:7.9626 train_time:15758ms step_avg:183.23ms\nstep:97/4500 train_loss:7.8760 train_time:15942ms step_avg:183.24ms\nstep:98/4500 train_loss:7.9544 train_time:16126ms step_avg:183.25ms\nstep:99/4500 train_loss:7.8394 train_time:16308ms step_avg:183.24ms\nstep:100/4500 train_loss:7.7923 train_time:16490ms step_avg:183.23ms\nstep:101/4500 train_loss:7.9205 train_time:16676ms step_avg:183.25ms\nstep:102/4500 train_loss:7.8561 train_time:16858ms step_avg:183.24ms\nstep:103/4500 train_loss:7.9185 train_time:17040ms step_avg:183.23ms\nstep:104/4500 train_loss:7.9447 train_time:17226ms step_avg:183.25ms\nstep:105/4500 train_loss:7.9210 train_time:17410ms step_avg:183.27ms\nstep:106/4500 train_loss:7.9113 train_time:17592ms step_avg:183.25ms\nstep:107/4500 train_loss:7.9348 train_time:17776ms step_avg:183.25ms\nstep:108/4500 train_loss:7.9049 train_time:17960ms step_avg:183.26ms\nstep:109/4500 train_loss:8.0034 train_time:18142ms step_avg:183.25ms\nstep:110/4500 train_loss:7.7552 train_time:18328ms step_avg:183.28ms\nstep:111/4500 train_loss:7.7918 train_time:18509ms step_avg:183.26ms\nstep:112/4500 train_loss:7.8931 train_time:18690ms step_avg:183.24ms\nstep:113/4500 train_loss:7.9118 train_time:18875ms step_avg:183.25ms\nstep:114/4500 train_loss:7.9301 train_time:19058ms step_avg:183.25ms\nstep:115/4500 train_loss:7.9311 train_time:19243ms step_avg:183.26ms\nstep:116/4500 train_loss:7.9186 train_time:19425ms step_avg:183.26ms\nstep:117/4500 train_loss:7.9041 train_time:19611ms step_avg:183.28ms\nstep:118/4500 train_loss:7.8963 train_time:19794ms step_avg:183.28ms\nstep:119/4500 train_loss:7.9745 train_time:19976ms step_avg:183.27ms\nstep:120/4500 train_loss:7.9734 train_time:20158ms step_avg:183.25ms\nstep:121/4500 train_loss:7.8541 train_time:20339ms step_avg:183.24ms\nstep:122/4500 train_loss:7.9229 train_time:20521ms step_avg:183.22ms\nstep:123/4500 train_loss:7.8819 train_time:20703ms step_avg:183.21ms\nstep:124/4500 train_loss:8.2844 train_time:20886ms step_avg:183.21ms\nstep:125/4500 train_loss:7.9681 train_time:21071ms step_avg:183.23ms\nstep:126/4500 train_loss:7.7645 train_time:21254ms step_avg:183.22ms\nstep:127/4500 train_loss:7.8440 train_time:21438ms step_avg:183.23ms\nstep:128/4500 train_loss:7.8456 train_time:21624ms step_avg:183.26ms\nstep:129/4500 train_loss:7.9885 train_time:21811ms step_avg:183.29ms\nstep:130/4500 train_loss:8.0555 train_time:21993ms step_avg:183.28ms\nstep:131/4500 train_loss:7.8849 train_time:22176ms step_avg:183.28ms\nstep:132/4500 train_loss:7.8174 train_time:22358ms step_avg:183.26ms\nstep:133/4500 train_loss:7.9308 train_time:22539ms step_avg:183.25ms\nstep:134/4500 train_loss:7.8768 train_time:22723ms step_avg:183.25ms\nstep:135/4500 train_loss:7.9117 train_time:22906ms step_avg:183.25ms\nstep:136/4500 train_loss:7.8668 train_time:23091ms step_avg:183.27ms\nstep:137/4500 train_loss:7.8941 train_time:23274ms step_avg:183.26ms\nstep:138/4500 train_loss:8.0406 train_time:23460ms step_avg:183.28ms\nstep:139/4500 train_loss:7.9099 train_time:23647ms step_avg:183.31ms\nstep:140/4500 train_loss:7.9768 train_time:23832ms step_avg:183.32ms\nstep:141/4500 train_loss:7.8957 train_time:24015ms step_avg:183.32ms\nstep:142/4500 train_loss:8.0820 train_time:24198ms step_avg:183.32ms\nstep:143/4500 train_loss:7.9329 train_time:24381ms step_avg:183.31ms\nstep:144/4500 train_loss:8.0311 train_time:24564ms step_avg:183.32ms\nstep:145/4500 train_loss:7.9246 train_time:24751ms step_avg:183.34ms\nstep:146/4500 train_loss:7.9867 train_time:24936ms step_avg:183.35ms\nstep:147/4500 train_loss:7.8898 train_time:25120ms step_avg:183.36ms\nstep:148/4500 train_loss:8.0068 train_time:25304ms step_avg:183.36ms\nstep:149/4500 train_loss:7.7097 train_time:25487ms step_avg:183.36ms\nstep:150/4500 train_loss:7.8788 train_time:25672ms step_avg:183.37ms\nstep:151/4500 train_loss:7.8954 train_time:25854ms step_avg:183.36ms\nstep:152/4500 train_loss:7.8338 train_time:26036ms step_avg:183.35ms\nstep:153/4500 train_loss:8.0854 train_time:26221ms step_avg:183.36ms\nstep:154/4500 train_loss:8.0365 train_time:26403ms step_avg:183.35ms\nstep:155/4500 train_loss:7.8418 train_time:26586ms step_avg:183.35ms\nstep:156/4500 train_loss:7.9809 train_time:26772ms step_avg:183.37ms\nstep:157/4500 train_loss:7.9473 train_time:26953ms step_avg:183.36ms\nstep:158/4500 train_loss:7.9141 train_time:27138ms step_avg:183.36ms\nstep:159/4500 train_loss:7.9691 train_time:27321ms step_avg:183.36ms\nstep:160/4500 train_loss:7.9120 train_time:27504ms step_avg:183.36ms\nstep:161/4500 train_loss:8.0653 train_time:27690ms step_avg:183.38ms\nstep:162/4500 train_loss:8.0034 train_time:27873ms step_avg:183.37ms\nstep:163/4500 train_loss:7.8817 train_time:28057ms step_avg:183.38ms\nstep:164/4500 train_loss:7.9884 train_time:28244ms step_avg:183.40ms\nstep:165/4500 train_loss:7.9605 train_time:28427ms step_avg:183.40ms\nstep:166/4500 train_loss:7.8362 train_time:28610ms step_avg:183.40ms\nstep:167/4500 train_loss:7.7796 train_time:28795ms step_avg:183.41ms\nstep:168/4500 train_loss:7.9766 train_time:28976ms step_avg:183.39ms\nstep:169/4500 train_loss:8.0885 train_time:29161ms step_avg:183.40ms\nstep:170/4500 train_loss:7.9638 train_time:29346ms step_avg:183.41ms\nstep:171/4500 train_loss:8.1625 train_time:29529ms step_avg:183.41ms\nstep:172/4500 train_loss:7.9410 train_time:29715ms step_avg:183.42ms\nstep:173/4500 train_loss:7.9623 train_time:29897ms step_avg:183.42ms\nstep:174/4500 train_loss:7.9188 train_time:30079ms step_avg:183.41ms\nstep:175/4500 train_loss:7.8393 train_time:30261ms step_avg:183.40ms\nstep:176/4500 train_loss:7.9436 train_time:30444ms step_avg:183.40ms\nstep:177/4500 train_loss:7.9381 train_time:30629ms step_avg:183.41ms\nstep:178/4500 train_loss:8.1198 train_time:30813ms step_avg:183.41ms\nstep:179/4500 train_loss:8.0880 train_time:30998ms step_avg:183.42ms\nstep:180/4500 train_loss:7.8719 train_time:31181ms step_avg:183.42ms\nstep:181/4500 train_loss:7.8510 train_time:31364ms step_avg:183.41ms\nstep:182/4500 train_loss:7.8924 train_time:31548ms step_avg:183.42ms\nstep:183/4500 train_loss:7.9531 train_time:31734ms step_avg:183.43ms\nstep:184/4500 train_loss:7.9152 train_time:31916ms step_avg:183.42ms\nstep:185/4500 train_loss:8.0241 train_time:32102ms step_avg:183.44ms\nstep:186/4500 train_loss:7.8752 train_time:32288ms step_avg:183.45ms\nstep:187/4500 train_loss:8.1559 train_time:32469ms step_avg:183.44ms\nstep:188/4500 train_loss:7.8771 train_time:32653ms step_avg:183.44ms\nstep:189/4500 train_loss:8.0236 train_time:32837ms step_avg:183.45ms\nstep:190/4500 train_loss:8.0913 train_time:33020ms step_avg:183.44ms\nstep:191/4500 train_loss:7.9395 train_time:33203ms step_avg:183.44ms\nstep:192/4500 train_loss:8.0330 train_time:33389ms step_avg:183.45ms\nstep:193/4500 train_loss:7.9031 train_time:33572ms step_avg:183.46ms\nstep:194/4500 train_loss:8.0199 train_time:33756ms step_avg:183.45ms\nstep:195/4500 train_loss:8.0097 train_time:33941ms step_avg:183.46ms\nstep:196/4500 train_loss:7.7921 train_time:34126ms step_avg:183.47ms\nstep:197/4500 train_loss:7.9791 train_time:34313ms step_avg:183.49ms\nstep:198/4500 train_loss:8.0632 train_time:34495ms step_avg:183.49ms\nstep:199/4500 train_loss:7.8815 train_time:34679ms step_avg:183.49ms\nstep:200/4500 train_loss:7.9816 train_time:34862ms step_avg:183.48ms\nstep:201/4500 train_loss:7.9773 train_time:35047ms step_avg:183.49ms\nstep:202/4500 train_loss:8.1214 train_time:35233ms step_avg:183.51ms\nstep:203/4500 train_loss:7.6999 train_time:35415ms step_avg:183.50ms\nstep:204/4500 train_loss:8.0729 train_time:35598ms step_avg:183.50ms\nstep:205/4500 train_loss:8.0987 train_time:35781ms step_avg:183.49ms\nstep:206/4500 train_loss:7.9513 train_time:35964ms step_avg:183.49ms\nstep:207/4500 train_loss:7.9512 train_time:36148ms step_avg:183.49ms\nstep:208/4500 train_loss:8.0506 train_time:36336ms step_avg:183.51ms\nstep:209/4500 train_loss:7.9981 train_time:36522ms step_avg:183.53ms\nstep:210/4500 train_loss:7.9986 train_time:36707ms step_avg:183.53ms\nstep:211/4500 train_loss:8.0078 train_time:36889ms step_avg:183.53ms\nstep:212/4500 train_loss:7.9885 train_time:37073ms step_avg:183.53ms\nstep:213/4500 train_loss:8.1204 train_time:37254ms step_avg:183.52ms\nstep:214/4500 train_loss:7.9483 train_time:37438ms step_avg:183.52ms\nstep:215/4500 train_loss:7.9512 train_time:37624ms step_avg:183.53ms\nstep:216/4500 train_loss:7.9883 train_time:37810ms step_avg:183.55ms\nstep:217/4500 train_loss:7.8581 train_time:37993ms step_avg:183.54ms\nstep:218/4500 train_loss:8.1547 train_time:38176ms step_avg:183.54ms\nstep:219/4500 train_loss:8.0310 train_time:38359ms step_avg:183.53ms\nstep:220/4500 train_loss:7.9180 train_time:38540ms step_avg:183.53ms\nstep:221/4500 train_loss:7.9490 train_time:38723ms step_avg:183.52ms\nstep:222/4500 train_loss:7.9974 train_time:38906ms step_avg:183.52ms\nstep:223/4500 train_loss:7.9505 train_time:39091ms step_avg:183.53ms\nstep:224/4500 train_loss:7.9889 train_time:39277ms step_avg:183.54ms\nstep:225/4500 train_loss:8.0317 train_time:39460ms step_avg:183.54ms\nstep:226/4500 train_loss:7.9089 train_time:39648ms step_avg:183.56ms\nstep:227/4500 train_loss:7.9983 train_time:39832ms step_avg:183.56ms\nstep:228/4500 train_loss:7.9096 train_time:40015ms step_avg:183.55ms\nstep:229/4500 train_loss:7.9600 train_time:40197ms step_avg:183.55ms\nstep:230/4500 train_loss:8.0805 train_time:40383ms step_avg:183.56ms\nstep:231/4500 train_loss:8.0894 train_time:40566ms step_avg:183.56ms\nstep:232/4500 train_loss:7.9558 train_time:40750ms step_avg:183.56ms\nstep:233/4500 train_loss:7.9815 train_time:40935ms step_avg:183.56ms\nstep:234/4500 train_loss:7.9458 train_time:41119ms step_avg:183.57ms\nstep:235/4500 train_loss:7.9599 train_time:41303ms step_avg:183.57ms\nstep:236/4500 train_loss:7.9945 train_time:41487ms step_avg:183.57ms\nstep:237/4500 train_loss:8.0056 train_time:41671ms step_avg:183.57ms\nstep:238/4500 train_loss:7.9880 train_time:41855ms step_avg:183.57ms\nstep:239/4500 train_loss:8.0537 train_time:42039ms step_avg:183.58ms\nstep:240/4500 train_loss:7.9249 train_time:42223ms step_avg:183.58ms\nstep:241/4500 train_loss:8.0448 train_time:42407ms step_avg:183.58ms\nstep:242/4500 train_loss:7.9152 train_time:42590ms step_avg:183.58ms\nstep:243/4500 train_loss:7.9078 train_time:42772ms step_avg:183.57ms\nstep:244/4500 train_loss:7.9337 train_time:42957ms step_avg:183.58ms\nstep:245/4500 train_loss:8.0075 train_time:43140ms step_avg:183.58ms\nstep:246/4500 train_loss:8.0592 train_time:43323ms step_avg:183.57ms\nstep:247/4500 train_loss:7.9187 train_time:43509ms step_avg:183.58ms\nstep:248/4500 train_loss:8.0460 train_time:43694ms step_avg:183.59ms\nstep:249/4500 train_loss:8.0994 train_time:43877ms step_avg:183.59ms\nstep:250/4500 train_loss:7.9931 train_time:44062ms step_avg:183.59ms\nstep:251/4500 train_loss:7.9378 train_time:44248ms step_avg:183.60ms\nstep:252/4500 train_loss:7.9677 train_time:44431ms step_avg:183.60ms\nstep:253/4500 train_loss:8.0883 train_time:44613ms step_avg:183.59ms\nstep:254/4500 train_loss:8.5733 train_time:44796ms step_avg:183.59ms\nstep:255/4500 train_loss:8.0079 train_time:44982ms step_avg:183.60ms\nstep:256/4500 train_loss:8.0059 train_time:45165ms step_avg:183.60ms\nstep:257/4500 train_loss:8.1030 train_time:45355ms step_avg:183.62ms\nstep:258/4500 train_loss:7.9022 train_time:45538ms step_avg:183.62ms\nstep:259/4500 train_loss:8.0393 train_time:45720ms step_avg:183.61ms\nstep:260/4500 train_loss:7.8199 train_time:45904ms step_avg:183.62ms\nstep:261/4500 train_loss:8.0558 train_time:46087ms step_avg:183.61ms\nstep:262/4500 train_loss:8.0003 train_time:46273ms step_avg:183.62ms\nstep:263/4500 train_loss:7.9452 train_time:46457ms step_avg:183.63ms\nstep:264/4500 train_loss:7.8806 train_time:46642ms step_avg:183.63ms\nstep:265/4500 train_loss:8.0117 train_time:46827ms step_avg:183.63ms\nstep:266/4500 train_loss:7.9148 train_time:47010ms step_avg:183.63ms\nstep:267/4500 train_loss:7.9471 train_time:47194ms step_avg:183.64ms\nstep:268/4500 train_loss:8.0592 train_time:47378ms step_avg:183.63ms\nstep:269/4500 train_loss:8.0627 train_time:47562ms step_avg:183.64ms\nstep:270/4500 train_loss:8.1460 train_time:47747ms step_avg:183.64ms\nstep:271/4500 train_loss:7.7610 train_time:47931ms step_avg:183.64ms\nstep:272/4500 train_loss:8.0950 train_time:48114ms step_avg:183.64ms\nstep:273/4500 train_loss:8.0969 train_time:48298ms step_avg:183.64ms\nstep:274/4500 train_loss:7.8692 train_time:48484ms step_avg:183.65ms\nstep:275/4500 train_loss:7.8958 train_time:48667ms step_avg:183.65ms\nstep:276/4500 train_loss:8.0574 train_time:48852ms step_avg:183.65ms\nstep:277/4500 train_loss:8.0249 train_time:49036ms step_avg:183.66ms\nstep:278/4500 train_loss:7.8270 train_time:49221ms step_avg:183.66ms\nstep:279/4500 train_loss:7.9920 train_time:49404ms step_avg:183.66ms\nstep:280/4500 train_loss:7.9552 train_time:49587ms step_avg:183.66ms\nstep:281/4500 train_loss:8.1057 train_time:49772ms step_avg:183.66ms\nstep:282/4500 train_loss:8.0862 train_time:49957ms step_avg:183.67ms\nstep:283/4500 train_loss:7.8990 train_time:50143ms step_avg:183.68ms\nstep:284/4500 train_loss:8.0665 train_time:50329ms step_avg:183.68ms\nstep:285/4500 train_loss:8.1730 train_time:50514ms step_avg:183.69ms\nstep:286/4500 train_loss:8.0146 train_time:50698ms step_avg:183.69ms\nstep:287/4500 train_loss:8.1284 train_time:50882ms step_avg:183.69ms\nstep:288/4500 train_loss:8.0077 train_time:51067ms step_avg:183.69ms\nstep:289/4500 train_loss:7.9481 train_time:51250ms step_avg:183.69ms\nstep:290/4500 train_loss:7.9794 train_time:51434ms step_avg:183.69ms\nstep:291/4500 train_loss:8.0788 train_time:51618ms step_avg:183.69ms\nstep:292/4500 train_loss:8.0372 train_time:51802ms step_avg:183.70ms\nstep:293/4500 train_loss:8.0181 train_time:51986ms step_avg:183.70ms\nstep:294/4500 train_loss:8.0618 train_time:52170ms step_avg:183.70ms\nstep:295/4500 train_loss:7.8447 train_time:52355ms step_avg:183.70ms\nstep:296/4500 train_loss:7.8277 train_time:52540ms step_avg:183.71ms\nstep:297/4500 train_loss:8.0004 train_time:52724ms step_avg:183.71ms\nstep:298/4500 train_loss:8.0488 train_time:52911ms step_avg:183.72ms\nstep:299/4500 train_loss:7.9436 train_time:53094ms step_avg:183.72ms\nstep:300/4500 train_loss:7.9184 train_time:53277ms step_avg:183.71ms\nstep:301/4500 train_loss:7.9953 train_time:53460ms step_avg:183.71ms\nstep:302/4500 train_loss:7.9680 train_time:53642ms step_avg:183.71ms\nstep:303/4500 train_loss:7.8318 train_time:53826ms step_avg:183.71ms\nstep:304/4500 train_loss:7.9369 train_time:54009ms step_avg:183.71ms\nstep:305/4500 train_loss:7.9975 train_time:54195ms step_avg:183.71ms\nstep:306/4500 train_loss:8.0942 train_time:54380ms step_avg:183.72ms\nstep:307/4500 train_loss:8.1240 train_time:54569ms step_avg:183.73ms\nstep:308/4500 train_loss:8.1233 train_time:54756ms step_avg:183.75ms\nstep:309/4500 train_loss:8.0761 train_time:54941ms step_avg:183.75ms\nstep:310/4500 train_loss:8.0353 train_time:55127ms step_avg:183.76ms\nstep:311/4500 train_loss:8.0503 train_time:55312ms step_avg:183.76ms\nstep:312/4500 train_loss:7.9934 train_time:55497ms step_avg:183.77ms\nstep:313/4500 train_loss:8.0453 train_time:55682ms step_avg:183.77ms\nstep:314/4500 train_loss:8.0801 train_time:55867ms step_avg:183.77ms\nstep:315/4500 train_loss:7.8983 train_time:56050ms step_avg:183.77ms\nstep:316/4500 train_loss:8.5228 train_time:56236ms step_avg:183.78ms\nstep:317/4500 train_loss:8.0994 train_time:56421ms step_avg:183.78ms\nstep:318/4500 train_loss:8.3098 train_time:56605ms step_avg:183.78ms\nstep:319/4500 train_loss:8.0011 train_time:56789ms step_avg:183.78ms\nstep:320/4500 train_loss:8.0677 train_time:56974ms step_avg:183.79ms\nstep:321/4500 train_loss:8.1181 train_time:57156ms step_avg:183.78ms\nstep:322/4500 train_loss:8.0187 train_time:57341ms step_avg:183.79ms\nstep:323/4500 train_loss:8.0814 train_time:57526ms step_avg:183.79ms\nstep:324/4500 train_loss:8.1192 train_time:57712ms step_avg:183.80ms\nstep:325/4500 train_loss:8.0345 train_time:57895ms step_avg:183.80ms\nstep:326/4500 train_loss:8.0763 train_time:58080ms step_avg:183.80ms\nstep:327/4500 train_loss:8.0850 train_time:58265ms step_avg:183.80ms\nstep:328/4500 train_loss:8.0432 train_time:58448ms step_avg:183.80ms\nstep:329/4500 train_loss:8.1907 train_time:58634ms step_avg:183.80ms\nstep:330/4500 train_loss:8.1441 train_time:58818ms step_avg:183.81ms\nstep:331/4500 train_loss:8.0240 train_time:59001ms step_avg:183.80ms\nstep:332/4500 train_loss:8.1182 train_time:59185ms step_avg:183.80ms\nstep:333/4500 train_loss:8.1576 train_time:59371ms step_avg:183.81ms\nstep:334/4500 train_loss:8.0398 train_time:59556ms step_avg:183.81ms\nstep:335/4500 train_loss:8.0110 train_time:59739ms step_avg:183.81ms\nstep:336/4500 train_loss:8.0051 train_time:59923ms step_avg:183.81ms\nstep:337/4500 train_loss:8.1323 train_time:60109ms step_avg:183.82ms\nstep:338/4500 train_loss:7.9181 train_time:60292ms step_avg:183.82ms\nstep:339/4500 train_loss:7.9808 train_time:60476ms step_avg:183.82ms\nstep:340/4500 train_loss:8.0645 train_time:60661ms step_avg:183.82ms\nstep:341/4500 train_loss:8.0411 train_time:60848ms step_avg:183.83ms\nstep:342/4500 train_loss:8.0732 train_time:61035ms step_avg:183.84ms\nstep:343/4500 train_loss:8.0219 train_time:61218ms step_avg:183.84ms\nstep:344/4500 train_loss:8.0664 train_time:61403ms step_avg:183.84ms\nstep:345/4500 train_loss:8.0382 train_time:61591ms step_avg:183.85ms\nstep:346/4500 train_loss:8.0772 train_time:61776ms step_avg:183.86ms\nstep:347/4500 train_loss:8.0786 train_time:61961ms step_avg:183.86ms\nstep:348/4500 train_loss:8.0877 train_time:62149ms step_avg:183.87ms\nstep:349/4500 train_loss:7.9825 train_time:62330ms step_avg:183.87ms\nstep:350/4500 train_loss:8.1357 train_time:62515ms step_avg:183.87ms\nstep:351/4500 train_loss:7.9679 train_time:62701ms step_avg:183.87ms\nstep:352/4500 train_loss:8.0683 train_time:62886ms step_avg:183.88ms\nstep:353/4500 train_loss:7.9129 train_time:63073ms step_avg:183.88ms\nstep:354/4500 train_loss:8.1300 train_time:63258ms step_avg:183.89ms\nstep:355/4500 train_loss:8.0169 train_time:63445ms step_avg:183.90ms\nstep:356/4500 train_loss:8.0661 train_time:63632ms step_avg:183.91ms\nstep:357/4500 train_loss:7.7906 train_time:63816ms step_avg:183.91ms\nstep:358/4500 train_loss:8.1243 train_time:64000ms step_avg:183.91ms\nstep:359/4500 train_loss:8.1093 train_time:64184ms step_avg:183.91ms\nstep:360/4500 train_loss:7.8286 train_time:64370ms step_avg:183.91ms\nstep:361/4500 train_loss:7.9449 train_time:64552ms step_avg:183.91ms\nstep:362/4500 train_loss:8.0688 train_time:64738ms step_avg:183.91ms\nstep:363/4500 train_loss:8.0202 train_time:64924ms step_avg:183.92ms\nstep:364/4500 train_loss:8.0522 train_time:65108ms step_avg:183.92ms\nstep:365/4500 train_loss:8.0637 train_time:65292ms step_avg:183.92ms\nstep:366/4500 train_loss:8.2160 train_time:65476ms step_avg:183.92ms\nstep:367/4500 train_loss:8.4067 train_time:65659ms step_avg:183.92ms\nstep:368/4500 train_loss:8.0586 train_time:65843ms step_avg:183.92ms\nstep:369/4500 train_loss:8.2342 train_time:66029ms step_avg:183.93ms\nstep:370/4500 train_loss:8.0745 train_time:66215ms step_avg:183.93ms\nstep:371/4500 train_loss:8.0640 train_time:66400ms step_avg:183.93ms\nstep:372/4500 train_loss:8.2697 train_time:66586ms step_avg:183.94ms\nstep:373/4500 train_loss:8.0390 train_time:66770ms step_avg:183.94ms\nstep:374/4500 train_loss:8.0672 train_time:66956ms step_avg:183.94ms\nstep:375/4500 train_loss:8.0864 train_time:67141ms step_avg:183.95ms\nstep:376/4500 train_loss:8.1994 train_time:67326ms step_avg:183.95ms\nstep:377/4500 train_loss:8.0000 train_time:67513ms step_avg:183.96ms\nstep:378/4500 train_loss:8.0860 train_time:67698ms step_avg:183.96ms\nstep:379/4500 train_loss:8.1000 train_time:67884ms step_avg:183.97ms\nstep:380/4500 train_loss:7.9639 train_time:68070ms step_avg:183.97ms\nstep:381/4500 train_loss:8.2275 train_time:68253ms step_avg:183.97ms\nstep:382/4500 train_loss:7.9662 train_time:68438ms step_avg:183.97ms\nstep:383/4500 train_loss:8.0812 train_time:68621ms step_avg:183.97ms\nstep:384/4500 train_loss:8.0710 train_time:68804ms step_avg:183.97ms\nstep:385/4500 train_loss:7.9449 train_time:68989ms step_avg:183.97ms\nstep:386/4500 train_loss:8.1003 train_time:69173ms step_avg:183.97ms\nstep:387/4500 train_loss:8.0310 train_time:69357ms step_avg:183.97ms\nstep:388/4500 train_loss:8.1524 train_time:69544ms step_avg:183.98ms\nstep:389/4500 train_loss:8.0525 train_time:69728ms step_avg:183.98ms\nstep:390/4500 train_loss:7.9808 train_time:69913ms step_avg:183.98ms\nstep:391/4500 train_loss:8.1512 train_time:70096ms step_avg:183.98ms\nstep:392/4500 train_loss:8.0538 train_time:70280ms step_avg:183.98ms\nstep:393/4500 train_loss:8.0321 train_time:70466ms step_avg:183.98ms\nstep:394/4500 train_loss:8.1753 train_time:70653ms step_avg:183.99ms\nstep:395/4500 train_loss:8.0008 train_time:70839ms step_avg:184.00ms\nstep:396/4500 train_loss:8.0247 train_time:71025ms step_avg:184.00ms\nstep:397/4500 train_loss:8.1337 train_time:71208ms step_avg:184.00ms\nstep:398/4500 train_loss:8.1071 train_time:71391ms step_avg:184.00ms\nstep:399/4500 train_loss:7.9928 train_time:71576ms step_avg:184.00ms\nstep:400/4500 train_loss:7.9538 train_time:71765ms step_avg:184.01ms\nstep:401/4500 train_loss:7.9643 train_time:71950ms step_avg:184.01ms\nstep:402/4500 train_loss:8.0550 train_time:72135ms step_avg:184.02ms\nstep:403/4500 train_loss:8.3929 train_time:72319ms step_avg:184.02ms\nstep:404/4500 train_loss:8.0241 train_time:72502ms step_avg:184.02ms\nstep:405/4500 train_loss:8.1575 train_time:72687ms step_avg:184.02ms\nstep:406/4500 train_loss:8.1120 train_time:72873ms step_avg:184.02ms\nstep:407/4500 train_loss:8.0955 train_time:73057ms step_avg:184.02ms\nstep:408/4500 train_loss:8.0553 train_time:73243ms step_avg:184.03ms\nstep:409/4500 train_loss:8.1627 train_time:73428ms step_avg:184.03ms\nstep:410/4500 train_loss:8.1044 train_time:73610ms step_avg:184.03ms\nstep:411/4500 train_loss:8.0915 train_time:73794ms step_avg:184.03ms\nstep:412/4500 train_loss:7.9035 train_time:73977ms step_avg:184.02ms\nstep:413/4500 train_loss:8.0568 train_time:74162ms step_avg:184.02ms\nstep:414/4500 train_loss:7.9190 train_time:74347ms step_avg:184.03ms\nstep:415/4500 train_loss:8.3285 train_time:74531ms step_avg:184.03ms\nstep:416/4500 train_loss:8.0564 train_time:74716ms step_avg:184.03ms\nstep:417/4500 train_loss:7.9612 train_time:74902ms step_avg:184.03ms\nstep:418/4500 train_loss:8.0999 train_time:75089ms step_avg:184.04ms\nstep:419/4500 train_loss:8.1413 train_time:75272ms step_avg:184.04ms\nstep:420/4500 train_loss:8.1733 train_time:75456ms step_avg:184.04ms\nstep:421/4500 train_loss:8.0209 train_time:75641ms step_avg:184.04ms\nstep:422/4500 train_loss:8.3654 train_time:75826ms step_avg:184.04ms\nstep:423/4500 train_loss:8.1700 train_time:76012ms step_avg:184.05ms\nstep:424/4500 train_loss:8.2363 train_time:76197ms step_avg:184.05ms\nstep:425/4500 train_loss:8.0863 train_time:76381ms step_avg:184.05ms\nstep:426/4500 train_loss:8.0896 train_time:76565ms step_avg:184.05ms\nstep:427/4500 train_loss:8.2419 train_time:76752ms step_avg:184.06ms\nstep:428/4500 train_loss:8.0686 train_time:76934ms step_avg:184.05ms\nstep:429/4500 train_loss:8.1123 train_time:77117ms step_avg:184.05ms\nstep:430/4500 train_loss:7.8643 train_time:77302ms step_avg:184.05ms\nstep:431/4500 train_loss:8.0213 train_time:77488ms step_avg:184.06ms\nstep:432/4500 train_loss:8.0388 train_time:77673ms step_avg:184.06ms\nstep:433/4500 train_loss:8.2513 train_time:77858ms step_avg:184.06ms\nstep:434/4500 train_loss:8.1579 train_time:78042ms step_avg:184.06ms\nstep:435/4500 train_loss:8.1569 train_time:78229ms step_avg:184.07ms\nstep:436/4500 train_loss:8.1806 train_time:78413ms step_avg:184.07ms\nstep:437/4500 train_loss:8.1045 train_time:78597ms step_avg:184.07ms\nstep:438/4500 train_loss:8.1580 train_time:78781ms step_avg:184.07ms\nstep:439/4500 train_loss:8.0825 train_time:78965ms step_avg:184.07ms\nstep:440/4500 train_loss:8.1958 train_time:79150ms step_avg:184.07ms\nstep:441/4500 train_loss:8.1353 train_time:79334ms step_avg:184.07ms\nstep:442/4500 train_loss:8.1398 train_time:79519ms step_avg:184.07ms\nstep:443/4500 train_loss:8.0810 train_time:79710ms step_avg:184.09ms\nstep:444/4500 train_loss:8.1294 train_time:79893ms step_avg:184.08ms\nstep:445/4500 train_loss:8.1811 train_time:80075ms step_avg:184.08ms\nstep:446/4500 train_loss:8.1638 train_time:80259ms step_avg:184.08ms\nstep:447/4500 train_loss:8.1691 train_time:80445ms step_avg:184.08ms\nstep:448/4500 train_loss:8.0827 train_time:80628ms step_avg:184.08ms\nstep:449/4500 train_loss:8.1581 train_time:80813ms step_avg:184.08ms\nstep:450/4500 train_loss:8.1671 train_time:80998ms step_avg:184.09ms\nstep:451/4500 train_loss:8.1489 train_time:81182ms step_avg:184.09ms\nstep:452/4500 train_loss:7.9558 train_time:81366ms step_avg:184.09ms\nstep:453/4500 train_loss:8.1846 train_time:81553ms step_avg:184.09ms\nstep:454/4500 train_loss:8.0457 train_time:81739ms step_avg:184.10ms\nstep:455/4500 train_loss:8.1201 train_time:81926ms step_avg:184.10ms\nstep:456/4500 train_loss:8.1594 train_time:82110ms step_avg:184.10ms\nstep:457/4500 train_loss:8.1056 train_time:82296ms step_avg:184.11ms\nstep:458/4500 train_loss:8.2077 train_time:82482ms step_avg:184.11ms\nstep:459/4500 train_loss:8.1162 train_time:82666ms step_avg:184.11ms\nstep:460/4500 train_loss:8.0615 train_time:82851ms step_avg:184.11ms\nstep:461/4500 train_loss:8.0723 train_time:83035ms step_avg:184.11ms\nstep:462/4500 train_loss:8.0019 train_time:83220ms step_avg:184.12ms\nstep:463/4500 train_loss:8.0765 train_time:83404ms step_avg:184.12ms\nstep:464/4500 train_loss:8.1901 train_time:83591ms step_avg:184.12ms\nstep:465/4500 train_loss:8.1576 train_time:83775ms step_avg:184.12ms\nstep:466/4500 train_loss:8.0749 train_time:83960ms step_avg:184.12ms\nstep:467/4500 train_loss:8.1533 train_time:84143ms step_avg:184.12ms\nstep:468/4500 train_loss:8.0447 train_time:84329ms step_avg:184.12ms\nstep:469/4500 train_loss:8.0570 train_time:84515ms step_avg:184.13ms\nstep:470/4500 train_loss:8.0078 train_time:84701ms step_avg:184.13ms\nstep:471/4500 train_loss:8.1228 train_time:84885ms step_avg:184.13ms\nstep:472/4500 train_loss:8.0860 train_time:85072ms step_avg:184.14ms\nstep:473/4500 train_loss:7.9986 train_time:85256ms step_avg:184.14ms\nstep:474/4500 train_loss:8.2057 train_time:85439ms step_avg:184.14ms\nstep:475/4500 train_loss:8.1495 train_time:85623ms step_avg:184.14ms\nstep:476/4500 train_loss:8.1772 train_time:85811ms step_avg:184.14ms\nstep:477/4500 train_loss:8.2046 train_time:85995ms step_avg:184.14ms\nstep:478/4500 train_loss:8.1307 train_time:86179ms step_avg:184.14ms\nstep:479/4500 train_loss:8.0934 train_time:86363ms step_avg:184.14ms\nstep:480/4500 train_loss:8.1670 train_time:86548ms step_avg:184.14ms\nstep:481/4500 train_loss:8.0555 train_time:86733ms step_avg:184.15ms\nstep:482/4500 train_loss:8.0580 train_time:86917ms step_avg:184.15ms\nstep:483/4500 train_loss:8.1024 train_time:87102ms step_avg:184.15ms\nstep:484/4500 train_loss:8.1458 train_time:87286ms step_avg:184.15ms\nstep:485/4500 train_loss:8.1417 train_time:87471ms step_avg:184.15ms\nstep:486/4500 train_loss:8.0854 train_time:87656ms step_avg:184.15ms\nstep:487/4500 train_loss:8.0786 train_time:87844ms step_avg:184.16ms\nstep:488/4500 train_loss:8.0868 train_time:88033ms step_avg:184.17ms\nstep:489/4500 train_loss:7.9817 train_time:88218ms step_avg:184.17ms\nstep:490/4500 train_loss:7.9848 train_time:88402ms step_avg:184.17ms\nstep:491/4500 train_loss:8.0811 train_time:88586ms step_avg:184.17ms\nstep:492/4500 train_loss:8.1616 train_time:88769ms step_avg:184.17ms\nstep:493/4500 train_loss:8.1297 train_time:88953ms step_avg:184.17ms\nstep:494/4500 train_loss:8.2568 train_time:89139ms step_avg:184.17ms\nstep:495/4500 train_loss:8.0870 train_time:89324ms step_avg:184.17ms\nstep:496/4500 train_loss:8.1439 train_time:89507ms step_avg:184.17ms\nstep:497/4500 train_loss:8.1496 train_time:89691ms step_avg:184.17ms\nstep:498/4500 train_loss:8.0893 train_time:89874ms step_avg:184.17ms\nstep:499/4500 train_loss:8.1082 train_time:90059ms step_avg:184.17ms\nstep:500/4500 train_loss:7.9786 train_time:90248ms step_avg:184.18ms\nstep:501/4500 train_loss:8.0639 train_time:90432ms step_avg:184.18ms\nstep:502/4500 train_loss:8.1542 train_time:90617ms step_avg:184.18ms\nstep:503/4500 train_loss:8.0197 train_time:90804ms step_avg:184.19ms\nstep:504/4500 train_loss:7.9996 train_time:90987ms step_avg:184.18ms\nstep:505/4500 train_loss:8.1452 train_time:91169ms step_avg:184.18ms\nstep:506/4500 train_loss:8.1633 train_time:91352ms step_avg:184.18ms\nstep:507/4500 train_loss:8.2188 train_time:91535ms step_avg:184.18ms\nstep:508/4500 train_loss:8.1053 train_time:91721ms step_avg:184.18ms\nstep:509/4500 train_loss:8.2613 train_time:91903ms step_avg:184.18ms\nstep:510/4500 train_loss:8.1186 train_time:92087ms step_avg:184.17ms\nstep:511/4500 train_loss:8.1441 train_time:92274ms step_avg:184.18ms\nstep:512/4500 train_loss:8.1633 train_time:92458ms step_avg:184.18ms\nstep:513/4500 train_loss:8.1388 train_time:92645ms step_avg:184.18ms\nstep:514/4500 train_loss:8.1943 train_time:92829ms step_avg:184.19ms\nstep:515/4500 train_loss:8.1012 train_time:93016ms step_avg:184.19ms\nstep:516/4500 train_loss:8.1583 train_time:93200ms step_avg:184.19ms\nstep:517/4500 train_loss:8.0673 train_time:93385ms step_avg:184.19ms\nstep:518/4500 train_loss:8.1085 train_time:93572ms step_avg:184.20ms\nstep:519/4500 train_loss:8.0331 train_time:93756ms step_avg:184.20ms\nstep:520/4500 train_loss:8.1438 train_time:93942ms step_avg:184.20ms\nstep:521/4500 train_loss:8.0834 train_time:94128ms step_avg:184.20ms\nstep:522/4500 train_loss:8.2539 train_time:94314ms step_avg:184.21ms\nstep:523/4500 train_loss:8.1510 train_time:94498ms step_avg:184.21ms\nstep:524/4500 train_loss:8.2806 train_time:94682ms step_avg:184.21ms\nstep:525/4500 train_loss:8.4331 train_time:94868ms step_avg:184.21ms\nstep:526/4500 train_loss:8.1520 train_time:95053ms step_avg:184.21ms\nstep:527/4500 train_loss:8.1237 train_time:95236ms step_avg:184.21ms\nstep:528/4500 train_loss:8.0717 train_time:95424ms step_avg:184.22ms\nstep:529/4500 train_loss:8.1023 train_time:95614ms step_avg:184.23ms\nstep:530/4500 train_loss:8.1035 train_time:95798ms step_avg:184.23ms\nstep:531/4500 train_loss:8.1330 train_time:95983ms step_avg:184.23ms\nstep:532/4500 train_loss:8.0866 train_time:96167ms step_avg:184.23ms\nstep:533/4500 train_loss:8.0886 train_time:96354ms step_avg:184.23ms\nstep:534/4500 train_loss:8.1219 train_time:96537ms step_avg:184.23ms\nstep:535/4500 train_loss:8.1164 train_time:96723ms step_avg:184.23ms\nstep:536/4500 train_loss:8.0861 train_time:96908ms step_avg:184.24ms\nstep:537/4500 train_loss:8.2090 train_time:97095ms step_avg:184.24ms\nstep:538/4500 train_loss:8.1252 train_time:97278ms step_avg:184.24ms\nstep:539/4500 train_loss:8.1497 train_time:97462ms step_avg:184.24ms\nstep:540/4500 train_loss:8.1366 train_time:97647ms step_avg:184.24ms\nstep:541/4500 train_loss:8.0689 train_time:97830ms step_avg:184.24ms\nstep:542/4500 train_loss:8.0870 train_time:98016ms step_avg:184.24ms\nstep:543/4500 train_loss:8.1505 train_time:98200ms step_avg:184.24ms\nstep:544/4500 train_loss:8.0641 train_time:98385ms step_avg:184.24ms\nstep:545/4500 train_loss:8.0863 train_time:98569ms step_avg:184.24ms\nstep:546/4500 train_loss:8.0868 train_time:98753ms step_avg:184.24ms\nstep:547/4500 train_loss:8.0490 train_time:98937ms step_avg:184.24ms\nstep:548/4500 train_loss:8.0982 train_time:99120ms step_avg:184.24ms\nstep:549/4500 train_loss:8.0737 train_time:99304ms step_avg:184.24ms\nstep:550/4500 train_loss:8.1144 train_time:99489ms step_avg:184.24ms\nstep:551/4500 train_loss:8.1387 train_time:99673ms step_avg:184.24ms\nstep:552/4500 train_loss:8.0942 train_time:99859ms step_avg:184.24ms\nstep:553/4500 train_loss:8.0584 train_time:100046ms step_avg:184.25ms\nstep:554/4500 train_loss:8.1696 train_time:100232ms step_avg:184.25ms\nstep:555/4500 train_loss:8.0498 train_time:100417ms step_avg:184.25ms\nstep:556/4500 train_loss:8.0686 train_time:100602ms step_avg:184.25ms\nstep:557/4500 train_loss:8.0479 train_time:100787ms step_avg:184.25ms\nstep:558/4500 train_loss:8.0280 train_time:100975ms step_avg:184.26ms\nstep:559/4500 train_loss:8.0915 train_time:101161ms step_avg:184.26ms\nstep:560/4500 train_loss:8.0408 train_time:101347ms step_avg:184.27ms\nstep:561/4500 train_loss:8.1786 train_time:101534ms step_avg:184.27ms\nstep:562/4500 train_loss:8.0260 train_time:101719ms step_avg:184.27ms\nstep:563/4500 train_loss:8.0031 train_time:101905ms step_avg:184.28ms\nstep:564/4500 train_loss:8.0635 train_time:102091ms step_avg:184.28ms\nstep:565/4500 train_loss:8.2048 train_time:102274ms step_avg:184.28ms\nstep:566/4500 train_loss:8.1715 train_time:102458ms step_avg:184.28ms\nstep:567/4500 train_loss:8.2078 train_time:102642ms step_avg:184.28ms\nstep:568/4500 train_loss:8.0204 train_time:102827ms step_avg:184.28ms\nstep:569/4500 train_loss:8.2158 train_time:103014ms step_avg:184.28ms\nstep:570/4500 train_loss:7.9945 train_time:103199ms step_avg:184.28ms\nstep:571/4500 train_loss:8.1230 train_time:103386ms step_avg:184.29ms\nstep:572/4500 train_loss:8.1947 train_time:103580ms step_avg:184.31ms\nstep:573/4500 train_loss:8.2004 train_time:103768ms step_avg:184.31ms\nstep:574/4500 train_loss:8.1045 train_time:103954ms step_avg:184.32ms\nstep:575/4500 train_loss:8.0692 train_time:104139ms step_avg:184.32ms\nstep:576/4500 train_loss:8.4048 train_time:104325ms step_avg:184.32ms\nstep:577/4500 train_loss:8.1163 train_time:104510ms step_avg:184.32ms\nstep:578/4500 train_loss:8.0940 train_time:104695ms step_avg:184.32ms\nstep:579/4500 train_loss:8.1492 train_time:104879ms step_avg:184.32ms\nstep:580/4500 train_loss:8.1151 train_time:105064ms step_avg:184.32ms\nstep:581/4500 train_loss:8.1011 train_time:105248ms step_avg:184.32ms\nstep:582/4500 train_loss:8.1132 train_time:105433ms step_avg:184.32ms\nstep:583/4500 train_loss:8.0958 train_time:105618ms step_avg:184.32ms\nstep:584/4500 train_loss:8.1691 train_time:105806ms step_avg:184.33ms\nstep:585/4500 train_loss:8.0370 train_time:105990ms step_avg:184.33ms\nstep:586/4500 train_loss:8.1086 train_time:106174ms step_avg:184.33ms\nstep:587/4500 train_loss:8.0889 train_time:106358ms step_avg:184.33ms\nstep:588/4500 train_loss:8.1288 train_time:106543ms step_avg:184.33ms\nstep:589/4500 train_loss:8.0590 train_time:106727ms step_avg:184.33ms\nstep:590/4500 train_loss:8.0679 train_time:106911ms step_avg:184.33ms\nstep:591/4500 train_loss:8.1036 train_time:107095ms step_avg:184.33ms\nstep:592/4500 train_loss:7.9841 train_time:107280ms step_avg:184.33ms\nstep:593/4500 train_loss:8.1741 train_time:107465ms step_avg:184.33ms\nstep:594/4500 train_loss:8.1952 train_time:107649ms step_avg:184.33ms\nstep:595/4500 train_loss:8.0623 train_time:107834ms step_avg:184.33ms\nstep:596/4500 train_loss:8.2374 train_time:108019ms step_avg:184.33ms\nstep:597/4500 train_loss:8.0992 train_time:108204ms step_avg:184.33ms\nstep:598/4500 train_loss:8.1972 train_time:108388ms step_avg:184.33ms\nstep:599/4500 train_loss:8.1636 train_time:108573ms step_avg:184.33ms\nstep:600/4500 train_loss:8.2630 train_time:108757ms step_avg:184.33ms\nstep:601/4500 train_loss:8.1477 train_time:108945ms step_avg:184.34ms\nstep:602/4500 train_loss:8.1340 train_time:109133ms step_avg:184.35ms\nstep:603/4500 train_loss:8.1811 train_time:109318ms step_avg:184.35ms\nstep:604/4500 train_loss:8.0668 train_time:109503ms step_avg:184.35ms\nstep:605/4500 train_loss:8.0674 train_time:109690ms step_avg:184.35ms\nstep:606/4500 train_loss:8.1282 train_time:109873ms step_avg:184.35ms\nstep:607/4500 train_loss:8.1499 train_time:110057ms step_avg:184.35ms\nstep:608/4500 train_loss:8.1920 train_time:110241ms step_avg:184.35ms\nstep:609/4500 train_loss:8.0626 train_time:110427ms step_avg:184.35ms\nstep:610/4500 train_loss:8.0764 train_time:110610ms step_avg:184.35ms\nstep:611/4500 train_loss:8.2452 train_time:110797ms step_avg:184.35ms\nstep:612/4500 train_loss:8.0552 train_time:110983ms step_avg:184.36ms\nstep:613/4500 train_loss:7.9313 train_time:111173ms step_avg:184.37ms\nstep:614/4500 train_loss:8.1138 train_time:111355ms step_avg:184.36ms\nstep:615/4500 train_loss:8.1034 train_time:111541ms step_avg:184.36ms\nstep:616/4500 train_loss:8.1175 train_time:111727ms step_avg:184.37ms\nstep:617/4500 train_loss:8.0990 train_time:111911ms step_avg:184.37ms\nstep:618/4500 train_loss:8.1764 train_time:112095ms step_avg:184.37ms\nstep:619/4500 train_loss:8.1875 train_time:112280ms step_avg:184.37ms\nstep:620/4500 train_loss:8.1071 train_time:112464ms step_avg:184.37ms\nstep:621/4500 train_loss:8.1417 train_time:112651ms step_avg:184.37ms\nstep:622/4500 train_loss:8.1192 train_time:112837ms step_avg:184.37ms\nstep:623/4500 train_loss:8.1166 train_time:113027ms step_avg:184.38ms\nstep:624/4500 train_loss:8.1549 train_time:113211ms step_avg:184.38ms\nstep:625/4500 train_loss:8.1792 train_time:113394ms step_avg:184.38ms\nstep:626/4500 train_loss:8.2368 train_time:113578ms step_avg:184.38ms\nstep:627/4500 train_loss:8.0625 train_time:113764ms step_avg:184.38ms\nstep:628/4500 train_loss:8.1046 train_time:113952ms step_avg:184.39ms\nstep:629/4500 train_loss:8.1335 train_time:114137ms step_avg:184.39ms\nstep:630/4500 train_loss:8.0583 train_time:114323ms step_avg:184.39ms\nstep:631/4500 train_loss:8.0599 train_time:114508ms step_avg:184.39ms\nstep:632/4500 train_loss:8.1742 train_time:114692ms step_avg:184.39ms\nstep:633/4500 train_loss:8.0692 train_time:114876ms step_avg:184.39ms\nstep:634/4500 train_loss:8.1336 train_time:115060ms step_avg:184.39ms\nstep:635/4500 train_loss:8.1743 train_time:115247ms step_avg:184.40ms\nstep:636/4500 train_loss:8.0247 train_time:115433ms step_avg:184.40ms\nstep:637/4500 train_loss:8.0081 train_time:115617ms step_avg:184.40ms\nstep:638/4500 train_loss:8.0842 train_time:115803ms step_avg:184.40ms\nstep:639/4500 train_loss:8.2227 train_time:115988ms step_avg:184.40ms\nstep:640/4500 train_loss:8.0884 train_time:116172ms step_avg:184.40ms\nstep:641/4500 train_loss:8.1208 train_time:116355ms step_avg:184.40ms\nstep:642/4500 train_loss:8.0707 train_time:116542ms step_avg:184.40ms\nstep:643/4500 train_loss:8.0946 train_time:116729ms step_avg:184.41ms\nstep:644/4500 train_loss:8.1620 train_time:116913ms step_avg:184.41ms\nstep:645/4500 train_loss:8.0173 train_time:117097ms step_avg:184.40ms\nstep:646/4500 train_loss:8.1403 train_time:117282ms step_avg:184.41ms\nstep:647/4500 train_loss:8.1767 train_time:117466ms step_avg:184.41ms\nstep:648/4500 train_loss:8.1463 train_time:117652ms step_avg:184.41ms\nstep:649/4500 train_loss:8.0779 train_time:117838ms step_avg:184.41ms\nstep:650/4500 train_loss:8.1587 train_time:118025ms step_avg:184.41ms\nstep:651/4500 train_loss:8.2945 train_time:118210ms step_avg:184.42ms\nstep:652/4500 train_loss:8.0739 train_time:118393ms step_avg:184.41ms\nstep:653/4500 train_loss:8.0802 train_time:118576ms step_avg:184.41ms\nstep:654/4500 train_loss:8.0531 train_time:118762ms step_avg:184.41ms\nstep:655/4500 train_loss:8.0845 train_time:118946ms step_avg:184.41ms\nstep:656/4500 train_loss:8.0321 train_time:119133ms step_avg:184.42ms\nstep:657/4500 train_loss:8.0288 train_time:119318ms step_avg:184.42ms\nstep:658/4500 train_loss:8.1385 train_time:119506ms step_avg:184.42ms\nstep:659/4500 train_loss:8.1478 train_time:119690ms step_avg:184.42ms\nstep:660/4500 train_loss:8.1357 train_time:119877ms step_avg:184.43ms\nstep:661/4500 train_loss:8.0074 train_time:120061ms step_avg:184.43ms\nstep:662/4500 train_loss:8.2367 train_time:120245ms step_avg:184.43ms\nstep:663/4500 train_loss:8.0929 train_time:120429ms step_avg:184.42ms\nstep:664/4500 train_loss:8.1719 train_time:120612ms step_avg:184.42ms\nstep:665/4500 train_loss:8.1589 train_time:120796ms step_avg:184.42ms\nstep:666/4500 train_loss:8.1182 train_time:120981ms step_avg:184.42ms\nstep:667/4500 train_loss:8.1499 train_time:121166ms step_avg:184.42ms\nstep:668/4500 train_loss:7.9577 train_time:121350ms step_avg:184.42ms\nstep:669/4500 train_loss:7.9923 train_time:121537ms step_avg:184.43ms\nstep:670/4500 train_loss:8.0902 train_time:121722ms step_avg:184.43ms\nstep:671/4500 train_loss:8.1452 train_time:121910ms step_avg:184.43ms\nstep:672/4500 train_loss:8.1341 train_time:122093ms step_avg:184.43ms\nstep:673/4500 train_loss:8.1018 train_time:122278ms step_avg:184.43ms\nstep:674/4500 train_loss:8.0778 train_time:122463ms step_avg:184.43ms\nstep:675/4500 train_loss:8.0419 train_time:122647ms step_avg:184.43ms\nstep:676/4500 train_loss:8.1039 train_time:122831ms step_avg:184.43ms\nstep:677/4500 train_loss:8.1463 train_time:123017ms step_avg:184.43ms\nstep:678/4500 train_loss:8.1612 train_time:123202ms step_avg:184.43ms\nstep:679/4500 train_loss:8.1082 train_time:123388ms step_avg:184.44ms\nstep:680/4500 train_loss:8.1261 train_time:123573ms step_avg:184.44ms\nstep:681/4500 train_loss:8.1386 train_time:123756ms step_avg:184.43ms\nstep:682/4500 train_loss:8.1061 train_time:123940ms step_avg:184.43ms\nstep:683/4500 train_loss:8.2569 train_time:124125ms step_avg:184.44ms\nstep:684/4500 train_loss:7.6260 train_time:124312ms step_avg:184.44ms\nstep:685/4500 train_loss:8.0455 train_time:124497ms step_avg:184.44ms\nstep:686/4500 train_loss:8.1303 train_time:124685ms step_avg:184.44ms\nstep:687/4500 train_loss:8.2172 train_time:124870ms step_avg:184.45ms\nstep:688/4500 train_loss:8.1226 train_time:125052ms step_avg:184.44ms\nstep:689/4500 train_loss:8.1703 train_time:125236ms step_avg:184.44ms\nstep:690/4500 train_loss:8.1444 train_time:125420ms step_avg:184.44ms\nstep:691/4500 train_loss:8.1487 train_time:125604ms step_avg:184.44ms\nstep:692/4500 train_loss:8.0527 train_time:125788ms step_avg:184.44ms\nstep:693/4500 train_loss:8.0512 train_time:125973ms step_avg:184.44ms\nstep:694/4500 train_loss:8.1734 train_time:126158ms step_avg:184.44ms\nstep:695/4500 train_loss:8.0118 train_time:126346ms step_avg:184.45ms\nstep:696/4500 train_loss:8.1647 train_time:126531ms step_avg:184.45ms\nstep:697/4500 train_loss:8.1763 train_time:126715ms step_avg:184.45ms\nstep:698/4500 train_loss:8.1241 train_time:126898ms step_avg:184.45ms\nstep:699/4500 train_loss:8.1064 train_time:127084ms step_avg:184.45ms\nstep:700/4500 train_loss:8.2233 train_time:127269ms step_avg:184.45ms\nstep:701/4500 train_loss:8.2921 train_time:127452ms step_avg:184.45ms\nstep:702/4500 train_loss:8.0747 train_time:127636ms step_avg:184.44ms\nstep:703/4500 train_loss:8.1934 train_time:127820ms step_avg:184.44ms\nstep:704/4500 train_loss:8.1285 train_time:128007ms step_avg:184.45ms\nstep:705/4500 train_loss:8.1013 train_time:128189ms step_avg:184.44ms\nstep:706/4500 train_loss:8.0679 train_time:128372ms step_avg:184.44ms\nstep:707/4500 train_loss:8.0250 train_time:128557ms step_avg:184.44ms\nstep:708/4500 train_loss:8.2637 train_time:128742ms step_avg:184.44ms\nstep:709/4500 train_loss:8.0924 train_time:128926ms step_avg:184.44ms\nstep:710/4500 train_loss:8.0412 train_time:129110ms step_avg:184.44ms\nstep:711/4500 train_loss:7.9604 train_time:129295ms step_avg:184.44ms\nstep:712/4500 train_loss:8.0462 train_time:129479ms step_avg:184.44ms\nstep:713/4500 train_loss:8.3100 train_time:129665ms step_avg:184.44ms\nstep:714/4500 train_loss:8.1528 train_time:129850ms step_avg:184.45ms\nstep:715/4500 train_loss:8.1817 train_time:130034ms step_avg:184.45ms\nstep:716/4500 train_loss:8.0942 train_time:130219ms step_avg:184.45ms\nstep:717/4500 train_loss:8.0910 train_time:130405ms step_avg:184.45ms\nstep:718/4500 train_loss:8.1594 train_time:130590ms step_avg:184.45ms\nstep:719/4500 train_loss:8.2047 train_time:130776ms step_avg:184.45ms\nstep:720/4500 train_loss:8.1425 train_time:130963ms step_avg:184.45ms\nstep:721/4500 train_loss:8.1621 train_time:131147ms step_avg:184.45ms\nstep:722/4500 train_loss:8.1310 train_time:131330ms step_avg:184.45ms\nstep:723/4500 train_loss:8.1015 train_time:131516ms step_avg:184.45ms\nstep:724/4500 train_loss:8.2961 train_time:131702ms step_avg:184.46ms\nstep:725/4500 train_loss:8.0071 train_time:131888ms step_avg:184.46ms\nstep:726/4500 train_loss:8.1287 train_time:132071ms step_avg:184.46ms\nstep:727/4500 train_loss:8.1033 train_time:132253ms step_avg:184.45ms\nstep:728/4500 train_loss:7.9894 train_time:132435ms step_avg:184.45ms\nstep:729/4500 train_loss:8.1659 train_time:132620ms step_avg:184.45ms\nstep:730/4500 train_loss:8.0554 train_time:132804ms step_avg:184.45ms\nstep:731/4500 train_loss:8.0343 train_time:132989ms step_avg:184.45ms\nstep:732/4500 train_loss:8.2024 train_time:133172ms step_avg:184.45ms\nstep:733/4500 train_loss:8.0902 train_time:133357ms step_avg:184.45ms\nstep:734/4500 train_loss:8.0688 train_time:133543ms step_avg:184.45ms\nstep:735/4500 train_loss:8.1081 train_time:133732ms step_avg:184.46ms\nstep:736/4500 train_loss:8.2693 train_time:133916ms step_avg:184.46ms\nstep:737/4500 train_loss:8.0674 train_time:134100ms step_avg:184.46ms\nstep:738/4500 train_loss:8.1479 train_time:134286ms step_avg:184.46ms\nstep:739/4500 train_loss:8.0373 train_time:134472ms step_avg:184.46ms\nstep:740/4500 train_loss:8.1250 train_time:134659ms step_avg:184.46ms\nstep:741/4500 train_loss:8.0808 train_time:134846ms step_avg:184.47ms\nstep:742/4500 train_loss:8.0425 train_time:135031ms step_avg:184.47ms\nstep:743/4500 train_loss:8.0825 train_time:135214ms step_avg:184.47ms\nstep:744/4500 train_loss:8.0640 train_time:135397ms step_avg:184.47ms\nstep:745/4500 train_loss:8.0269 train_time:135581ms step_avg:184.46ms\nstep:746/4500 train_loss:8.3563 train_time:135765ms step_avg:184.46ms\nstep:747/4500 train_loss:8.0965 train_time:135950ms step_avg:184.46ms\nstep:748/4500 train_loss:8.1587 train_time:136133ms step_avg:184.46ms\nstep:749/4500 train_loss:8.2013 train_time:136318ms step_avg:184.46ms\nstep:750/4500 train_loss:8.1028 train_time:136504ms step_avg:184.46ms\nstep:751/4500 train_loss:8.0821 train_time:136688ms step_avg:184.46ms\nstep:752/4500 train_loss:8.1994 train_time:136870ms step_avg:184.46ms\nstep:753/4500 train_loss:8.1835 train_time:137055ms step_avg:184.46ms\nstep:754/4500 train_loss:8.0879 train_time:137238ms step_avg:184.46ms\nstep:755/4500 train_loss:8.1817 train_time:137422ms step_avg:184.46ms\nstep:756/4500 train_loss:7.8842 train_time:137608ms step_avg:184.46ms\nstep:757/4500 train_loss:7.7935 train_time:137792ms step_avg:184.46ms\nstep:758/4500 train_loss:8.2401 train_time:137978ms step_avg:184.46ms\nstep:759/4500 train_loss:8.1512 train_time:138163ms step_avg:184.46ms\nstep:760/4500 train_loss:8.1053 train_time:138347ms step_avg:184.46ms\nstep:761/4500 train_loss:8.0426 train_time:138533ms step_avg:184.47ms\nstep:762/4500 train_loss:8.2037 train_time:138916ms step_avg:184.73ms\nstep:763/4500 train_loss:8.1454 train_time:139099ms step_avg:184.73ms\nstep:764/4500 train_loss:8.0599 train_time:139283ms step_avg:184.73ms\nstep:765/4500 train_loss:8.0573 train_time:139465ms step_avg:184.72ms\nstep:766/4500 train_loss:8.2219 train_time:139650ms step_avg:184.72ms\nstep:767/4500 train_loss:8.1274 train_time:139836ms step_avg:184.72ms\nstep:768/4500 train_loss:8.0756 train_time:140023ms step_avg:184.73ms\nstep:769/4500 train_loss:8.1335 train_time:140208ms step_avg:184.73ms\nstep:770/4500 train_loss:8.0877 train_time:140391ms step_avg:184.73ms\nstep:771/4500 train_loss:8.1683 train_time:140574ms step_avg:184.72ms\nstep:772/4500 train_loss:8.0688 train_time:140758ms step_avg:184.72ms\nstep:773/4500 train_loss:7.8194 train_time:140944ms step_avg:184.72ms\nstep:774/4500 train_loss:8.0938 train_time:141130ms step_avg:184.73ms\nstep:775/4500 train_loss:8.1884 train_time:141315ms step_avg:184.73ms\nstep:776/4500 train_loss:8.1235 train_time:141500ms step_avg:184.73ms\nstep:777/4500 train_loss:8.1826 train_time:141685ms step_avg:184.73ms\nstep:778/4500 train_loss:8.1803 train_time:141870ms step_avg:184.73ms\nstep:779/4500 train_loss:8.1205 train_time:142058ms step_avg:184.73ms\nstep:780/4500 train_loss:8.2866 train_time:142245ms step_avg:184.73ms\nstep:781/4500 train_loss:8.0929 train_time:142434ms step_avg:184.74ms\nstep:782/4500 train_loss:8.0550 train_time:142617ms step_avg:184.74ms\nstep:783/4500 train_loss:8.1019 train_time:142802ms step_avg:184.74ms\nstep:784/4500 train_loss:8.1998 train_time:142988ms step_avg:184.74ms\nstep:785/4500 train_loss:8.0395 train_time:143170ms step_avg:184.74ms\nstep:786/4500 train_loss:8.1966 train_time:143357ms step_avg:184.74ms\nstep:787/4500 train_loss:8.1902 train_time:143544ms step_avg:184.74ms\nstep:788/4500 train_loss:8.1125 train_time:143727ms step_avg:184.74ms\nstep:789/4500 train_loss:8.0724 train_time:143910ms step_avg:184.74ms\nstep:790/4500 train_loss:8.1033 train_time:144093ms step_avg:184.73ms\nstep:791/4500 train_loss:8.2247 train_time:144277ms step_avg:184.73ms\nstep:792/4500 train_loss:8.0994 train_time:144461ms step_avg:184.73ms\nstep:793/4500 train_loss:8.0109 train_time:144649ms step_avg:184.74ms\nstep:794/4500 train_loss:8.1981 train_time:144832ms step_avg:184.73ms\nstep:795/4500 train_loss:8.1012 train_time:145015ms step_avg:184.73ms\nstep:796/4500 train_loss:8.0705 train_time:145200ms step_avg:184.73ms\nstep:797/4500 train_loss:8.0858 train_time:145386ms step_avg:184.73ms\nstep:798/4500 train_loss:8.1330 train_time:145570ms step_avg:184.73ms\nstep:799/4500 train_loss:8.2735 train_time:145756ms step_avg:184.73ms\nstep:800/4500 train_loss:8.1454 train_time:145941ms step_avg:184.74ms\nstep:801/4500 train_loss:8.0222 train_time:146129ms step_avg:184.74ms\nstep:802/4500 train_loss:8.3427 train_time:146313ms step_avg:184.74ms\nstep:803/4500 train_loss:8.2394 train_time:146497ms step_avg:184.74ms\nstep:804/4500 train_loss:8.1359 train_time:146682ms step_avg:184.74ms\nstep:805/4500 train_loss:8.2090 train_time:146868ms step_avg:184.74ms\nstep:806/4500 train_loss:8.1796 train_time:147052ms step_avg:184.74ms\nstep:807/4500 train_loss:8.1220 train_time:147237ms step_avg:184.74ms\nstep:808/4500 train_loss:8.1686 train_time:147424ms step_avg:184.74ms\nstep:809/4500 train_loss:8.1901 train_time:147613ms step_avg:184.75ms\nstep:810/4500 train_loss:8.1086 train_time:147797ms step_avg:184.75ms\nstep:811/4500 train_loss:8.1258 train_time:147981ms step_avg:184.75ms\nstep:812/4500 train_loss:8.1897 train_time:148165ms step_avg:184.74ms\nstep:813/4500 train_loss:8.2171 train_time:148351ms step_avg:184.75ms\nstep:814/4500 train_loss:8.0874 train_time:148536ms step_avg:184.75ms\nstep:815/4500 train_loss:8.0937 train_time:148722ms step_avg:184.75ms\nstep:816/4500 train_loss:8.0663 train_time:148906ms step_avg:184.75ms\nstep:817/4500 train_loss:8.0841 train_time:149091ms step_avg:184.75ms\nstep:818/4500 train_loss:8.0828 train_time:149276ms step_avg:184.75ms\nstep:819/4500 train_loss:8.1696 train_time:149460ms step_avg:184.75ms\nstep:820/4500 train_loss:8.2856 train_time:149649ms step_avg:184.75ms\nstep:821/4500 train_loss:8.1899 train_time:149834ms step_avg:184.75ms\nstep:822/4500 train_loss:8.1041 train_time:150018ms step_avg:184.75ms\nstep:823/4500 train_loss:8.1189 train_time:150202ms step_avg:184.75ms\nstep:824/4500 train_loss:8.0628 train_time:150387ms step_avg:184.75ms\nstep:825/4500 train_loss:8.1335 train_time:150570ms step_avg:184.75ms\nstep:826/4500 train_loss:8.2433 train_time:150754ms step_avg:184.75ms\nstep:827/4500 train_loss:8.2015 train_time:150940ms step_avg:184.75ms\nstep:828/4500 train_loss:8.1308 train_time:151126ms step_avg:184.75ms\nstep:829/4500 train_loss:8.0333 train_time:151310ms step_avg:184.75ms\nstep:830/4500 train_loss:8.0696 train_time:151495ms step_avg:184.75ms\nstep:831/4500 train_loss:8.0438 train_time:151680ms step_avg:184.75ms\nstep:832/4500 train_loss:8.0950 train_time:151864ms step_avg:184.75ms\nstep:833/4500 train_loss:8.0851 train_time:152048ms step_avg:184.75ms\nstep:834/4500 train_loss:8.1000 train_time:152234ms step_avg:184.75ms\nstep:835/4500 train_loss:7.9519 train_time:152418ms step_avg:184.75ms\nstep:836/4500 train_loss:8.2125 train_time:152602ms step_avg:184.75ms\nstep:837/4500 train_loss:8.0938 train_time:152792ms step_avg:184.75ms\nstep:838/4500 train_loss:8.1008 train_time:152977ms step_avg:184.75ms\nstep:839/4500 train_loss:8.1954 train_time:153161ms step_avg:184.75ms\nstep:840/4500 train_loss:7.9489 train_time:153347ms step_avg:184.76ms\nstep:841/4500 train_loss:8.1463 train_time:153530ms step_avg:184.75ms\nstep:842/4500 train_loss:8.0426 train_time:153713ms step_avg:184.75ms\nstep:843/4500 train_loss:8.1326 train_time:153897ms step_avg:184.75ms\nstep:844/4500 train_loss:8.0769 train_time:154083ms step_avg:184.75ms\nstep:845/4500 train_loss:8.1630 train_time:154268ms step_avg:184.75ms\nstep:846/4500 train_loss:8.0608 train_time:154455ms step_avg:184.75ms\nstep:847/4500 train_loss:8.1855 train_time:154641ms step_avg:184.76ms\nstep:848/4500 train_loss:8.4010 train_time:154827ms step_avg:184.76ms\nstep:849/4500 train_loss:8.0701 train_time:155015ms step_avg:184.76ms\nstep:850/4500 train_loss:8.0698 train_time:155199ms step_avg:184.76ms\nstep:851/4500 train_loss:8.3041 train_time:155383ms step_avg:184.76ms\nstep:852/4500 train_loss:8.0423 train_time:155570ms step_avg:184.76ms\nstep:853/4500 train_loss:8.1213 train_time:155754ms step_avg:184.76ms\nstep:854/4500 train_loss:8.1298 train_time:155937ms step_avg:184.76ms\nstep:855/4500 train_loss:8.1830 train_time:156122ms step_avg:184.76ms\nstep:856/4500 train_loss:8.1435 train_time:156307ms step_avg:184.76ms\nstep:857/4500 train_loss:8.0072 train_time:156494ms step_avg:184.76ms\nstep:858/4500 train_loss:8.1706 train_time:156679ms step_avg:184.76ms\nstep:859/4500 train_loss:8.2117 train_time:156868ms step_avg:184.77ms\nstep:860/4500 train_loss:8.0289 train_time:157052ms step_avg:184.77ms\nstep:861/4500 train_loss:8.3464 train_time:157235ms step_avg:184.76ms\nstep:862/4500 train_loss:8.0679 train_time:157418ms step_avg:184.76ms\nstep:863/4500 train_loss:8.1048 train_time:157602ms step_avg:184.76ms\nstep:864/4500 train_loss:8.1924 train_time:157786ms step_avg:184.76ms\nstep:865/4500 train_loss:8.0674 train_time:157973ms step_avg:184.76ms\nstep:866/4500 train_loss:8.1586 train_time:158156ms step_avg:184.76ms\nstep:867/4500 train_loss:8.0911 train_time:158342ms step_avg:184.76ms\nstep:868/4500 train_loss:8.1038 train_time:158528ms step_avg:184.76ms\nstep:869/4500 train_loss:8.0445 train_time:158714ms step_avg:184.77ms\nstep:870/4500 train_loss:8.1081 train_time:158899ms step_avg:184.77ms\nstep:871/4500 train_loss:8.0679 train_time:159084ms step_avg:184.77ms\nstep:872/4500 train_loss:8.0513 train_time:159269ms step_avg:184.77ms\nstep:873/4500 train_loss:8.0087 train_time:159453ms step_avg:184.77ms\nstep:874/4500 train_loss:7.9856 train_time:159636ms step_avg:184.76ms\nstep:875/4500 train_loss:8.1825 train_time:159821ms step_avg:184.76ms\nstep:876/4500 train_loss:8.1226 train_time:160007ms step_avg:184.77ms\nstep:877/4500 train_loss:8.1583 train_time:160190ms step_avg:184.76ms\nstep:878/4500 train_loss:8.0750 train_time:160375ms step_avg:184.76ms\nstep:879/4500 train_loss:7.9606 train_time:160562ms step_avg:184.77ms\nstep:880/4500 train_loss:8.0810 train_time:160746ms step_avg:184.77ms\nstep:881/4500 train_loss:8.0939 train_time:160929ms step_avg:184.76ms\nstep:882/4500 train_loss:8.2202 train_time:161115ms step_avg:184.77ms\nstep:883/4500 train_loss:8.0799 train_time:161301ms step_avg:184.77ms\nstep:884/4500 train_loss:8.1849 train_time:161486ms step_avg:184.77ms\nstep:885/4500 train_loss:8.0440 train_time:161670ms step_avg:184.77ms\nstep:886/4500 train_loss:8.1579 train_time:161854ms step_avg:184.76ms\nstep:887/4500 train_loss:8.0940 train_time:162036ms step_avg:184.76ms\nstep:888/4500 train_loss:8.1924 train_time:162220ms step_avg:184.76ms\nstep:889/4500 train_loss:8.1390 train_time:162406ms step_avg:184.76ms\nstep:890/4500 train_loss:8.1521 train_time:162590ms step_avg:184.76ms\nstep:891/4500 train_loss:8.1142 train_time:162774ms step_avg:184.76ms\nstep:892/4500 train_loss:8.1808 train_time:162962ms step_avg:184.76ms\nstep:893/4500 train_loss:8.1633 train_time:163150ms step_avg:184.77ms\nstep:894/4500 train_loss:8.0768 train_time:163334ms step_avg:184.77ms\nstep:895/4500 train_loss:8.1904 train_time:163517ms step_avg:184.77ms\nstep:896/4500 train_loss:8.1449 train_time:163701ms step_avg:184.76ms\nstep:897/4500 train_loss:8.0702 train_time:163886ms step_avg:184.76ms\nstep:898/4500 train_loss:8.1743 train_time:164069ms step_avg:184.76ms\nstep:899/4500 train_loss:8.1899 train_time:164254ms step_avg:184.76ms\nstep:900/4500 train_loss:7.9670 train_time:164439ms step_avg:184.76ms\nstep:901/4500 train_loss:8.1430 train_time:164624ms step_avg:184.76ms\nstep:902/4500 train_loss:8.0198 train_time:164809ms step_avg:184.76ms\nstep:903/4500 train_loss:8.2179 train_time:164993ms step_avg:184.76ms\nstep:904/4500 train_loss:8.0627 train_time:165177ms step_avg:184.76ms\nstep:905/4500 train_loss:8.1469 train_time:165362ms step_avg:184.76ms\nstep:906/4500 train_loss:8.0482 train_time:165546ms step_avg:184.76ms\nstep:907/4500 train_loss:8.0607 train_time:165729ms step_avg:184.76ms\nstep:908/4500 train_loss:8.0386 train_time:165914ms step_avg:184.76ms\nstep:909/4500 train_loss:8.1581 train_time:166097ms step_avg:184.76ms\nstep:910/4500 train_loss:8.0651 train_time:166281ms step_avg:184.76ms\nstep:911/4500 train_loss:8.1654 train_time:166465ms step_avg:184.76ms\nstep:912/4500 train_loss:8.1274 train_time:166652ms step_avg:184.76ms\nstep:913/4500 train_loss:8.0949 train_time:166835ms step_avg:184.76ms\nstep:914/4500 train_loss:8.1928 train_time:167020ms step_avg:184.76ms\nstep:915/4500 train_loss:8.1610 train_time:167205ms step_avg:184.76ms\nstep:916/4500 train_loss:8.1418 train_time:167389ms step_avg:184.76ms\nstep:917/4500 train_loss:8.2036 train_time:167572ms step_avg:184.75ms\nstep:918/4500 train_loss:8.1798 train_time:167756ms step_avg:184.75ms\nstep:919/4500 train_loss:8.3501 train_time:167942ms step_avg:184.75ms\nstep:920/4500 train_loss:8.2216 train_time:168129ms step_avg:184.76ms\nstep:921/4500 train_loss:8.1613 train_time:168314ms step_avg:184.76ms\nstep:922/4500 train_loss:8.0908 train_time:168498ms step_avg:184.76ms\nstep:923/4500 train_loss:8.0231 train_time:168683ms step_avg:184.76ms\nstep:924/4500 train_loss:8.2051 train_time:168868ms step_avg:184.76ms\nstep:925/4500 train_loss:8.0080 train_time:169051ms step_avg:184.76ms\nstep:926/4500 train_loss:8.1969 train_time:169234ms step_avg:184.75ms\nstep:927/4500 train_loss:8.0696 train_time:169421ms step_avg:184.76ms\nstep:928/4500 train_loss:8.0081 train_time:169610ms step_avg:184.76ms\nstep:929/4500 train_loss:8.1298 train_time:169795ms step_avg:184.76ms\nstep:930/4500 train_loss:8.0530 train_time:169980ms step_avg:184.76ms\nstep:931/4500 train_loss:8.0819 train_time:170165ms step_avg:184.76ms\nstep:932/4500 train_loss:8.1511 train_time:170352ms step_avg:184.76ms\nstep:933/4500 train_loss:8.1651 train_time:170535ms step_avg:184.76ms\nstep:934/4500 train_loss:8.0421 train_time:170720ms step_avg:184.76ms\nstep:935/4500 train_loss:8.0749 train_time:170907ms step_avg:184.76ms\nstep:936/4500 train_loss:8.0939 train_time:171094ms step_avg:184.77ms\nstep:937/4500 train_loss:8.1204 train_time:171278ms step_avg:184.77ms\nstep:938/4500 train_loss:8.2941 train_time:171462ms step_avg:184.77ms\nstep:939/4500 train_loss:8.2280 train_time:171646ms step_avg:184.76ms\nstep:940/4500 train_loss:7.8436 train_time:171832ms step_avg:184.77ms\nstep:941/4500 train_loss:8.1766 train_time:172017ms step_avg:184.77ms\nstep:942/4500 train_loss:8.1791 train_time:172205ms step_avg:184.77ms\nstep:943/4500 train_loss:8.0141 train_time:172391ms step_avg:184.77ms\nstep:944/4500 train_loss:8.2454 train_time:172577ms step_avg:184.77ms\nstep:945/4500 train_loss:7.9978 train_time:172762ms step_avg:184.77ms\nstep:946/4500 train_loss:8.2171 train_time:172945ms step_avg:184.77ms\nstep:947/4500 train_loss:8.1689 train_time:173131ms step_avg:184.77ms\nstep:948/4500 train_loss:8.0977 train_time:173316ms step_avg:184.77ms\nstep:949/4500 train_loss:7.9046 train_time:173501ms step_avg:184.77ms\nstep:950/4500 train_loss:8.1524 train_time:173689ms step_avg:184.78ms\nstep:951/4500 train_loss:8.1858 train_time:173874ms step_avg:184.78ms\nstep:952/4500 train_loss:8.0585 train_time:174059ms step_avg:184.78ms\nstep:953/4500 train_loss:8.2861 train_time:174243ms step_avg:184.77ms\nstep:954/4500 train_loss:8.0175 train_time:174429ms step_avg:184.78ms\nstep:955/4500 train_loss:8.1380 train_time:174612ms step_avg:184.77ms\nstep:956/4500 train_loss:8.1593 train_time:174795ms step_avg:184.77ms\nstep:957/4500 train_loss:8.0587 train_time:174979ms step_avg:184.77ms\nstep:958/4500 train_loss:8.1439 train_time:175163ms step_avg:184.77ms\nstep:959/4500 train_loss:8.0649 train_time:175347ms step_avg:184.77ms\nstep:960/4500 train_loss:8.2089 train_time:175533ms step_avg:184.77ms\nstep:961/4500 train_loss:8.0542 train_time:175719ms step_avg:184.77ms\nstep:962/4500 train_loss:8.2994 train_time:175907ms step_avg:184.78ms\nstep:963/4500 train_loss:8.3165 train_time:176092ms step_avg:184.78ms\nstep:964/4500 train_loss:8.1454 train_time:176279ms step_avg:184.78ms\nstep:965/4500 train_loss:8.1687 train_time:176463ms step_avg:184.78ms\nstep:966/4500 train_loss:8.0856 train_time:176648ms step_avg:184.78ms\nstep:967/4500 train_loss:8.1840 train_time:176834ms step_avg:184.78ms\nstep:968/4500 train_loss:8.2602 train_time:177020ms step_avg:184.78ms\nstep:969/4500 train_loss:8.1978 train_time:177205ms step_avg:184.78ms\nstep:970/4500 train_loss:8.2297 train_time:177389ms step_avg:184.78ms\nstep:971/4500 train_loss:8.1542 train_time:177574ms step_avg:184.78ms\nstep:972/4500 train_loss:8.0894 train_time:177757ms step_avg:184.78ms\nstep:973/4500 train_loss:8.0872 train_time:177943ms step_avg:184.78ms\nstep:974/4500 train_loss:8.0581 train_time:178129ms step_avg:184.78ms\nstep:975/4500 train_loss:8.0201 train_time:178315ms step_avg:184.78ms\nstep:976/4500 train_loss:8.0315 train_time:178500ms step_avg:184.78ms\nstep:977/4500 train_loss:8.1293 train_time:178684ms step_avg:184.78ms\nstep:978/4500 train_loss:8.0641 train_time:178867ms step_avg:184.78ms\nstep:979/4500 train_loss:8.1679 train_time:179050ms step_avg:184.78ms\nstep:980/4500 train_loss:8.1182 train_time:179233ms step_avg:184.78ms\nstep:981/4500 train_loss:8.1748 train_time:179418ms step_avg:184.78ms\nstep:982/4500 train_loss:8.0823 train_time:179604ms step_avg:184.78ms\nstep:983/4500 train_loss:8.2154 train_time:179792ms step_avg:184.78ms\nstep:984/4500 train_loss:8.0896 train_time:179975ms step_avg:184.78ms\nstep:985/4500 train_loss:8.0912 train_time:180159ms step_avg:184.78ms\nstep:986/4500 train_loss:8.1520 train_time:180346ms step_avg:184.78ms\nstep:987/4500 train_loss:8.1930 train_time:180530ms step_avg:184.78ms\nstep:988/4500 train_loss:8.1164 train_time:180716ms step_avg:184.78ms\nstep:989/4500 train_loss:8.0140 train_time:180902ms step_avg:184.78ms\nstep:990/4500 train_loss:8.1491 train_time:181090ms step_avg:184.79ms\nstep:991/4500 train_loss:8.1537 train_time:181274ms step_avg:184.79ms\nstep:992/4500 train_loss:8.2835 train_time:181461ms step_avg:184.79ms\nstep:993/4500 train_loss:8.1277 train_time:181645ms step_avg:184.79ms\nstep:994/4500 train_loss:8.0602 train_time:181828ms step_avg:184.78ms\nstep:995/4500 train_loss:8.1574 train_time:182016ms step_avg:184.79ms\nstep:996/4500 train_loss:8.0856 train_time:182200ms step_avg:184.79ms\nstep:997/4500 train_loss:8.1439 train_time:182387ms step_avg:184.79ms\nstep:998/4500 train_loss:8.1521 train_time:182571ms step_avg:184.79ms\nstep:999/4500 train_loss:8.0917 train_time:182755ms step_avg:184.79ms\nstep:1000/4500 train_loss:8.0606 train_time:182939ms step_avg:184.79ms\nstep:1001/4500 train_loss:8.0881 train_time:183126ms step_avg:184.79ms\nstep:1002/4500 train_loss:8.1214 train_time:183314ms step_avg:184.79ms\nstep:1003/4500 train_loss:7.9985 train_time:183498ms step_avg:184.79ms\nstep:1004/4500 train_loss:8.1494 train_time:183682ms step_avg:184.79ms\nstep:1005/4500 train_loss:8.1656 train_time:183867ms step_avg:184.79ms\nstep:1006/4500 train_loss:8.0610 train_time:184051ms step_avg:184.79ms\nstep:1007/4500 train_loss:8.2847 train_time:184235ms step_avg:184.79ms\nstep:1008/4500 train_loss:8.0450 train_time:184421ms step_avg:184.79ms\nstep:1009/4500 train_loss:8.1165 train_time:184610ms step_avg:184.80ms\nstep:1010/4500 train_loss:8.0200 train_time:184795ms step_avg:184.79ms\nstep:1011/4500 train_loss:8.1235 train_time:184981ms step_avg:184.80ms\nstep:1012/4500 train_loss:8.0448 train_time:185169ms step_avg:184.80ms\nstep:1013/4500 train_loss:7.9990 train_time:185353ms step_avg:184.80ms\nstep:1014/4500 train_loss:8.0816 train_time:185538ms step_avg:184.80ms\nstep:1015/4500 train_loss:8.1033 train_time:185725ms step_avg:184.80ms\nstep:1016/4500 train_loss:8.1336 train_time:185909ms step_avg:184.80ms\nstep:1017/4500 train_loss:8.0697 train_time:186094ms step_avg:184.80ms\nstep:1018/4500 train_loss:8.1171 train_time:186278ms step_avg:184.80ms\nstep:1019/4500 train_loss:8.0194 train_time:186464ms step_avg:184.80ms\nstep:1020/4500 train_loss:8.1473 train_time:186647ms step_avg:184.80ms\nstep:1021/4500 train_loss:8.1987 train_time:186832ms step_avg:184.80ms\nstep:1022/4500 train_loss:8.0466 train_time:187016ms step_avg:184.80ms\nstep:1023/4500 train_loss:8.0711 train_time:187199ms step_avg:184.80ms\nstep:1024/4500 train_loss:8.2340 train_time:187383ms step_avg:184.80ms\nstep:1025/4500 train_loss:8.1290 train_time:187568ms step_avg:184.80ms\nstep:1026/4500 train_loss:8.1476 train_time:187753ms step_avg:184.80ms\nstep:1027/4500 train_loss:8.0213 train_time:187936ms step_avg:184.79ms\nstep:1028/4500 train_loss:8.2073 train_time:188124ms step_avg:184.80ms\nstep:1029/4500 train_loss:8.1240 train_time:188311ms step_avg:184.80ms\nstep:1030/4500 train_loss:8.2834 train_time:188496ms step_avg:184.80ms\nstep:1031/4500 train_loss:8.1029 train_time:188680ms step_avg:184.80ms\nstep:1032/4500 train_loss:8.0464 train_time:188865ms step_avg:184.80ms\nstep:1033/4500 train_loss:8.2347 train_time:189051ms step_avg:184.80ms\nstep:1034/4500 train_loss:8.0643 train_time:189238ms step_avg:184.80ms\nstep:1035/4500 train_loss:8.0273 train_time:189423ms step_avg:184.80ms\nstep:1036/4500 train_loss:8.0460 train_time:189607ms step_avg:184.80ms\nstep:1037/4500 train_loss:8.0478 train_time:189791ms step_avg:184.80ms\nstep:1038/4500 train_loss:8.0895 train_time:189974ms step_avg:184.80ms\nstep:1039/4500 train_loss:8.1034 train_time:190160ms step_avg:184.80ms\nstep:1040/4500 train_loss:8.0677 train_time:190345ms step_avg:184.80ms\nstep:1041/4500 train_loss:8.1958 train_time:190533ms step_avg:184.80ms\nstep:1042/4500 train_loss:8.0278 train_time:190718ms step_avg:184.80ms\nstep:1043/4500 train_loss:8.1790 train_time:190903ms step_avg:184.80ms\nstep:1044/4500 train_loss:8.1443 train_time:191087ms step_avg:184.80ms\nstep:1045/4500 train_loss:7.9515 train_time:191271ms step_avg:184.80ms\nstep:1046/4500 train_loss:8.0406 train_time:191455ms step_avg:184.80ms\nstep:1047/4500 train_loss:8.0482 train_time:191638ms step_avg:184.80ms\nstep:1048/4500 train_loss:8.0862 train_time:191823ms step_avg:184.80ms\nstep:1049/4500 train_loss:8.2611 train_time:192006ms step_avg:184.80ms\nstep:1050/4500 train_loss:8.1127 train_time:192190ms step_avg:184.80ms\nstep:1051/4500 train_loss:8.2019 train_time:192375ms step_avg:184.80ms\nstep:1052/4500 train_loss:8.1614 train_time:192562ms step_avg:184.80ms\nstep:1053/4500 train_loss:8.1619 train_time:192747ms step_avg:184.80ms\nstep:1054/4500 train_loss:8.2140 train_time:192935ms step_avg:184.80ms\nstep:1055/4500 train_loss:8.1407 train_time:193119ms step_avg:184.80ms\nstep:1056/4500 train_loss:8.0432 train_time:193305ms step_avg:184.80ms\nstep:1057/4500 train_loss:8.1241 train_time:193489ms step_avg:184.80ms\nstep:1058/4500 train_loss:8.3307 train_time:193676ms step_avg:184.81ms\nstep:1059/4500 train_loss:8.1195 train_time:193861ms step_avg:184.81ms\nstep:1060/4500 train_loss:8.0051 train_time:194045ms step_avg:184.81ms\nstep:1061/4500 train_loss:7.9432 train_time:194229ms step_avg:184.80ms\nstep:1062/4500 train_loss:8.1622 train_time:194413ms step_avg:184.80ms\nstep:1063/4500 train_loss:8.3402 train_time:194597ms step_avg:184.80ms\nstep:1064/4500 train_loss:8.0458 train_time:194782ms step_avg:184.80ms\nstep:1065/4500 train_loss:8.0013 train_time:194969ms step_avg:184.81ms\nstep:1066/4500 train_loss:8.1379 train_time:195152ms step_avg:184.80ms\nstep:1067/4500 train_loss:8.1550 train_time:195336ms step_avg:184.80ms\nstep:1068/4500 train_loss:8.1383 train_time:195522ms step_avg:184.80ms\nstep:1069/4500 train_loss:8.0298 train_time:195706ms step_avg:184.80ms\nstep:1070/4500 train_loss:8.0297 train_time:195894ms step_avg:184.81ms\nstep:1071/4500 train_loss:8.1592 train_time:196080ms step_avg:184.81ms\nstep:1072/4500 train_loss:7.9884 train_time:196262ms step_avg:184.80ms\nstep:1073/4500 train_loss:8.1687 train_time:196450ms step_avg:184.81ms\nstep:1074/4500 train_loss:8.0224 train_time:196636ms step_avg:184.81ms\nstep:1075/4500 train_loss:8.0742 train_time:196822ms step_avg:184.81ms\nstep:1076/4500 train_loss:8.1468 train_time:197006ms step_avg:184.81ms\nstep:1077/4500 train_loss:8.1848 train_time:197189ms step_avg:184.81ms\nstep:1078/4500 train_loss:8.1463 train_time:197372ms step_avg:184.81ms\nstep:1079/4500 train_loss:8.0497 train_time:197556ms step_avg:184.80ms\nstep:1080/4500 train_loss:8.1740 train_time:197740ms step_avg:184.80ms\nstep:1081/4500 train_loss:8.0433 train_time:197927ms step_avg:184.81ms\nstep:1082/4500 train_loss:8.1076 train_time:198115ms step_avg:184.81ms\nstep:1083/4500 train_loss:8.1013 train_time:198300ms step_avg:184.81ms\nstep:1084/4500 train_loss:8.1359 train_time:198483ms step_avg:184.81ms\nstep:1085/4500 train_loss:8.1422 train_time:198668ms step_avg:184.81ms\nstep:1086/4500 train_loss:8.1231 train_time:198854ms step_avg:184.81ms\nstep:1087/4500 train_loss:8.2141 train_time:199039ms step_avg:184.81ms\nstep:1088/4500 train_loss:7.9993 train_time:199229ms step_avg:184.81ms\nstep:1089/4500 train_loss:8.0443 train_time:199413ms step_avg:184.81ms\nstep:1090/4500 train_loss:8.0276 train_time:199596ms step_avg:184.81ms\nstep:1091/4500 train_loss:8.2148 train_time:199779ms step_avg:184.81ms\nstep:1092/4500 train_loss:8.0532 train_time:199963ms step_avg:184.81ms\nstep:1093/4500 train_loss:8.1639 train_time:200150ms step_avg:184.81ms\nstep:1094/4500 train_loss:8.3255 train_time:200336ms step_avg:184.81ms\nstep:1095/4500 train_loss:8.0076 train_time:200523ms step_avg:184.81ms\nstep:1096/4500 train_loss:8.1626 train_time:200708ms step_avg:184.81ms\nstep:1097/4500 train_loss:8.1802 train_time:200893ms step_avg:184.81ms\nstep:1098/4500 train_loss:8.0245 train_time:201075ms step_avg:184.81ms\nstep:1099/4500 train_loss:7.9876 train_time:201260ms step_avg:184.81ms\nstep:1100/4500 train_loss:8.1147 train_time:201447ms step_avg:184.81ms\nstep:1101/4500 train_loss:7.9148 train_time:201634ms step_avg:184.82ms\nstep:1102/4500 train_loss:8.1374 train_time:201820ms step_avg:184.82ms\nstep:1103/4500 train_loss:8.1566 train_time:202008ms step_avg:184.82ms\nstep:1104/4500 train_loss:8.1942 train_time:202193ms step_avg:184.82ms\nstep:1105/4500 train_loss:8.1325 train_time:202377ms step_avg:184.82ms\nstep:1106/4500 train_loss:8.1165 train_time:202562ms step_avg:184.82ms\nstep:1107/4500 train_loss:8.0805 train_time:202748ms step_avg:184.82ms\nstep:1108/4500 train_loss:8.0881 train_time:202935ms step_avg:184.82ms\nstep:1109/4500 train_loss:8.1207 train_time:203121ms step_avg:184.82ms\nstep:1110/4500 train_loss:8.0633 train_time:203306ms step_avg:184.82ms\nstep:1111/4500 train_loss:8.0213 train_time:203490ms step_avg:184.82ms\nstep:1112/4500 train_loss:8.1666 train_time:203674ms step_avg:184.82ms\nstep:1113/4500 train_loss:7.9941 train_time:203857ms step_avg:184.82ms\nstep:1114/4500 train_loss:8.1982 train_time:204042ms step_avg:184.82ms\nstep:1115/4500 train_loss:8.0939 train_time:204226ms step_avg:184.82ms\nstep:1116/4500 train_loss:8.0963 train_time:204410ms step_avg:184.82ms\nstep:1117/4500 train_loss:8.1079 train_time:204595ms step_avg:184.82ms\nstep:1118/4500 train_loss:8.0980 train_time:204780ms step_avg:184.82ms\nstep:1119/4500 train_loss:8.0928 train_time:204964ms step_avg:184.82ms\nstep:1120/4500 train_loss:7.9983 train_time:205147ms step_avg:184.82ms\nstep:1121/4500 train_loss:8.1683 train_time:205331ms step_avg:184.82ms\nstep:1122/4500 train_loss:8.0325 train_time:205517ms step_avg:184.82ms\nstep:1123/4500 train_loss:8.0078 train_time:205701ms step_avg:184.82ms\nstep:1124/4500 train_loss:8.1008 train_time:205886ms step_avg:184.82ms\nstep:1125/4500 train_loss:8.1264 train_time:206069ms step_avg:184.82ms\nstep:1126/4500 train_loss:8.0868 train_time:206252ms step_avg:184.81ms\nstep:1127/4500 train_loss:8.1122 train_time:206435ms step_avg:184.81ms\nstep:1128/4500 train_loss:8.2647 train_time:206620ms step_avg:184.81ms\nstep:1129/4500 train_loss:8.1074 train_time:206804ms step_avg:184.81ms\nstep:1130/4500 train_loss:8.2021 train_time:206989ms step_avg:184.81ms\nstep:1131/4500 train_loss:8.2754 train_time:207173ms step_avg:184.81ms\nstep:1132/4500 train_loss:8.1047 train_time:207359ms step_avg:184.81ms\nstep:1133/4500 train_loss:8.1098 train_time:207544ms step_avg:184.81ms\nstep:1134/4500 train_loss:8.1009 train_time:207728ms step_avg:184.81ms\nstep:1135/4500 train_loss:8.1334 train_time:207912ms step_avg:184.81ms\nstep:1136/4500 train_loss:8.1546 train_time:208096ms step_avg:184.81ms\nstep:1137/4500 train_loss:8.0281 train_time:208280ms step_avg:184.81ms\nstep:1138/4500 train_loss:8.0480 train_time:208464ms step_avg:184.81ms\nstep:1139/4500 train_loss:8.1713 train_time:208648ms step_avg:184.81ms\nstep:1140/4500 train_loss:8.1248 train_time:208832ms step_avg:184.81ms\nstep:1141/4500 train_loss:8.0826 train_time:209016ms step_avg:184.81ms\nstep:1142/4500 train_loss:8.0580 train_time:209204ms step_avg:184.81ms\nstep:1143/4500 train_loss:8.0590 train_time:209390ms step_avg:184.81ms\nstep:1144/4500 train_loss:8.1701 train_time:209571ms step_avg:184.81ms\nstep:1145/4500 train_loss:8.1663 train_time:209755ms step_avg:184.81ms\nstep:1146/4500 train_loss:8.1032 train_time:209940ms step_avg:184.81ms\nstep:1147/4500 train_loss:8.1102 train_time:210124ms step_avg:184.81ms\nstep:1148/4500 train_loss:8.1441 train_time:210309ms step_avg:184.81ms\nstep:1149/4500 train_loss:8.0799 train_time:210495ms step_avg:184.81ms\nstep:1150/4500 train_loss:8.0543 train_time:210679ms step_avg:184.81ms\nstep:1151/4500 train_loss:8.0782 train_time:210864ms step_avg:184.81ms\nstep:1152/4500 train_loss:8.1226 train_time:211049ms step_avg:184.81ms\nstep:1153/4500 train_loss:8.1274 train_time:211234ms step_avg:184.81ms\nstep:1154/4500 train_loss:8.2556 train_time:211420ms step_avg:184.81ms\nstep:1155/4500 train_loss:8.1160 train_time:211605ms step_avg:184.81ms\nstep:1156/4500 train_loss:8.0682 train_time:211792ms step_avg:184.81ms\nstep:1157/4500 train_loss:8.0134 train_time:211975ms step_avg:184.81ms\nstep:1158/4500 train_loss:8.2518 train_time:212161ms step_avg:184.81ms\nstep:1159/4500 train_loss:8.0472 train_time:212348ms step_avg:184.81ms\nstep:1160/4500 train_loss:8.0684 train_time:212535ms step_avg:184.81ms\nstep:1161/4500 train_loss:8.1485 train_time:212720ms step_avg:184.81ms\nstep:1162/4500 train_loss:8.1048 train_time:212908ms step_avg:184.82ms\nstep:1163/4500 train_loss:8.1162 train_time:213092ms step_avg:184.82ms\nstep:1164/4500 train_loss:8.1833 train_time:213277ms step_avg:184.82ms\nstep:1165/4500 train_loss:8.0676 train_time:213462ms step_avg:184.82ms\nstep:1166/4500 train_loss:8.0776 train_time:213649ms step_avg:184.82ms\nstep:1167/4500 train_loss:8.1575 train_time:213833ms step_avg:184.82ms\nstep:1168/4500 train_loss:8.0929 train_time:214016ms step_avg:184.82ms\nstep:1169/4500 train_loss:8.1705 train_time:214201ms step_avg:184.82ms\nstep:1170/4500 train_loss:8.1278 train_time:214386ms step_avg:184.82ms\nstep:1171/4500 train_loss:8.0582 train_time:214571ms step_avg:184.82ms\nstep:1172/4500 train_loss:8.1071 train_time:214755ms step_avg:184.81ms\nstep:1173/4500 train_loss:8.1076 train_time:214940ms step_avg:184.81ms\nstep:1174/4500 train_loss:8.0942 train_time:215123ms step_avg:184.81ms\nstep:1175/4500 train_loss:8.0651 train_time:215310ms step_avg:184.82ms\nstep:1176/4500 train_loss:8.0945 train_time:215494ms step_avg:184.81ms\nstep:1177/4500 train_loss:8.2365 train_time:215678ms step_avg:184.81ms\nstep:1178/4500 train_loss:8.0770 train_time:215866ms step_avg:184.82ms\nstep:1179/4500 train_loss:8.1247 train_time:216054ms step_avg:184.82ms\nstep:1180/4500 train_loss:8.0744 train_time:216239ms step_avg:184.82ms\nstep:1181/4500 train_loss:7.9592 train_time:216424ms step_avg:184.82ms\nstep:1182/4500 train_loss:8.2089 train_time:216609ms step_avg:184.82ms\nstep:1183/4500 train_loss:8.0842 train_time:216793ms step_avg:184.82ms\nstep:1184/4500 train_loss:8.0182 train_time:216978ms step_avg:184.82ms\nstep:1185/4500 train_loss:8.1266 train_time:217163ms step_avg:184.82ms\nstep:1186/4500 train_loss:8.1365 train_time:217346ms step_avg:184.82ms\nstep:1187/4500 train_loss:8.2033 train_time:217533ms step_avg:184.82ms\nstep:1188/4500 train_loss:8.0280 train_time:217717ms step_avg:184.82ms\nstep:1189/4500 train_loss:8.3096 train_time:217903ms step_avg:184.82ms\nstep:1190/4500 train_loss:8.2382 train_time:218090ms step_avg:184.82ms\nstep:1191/4500 train_loss:8.1559 train_time:218274ms step_avg:184.82ms\nstep:1192/4500 train_loss:8.0890 train_time:218458ms step_avg:184.82ms\nstep:1193/4500 train_loss:8.2271 train_time:218644ms step_avg:184.82ms\nstep:1194/4500 train_loss:8.1019 train_time:218832ms step_avg:184.82ms\nstep:1195/4500 train_loss:8.0914 train_time:219016ms step_avg:184.82ms\nstep:1196/4500 train_loss:8.0799 train_time:219200ms step_avg:184.82ms\nstep:1197/4500 train_loss:8.0947 train_time:219383ms step_avg:184.82ms\nstep:1198/4500 train_loss:8.2866 train_time:219568ms step_avg:184.82ms\nstep:1199/4500 train_loss:8.0067 train_time:219753ms step_avg:184.82ms\nstep:1200/4500 train_loss:8.1869 train_time:219937ms step_avg:184.82ms\nstep:1201/4500 train_loss:8.1454 train_time:220126ms step_avg:184.82ms\nstep:1202/4500 train_loss:8.1362 train_time:220308ms step_avg:184.82ms\nstep:1203/4500 train_loss:8.0825 train_time:220494ms step_avg:184.82ms\nstep:1204/4500 train_loss:8.1250 train_time:220678ms step_avg:184.82ms\nstep:1205/4500 train_loss:8.1161 train_time:220862ms step_avg:184.82ms\nstep:1206/4500 train_loss:8.0947 train_time:221046ms step_avg:184.82ms\nstep:1207/4500 train_loss:8.1566 train_time:221231ms step_avg:184.82ms\nstep:1208/4500 train_loss:8.0834 train_time:221416ms step_avg:184.82ms\nstep:1209/4500 train_loss:8.1920 train_time:221601ms step_avg:184.82ms\nstep:1210/4500 train_loss:8.1147 train_time:221786ms step_avg:184.82ms\nstep:1211/4500 train_loss:8.1370 train_time:221973ms step_avg:184.82ms\nstep:1212/4500 train_loss:8.1478 train_time:222157ms step_avg:184.82ms\nstep:1213/4500 train_loss:8.0207 train_time:222342ms step_avg:184.82ms\nstep:1214/4500 train_loss:8.1658 train_time:222527ms step_avg:184.82ms\nstep:1215/4500 train_loss:8.0897 train_time:222712ms step_avg:184.82ms\nstep:1216/4500 train_loss:8.1561 train_time:222898ms step_avg:184.82ms\nstep:1217/4500 train_loss:8.1801 train_time:223085ms step_avg:184.83ms\nstep:1218/4500 train_loss:8.0972 train_time:223271ms step_avg:184.83ms\nstep:1219/4500 train_loss:8.1903 train_time:223455ms step_avg:184.83ms\nstep:1220/4500 train_loss:8.1883 train_time:223640ms step_avg:184.83ms\nstep:1221/4500 train_loss:8.0496 train_time:223829ms step_avg:184.83ms\nstep:1222/4500 train_loss:8.1971 train_time:224012ms step_avg:184.83ms\nstep:1223/4500 train_loss:7.9546 train_time:224197ms step_avg:184.83ms\nstep:1224/4500 train_loss:8.0783 train_time:224383ms step_avg:184.83ms\nstep:1225/4500 train_loss:8.0741 train_time:224568ms step_avg:184.83ms\nstep:1226/4500 train_loss:8.1142 train_time:224753ms step_avg:184.83ms\nstep:1227/4500 train_loss:8.2343 train_time:224938ms step_avg:184.83ms\nstep:1228/4500 train_loss:8.0429 train_time:225126ms step_avg:184.83ms\nstep:1229/4500 train_loss:8.1164 train_time:225311ms step_avg:184.83ms\nstep:1230/4500 train_loss:8.1333 train_time:225496ms step_avg:184.83ms\nstep:1231/4500 train_loss:8.2325 train_time:225680ms step_avg:184.83ms\nstep:1232/4500 train_loss:8.2086 train_time:225864ms step_avg:184.83ms\nstep:1233/4500 train_loss:8.0286 train_time:226051ms step_avg:184.83ms\nstep:1234/4500 train_loss:8.2116 train_time:226234ms step_avg:184.83ms\nstep:1235/4500 train_loss:8.0367 train_time:226417ms step_avg:184.83ms\nstep:1236/4500 train_loss:8.0025 train_time:226602ms step_avg:184.83ms\nstep:1237/4500 train_loss:8.1338 train_time:226790ms step_avg:184.83ms\nstep:1238/4500 train_loss:8.1038 train_time:226973ms step_avg:184.83ms\nstep:1239/4500 train_loss:8.1827 train_time:227156ms step_avg:184.83ms\nstep:1240/4500 train_loss:8.0977 train_time:227341ms step_avg:184.83ms\nstep:1241/4500 train_loss:8.1381 train_time:227527ms step_avg:184.83ms\nstep:1242/4500 train_loss:8.1113 train_time:227710ms step_avg:184.83ms\nstep:1243/4500 train_loss:8.2128 train_time:227898ms step_avg:184.83ms\nstep:1244/4500 train_loss:8.1752 train_time:228086ms step_avg:184.83ms\nstep:1245/4500 train_loss:8.1514 train_time:228270ms step_avg:184.83ms\nstep:1246/4500 train_loss:8.1143 train_time:228454ms step_avg:184.83ms\nstep:1247/4500 train_loss:8.2252 train_time:228640ms step_avg:184.83ms\nstep:1248/4500 train_loss:8.1141 train_time:228825ms step_avg:184.83ms\nstep:1249/4500 train_loss:8.1293 train_time:229012ms step_avg:184.84ms\nstep:1250/4500 train_loss:8.0536 train_time:229195ms step_avg:184.84ms\nstep:1251/4500 train_loss:8.1279 train_time:229381ms step_avg:184.84ms\nstep:1252/4500 train_loss:8.3242 train_time:229567ms step_avg:184.84ms\nstep:1253/4500 train_loss:8.1617 train_time:229751ms step_avg:184.84ms\nstep:1254/4500 train_loss:8.0891 train_time:229935ms step_avg:184.84ms\nstep:1255/4500 train_loss:8.2172 train_time:230120ms step_avg:184.84ms\nstep:1256/4500 train_loss:8.2681 train_time:230306ms step_avg:184.84ms\nstep:1257/4500 train_loss:8.1512 train_time:230493ms step_avg:184.84ms\nstep:1258/4500 train_loss:8.1499 train_time:230681ms step_avg:184.84ms\nstep:1259/4500 train_loss:8.1348 train_time:230885ms step_avg:184.86ms\nstep:1260/4500 train_loss:8.1637 train_time:231068ms step_avg:184.85ms\nstep:1261/4500 train_loss:8.2306 train_time:231251ms step_avg:184.85ms\nstep:1262/4500 train_loss:8.0078 train_time:231435ms step_avg:184.85ms\nstep:1263/4500 train_loss:8.2061 train_time:231621ms step_avg:184.85ms\nstep:1264/4500 train_loss:8.1567 train_time:231806ms step_avg:184.85ms\nstep:1265/4500 train_loss:8.1280 train_time:231993ms step_avg:184.86ms\nstep:1266/4500 train_loss:8.1829 train_time:232177ms step_avg:184.85ms\nstep:1267/4500 train_loss:8.0979 train_time:232360ms step_avg:184.85ms\nstep:1268/4500 train_loss:8.1110 train_time:232544ms step_avg:184.85ms\nstep:1269/4500 train_loss:8.0094 train_time:232730ms step_avg:184.85ms\nstep:1270/4500 train_loss:8.1364 train_time:232914ms step_avg:184.85ms\nstep:1271/4500 train_loss:8.1179 train_time:233100ms step_avg:184.85ms\nstep:1272/4500 train_loss:8.0311 train_time:233287ms step_avg:184.85ms\nstep:1273/4500 train_loss:8.0152 train_time:233471ms step_avg:184.85ms\nstep:1274/4500 train_loss:8.0584 train_time:233653ms step_avg:184.85ms\nstep:1275/4500 train_loss:8.1194 train_time:233837ms step_avg:184.85ms\nstep:1276/4500 train_loss:8.0443 train_time:234021ms step_avg:184.85ms\nstep:1277/4500 train_loss:8.1944 train_time:234207ms step_avg:184.85ms\nstep:1278/4500 train_loss:8.1527 train_time:234395ms step_avg:184.85ms\nstep:1279/4500 train_loss:8.1652 train_time:234580ms step_avg:184.85ms\nstep:1280/4500 train_loss:7.9978 train_time:234763ms step_avg:184.85ms\nstep:1281/4500 train_loss:8.1217 train_time:234947ms step_avg:184.85ms\nstep:1282/4500 train_loss:8.0109 train_time:235131ms step_avg:184.85ms\nstep:1283/4500 train_loss:8.0981 train_time:235313ms step_avg:184.85ms\nstep:1284/4500 train_loss:8.1181 train_time:235497ms step_avg:184.85ms\nstep:1285/4500 train_loss:8.0498 train_time:235681ms step_avg:184.85ms\nstep:1286/4500 train_loss:8.1811 train_time:235867ms step_avg:184.85ms\nstep:1287/4500 train_loss:8.1885 train_time:236052ms step_avg:184.85ms\nstep:1288/4500 train_loss:8.0178 train_time:236235ms step_avg:184.85ms\nstep:1289/4500 train_loss:8.0969 train_time:236419ms step_avg:184.85ms\nstep:1290/4500 train_loss:8.0829 train_time:236604ms step_avg:184.85ms\nstep:1291/4500 train_loss:8.0640 train_time:236788ms step_avg:184.85ms\nstep:1292/4500 train_loss:8.1550 train_time:236973ms step_avg:184.85ms\nstep:1293/4500 train_loss:8.1338 train_time:237155ms step_avg:184.84ms\nstep:1294/4500 train_loss:8.1547 train_time:237339ms step_avg:184.84ms\nstep:1295/4500 train_loss:8.1454 train_time:237523ms step_avg:184.84ms\nstep:1296/4500 train_loss:8.3640 train_time:237707ms step_avg:184.84ms\nstep:1297/4500 train_loss:8.0665 train_time:237893ms step_avg:184.84ms\nstep:1298/4500 train_loss:8.1292 train_time:238078ms step_avg:184.84ms\nstep:1299/4500 train_loss:8.1603 train_time:238265ms step_avg:184.84ms\nstep:1300/4500 train_loss:8.2019 train_time:238448ms step_avg:184.84ms\nstep:1301/4500 train_loss:8.1410 train_time:238632ms step_avg:184.84ms\nstep:1302/4500 train_loss:8.0798 train_time:238815ms step_avg:184.84ms\nstep:1303/4500 train_loss:8.2478 train_time:239000ms step_avg:184.84ms\nstep:1304/4500 train_loss:8.1127 train_time:239185ms step_avg:184.84ms\nstep:1305/4500 train_loss:8.0764 train_time:239370ms step_avg:184.84ms\nstep:1306/4500 train_loss:8.1536 train_time:239554ms step_avg:184.84ms\nstep:1307/4500 train_loss:8.1830 train_time:239738ms step_avg:184.84ms\nstep:1308/4500 train_loss:8.2552 train_time:239923ms step_avg:184.84ms\nstep:1309/4500 train_loss:7.9779 train_time:240107ms step_avg:184.84ms\nstep:1310/4500 train_loss:7.9848 train_time:240290ms step_avg:184.84ms\nstep:1311/4500 train_loss:8.1216 train_time:240476ms step_avg:184.84ms\nstep:1312/4500 train_loss:8.2199 train_time:240664ms step_avg:184.84ms\nstep:1313/4500 train_loss:8.2181 train_time:240849ms step_avg:184.84ms\nstep:1314/4500 train_loss:8.2283 train_time:241037ms step_avg:184.84ms\nstep:1315/4500 train_loss:8.1199 train_time:241221ms step_avg:184.84ms\nstep:1316/4500 train_loss:8.1345 train_time:241405ms step_avg:184.84ms\nstep:1317/4500 train_loss:8.0459 train_time:241589ms step_avg:184.84ms\nstep:1318/4500 train_loss:8.0301 train_time:241772ms step_avg:184.84ms\nstep:1319/4500 train_loss:8.1178 train_time:241957ms step_avg:184.84ms\nstep:1320/4500 train_loss:8.0958 train_time:242141ms step_avg:184.84ms\nstep:1321/4500 train_loss:8.1592 train_time:242328ms step_avg:184.84ms\nstep:1322/4500 train_loss:8.1432 train_time:242512ms step_avg:184.84ms\nstep:1323/4500 train_loss:8.1147 train_time:242698ms step_avg:184.84ms\nstep:1324/4500 train_loss:8.1144 train_time:242883ms step_avg:184.84ms\nstep:1325/4500 train_loss:8.0642 train_time:243067ms step_avg:184.84ms\nstep:1326/4500 train_loss:8.3987 train_time:243252ms step_avg:184.84ms\nstep:1327/4500 train_loss:8.0927 train_time:243437ms step_avg:184.84ms\nstep:1328/4500 train_loss:7.9949 train_time:243622ms step_avg:184.84ms\nstep:1329/4500 train_loss:8.0777 train_time:243807ms step_avg:184.84ms\nstep:1330/4500 train_loss:8.0678 train_time:243991ms step_avg:184.84ms\nstep:1331/4500 train_loss:8.0277 train_time:244174ms step_avg:184.84ms\nstep:1332/4500 train_loss:8.0743 train_time:244358ms step_avg:184.84ms\nstep:1333/4500 train_loss:8.0402 train_time:244542ms step_avg:184.84ms\nstep:1334/4500 train_loss:8.1000 train_time:244728ms step_avg:184.84ms\nstep:1335/4500 train_loss:8.0968 train_time:244911ms step_avg:184.84ms\nstep:1336/4500 train_loss:8.0548 train_time:245096ms step_avg:184.84ms\nstep:1337/4500 train_loss:8.1376 train_time:245280ms step_avg:184.84ms\nstep:1338/4500 train_loss:8.1365 train_time:245464ms step_avg:184.84ms\nstep:1339/4500 train_loss:8.1039 train_time:245650ms step_avg:184.84ms\nstep:1340/4500 train_loss:8.2261 train_time:245834ms step_avg:184.84ms\nstep:1341/4500 train_loss:8.1948 train_time:246018ms step_avg:184.84ms\nstep:1342/4500 train_loss:8.1335 train_time:246203ms step_avg:184.84ms\nstep:1343/4500 train_loss:8.0286 train_time:246388ms step_avg:184.84ms\nstep:1344/4500 train_loss:8.1560 train_time:246574ms step_avg:184.84ms\nstep:1345/4500 train_loss:8.0311 train_time:246759ms step_avg:184.84ms\nstep:1346/4500 train_loss:8.0696 train_time:246945ms step_avg:184.84ms\nstep:1347/4500 train_loss:8.0529 train_time:247132ms step_avg:184.84ms\nstep:1348/4500 train_loss:8.3101 train_time:247314ms step_avg:184.84ms\nstep:1349/4500 train_loss:8.6110 train_time:247497ms step_avg:184.84ms\nstep:1350/4500 train_loss:8.1668 train_time:247681ms step_avg:184.84ms\nstep:1351/4500 train_loss:8.1514 train_time:247867ms step_avg:184.84ms\nstep:1352/4500 train_loss:8.1587 train_time:248053ms step_avg:184.84ms\nstep:1353/4500 train_loss:8.1722 train_time:248237ms step_avg:184.84ms\nstep:1354/4500 train_loss:8.0364 train_time:248421ms step_avg:184.84ms\nstep:1355/4500 train_loss:8.1940 train_time:248605ms step_avg:184.84ms\nstep:1356/4500 train_loss:8.1795 train_time:248789ms step_avg:184.84ms\nstep:1357/4500 train_loss:8.1183 train_time:248972ms step_avg:184.83ms\nstep:1358/4500 train_loss:8.1063 train_time:249158ms step_avg:184.84ms\nstep:1359/4500 train_loss:8.0948 train_time:249346ms step_avg:184.84ms\nstep:1360/4500 train_loss:8.0228 train_time:249532ms step_avg:184.84ms\nstep:1361/4500 train_loss:8.0362 train_time:249716ms step_avg:184.84ms\nstep:1362/4500 train_loss:7.9974 train_time:249901ms step_avg:184.84ms\nstep:1363/4500 train_loss:8.0219 train_time:250084ms step_avg:184.84ms\nstep:1364/4500 train_loss:8.0744 train_time:250270ms step_avg:184.84ms\nstep:1365/4500 train_loss:8.1289 train_time:250455ms step_avg:184.84ms\nstep:1366/4500 train_loss:8.1477 train_time:250639ms step_avg:184.84ms\nstep:1367/4500 train_loss:7.9847 train_time:250826ms step_avg:184.84ms\nstep:1368/4500 train_loss:8.0982 train_time:251011ms step_avg:184.84ms\nstep:1369/4500 train_loss:8.1305 train_time:251194ms step_avg:184.84ms\nstep:1370/4500 train_loss:8.0720 train_time:251379ms step_avg:184.84ms\nstep:1371/4500 train_loss:8.1427 train_time:251567ms step_avg:184.84ms\nstep:1372/4500 train_loss:8.1407 train_time:251750ms step_avg:184.84ms\nstep:1373/4500 train_loss:8.0087 train_time:251935ms step_avg:184.84ms\nstep:1374/4500 train_loss:8.2090 train_time:252120ms step_avg:184.84ms\nstep:1375/4500 train_loss:8.0648 train_time:252306ms step_avg:184.84ms\nstep:1376/4500 train_loss:8.1588 train_time:252490ms step_avg:184.84ms\nstep:1377/4500 train_loss:7.9535 train_time:252674ms step_avg:184.84ms\nstep:1378/4500 train_loss:8.1120 train_time:252859ms step_avg:184.84ms\nstep:1379/4500 train_loss:8.2112 train_time:253043ms step_avg:184.84ms\nstep:1380/4500 train_loss:8.0805 train_time:253227ms step_avg:184.84ms\nstep:1381/4500 train_loss:8.0355 train_time:253410ms step_avg:184.84ms\nstep:1382/4500 train_loss:8.1154 train_time:253593ms step_avg:184.83ms\nstep:1383/4500 train_loss:8.0921 train_time:253778ms step_avg:184.83ms\nstep:1384/4500 train_loss:8.1453 train_time:253965ms step_avg:184.84ms\nstep:1385/4500 train_loss:8.0422 train_time:254149ms step_avg:184.84ms\nstep:1386/4500 train_loss:8.1621 train_time:254333ms step_avg:184.84ms\nstep:1387/4500 train_loss:7.9862 train_time:254517ms step_avg:184.83ms\nstep:1388/4500 train_loss:8.1741 train_time:254701ms step_avg:184.83ms\nstep:1389/4500 train_loss:8.0301 train_time:254885ms step_avg:184.83ms\nstep:1390/4500 train_loss:7.9936 train_time:255070ms step_avg:184.83ms\nstep:1391/4500 train_loss:8.0548 train_time:255254ms step_avg:184.83ms\nstep:1392/4500 train_loss:8.0943 train_time:255440ms step_avg:184.83ms\nstep:1393/4500 train_loss:8.1032 train_time:255627ms step_avg:184.84ms\nstep:1394/4500 train_loss:8.1481 train_time:255812ms step_avg:184.84ms\nstep:1395/4500 train_loss:7.9892 train_time:255998ms step_avg:184.84ms\nstep:1396/4500 train_loss:8.0120 train_time:256182ms step_avg:184.84ms\nstep:1397/4500 train_loss:7.9040 train_time:256367ms step_avg:184.84ms\nstep:1398/4500 train_loss:8.1581 train_time:256553ms step_avg:184.84ms\nstep:1399/4500 train_loss:7.8982 train_time:256738ms step_avg:184.84ms\nstep:1400/4500 train_loss:7.7841 train_time:256923ms step_avg:184.84ms\nstep:1401/4500 train_loss:8.0573 train_time:257107ms step_avg:184.84ms\nstep:1402/4500 train_loss:8.1259 train_time:257293ms step_avg:184.84ms\nstep:1403/4500 train_loss:8.0312 train_time:257476ms step_avg:184.84ms\nstep:1404/4500 train_loss:8.0306 train_time:257660ms step_avg:184.83ms\nstep:1405/4500 train_loss:8.1688 train_time:257845ms step_avg:184.83ms\nstep:1406/4500 train_loss:8.1169 train_time:258028ms step_avg:184.83ms\nstep:1407/4500 train_loss:8.1344 train_time:258214ms step_avg:184.83ms\nstep:1408/4500 train_loss:8.1777 train_time:258401ms step_avg:184.84ms\nstep:1409/4500 train_loss:8.0762 train_time:258587ms step_avg:184.84ms\nstep:1410/4500 train_loss:8.1128 train_time:258773ms step_avg:184.84ms\nstep:1411/4500 train_loss:8.0082 train_time:258958ms step_avg:184.84ms\nstep:1412/4500 train_loss:8.0640 train_time:259142ms step_avg:184.84ms\nstep:1413/4500 train_loss:8.1478 train_time:259328ms step_avg:184.84ms\nstep:1414/4500 train_loss:8.1063 train_time:259515ms step_avg:184.84ms\nstep:1415/4500 train_loss:8.1152 train_time:259701ms step_avg:184.84ms\nstep:1416/4500 train_loss:8.0938 train_time:259889ms step_avg:184.84ms\nstep:1417/4500 train_loss:8.0522 train_time:260075ms step_avg:184.84ms\nstep:1418/4500 train_loss:8.1554 train_time:260259ms step_avg:184.84ms\nstep:1419/4500 train_loss:8.0505 train_time:260444ms step_avg:184.84ms\nstep:1420/4500 train_loss:8.1474 train_time:260628ms step_avg:184.84ms\nstep:1421/4500 train_loss:8.1258 train_time:260814ms step_avg:184.84ms\nstep:1422/4500 train_loss:8.0823 train_time:261001ms step_avg:184.84ms\nstep:1423/4500 train_loss:8.1583 train_time:261186ms step_avg:184.84ms\nstep:1424/4500 train_loss:8.0727 train_time:261368ms step_avg:184.84ms\nstep:1425/4500 train_loss:8.0573 train_time:261553ms step_avg:184.84ms\nstep:1426/4500 train_loss:8.0914 train_time:261736ms step_avg:184.84ms\nstep:1427/4500 train_loss:7.9926 train_time:261923ms step_avg:184.84ms\nstep:1428/4500 train_loss:8.2276 train_time:262110ms step_avg:184.85ms\nstep:1429/4500 train_loss:7.9996 train_time:262295ms step_avg:184.85ms\nstep:1430/4500 train_loss:8.1142 train_time:262480ms step_avg:184.85ms\nstep:1431/4500 train_loss:8.1858 train_time:262664ms step_avg:184.84ms\nstep:1432/4500 train_loss:8.1279 train_time:262849ms step_avg:184.84ms\nstep:1433/4500 train_loss:8.0762 train_time:263033ms step_avg:184.84ms\nstep:1434/4500 train_loss:8.1566 train_time:263218ms step_avg:184.84ms\nstep:1435/4500 train_loss:8.1977 train_time:263404ms step_avg:184.85ms\nstep:1436/4500 train_loss:8.4532 train_time:263594ms step_avg:184.85ms\nstep:1437/4500 train_loss:8.1274 train_time:263782ms step_avg:184.85ms\nstep:1438/4500 train_loss:7.9809 train_time:263962ms step_avg:184.85ms\nstep:1439/4500 train_loss:8.1573 train_time:264146ms step_avg:184.85ms\nstep:1440/4500 train_loss:8.0083 train_time:264334ms step_avg:184.85ms\nstep:1441/4500 train_loss:8.1382 train_time:264518ms step_avg:184.85ms\nstep:1442/4500 train_loss:8.2322 train_time:264702ms step_avg:184.85ms\nstep:1443/4500 train_loss:8.1762 train_time:264887ms step_avg:184.85ms\nstep:1444/4500 train_loss:8.2094 train_time:265074ms step_avg:184.85ms\nstep:1445/4500 train_loss:8.1424 train_time:265256ms step_avg:184.85ms\nstep:1446/4500 train_loss:8.1363 train_time:265440ms step_avg:184.85ms\nstep:1447/4500 train_loss:8.0307 train_time:265627ms step_avg:184.85ms\nstep:1448/4500 train_loss:8.2222 train_time:265812ms step_avg:184.85ms\nstep:1449/4500 train_loss:8.1665 train_time:265996ms step_avg:184.85ms\nstep:1450/4500 train_loss:8.1474 train_time:266180ms step_avg:184.85ms\nstep:1451/4500 train_loss:8.1296 train_time:266367ms step_avg:184.85ms\nstep:1452/4500 train_loss:8.0812 train_time:266554ms step_avg:184.85ms\nstep:1453/4500 train_loss:8.0597 train_time:266738ms step_avg:184.85ms\nstep:1454/4500 train_loss:8.1416 train_time:266925ms step_avg:184.85ms\nstep:1455/4500 train_loss:8.0518 train_time:267110ms step_avg:184.85ms\nstep:1456/4500 train_loss:8.1566 train_time:267294ms step_avg:184.85ms\nstep:1457/4500 train_loss:8.8087 train_time:267478ms step_avg:184.85ms\nstep:1458/4500 train_loss:8.1682 train_time:267662ms step_avg:184.85ms\nstep:1459/4500 train_loss:8.1535 train_time:267846ms step_avg:184.85ms\nstep:1460/4500 train_loss:8.1422 train_time:268034ms step_avg:184.85ms\nstep:1461/4500 train_loss:8.0744 train_time:268217ms step_avg:184.85ms\nstep:1462/4500 train_loss:8.2186 train_time:268401ms step_avg:184.85ms\nstep:1463/4500 train_loss:8.1506 train_time:268586ms step_avg:184.85ms\nstep:1464/4500 train_loss:8.0833 train_time:268768ms step_avg:184.85ms\nstep:1465/4500 train_loss:8.0596 train_time:268952ms step_avg:184.85ms\nstep:1466/4500 train_loss:8.1010 train_time:269137ms step_avg:184.85ms\nstep:1467/4500 train_loss:8.1446 train_time:269323ms step_avg:184.85ms\nstep:1468/4500 train_loss:8.0876 train_time:269509ms step_avg:184.85ms\nstep:1469/4500 train_loss:8.1491 train_time:269696ms step_avg:184.85ms\nstep:1470/4500 train_loss:8.1459 train_time:269881ms step_avg:184.85ms\nstep:1471/4500 train_loss:8.0883 train_time:270069ms step_avg:184.85ms\nstep:1472/4500 train_loss:8.1106 train_time:270254ms step_avg:184.85ms\nstep:1473/4500 train_loss:8.0078 train_time:270438ms step_avg:184.85ms\nstep:1474/4500 train_loss:8.1453 train_time:270622ms step_avg:184.85ms\nstep:1475/4500 train_loss:8.1021 train_time:270806ms step_avg:184.85ms\nstep:1476/4500 train_loss:8.0291 train_time:270989ms step_avg:184.85ms\nstep:1477/4500 train_loss:8.2283 train_time:271175ms step_avg:184.85ms\nstep:1478/4500 train_loss:8.0677 train_time:271365ms step_avg:184.85ms\nstep:1479/4500 train_loss:8.0123 train_time:271547ms step_avg:184.85ms\nstep:1480/4500 train_loss:8.0553 train_time:271728ms step_avg:184.85ms\nstep:1481/4500 train_loss:8.0762 train_time:271912ms step_avg:184.85ms\nstep:1482/4500 train_loss:8.0235 train_time:272095ms step_avg:184.85ms\nstep:1483/4500 train_loss:8.1322 train_time:272280ms step_avg:184.85ms\nstep:1484/4500 train_loss:8.2552 train_time:272468ms step_avg:184.85ms\nstep:1485/4500 train_loss:8.1118 train_time:272656ms step_avg:184.85ms\nstep:1486/4500 train_loss:8.1113 train_time:272841ms step_avg:184.85ms\nstep:1487/4500 train_loss:8.0875 train_time:273028ms step_avg:184.85ms\nstep:1488/4500 train_loss:8.0396 train_time:273215ms step_avg:184.85ms\nstep:1489/4500 train_loss:8.0354 train_time:273400ms step_avg:184.85ms\nstep:1490/4500 train_loss:8.0548 train_time:273589ms step_avg:184.86ms\nstep:1491/4500 train_loss:8.3200 train_time:273773ms step_avg:184.86ms\nstep:1492/4500 train_loss:8.1012 train_time:273956ms step_avg:184.86ms\nstep:1493/4500 train_loss:8.1138 train_time:274140ms step_avg:184.86ms\nstep:1494/4500 train_loss:8.1796 train_time:274325ms step_avg:184.86ms\nstep:1495/4500 train_loss:8.1426 train_time:274511ms step_avg:184.86ms\nstep:1496/4500 train_loss:8.0949 train_time:274696ms step_avg:184.86ms\nstep:1497/4500 train_loss:8.0173 train_time:274882ms step_avg:184.86ms\nstep:1498/4500 train_loss:8.0542 train_time:275066ms step_avg:184.86ms\nstep:1499/4500 train_loss:8.0369 train_time:275250ms step_avg:184.86ms\nstep:1500/4500 train_loss:8.0966 train_time:275433ms step_avg:184.85ms\nstep:1501/4500 train_loss:8.0514 train_time:275617ms step_avg:184.85ms\nstep:1502/4500 train_loss:8.2213 train_time:275803ms step_avg:184.85ms\nstep:1503/4500 train_loss:8.1259 train_time:275990ms step_avg:184.86ms\nstep:1504/4500 train_loss:8.1798 train_time:276175ms step_avg:184.86ms\nstep:1505/4500 train_loss:8.2055 train_time:276360ms step_avg:184.86ms\nstep:1506/4500 train_loss:8.1095 train_time:276545ms step_avg:184.86ms\nstep:1507/4500 train_loss:8.0043 train_time:276729ms step_avg:184.86ms\nstep:1508/4500 train_loss:8.0598 train_time:276913ms step_avg:184.86ms\nstep:1509/4500 train_loss:8.0588 train_time:277096ms step_avg:184.85ms\nstep:1510/4500 train_loss:7.4424 train_time:277281ms step_avg:184.85ms\nstep:1511/4500 train_loss:8.0035 train_time:277467ms step_avg:184.85ms\nstep:1512/4500 train_loss:8.1255 train_time:277650ms step_avg:184.85ms\nstep:1513/4500 train_loss:8.1564 train_time:277836ms step_avg:184.85ms\nstep:1514/4500 train_loss:7.9957 train_time:278023ms step_avg:184.86ms\nstep:1515/4500 train_loss:7.9981 train_time:278211ms step_avg:184.86ms\nstep:1516/4500 train_loss:8.2266 train_time:278395ms step_avg:184.86ms\nstep:1517/4500 train_loss:8.1068 train_time:278580ms step_avg:184.86ms\nstep:1518/4500 train_loss:8.1188 train_time:278765ms step_avg:184.86ms\nstep:1519/4500 train_loss:8.2836 train_time:278949ms step_avg:184.86ms\nstep:1520/4500 train_loss:8.1978 train_time:279135ms step_avg:184.86ms\nstep:1521/4500 train_loss:8.0628 train_time:279320ms step_avg:184.86ms\nstep:1522/4500 train_loss:8.1524 train_time:279505ms step_avg:184.86ms\nstep:1523/4500 train_loss:8.1824 train_time:279689ms step_avg:184.86ms\nstep:1524/4500 train_loss:8.0753 train_time:280102ms step_avg:185.01ms\nstep:1525/4500 train_loss:8.0300 train_time:280285ms step_avg:185.01ms\nstep:1526/4500 train_loss:8.1944 train_time:280469ms step_avg:185.01ms\nstep:1527/4500 train_loss:8.2161 train_time:280654ms step_avg:185.01ms\nstep:1528/4500 train_loss:8.1844 train_time:280837ms step_avg:185.00ms\nstep:1529/4500 train_loss:8.0326 train_time:281028ms step_avg:185.01ms\nstep:1530/4500 train_loss:8.2767 train_time:281212ms step_avg:185.01ms\nstep:1531/4500 train_loss:8.1033 train_time:281394ms step_avg:185.01ms\nstep:1532/4500 train_loss:8.1124 train_time:281577ms step_avg:185.00ms\nstep:1533/4500 train_loss:8.1616 train_time:281761ms step_avg:185.00ms\nstep:1534/4500 train_loss:8.0028 train_time:281946ms step_avg:185.00ms\nstep:1535/4500 train_loss:8.1042 train_time:282129ms step_avg:185.00ms\nstep:1536/4500 train_loss:8.1061 train_time:282312ms step_avg:185.00ms\nstep:1537/4500 train_loss:8.1412 train_time:282495ms step_avg:185.00ms\nstep:1538/4500 train_loss:8.0710 train_time:282682ms step_avg:185.00ms\nstep:1539/4500 train_loss:8.0606 train_time:282868ms step_avg:185.00ms\nstep:1540/4500 train_loss:8.0624 train_time:283054ms step_avg:185.00ms\nstep:1541/4500 train_loss:8.1832 train_time:283238ms step_avg:185.00ms\nstep:1542/4500 train_loss:8.0820 train_time:283423ms step_avg:185.00ms\nstep:1543/4500 train_loss:8.1244 train_time:283607ms step_avg:185.00ms\nstep:1544/4500 train_loss:8.1669 train_time:283792ms step_avg:185.00ms\nstep:1545/4500 train_loss:8.1452 train_time:283975ms step_avg:185.00ms\nstep:1546/4500 train_loss:8.1692 train_time:284160ms step_avg:185.00ms\nstep:1547/4500 train_loss:8.0981 train_time:284344ms step_avg:185.00ms\nstep:1548/4500 train_loss:8.0630 train_time:284529ms step_avg:185.00ms\nstep:1549/4500 train_loss:8.0820 train_time:284713ms step_avg:185.00ms\nstep:1550/4500 train_loss:8.2680 train_time:284897ms step_avg:185.00ms\nstep:1551/4500 train_loss:8.1222 train_time:285082ms step_avg:185.00ms\nstep:1552/4500 train_loss:8.0387 train_time:285266ms step_avg:185.00ms\nstep:1553/4500 train_loss:8.1793 train_time:285451ms step_avg:185.00ms\nstep:1554/4500 train_loss:8.1306 train_time:285635ms step_avg:185.00ms\nstep:1555/4500 train_loss:8.0652 train_time:285821ms step_avg:185.00ms\nstep:1556/4500 train_loss:8.1401 train_time:286010ms step_avg:185.00ms\nstep:1557/4500 train_loss:8.1292 train_time:286194ms step_avg:185.00ms\nstep:1558/4500 train_loss:8.0091 train_time:286377ms step_avg:185.00ms\nstep:1559/4500 train_loss:8.1500 train_time:286561ms step_avg:185.00ms\nstep:1560/4500 train_loss:8.0633 train_time:286748ms step_avg:185.00ms\nstep:1561/4500 train_loss:8.0553 train_time:286934ms step_avg:185.00ms\nstep:1562/4500 train_loss:8.1383 train_time:287119ms step_avg:185.00ms\nstep:1563/4500 train_loss:8.1763 train_time:287306ms step_avg:185.00ms\nstep:1564/4500 train_loss:8.1813 train_time:287496ms step_avg:185.00ms\nstep:1565/4500 train_loss:8.0817 train_time:287680ms step_avg:185.00ms\nstep:1566/4500 train_loss:8.0735 train_time:287865ms step_avg:185.00ms\nstep:1567/4500 train_loss:8.1198 train_time:288049ms step_avg:185.00ms\nstep:1568/4500 train_loss:8.1283 train_time:288233ms step_avg:185.00ms\nstep:1569/4500 train_loss:8.0708 train_time:288418ms step_avg:185.00ms\nstep:1570/4500 train_loss:8.1507 train_time:288602ms step_avg:185.00ms\nstep:1571/4500 train_loss:8.2219 train_time:288790ms step_avg:185.00ms\nstep:1572/4500 train_loss:8.0270 train_time:288973ms step_avg:185.00ms\nstep:1573/4500 train_loss:8.0815 train_time:289158ms step_avg:185.00ms\nstep:1574/4500 train_loss:8.0351 train_time:289343ms step_avg:185.00ms\nstep:1575/4500 train_loss:8.1464 train_time:289530ms step_avg:185.00ms\nstep:1576/4500 train_loss:8.1356 train_time:289715ms step_avg:185.00ms\nstep:1577/4500 train_loss:8.1856 train_time:289901ms step_avg:185.00ms\nstep:1578/4500 train_loss:8.0941 train_time:290090ms step_avg:185.01ms\nstep:1579/4500 train_loss:8.1149 train_time:290274ms step_avg:185.01ms\nstep:1580/4500 train_loss:8.0665 train_time:290458ms step_avg:185.00ms\nstep:1581/4500 train_loss:8.1490 train_time:290642ms step_avg:185.00ms\nstep:1582/4500 train_loss:8.0970 train_time:290828ms step_avg:185.00ms\nstep:1583/4500 train_loss:8.0928 train_time:291012ms step_avg:185.00ms\nstep:1584/4500 train_loss:8.1856 train_time:291195ms step_avg:185.00ms\nstep:1585/4500 train_loss:8.0566 train_time:291381ms step_avg:185.00ms\nstep:1586/4500 train_loss:8.0436 train_time:291566ms step_avg:185.00ms\nstep:1587/4500 train_loss:8.0602 train_time:291751ms step_avg:185.00ms\nstep:1588/4500 train_loss:7.9677 train_time:291935ms step_avg:185.00ms\nstep:1589/4500 train_loss:8.0858 train_time:292120ms step_avg:185.00ms\nstep:1590/4500 train_loss:8.2812 train_time:292306ms step_avg:185.00ms\nstep:1591/4500 train_loss:8.1525 train_time:292494ms step_avg:185.01ms\nstep:1592/4500 train_loss:8.0276 train_time:292679ms step_avg:185.01ms\nstep:1593/4500 train_loss:8.2132 train_time:292863ms step_avg:185.00ms\nstep:1594/4500 train_loss:8.3445 train_time:293051ms step_avg:185.01ms\nstep:1595/4500 train_loss:8.0838 train_time:293236ms step_avg:185.01ms\nstep:1596/4500 train_loss:8.0845 train_time:293420ms step_avg:185.01ms\nstep:1597/4500 train_loss:8.1509 train_time:293605ms step_avg:185.01ms\nstep:1598/4500 train_loss:8.1047 train_time:293788ms step_avg:185.01ms\nstep:1599/4500 train_loss:8.0850 train_time:293971ms step_avg:185.00ms\nstep:1600/4500 train_loss:8.0970 train_time:294155ms step_avg:185.00ms\nstep:1601/4500 train_loss:8.3300 train_time:294340ms step_avg:185.00ms\nstep:1602/4500 train_loss:8.2886 train_time:294527ms step_avg:185.00ms\nstep:1603/4500 train_loss:8.1747 train_time:294715ms step_avg:185.01ms\nstep:1604/4500 train_loss:8.2004 train_time:294904ms step_avg:185.01ms\nstep:1605/4500 train_loss:8.0399 train_time:295084ms step_avg:185.01ms\nstep:1606/4500 train_loss:8.1100 train_time:295268ms step_avg:185.01ms\nstep:1607/4500 train_loss:8.2867 train_time:295453ms step_avg:185.00ms\nstep:1608/4500 train_loss:8.1447 train_time:295637ms step_avg:185.00ms\nstep:1609/4500 train_loss:8.1004 train_time:295824ms step_avg:185.01ms\nstep:1610/4500 train_loss:8.1004 train_time:296008ms step_avg:185.01ms\nstep:1611/4500 train_loss:8.1353 train_time:296193ms step_avg:185.00ms\nstep:1612/4500 train_loss:8.2111 train_time:296377ms step_avg:185.00ms\nstep:1613/4500 train_loss:8.1456 train_time:296561ms step_avg:185.00ms\nstep:1614/4500 train_loss:8.1040 train_time:296747ms step_avg:185.00ms\nstep:1615/4500 train_loss:7.9998 train_time:296930ms step_avg:185.00ms\nstep:1616/4500 train_loss:8.2175 train_time:297114ms step_avg:185.00ms\nstep:1617/4500 train_loss:8.1822 train_time:297301ms step_avg:185.00ms\nstep:1618/4500 train_loss:8.3352 train_time:297486ms step_avg:185.00ms\nstep:1619/4500 train_loss:8.0921 train_time:297672ms step_avg:185.00ms\nstep:1620/4500 train_loss:8.1308 train_time:297858ms step_avg:185.00ms\nstep:1621/4500 train_loss:8.1437 train_time:298042ms step_avg:185.00ms\nstep:1622/4500 train_loss:8.0874 train_time:298228ms step_avg:185.01ms\nstep:1623/4500 train_loss:8.1299 train_time:298413ms step_avg:185.01ms\nstep:1624/4500 train_loss:8.1732 train_time:298597ms step_avg:185.00ms\nstep:1625/4500 train_loss:8.2163 train_time:298783ms step_avg:185.00ms\nstep:1626/4500 train_loss:8.1849 train_time:298970ms step_avg:185.01ms\nstep:1627/4500 train_loss:8.0812 train_time:299153ms step_avg:185.00ms\nstep:1628/4500 train_loss:8.0617 train_time:299336ms step_avg:185.00ms\nstep:1629/4500 train_loss:8.2170 train_time:299520ms step_avg:185.00ms\nstep:1630/4500 train_loss:8.0955 train_time:299705ms step_avg:185.00ms\nstep:1631/4500 train_loss:8.1339 train_time:299890ms step_avg:185.00ms\nstep:1632/4500 train_loss:8.0629 train_time:300077ms step_avg:185.00ms\nstep:1633/4500 train_loss:8.2110 train_time:300266ms step_avg:185.01ms\nstep:1634/4500 train_loss:8.1282 train_time:300450ms step_avg:185.01ms\nstep:1635/4500 train_loss:8.0942 train_time:300632ms step_avg:185.00ms\nstep:1636/4500 train_loss:8.1814 train_time:300821ms step_avg:185.01ms\nstep:1637/4500 train_loss:8.1614 train_time:301002ms step_avg:185.00ms\nstep:1638/4500 train_loss:8.1049 train_time:301188ms step_avg:185.01ms\nstep:1639/4500 train_loss:8.1082 train_time:301374ms step_avg:185.01ms\nstep:1640/4500 train_loss:8.0912 train_time:301559ms step_avg:185.01ms\nstep:1641/4500 train_loss:8.0359 train_time:301744ms step_avg:185.01ms\nstep:1642/4500 train_loss:8.0757 train_time:301927ms step_avg:185.00ms\nstep:1643/4500 train_loss:8.0175 train_time:302113ms step_avg:185.00ms\nstep:1644/4500 train_loss:8.1050 train_time:302298ms step_avg:185.00ms\nstep:1645/4500 train_loss:8.0699 train_time:302485ms step_avg:185.01ms\nstep:1646/4500 train_loss:8.0995 train_time:302674ms step_avg:185.01ms\nstep:1647/4500 train_loss:8.0762 train_time:302859ms step_avg:185.01ms\nstep:1648/4500 train_loss:8.0911 train_time:303044ms step_avg:185.01ms\nstep:1649/4500 train_loss:8.1449 train_time:303228ms step_avg:185.01ms\nstep:1650/4500 train_loss:8.2482 train_time:303413ms step_avg:185.01ms\nstep:1651/4500 train_loss:8.1320 train_time:303598ms step_avg:185.01ms\nstep:1652/4500 train_loss:8.1259 train_time:303783ms step_avg:185.01ms\nstep:1653/4500 train_loss:8.2135 train_time:303969ms step_avg:185.01ms\nstep:1654/4500 train_loss:8.1287 train_time:304153ms step_avg:185.01ms\nstep:1655/4500 train_loss:8.0756 train_time:304338ms step_avg:185.01ms\nstep:1656/4500 train_loss:8.0443 train_time:304522ms step_avg:185.01ms\nstep:1657/4500 train_loss:8.0644 train_time:304708ms step_avg:185.01ms\nstep:1658/4500 train_loss:8.2312 train_time:304891ms step_avg:185.01ms\nstep:1659/4500 train_loss:8.1739 train_time:305076ms step_avg:185.01ms\nstep:1660/4500 train_loss:8.1944 train_time:305262ms step_avg:185.01ms\nstep:1661/4500 train_loss:8.1593 train_time:305449ms step_avg:185.01ms\nstep:1662/4500 train_loss:8.1110 train_time:305634ms step_avg:185.01ms\nstep:1663/4500 train_loss:8.0758 train_time:305818ms step_avg:185.01ms\nstep:1664/4500 train_loss:8.0705 train_time:306004ms step_avg:185.01ms\nstep:1665/4500 train_loss:8.2804 train_time:306190ms step_avg:185.01ms\nstep:1666/4500 train_loss:8.0758 train_time:306376ms step_avg:185.01ms\nstep:1667/4500 train_loss:7.9044 train_time:306561ms step_avg:185.01ms\nstep:1668/4500 train_loss:8.0336 train_time:306746ms step_avg:185.01ms\nstep:1669/4500 train_loss:8.1805 train_time:306931ms step_avg:185.01ms\nstep:1670/4500 train_loss:8.1143 train_time:307114ms step_avg:185.01ms\nstep:1671/4500 train_loss:8.1092 train_time:307298ms step_avg:185.01ms\nstep:1672/4500 train_loss:7.9598 train_time:307482ms step_avg:185.01ms\nstep:1673/4500 train_loss:8.0949 train_time:307667ms step_avg:185.01ms\nstep:1674/4500 train_loss:8.0587 train_time:307850ms step_avg:185.01ms\nstep:1675/4500 train_loss:8.0857 train_time:308034ms step_avg:185.01ms\nstep:1676/4500 train_loss:8.2180 train_time:308216ms step_avg:185.00ms\nstep:1677/4500 train_loss:7.9104 train_time:308401ms step_avg:185.00ms\nstep:1678/4500 train_loss:8.0881 train_time:308586ms step_avg:185.00ms\nstep:1679/4500 train_loss:8.0701 train_time:308772ms step_avg:185.00ms\nstep:1680/4500 train_loss:8.0074 train_time:308956ms step_avg:185.00ms\nstep:1681/4500 train_loss:8.0770 train_time:309141ms step_avg:185.00ms\nstep:1682/4500 train_loss:8.1884 train_time:309329ms step_avg:185.01ms\nstep:1683/4500 train_loss:8.1886 train_time:309516ms step_avg:185.01ms\nstep:1684/4500 train_loss:8.1613 train_time:309700ms step_avg:185.01ms\nstep:1685/4500 train_loss:8.1088 train_time:309885ms step_avg:185.01ms\nstep:1686/4500 train_loss:8.0378 train_time:310071ms step_avg:185.01ms\nstep:1687/4500 train_loss:8.0116 train_time:310255ms step_avg:185.01ms\nstep:1688/4500 train_loss:8.1436 train_time:310442ms step_avg:185.01ms\nstep:1689/4500 train_loss:8.1868 train_time:310631ms step_avg:185.01ms\nstep:1690/4500 train_loss:7.9677 train_time:310817ms step_avg:185.01ms\nstep:1691/4500 train_loss:8.1664 train_time:311002ms step_avg:185.01ms\nstep:1692/4500 train_loss:8.0828 train_time:311186ms step_avg:185.01ms\nstep:1693/4500 train_loss:8.0500 train_time:311372ms step_avg:185.01ms\nstep:1694/4500 train_loss:8.0965 train_time:311559ms step_avg:185.01ms\nstep:1695/4500 train_loss:8.1108 train_time:311744ms step_avg:185.01ms\nstep:1696/4500 train_loss:8.0111 train_time:311927ms step_avg:185.01ms\nstep:1697/4500 train_loss:8.1720 train_time:312114ms step_avg:185.01ms\nstep:1698/4500 train_loss:8.1262 train_time:312297ms step_avg:185.01ms\nstep:1699/4500 train_loss:8.1431 train_time:312483ms step_avg:185.01ms\nstep:1700/4500 train_loss:8.0766 train_time:312668ms step_avg:185.01ms\nstep:1701/4500 train_loss:8.0687 train_time:312854ms step_avg:185.01ms\nstep:1702/4500 train_loss:8.0632 train_time:313040ms step_avg:185.01ms\nstep:1703/4500 train_loss:8.1110 train_time:313226ms step_avg:185.01ms\nstep:1704/4500 train_loss:8.1696 train_time:313414ms step_avg:185.01ms\nstep:1705/4500 train_loss:8.1060 train_time:313599ms step_avg:185.01ms\nstep:1706/4500 train_loss:8.0759 train_time:313784ms step_avg:185.01ms\nstep:1707/4500 train_loss:8.1858 train_time:313969ms step_avg:185.01ms\nstep:1708/4500 train_loss:8.1390 train_time:314155ms step_avg:185.01ms\nstep:1709/4500 train_loss:8.0637 train_time:314338ms step_avg:185.01ms\nstep:1710/4500 train_loss:8.1894 train_time:314523ms step_avg:185.01ms\nstep:1711/4500 train_loss:8.1373 train_time:314708ms step_avg:185.01ms\nstep:1712/4500 train_loss:7.9606 train_time:314894ms step_avg:185.01ms\nstep:1713/4500 train_loss:8.0903 train_time:315076ms step_avg:185.01ms\nstep:1714/4500 train_loss:8.2388 train_time:315264ms step_avg:185.01ms\nstep:1715/4500 train_loss:8.2067 train_time:315447ms step_avg:185.01ms\nstep:1716/4500 train_loss:8.0647 train_time:315631ms step_avg:185.01ms\nstep:1717/4500 train_loss:7.9580 train_time:315815ms step_avg:185.01ms\nstep:1718/4500 train_loss:8.0462 train_time:315998ms step_avg:185.01ms\nstep:1719/4500 train_loss:8.0448 train_time:316182ms step_avg:185.01ms\nstep:1720/4500 train_loss:8.1130 train_time:316370ms step_avg:185.01ms\nstep:1721/4500 train_loss:8.1284 train_time:316556ms step_avg:185.01ms\nstep:1722/4500 train_loss:8.0588 train_time:316741ms step_avg:185.01ms\nstep:1723/4500 train_loss:8.0756 train_time:316925ms step_avg:185.01ms\nstep:1724/4500 train_loss:8.0610 train_time:317110ms step_avg:185.01ms\nstep:1725/4500 train_loss:7.9724 train_time:317295ms step_avg:185.01ms\nstep:1726/4500 train_loss:8.1970 train_time:317481ms step_avg:185.01ms\nstep:1727/4500 train_loss:8.3212 train_time:317669ms step_avg:185.01ms\nstep:1728/4500 train_loss:8.1328 train_time:317855ms step_avg:185.01ms\nstep:1729/4500 train_loss:8.2086 train_time:318038ms step_avg:185.01ms\nstep:1730/4500 train_loss:8.1102 train_time:318221ms step_avg:185.01ms\nstep:1731/4500 train_loss:8.1576 train_time:318405ms step_avg:185.01ms\nstep:1732/4500 train_loss:8.2012 train_time:318591ms step_avg:185.01ms\nstep:1733/4500 train_loss:8.1465 train_time:318774ms step_avg:185.01ms\nstep:1734/4500 train_loss:8.1996 train_time:318957ms step_avg:185.01ms\nstep:1735/4500 train_loss:8.0577 train_time:319142ms step_avg:185.01ms\nstep:1736/4500 train_loss:8.1428 train_time:319328ms step_avg:185.01ms\nstep:1737/4500 train_loss:8.0433 train_time:319514ms step_avg:185.01ms\nstep:1738/4500 train_loss:8.1234 train_time:319699ms step_avg:185.01ms\nstep:1739/4500 train_loss:8.1287 train_time:319885ms step_avg:185.01ms\nstep:1740/4500 train_loss:8.0452 train_time:320069ms step_avg:185.01ms\nstep:1741/4500 train_loss:8.1172 train_time:320256ms step_avg:185.01ms\nstep:1742/4500 train_loss:8.1096 train_time:320440ms step_avg:185.01ms\nstep:1743/4500 train_loss:8.1487 train_time:320627ms step_avg:185.01ms\nstep:1744/4500 train_loss:8.1358 train_time:320810ms step_avg:185.01ms\nstep:1745/4500 train_loss:8.0885 train_time:320997ms step_avg:185.01ms\nstep:1746/4500 train_loss:8.1219 train_time:321181ms step_avg:185.01ms\nstep:1747/4500 train_loss:8.1452 train_time:321366ms step_avg:185.01ms\nstep:1748/4500 train_loss:8.2400 train_time:321551ms step_avg:185.01ms\nstep:1749/4500 train_loss:8.1234 train_time:321737ms step_avg:185.01ms\nstep:1750/4500 train_loss:8.1364 train_time:321923ms step_avg:185.01ms\nstep:1751/4500 train_loss:8.2620 train_time:322107ms step_avg:185.01ms\nstep:1752/4500 train_loss:8.1320 train_time:322292ms step_avg:185.01ms\nstep:1753/4500 train_loss:8.1510 train_time:322477ms step_avg:185.01ms\nstep:1754/4500 train_loss:8.1029 train_time:322661ms step_avg:185.01ms\nstep:1755/4500 train_loss:8.0652 train_time:322847ms step_avg:185.01ms\nstep:1756/4500 train_loss:8.1610 train_time:323035ms step_avg:185.01ms\nstep:1757/4500 train_loss:8.1454 train_time:323219ms step_avg:185.01ms\nstep:1758/4500 train_loss:8.0207 train_time:323402ms step_avg:185.01ms\nstep:1759/4500 train_loss:8.1748 train_time:323585ms step_avg:185.01ms\nstep:1760/4500 train_loss:8.0614 train_time:323769ms step_avg:185.01ms\nstep:1761/4500 train_loss:8.1705 train_time:323956ms step_avg:185.01ms\nstep:1762/4500 train_loss:8.1404 train_time:324143ms step_avg:185.01ms\nstep:1763/4500 train_loss:8.0286 train_time:324329ms step_avg:185.01ms\nstep:1764/4500 train_loss:8.0408 train_time:324514ms step_avg:185.01ms\nstep:1765/4500 train_loss:8.0026 train_time:324697ms step_avg:185.01ms\nstep:1766/4500 train_loss:8.1252 train_time:324882ms step_avg:185.01ms\nstep:1767/4500 train_loss:8.0926 train_time:325067ms step_avg:185.01ms\nstep:1768/4500 train_loss:8.1619 train_time:325253ms step_avg:185.01ms\nstep:1769/4500 train_loss:8.3217 train_time:325438ms step_avg:185.01ms\nstep:1770/4500 train_loss:8.1539 train_time:325625ms step_avg:185.01ms\nstep:1771/4500 train_loss:8.1115 train_time:325808ms step_avg:185.01ms\nstep:1772/4500 train_loss:8.1117 train_time:325994ms step_avg:185.01ms\nstep:1773/4500 train_loss:8.1130 train_time:326177ms step_avg:185.01ms\nstep:1774/4500 train_loss:8.0479 train_time:326361ms step_avg:185.01ms\nstep:1775/4500 train_loss:8.0824 train_time:326546ms step_avg:185.01ms\nstep:1776/4500 train_loss:8.1451 train_time:326733ms step_avg:185.01ms\nstep:1777/4500 train_loss:8.1512 train_time:326918ms step_avg:185.01ms\nstep:1778/4500 train_loss:8.0933 train_time:327102ms step_avg:185.01ms\nstep:1779/4500 train_loss:8.1286 train_time:327288ms step_avg:185.01ms\nstep:1780/4500 train_loss:8.1100 train_time:327472ms step_avg:185.01ms\nstep:1781/4500 train_loss:8.1202 train_time:327656ms step_avg:185.01ms\nstep:1782/4500 train_loss:8.0901 train_time:327842ms step_avg:185.01ms\nstep:1783/4500 train_loss:8.1460 train_time:328029ms step_avg:185.01ms\nstep:1784/4500 train_loss:8.0706 train_time:328213ms step_avg:185.01ms\nstep:1785/4500 train_loss:8.1349 train_time:328397ms step_avg:185.01ms\nstep:1786/4500 train_loss:8.1458 train_time:328581ms step_avg:185.01ms\nstep:1787/4500 train_loss:8.1039 train_time:328766ms step_avg:185.01ms\nstep:1788/4500 train_loss:8.1074 train_time:328950ms step_avg:185.01ms\nstep:1789/4500 train_loss:8.2142 train_time:329137ms step_avg:185.01ms\nstep:1790/4500 train_loss:8.1121 train_time:329323ms step_avg:185.01ms\nstep:1791/4500 train_loss:8.1221 train_time:329508ms step_avg:185.01ms\nstep:1792/4500 train_loss:8.0892 train_time:329694ms step_avg:185.01ms\nstep:1793/4500 train_loss:8.1505 train_time:329878ms step_avg:185.01ms\nstep:1794/4500 train_loss:8.0784 train_time:330062ms step_avg:185.01ms\nstep:1795/4500 train_loss:8.0680 train_time:330247ms step_avg:185.01ms\nstep:1796/4500 train_loss:8.0053 train_time:330433ms step_avg:185.01ms\nstep:1797/4500 train_loss:8.1919 train_time:330618ms step_avg:185.01ms\nstep:1798/4500 train_loss:7.9943 train_time:330804ms step_avg:185.01ms\nstep:1799/4500 train_loss:8.1620 train_time:330989ms step_avg:185.01ms\nstep:1800/4500 train_loss:8.1078 train_time:331171ms step_avg:185.01ms\nstep:1801/4500 train_loss:8.2280 train_time:331355ms step_avg:185.01ms\nstep:1802/4500 train_loss:7.9352 train_time:331540ms step_avg:185.01ms\nstep:1803/4500 train_loss:8.1861 train_time:331725ms step_avg:185.01ms\nstep:1804/4500 train_loss:8.1583 train_time:331911ms step_avg:185.01ms\nstep:1805/4500 train_loss:8.0976 train_time:332097ms step_avg:185.01ms\nstep:1806/4500 train_loss:8.0538 train_time:332281ms step_avg:185.01ms\nstep:1807/4500 train_loss:8.0622 train_time:332468ms step_avg:185.01ms\nstep:1808/4500 train_loss:8.1540 train_time:332654ms step_avg:185.01ms\nstep:1809/4500 train_loss:8.0997 train_time:332840ms step_avg:185.01ms\nstep:1810/4500 train_loss:8.0760 train_time:333027ms step_avg:185.01ms\nstep:1811/4500 train_loss:8.1416 train_time:333215ms step_avg:185.02ms\nstep:1812/4500 train_loss:8.2110 train_time:333401ms step_avg:185.02ms\nstep:1813/4500 train_loss:8.0802 train_time:333585ms step_avg:185.02ms\nstep:1814/4500 train_loss:8.0444 train_time:333772ms step_avg:185.02ms\nstep:1815/4500 train_loss:8.0715 train_time:333955ms step_avg:185.02ms\nstep:1816/4500 train_loss:7.9939 train_time:334140ms step_avg:185.02ms\nstep:1817/4500 train_loss:8.1002 train_time:334324ms step_avg:185.02ms\nstep:1818/4500 train_loss:8.0569 train_time:334509ms step_avg:185.02ms\nstep:1819/4500 train_loss:7.9449 train_time:334694ms step_avg:185.02ms\nstep:1820/4500 train_loss:8.0652 train_time:334878ms step_avg:185.02ms\nstep:1821/4500 train_loss:8.1959 train_time:335063ms step_avg:185.02ms\nstep:1822/4500 train_loss:8.1045 train_time:335247ms step_avg:185.02ms\nstep:1823/4500 train_loss:8.0241 train_time:335433ms step_avg:185.02ms\nstep:1824/4500 train_loss:8.1581 train_time:335616ms step_avg:185.01ms\nstep:1825/4500 train_loss:8.1661 train_time:335801ms step_avg:185.01ms\nstep:1826/4500 train_loss:8.0763 train_time:335988ms step_avg:185.02ms\nstep:1827/4500 train_loss:8.1132 train_time:336175ms step_avg:185.02ms\nstep:1828/4500 train_loss:8.1618 train_time:336360ms step_avg:185.02ms\nstep:1829/4500 train_loss:8.1799 train_time:336545ms step_avg:185.02ms\nstep:1830/4500 train_loss:8.1435 train_time:336729ms step_avg:185.02ms\nstep:1831/4500 train_loss:7.9971 train_time:336915ms step_avg:185.02ms\nstep:1832/4500 train_loss:8.1510 train_time:337100ms step_avg:185.02ms\nstep:1833/4500 train_loss:8.0359 train_time:337288ms step_avg:185.02ms\nstep:1834/4500 train_loss:8.1624 train_time:337473ms step_avg:185.02ms\nstep:1835/4500 train_loss:8.0850 train_time:337656ms step_avg:185.02ms\nstep:1836/4500 train_loss:8.1494 train_time:337841ms step_avg:185.02ms\nstep:1837/4500 train_loss:8.0707 train_time:338029ms step_avg:185.02ms\nstep:1838/4500 train_loss:8.0119 train_time:338213ms step_avg:185.02ms\nstep:1839/4500 train_loss:8.0476 train_time:338397ms step_avg:185.02ms\nstep:1840/4500 train_loss:8.1795 train_time:338583ms step_avg:185.02ms\nstep:1841/4500 train_loss:8.1129 train_time:338769ms step_avg:185.02ms\nstep:1842/4500 train_loss:8.0545 train_time:338951ms step_avg:185.02ms\nstep:1843/4500 train_loss:8.1085 train_time:339134ms step_avg:185.02ms\nstep:1844/4500 train_loss:8.1830 train_time:339317ms step_avg:185.01ms\nstep:1845/4500 train_loss:8.1278 train_time:339502ms step_avg:185.01ms\nstep:1846/4500 train_loss:8.1342 train_time:339689ms step_avg:185.02ms\nstep:1847/4500 train_loss:8.0964 train_time:339874ms step_avg:185.02ms\nstep:1848/4500 train_loss:8.1105 train_time:340058ms step_avg:185.02ms\nstep:1849/4500 train_loss:8.0988 train_time:340244ms step_avg:185.02ms\nstep:1850/4500 train_loss:7.9457 train_time:340431ms step_avg:185.02ms\nstep:1851/4500 train_loss:8.2194 train_time:340615ms step_avg:185.02ms\nstep:1852/4500 train_loss:7.9649 train_time:340801ms step_avg:185.02ms\nstep:1853/4500 train_loss:8.0110 train_time:340987ms step_avg:185.02ms\nstep:1854/4500 train_loss:8.1628 train_time:341171ms step_avg:185.02ms\nstep:1855/4500 train_loss:8.2996 train_time:341354ms step_avg:185.02ms\nstep:1856/4500 train_loss:8.0760 train_time:341539ms step_avg:185.02ms\nstep:1857/4500 train_loss:8.1698 train_time:341726ms step_avg:185.02ms\nstep:1858/4500 train_loss:8.1144 train_time:341912ms step_avg:185.02ms\nstep:1859/4500 train_loss:8.1005 train_time:342096ms step_avg:185.02ms\nstep:1860/4500 train_loss:8.2078 train_time:342283ms step_avg:185.02ms\nstep:1861/4500 train_loss:8.1675 train_time:342469ms step_avg:185.02ms\nstep:1862/4500 train_loss:8.1045 train_time:342656ms step_avg:185.02ms\nstep:1863/4500 train_loss:8.0700 train_time:342843ms step_avg:185.02ms\nstep:1864/4500 train_loss:8.0108 train_time:343031ms step_avg:185.02ms\nstep:1865/4500 train_loss:8.1576 train_time:343218ms step_avg:185.02ms\nstep:1866/4500 train_loss:8.1850 train_time:343402ms step_avg:185.02ms\nstep:1867/4500 train_loss:8.1589 train_time:343587ms step_avg:185.02ms\nstep:1868/4500 train_loss:8.1259 train_time:343770ms step_avg:185.02ms\nstep:1869/4500 train_loss:8.1996 train_time:343954ms step_avg:185.02ms\nstep:1870/4500 train_loss:8.3102 train_time:344138ms step_avg:185.02ms\nstep:1871/4500 train_loss:8.1187 train_time:344322ms step_avg:185.02ms\nstep:1872/4500 train_loss:8.0789 train_time:344506ms step_avg:185.02ms\nstep:1873/4500 train_loss:8.1568 train_time:344689ms step_avg:185.02ms\nstep:1874/4500 train_loss:8.1284 train_time:344873ms step_avg:185.02ms\nstep:1875/4500 train_loss:8.1530 train_time:345056ms step_avg:185.02ms\nstep:1876/4500 train_loss:8.1697 train_time:345242ms step_avg:185.02ms\nstep:1877/4500 train_loss:8.1954 train_time:345427ms step_avg:185.02ms\nstep:1878/4500 train_loss:8.1307 train_time:345611ms step_avg:185.02ms\nstep:1879/4500 train_loss:8.1305 train_time:345796ms step_avg:185.02ms\nstep:1880/4500 train_loss:8.1386 train_time:345982ms step_avg:185.02ms\nstep:1881/4500 train_loss:8.0145 train_time:346170ms step_avg:185.02ms\nstep:1882/4500 train_loss:8.1584 train_time:346353ms step_avg:185.02ms\nstep:1883/4500 train_loss:8.1995 train_time:346536ms step_avg:185.02ms\nstep:1884/4500 train_loss:8.0328 train_time:346721ms step_avg:185.02ms\nstep:1885/4500 train_loss:7.9448 train_time:346906ms step_avg:185.02ms\nstep:1886/4500 train_loss:8.1257 train_time:347091ms step_avg:185.02ms\nstep:1887/4500 train_loss:8.4668 train_time:347276ms step_avg:185.02ms\nstep:1888/4500 train_loss:8.1020 train_time:347462ms step_avg:185.02ms\nstep:1889/4500 train_loss:8.0784 train_time:347646ms step_avg:185.02ms\nstep:1890/4500 train_loss:8.0881 train_time:347833ms step_avg:185.02ms\nstep:1891/4500 train_loss:8.1793 train_time:348018ms step_avg:185.02ms\nstep:1892/4500 train_loss:8.1354 train_time:348203ms step_avg:185.02ms\nstep:1893/4500 train_loss:8.1937 train_time:348389ms step_avg:185.02ms\nstep:1894/4500 train_loss:8.1067 train_time:348573ms step_avg:185.02ms\nstep:1895/4500 train_loss:8.0836 train_time:348757ms step_avg:185.02ms\nstep:1896/4500 train_loss:8.1956 train_time:348941ms step_avg:185.02ms\nstep:1897/4500 train_loss:8.1076 train_time:349126ms step_avg:185.02ms\nstep:1898/4500 train_loss:8.0532 train_time:349311ms step_avg:185.02ms\nstep:1899/4500 train_loss:8.2849 train_time:349493ms step_avg:185.01ms\nstep:1900/4500 train_loss:8.0735 train_time:349680ms step_avg:185.02ms\nstep:1901/4500 train_loss:8.1194 train_time:349866ms step_avg:185.02ms\nstep:1902/4500 train_loss:8.1734 train_time:350049ms step_avg:185.02ms\nstep:1903/4500 train_loss:8.0029 train_time:350234ms step_avg:185.02ms\nstep:1904/4500 train_loss:8.0107 train_time:350417ms step_avg:185.01ms\nstep:1905/4500 train_loss:8.1401 train_time:350602ms step_avg:185.01ms\nstep:1906/4500 train_loss:8.1362 train_time:350788ms step_avg:185.01ms\nstep:1907/4500 train_loss:8.2054 train_time:350975ms step_avg:185.02ms\nstep:1908/4500 train_loss:8.1774 train_time:351160ms step_avg:185.02ms\nstep:1909/4500 train_loss:7.9521 train_time:351346ms step_avg:185.02ms\nstep:1910/4500 train_loss:8.1410 train_time:351533ms step_avg:185.02ms\nstep:1911/4500 train_loss:8.2143 train_time:351716ms step_avg:185.02ms\nstep:1912/4500 train_loss:8.1769 train_time:351900ms step_avg:185.02ms\nstep:1913/4500 train_loss:7.9949 train_time:352085ms step_avg:185.02ms\nstep:1914/4500 train_loss:8.0507 train_time:352270ms step_avg:185.02ms\nstep:1915/4500 train_loss:8.0597 train_time:352455ms step_avg:185.02ms\nstep:1916/4500 train_loss:8.1409 train_time:352641ms step_avg:185.02ms\nstep:1917/4500 train_loss:8.0902 train_time:352827ms step_avg:185.02ms\nstep:1918/4500 train_loss:8.1334 train_time:353010ms step_avg:185.02ms\nstep:1919/4500 train_loss:8.1223 train_time:353193ms step_avg:185.01ms\nstep:1920/4500 train_loss:8.0446 train_time:353375ms step_avg:185.01ms\nstep:1921/4500 train_loss:8.2145 train_time:353561ms step_avg:185.01ms\nstep:1922/4500 train_loss:8.1788 train_time:353744ms step_avg:185.01ms\nstep:1923/4500 train_loss:8.0677 train_time:353933ms step_avg:185.01ms\nstep:1924/4500 train_loss:8.1990 train_time:354118ms step_avg:185.01ms\nstep:1925/4500 train_loss:8.0482 train_time:354305ms step_avg:185.02ms\nstep:1926/4500 train_loss:8.1145 train_time:354489ms step_avg:185.02ms\nstep:1927/4500 train_loss:8.0309 train_time:354674ms step_avg:185.02ms\nstep:1928/4500 train_loss:8.1891 train_time:354858ms step_avg:185.01ms\nstep:1929/4500 train_loss:8.0905 train_time:355043ms step_avg:185.01ms\nstep:1930/4500 train_loss:8.0018 train_time:355227ms step_avg:185.01ms\nstep:1931/4500 train_loss:8.0868 train_time:355411ms step_avg:185.01ms\nstep:1932/4500 train_loss:8.0286 train_time:355595ms step_avg:185.01ms\nstep:1933/4500 train_loss:7.9350 train_time:355781ms step_avg:185.01ms\nstep:1934/4500 train_loss:8.1240 train_time:355965ms step_avg:185.01ms\nstep:1935/4500 train_loss:8.0574 train_time:356148ms step_avg:185.01ms\nstep:1936/4500 train_loss:8.0940 train_time:356335ms step_avg:185.01ms\nstep:1937/4500 train_loss:8.0857 train_time:356517ms step_avg:185.01ms\nstep:1938/4500 train_loss:8.1264 train_time:356701ms step_avg:185.01ms\nstep:1939/4500 train_loss:8.1202 train_time:356886ms step_avg:185.01ms\nstep:1940/4500 train_loss:8.1142 train_time:357070ms step_avg:185.01ms\nstep:1941/4500 train_loss:8.1602 train_time:357254ms step_avg:185.01ms\nstep:1942/4500 train_loss:8.1238 train_time:357438ms step_avg:185.01ms\nstep:1943/4500 train_loss:8.1035 train_time:357624ms step_avg:185.01ms\nstep:1944/4500 train_loss:8.0402 train_time:357808ms step_avg:185.01ms\nstep:1945/4500 train_loss:8.0860 train_time:357995ms step_avg:185.01ms\nstep:1946/4500 train_loss:8.1313 train_time:358180ms step_avg:185.01ms\nstep:1947/4500 train_loss:8.1090 train_time:358369ms step_avg:185.01ms\nstep:1948/4500 train_loss:8.2597 train_time:358557ms step_avg:185.01ms\nstep:1949/4500 train_loss:8.0755 train_time:358742ms step_avg:185.01ms\nstep:1950/4500 train_loss:8.0198 train_time:358928ms step_avg:185.01ms\nstep:1951/4500 train_loss:8.1167 train_time:359112ms step_avg:185.01ms\nstep:1952/4500 train_loss:8.1521 train_time:359295ms step_avg:185.01ms\nstep:1953/4500 train_loss:8.0770 train_time:359479ms step_avg:185.01ms\nstep:1954/4500 train_loss:8.2290 train_time:359664ms step_avg:185.01ms\nstep:1955/4500 train_loss:8.1304 train_time:359847ms step_avg:185.01ms\nstep:1956/4500 train_loss:8.0278 train_time:360030ms step_avg:185.01ms\nstep:1957/4500 train_loss:8.1071 train_time:360214ms step_avg:185.01ms\nstep:1958/4500 train_loss:8.1004 train_time:360397ms step_avg:185.01ms\nstep:1959/4500 train_loss:8.0651 train_time:360583ms step_avg:185.01ms\nstep:1960/4500 train_loss:8.1540 train_time:360766ms step_avg:185.01ms\nstep:1961/4500 train_loss:8.0737 train_time:360951ms step_avg:185.01ms\nstep:1962/4500 train_loss:8.1163 train_time:361136ms step_avg:185.01ms\nstep:1963/4500 train_loss:8.0554 train_time:361323ms step_avg:185.01ms\nstep:1964/4500 train_loss:8.1182 train_time:361511ms step_avg:185.01ms\nstep:1965/4500 train_loss:7.9627 train_time:361695ms step_avg:185.01ms\nstep:1966/4500 train_loss:8.0588 train_time:361880ms step_avg:185.01ms\nstep:1967/4500 train_loss:8.1948 train_time:362065ms step_avg:185.01ms\nstep:1968/4500 train_loss:8.0878 train_time:362257ms step_avg:185.01ms\nstep:1969/4500 train_loss:8.0275 train_time:362439ms step_avg:185.01ms\nstep:1970/4500 train_loss:8.1478 train_time:362626ms step_avg:185.01ms\nstep:1971/4500 train_loss:8.0853 train_time:362810ms step_avg:185.01ms\nstep:1972/4500 train_loss:8.1632 train_time:362994ms step_avg:185.01ms\nstep:1973/4500 train_loss:8.3069 train_time:363176ms step_avg:185.01ms\nstep:1974/4500 train_loss:8.0261 train_time:363360ms step_avg:185.01ms\nstep:1975/4500 train_loss:8.0573 train_time:363544ms step_avg:185.01ms\nstep:1976/4500 train_loss:8.0849 train_time:363729ms step_avg:185.01ms\nstep:1977/4500 train_loss:8.0976 train_time:363912ms step_avg:185.01ms\nstep:1978/4500 train_loss:8.1440 train_time:364097ms step_avg:185.01ms\nstep:1979/4500 train_loss:8.0669 train_time:364282ms step_avg:185.01ms\nstep:1980/4500 train_loss:8.1216 train_time:364466ms step_avg:185.01ms\nstep:1981/4500 train_loss:8.0900 train_time:364650ms step_avg:185.01ms\nstep:1982/4500 train_loss:7.9910 train_time:364835ms step_avg:185.01ms\nstep:1983/4500 train_loss:8.1745 train_time:365021ms step_avg:185.01ms\nstep:1984/4500 train_loss:8.1477 train_time:365210ms step_avg:185.01ms\nstep:1985/4500 train_loss:8.0919 train_time:365393ms step_avg:185.01ms\nstep:1986/4500 train_loss:8.2825 train_time:365576ms step_avg:185.01ms\nstep:1987/4500 train_loss:8.3036 train_time:365759ms step_avg:185.01ms\nstep:1988/4500 train_loss:8.0992 train_time:365945ms step_avg:185.01ms\nstep:1989/4500 train_loss:8.1086 train_time:366129ms step_avg:185.01ms\nstep:1990/4500 train_loss:8.1558 train_time:366312ms step_avg:185.01ms\nstep:1991/4500 train_loss:8.0972 train_time:366496ms step_avg:185.01ms\nstep:1992/4500 train_loss:8.0131 train_time:366680ms step_avg:185.00ms\nstep:1993/4500 train_loss:8.4514 train_time:366863ms step_avg:185.00ms\nstep:1994/4500 train_loss:8.2224 train_time:367046ms step_avg:185.00ms\nstep:1995/4500 train_loss:8.2087 train_time:367230ms step_avg:185.00ms\nstep:1996/4500 train_loss:8.0843 train_time:367413ms step_avg:185.00ms\nstep:1997/4500 train_loss:8.0988 train_time:367595ms step_avg:185.00ms\nstep:1998/4500 train_loss:8.1741 train_time:367780ms step_avg:185.00ms\nstep:1999/4500 train_loss:8.0893 train_time:367966ms step_avg:185.00ms\nstep:2000/4500 train_loss:8.1770 train_time:368149ms step_avg:185.00ms\nstep:2001/4500 train_loss:8.2192 train_time:368332ms step_avg:185.00ms\nstep:2002/4500 train_loss:8.1887 train_time:368516ms step_avg:185.00ms\nstep:2003/4500 train_loss:8.1538 train_time:368702ms step_avg:185.00ms\nstep:2004/4500 train_loss:8.1466 train_time:368887ms step_avg:185.00ms\nstep:2005/4500 train_loss:8.1170 train_time:369071ms step_avg:185.00ms\nstep:2006/4500 train_loss:8.0124 train_time:369255ms step_avg:185.00ms\nstep:2007/4500 train_loss:8.2359 train_time:369441ms step_avg:185.00ms\nstep:2008/4500 train_loss:8.0108 train_time:369626ms step_avg:185.00ms\nstep:2009/4500 train_loss:8.1389 train_time:369813ms step_avg:185.00ms\nstep:2010/4500 train_loss:8.0968 train_time:369998ms step_avg:185.00ms\nstep:2011/4500 train_loss:8.0537 train_time:370186ms step_avg:185.00ms\nstep:2012/4500 train_loss:8.0347 train_time:370369ms step_avg:185.00ms\nstep:2013/4500 train_loss:7.9588 train_time:370556ms step_avg:185.00ms\nstep:2014/4500 train_loss:8.0665 train_time:370741ms step_avg:185.00ms\nstep:2015/4500 train_loss:8.0555 train_time:370926ms step_avg:185.00ms\nstep:2016/4500 train_loss:8.1348 train_time:371113ms step_avg:185.00ms\nstep:2017/4500 train_loss:8.0126 train_time:371299ms step_avg:185.00ms\nstep:2018/4500 train_loss:8.1667 train_time:371483ms step_avg:185.00ms\nstep:2019/4500 train_loss:8.1511 train_time:371667ms step_avg:185.00ms\nstep:2020/4500 train_loss:8.3220 train_time:371853ms step_avg:185.00ms\nstep:2021/4500 train_loss:7.9185 train_time:372036ms step_avg:185.00ms\nstep:2022/4500 train_loss:8.2781 train_time:372220ms step_avg:185.00ms\nstep:2023/4500 train_loss:7.9536 train_time:372404ms step_avg:185.00ms\nstep:2024/4500 train_loss:8.1563 train_time:372588ms step_avg:185.00ms\nstep:2025/4500 train_loss:8.1197 train_time:372774ms step_avg:185.00ms\nstep:2026/4500 train_loss:7.9911 train_time:372961ms step_avg:185.00ms\nstep:2027/4500 train_loss:8.0295 train_time:373146ms step_avg:185.00ms\nstep:2028/4500 train_loss:8.1334 train_time:373333ms step_avg:185.00ms\nstep:2029/4500 train_loss:7.9978 train_time:373519ms step_avg:185.00ms\nstep:2030/4500 train_loss:8.1457 train_time:373704ms step_avg:185.00ms\nstep:2031/4500 train_loss:8.1122 train_time:373889ms step_avg:185.00ms\nstep:2032/4500 train_loss:8.0760 train_time:374076ms step_avg:185.00ms\nstep:2033/4500 train_loss:8.1512 train_time:374260ms step_avg:185.00ms\nstep:2034/4500 train_loss:8.2326 train_time:374443ms step_avg:185.00ms\nstep:2035/4500 train_loss:8.1240 train_time:374628ms step_avg:185.00ms\nstep:2036/4500 train_loss:8.0510 train_time:374812ms step_avg:185.00ms\nstep:2037/4500 train_loss:8.2103 train_time:374996ms step_avg:185.00ms\nstep:2038/4500 train_loss:8.1412 train_time:375181ms step_avg:185.00ms\nstep:2039/4500 train_loss:8.1642 train_time:375366ms step_avg:185.00ms\nstep:2040/4500 train_loss:8.0933 train_time:375550ms step_avg:185.00ms\nstep:2041/4500 train_loss:8.1285 train_time:375736ms step_avg:185.00ms\nstep:2042/4500 train_loss:8.0801 train_time:375922ms step_avg:185.00ms\nstep:2043/4500 train_loss:8.0676 train_time:376107ms step_avg:185.00ms\nstep:2044/4500 train_loss:8.1324 train_time:376292ms step_avg:185.00ms\nstep:2045/4500 train_loss:8.1394 train_time:376476ms step_avg:185.00ms\nstep:2046/4500 train_loss:8.1895 train_time:376660ms step_avg:185.00ms\nstep:2047/4500 train_loss:8.0117 train_time:376846ms step_avg:185.00ms\nstep:2048/4500 train_loss:7.9576 train_time:377033ms step_avg:185.00ms\nstep:2049/4500 train_loss:8.0964 train_time:377219ms step_avg:185.00ms\nstep:2050/4500 train_loss:8.0133 train_time:377406ms step_avg:185.00ms\nstep:2051/4500 train_loss:8.1185 train_time:377595ms step_avg:185.00ms\nstep:2052/4500 train_loss:8.0627 train_time:377779ms step_avg:185.00ms\nstep:2053/4500 train_loss:8.1379 train_time:377965ms step_avg:185.00ms\nstep:2054/4500 train_loss:8.1884 train_time:378147ms step_avg:185.00ms\nstep:2055/4500 train_loss:8.1382 train_time:378333ms step_avg:185.00ms\nstep:2056/4500 train_loss:8.1772 train_time:378518ms step_avg:185.00ms\nstep:2057/4500 train_loss:8.0961 train_time:378706ms step_avg:185.01ms\nstep:2058/4500 train_loss:8.0857 train_time:378895ms step_avg:185.01ms\nstep:2059/4500 train_loss:7.9842 train_time:379080ms step_avg:185.01ms\nstep:2060/4500 train_loss:8.3465 train_time:379269ms step_avg:185.01ms\nstep:2061/4500 train_loss:7.9499 train_time:379452ms step_avg:185.01ms\nstep:2062/4500 train_loss:8.2783 train_time:379637ms step_avg:185.01ms\nstep:2063/4500 train_loss:8.1353 train_time:379821ms step_avg:185.01ms\nstep:2064/4500 train_loss:8.1171 train_time:380008ms step_avg:185.01ms\nstep:2065/4500 train_loss:8.0597 train_time:380193ms step_avg:185.01ms\nstep:2066/4500 train_loss:8.5935 train_time:380377ms step_avg:185.01ms\nstep:2067/4500 train_loss:8.0958 train_time:380565ms step_avg:185.01ms\nstep:2068/4500 train_loss:8.0335 train_time:380751ms step_avg:185.01ms\nstep:2069/4500 train_loss:8.1055 train_time:380935ms step_avg:185.01ms\nstep:2070/4500 train_loss:8.1643 train_time:381119ms step_avg:185.01ms\nstep:2071/4500 train_loss:8.1020 train_time:381304ms step_avg:185.01ms\nstep:2072/4500 train_loss:8.1242 train_time:381488ms step_avg:185.01ms\nstep:2073/4500 train_loss:8.0850 train_time:381672ms step_avg:185.01ms\nstep:2074/4500 train_loss:8.2191 train_time:381857ms step_avg:185.01ms\nstep:2075/4500 train_loss:8.0284 train_time:382042ms step_avg:185.01ms\nstep:2076/4500 train_loss:8.0562 train_time:382226ms step_avg:185.01ms\nstep:2077/4500 train_loss:8.0951 train_time:382414ms step_avg:185.01ms\nstep:2078/4500 train_loss:8.1457 train_time:382598ms step_avg:185.01ms\nstep:2079/4500 train_loss:8.0290 train_time:382784ms step_avg:185.01ms\nstep:2080/4500 train_loss:8.0828 train_time:382970ms step_avg:185.01ms\nstep:2081/4500 train_loss:8.1115 train_time:383155ms step_avg:185.01ms\nstep:2082/4500 train_loss:8.2073 train_time:383339ms step_avg:185.01ms\nstep:2083/4500 train_loss:8.0862 train_time:383523ms step_avg:185.01ms\nstep:2084/4500 train_loss:8.0793 train_time:383711ms step_avg:185.01ms\nstep:2085/4500 train_loss:8.0908 train_time:383894ms step_avg:185.01ms\nstep:2086/4500 train_loss:8.0249 train_time:384079ms step_avg:185.01ms\nstep:2087/4500 train_loss:8.1493 train_time:384263ms step_avg:185.01ms\nstep:2088/4500 train_loss:8.0395 train_time:384448ms step_avg:185.01ms\nstep:2089/4500 train_loss:8.1080 train_time:384634ms step_avg:185.01ms\nstep:2090/4500 train_loss:8.0824 train_time:384819ms step_avg:185.01ms\nstep:2091/4500 train_loss:8.1046 train_time:385008ms step_avg:185.01ms\nstep:2092/4500 train_loss:8.1391 train_time:385193ms step_avg:185.01ms\nstep:2093/4500 train_loss:8.1682 train_time:385375ms step_avg:185.01ms\nstep:2094/4500 train_loss:8.4318 train_time:385558ms step_avg:185.01ms\nstep:2095/4500 train_loss:8.0920 train_time:385743ms step_avg:185.01ms\nstep:2096/4500 train_loss:8.0286 train_time:385928ms step_avg:185.01ms\nstep:2097/4500 train_loss:8.1270 train_time:386112ms step_avg:185.01ms\nstep:2098/4500 train_loss:8.1732 train_time:386296ms step_avg:185.01ms\nstep:2099/4500 train_loss:8.0926 train_time:386482ms step_avg:185.01ms\nstep:2100/4500 train_loss:8.0784 train_time:386666ms step_avg:185.01ms\nstep:2101/4500 train_loss:8.1711 train_time:386853ms step_avg:185.01ms\nstep:2102/4500 train_loss:8.1681 train_time:387039ms step_avg:185.01ms\nstep:2103/4500 train_loss:8.0887 train_time:387226ms step_avg:185.01ms\nstep:2104/4500 train_loss:8.0977 train_time:387409ms step_avg:185.01ms\nstep:2105/4500 train_loss:8.0300 train_time:387593ms step_avg:185.01ms\nstep:2106/4500 train_loss:8.1323 train_time:387777ms step_avg:185.01ms\nstep:2107/4500 train_loss:8.2252 train_time:387960ms step_avg:185.01ms\nstep:2108/4500 train_loss:8.1612 train_time:388144ms step_avg:185.01ms\nstep:2109/4500 train_loss:8.0686 train_time:388328ms step_avg:185.01ms\nstep:2110/4500 train_loss:8.0427 train_time:388512ms step_avg:185.01ms\nstep:2111/4500 train_loss:8.1035 train_time:388697ms step_avg:185.01ms\nstep:2112/4500 train_loss:7.9793 train_time:388886ms step_avg:185.01ms\nstep:2113/4500 train_loss:8.1194 train_time:389069ms step_avg:185.01ms\nstep:2114/4500 train_loss:8.0821 train_time:389256ms step_avg:185.01ms\nstep:2115/4500 train_loss:8.2176 train_time:389441ms step_avg:185.01ms\nstep:2116/4500 train_loss:8.0414 train_time:389625ms step_avg:185.01ms\nstep:2117/4500 train_loss:8.0712 train_time:389810ms step_avg:185.01ms\nstep:2118/4500 train_loss:8.0869 train_time:389993ms step_avg:185.01ms\nstep:2119/4500 train_loss:8.1885 train_time:390177ms step_avg:185.01ms\nstep:2120/4500 train_loss:8.1443 train_time:390361ms step_avg:185.01ms\nstep:2121/4500 train_loss:8.1394 train_time:390545ms step_avg:185.00ms\nstep:2122/4500 train_loss:8.0694 train_time:390728ms step_avg:185.00ms\nstep:2123/4500 train_loss:8.0348 train_time:390912ms step_avg:185.00ms\nstep:2124/4500 train_loss:8.1896 train_time:391095ms step_avg:185.00ms\nstep:2125/4500 train_loss:8.1779 train_time:391279ms step_avg:185.00ms\nstep:2126/4500 train_loss:8.1979 train_time:391465ms step_avg:185.00ms\nstep:2127/4500 train_loss:8.1086 train_time:391649ms step_avg:185.00ms\nstep:2128/4500 train_loss:8.0752 train_time:391832ms step_avg:185.00ms\nstep:2129/4500 train_loss:8.0550 train_time:392016ms step_avg:185.00ms\nstep:2130/4500 train_loss:8.0933 train_time:392200ms step_avg:185.00ms\nstep:2131/4500 train_loss:8.1480 train_time:392389ms step_avg:185.00ms\nstep:2132/4500 train_loss:8.1400 train_time:392573ms step_avg:185.00ms\nstep:2133/4500 train_loss:7.6799 train_time:392757ms step_avg:185.00ms\nstep:2134/4500 train_loss:8.2418 train_time:392942ms step_avg:185.00ms\nstep:2135/4500 train_loss:8.0411 train_time:393126ms step_avg:185.00ms\nstep:2136/4500 train_loss:8.1572 train_time:393312ms step_avg:185.00ms\nstep:2137/4500 train_loss:8.2121 train_time:393496ms step_avg:185.00ms\nstep:2138/4500 train_loss:8.0653 train_time:393682ms step_avg:185.00ms\nstep:2139/4500 train_loss:8.1292 train_time:393869ms step_avg:185.00ms\nstep:2140/4500 train_loss:8.1544 train_time:394052ms step_avg:185.00ms\nstep:2141/4500 train_loss:8.1098 train_time:394236ms step_avg:185.00ms\nstep:2142/4500 train_loss:8.0805 train_time:394421ms step_avg:185.00ms\nstep:2143/4500 train_loss:7.8744 train_time:394606ms step_avg:185.00ms\nstep:2144/4500 train_loss:8.2427 train_time:394794ms step_avg:185.00ms\nstep:2145/4500 train_loss:8.2901 train_time:394981ms step_avg:185.00ms\nstep:2146/4500 train_loss:8.0839 train_time:395168ms step_avg:185.00ms\nstep:2147/4500 train_loss:8.0807 train_time:395352ms step_avg:185.00ms\nstep:2148/4500 train_loss:8.1871 train_time:395536ms step_avg:185.00ms\nstep:2149/4500 train_loss:8.0708 train_time:395720ms step_avg:185.00ms\nstep:2150/4500 train_loss:8.1090 train_time:395906ms step_avg:185.00ms\nstep:2151/4500 train_loss:8.4222 train_time:396090ms step_avg:185.00ms\nstep:2152/4500 train_loss:8.0143 train_time:396275ms step_avg:185.00ms\nstep:2153/4500 train_loss:8.0703 train_time:396459ms step_avg:185.00ms\nstep:2154/4500 train_loss:8.1399 train_time:396643ms step_avg:185.00ms\nstep:2155/4500 train_loss:8.1113 train_time:396829ms step_avg:185.00ms\nstep:2156/4500 train_loss:7.9729 train_time:397014ms step_avg:185.00ms\nstep:2157/4500 train_loss:8.1156 train_time:397200ms step_avg:185.00ms\nstep:2158/4500 train_loss:8.2069 train_time:397387ms step_avg:185.00ms\nstep:2159/4500 train_loss:8.0320 train_time:397570ms step_avg:185.00ms\nstep:2160/4500 train_loss:8.0154 train_time:397755ms step_avg:185.00ms\nstep:2161/4500 train_loss:8.1090 train_time:397941ms step_avg:185.00ms\nstep:2162/4500 train_loss:8.0957 train_time:398127ms step_avg:185.00ms\nstep:2163/4500 train_loss:8.1983 train_time:398313ms step_avg:185.00ms\nstep:2164/4500 train_loss:8.1520 train_time:398496ms step_avg:185.00ms\nstep:2165/4500 train_loss:7.8016 train_time:398681ms step_avg:185.00ms\nstep:2166/4500 train_loss:8.1162 train_time:398868ms step_avg:185.00ms\nstep:2167/4500 train_loss:8.4817 train_time:399051ms step_avg:185.00ms\nstep:2168/4500 train_loss:7.9955 train_time:399235ms step_avg:185.00ms\nstep:2169/4500 train_loss:8.1032 train_time:399420ms step_avg:185.00ms\nstep:2170/4500 train_loss:8.1082 train_time:399606ms step_avg:185.00ms\nstep:2171/4500 train_loss:8.2913 train_time:399792ms step_avg:185.00ms\nstep:2172/4500 train_loss:8.1280 train_time:399974ms step_avg:185.00ms\nstep:2173/4500 train_loss:8.1538 train_time:400159ms step_avg:185.00ms\nstep:2174/4500 train_loss:8.1333 train_time:400346ms step_avg:185.00ms\nstep:2175/4500 train_loss:8.1273 train_time:400533ms step_avg:185.00ms\nstep:2176/4500 train_loss:8.1647 train_time:400716ms step_avg:185.00ms\nstep:2177/4500 train_loss:8.1101 train_time:400900ms step_avg:185.00ms\nstep:2178/4500 train_loss:8.2425 train_time:401084ms step_avg:185.00ms\nstep:2179/4500 train_loss:8.0370 train_time:401268ms step_avg:185.00ms\nstep:2180/4500 train_loss:8.2141 train_time:401452ms step_avg:185.00ms\nstep:2181/4500 train_loss:8.3113 train_time:401636ms step_avg:185.00ms\nstep:2182/4500 train_loss:8.1438 train_time:401822ms step_avg:185.00ms\nstep:2183/4500 train_loss:8.0117 train_time:402006ms step_avg:185.00ms\nstep:2184/4500 train_loss:8.0836 train_time:402191ms step_avg:185.00ms\nstep:2185/4500 train_loss:8.1236 train_time:402375ms step_avg:185.00ms\nstep:2186/4500 train_loss:8.1397 train_time:402561ms step_avg:185.00ms\nstep:2187/4500 train_loss:8.0503 train_time:402746ms step_avg:185.00ms\nstep:2188/4500 train_loss:7.9759 train_time:402932ms step_avg:185.00ms\nstep:2189/4500 train_loss:8.0868 train_time:403115ms step_avg:185.00ms\nstep:2190/4500 train_loss:8.1734 train_time:403299ms step_avg:185.00ms\nstep:2191/4500 train_loss:8.0086 train_time:403482ms step_avg:185.00ms\nstep:2192/4500 train_loss:8.1531 train_time:403668ms step_avg:185.00ms\nstep:2193/4500 train_loss:8.3872 train_time:403852ms step_avg:185.00ms\nstep:2194/4500 train_loss:8.2114 train_time:404036ms step_avg:185.00ms\nstep:2195/4500 train_loss:8.1361 train_time:404222ms step_avg:185.00ms\nstep:2196/4500 train_loss:8.3585 train_time:404406ms step_avg:185.00ms\nstep:2197/4500 train_loss:8.0903 train_time:404589ms step_avg:185.00ms\nstep:2198/4500 train_loss:8.1260 train_time:404774ms step_avg:185.00ms\nstep:2199/4500 train_loss:8.0961 train_time:404960ms step_avg:185.00ms\nstep:2200/4500 train_loss:8.2323 train_time:405150ms step_avg:185.00ms\nstep:2201/4500 train_loss:8.0360 train_time:405336ms step_avg:185.00ms\nstep:2202/4500 train_loss:8.0545 train_time:405519ms step_avg:185.00ms\nstep:2203/4500 train_loss:8.1706 train_time:405705ms step_avg:185.00ms\nstep:2204/4500 train_loss:8.0782 train_time:405892ms step_avg:185.00ms\nstep:2205/4500 train_loss:8.1287 train_time:406077ms step_avg:185.00ms\nstep:2206/4500 train_loss:8.1114 train_time:406262ms step_avg:185.00ms\nstep:2207/4500 train_loss:8.1080 train_time:406447ms step_avg:185.00ms\nstep:2208/4500 train_loss:8.0889 train_time:406630ms step_avg:185.00ms\nstep:2209/4500 train_loss:8.0781 train_time:406814ms step_avg:185.00ms\nstep:2210/4500 train_loss:8.1493 train_time:406998ms step_avg:185.00ms\nstep:2211/4500 train_loss:8.0348 train_time:407183ms step_avg:185.00ms\nstep:2212/4500 train_loss:8.3015 train_time:407368ms step_avg:185.00ms\nstep:2213/4500 train_loss:8.2241 train_time:407554ms step_avg:185.00ms\nstep:2214/4500 train_loss:8.1243 train_time:407737ms step_avg:185.00ms\nstep:2215/4500 train_loss:8.1282 train_time:407923ms step_avg:185.00ms\nstep:2216/4500 train_loss:8.1860 train_time:408109ms step_avg:185.00ms\nstep:2217/4500 train_loss:8.1413 train_time:408296ms step_avg:185.00ms\nstep:2218/4500 train_loss:8.1021 train_time:408481ms step_avg:185.00ms\nstep:2219/4500 train_loss:8.1675 train_time:408666ms step_avg:185.00ms\nstep:2220/4500 train_loss:8.1697 train_time:408850ms step_avg:185.00ms\nstep:2221/4500 train_loss:8.1105 train_time:409035ms step_avg:185.00ms\nstep:2222/4500 train_loss:8.3573 train_time:409218ms step_avg:185.00ms\nstep:2223/4500 train_loss:8.2385 train_time:409403ms step_avg:185.00ms\nstep:2224/4500 train_loss:8.0849 train_time:409590ms step_avg:185.00ms\nstep:2225/4500 train_loss:8.0951 train_time:409776ms step_avg:185.00ms\nstep:2226/4500 train_loss:8.1633 train_time:409961ms step_avg:185.00ms\nstep:2227/4500 train_loss:8.1491 train_time:410146ms step_avg:185.00ms\nstep:2228/4500 train_loss:8.1460 train_time:410331ms step_avg:185.00ms\nstep:2229/4500 train_loss:8.0268 train_time:410516ms step_avg:185.00ms\nstep:2230/4500 train_loss:8.3551 train_time:410701ms step_avg:185.00ms\nstep:2231/4500 train_loss:8.1522 train_time:410887ms step_avg:185.00ms\nstep:2232/4500 train_loss:8.0828 train_time:411073ms step_avg:185.00ms\nstep:2233/4500 train_loss:8.1135 train_time:411257ms step_avg:185.00ms\nstep:2234/4500 train_loss:8.0212 train_time:411443ms step_avg:185.00ms\nstep:2235/4500 train_loss:8.1429 train_time:411627ms step_avg:185.00ms\nstep:2236/4500 train_loss:8.1041 train_time:411814ms step_avg:185.00ms\nstep:2237/4500 train_loss:8.2343 train_time:411999ms step_avg:185.00ms\nstep:2238/4500 train_loss:8.1686 train_time:412187ms step_avg:185.00ms\nstep:2239/4500 train_loss:8.2159 train_time:412371ms step_avg:185.00ms\nstep:2240/4500 train_loss:8.0502 train_time:412555ms step_avg:185.00ms\nstep:2241/4500 train_loss:8.2166 train_time:412739ms step_avg:185.00ms\nstep:2242/4500 train_loss:8.2302 train_time:412923ms step_avg:185.00ms\nstep:2243/4500 train_loss:8.1793 train_time:413107ms step_avg:185.00ms\nstep:2244/4500 train_loss:8.1074 train_time:413293ms step_avg:185.00ms\nstep:2245/4500 train_loss:8.0363 train_time:413476ms step_avg:185.00ms\nstep:2246/4500 train_loss:8.0869 train_time:413661ms step_avg:185.00ms\nstep:2247/4500 train_loss:8.1217 train_time:413850ms step_avg:185.00ms\nstep:2248/4500 train_loss:8.0778 train_time:414035ms step_avg:185.00ms\nstep:2249/4500 train_loss:8.0711 train_time:414221ms step_avg:185.00ms\nstep:2250/4500 train_loss:8.0301 train_time:414408ms step_avg:185.00ms\nstep:2251/4500 train_loss:8.0984 train_time:414593ms step_avg:185.00ms\nstep:2252/4500 train_loss:8.1100 train_time:414777ms step_avg:185.00ms\nstep:2253/4500 train_loss:8.1459 train_time:414962ms step_avg:185.00ms\nstep:2254/4500 train_loss:8.1113 train_time:415149ms step_avg:185.00ms\nstep:2255/4500 train_loss:8.0505 train_time:415334ms step_avg:185.00ms\nstep:2256/4500 train_loss:8.1459 train_time:415517ms step_avg:185.00ms\nstep:2257/4500 train_loss:8.0642 train_time:415701ms step_avg:185.00ms\nstep:2258/4500 train_loss:8.1808 train_time:415885ms step_avg:185.00ms\nstep:2259/4500 train_loss:8.1578 train_time:416069ms step_avg:185.00ms\nstep:2260/4500 train_loss:8.1196 train_time:416253ms step_avg:185.00ms\nstep:2261/4500 train_loss:8.0456 train_time:416438ms step_avg:185.00ms\nstep:2262/4500 train_loss:8.1850 train_time:416623ms step_avg:185.00ms\nstep:2263/4500 train_loss:8.0085 train_time:416807ms step_avg:185.00ms\nstep:2264/4500 train_loss:8.1085 train_time:416994ms step_avg:185.00ms\nstep:2265/4500 train_loss:8.1631 train_time:417179ms step_avg:185.00ms\nstep:2266/4500 train_loss:8.0841 train_time:417366ms step_avg:185.00ms\nstep:2267/4500 train_loss:8.0982 train_time:417554ms step_avg:185.00ms\nstep:2268/4500 train_loss:8.1573 train_time:417740ms step_avg:185.00ms\nstep:2269/4500 train_loss:8.0897 train_time:417925ms step_avg:185.00ms\nstep:2270/4500 train_loss:8.0745 train_time:418112ms step_avg:185.01ms\nstep:2271/4500 train_loss:8.1030 train_time:418296ms step_avg:185.00ms\nstep:2272/4500 train_loss:8.3052 train_time:418482ms step_avg:185.01ms\nstep:2273/4500 train_loss:8.0649 train_time:418670ms step_avg:185.01ms\nstep:2274/4500 train_loss:8.1026 train_time:418853ms step_avg:185.01ms\nstep:2275/4500 train_loss:8.0729 train_time:419036ms step_avg:185.00ms\nstep:2276/4500 train_loss:8.0451 train_time:419221ms step_avg:185.00ms\nstep:2277/4500 train_loss:8.1414 train_time:419405ms step_avg:185.00ms\nstep:2278/4500 train_loss:8.1105 train_time:419590ms step_avg:185.00ms\nstep:2279/4500 train_loss:8.1269 train_time:419774ms step_avg:185.00ms\nstep:2280/4500 train_loss:8.0401 train_time:419958ms step_avg:185.00ms\nstep:2281/4500 train_loss:8.0847 train_time:420142ms step_avg:185.00ms\nstep:2282/4500 train_loss:8.1187 train_time:420327ms step_avg:185.00ms\nstep:2283/4500 train_loss:8.0560 train_time:420509ms step_avg:185.00ms\nstep:2284/4500 train_loss:8.0611 train_time:420694ms step_avg:185.00ms\nstep:2285/4500 train_loss:8.0527 train_time:420881ms step_avg:185.00ms\nstep:2286/4500 train_loss:8.1467 train_time:421299ms step_avg:185.10ms\nstep:2287/4500 train_loss:8.0488 train_time:421483ms step_avg:185.10ms\nstep:2288/4500 train_loss:8.1989 train_time:421667ms step_avg:185.10ms\nstep:2289/4500 train_loss:8.0931 train_time:421850ms step_avg:185.10ms\nstep:2290/4500 train_loss:7.9996 train_time:422034ms step_avg:185.10ms\nstep:2291/4500 train_loss:8.1547 train_time:422221ms step_avg:185.10ms\nstep:2292/4500 train_loss:8.0503 train_time:422409ms step_avg:185.10ms\nstep:2293/4500 train_loss:8.3620 train_time:422591ms step_avg:185.10ms\nstep:2294/4500 train_loss:8.1328 train_time:422775ms step_avg:185.10ms\nstep:2295/4500 train_loss:8.0798 train_time:422960ms step_avg:185.10ms\nstep:2296/4500 train_loss:8.0375 train_time:423145ms step_avg:185.10ms\nstep:2297/4500 train_loss:8.1109 train_time:423331ms step_avg:185.10ms\nstep:2298/4500 train_loss:8.0505 train_time:423518ms step_avg:185.10ms\nstep:2299/4500 train_loss:8.1487 train_time:423702ms step_avg:185.10ms\nstep:2300/4500 train_loss:8.1240 train_time:423886ms step_avg:185.10ms\nstep:2301/4500 train_loss:8.1059 train_time:424071ms step_avg:185.10ms\nstep:2302/4500 train_loss:8.1493 train_time:424253ms step_avg:185.10ms\nstep:2303/4500 train_loss:8.1237 train_time:424436ms step_avg:185.10ms\nstep:2304/4500 train_loss:8.1824 train_time:424622ms step_avg:185.10ms\nstep:2305/4500 train_loss:8.0727 train_time:424807ms step_avg:185.10ms\nstep:2306/4500 train_loss:8.1298 train_time:424990ms step_avg:185.10ms\nstep:2307/4500 train_loss:8.1951 train_time:425174ms step_avg:185.10ms\nstep:2308/4500 train_loss:8.1340 train_time:425358ms step_avg:185.10ms\nstep:2309/4500 train_loss:8.2767 train_time:425544ms step_avg:185.10ms\nstep:2310/4500 train_loss:8.1958 train_time:425729ms step_avg:185.10ms\nstep:2311/4500 train_loss:8.1036 train_time:425913ms step_avg:185.10ms\nstep:2312/4500 train_loss:8.1607 train_time:426098ms step_avg:185.10ms\nstep:2313/4500 train_loss:8.1657 train_time:426284ms step_avg:185.10ms\nstep:2314/4500 train_loss:8.0898 train_time:426470ms step_avg:185.10ms\nstep:2315/4500 train_loss:8.0320 train_time:426653ms step_avg:185.10ms\nstep:2316/4500 train_loss:8.1003 train_time:426836ms step_avg:185.10ms\nstep:2317/4500 train_loss:8.1709 train_time:427020ms step_avg:185.10ms\nstep:2318/4500 train_loss:7.9976 train_time:427202ms step_avg:185.10ms\nstep:2319/4500 train_loss:8.2704 train_time:427387ms step_avg:185.10ms\nstep:2320/4500 train_loss:8.0339 train_time:427574ms step_avg:185.10ms\nstep:2321/4500 train_loss:8.2409 train_time:427759ms step_avg:185.10ms\nstep:2322/4500 train_loss:8.1745 train_time:427946ms step_avg:185.10ms\nstep:2323/4500 train_loss:8.0171 train_time:428133ms step_avg:185.10ms\nstep:2324/4500 train_loss:8.1063 train_time:428317ms step_avg:185.10ms\nstep:2325/4500 train_loss:8.0372 train_time:428503ms step_avg:185.10ms\nstep:2326/4500 train_loss:8.1494 train_time:428690ms step_avg:185.10ms\nstep:2327/4500 train_loss:8.0933 train_time:428875ms step_avg:185.10ms\nstep:2328/4500 train_loss:8.1025 train_time:429057ms step_avg:185.10ms\nstep:2329/4500 train_loss:8.0897 train_time:429241ms step_avg:185.10ms\nstep:2330/4500 train_loss:8.1348 train_time:429426ms step_avg:185.10ms\nstep:2331/4500 train_loss:8.0853 train_time:429613ms step_avg:185.10ms\nstep:2332/4500 train_loss:8.2410 train_time:429800ms step_avg:185.10ms\nstep:2333/4500 train_loss:8.0639 train_time:429986ms step_avg:185.10ms\nstep:2334/4500 train_loss:8.2710 train_time:430170ms step_avg:185.10ms\nstep:2335/4500 train_loss:7.9695 train_time:430352ms step_avg:185.10ms\nstep:2336/4500 train_loss:8.0638 train_time:430534ms step_avg:185.10ms\nstep:2337/4500 train_loss:8.1280 train_time:430718ms step_avg:185.10ms\nstep:2338/4500 train_loss:8.0021 train_time:430904ms step_avg:185.10ms\nstep:2339/4500 train_loss:8.1129 train_time:431092ms step_avg:185.10ms\nstep:2340/4500 train_loss:8.0559 train_time:431282ms step_avg:185.10ms\nstep:2341/4500 train_loss:8.0299 train_time:431463ms step_avg:185.10ms\nstep:2342/4500 train_loss:8.1463 train_time:431648ms step_avg:185.10ms\nstep:2343/4500 train_loss:8.1600 train_time:431836ms step_avg:185.10ms\nstep:2344/4500 train_loss:8.1083 train_time:432018ms step_avg:185.10ms\nstep:2345/4500 train_loss:8.2005 train_time:432203ms step_avg:185.10ms\nstep:2346/4500 train_loss:8.1113 train_time:432389ms step_avg:185.10ms\nstep:2347/4500 train_loss:8.0629 train_time:432572ms step_avg:185.10ms\nstep:2348/4500 train_loss:8.0631 train_time:432761ms step_avg:185.10ms\nstep:2349/4500 train_loss:8.0922 train_time:432945ms step_avg:185.10ms\nstep:2350/4500 train_loss:8.1589 train_time:433129ms step_avg:185.10ms\nstep:2351/4500 train_loss:8.0016 train_time:433317ms step_avg:185.10ms\nstep:2352/4500 train_loss:8.1579 train_time:433503ms step_avg:185.10ms\nstep:2353/4500 train_loss:8.1019 train_time:433688ms step_avg:185.10ms\nstep:2354/4500 train_loss:7.9985 train_time:433875ms step_avg:185.10ms\nstep:2355/4500 train_loss:8.1318 train_time:434060ms step_avg:185.10ms\nstep:2356/4500 train_loss:8.1744 train_time:434245ms step_avg:185.10ms\nstep:2357/4500 train_loss:8.0169 train_time:434428ms step_avg:185.10ms\nstep:2358/4500 train_loss:8.2040 train_time:434611ms step_avg:185.10ms\nstep:2359/4500 train_loss:8.1212 train_time:434796ms step_avg:185.10ms\nstep:2360/4500 train_loss:8.2339 train_time:434982ms step_avg:185.10ms\nstep:2361/4500 train_loss:8.1283 train_time:435167ms step_avg:185.10ms\nstep:2362/4500 train_loss:8.1634 train_time:435352ms step_avg:185.10ms\nstep:2363/4500 train_loss:8.1763 train_time:435536ms step_avg:185.10ms\nstep:2364/4500 train_loss:8.1494 train_time:435720ms step_avg:185.10ms\nstep:2365/4500 train_loss:8.1419 train_time:435906ms step_avg:185.10ms\nstep:2366/4500 train_loss:8.0409 train_time:436089ms step_avg:185.10ms\nstep:2367/4500 train_loss:8.0979 train_time:436276ms step_avg:185.10ms\nstep:2368/4500 train_loss:8.0205 train_time:436461ms step_avg:185.10ms\nstep:2369/4500 train_loss:8.1911 train_time:436647ms step_avg:185.10ms\nstep:2370/4500 train_loss:7.9731 train_time:436829ms step_avg:185.10ms\nstep:2371/4500 train_loss:8.1585 train_time:437015ms step_avg:185.10ms\nstep:2372/4500 train_loss:8.1279 train_time:437199ms step_avg:185.10ms\nstep:2373/4500 train_loss:8.1369 train_time:437383ms step_avg:185.10ms\nstep:2374/4500 train_loss:8.1587 train_time:437570ms step_avg:185.10ms\nstep:2375/4500 train_loss:8.1570 train_time:437754ms step_avg:185.10ms\nstep:2376/4500 train_loss:8.1414 train_time:437938ms step_avg:185.10ms\nstep:2377/4500 train_loss:8.1292 train_time:438123ms step_avg:185.10ms\nstep:2378/4500 train_loss:8.1072 train_time:438308ms step_avg:185.10ms\nstep:2379/4500 train_loss:8.1425 train_time:438492ms step_avg:185.10ms\nstep:2380/4500 train_loss:8.2442 train_time:438675ms step_avg:185.09ms\nstep:2381/4500 train_loss:8.0534 train_time:438860ms step_avg:185.09ms\nstep:2382/4500 train_loss:8.0642 train_time:439044ms step_avg:185.09ms\nstep:2383/4500 train_loss:8.2427 train_time:439230ms step_avg:185.10ms\nstep:2384/4500 train_loss:7.9275 train_time:439415ms step_avg:185.09ms\nstep:2385/4500 train_loss:8.1632 train_time:439601ms step_avg:185.10ms\nstep:2386/4500 train_loss:8.1210 train_time:439788ms step_avg:185.10ms\nstep:2387/4500 train_loss:8.0954 train_time:439973ms step_avg:185.10ms\nstep:2388/4500 train_loss:8.0882 train_time:440157ms step_avg:185.10ms\nstep:2389/4500 train_loss:8.1676 train_time:440342ms step_avg:185.10ms\nstep:2390/4500 train_loss:8.0581 train_time:440526ms step_avg:185.10ms\nstep:2391/4500 train_loss:8.0404 train_time:440712ms step_avg:185.10ms\nstep:2392/4500 train_loss:8.1128 train_time:440896ms step_avg:185.10ms\nstep:2393/4500 train_loss:8.1201 train_time:441081ms step_avg:185.09ms\nstep:2394/4500 train_loss:8.1962 train_time:441267ms step_avg:185.10ms\nstep:2395/4500 train_loss:8.1380 train_time:441450ms step_avg:185.09ms\nstep:2396/4500 train_loss:8.1799 train_time:441636ms step_avg:185.09ms\nstep:2397/4500 train_loss:8.1437 train_time:441822ms step_avg:185.09ms\nstep:2398/4500 train_loss:8.1421 train_time:442012ms step_avg:185.10ms\nstep:2399/4500 train_loss:8.0699 train_time:442196ms step_avg:185.10ms\nstep:2400/4500 train_loss:8.0597 train_time:442383ms step_avg:185.10ms\nstep:2401/4500 train_loss:8.1777 train_time:442567ms step_avg:185.10ms\nstep:2402/4500 train_loss:8.1401 train_time:442751ms step_avg:185.10ms\nstep:2403/4500 train_loss:8.1696 train_time:442936ms step_avg:185.10ms\nstep:2404/4500 train_loss:8.0427 train_time:443123ms step_avg:185.10ms\nstep:2405/4500 train_loss:8.1555 train_time:443308ms step_avg:185.10ms\nstep:2406/4500 train_loss:7.9861 train_time:443492ms step_avg:185.10ms\nstep:2407/4500 train_loss:8.0388 train_time:443678ms step_avg:185.10ms\nstep:2408/4500 train_loss:8.0199 train_time:443860ms step_avg:185.10ms\nstep:2409/4500 train_loss:8.0534 train_time:444046ms step_avg:185.10ms\nstep:2410/4500 train_loss:8.0775 train_time:444232ms step_avg:185.10ms\nstep:2411/4500 train_loss:8.1048 train_time:444415ms step_avg:185.10ms\nstep:2412/4500 train_loss:8.0405 train_time:444598ms step_avg:185.09ms\nstep:2413/4500 train_loss:8.0182 train_time:444786ms step_avg:185.10ms\nstep:2414/4500 train_loss:8.0635 train_time:444968ms step_avg:185.09ms\nstep:2415/4500 train_loss:8.2231 train_time:445151ms step_avg:185.09ms\nstep:2416/4500 train_loss:8.1263 train_time:445335ms step_avg:185.09ms\nstep:2417/4500 train_loss:8.0189 train_time:445521ms step_avg:185.09ms\nstep:2418/4500 train_loss:8.1386 train_time:445706ms step_avg:185.09ms\nstep:2419/4500 train_loss:8.1990 train_time:445891ms step_avg:185.09ms\nstep:2420/4500 train_loss:8.2152 train_time:446075ms step_avg:185.09ms\nstep:2421/4500 train_loss:8.1211 train_time:446259ms step_avg:185.09ms\nstep:2422/4500 train_loss:8.1683 train_time:446444ms step_avg:185.09ms\nstep:2423/4500 train_loss:8.1255 train_time:446627ms step_avg:185.09ms\nstep:2424/4500 train_loss:8.1272 train_time:446814ms step_avg:185.09ms\nstep:2425/4500 train_loss:8.1361 train_time:447000ms step_avg:185.09ms\nstep:2426/4500 train_loss:8.1624 train_time:447184ms step_avg:185.09ms\nstep:2427/4500 train_loss:8.0130 train_time:447371ms step_avg:185.09ms\nstep:2428/4500 train_loss:8.0558 train_time:447555ms step_avg:185.09ms\nstep:2429/4500 train_loss:8.0416 train_time:447738ms step_avg:185.09ms\nstep:2430/4500 train_loss:8.1269 train_time:447923ms step_avg:185.09ms\nstep:2431/4500 train_loss:8.0952 train_time:448109ms step_avg:185.09ms\nstep:2432/4500 train_loss:8.0881 train_time:448295ms step_avg:185.09ms\nstep:2433/4500 train_loss:8.0102 train_time:448481ms step_avg:185.09ms\nstep:2434/4500 train_loss:8.1131 train_time:448668ms step_avg:185.09ms\nstep:2435/4500 train_loss:7.9457 train_time:448856ms step_avg:185.10ms\nstep:2436/4500 train_loss:8.0839 train_time:449040ms step_avg:185.09ms\nstep:2437/4500 train_loss:8.0460 train_time:449225ms step_avg:185.09ms\nstep:2438/4500 train_loss:8.0784 train_time:449411ms step_avg:185.10ms\nstep:2439/4500 train_loss:8.0706 train_time:449593ms step_avg:185.09ms\nstep:2440/4500 train_loss:8.0936 train_time:449777ms step_avg:185.09ms\nstep:2441/4500 train_loss:8.0523 train_time:449960ms step_avg:185.09ms\nstep:2442/4500 train_loss:8.0751 train_time:450145ms step_avg:185.09ms\nstep:2443/4500 train_loss:8.0125 train_time:450332ms step_avg:185.09ms\nstep:2444/4500 train_loss:8.1257 train_time:450515ms step_avg:185.09ms\nstep:2445/4500 train_loss:8.0363 train_time:450701ms step_avg:185.09ms\nstep:2446/4500 train_loss:8.1023 train_time:450888ms step_avg:185.09ms\nstep:2447/4500 train_loss:8.0922 train_time:451070ms step_avg:185.09ms\nstep:2448/4500 train_loss:8.1097 train_time:451254ms step_avg:185.09ms\nstep:2449/4500 train_loss:8.2105 train_time:451438ms step_avg:185.09ms\nstep:2450/4500 train_loss:8.1900 train_time:451623ms step_avg:185.09ms\nstep:2451/4500 train_loss:8.1168 train_time:451808ms step_avg:185.09ms\nstep:2452/4500 train_loss:8.0923 train_time:451993ms step_avg:185.09ms\nstep:2453/4500 train_loss:7.9897 train_time:452178ms step_avg:185.09ms\nstep:2454/4500 train_loss:7.9651 train_time:452364ms step_avg:185.09ms\nstep:2455/4500 train_loss:8.0890 train_time:452547ms step_avg:185.09ms\nstep:2456/4500 train_loss:8.1403 train_time:452731ms step_avg:185.09ms\nstep:2457/4500 train_loss:8.1175 train_time:452915ms step_avg:185.09ms\nstep:2458/4500 train_loss:8.2173 train_time:453100ms step_avg:185.09ms\nstep:2459/4500 train_loss:8.1916 train_time:453285ms step_avg:185.09ms\nstep:2460/4500 train_loss:8.2879 train_time:453472ms step_avg:185.09ms\nstep:2461/4500 train_loss:8.1687 train_time:453656ms step_avg:185.09ms\nstep:2462/4500 train_loss:8.1301 train_time:453840ms step_avg:185.09ms\nstep:2463/4500 train_loss:8.2174 train_time:454025ms step_avg:185.09ms\nstep:2464/4500 train_loss:8.1664 train_time:454212ms step_avg:185.09ms\nstep:2465/4500 train_loss:8.1865 train_time:454395ms step_avg:185.09ms\nstep:2466/4500 train_loss:8.1311 train_time:454581ms step_avg:185.09ms\nstep:2467/4500 train_loss:8.1454 train_time:454768ms step_avg:185.09ms\nstep:2468/4500 train_loss:7.9842 train_time:454952ms step_avg:185.09ms\nstep:2469/4500 train_loss:8.1643 train_time:455139ms step_avg:185.09ms\nstep:2470/4500 train_loss:8.1308 train_time:455322ms step_avg:185.09ms\nstep:2471/4500 train_loss:8.0581 train_time:455507ms step_avg:185.09ms\nstep:2472/4500 train_loss:8.2270 train_time:455692ms step_avg:185.09ms\nstep:2473/4500 train_loss:8.0439 train_time:455878ms step_avg:185.09ms\nstep:2474/4500 train_loss:8.0576 train_time:456062ms step_avg:185.09ms\nstep:2475/4500 train_loss:8.0862 train_time:456247ms step_avg:185.09ms\nstep:2476/4500 train_loss:8.0960 train_time:456430ms step_avg:185.09ms\nstep:2477/4500 train_loss:8.2584 train_time:456613ms step_avg:185.09ms\nstep:2478/4500 train_loss:8.1929 train_time:456797ms step_avg:185.09ms\nstep:2479/4500 train_loss:8.1155 train_time:456983ms step_avg:185.09ms\nstep:2480/4500 train_loss:8.2317 train_time:457167ms step_avg:185.09ms\nstep:2481/4500 train_loss:8.0682 train_time:457350ms step_avg:185.09ms\nstep:2482/4500 train_loss:8.1617 train_time:457535ms step_avg:185.09ms\nstep:2483/4500 train_loss:7.8968 train_time:457721ms step_avg:185.09ms\nstep:2484/4500 train_loss:8.1811 train_time:457905ms step_avg:185.09ms\nstep:2485/4500 train_loss:8.2309 train_time:458092ms step_avg:185.09ms\nstep:2486/4500 train_loss:8.1068 train_time:458275ms step_avg:185.09ms\nstep:2487/4500 train_loss:8.1132 train_time:458461ms step_avg:185.09ms\nstep:2488/4500 train_loss:8.1384 train_time:458648ms step_avg:185.09ms\nstep:2489/4500 train_loss:8.3093 train_time:458832ms step_avg:185.09ms\nstep:2490/4500 train_loss:8.0799 train_time:459017ms step_avg:185.09ms\nstep:2491/4500 train_loss:8.1062 train_time:459201ms step_avg:185.09ms\nstep:2492/4500 train_loss:8.1229 train_time:459385ms step_avg:185.09ms\nstep:2493/4500 train_loss:8.0988 train_time:459569ms step_avg:185.09ms\nstep:2494/4500 train_loss:8.1958 train_time:459756ms step_avg:185.09ms\nstep:2495/4500 train_loss:8.1059 train_time:459944ms step_avg:185.09ms\nstep:2496/4500 train_loss:8.2186 train_time:460130ms step_avg:185.09ms\nstep:2497/4500 train_loss:8.1025 train_time:460315ms step_avg:185.09ms\nstep:2498/4500 train_loss:8.1136 train_time:460500ms step_avg:185.09ms\nstep:2499/4500 train_loss:8.0550 train_time:460684ms step_avg:185.09ms\nstep:2500/4500 train_loss:8.1567 train_time:460869ms step_avg:185.09ms\nstep:2501/4500 train_loss:8.1690 train_time:461052ms step_avg:185.09ms\nstep:2502/4500 train_loss:8.0940 train_time:461235ms step_avg:185.09ms\nstep:2503/4500 train_loss:8.0912 train_time:461420ms step_avg:185.09ms\nstep:2504/4500 train_loss:8.3161 train_time:461605ms step_avg:185.09ms\nstep:2505/4500 train_loss:8.1404 train_time:461790ms step_avg:185.09ms\nstep:2506/4500 train_loss:8.0829 train_time:461974ms step_avg:185.09ms\nstep:2507/4500 train_loss:8.1767 train_time:462159ms step_avg:185.09ms\nstep:2508/4500 train_loss:8.2039 train_time:462345ms step_avg:185.09ms\nstep:2509/4500 train_loss:8.0615 train_time:462531ms step_avg:185.09ms\nstep:2510/4500 train_loss:8.0556 train_time:462714ms step_avg:185.09ms\nstep:2511/4500 train_loss:8.0204 train_time:462898ms step_avg:185.09ms\nstep:2512/4500 train_loss:8.0438 train_time:463082ms step_avg:185.08ms\nstep:2513/4500 train_loss:8.1807 train_time:463268ms step_avg:185.08ms\nstep:2514/4500 train_loss:8.1391 train_time:463451ms step_avg:185.08ms\nstep:2515/4500 train_loss:8.1170 train_time:463635ms step_avg:185.08ms\nstep:2516/4500 train_loss:8.2121 train_time:463821ms step_avg:185.08ms\nstep:2517/4500 train_loss:8.0579 train_time:464005ms step_avg:185.08ms\nstep:2518/4500 train_loss:8.1359 train_time:464188ms step_avg:185.08ms\nstep:2519/4500 train_loss:8.1140 train_time:464373ms step_avg:185.08ms\nstep:2520/4500 train_loss:8.0277 train_time:464558ms step_avg:185.08ms\nstep:2521/4500 train_loss:8.1066 train_time:464747ms step_avg:185.08ms\nstep:2522/4500 train_loss:8.1644 train_time:464932ms step_avg:185.08ms\nstep:2523/4500 train_loss:8.1679 train_time:465117ms step_avg:185.08ms\nstep:2524/4500 train_loss:8.0789 train_time:465301ms step_avg:185.08ms\nstep:2525/4500 train_loss:8.1146 train_time:465485ms step_avg:185.08ms\nstep:2526/4500 train_loss:8.1265 train_time:465672ms step_avg:185.08ms\nstep:2527/4500 train_loss:8.0656 train_time:465854ms step_avg:185.08ms\nstep:2528/4500 train_loss:8.1929 train_time:466039ms step_avg:185.08ms\nstep:2529/4500 train_loss:8.1206 train_time:466222ms step_avg:185.08ms\nstep:2530/4500 train_loss:8.1235 train_time:466406ms step_avg:185.08ms\nstep:2531/4500 train_loss:8.1291 train_time:466591ms step_avg:185.08ms\nstep:2532/4500 train_loss:8.1603 train_time:466776ms step_avg:185.08ms\nstep:2533/4500 train_loss:8.0872 train_time:466963ms step_avg:185.08ms\nstep:2534/4500 train_loss:8.2761 train_time:467150ms step_avg:185.08ms\nstep:2535/4500 train_loss:8.1506 train_time:467335ms step_avg:185.08ms\nstep:2536/4500 train_loss:8.0205 train_time:467521ms step_avg:185.08ms\nstep:2537/4500 train_loss:8.1006 train_time:467706ms step_avg:185.08ms\nstep:2538/4500 train_loss:8.1354 train_time:467894ms step_avg:185.08ms\nstep:2539/4500 train_loss:8.2075 train_time:468078ms step_avg:185.08ms\nstep:2540/4500 train_loss:7.9974 train_time:468262ms step_avg:185.08ms\nstep:2541/4500 train_loss:8.0683 train_time:468450ms step_avg:185.08ms\nstep:2542/4500 train_loss:8.1642 train_time:468633ms step_avg:185.08ms\nstep:2543/4500 train_loss:8.2603 train_time:468817ms step_avg:185.08ms\nstep:2544/4500 train_loss:8.1160 train_time:469001ms step_avg:185.08ms\nstep:2545/4500 train_loss:8.1952 train_time:469186ms step_avg:185.08ms\nstep:2546/4500 train_loss:8.0348 train_time:469371ms step_avg:185.08ms\nstep:2547/4500 train_loss:8.0974 train_time:469554ms step_avg:185.08ms\nstep:2548/4500 train_loss:8.0339 train_time:469737ms step_avg:185.08ms\nstep:2549/4500 train_loss:8.0852 train_time:469922ms step_avg:185.08ms\nstep:2550/4500 train_loss:8.0065 train_time:470110ms step_avg:185.08ms\nstep:2551/4500 train_loss:8.1894 train_time:470293ms step_avg:185.08ms\nstep:2552/4500 train_loss:8.3211 train_time:470477ms step_avg:185.08ms\nstep:2553/4500 train_loss:8.1278 train_time:470665ms step_avg:185.08ms\nstep:2554/4500 train_loss:8.0112 train_time:470848ms step_avg:185.08ms\nstep:2555/4500 train_loss:8.0918 train_time:471033ms step_avg:185.08ms\nstep:2556/4500 train_loss:8.0972 train_time:471219ms step_avg:185.08ms\nstep:2557/4500 train_loss:8.1161 train_time:471407ms step_avg:185.08ms\nstep:2558/4500 train_loss:8.0900 train_time:471591ms step_avg:185.08ms\nstep:2559/4500 train_loss:8.1092 train_time:471775ms step_avg:185.08ms\nstep:2560/4500 train_loss:8.0247 train_time:471958ms step_avg:185.08ms\nstep:2561/4500 train_loss:8.1917 train_time:472141ms step_avg:185.08ms\nstep:2562/4500 train_loss:8.0661 train_time:472327ms step_avg:185.08ms\nstep:2563/4500 train_loss:8.0870 train_time:472512ms step_avg:185.08ms\nstep:2564/4500 train_loss:8.2256 train_time:472696ms step_avg:185.08ms\nstep:2565/4500 train_loss:8.0757 train_time:472882ms step_avg:185.08ms\nstep:2566/4500 train_loss:8.1447 train_time:473067ms step_avg:185.08ms\nstep:2567/4500 train_loss:8.1772 train_time:473250ms step_avg:185.08ms\nstep:2568/4500 train_loss:8.1400 train_time:473432ms step_avg:185.08ms\nstep:2569/4500 train_loss:8.1382 train_time:473616ms step_avg:185.08ms\nstep:2570/4500 train_loss:8.1112 train_time:473800ms step_avg:185.08ms\nstep:2571/4500 train_loss:8.0814 train_time:473988ms step_avg:185.08ms\nstep:2572/4500 train_loss:8.1429 train_time:474173ms step_avg:185.08ms\nstep:2573/4500 train_loss:8.2402 train_time:474356ms step_avg:185.08ms\nstep:2574/4500 train_loss:8.0587 train_time:474541ms step_avg:185.08ms\nstep:2575/4500 train_loss:8.0838 train_time:474730ms step_avg:185.08ms\nstep:2576/4500 train_loss:8.2059 train_time:474915ms step_avg:185.08ms\nstep:2577/4500 train_loss:8.1176 train_time:475100ms step_avg:185.08ms\nstep:2578/4500 train_loss:8.1205 train_time:475288ms step_avg:185.08ms\nstep:2579/4500 train_loss:8.1784 train_time:475472ms step_avg:185.08ms\nstep:2580/4500 train_loss:8.1525 train_time:475656ms step_avg:185.08ms\nstep:2581/4500 train_loss:8.0939 train_time:475840ms step_avg:185.08ms\nstep:2582/4500 train_loss:7.9845 train_time:476024ms step_avg:185.08ms\nstep:2583/4500 train_loss:8.1417 train_time:476212ms step_avg:185.08ms\nstep:2584/4500 train_loss:8.1868 train_time:476395ms step_avg:185.08ms\nstep:2585/4500 train_loss:8.0953 train_time:476580ms step_avg:185.08ms\nstep:2586/4500 train_loss:8.0849 train_time:476765ms step_avg:185.08ms\nstep:2587/4500 train_loss:8.0119 train_time:476948ms step_avg:185.08ms\nstep:2588/4500 train_loss:8.1323 train_time:477133ms step_avg:185.08ms\nstep:2589/4500 train_loss:8.0596 train_time:477318ms step_avg:185.08ms\nstep:2590/4500 train_loss:8.0642 train_time:477503ms step_avg:185.08ms\nstep:2591/4500 train_loss:8.0878 train_time:477691ms step_avg:185.08ms\nstep:2592/4500 train_loss:8.1537 train_time:477874ms step_avg:185.08ms\nstep:2593/4500 train_loss:8.1030 train_time:478058ms step_avg:185.08ms\nstep:2594/4500 train_loss:8.2472 train_time:478242ms step_avg:185.08ms\nstep:2595/4500 train_loss:8.1781 train_time:478427ms step_avg:185.08ms\nstep:2596/4500 train_loss:8.0483 train_time:478610ms step_avg:185.08ms\nstep:2597/4500 train_loss:8.2482 train_time:478795ms step_avg:185.08ms\nstep:2598/4500 train_loss:8.1755 train_time:478982ms step_avg:185.08ms\nstep:2599/4500 train_loss:8.1588 train_time:479166ms step_avg:185.08ms\nstep:2600/4500 train_loss:8.2178 train_time:479350ms step_avg:185.08ms\nstep:2601/4500 train_loss:8.2659 train_time:479533ms step_avg:185.08ms\nstep:2602/4500 train_loss:8.4685 train_time:479716ms step_avg:185.08ms\nstep:2603/4500 train_loss:8.1436 train_time:479903ms step_avg:185.08ms\nstep:2604/4500 train_loss:8.0977 train_time:480088ms step_avg:185.08ms\nstep:2605/4500 train_loss:8.2538 train_time:480272ms step_avg:185.08ms\nstep:2606/4500 train_loss:7.9969 train_time:480456ms step_avg:185.08ms\nstep:2607/4500 train_loss:8.1038 train_time:480640ms step_avg:185.08ms\nstep:2608/4500 train_loss:8.1419 train_time:480827ms step_avg:185.08ms\nstep:2609/4500 train_loss:8.2011 train_time:481011ms step_avg:185.08ms\nstep:2610/4500 train_loss:8.1210 train_time:481194ms step_avg:185.07ms\nstep:2611/4500 train_loss:8.0529 train_time:481378ms step_avg:185.07ms\nstep:2612/4500 train_loss:8.1430 train_time:481564ms step_avg:185.07ms\nstep:2613/4500 train_loss:8.1520 train_time:481751ms step_avg:185.08ms\nstep:2614/4500 train_loss:8.1202 train_time:481939ms step_avg:185.08ms\nstep:2615/4500 train_loss:8.0992 train_time:482125ms step_avg:185.08ms\nstep:2616/4500 train_loss:8.0600 train_time:482310ms step_avg:185.08ms\nstep:2617/4500 train_loss:8.2445 train_time:482494ms step_avg:185.08ms\nstep:2618/4500 train_loss:8.1255 train_time:482678ms step_avg:185.08ms\nstep:2619/4500 train_loss:8.0740 train_time:482863ms step_avg:185.08ms\nstep:2620/4500 train_loss:7.9944 train_time:483048ms step_avg:185.08ms\nstep:2621/4500 train_loss:8.1249 train_time:483232ms step_avg:185.08ms\nstep:2622/4500 train_loss:8.2310 train_time:483416ms step_avg:185.08ms\nstep:2623/4500 train_loss:8.1090 train_time:483601ms step_avg:185.07ms\nstep:2624/4500 train_loss:8.0605 train_time:483789ms step_avg:185.08ms\nstep:2625/4500 train_loss:8.0420 train_time:483975ms step_avg:185.08ms\nstep:2626/4500 train_loss:8.0670 train_time:484161ms step_avg:185.08ms\nstep:2627/4500 train_loss:8.1459 train_time:484345ms step_avg:185.08ms\nstep:2628/4500 train_loss:8.0737 train_time:484529ms step_avg:185.08ms\nstep:2629/4500 train_loss:8.0902 train_time:484712ms step_avg:185.08ms\nstep:2630/4500 train_loss:8.1733 train_time:484897ms step_avg:185.08ms\nstep:2631/4500 train_loss:8.1433 train_time:485080ms step_avg:185.07ms\nstep:2632/4500 train_loss:8.1420 train_time:485266ms step_avg:185.07ms\nstep:2633/4500 train_loss:7.9646 train_time:485450ms step_avg:185.07ms\nstep:2634/4500 train_loss:8.1393 train_time:485635ms step_avg:185.07ms\nstep:2635/4500 train_loss:8.0673 train_time:485819ms step_avg:185.07ms\nstep:2636/4500 train_loss:8.1675 train_time:486007ms step_avg:185.08ms\nstep:2637/4500 train_loss:8.1029 train_time:486191ms step_avg:185.07ms\nstep:2638/4500 train_loss:8.1044 train_time:486375ms step_avg:185.07ms\nstep:2639/4500 train_loss:8.1439 train_time:486559ms step_avg:185.07ms\nstep:2640/4500 train_loss:8.1472 train_time:486745ms step_avg:185.07ms\nstep:2641/4500 train_loss:8.1153 train_time:486933ms step_avg:185.08ms\nstep:2642/4500 train_loss:8.2216 train_time:487118ms step_avg:185.08ms\nstep:2643/4500 train_loss:8.0798 train_time:487303ms step_avg:185.08ms\nstep:2644/4500 train_loss:8.1063 train_time:487488ms step_avg:185.08ms\nstep:2645/4500 train_loss:8.1597 train_time:487672ms step_avg:185.07ms\nstep:2646/4500 train_loss:8.0657 train_time:487856ms step_avg:185.07ms\nstep:2647/4500 train_loss:8.1210 train_time:488044ms step_avg:185.08ms\nstep:2648/4500 train_loss:8.0541 train_time:488230ms step_avg:185.08ms\nstep:2649/4500 train_loss:8.1439 train_time:488413ms step_avg:185.08ms\nstep:2650/4500 train_loss:8.1478 train_time:488597ms step_avg:185.07ms\nstep:2651/4500 train_loss:8.1555 train_time:488781ms step_avg:185.07ms\nstep:2652/4500 train_loss:8.1387 train_time:488966ms step_avg:185.07ms\nstep:2653/4500 train_loss:8.0841 train_time:489149ms step_avg:185.07ms\nstep:2654/4500 train_loss:8.1019 train_time:489336ms step_avg:185.07ms\nstep:2655/4500 train_loss:8.1046 train_time:489520ms step_avg:185.07ms\nstep:2656/4500 train_loss:8.1001 train_time:489705ms step_avg:185.07ms\nstep:2657/4500 train_loss:8.0972 train_time:489893ms step_avg:185.07ms\nstep:2658/4500 train_loss:7.9847 train_time:490076ms step_avg:185.07ms\nstep:2659/4500 train_loss:8.0829 train_time:490260ms step_avg:185.07ms\nstep:2660/4500 train_loss:8.2415 train_time:490445ms step_avg:185.07ms\nstep:2661/4500 train_loss:8.1011 train_time:490629ms step_avg:185.07ms\nstep:2662/4500 train_loss:8.0327 train_time:490815ms step_avg:185.07ms\nstep:2663/4500 train_loss:8.1690 train_time:491000ms step_avg:185.07ms\nstep:2664/4500 train_loss:8.0476 train_time:491189ms step_avg:185.07ms\nstep:2665/4500 train_loss:8.1883 train_time:491373ms step_avg:185.07ms\nstep:2666/4500 train_loss:8.1816 train_time:491556ms step_avg:185.07ms\nstep:2667/4500 train_loss:8.0688 train_time:491740ms step_avg:185.07ms\nstep:2668/4500 train_loss:8.2295 train_time:491924ms step_avg:185.07ms\nstep:2669/4500 train_loss:8.1616 train_time:492110ms step_avg:185.07ms\nstep:2670/4500 train_loss:8.1070 train_time:492295ms step_avg:185.07ms\nstep:2671/4500 train_loss:8.1903 train_time:492483ms step_avg:185.07ms\nstep:2672/4500 train_loss:8.1170 train_time:492667ms step_avg:185.07ms\nstep:2673/4500 train_loss:8.0562 train_time:492853ms step_avg:185.07ms\nstep:2674/4500 train_loss:8.1684 train_time:493036ms step_avg:185.07ms\nstep:2675/4500 train_loss:8.2719 train_time:493220ms step_avg:185.07ms\nstep:2676/4500 train_loss:8.0848 train_time:493406ms step_avg:185.07ms\nstep:2677/4500 train_loss:8.1330 train_time:493590ms step_avg:185.07ms\nstep:2678/4500 train_loss:8.1226 train_time:493773ms step_avg:185.07ms\nstep:2679/4500 train_loss:8.1109 train_time:493956ms step_avg:185.07ms\nstep:2680/4500 train_loss:8.0045 train_time:494140ms step_avg:185.07ms\nstep:2681/4500 train_loss:8.0888 train_time:494327ms step_avg:185.07ms\nstep:2682/4500 train_loss:8.0173 train_time:494511ms step_avg:185.07ms\nstep:2683/4500 train_loss:8.0172 train_time:494696ms step_avg:185.07ms\nstep:2684/4500 train_loss:8.0571 train_time:494882ms step_avg:185.07ms\nstep:2685/4500 train_loss:8.1498 train_time:495068ms step_avg:185.07ms\nstep:2686/4500 train_loss:8.1161 train_time:495253ms step_avg:185.07ms\nstep:2687/4500 train_loss:8.2374 train_time:495439ms step_avg:185.07ms\nstep:2688/4500 train_loss:8.0522 train_time:495627ms step_avg:185.07ms\nstep:2689/4500 train_loss:8.0318 train_time:495814ms step_avg:185.07ms\nstep:2690/4500 train_loss:7.9833 train_time:495999ms step_avg:185.07ms\nstep:2691/4500 train_loss:8.0888 train_time:496184ms step_avg:185.07ms\nstep:2692/4500 train_loss:8.1282 train_time:496367ms step_avg:185.07ms\nstep:2693/4500 train_loss:8.1472 train_time:496553ms step_avg:185.07ms\nstep:2694/4500 train_loss:8.1600 train_time:496737ms step_avg:185.07ms\nstep:2695/4500 train_loss:8.1290 train_time:496920ms step_avg:185.07ms\nstep:2696/4500 train_loss:8.1984 train_time:497107ms step_avg:185.07ms\nstep:2697/4500 train_loss:8.0857 train_time:497289ms step_avg:185.07ms\nstep:2698/4500 train_loss:8.2971 train_time:497473ms step_avg:185.07ms\nstep:2699/4500 train_loss:8.0880 train_time:497658ms step_avg:185.07ms\nstep:2700/4500 train_loss:8.2089 train_time:497844ms step_avg:185.07ms\nstep:2701/4500 train_loss:8.1013 train_time:498029ms step_avg:185.07ms\nstep:2702/4500 train_loss:8.1559 train_time:498214ms step_avg:185.07ms\nstep:2703/4500 train_loss:8.0879 train_time:498400ms step_avg:185.07ms\nstep:2704/4500 train_loss:8.0756 train_time:498588ms step_avg:185.07ms\nstep:2705/4500 train_loss:8.1042 train_time:498774ms step_avg:185.07ms\nstep:2706/4500 train_loss:8.0026 train_time:498960ms step_avg:185.07ms\nstep:2707/4500 train_loss:8.0595 train_time:499145ms step_avg:185.07ms\nstep:2708/4500 train_loss:8.1085 train_time:499329ms step_avg:185.07ms\nstep:2709/4500 train_loss:8.0724 train_time:499513ms step_avg:185.07ms\nstep:2710/4500 train_loss:8.1244 train_time:499698ms step_avg:185.07ms\nstep:2711/4500 train_loss:8.0289 train_time:499885ms step_avg:185.07ms\nstep:2712/4500 train_loss:7.9891 train_time:500073ms step_avg:185.08ms\nstep:2713/4500 train_loss:8.1184 train_time:500259ms step_avg:185.08ms\nstep:2714/4500 train_loss:8.1854 train_time:500444ms step_avg:185.08ms\nstep:2715/4500 train_loss:8.0726 train_time:500627ms step_avg:185.07ms\nstep:2716/4500 train_loss:8.0254 train_time:500811ms step_avg:185.07ms\nstep:2717/4500 train_loss:8.0034 train_time:500994ms step_avg:185.07ms\nstep:2718/4500 train_loss:8.2798 train_time:501179ms step_avg:185.07ms\nstep:2719/4500 train_loss:8.1174 train_time:501368ms step_avg:185.07ms\nstep:2720/4500 train_loss:8.1518 train_time:501554ms step_avg:185.08ms\nstep:2721/4500 train_loss:8.1508 train_time:501739ms step_avg:185.08ms\nstep:2722/4500 train_loss:8.1249 train_time:501923ms step_avg:185.07ms\nstep:2723/4500 train_loss:8.0383 train_time:502106ms step_avg:185.07ms\nstep:2724/4500 train_loss:8.1037 train_time:502291ms step_avg:185.07ms\nstep:2725/4500 train_loss:8.1461 train_time:502475ms step_avg:185.07ms\nstep:2726/4500 train_loss:8.1185 train_time:502660ms step_avg:185.07ms\nstep:2727/4500 train_loss:8.2633 train_time:502844ms step_avg:185.07ms\nstep:2728/4500 train_loss:8.1279 train_time:503027ms step_avg:185.07ms\nstep:2729/4500 train_loss:8.1174 train_time:503212ms step_avg:185.07ms\nstep:2730/4500 train_loss:8.1310 train_time:503395ms step_avg:185.07ms\nstep:2731/4500 train_loss:8.0013 train_time:503581ms step_avg:185.07ms\nstep:2732/4500 train_loss:8.0540 train_time:503770ms step_avg:185.07ms\nstep:2733/4500 train_loss:8.1258 train_time:503955ms step_avg:185.07ms\nstep:2734/4500 train_loss:8.0956 train_time:504140ms step_avg:185.07ms\nstep:2735/4500 train_loss:8.0923 train_time:504329ms step_avg:185.07ms\nstep:2736/4500 train_loss:8.1637 train_time:504513ms step_avg:185.07ms\nstep:2737/4500 train_loss:8.2423 train_time:504697ms step_avg:185.07ms\nstep:2738/4500 train_loss:8.0592 train_time:504882ms step_avg:185.07ms\nstep:2739/4500 train_loss:7.9803 train_time:505067ms step_avg:185.07ms\nstep:2740/4500 train_loss:8.1185 train_time:505253ms step_avg:185.07ms\nstep:2741/4500 train_loss:8.3471 train_time:505438ms step_avg:185.07ms\nstep:2742/4500 train_loss:8.0823 train_time:505624ms step_avg:185.07ms\nstep:2743/4500 train_loss:8.1464 train_time:505812ms step_avg:185.08ms\nstep:2744/4500 train_loss:8.1450 train_time:505996ms step_avg:185.08ms\nstep:2745/4500 train_loss:8.0728 train_time:506180ms step_avg:185.08ms\nstep:2746/4500 train_loss:8.1558 train_time:506364ms step_avg:185.07ms\nstep:2747/4500 train_loss:8.1294 train_time:506552ms step_avg:185.08ms\nstep:2748/4500 train_loss:8.1871 train_time:506737ms step_avg:185.08ms\nstep:2749/4500 train_loss:8.0564 train_time:506923ms step_avg:185.08ms\nstep:2750/4500 train_loss:8.1193 train_time:507108ms step_avg:185.08ms\nstep:2751/4500 train_loss:8.1742 train_time:507293ms step_avg:185.08ms\nstep:2752/4500 train_loss:8.1278 train_time:507477ms step_avg:185.08ms\nstep:2753/4500 train_loss:8.1064 train_time:507662ms step_avg:185.08ms\nstep:2754/4500 train_loss:8.1228 train_time:507846ms step_avg:185.08ms\nstep:2755/4500 train_loss:8.1022 train_time:508030ms step_avg:185.07ms\nstep:2756/4500 train_loss:7.8160 train_time:508217ms step_avg:185.08ms\nstep:2757/4500 train_loss:8.0396 train_time:508401ms step_avg:185.08ms\nstep:2758/4500 train_loss:8.1195 train_time:508587ms step_avg:185.08ms\nstep:2759/4500 train_loss:8.2469 train_time:508771ms step_avg:185.07ms\nstep:2760/4500 train_loss:8.0551 train_time:508955ms step_avg:185.07ms\nstep:2761/4500 train_loss:8.0576 train_time:509140ms step_avg:185.07ms\nstep:2762/4500 train_loss:8.0150 train_time:509325ms step_avg:185.07ms\nstep:2763/4500 train_loss:8.0854 train_time:509512ms step_avg:185.08ms\nstep:2764/4500 train_loss:8.0488 train_time:509695ms step_avg:185.07ms\nstep:2765/4500 train_loss:8.1125 train_time:509880ms step_avg:185.07ms\nstep:2766/4500 train_loss:8.2223 train_time:510065ms step_avg:185.07ms\nstep:2767/4500 train_loss:8.0933 train_time:510248ms step_avg:185.07ms\nstep:2768/4500 train_loss:8.2113 train_time:510434ms step_avg:185.07ms\nstep:2769/4500 train_loss:8.1295 train_time:510619ms step_avg:185.07ms\nstep:2770/4500 train_loss:8.1098 train_time:510805ms step_avg:185.07ms\nstep:2771/4500 train_loss:8.0582 train_time:510988ms step_avg:185.07ms\nstep:2772/4500 train_loss:8.1087 train_time:511173ms step_avg:185.07ms\nstep:2773/4500 train_loss:8.1353 train_time:511356ms step_avg:185.07ms\nstep:2774/4500 train_loss:8.1343 train_time:511541ms step_avg:185.07ms\nstep:2775/4500 train_loss:8.1035 train_time:511724ms step_avg:185.07ms\nstep:2776/4500 train_loss:8.1271 train_time:511909ms step_avg:185.07ms\nstep:2777/4500 train_loss:7.8381 train_time:512092ms step_avg:185.07ms\nstep:2778/4500 train_loss:8.0862 train_time:512278ms step_avg:185.07ms\nstep:2779/4500 train_loss:8.1368 train_time:512463ms step_avg:185.07ms\nstep:2780/4500 train_loss:8.1432 train_time:512647ms step_avg:185.07ms\nstep:2781/4500 train_loss:7.9914 train_time:512832ms step_avg:185.07ms\nstep:2782/4500 train_loss:8.1188 train_time:513017ms step_avg:185.07ms\nstep:2783/4500 train_loss:8.1205 train_time:513201ms step_avg:185.07ms\nstep:2784/4500 train_loss:8.0929 train_time:513386ms step_avg:185.07ms\nstep:2785/4500 train_loss:8.1051 train_time:513570ms step_avg:185.07ms\nstep:2786/4500 train_loss:7.9851 train_time:513753ms step_avg:185.07ms\nstep:2787/4500 train_loss:8.1280 train_time:513937ms step_avg:185.07ms\nstep:2788/4500 train_loss:8.1363 train_time:514123ms step_avg:185.07ms\nstep:2789/4500 train_loss:8.1134 train_time:514307ms step_avg:185.07ms\nstep:2790/4500 train_loss:8.3083 train_time:514490ms step_avg:185.07ms\nstep:2791/4500 train_loss:8.0532 train_time:514674ms step_avg:185.07ms\nstep:2792/4500 train_loss:8.2096 train_time:514860ms step_avg:185.07ms\nstep:2793/4500 train_loss:8.0511 train_time:515044ms step_avg:185.07ms\nstep:2794/4500 train_loss:8.1364 train_time:515229ms step_avg:185.07ms\nstep:2795/4500 train_loss:8.1460 train_time:515413ms step_avg:185.07ms\nstep:2796/4500 train_loss:8.1137 train_time:515597ms step_avg:185.07ms\nstep:2797/4500 train_loss:8.0448 train_time:515783ms step_avg:185.07ms\nstep:2798/4500 train_loss:8.1422 train_time:515967ms step_avg:185.07ms\nstep:2799/4500 train_loss:8.0301 train_time:516150ms step_avg:185.07ms\nstep:2800/4500 train_loss:8.2738 train_time:516333ms step_avg:185.07ms\nstep:2801/4500 train_loss:8.1424 train_time:516519ms step_avg:185.07ms\nstep:2802/4500 train_loss:8.0446 train_time:516703ms step_avg:185.07ms\nstep:2803/4500 train_loss:8.2320 train_time:516889ms step_avg:185.07ms\nstep:2804/4500 train_loss:8.1267 train_time:517073ms step_avg:185.07ms\nstep:2805/4500 train_loss:8.0833 train_time:517258ms step_avg:185.07ms\nstep:2806/4500 train_loss:8.0063 train_time:517444ms step_avg:185.07ms\nstep:2807/4500 train_loss:8.0405 train_time:517632ms step_avg:185.07ms\nstep:2808/4500 train_loss:7.9890 train_time:517817ms step_avg:185.07ms\nstep:2809/4500 train_loss:8.2276 train_time:518001ms step_avg:185.07ms\nstep:2810/4500 train_loss:8.4128 train_time:518187ms step_avg:185.07ms\nstep:2811/4500 train_loss:8.1027 train_time:518372ms step_avg:185.07ms\nstep:2812/4500 train_loss:8.0574 train_time:518556ms step_avg:185.07ms\nstep:2813/4500 train_loss:8.2516 train_time:518742ms step_avg:185.07ms\nstep:2814/4500 train_loss:8.1014 train_time:518927ms step_avg:185.07ms\nstep:2815/4500 train_loss:8.0961 train_time:519110ms step_avg:185.07ms\nstep:2816/4500 train_loss:7.9316 train_time:519295ms step_avg:185.07ms\nstep:2817/4500 train_loss:8.1081 train_time:519481ms step_avg:185.07ms\nstep:2818/4500 train_loss:8.1046 train_time:519665ms step_avg:185.07ms\nstep:2819/4500 train_loss:8.0623 train_time:519853ms step_avg:185.07ms\nstep:2820/4500 train_loss:8.0588 train_time:520037ms step_avg:185.07ms\nstep:2821/4500 train_loss:8.1710 train_time:520221ms step_avg:185.07ms\nstep:2822/4500 train_loss:7.9603 train_time:520408ms step_avg:185.07ms\nstep:2823/4500 train_loss:8.0909 train_time:520595ms step_avg:185.07ms\nstep:2824/4500 train_loss:8.1776 train_time:520780ms step_avg:185.07ms\nstep:2825/4500 train_loss:8.0983 train_time:520967ms step_avg:185.07ms\nstep:2826/4500 train_loss:7.9948 train_time:521152ms step_avg:185.07ms\nstep:2827/4500 train_loss:8.1005 train_time:521337ms step_avg:185.07ms\nstep:2828/4500 train_loss:8.0781 train_time:521520ms step_avg:185.07ms\nstep:2829/4500 train_loss:8.0783 train_time:521707ms step_avg:185.07ms\nstep:2830/4500 train_loss:8.2139 train_time:521889ms step_avg:185.07ms\nstep:2831/4500 train_loss:8.1370 train_time:522074ms step_avg:185.07ms\nstep:2832/4500 train_loss:8.0749 train_time:522258ms step_avg:185.07ms\nstep:2833/4500 train_loss:8.0945 train_time:522441ms step_avg:185.07ms\nstep:2834/4500 train_loss:8.0882 train_time:522628ms step_avg:185.07ms\nstep:2835/4500 train_loss:8.6216 train_time:522813ms step_avg:185.07ms\nstep:2836/4500 train_loss:8.0749 train_time:522996ms step_avg:185.07ms\nstep:2837/4500 train_loss:8.0587 train_time:523180ms step_avg:185.07ms\nstep:2838/4500 train_loss:8.1458 train_time:523365ms step_avg:185.07ms\nstep:2839/4500 train_loss:8.0968 train_time:523551ms step_avg:185.07ms\nstep:2840/4500 train_loss:8.0170 train_time:523734ms step_avg:185.06ms\nstep:2841/4500 train_loss:8.0623 train_time:523919ms step_avg:185.06ms\nstep:2842/4500 train_loss:8.0534 train_time:524104ms step_avg:185.07ms\nstep:2843/4500 train_loss:8.0880 train_time:524289ms step_avg:185.06ms\nstep:2844/4500 train_loss:8.1733 train_time:524472ms step_avg:185.06ms\nstep:2845/4500 train_loss:8.1669 train_time:524657ms step_avg:185.06ms\nstep:2846/4500 train_loss:8.2338 train_time:524843ms step_avg:185.06ms\nstep:2847/4500 train_loss:8.1414 train_time:525027ms step_avg:185.06ms\nstep:2848/4500 train_loss:8.1506 train_time:525210ms step_avg:185.06ms\nstep:2849/4500 train_loss:7.8813 train_time:525392ms step_avg:185.06ms\nstep:2850/4500 train_loss:8.0396 train_time:525575ms step_avg:185.06ms\nstep:2851/4500 train_loss:8.1548 train_time:525758ms step_avg:185.06ms\nstep:2852/4500 train_loss:8.1058 train_time:525944ms step_avg:185.06ms\nstep:2853/4500 train_loss:7.9816 train_time:526130ms step_avg:185.06ms\nstep:2854/4500 train_loss:8.1901 train_time:526315ms step_avg:185.06ms\nstep:2855/4500 train_loss:8.0169 train_time:526500ms step_avg:185.06ms\nstep:2856/4500 train_loss:8.1946 train_time:526688ms step_avg:185.06ms\nstep:2857/4500 train_loss:8.0776 train_time:526873ms step_avg:185.06ms\nstep:2858/4500 train_loss:8.1754 train_time:527056ms step_avg:185.06ms\nstep:2859/4500 train_loss:8.1897 train_time:527239ms step_avg:185.06ms\nstep:2860/4500 train_loss:8.1734 train_time:527424ms step_avg:185.06ms\nstep:2861/4500 train_loss:8.1149 train_time:527611ms step_avg:185.06ms\nstep:2862/4500 train_loss:8.0459 train_time:527794ms step_avg:185.06ms\nstep:2863/4500 train_loss:8.0572 train_time:527978ms step_avg:185.06ms\nstep:2864/4500 train_loss:8.2055 train_time:528164ms step_avg:185.06ms\nstep:2865/4500 train_loss:8.1363 train_time:528348ms step_avg:185.06ms\nstep:2866/4500 train_loss:8.1585 train_time:528533ms step_avg:185.06ms\nstep:2867/4500 train_loss:8.1019 train_time:528718ms step_avg:185.06ms\nstep:2868/4500 train_loss:8.1852 train_time:528908ms step_avg:185.06ms\nstep:2869/4500 train_loss:7.9126 train_time:529092ms step_avg:185.06ms\nstep:2870/4500 train_loss:8.0992 train_time:529276ms step_avg:185.06ms\nstep:2871/4500 train_loss:7.9881 train_time:529459ms step_avg:185.06ms\nstep:2872/4500 train_loss:8.2526 train_time:529642ms step_avg:185.06ms\nstep:2873/4500 train_loss:8.1122 train_time:529825ms step_avg:185.06ms\nstep:2874/4500 train_loss:8.1915 train_time:530011ms step_avg:185.06ms\nstep:2875/4500 train_loss:8.1251 train_time:530195ms step_avg:185.06ms\nstep:2876/4500 train_loss:8.0985 train_time:530381ms step_avg:185.06ms\nstep:2877/4500 train_loss:8.1457 train_time:530569ms step_avg:185.06ms\nstep:2878/4500 train_loss:8.1183 train_time:530755ms step_avg:185.06ms\nstep:2879/4500 train_loss:8.2034 train_time:530940ms step_avg:185.06ms\nstep:2880/4500 train_loss:8.1047 train_time:531124ms step_avg:185.06ms\nstep:2881/4500 train_loss:8.1404 train_time:531307ms step_avg:185.06ms\nstep:2882/4500 train_loss:8.1078 train_time:531492ms step_avg:185.06ms\nstep:2883/4500 train_loss:8.0044 train_time:531678ms step_avg:185.06ms\nstep:2884/4500 train_loss:8.0480 train_time:531865ms step_avg:185.06ms\nstep:2885/4500 train_loss:8.1147 train_time:532053ms step_avg:185.06ms\nstep:2886/4500 train_loss:8.1307 train_time:532237ms step_avg:185.06ms\nstep:2887/4500 train_loss:7.9933 train_time:532422ms step_avg:185.06ms\nstep:2888/4500 train_loss:8.0253 train_time:532607ms step_avg:185.06ms\nstep:2889/4500 train_loss:8.1670 train_time:532791ms step_avg:185.06ms\nstep:2890/4500 train_loss:8.0060 train_time:532974ms step_avg:185.06ms\nstep:2891/4500 train_loss:8.2300 train_time:533157ms step_avg:185.06ms\nstep:2892/4500 train_loss:8.1556 train_time:533341ms step_avg:185.06ms\nstep:2893/4500 train_loss:8.1842 train_time:533525ms step_avg:185.06ms\nstep:2894/4500 train_loss:8.1097 train_time:533707ms step_avg:185.06ms\nstep:2895/4500 train_loss:8.2217 train_time:533890ms step_avg:185.06ms\nstep:2896/4500 train_loss:8.1461 train_time:534074ms step_avg:185.06ms\nstep:2897/4500 train_loss:8.0804 train_time:534258ms step_avg:185.06ms\nstep:2898/4500 train_loss:8.1089 train_time:534443ms step_avg:185.06ms\nstep:2899/4500 train_loss:8.0766 train_time:534631ms step_avg:185.06ms\nstep:2900/4500 train_loss:8.1295 train_time:534818ms step_avg:185.06ms\nstep:2901/4500 train_loss:8.1013 train_time:535002ms step_avg:185.06ms\nstep:2902/4500 train_loss:8.0958 train_time:535189ms step_avg:185.06ms\nstep:2903/4500 train_loss:8.1058 train_time:535373ms step_avg:185.06ms\nstep:2904/4500 train_loss:8.1344 train_time:535556ms step_avg:185.06ms\nstep:2905/4500 train_loss:8.0504 train_time:535739ms step_avg:185.06ms\nstep:2906/4500 train_loss:8.0798 train_time:535927ms step_avg:185.06ms\nstep:2907/4500 train_loss:8.0665 train_time:536110ms step_avg:185.06ms\nstep:2908/4500 train_loss:8.1208 train_time:536294ms step_avg:185.06ms\nstep:2909/4500 train_loss:8.2793 train_time:536481ms step_avg:185.06ms\nstep:2910/4500 train_loss:8.0461 train_time:536669ms step_avg:185.06ms\nstep:2911/4500 train_loss:8.1718 train_time:536854ms step_avg:185.06ms\nstep:2912/4500 train_loss:7.9426 train_time:537037ms step_avg:185.06ms\nstep:2913/4500 train_loss:8.1255 train_time:537223ms step_avg:185.06ms\nstep:2914/4500 train_loss:8.1549 train_time:537408ms step_avg:185.06ms\nstep:2915/4500 train_loss:8.1176 train_time:537592ms step_avg:185.06ms\nstep:2916/4500 train_loss:8.1407 train_time:537778ms step_avg:185.06ms\nstep:2917/4500 train_loss:8.0294 train_time:537963ms step_avg:185.06ms\nstep:2918/4500 train_loss:7.9851 train_time:538149ms step_avg:185.06ms\nstep:2919/4500 train_loss:7.9379 train_time:538332ms step_avg:185.06ms\nstep:2920/4500 train_loss:8.2541 train_time:538517ms step_avg:185.06ms\nstep:2921/4500 train_loss:8.1778 train_time:538702ms step_avg:185.06ms\nstep:2922/4500 train_loss:7.9750 train_time:538890ms step_avg:185.06ms\nstep:2923/4500 train_loss:8.1794 train_time:539073ms step_avg:185.06ms\nstep:2924/4500 train_loss:8.1708 train_time:539257ms step_avg:185.06ms\nstep:2925/4500 train_loss:8.0902 train_time:539443ms step_avg:185.06ms\nstep:2926/4500 train_loss:8.0524 train_time:539628ms step_avg:185.06ms\nstep:2927/4500 train_loss:8.1658 train_time:539813ms step_avg:185.06ms\nstep:2928/4500 train_loss:7.9904 train_time:539998ms step_avg:185.06ms\nstep:2929/4500 train_loss:8.0718 train_time:540188ms step_avg:185.06ms\nstep:2930/4500 train_loss:8.0506 train_time:540375ms step_avg:185.06ms\nstep:2931/4500 train_loss:8.1728 train_time:540560ms step_avg:185.06ms\nstep:2932/4500 train_loss:8.1693 train_time:540744ms step_avg:185.06ms\nstep:2933/4500 train_loss:8.0439 train_time:540932ms step_avg:185.06ms\nstep:2934/4500 train_loss:8.1294 train_time:541116ms step_avg:185.06ms\nstep:2935/4500 train_loss:8.0673 train_time:541302ms step_avg:185.06ms\nstep:2936/4500 train_loss:8.2933 train_time:541491ms step_avg:185.06ms\nstep:2937/4500 train_loss:8.0012 train_time:541675ms step_avg:185.06ms\nstep:2938/4500 train_loss:8.1215 train_time:541859ms step_avg:185.06ms\nstep:2939/4500 train_loss:8.1384 train_time:542044ms step_avg:185.06ms\nstep:2940/4500 train_loss:8.0291 train_time:542230ms step_avg:185.06ms\nstep:2941/4500 train_loss:8.1181 train_time:542415ms step_avg:185.06ms\nstep:2942/4500 train_loss:8.3228 train_time:542601ms step_avg:185.06ms\nstep:2943/4500 train_loss:8.0475 train_time:542787ms step_avg:185.06ms\nstep:2944/4500 train_loss:8.1528 train_time:542971ms step_avg:185.06ms\nstep:2945/4500 train_loss:8.1977 train_time:543155ms step_avg:185.06ms\nstep:2946/4500 train_loss:8.1487 train_time:543341ms step_avg:185.06ms\nstep:2947/4500 train_loss:8.0476 train_time:543527ms step_avg:185.06ms\nstep:2948/4500 train_loss:8.1801 train_time:543711ms step_avg:185.06ms\nstep:2949/4500 train_loss:7.9955 train_time:543895ms step_avg:185.06ms\nstep:2950/4500 train_loss:8.0938 train_time:544081ms step_avg:185.06ms\nstep:2951/4500 train_loss:8.2034 train_time:544266ms step_avg:185.06ms\nstep:2952/4500 train_loss:8.0775 train_time:544450ms step_avg:185.06ms\nstep:2953/4500 train_loss:8.0702 train_time:544636ms step_avg:185.06ms\nstep:2954/4500 train_loss:8.0561 train_time:544821ms step_avg:185.06ms\nstep:2955/4500 train_loss:8.1369 train_time:545006ms step_avg:185.06ms\nstep:2956/4500 train_loss:8.0972 train_time:545190ms step_avg:185.06ms\nstep:2957/4500 train_loss:8.0744 train_time:545374ms step_avg:185.06ms\nstep:2958/4500 train_loss:8.1731 train_time:545559ms step_avg:185.06ms\nstep:2959/4500 train_loss:8.1927 train_time:545745ms step_avg:185.06ms\nstep:2960/4500 train_loss:8.0191 train_time:545929ms step_avg:185.06ms\nstep:2961/4500 train_loss:8.1263 train_time:546113ms step_avg:185.06ms\nstep:2962/4500 train_loss:8.1349 train_time:546298ms step_avg:185.06ms\nstep:2963/4500 train_loss:8.0969 train_time:546485ms step_avg:185.06ms\nstep:2964/4500 train_loss:7.9820 train_time:546670ms step_avg:185.06ms\nstep:2965/4500 train_loss:8.0794 train_time:546856ms step_avg:185.06ms\nstep:2966/4500 train_loss:8.1524 train_time:547041ms step_avg:185.06ms\nstep:2967/4500 train_loss:8.1073 train_time:547229ms step_avg:185.06ms\nstep:2968/4500 train_loss:8.1280 train_time:547415ms step_avg:185.06ms\nstep:2969/4500 train_loss:8.1398 train_time:547599ms step_avg:185.06ms\nstep:2970/4500 train_loss:8.1700 train_time:547785ms step_avg:185.06ms\nstep:2971/4500 train_loss:8.1176 train_time:547972ms step_avg:185.06ms\nstep:2972/4500 train_loss:8.2694 train_time:548155ms step_avg:185.06ms\nstep:2973/4500 train_loss:8.3236 train_time:548340ms step_avg:185.06ms\nstep:2974/4500 train_loss:8.1369 train_time:548523ms step_avg:185.06ms\nstep:2975/4500 train_loss:8.1289 train_time:548709ms step_avg:185.06ms\nstep:2976/4500 train_loss:8.1649 train_time:548896ms step_avg:185.06ms\nstep:2977/4500 train_loss:8.1382 train_time:549083ms step_avg:185.06ms\nstep:2978/4500 train_loss:8.1806 train_time:549267ms step_avg:185.06ms\nstep:2979/4500 train_loss:8.1450 train_time:549451ms step_avg:185.06ms\nstep:2980/4500 train_loss:8.1175 train_time:549635ms step_avg:185.06ms\nstep:2981/4500 train_loss:8.0928 train_time:549820ms step_avg:185.06ms\nstep:2982/4500 train_loss:8.0856 train_time:550008ms step_avg:185.06ms\nstep:2983/4500 train_loss:8.0120 train_time:550190ms step_avg:185.06ms\nstep:2984/4500 train_loss:8.1371 train_time:550374ms step_avg:185.06ms\nstep:2985/4500 train_loss:8.0714 train_time:550558ms step_avg:185.06ms\nstep:2986/4500 train_loss:8.1060 train_time:550744ms step_avg:185.06ms\nstep:2987/4500 train_loss:8.0270 train_time:550930ms step_avg:185.06ms\nstep:2988/4500 train_loss:8.1111 train_time:551116ms step_avg:185.06ms\nstep:2989/4500 train_loss:8.1678 train_time:551302ms step_avg:185.06ms\nstep:2990/4500 train_loss:8.1600 train_time:551490ms step_avg:185.06ms\nstep:2991/4500 train_loss:8.0284 train_time:551675ms step_avg:185.06ms\nstep:2992/4500 train_loss:8.1236 train_time:551860ms step_avg:185.06ms\nstep:2993/4500 train_loss:8.0527 train_time:552046ms step_avg:185.06ms\nstep:2994/4500 train_loss:8.0868 train_time:552229ms step_avg:185.06ms\nstep:2995/4500 train_loss:8.1826 train_time:552413ms step_avg:185.06ms\nstep:2996/4500 train_loss:8.0192 train_time:552596ms step_avg:185.06ms\nstep:2997/4500 train_loss:8.1237 train_time:552780ms step_avg:185.06ms\nstep:2998/4500 train_loss:8.1679 train_time:552964ms step_avg:185.06ms\nstep:2999/4500 train_loss:8.1438 train_time:553150ms step_avg:185.06ms\nstep:3000/4500 train_loss:8.1039 train_time:553338ms step_avg:185.06ms\nstep:3001/4500 train_loss:8.1113 train_time:553522ms step_avg:185.06ms\nstep:3002/4500 train_loss:7.9947 train_time:553707ms step_avg:185.06ms\nstep:3003/4500 train_loss:8.0743 train_time:553891ms step_avg:185.06ms\nstep:3004/4500 train_loss:8.0921 train_time:554073ms step_avg:185.06ms\nstep:3005/4500 train_loss:8.1508 train_time:554257ms step_avg:185.06ms\nstep:3006/4500 train_loss:8.1654 train_time:554441ms step_avg:185.06ms\nstep:3007/4500 train_loss:7.9335 train_time:554626ms step_avg:185.06ms\nstep:3008/4500 train_loss:8.0885 train_time:554810ms step_avg:185.06ms\nstep:3009/4500 train_loss:8.1606 train_time:554997ms step_avg:185.06ms\nstep:3010/4500 train_loss:7.9553 train_time:555183ms step_avg:185.06ms\nstep:3011/4500 train_loss:8.0415 train_time:555370ms step_avg:185.06ms\nstep:3012/4500 train_loss:8.1079 train_time:555554ms step_avg:185.06ms\nstep:3013/4500 train_loss:8.1477 train_time:555741ms step_avg:185.06ms\nstep:3014/4500 train_loss:8.1346 train_time:555929ms step_avg:185.06ms\nstep:3015/4500 train_loss:8.0036 train_time:556115ms step_avg:185.06ms\nstep:3016/4500 train_loss:8.0788 train_time:556302ms step_avg:185.06ms\nstep:3017/4500 train_loss:8.0637 train_time:556486ms step_avg:185.06ms\nstep:3018/4500 train_loss:8.0289 train_time:556672ms step_avg:185.06ms\nstep:3019/4500 train_loss:8.1153 train_time:556856ms step_avg:185.06ms\nstep:3020/4500 train_loss:8.0582 train_time:557040ms step_avg:185.06ms\nstep:3021/4500 train_loss:8.1530 train_time:557226ms step_avg:185.06ms\nstep:3022/4500 train_loss:8.0359 train_time:557409ms step_avg:185.06ms\nstep:3023/4500 train_loss:7.9959 train_time:557595ms step_avg:185.06ms\nstep:3024/4500 train_loss:8.1515 train_time:557781ms step_avg:185.06ms\nstep:3025/4500 train_loss:8.1601 train_time:557967ms step_avg:185.06ms\nstep:3026/4500 train_loss:8.0933 train_time:558150ms step_avg:185.06ms\nstep:3027/4500 train_loss:8.1706 train_time:558336ms step_avg:185.06ms\nstep:3028/4500 train_loss:8.1365 train_time:558520ms step_avg:185.06ms\nstep:3029/4500 train_loss:8.1235 train_time:558706ms step_avg:185.06ms\nstep:3030/4500 train_loss:8.2514 train_time:558893ms step_avg:185.06ms\nstep:3031/4500 train_loss:8.2156 train_time:559079ms step_avg:185.06ms\nstep:3032/4500 train_loss:8.1227 train_time:559265ms step_avg:185.06ms\nstep:3033/4500 train_loss:8.1244 train_time:559452ms step_avg:185.07ms\nstep:3034/4500 train_loss:8.1821 train_time:559636ms step_avg:185.06ms\nstep:3035/4500 train_loss:8.0623 train_time:559821ms step_avg:185.06ms\nstep:3036/4500 train_loss:8.1009 train_time:560007ms step_avg:185.07ms\nstep:3037/4500 train_loss:8.1327 train_time:560190ms step_avg:185.06ms\nstep:3038/4500 train_loss:8.1324 train_time:560374ms step_avg:185.06ms\nstep:3039/4500 train_loss:8.0421 train_time:560559ms step_avg:185.06ms\nstep:3040/4500 train_loss:7.9195 train_time:560745ms step_avg:185.06ms\nstep:3041/4500 train_loss:8.1024 train_time:560933ms step_avg:185.07ms\nstep:3042/4500 train_loss:8.1024 train_time:561119ms step_avg:185.07ms\nstep:3043/4500 train_loss:8.1266 train_time:561304ms step_avg:185.07ms\nstep:3044/4500 train_loss:8.0716 train_time:561491ms step_avg:185.07ms\nstep:3045/4500 train_loss:8.1438 train_time:561675ms step_avg:185.07ms\nstep:3046/4500 train_loss:8.3157 train_time:561860ms step_avg:185.07ms\nstep:3047/4500 train_loss:8.1463 train_time:562045ms step_avg:185.07ms\nstep:3048/4500 train_loss:8.1201 train_time:562456ms step_avg:185.14ms\nstep:3049/4500 train_loss:8.1373 train_time:562640ms step_avg:185.14ms\nstep:3050/4500 train_loss:8.0379 train_time:562823ms step_avg:185.14ms\nstep:3051/4500 train_loss:8.0350 train_time:563005ms step_avg:185.14ms\nstep:3052/4500 train_loss:8.2389 train_time:563190ms step_avg:185.14ms\nstep:3053/4500 train_loss:8.0210 train_time:563373ms step_avg:185.14ms\nstep:3054/4500 train_loss:8.2306 train_time:563558ms step_avg:185.14ms\nstep:3055/4500 train_loss:8.2494 train_time:563743ms step_avg:185.14ms\nstep:3056/4500 train_loss:7.9195 train_time:563932ms step_avg:185.14ms\nstep:3057/4500 train_loss:8.1133 train_time:564116ms step_avg:185.14ms\nstep:3058/4500 train_loss:8.1365 train_time:564304ms step_avg:185.14ms\nstep:3059/4500 train_loss:8.1657 train_time:564492ms step_avg:185.14ms\nstep:3060/4500 train_loss:8.2129 train_time:564677ms step_avg:185.14ms\nstep:3061/4500 train_loss:8.1121 train_time:564862ms step_avg:185.14ms\nstep:3062/4500 train_loss:8.1564 train_time:565046ms step_avg:185.14ms\nstep:3063/4500 train_loss:8.1161 train_time:565230ms step_avg:185.14ms\nstep:3064/4500 train_loss:8.1308 train_time:565413ms step_avg:185.14ms\nstep:3065/4500 train_loss:8.0914 train_time:565597ms step_avg:185.14ms\nstep:3066/4500 train_loss:8.1150 train_time:565785ms step_avg:185.14ms\nstep:3067/4500 train_loss:8.2271 train_time:565969ms step_avg:185.14ms\nstep:3068/4500 train_loss:8.2512 train_time:566151ms step_avg:185.14ms\nstep:3069/4500 train_loss:8.1046 train_time:566335ms step_avg:185.14ms\nstep:3070/4500 train_loss:8.2179 train_time:566520ms step_avg:185.14ms\nstep:3071/4500 train_loss:8.0361 train_time:566704ms step_avg:185.14ms\nstep:3072/4500 train_loss:8.0362 train_time:566890ms step_avg:185.14ms\nstep:3073/4500 train_loss:8.1195 train_time:567074ms step_avg:185.14ms\nstep:3074/4500 train_loss:8.1339 train_time:567259ms step_avg:185.14ms\nstep:3075/4500 train_loss:8.1933 train_time:567447ms step_avg:185.14ms\nstep:3076/4500 train_loss:8.2196 train_time:567634ms step_avg:185.14ms\nstep:3077/4500 train_loss:8.1085 train_time:567820ms step_avg:185.14ms\nstep:3078/4500 train_loss:8.0848 train_time:568004ms step_avg:185.14ms\nstep:3079/4500 train_loss:8.0885 train_time:568190ms step_avg:185.14ms\nstep:3080/4500 train_loss:8.0849 train_time:568373ms step_avg:185.14ms\nstep:3081/4500 train_loss:8.0953 train_time:568557ms step_avg:185.14ms\nstep:3082/4500 train_loss:8.1924 train_time:568740ms step_avg:185.14ms\nstep:3083/4500 train_loss:8.1344 train_time:568928ms step_avg:185.14ms\nstep:3084/4500 train_loss:8.3288 train_time:569113ms step_avg:185.14ms\nstep:3085/4500 train_loss:7.8448 train_time:569295ms step_avg:185.14ms\nstep:3086/4500 train_loss:8.1832 train_time:569482ms step_avg:185.14ms\nstep:3087/4500 train_loss:8.0750 train_time:569667ms step_avg:185.14ms\nstep:3088/4500 train_loss:8.2155 train_time:569851ms step_avg:185.14ms\nstep:3089/4500 train_loss:8.1166 train_time:570036ms step_avg:185.14ms\nstep:3090/4500 train_loss:8.2311 train_time:570221ms step_avg:185.14ms\nstep:3091/4500 train_loss:8.0919 train_time:570408ms step_avg:185.14ms\nstep:3092/4500 train_loss:8.1027 train_time:570595ms step_avg:185.14ms\nstep:3093/4500 train_loss:8.1563 train_time:570781ms step_avg:185.14ms\nstep:3094/4500 train_loss:8.0576 train_time:570967ms step_avg:185.14ms\nstep:3095/4500 train_loss:8.0373 train_time:571150ms step_avg:185.14ms\nstep:3096/4500 train_loss:8.1761 train_time:571334ms step_avg:185.14ms\nstep:3097/4500 train_loss:8.0539 train_time:571520ms step_avg:185.14ms\nstep:3098/4500 train_loss:8.2372 train_time:571705ms step_avg:185.14ms\nstep:3099/4500 train_loss:8.1057 train_time:571890ms step_avg:185.14ms\nstep:3100/4500 train_loss:8.1852 train_time:572076ms step_avg:185.14ms\nstep:3101/4500 train_loss:8.1033 train_time:572261ms step_avg:185.14ms\nstep:3102/4500 train_loss:8.0793 train_time:572445ms step_avg:185.14ms\nstep:3103/4500 train_loss:8.2843 train_time:572629ms step_avg:185.14ms\nstep:3104/4500 train_loss:8.0701 train_time:572814ms step_avg:185.14ms\nstep:3105/4500 train_loss:8.1471 train_time:573000ms step_avg:185.14ms\nstep:3106/4500 train_loss:8.1025 train_time:573185ms step_avg:185.14ms\nstep:3107/4500 train_loss:8.0701 train_time:573368ms step_avg:185.14ms\nstep:3108/4500 train_loss:8.0334 train_time:573552ms step_avg:185.14ms\nstep:3109/4500 train_loss:8.1018 train_time:573737ms step_avg:185.14ms\nstep:3110/4500 train_loss:8.2359 train_time:573922ms step_avg:185.14ms\nstep:3111/4500 train_loss:8.1800 train_time:574107ms step_avg:185.14ms\nstep:3112/4500 train_loss:8.1058 train_time:574291ms step_avg:185.14ms\nstep:3113/4500 train_loss:8.1538 train_time:574475ms step_avg:185.14ms\nstep:3114/4500 train_loss:8.0566 train_time:574660ms step_avg:185.14ms\nstep:3115/4500 train_loss:8.1295 train_time:574844ms step_avg:185.13ms\nstep:3116/4500 train_loss:8.1272 train_time:575028ms step_avg:185.13ms\nstep:3117/4500 train_loss:8.0742 train_time:575212ms step_avg:185.13ms\nstep:3118/4500 train_loss:8.0471 train_time:575397ms step_avg:185.13ms\nstep:3119/4500 train_loss:8.1008 train_time:575585ms step_avg:185.13ms\nstep:3120/4500 train_loss:8.0778 train_time:575769ms step_avg:185.13ms\nstep:3121/4500 train_loss:8.1336 train_time:575953ms step_avg:185.13ms\nstep:3122/4500 train_loss:8.0083 train_time:576137ms step_avg:185.13ms\nstep:3123/4500 train_loss:8.1851 train_time:576323ms step_avg:185.13ms\nstep:3124/4500 train_loss:7.8816 train_time:576511ms step_avg:185.14ms\nstep:3125/4500 train_loss:8.0782 train_time:576694ms step_avg:185.13ms\nstep:3126/4500 train_loss:8.1426 train_time:576878ms step_avg:185.13ms\nstep:3127/4500 train_loss:8.1719 train_time:577064ms step_avg:185.13ms\nstep:3128/4500 train_loss:7.9831 train_time:577253ms step_avg:185.14ms\nstep:3129/4500 train_loss:8.0900 train_time:577438ms step_avg:185.14ms\nstep:3130/4500 train_loss:8.1497 train_time:577626ms step_avg:185.14ms\nstep:3131/4500 train_loss:8.0765 train_time:577808ms step_avg:185.14ms\nstep:3132/4500 train_loss:8.1670 train_time:577992ms step_avg:185.14ms\nstep:3133/4500 train_loss:8.1749 train_time:578177ms step_avg:185.13ms\nstep:3134/4500 train_loss:7.9944 train_time:578361ms step_avg:185.13ms\nstep:3135/4500 train_loss:8.0581 train_time:578546ms step_avg:185.13ms\nstep:3136/4500 train_loss:8.1294 train_time:578732ms step_avg:185.14ms\nstep:3137/4500 train_loss:8.0495 train_time:578917ms step_avg:185.13ms\nstep:3138/4500 train_loss:8.2013 train_time:579101ms step_avg:185.13ms\nstep:3139/4500 train_loss:8.0517 train_time:579288ms step_avg:185.14ms\nstep:3140/4500 train_loss:8.0067 train_time:579472ms step_avg:185.13ms\nstep:3141/4500 train_loss:8.1021 train_time:579656ms step_avg:185.13ms\nstep:3142/4500 train_loss:7.9696 train_time:579840ms step_avg:185.13ms\nstep:3143/4500 train_loss:8.0755 train_time:580024ms step_avg:185.13ms\nstep:3144/4500 train_loss:8.2852 train_time:580210ms step_avg:185.13ms\nstep:3145/4500 train_loss:8.0777 train_time:580393ms step_avg:185.13ms\nstep:3146/4500 train_loss:8.0827 train_time:580577ms step_avg:185.13ms\nstep:3147/4500 train_loss:8.1394 train_time:580761ms step_avg:185.13ms\nstep:3148/4500 train_loss:8.1091 train_time:580949ms step_avg:185.13ms\nstep:3149/4500 train_loss:7.8691 train_time:581133ms step_avg:185.13ms\nstep:3150/4500 train_loss:8.0335 train_time:581318ms step_avg:185.13ms\nstep:3151/4500 train_loss:8.0951 train_time:581503ms step_avg:185.13ms\nstep:3152/4500 train_loss:8.0793 train_time:581686ms step_avg:185.13ms\nstep:3153/4500 train_loss:8.2091 train_time:581868ms step_avg:185.13ms\nstep:3154/4500 train_loss:8.2629 train_time:582054ms step_avg:185.13ms\nstep:3155/4500 train_loss:8.1536 train_time:582240ms step_avg:185.13ms\nstep:3156/4500 train_loss:8.2907 train_time:582425ms step_avg:185.13ms\nstep:3157/4500 train_loss:8.1194 train_time:582611ms step_avg:185.13ms\nstep:3158/4500 train_loss:8.0118 train_time:582794ms step_avg:185.13ms\nstep:3159/4500 train_loss:7.9581 train_time:582978ms step_avg:185.13ms\nstep:3160/4500 train_loss:8.1096 train_time:583164ms step_avg:185.13ms\nstep:3161/4500 train_loss:8.1219 train_time:583350ms step_avg:185.13ms\nstep:3162/4500 train_loss:8.1356 train_time:583534ms step_avg:185.13ms\nstep:3163/4500 train_loss:8.1037 train_time:583720ms step_avg:185.13ms\nstep:3164/4500 train_loss:8.1801 train_time:583904ms step_avg:185.13ms\nstep:3165/4500 train_loss:8.4969 train_time:584087ms step_avg:185.13ms\nstep:3166/4500 train_loss:8.1283 train_time:584270ms step_avg:185.13ms\nstep:3167/4500 train_loss:7.9566 train_time:584455ms step_avg:185.13ms\nstep:3168/4500 train_loss:8.1627 train_time:584639ms step_avg:185.13ms\nstep:3169/4500 train_loss:8.1708 train_time:584824ms step_avg:185.13ms\nstep:3170/4500 train_loss:8.1280 train_time:585007ms step_avg:185.13ms\nstep:3171/4500 train_loss:8.1448 train_time:585192ms step_avg:185.13ms\nstep:3172/4500 train_loss:8.1894 train_time:585376ms step_avg:185.13ms\nstep:3173/4500 train_loss:8.0529 train_time:585561ms step_avg:185.13ms\nstep:3174/4500 train_loss:8.1603 train_time:585747ms step_avg:185.13ms\nstep:3175/4500 train_loss:8.1497 train_time:585935ms step_avg:185.13ms\nstep:3176/4500 train_loss:8.0336 train_time:586119ms step_avg:185.13ms\nstep:3177/4500 train_loss:8.2137 train_time:586304ms step_avg:185.13ms\nstep:3178/4500 train_loss:8.0977 train_time:586488ms step_avg:185.13ms\nstep:3179/4500 train_loss:8.0182 train_time:586672ms step_avg:185.13ms\nstep:3180/4500 train_loss:8.0966 train_time:586857ms step_avg:185.13ms\nstep:3181/4500 train_loss:8.0193 train_time:587045ms step_avg:185.13ms\nstep:3182/4500 train_loss:8.0801 train_time:587227ms step_avg:185.13ms\nstep:3183/4500 train_loss:8.0643 train_time:587413ms step_avg:185.13ms\nstep:3184/4500 train_loss:8.0467 train_time:587600ms step_avg:185.13ms\nstep:3185/4500 train_loss:8.2693 train_time:587785ms step_avg:185.13ms\nstep:3186/4500 train_loss:8.0567 train_time:587973ms step_avg:185.13ms\nstep:3187/4500 train_loss:8.0717 train_time:588160ms step_avg:185.13ms\nstep:3188/4500 train_loss:8.1207 train_time:588344ms step_avg:185.13ms\nstep:3189/4500 train_loss:8.1543 train_time:588531ms step_avg:185.13ms\nstep:3190/4500 train_loss:8.1274 train_time:588717ms step_avg:185.13ms\nstep:3191/4500 train_loss:8.1824 train_time:588904ms step_avg:185.13ms\nstep:3192/4500 train_loss:7.9985 train_time:589089ms step_avg:185.13ms\nstep:3193/4500 train_loss:8.2206 train_time:589273ms step_avg:185.13ms\nstep:3194/4500 train_loss:8.1215 train_time:589458ms step_avg:185.13ms\nstep:3195/4500 train_loss:8.1086 train_time:589645ms step_avg:185.13ms\nstep:3196/4500 train_loss:7.9917 train_time:589828ms step_avg:185.13ms\nstep:3197/4500 train_loss:8.0877 train_time:590012ms step_avg:185.13ms\nstep:3198/4500 train_loss:8.0818 train_time:590195ms step_avg:185.13ms\nstep:3199/4500 train_loss:8.0903 train_time:590379ms step_avg:185.13ms\nstep:3200/4500 train_loss:8.8270 train_time:590563ms step_avg:185.13ms\nstep:3201/4500 train_loss:8.0471 train_time:590749ms step_avg:185.13ms\nstep:3202/4500 train_loss:8.0466 train_time:590934ms step_avg:185.13ms\nstep:3203/4500 train_loss:8.1696 train_time:591120ms step_avg:185.13ms\nstep:3204/4500 train_loss:8.1017 train_time:591308ms step_avg:185.13ms\nstep:3205/4500 train_loss:8.0992 train_time:591495ms step_avg:185.13ms\nstep:3206/4500 train_loss:8.2057 train_time:591679ms step_avg:185.13ms\nstep:3207/4500 train_loss:8.1203 train_time:591862ms step_avg:185.13ms\nstep:3208/4500 train_loss:8.1118 train_time:592046ms step_avg:185.13ms\nstep:3209/4500 train_loss:8.0993 train_time:592229ms step_avg:185.13ms\nstep:3210/4500 train_loss:8.3077 train_time:592413ms step_avg:185.13ms\nstep:3211/4500 train_loss:8.2992 train_time:592600ms step_avg:185.13ms\nstep:3212/4500 train_loss:8.1173 train_time:592789ms step_avg:185.13ms\nstep:3213/4500 train_loss:8.2176 train_time:592974ms step_avg:185.13ms\nstep:3214/4500 train_loss:8.0857 train_time:593158ms step_avg:185.13ms\nstep:3215/4500 train_loss:8.1487 train_time:593343ms step_avg:185.13ms\nstep:3216/4500 train_loss:8.1331 train_time:593527ms step_avg:185.13ms\nstep:3217/4500 train_loss:8.1322 train_time:593711ms step_avg:185.13ms\nstep:3218/4500 train_loss:8.1140 train_time:593896ms step_avg:185.13ms\nstep:3219/4500 train_loss:8.1343 train_time:594080ms step_avg:185.13ms\nstep:3220/4500 train_loss:8.0406 train_time:594265ms step_avg:185.13ms\nstep:3221/4500 train_loss:8.0923 train_time:594449ms step_avg:185.13ms\nstep:3222/4500 train_loss:8.1748 train_time:594635ms step_avg:185.13ms\nstep:3223/4500 train_loss:8.0428 train_time:594823ms step_avg:185.13ms\nstep:3224/4500 train_loss:7.9306 train_time:595012ms step_avg:185.13ms\nstep:3225/4500 train_loss:8.2287 train_time:595196ms step_avg:185.13ms\nstep:3226/4500 train_loss:8.0886 train_time:595380ms step_avg:185.13ms\nstep:3227/4500 train_loss:8.4151 train_time:595566ms step_avg:185.13ms\nstep:3228/4500 train_loss:8.0767 train_time:595753ms step_avg:185.13ms\nstep:3229/4500 train_loss:8.1472 train_time:595939ms step_avg:185.13ms\nstep:3230/4500 train_loss:8.1201 train_time:596128ms step_avg:185.13ms\nstep:3231/4500 train_loss:8.1711 train_time:596312ms step_avg:185.13ms\nstep:3232/4500 train_loss:8.1261 train_time:596496ms step_avg:185.13ms\nstep:3233/4500 train_loss:8.1932 train_time:596681ms step_avg:185.13ms\nstep:3234/4500 train_loss:8.1786 train_time:596868ms step_avg:185.13ms\nstep:3235/4500 train_loss:8.3113 train_time:597054ms step_avg:185.13ms\nstep:3236/4500 train_loss:8.1664 train_time:597240ms step_avg:185.13ms\nstep:3237/4500 train_loss:8.1431 train_time:597428ms step_avg:185.13ms\nstep:3238/4500 train_loss:8.1120 train_time:597614ms step_avg:185.13ms\nstep:3239/4500 train_loss:8.0815 train_time:597797ms step_avg:185.13ms\nstep:3240/4500 train_loss:8.1411 train_time:597982ms step_avg:185.13ms\nstep:3241/4500 train_loss:8.1210 train_time:598169ms step_avg:185.13ms\nstep:3242/4500 train_loss:8.1074 train_time:598353ms step_avg:185.13ms\nstep:3243/4500 train_loss:8.3865 train_time:598540ms step_avg:185.13ms\nstep:3244/4500 train_loss:8.1500 train_time:598728ms step_avg:185.14ms\nstep:3245/4500 train_loss:8.1042 train_time:598912ms step_avg:185.14ms\nstep:3246/4500 train_loss:8.1309 train_time:599097ms step_avg:185.13ms\nstep:3247/4500 train_loss:8.1116 train_time:599281ms step_avg:185.13ms\nstep:3248/4500 train_loss:8.1045 train_time:599466ms step_avg:185.13ms\nstep:3249/4500 train_loss:8.1399 train_time:599653ms step_avg:185.14ms\nstep:3250/4500 train_loss:8.1383 train_time:599838ms step_avg:185.14ms\nstep:3251/4500 train_loss:8.1689 train_time:600024ms step_avg:185.14ms\nstep:3252/4500 train_loss:8.0709 train_time:600208ms step_avg:185.14ms\nstep:3253/4500 train_loss:8.1626 train_time:600392ms step_avg:185.13ms\nstep:3254/4500 train_loss:8.1138 train_time:600575ms step_avg:185.13ms\nstep:3255/4500 train_loss:8.1240 train_time:600761ms step_avg:185.13ms\nstep:3256/4500 train_loss:8.1667 train_time:600945ms step_avg:185.13ms\nstep:3257/4500 train_loss:8.0570 train_time:601128ms step_avg:185.13ms\nstep:3258/4500 train_loss:8.1483 train_time:601314ms step_avg:185.13ms\nstep:3259/4500 train_loss:8.1895 train_time:601499ms step_avg:185.13ms\nstep:3260/4500 train_loss:8.0382 train_time:601685ms step_avg:185.13ms\nstep:3261/4500 train_loss:8.2139 train_time:601874ms step_avg:185.14ms\nstep:3262/4500 train_loss:8.1446 train_time:602058ms step_avg:185.13ms\nstep:3263/4500 train_loss:8.2269 train_time:602244ms step_avg:185.13ms\nstep:3264/4500 train_loss:8.2436 train_time:602428ms step_avg:185.13ms\nstep:3265/4500 train_loss:8.0771 train_time:602615ms step_avg:185.14ms\nstep:3266/4500 train_loss:8.1162 train_time:602797ms step_avg:185.13ms\nstep:3267/4500 train_loss:8.0971 train_time:602981ms step_avg:185.13ms\nstep:3268/4500 train_loss:7.9944 train_time:603166ms step_avg:185.13ms\nstep:3269/4500 train_loss:8.0446 train_time:603352ms step_avg:185.13ms\nstep:3270/4500 train_loss:8.0550 train_time:603538ms step_avg:185.13ms\nstep:3271/4500 train_loss:8.0469 train_time:603725ms step_avg:185.13ms\nstep:3272/4500 train_loss:8.0920 train_time:603909ms step_avg:185.13ms\nstep:3273/4500 train_loss:8.0400 train_time:604093ms step_avg:185.13ms\nstep:3274/4500 train_loss:8.1107 train_time:604277ms step_avg:185.13ms\nstep:3275/4500 train_loss:8.1385 train_time:604462ms step_avg:185.13ms\nstep:3276/4500 train_loss:8.0316 train_time:604649ms step_avg:185.13ms\nstep:3277/4500 train_loss:8.0870 train_time:604834ms step_avg:185.13ms\nstep:3278/4500 train_loss:8.1620 train_time:605020ms step_avg:185.13ms\nstep:3279/4500 train_loss:8.0245 train_time:605207ms step_avg:185.14ms\nstep:3280/4500 train_loss:8.1897 train_time:605391ms step_avg:185.13ms\nstep:3281/4500 train_loss:7.9378 train_time:605574ms step_avg:185.13ms\nstep:3282/4500 train_loss:8.2344 train_time:605758ms step_avg:185.13ms\nstep:3283/4500 train_loss:8.2867 train_time:605944ms step_avg:185.13ms\nstep:3284/4500 train_loss:7.9428 train_time:606129ms step_avg:185.13ms\nstep:3285/4500 train_loss:8.0743 train_time:606315ms step_avg:185.13ms\nstep:3286/4500 train_loss:8.1420 train_time:606501ms step_avg:185.13ms\nstep:3287/4500 train_loss:8.0506 train_time:606688ms step_avg:185.14ms\nstep:3288/4500 train_loss:8.1169 train_time:606873ms step_avg:185.14ms\nstep:3289/4500 train_loss:8.1305 train_time:607060ms step_avg:185.14ms\nstep:3290/4500 train_loss:8.0872 train_time:607245ms step_avg:185.14ms\nstep:3291/4500 train_loss:8.0287 train_time:607429ms step_avg:185.14ms\nstep:3292/4500 train_loss:8.1400 train_time:607616ms step_avg:185.14ms\nstep:3293/4500 train_loss:8.0093 train_time:607799ms step_avg:185.14ms\nstep:3294/4500 train_loss:8.0407 train_time:607985ms step_avg:185.14ms\nstep:3295/4500 train_loss:8.1053 train_time:608171ms step_avg:185.14ms\nstep:3296/4500 train_loss:7.9859 train_time:608357ms step_avg:185.14ms\nstep:3297/4500 train_loss:8.1440 train_time:608540ms step_avg:185.14ms\nstep:3298/4500 train_loss:8.0776 train_time:608728ms step_avg:185.14ms\nstep:3299/4500 train_loss:8.1308 train_time:608912ms step_avg:185.14ms\nstep:3300/4500 train_loss:8.0786 train_time:609095ms step_avg:185.14ms\nstep:3301/4500 train_loss:7.9865 train_time:609281ms step_avg:185.14ms\nstep:3302/4500 train_loss:8.0394 train_time:609466ms step_avg:185.14ms\nstep:3303/4500 train_loss:8.0503 train_time:609649ms step_avg:185.13ms\nstep:3304/4500 train_loss:8.1242 train_time:609834ms step_avg:185.13ms\nstep:3305/4500 train_loss:7.9806 train_time:610021ms step_avg:185.14ms\nstep:3306/4500 train_loss:8.0336 train_time:610207ms step_avg:185.14ms\nstep:3307/4500 train_loss:8.1699 train_time:610392ms step_avg:185.14ms\nstep:3308/4500 train_loss:8.1159 train_time:610578ms step_avg:185.14ms\nstep:3309/4500 train_loss:8.1005 train_time:610761ms step_avg:185.14ms\nstep:3310/4500 train_loss:8.1031 train_time:610949ms step_avg:185.14ms\nstep:3311/4500 train_loss:8.1515 train_time:611131ms step_avg:185.14ms\nstep:3312/4500 train_loss:8.1672 train_time:611315ms step_avg:185.13ms\nstep:3313/4500 train_loss:8.1622 train_time:611501ms step_avg:185.13ms\nstep:3314/4500 train_loss:8.2284 train_time:611684ms step_avg:185.13ms\nstep:3315/4500 train_loss:8.1524 train_time:611872ms step_avg:185.14ms\nstep:3316/4500 train_loss:8.1638 train_time:612055ms step_avg:185.13ms\nstep:3317/4500 train_loss:8.1402 train_time:612239ms step_avg:185.13ms\nstep:3318/4500 train_loss:8.1000 train_time:612430ms step_avg:185.14ms\nstep:3319/4500 train_loss:8.1903 train_time:612611ms step_avg:185.13ms\nstep:3320/4500 train_loss:8.1958 train_time:612795ms step_avg:185.13ms\nstep:3321/4500 train_loss:8.1452 train_time:612982ms step_avg:185.14ms\nstep:3322/4500 train_loss:8.2206 train_time:613166ms step_avg:185.13ms\nstep:3323/4500 train_loss:8.1604 train_time:613354ms step_avg:185.14ms\nstep:3324/4500 train_loss:8.0207 train_time:613539ms step_avg:185.14ms\nstep:3325/4500 train_loss:8.1039 train_time:613727ms step_avg:185.14ms\nstep:3326/4500 train_loss:8.1198 train_time:613912ms step_avg:185.14ms\nstep:3327/4500 train_loss:8.1829 train_time:614096ms step_avg:185.14ms\nstep:3328/4500 train_loss:8.1549 train_time:614280ms step_avg:185.14ms\nstep:3329/4500 train_loss:8.1005 train_time:614465ms step_avg:185.14ms\nstep:3330/4500 train_loss:8.0241 train_time:614650ms step_avg:185.14ms\nstep:3331/4500 train_loss:8.2481 train_time:614834ms step_avg:185.14ms\nstep:3332/4500 train_loss:8.0919 train_time:615019ms step_avg:185.14ms\nstep:3333/4500 train_loss:8.1517 train_time:615212ms step_avg:185.14ms\nstep:3334/4500 train_loss:8.1494 train_time:615391ms step_avg:185.14ms\nstep:3335/4500 train_loss:8.1894 train_time:615576ms step_avg:185.14ms\nstep:3336/4500 train_loss:8.1280 train_time:615762ms step_avg:185.14ms\nstep:3337/4500 train_loss:8.1769 train_time:615948ms step_avg:185.14ms\nstep:3338/4500 train_loss:8.1059 train_time:616132ms step_avg:185.14ms\nstep:3339/4500 train_loss:8.2289 train_time:616317ms step_avg:185.14ms\nstep:3340/4500 train_loss:8.1599 train_time:616501ms step_avg:185.14ms\nstep:3341/4500 train_loss:8.1497 train_time:616689ms step_avg:185.14ms\nstep:3342/4500 train_loss:8.2148 train_time:616872ms step_avg:185.14ms\nstep:3343/4500 train_loss:8.1428 train_time:617054ms step_avg:185.13ms\nstep:3344/4500 train_loss:8.1393 train_time:617238ms step_avg:185.13ms\nstep:3345/4500 train_loss:8.0635 train_time:617423ms step_avg:185.13ms\nstep:3346/4500 train_loss:8.1102 train_time:617610ms step_avg:185.13ms\nstep:3347/4500 train_loss:8.1425 train_time:617796ms step_avg:185.14ms\nstep:3348/4500 train_loss:7.9965 train_time:617983ms step_avg:185.14ms\nstep:3349/4500 train_loss:8.0667 train_time:618168ms step_avg:185.14ms\nstep:3350/4500 train_loss:8.3229 train_time:618354ms step_avg:185.14ms\nstep:3351/4500 train_loss:8.1040 train_time:618539ms step_avg:185.14ms\nstep:3352/4500 train_loss:8.0873 train_time:618727ms step_avg:185.14ms\nstep:3353/4500 train_loss:7.8938 train_time:618912ms step_avg:185.14ms\nstep:3354/4500 train_loss:8.0928 train_time:619096ms step_avg:185.14ms\nstep:3355/4500 train_loss:8.0649 train_time:619280ms step_avg:185.14ms\nstep:3356/4500 train_loss:8.0648 train_time:619467ms step_avg:185.14ms\nstep:3357/4500 train_loss:8.1205 train_time:619654ms step_avg:185.14ms\nstep:3358/4500 train_loss:8.1349 train_time:619838ms step_avg:185.14ms\nstep:3359/4500 train_loss:8.1443 train_time:620026ms step_avg:185.14ms\nstep:3360/4500 train_loss:8.0749 train_time:620209ms step_avg:185.14ms\nstep:3361/4500 train_loss:8.0627 train_time:620394ms step_avg:185.14ms\nstep:3362/4500 train_loss:8.1186 train_time:620578ms step_avg:185.14ms\nstep:3363/4500 train_loss:8.0906 train_time:620766ms step_avg:185.14ms\nstep:3364/4500 train_loss:8.1063 train_time:620949ms step_avg:185.14ms\nstep:3365/4500 train_loss:8.0914 train_time:621134ms step_avg:185.14ms\nstep:3366/4500 train_loss:8.0949 train_time:621320ms step_avg:185.14ms\nstep:3367/4500 train_loss:8.1588 train_time:621504ms step_avg:185.14ms\nstep:3368/4500 train_loss:8.1176 train_time:621689ms step_avg:185.14ms\nstep:3369/4500 train_loss:8.1127 train_time:621875ms step_avg:185.14ms\nstep:3370/4500 train_loss:8.1898 train_time:622060ms step_avg:185.14ms\nstep:3371/4500 train_loss:8.1994 train_time:622248ms step_avg:185.14ms\nstep:3372/4500 train_loss:7.9739 train_time:622433ms step_avg:185.14ms\nstep:3373/4500 train_loss:8.0576 train_time:622618ms step_avg:185.14ms\nstep:3374/4500 train_loss:8.0899 train_time:622802ms step_avg:185.14ms\nstep:3375/4500 train_loss:8.0929 train_time:622988ms step_avg:185.14ms\nstep:3376/4500 train_loss:8.0126 train_time:623170ms step_avg:185.14ms\nstep:3377/4500 train_loss:8.1090 train_time:623354ms step_avg:185.14ms\nstep:3378/4500 train_loss:8.1114 train_time:623542ms step_avg:185.14ms\nstep:3379/4500 train_loss:8.2553 train_time:623727ms step_avg:185.14ms\nstep:3380/4500 train_loss:8.1995 train_time:623913ms step_avg:185.14ms\nstep:3381/4500 train_loss:7.9632 train_time:624100ms step_avg:185.14ms\nstep:3382/4500 train_loss:7.9988 train_time:624286ms step_avg:185.14ms\nstep:3383/4500 train_loss:8.1204 train_time:624469ms step_avg:185.14ms\nstep:3384/4500 train_loss:8.0406 train_time:624654ms step_avg:185.14ms\nstep:3385/4500 train_loss:8.1784 train_time:624839ms step_avg:185.14ms\nstep:3386/4500 train_loss:8.1421 train_time:625024ms step_avg:185.14ms\nstep:3387/4500 train_loss:8.0699 train_time:625209ms step_avg:185.14ms\nstep:3388/4500 train_loss:8.1376 train_time:625393ms step_avg:185.14ms\nstep:3389/4500 train_loss:8.1380 train_time:625577ms step_avg:185.14ms\nstep:3390/4500 train_loss:8.1051 train_time:625763ms step_avg:185.14ms\nstep:3391/4500 train_loss:8.1521 train_time:625947ms step_avg:185.14ms\nstep:3392/4500 train_loss:8.1391 train_time:626131ms step_avg:185.14ms\nstep:3393/4500 train_loss:8.1445 train_time:626313ms step_avg:185.14ms\nstep:3394/4500 train_loss:8.1036 train_time:626497ms step_avg:185.14ms\nstep:3395/4500 train_loss:8.2239 train_time:626683ms step_avg:185.14ms\nstep:3396/4500 train_loss:8.2400 train_time:626870ms step_avg:185.14ms\nstep:3397/4500 train_loss:8.2541 train_time:627053ms step_avg:185.14ms\nstep:3398/4500 train_loss:8.1098 train_time:627237ms step_avg:185.13ms\nstep:3399/4500 train_loss:8.1652 train_time:627423ms step_avg:185.14ms\nstep:3400/4500 train_loss:8.2292 train_time:627611ms step_avg:185.14ms\nstep:3401/4500 train_loss:8.1965 train_time:627794ms step_avg:185.14ms\nstep:3402/4500 train_loss:8.0598 train_time:627980ms step_avg:185.14ms\nstep:3403/4500 train_loss:8.0814 train_time:628168ms step_avg:185.14ms\nstep:3404/4500 train_loss:8.1355 train_time:628352ms step_avg:185.14ms\nstep:3405/4500 train_loss:8.0627 train_time:628535ms step_avg:185.14ms\nstep:3406/4500 train_loss:8.1432 train_time:628721ms step_avg:185.14ms\nstep:3407/4500 train_loss:8.2427 train_time:628906ms step_avg:185.14ms\nstep:3408/4500 train_loss:8.0873 train_time:629091ms step_avg:185.14ms\nstep:3409/4500 train_loss:8.1761 train_time:629275ms step_avg:185.14ms\nstep:3410/4500 train_loss:8.3306 train_time:629461ms step_avg:185.14ms\nstep:3411/4500 train_loss:8.1171 train_time:629645ms step_avg:185.14ms\nstep:3412/4500 train_loss:7.9937 train_time:629828ms step_avg:185.13ms\nstep:3413/4500 train_loss:8.1524 train_time:630011ms step_avg:185.13ms\nstep:3414/4500 train_loss:8.0517 train_time:630196ms step_avg:185.13ms\nstep:3415/4500 train_loss:8.1592 train_time:630380ms step_avg:185.13ms\nstep:3416/4500 train_loss:8.1739 train_time:630566ms step_avg:185.13ms\nstep:3417/4500 train_loss:8.0756 train_time:630751ms step_avg:185.13ms\nstep:3418/4500 train_loss:8.1276 train_time:630937ms step_avg:185.13ms\nstep:3419/4500 train_loss:8.1338 train_time:631123ms step_avg:185.13ms\nstep:3420/4500 train_loss:7.9863 train_time:631309ms step_avg:185.13ms\nstep:3421/4500 train_loss:8.1265 train_time:631494ms step_avg:185.13ms\nstep:3422/4500 train_loss:8.2089 train_time:631680ms step_avg:185.13ms\nstep:3423/4500 train_loss:8.1807 train_time:631867ms step_avg:185.14ms\nstep:3424/4500 train_loss:7.9110 train_time:632052ms step_avg:185.14ms\nstep:3425/4500 train_loss:8.1450 train_time:632238ms step_avg:185.14ms\nstep:3426/4500 train_loss:8.1799 train_time:632424ms step_avg:185.14ms\nstep:3427/4500 train_loss:7.9987 train_time:632607ms step_avg:185.14ms\nstep:3428/4500 train_loss:8.0540 train_time:632794ms step_avg:185.14ms\nstep:3429/4500 train_loss:8.1817 train_time:632977ms step_avg:185.14ms\nstep:3430/4500 train_loss:8.1189 train_time:633162ms step_avg:185.14ms\nstep:3431/4500 train_loss:8.0662 train_time:633347ms step_avg:185.14ms\nstep:3432/4500 train_loss:8.1563 train_time:633535ms step_avg:185.14ms\nstep:3433/4500 train_loss:8.0773 train_time:633720ms step_avg:185.14ms\nstep:3434/4500 train_loss:8.0621 train_time:633905ms step_avg:185.14ms\nstep:3435/4500 train_loss:8.0749 train_time:634090ms step_avg:185.14ms\nstep:3436/4500 train_loss:8.1594 train_time:634273ms step_avg:185.14ms\nstep:3437/4500 train_loss:8.2437 train_time:634458ms step_avg:185.14ms\nstep:3438/4500 train_loss:8.1049 train_time:634642ms step_avg:185.13ms\nstep:3439/4500 train_loss:8.1240 train_time:634826ms step_avg:185.13ms\nstep:3440/4500 train_loss:8.0980 train_time:635010ms step_avg:185.13ms\nstep:3441/4500 train_loss:8.3492 train_time:635195ms step_avg:185.13ms\nstep:3442/4500 train_loss:8.1453 train_time:635380ms step_avg:185.13ms\nstep:3443/4500 train_loss:8.1149 train_time:635566ms step_avg:185.13ms\nstep:3444/4500 train_loss:8.1924 train_time:635753ms step_avg:185.13ms\nstep:3445/4500 train_loss:8.1542 train_time:635936ms step_avg:185.13ms\nstep:3446/4500 train_loss:8.1599 train_time:636121ms step_avg:185.13ms\nstep:3447/4500 train_loss:8.1881 train_time:636309ms step_avg:185.13ms\nstep:3448/4500 train_loss:8.0710 train_time:636494ms step_avg:185.13ms\nstep:3449/4500 train_loss:8.2214 train_time:636680ms step_avg:185.14ms\nstep:3450/4500 train_loss:8.2141 train_time:636865ms step_avg:185.14ms\nstep:3451/4500 train_loss:8.1474 train_time:637050ms step_avg:185.14ms\nstep:3452/4500 train_loss:8.0512 train_time:637232ms step_avg:185.13ms\nstep:3453/4500 train_loss:8.2262 train_time:637415ms step_avg:185.13ms\nstep:3454/4500 train_loss:8.0367 train_time:637601ms step_avg:185.13ms\nstep:3455/4500 train_loss:8.1934 train_time:637786ms step_avg:185.13ms\nstep:3456/4500 train_loss:8.2301 train_time:637973ms step_avg:185.13ms\nstep:3457/4500 train_loss:8.1110 train_time:638157ms step_avg:185.13ms\nstep:3458/4500 train_loss:8.0503 train_time:638342ms step_avg:185.13ms\nstep:3459/4500 train_loss:8.1309 train_time:638526ms step_avg:185.13ms\nstep:3460/4500 train_loss:8.2623 train_time:638710ms step_avg:185.13ms\nstep:3461/4500 train_loss:8.0230 train_time:638895ms step_avg:185.13ms\nstep:3462/4500 train_loss:8.2328 train_time:639080ms step_avg:185.13ms\nstep:3463/4500 train_loss:8.1763 train_time:639266ms step_avg:185.13ms\nstep:3464/4500 train_loss:8.1320 train_time:639451ms step_avg:185.13ms\nstep:3465/4500 train_loss:8.2049 train_time:639635ms step_avg:185.13ms\nstep:3466/4500 train_loss:8.1917 train_time:639819ms step_avg:185.13ms\nstep:3467/4500 train_loss:8.0603 train_time:640002ms step_avg:185.13ms\nstep:3468/4500 train_loss:8.1228 train_time:640187ms step_avg:185.13ms\nstep:3469/4500 train_loss:7.9930 train_time:640370ms step_avg:185.13ms\nstep:3470/4500 train_loss:8.0318 train_time:640556ms step_avg:185.13ms\nstep:3471/4500 train_loss:8.0288 train_time:640740ms step_avg:185.13ms\nstep:3472/4500 train_loss:8.2249 train_time:640926ms step_avg:185.13ms\nstep:3473/4500 train_loss:8.1173 train_time:641111ms step_avg:185.13ms\nstep:3474/4500 train_loss:8.0975 train_time:641296ms step_avg:185.13ms\nstep:3475/4500 train_loss:8.0988 train_time:641480ms step_avg:185.13ms\nstep:3476/4500 train_loss:8.1440 train_time:641666ms step_avg:185.13ms\nstep:3477/4500 train_loss:8.0443 train_time:641849ms step_avg:185.13ms\nstep:3478/4500 train_loss:8.0813 train_time:642033ms step_avg:185.13ms\nstep:3479/4500 train_loss:8.1243 train_time:642218ms step_avg:185.13ms\nstep:3480/4500 train_loss:7.9734 train_time:642407ms step_avg:185.13ms\nstep:3481/4500 train_loss:8.1762 train_time:642594ms step_avg:185.13ms\nstep:3482/4500 train_loss:8.1026 train_time:642781ms step_avg:185.13ms\nstep:3483/4500 train_loss:8.1676 train_time:642964ms step_avg:185.13ms\nstep:3484/4500 train_loss:8.2770 train_time:643150ms step_avg:185.13ms\nstep:3485/4500 train_loss:8.2449 train_time:643334ms step_avg:185.13ms\nstep:3486/4500 train_loss:8.0461 train_time:643522ms step_avg:185.13ms\nstep:3487/4500 train_loss:7.9170 train_time:643711ms step_avg:185.13ms\nstep:3488/4500 train_loss:8.1539 train_time:643896ms step_avg:185.13ms\nstep:3489/4500 train_loss:8.0255 train_time:644080ms step_avg:185.13ms\nstep:3490/4500 train_loss:8.0517 train_time:644266ms step_avg:185.13ms\nstep:3491/4500 train_loss:8.0812 train_time:644451ms step_avg:185.13ms\nstep:3492/4500 train_loss:8.1939 train_time:644636ms step_avg:185.13ms\nstep:3493/4500 train_loss:8.1119 train_time:644821ms step_avg:185.13ms\nstep:3494/4500 train_loss:8.1771 train_time:645005ms step_avg:185.13ms\nstep:3495/4500 train_loss:8.1083 train_time:645192ms step_avg:185.13ms\nstep:3496/4500 train_loss:8.1568 train_time:645377ms step_avg:185.13ms\nstep:3497/4500 train_loss:8.0677 train_time:645562ms step_avg:185.13ms\nstep:3498/4500 train_loss:8.0958 train_time:645750ms step_avg:185.13ms\nstep:3499/4500 train_loss:8.0549 train_time:645937ms step_avg:185.14ms\nstep:3500/4500 train_loss:8.0293 train_time:646123ms step_avg:185.14ms\nstep:3501/4500 train_loss:8.1108 train_time:646307ms step_avg:185.14ms\nstep:3502/4500 train_loss:7.6203 train_time:646491ms step_avg:185.13ms\nstep:3503/4500 train_loss:8.1957 train_time:646678ms step_avg:185.14ms\nstep:3504/4500 train_loss:8.1148 train_time:646862ms step_avg:185.14ms\nstep:3505/4500 train_loss:8.0790 train_time:647048ms step_avg:185.14ms\nstep:3506/4500 train_loss:8.0810 train_time:647230ms step_avg:185.13ms\nstep:3507/4500 train_loss:8.0536 train_time:647414ms step_avg:185.13ms\nstep:3508/4500 train_loss:8.1318 train_time:647597ms step_avg:185.13ms\nstep:3509/4500 train_loss:8.5087 train_time:647783ms step_avg:185.13ms\nstep:3510/4500 train_loss:8.1964 train_time:647967ms step_avg:185.13ms\nstep:3511/4500 train_loss:7.9655 train_time:648152ms step_avg:185.13ms\nstep:3512/4500 train_loss:8.0849 train_time:648337ms step_avg:185.13ms\nstep:3513/4500 train_loss:8.1354 train_time:648523ms step_avg:185.13ms\nstep:3514/4500 train_loss:8.1416 train_time:648709ms step_avg:185.13ms\nstep:3515/4500 train_loss:8.1117 train_time:648893ms step_avg:185.13ms\nstep:3516/4500 train_loss:8.1270 train_time:649077ms step_avg:185.13ms\nstep:3517/4500 train_loss:8.0647 train_time:649262ms step_avg:185.13ms\nstep:3518/4500 train_loss:8.0394 train_time:649449ms step_avg:185.13ms\nstep:3519/4500 train_loss:8.0770 train_time:649633ms step_avg:185.13ms\nstep:3520/4500 train_loss:8.3264 train_time:649819ms step_avg:185.13ms\nstep:3521/4500 train_loss:8.0727 train_time:650005ms step_avg:185.13ms\nstep:3522/4500 train_loss:8.2182 train_time:650190ms step_avg:185.13ms\nstep:3523/4500 train_loss:8.1744 train_time:650374ms step_avg:185.13ms\nstep:3524/4500 train_loss:8.0577 train_time:650560ms step_avg:185.13ms\nstep:3525/4500 train_loss:8.1189 train_time:650746ms step_avg:185.13ms\nstep:3526/4500 train_loss:8.1042 train_time:650932ms step_avg:185.13ms\nstep:3527/4500 train_loss:8.0998 train_time:651117ms step_avg:185.13ms\nstep:3528/4500 train_loss:8.1111 train_time:651304ms step_avg:185.13ms\nstep:3529/4500 train_loss:8.0311 train_time:651488ms step_avg:185.13ms\nstep:3530/4500 train_loss:8.1508 train_time:651672ms step_avg:185.13ms\nstep:3531/4500 train_loss:8.1251 train_time:651857ms step_avg:185.13ms\nstep:3532/4500 train_loss:8.0498 train_time:652043ms step_avg:185.13ms\nstep:3533/4500 train_loss:8.1678 train_time:652232ms step_avg:185.14ms\nstep:3534/4500 train_loss:8.2078 train_time:652417ms step_avg:185.14ms\nstep:3535/4500 train_loss:8.0302 train_time:652601ms step_avg:185.13ms\nstep:3536/4500 train_loss:8.0318 train_time:652785ms step_avg:185.13ms\nstep:3537/4500 train_loss:8.0053 train_time:652968ms step_avg:185.13ms\nstep:3538/4500 train_loss:8.2494 train_time:653153ms step_avg:185.13ms\nstep:3539/4500 train_loss:8.0712 train_time:653342ms step_avg:185.14ms\nstep:3540/4500 train_loss:8.0644 train_time:653527ms step_avg:185.14ms\nstep:3541/4500 train_loss:8.1889 train_time:653713ms step_avg:185.14ms\nstep:3542/4500 train_loss:8.2106 train_time:653896ms step_avg:185.13ms\nstep:3543/4500 train_loss:8.0496 train_time:654080ms step_avg:185.13ms\nstep:3544/4500 train_loss:8.0587 train_time:654265ms step_avg:185.13ms\nstep:3545/4500 train_loss:8.0363 train_time:654449ms step_avg:185.13ms\nstep:3546/4500 train_loss:8.1017 train_time:654635ms step_avg:185.13ms\nstep:3547/4500 train_loss:8.0577 train_time:654821ms step_avg:185.13ms\nstep:3548/4500 train_loss:8.0631 train_time:655008ms step_avg:185.14ms\nstep:3549/4500 train_loss:8.2430 train_time:655192ms step_avg:185.13ms\nstep:3550/4500 train_loss:8.1752 train_time:655376ms step_avg:185.13ms\nstep:3551/4500 train_loss:8.1749 train_time:655560ms step_avg:185.13ms\nstep:3552/4500 train_loss:8.1143 train_time:655744ms step_avg:185.13ms\nstep:3553/4500 train_loss:8.1425 train_time:655927ms step_avg:185.13ms\nstep:3554/4500 train_loss:8.2482 train_time:656110ms step_avg:185.13ms\nstep:3555/4500 train_loss:8.0754 train_time:656295ms step_avg:185.13ms\nstep:3556/4500 train_loss:8.2116 train_time:656479ms step_avg:185.13ms\nstep:3557/4500 train_loss:8.1263 train_time:656663ms step_avg:185.13ms\nstep:3558/4500 train_loss:8.0233 train_time:656847ms step_avg:185.13ms\nstep:3559/4500 train_loss:8.0596 train_time:657034ms step_avg:185.13ms\nstep:3560/4500 train_loss:8.1517 train_time:657220ms step_avg:185.13ms\nstep:3561/4500 train_loss:8.1050 train_time:657405ms step_avg:185.13ms\nstep:3562/4500 train_loss:8.0739 train_time:657593ms step_avg:185.13ms\nstep:3563/4500 train_loss:8.0129 train_time:657778ms step_avg:185.13ms\nstep:3564/4500 train_loss:8.0649 train_time:657962ms step_avg:185.13ms\nstep:3565/4500 train_loss:8.0519 train_time:658146ms step_avg:185.13ms\nstep:3566/4500 train_loss:8.0892 train_time:658334ms step_avg:185.13ms\nstep:3567/4500 train_loss:8.2448 train_time:658518ms step_avg:185.13ms\nstep:3568/4500 train_loss:8.1179 train_time:658707ms step_avg:185.13ms\nstep:3569/4500 train_loss:8.1558 train_time:658895ms step_avg:185.13ms\nstep:3570/4500 train_loss:8.0746 train_time:659080ms step_avg:185.13ms\nstep:3571/4500 train_loss:8.0048 train_time:659266ms step_avg:185.13ms\nstep:3572/4500 train_loss:8.0733 train_time:659454ms step_avg:185.14ms\nstep:3573/4500 train_loss:8.1878 train_time:659639ms step_avg:185.14ms\nstep:3574/4500 train_loss:8.1163 train_time:659824ms step_avg:185.14ms\nstep:3575/4500 train_loss:8.1134 train_time:660012ms step_avg:185.14ms\nstep:3576/4500 train_loss:8.0714 train_time:660197ms step_avg:185.14ms\nstep:3577/4500 train_loss:8.0258 train_time:660382ms step_avg:185.14ms\nstep:3578/4500 train_loss:8.1342 train_time:660567ms step_avg:185.14ms\nstep:3579/4500 train_loss:8.0772 train_time:660751ms step_avg:185.14ms\nstep:3580/4500 train_loss:8.0519 train_time:660936ms step_avg:185.14ms\nstep:3581/4500 train_loss:8.0955 train_time:661123ms step_avg:185.14ms\nstep:3582/4500 train_loss:8.2867 train_time:661306ms step_avg:185.14ms\nstep:3583/4500 train_loss:8.1842 train_time:661490ms step_avg:185.14ms\nstep:3584/4500 train_loss:7.9710 train_time:661674ms step_avg:185.14ms\nstep:3585/4500 train_loss:8.0970 train_time:661857ms step_avg:185.13ms\nstep:3586/4500 train_loss:8.2355 train_time:662041ms step_avg:185.13ms\nstep:3587/4500 train_loss:8.0535 train_time:662228ms step_avg:185.14ms\nstep:3588/4500 train_loss:8.0581 train_time:662414ms step_avg:185.14ms\nstep:3589/4500 train_loss:8.0854 train_time:662599ms step_avg:185.14ms\nstep:3590/4500 train_loss:8.1561 train_time:662786ms step_avg:185.14ms\nstep:3591/4500 train_loss:7.9854 train_time:662970ms step_avg:185.14ms\nstep:3592/4500 train_loss:8.2113 train_time:663154ms step_avg:185.14ms\nstep:3593/4500 train_loss:8.1401 train_time:663339ms step_avg:185.14ms\nstep:3594/4500 train_loss:8.0579 train_time:663526ms step_avg:185.14ms\nstep:3595/4500 train_loss:8.2143 train_time:663711ms step_avg:185.14ms\nstep:3596/4500 train_loss:8.0505 train_time:663895ms step_avg:185.14ms\nstep:3597/4500 train_loss:8.2130 train_time:664079ms step_avg:185.13ms\nstep:3598/4500 train_loss:7.9341 train_time:664265ms step_avg:185.14ms\nstep:3599/4500 train_loss:8.0082 train_time:664449ms step_avg:185.13ms\nstep:3600/4500 train_loss:8.0513 train_time:664634ms step_avg:185.13ms\nstep:3601/4500 train_loss:8.5786 train_time:664821ms step_avg:185.14ms\nstep:3602/4500 train_loss:8.0945 train_time:665009ms step_avg:185.14ms\nstep:3603/4500 train_loss:8.1433 train_time:665195ms step_avg:185.14ms\nstep:3604/4500 train_loss:8.1365 train_time:665382ms step_avg:185.14ms\nstep:3605/4500 train_loss:8.0129 train_time:665568ms step_avg:185.14ms\nstep:3606/4500 train_loss:8.1150 train_time:665751ms step_avg:185.14ms\nstep:3607/4500 train_loss:8.1233 train_time:665936ms step_avg:185.14ms\nstep:3608/4500 train_loss:8.0815 train_time:666119ms step_avg:185.14ms\nstep:3609/4500 train_loss:8.0476 train_time:666306ms step_avg:185.14ms\nstep:3610/4500 train_loss:8.0680 train_time:666492ms step_avg:185.14ms\nstep:3611/4500 train_loss:8.0474 train_time:666675ms step_avg:185.14ms\nstep:3612/4500 train_loss:8.0965 train_time:666859ms step_avg:185.14ms\nstep:3613/4500 train_loss:8.1317 train_time:667044ms step_avg:185.14ms\nstep:3614/4500 train_loss:8.1712 train_time:667231ms step_avg:185.14ms\nstep:3615/4500 train_loss:8.1915 train_time:667415ms step_avg:185.14ms\nstep:3616/4500 train_loss:8.2536 train_time:667600ms step_avg:185.14ms\nstep:3617/4500 train_loss:8.0937 train_time:667786ms step_avg:185.14ms\nstep:3618/4500 train_loss:8.0532 train_time:667970ms step_avg:185.14ms\nstep:3619/4500 train_loss:8.1241 train_time:668156ms step_avg:185.14ms\nstep:3620/4500 train_loss:8.0751 train_time:668346ms step_avg:185.14ms\nstep:3621/4500 train_loss:8.1080 train_time:668527ms step_avg:185.14ms\nstep:3622/4500 train_loss:8.1521 train_time:668710ms step_avg:185.14ms\nstep:3623/4500 train_loss:8.0796 train_time:668895ms step_avg:185.14ms\nstep:3624/4500 train_loss:8.2166 train_time:669080ms step_avg:185.14ms\nstep:3625/4500 train_loss:8.0947 train_time:669264ms step_avg:185.14ms\nstep:3626/4500 train_loss:8.0781 train_time:669449ms step_avg:185.14ms\nstep:3627/4500 train_loss:8.0352 train_time:669635ms step_avg:185.14ms\nstep:3628/4500 train_loss:8.0456 train_time:669821ms step_avg:185.14ms\nstep:3629/4500 train_loss:8.0786 train_time:670008ms step_avg:185.14ms\nstep:3630/4500 train_loss:8.3426 train_time:670195ms step_avg:185.14ms\nstep:3631/4500 train_loss:8.0950 train_time:670379ms step_avg:185.14ms\nstep:3632/4500 train_loss:8.1278 train_time:670568ms step_avg:185.14ms\nstep:3633/4500 train_loss:8.0903 train_time:670751ms step_avg:185.14ms\nstep:3634/4500 train_loss:8.1136 train_time:670939ms step_avg:185.14ms\nstep:3635/4500 train_loss:8.2431 train_time:671125ms step_avg:185.14ms\nstep:3636/4500 train_loss:8.0529 train_time:671307ms step_avg:185.14ms\nstep:3637/4500 train_loss:8.1589 train_time:671491ms step_avg:185.14ms\nstep:3638/4500 train_loss:8.0804 train_time:671676ms step_avg:185.14ms\nstep:3639/4500 train_loss:8.0710 train_time:671861ms step_avg:185.14ms\nstep:3640/4500 train_loss:8.0473 train_time:672045ms step_avg:185.14ms\nstep:3641/4500 train_loss:8.1235 train_time:672229ms step_avg:185.14ms\nstep:3642/4500 train_loss:8.0688 train_time:672414ms step_avg:185.14ms\nstep:3643/4500 train_loss:8.1413 train_time:672599ms step_avg:185.14ms\nstep:3644/4500 train_loss:8.0913 train_time:672783ms step_avg:185.14ms\nstep:3645/4500 train_loss:8.2128 train_time:672965ms step_avg:185.13ms\nstep:3646/4500 train_loss:8.1949 train_time:673152ms step_avg:185.14ms\nstep:3647/4500 train_loss:8.0427 train_time:673336ms step_avg:185.13ms\nstep:3648/4500 train_loss:8.1507 train_time:673521ms step_avg:185.13ms\nstep:3649/4500 train_loss:8.1157 train_time:673707ms step_avg:185.14ms\nstep:3650/4500 train_loss:8.0843 train_time:673893ms step_avg:185.14ms\nstep:3651/4500 train_loss:8.1006 train_time:674076ms step_avg:185.13ms\nstep:3652/4500 train_loss:8.1553 train_time:674261ms step_avg:185.13ms\nstep:3653/4500 train_loss:8.1200 train_time:674446ms step_avg:185.13ms\nstep:3654/4500 train_loss:8.4497 train_time:674630ms step_avg:185.13ms\nstep:3655/4500 train_loss:8.1048 train_time:674814ms step_avg:185.13ms\nstep:3656/4500 train_loss:8.2311 train_time:675001ms step_avg:185.13ms\nstep:3657/4500 train_loss:8.1456 train_time:675186ms step_avg:185.13ms\nstep:3658/4500 train_loss:8.1612 train_time:675369ms step_avg:185.13ms\nstep:3659/4500 train_loss:8.0895 train_time:675555ms step_avg:185.13ms\nstep:3660/4500 train_loss:8.1286 train_time:675739ms step_avg:185.13ms\nstep:3661/4500 train_loss:8.0283 train_time:675923ms step_avg:185.13ms\nstep:3662/4500 train_loss:8.1412 train_time:676108ms step_avg:185.13ms\nstep:3663/4500 train_loss:8.0355 train_time:676292ms step_avg:185.13ms\nstep:3664/4500 train_loss:8.3172 train_time:676476ms step_avg:185.13ms\nstep:3665/4500 train_loss:8.0936 train_time:676662ms step_avg:185.13ms\nstep:3666/4500 train_loss:8.1691 train_time:676847ms step_avg:185.13ms\nstep:3667/4500 train_loss:8.1073 train_time:677034ms step_avg:185.13ms\nstep:3668/4500 train_loss:8.0612 train_time:677219ms step_avg:185.13ms\nstep:3669/4500 train_loss:8.1071 train_time:677407ms step_avg:185.13ms\nstep:3670/4500 train_loss:8.0825 train_time:677593ms step_avg:185.13ms\nstep:3671/4500 train_loss:8.0528 train_time:677777ms step_avg:185.13ms\nstep:3672/4500 train_loss:8.1042 train_time:677961ms step_avg:185.13ms\nstep:3673/4500 train_loss:8.3354 train_time:678146ms step_avg:185.13ms\nstep:3674/4500 train_loss:8.2048 train_time:678331ms step_avg:185.13ms\nstep:3675/4500 train_loss:8.2493 train_time:678516ms step_avg:185.13ms\nstep:3676/4500 train_loss:8.0958 train_time:678703ms step_avg:185.13ms\nstep:3677/4500 train_loss:8.1539 train_time:678886ms step_avg:185.13ms\nstep:3678/4500 train_loss:8.2331 train_time:679070ms step_avg:185.13ms\nstep:3679/4500 train_loss:8.1007 train_time:679253ms step_avg:185.13ms\nstep:3680/4500 train_loss:8.1071 train_time:679437ms step_avg:185.13ms\nstep:3681/4500 train_loss:7.7726 train_time:679622ms step_avg:185.13ms\nstep:3682/4500 train_loss:8.2196 train_time:679807ms step_avg:185.13ms\nstep:3683/4500 train_loss:7.8967 train_time:679992ms step_avg:185.13ms\nstep:3684/4500 train_loss:8.1093 train_time:680176ms step_avg:185.13ms\nstep:3685/4500 train_loss:8.1074 train_time:680360ms step_avg:185.13ms\nstep:3686/4500 train_loss:8.0676 train_time:680543ms step_avg:185.13ms\nstep:3687/4500 train_loss:8.0604 train_time:680730ms step_avg:185.13ms\nstep:3688/4500 train_loss:8.0004 train_time:680912ms step_avg:185.13ms\nstep:3689/4500 train_loss:8.1199 train_time:681097ms step_avg:185.13ms\nstep:3690/4500 train_loss:8.0368 train_time:681282ms step_avg:185.13ms\nstep:3691/4500 train_loss:8.0801 train_time:681467ms step_avg:185.13ms\nstep:3692/4500 train_loss:7.9665 train_time:681650ms step_avg:185.13ms\nstep:3693/4500 train_loss:8.1415 train_time:681834ms step_avg:185.13ms\nstep:3694/4500 train_loss:8.1571 train_time:682018ms step_avg:185.13ms\nstep:3695/4500 train_loss:8.1676 train_time:682205ms step_avg:185.13ms\nstep:3696/4500 train_loss:8.1630 train_time:682390ms step_avg:185.13ms\nstep:3697/4500 train_loss:8.0693 train_time:682575ms step_avg:185.13ms\nstep:3698/4500 train_loss:8.1574 train_time:682759ms step_avg:185.13ms\nstep:3699/4500 train_loss:8.0322 train_time:682944ms step_avg:185.13ms\nstep:3700/4500 train_loss:7.8887 train_time:683129ms step_avg:185.13ms\nstep:3701/4500 train_loss:8.0820 train_time:683314ms step_avg:185.13ms\nstep:3702/4500 train_loss:8.0767 train_time:683500ms step_avg:185.13ms\nstep:3703/4500 train_loss:8.0352 train_time:683686ms step_avg:185.13ms\nstep:3704/4500 train_loss:7.9959 train_time:683873ms step_avg:185.13ms\nstep:3705/4500 train_loss:8.0715 train_time:684057ms step_avg:185.13ms\nstep:3706/4500 train_loss:8.1221 train_time:684241ms step_avg:185.13ms\nstep:3707/4500 train_loss:8.0837 train_time:684425ms step_avg:185.13ms\nstep:3708/4500 train_loss:8.0252 train_time:684608ms step_avg:185.13ms\nstep:3709/4500 train_loss:7.9508 train_time:684791ms step_avg:185.13ms\nstep:3710/4500 train_loss:8.0369 train_time:684975ms step_avg:185.13ms\nstep:3711/4500 train_loss:8.1698 train_time:685161ms step_avg:185.13ms\nstep:3712/4500 train_loss:8.1392 train_time:685349ms step_avg:185.13ms\nstep:3713/4500 train_loss:8.1814 train_time:685534ms step_avg:185.13ms\nstep:3714/4500 train_loss:8.1248 train_time:685720ms step_avg:185.13ms\nstep:3715/4500 train_loss:8.0998 train_time:685904ms step_avg:185.13ms\nstep:3716/4500 train_loss:8.0876 train_time:686092ms step_avg:185.13ms\nstep:3717/4500 train_loss:8.1293 train_time:686275ms step_avg:185.13ms\nstep:3718/4500 train_loss:8.0735 train_time:686460ms step_avg:185.13ms\nstep:3719/4500 train_loss:8.1270 train_time:686645ms step_avg:185.13ms\nstep:3720/4500 train_loss:7.9790 train_time:686833ms step_avg:185.13ms\nstep:3721/4500 train_loss:8.1462 train_time:687017ms step_avg:185.13ms\nstep:3722/4500 train_loss:8.1600 train_time:687201ms step_avg:185.13ms\nstep:3723/4500 train_loss:8.1694 train_time:687385ms step_avg:185.13ms\nstep:3724/4500 train_loss:8.1257 train_time:687569ms step_avg:185.13ms\nstep:3725/4500 train_loss:8.2114 train_time:687752ms step_avg:185.13ms\nstep:3726/4500 train_loss:8.0061 train_time:687937ms step_avg:185.13ms\nstep:3727/4500 train_loss:8.1038 train_time:688126ms step_avg:185.13ms\nstep:3728/4500 train_loss:8.2986 train_time:688308ms step_avg:185.13ms\nstep:3729/4500 train_loss:8.1458 train_time:688494ms step_avg:185.13ms\nstep:3730/4500 train_loss:8.0600 train_time:688679ms step_avg:185.13ms\nstep:3731/4500 train_loss:8.1142 train_time:688865ms step_avg:185.13ms\nstep:3732/4500 train_loss:8.1658 train_time:689050ms step_avg:185.13ms\nstep:3733/4500 train_loss:8.0140 train_time:689235ms step_avg:185.13ms\nstep:3734/4500 train_loss:8.1468 train_time:689419ms step_avg:185.13ms\nstep:3735/4500 train_loss:8.1205 train_time:689606ms step_avg:185.13ms\nstep:3736/4500 train_loss:8.0318 train_time:689793ms step_avg:185.13ms\nstep:3737/4500 train_loss:8.1152 train_time:689979ms step_avg:185.13ms\nstep:3738/4500 train_loss:8.1223 train_time:690167ms step_avg:185.13ms\nstep:3739/4500 train_loss:8.2053 train_time:690352ms step_avg:185.13ms\nstep:3740/4500 train_loss:8.1446 train_time:690535ms step_avg:185.13ms\nstep:3741/4500 train_loss:8.1301 train_time:690719ms step_avg:185.13ms\nstep:3742/4500 train_loss:8.0823 train_time:690903ms step_avg:185.13ms\nstep:3743/4500 train_loss:8.1901 train_time:691089ms step_avg:185.13ms\nstep:3744/4500 train_loss:8.1569 train_time:691274ms step_avg:185.13ms\nstep:3745/4500 train_loss:8.2518 train_time:691459ms step_avg:185.13ms\nstep:3746/4500 train_loss:8.1700 train_time:691647ms step_avg:185.13ms\nstep:3747/4500 train_loss:8.0175 train_time:691832ms step_avg:185.13ms\nstep:3748/4500 train_loss:8.2234 train_time:692015ms step_avg:185.13ms\nstep:3749/4500 train_loss:8.1734 train_time:692198ms step_avg:185.13ms\nstep:3750/4500 train_loss:8.0548 train_time:692383ms step_avg:185.13ms\nstep:3751/4500 train_loss:8.1971 train_time:692570ms step_avg:185.13ms\nstep:3752/4500 train_loss:8.1195 train_time:692756ms step_avg:185.13ms\nstep:3753/4500 train_loss:8.0255 train_time:692942ms step_avg:185.13ms\nstep:3754/4500 train_loss:8.1288 train_time:693128ms step_avg:185.13ms\nstep:3755/4500 train_loss:8.1631 train_time:693313ms step_avg:185.13ms\nstep:3756/4500 train_loss:8.1439 train_time:693497ms step_avg:185.13ms\nstep:3757/4500 train_loss:8.0012 train_time:693683ms step_avg:185.13ms\nstep:3758/4500 train_loss:8.0735 train_time:693871ms step_avg:185.13ms\nstep:3759/4500 train_loss:8.1501 train_time:694055ms step_avg:185.13ms\nstep:3760/4500 train_loss:8.0822 train_time:694239ms step_avg:185.13ms\nstep:3761/4500 train_loss:8.0455 train_time:694424ms step_avg:185.13ms\nstep:3762/4500 train_loss:8.1904 train_time:694610ms step_avg:185.13ms\nstep:3763/4500 train_loss:8.0110 train_time:694794ms step_avg:185.13ms\nstep:3764/4500 train_loss:8.1234 train_time:694979ms step_avg:185.13ms\nstep:3765/4500 train_loss:8.0852 train_time:695168ms step_avg:185.13ms\nstep:3766/4500 train_loss:8.1211 train_time:695355ms step_avg:185.13ms\nstep:3767/4500 train_loss:8.0464 train_time:695538ms step_avg:185.13ms\nstep:3768/4500 train_loss:7.9379 train_time:695722ms step_avg:185.13ms\nstep:3769/4500 train_loss:8.0897 train_time:695910ms step_avg:185.13ms\nstep:3770/4500 train_loss:8.1094 train_time:696095ms step_avg:185.13ms\nstep:3771/4500 train_loss:8.2664 train_time:696280ms step_avg:185.13ms\nstep:3772/4500 train_loss:8.0640 train_time:696468ms step_avg:185.13ms\nstep:3773/4500 train_loss:8.0119 train_time:696654ms step_avg:185.13ms\nstep:3774/4500 train_loss:8.1085 train_time:696837ms step_avg:185.13ms\nstep:3775/4500 train_loss:8.0430 train_time:697022ms step_avg:185.13ms\nstep:3776/4500 train_loss:8.0429 train_time:697207ms step_avg:185.13ms\nstep:3777/4500 train_loss:8.1685 train_time:697391ms step_avg:185.13ms\nstep:3778/4500 train_loss:8.1285 train_time:697574ms step_avg:185.13ms\nstep:3779/4500 train_loss:8.1157 train_time:697762ms step_avg:185.13ms\nstep:3780/4500 train_loss:8.1408 train_time:697948ms step_avg:185.13ms\nstep:3781/4500 train_loss:8.0090 train_time:698132ms step_avg:185.13ms\nstep:3782/4500 train_loss:8.0234 train_time:698318ms step_avg:185.13ms\nstep:3783/4500 train_loss:8.1008 train_time:698502ms step_avg:185.13ms\nstep:3784/4500 train_loss:8.4333 train_time:698686ms step_avg:185.13ms\nstep:3785/4500 train_loss:8.1243 train_time:698868ms step_avg:185.13ms\nstep:3786/4500 train_loss:8.1629 train_time:699054ms step_avg:185.13ms\nstep:3787/4500 train_loss:8.1388 train_time:699240ms step_avg:185.13ms\nstep:3788/4500 train_loss:8.1836 train_time:699425ms step_avg:185.13ms\nstep:3789/4500 train_loss:7.9790 train_time:699610ms step_avg:185.13ms\nstep:3790/4500 train_loss:8.5159 train_time:699794ms step_avg:185.13ms\nstep:3791/4500 train_loss:8.0704 train_time:699978ms step_avg:185.13ms\nstep:3792/4500 train_loss:8.1511 train_time:700163ms step_avg:185.13ms\nstep:3793/4500 train_loss:8.0935 train_time:700346ms step_avg:185.13ms\nstep:3794/4500 train_loss:8.0723 train_time:700532ms step_avg:185.13ms\nstep:3795/4500 train_loss:8.3597 train_time:700718ms step_avg:185.13ms\nstep:3796/4500 train_loss:8.1365 train_time:700906ms step_avg:185.13ms\nstep:3797/4500 train_loss:8.1671 train_time:701094ms step_avg:185.13ms\nstep:3798/4500 train_loss:8.0720 train_time:701280ms step_avg:185.13ms\nstep:3799/4500 train_loss:8.0818 train_time:701466ms step_avg:185.13ms\nstep:3800/4500 train_loss:7.9365 train_time:701651ms step_avg:185.13ms\nstep:3801/4500 train_loss:8.1706 train_time:701835ms step_avg:185.13ms\nstep:3802/4500 train_loss:8.1624 train_time:702019ms step_avg:185.13ms\nstep:3803/4500 train_loss:8.0634 train_time:702205ms step_avg:185.13ms\nstep:3804/4500 train_loss:8.0344 train_time:702389ms step_avg:185.13ms\nstep:3805/4500 train_loss:8.0135 train_time:702573ms step_avg:185.13ms\nstep:3806/4500 train_loss:8.0805 train_time:702757ms step_avg:185.13ms\nstep:3807/4500 train_loss:7.9430 train_time:702944ms step_avg:185.13ms\nstep:3808/4500 train_loss:8.0788 train_time:703129ms step_avg:185.13ms\nstep:3809/4500 train_loss:8.2917 train_time:703312ms step_avg:185.13ms\nstep:3810/4500 train_loss:8.1260 train_time:703730ms step_avg:185.19ms\nstep:3811/4500 train_loss:8.1972 train_time:703912ms step_avg:185.19ms\nstep:3812/4500 train_loss:8.0839 train_time:704095ms step_avg:185.19ms\nstep:3813/4500 train_loss:8.0713 train_time:704279ms step_avg:185.19ms\nstep:3814/4500 train_loss:8.1068 train_time:704463ms step_avg:185.19ms\nstep:3815/4500 train_loss:8.1311 train_time:704648ms step_avg:185.19ms\nstep:3816/4500 train_loss:7.9963 train_time:704833ms step_avg:185.19ms\nstep:3817/4500 train_loss:8.0668 train_time:705018ms step_avg:185.19ms\nstep:3818/4500 train_loss:7.9740 train_time:705202ms step_avg:185.19ms\nstep:3819/4500 train_loss:8.0768 train_time:705387ms step_avg:185.19ms\nstep:3820/4500 train_loss:8.0011 train_time:705574ms step_avg:185.19ms\nstep:3821/4500 train_loss:8.0541 train_time:705760ms step_avg:185.19ms\nstep:3822/4500 train_loss:8.1838 train_time:705945ms step_avg:185.19ms\nstep:3823/4500 train_loss:8.0580 train_time:706130ms step_avg:185.19ms\nstep:3824/4500 train_loss:7.9470 train_time:706315ms step_avg:185.19ms\nstep:3825/4500 train_loss:7.9450 train_time:706499ms step_avg:185.19ms\nstep:3826/4500 train_loss:8.1257 train_time:706684ms step_avg:185.19ms\nstep:3827/4500 train_loss:7.9946 train_time:706868ms step_avg:185.19ms\nstep:3828/4500 train_loss:8.1003 train_time:707051ms step_avg:185.19ms\nstep:3829/4500 train_loss:8.1685 train_time:707234ms step_avg:185.19ms\nstep:3830/4500 train_loss:8.1182 train_time:707419ms step_avg:185.19ms\nstep:3831/4500 train_loss:8.2815 train_time:707608ms step_avg:185.19ms\nstep:3832/4500 train_loss:7.9852 train_time:707792ms step_avg:185.19ms\nstep:3833/4500 train_loss:8.2308 train_time:707975ms step_avg:185.19ms\nstep:3834/4500 train_loss:8.1068 train_time:708159ms step_avg:185.19ms\nstep:3835/4500 train_loss:7.9914 train_time:708348ms step_avg:185.19ms\nstep:3836/4500 train_loss:8.1296 train_time:708531ms step_avg:185.19ms\nstep:3837/4500 train_loss:8.1514 train_time:708714ms step_avg:185.19ms\nstep:3838/4500 train_loss:8.2721 train_time:708899ms step_avg:185.19ms\nstep:3839/4500 train_loss:8.0889 train_time:709084ms step_avg:185.19ms\nstep:3840/4500 train_loss:8.0705 train_time:709270ms step_avg:185.19ms\nstep:3841/4500 train_loss:8.1685 train_time:709456ms step_avg:185.19ms\nstep:3842/4500 train_loss:8.1268 train_time:709646ms step_avg:185.19ms\nstep:3843/4500 train_loss:8.2155 train_time:709835ms step_avg:185.19ms\nstep:3844/4500 train_loss:7.9931 train_time:710014ms step_avg:185.19ms\nstep:3845/4500 train_loss:8.3009 train_time:710196ms step_avg:185.19ms\nstep:3846/4500 train_loss:8.1428 train_time:710383ms step_avg:185.19ms\nstep:3847/4500 train_loss:8.1212 train_time:710564ms step_avg:185.19ms\nstep:3848/4500 train_loss:7.9311 train_time:710749ms step_avg:185.19ms\nstep:3849/4500 train_loss:8.1734 train_time:710938ms step_avg:185.19ms\nstep:3850/4500 train_loss:8.1361 train_time:711124ms step_avg:185.19ms\nstep:3851/4500 train_loss:7.9994 train_time:711313ms step_avg:185.19ms\nstep:3852/4500 train_loss:8.2053 train_time:711498ms step_avg:185.19ms\nstep:3853/4500 train_loss:8.1727 train_time:711684ms step_avg:185.19ms\nstep:3854/4500 train_loss:8.0432 train_time:711871ms step_avg:185.19ms\nstep:3855/4500 train_loss:8.1642 train_time:712059ms step_avg:185.19ms\nstep:3856/4500 train_loss:8.1195 train_time:712246ms step_avg:185.19ms\nstep:3857/4500 train_loss:8.1268 train_time:712432ms step_avg:185.19ms\nstep:3858/4500 train_loss:8.0443 train_time:712616ms step_avg:185.19ms\nstep:3859/4500 train_loss:8.0388 train_time:712801ms step_avg:185.19ms\nstep:3860/4500 train_loss:8.0724 train_time:712987ms step_avg:185.19ms\nstep:3861/4500 train_loss:8.1367 train_time:713174ms step_avg:185.19ms\nstep:3862/4500 train_loss:8.0910 train_time:713357ms step_avg:185.19ms\nstep:3863/4500 train_loss:8.1147 train_time:713541ms step_avg:185.19ms\nstep:3864/4500 train_loss:8.0573 train_time:713725ms step_avg:185.19ms\nstep:3865/4500 train_loss:7.9883 train_time:713909ms step_avg:185.19ms\nstep:3866/4500 train_loss:8.1183 train_time:714092ms step_avg:185.19ms\nstep:3867/4500 train_loss:8.1046 train_time:714276ms step_avg:185.19ms\nstep:3868/4500 train_loss:8.0524 train_time:714459ms step_avg:185.19ms\nstep:3869/4500 train_loss:8.0498 train_time:714643ms step_avg:185.19ms\nstep:3870/4500 train_loss:8.0327 train_time:714827ms step_avg:185.19ms\nstep:3871/4500 train_loss:8.0678 train_time:715010ms step_avg:185.19ms\nstep:3872/4500 train_loss:8.0799 train_time:715193ms step_avg:185.19ms\nstep:3873/4500 train_loss:8.1038 train_time:715377ms step_avg:185.19ms\nstep:3874/4500 train_loss:8.2034 train_time:715564ms step_avg:185.19ms\nstep:3875/4500 train_loss:7.9560 train_time:715748ms step_avg:185.19ms\nstep:3876/4500 train_loss:8.1165 train_time:715932ms step_avg:185.19ms\nstep:3877/4500 train_loss:8.1233 train_time:716118ms step_avg:185.19ms\nstep:3878/4500 train_loss:8.1511 train_time:716303ms step_avg:185.19ms\nstep:3879/4500 train_loss:8.0747 train_time:716488ms step_avg:185.19ms\nstep:3880/4500 train_loss:8.1703 train_time:716672ms step_avg:185.19ms\nstep:3881/4500 train_loss:8.0705 train_time:716857ms step_avg:185.19ms\nstep:3882/4500 train_loss:8.1719 train_time:717042ms step_avg:185.19ms\nstep:3883/4500 train_loss:8.0479 train_time:717230ms step_avg:185.19ms\nstep:3884/4500 train_loss:8.0850 train_time:717412ms step_avg:185.19ms\nstep:3885/4500 train_loss:8.0681 train_time:717598ms step_avg:185.19ms\nstep:3886/4500 train_loss:8.0343 train_time:717782ms step_avg:185.19ms\nstep:3887/4500 train_loss:8.0708 train_time:717968ms step_avg:185.19ms\nstep:3888/4500 train_loss:8.1129 train_time:718153ms step_avg:185.19ms\nstep:3889/4500 train_loss:8.2263 train_time:718338ms step_avg:185.19ms\nstep:3890/4500 train_loss:8.1418 train_time:718526ms step_avg:185.19ms\nstep:3891/4500 train_loss:8.0894 train_time:718712ms step_avg:185.19ms\nstep:3892/4500 train_loss:8.1214 train_time:718897ms step_avg:185.19ms\nstep:3893/4500 train_loss:8.1104 train_time:719082ms step_avg:185.19ms\nstep:3894/4500 train_loss:8.1651 train_time:719265ms step_avg:185.19ms\nstep:3895/4500 train_loss:7.9814 train_time:719454ms step_avg:185.19ms\nstep:3896/4500 train_loss:8.1777 train_time:719637ms step_avg:185.19ms\nstep:3897/4500 train_loss:8.1861 train_time:719822ms step_avg:185.19ms\nstep:3898/4500 train_loss:8.1471 train_time:720007ms step_avg:185.19ms\nstep:3899/4500 train_loss:7.9983 train_time:720190ms step_avg:185.19ms\nstep:3900/4500 train_loss:8.1763 train_time:720373ms step_avg:185.19ms\nstep:3901/4500 train_loss:8.1270 train_time:720556ms step_avg:185.19ms\nstep:3902/4500 train_loss:8.1042 train_time:720741ms step_avg:185.19ms\nstep:3903/4500 train_loss:8.1259 train_time:720926ms step_avg:185.19ms\nstep:3904/4500 train_loss:8.1397 train_time:721110ms step_avg:185.18ms\nstep:3905/4500 train_loss:8.0214 train_time:721295ms step_avg:185.18ms\nstep:3906/4500 train_loss:8.1031 train_time:721481ms step_avg:185.19ms\nstep:3907/4500 train_loss:8.0107 train_time:721665ms step_avg:185.18ms\nstep:3908/4500 train_loss:8.0674 train_time:721849ms step_avg:185.18ms\nstep:3909/4500 train_loss:8.0693 train_time:722033ms step_avg:185.18ms\nstep:3910/4500 train_loss:8.2255 train_time:722217ms step_avg:185.18ms\nstep:3911/4500 train_loss:8.2242 train_time:722402ms step_avg:185.18ms\nstep:3912/4500 train_loss:8.1769 train_time:722590ms step_avg:185.18ms\nstep:3913/4500 train_loss:8.1306 train_time:722776ms step_avg:185.18ms\nstep:3914/4500 train_loss:8.0092 train_time:722960ms step_avg:185.18ms\nstep:3915/4500 train_loss:8.1255 train_time:723143ms step_avg:185.18ms\nstep:3916/4500 train_loss:8.1854 train_time:723329ms step_avg:185.18ms\nstep:3917/4500 train_loss:8.0147 train_time:723512ms step_avg:185.18ms\nstep:3918/4500 train_loss:8.1517 train_time:723695ms step_avg:185.18ms\nstep:3919/4500 train_loss:8.0872 train_time:723882ms step_avg:185.18ms\nstep:3920/4500 train_loss:8.1722 train_time:724067ms step_avg:185.18ms\nstep:3921/4500 train_loss:8.1423 train_time:724251ms step_avg:185.18ms\nstep:3922/4500 train_loss:8.0178 train_time:724435ms step_avg:185.18ms\nstep:3923/4500 train_loss:8.0156 train_time:724620ms step_avg:185.18ms\nstep:3924/4500 train_loss:8.3139 train_time:724805ms step_avg:185.18ms\nstep:3925/4500 train_loss:8.0567 train_time:724991ms step_avg:185.18ms\nstep:3926/4500 train_loss:7.9119 train_time:725176ms step_avg:185.18ms\nstep:3927/4500 train_loss:8.0275 train_time:725361ms step_avg:185.18ms\nstep:3928/4500 train_loss:8.1063 train_time:725547ms step_avg:185.18ms\nstep:3929/4500 train_loss:8.1637 train_time:725730ms step_avg:185.18ms\nstep:3930/4500 train_loss:7.9990 train_time:725913ms step_avg:185.18ms\nstep:3931/4500 train_loss:8.1414 train_time:726098ms step_avg:185.18ms\nstep:3932/4500 train_loss:8.0865 train_time:726283ms step_avg:185.18ms\nstep:3933/4500 train_loss:8.1611 train_time:726469ms step_avg:185.18ms\nstep:3934/4500 train_loss:8.1464 train_time:726653ms step_avg:185.18ms\nstep:3935/4500 train_loss:8.0903 train_time:726837ms step_avg:185.18ms\nstep:3936/4500 train_loss:8.1833 train_time:727022ms step_avg:185.18ms\nstep:3937/4500 train_loss:8.1182 train_time:727206ms step_avg:185.18ms\nstep:3938/4500 train_loss:8.1470 train_time:727391ms step_avg:185.18ms\nstep:3939/4500 train_loss:8.0349 train_time:727576ms step_avg:185.18ms\nstep:3940/4500 train_loss:8.3413 train_time:727760ms step_avg:185.18ms\nstep:3941/4500 train_loss:8.0939 train_time:727945ms step_avg:185.18ms\nstep:3942/4500 train_loss:8.1567 train_time:728133ms step_avg:185.18ms\nstep:3943/4500 train_loss:8.1288 train_time:728317ms step_avg:185.18ms\nstep:3944/4500 train_loss:8.0536 train_time:728503ms step_avg:185.18ms\nstep:3945/4500 train_loss:8.0845 train_time:728690ms step_avg:185.18ms\nstep:3946/4500 train_loss:8.0977 train_time:728874ms step_avg:185.18ms\nstep:3947/4500 train_loss:7.9550 train_time:729060ms step_avg:185.18ms\nstep:3948/4500 train_loss:8.0969 train_time:729246ms step_avg:185.18ms\nstep:3949/4500 train_loss:8.1862 train_time:729432ms step_avg:185.18ms\nstep:3950/4500 train_loss:8.0510 train_time:729616ms step_avg:185.18ms\nstep:3951/4500 train_loss:8.4405 train_time:729800ms step_avg:185.18ms\nstep:3952/4500 train_loss:8.0964 train_time:729985ms step_avg:185.18ms\nstep:3953/4500 train_loss:8.1338 train_time:730171ms step_avg:185.18ms\nstep:3954/4500 train_loss:8.2340 train_time:730355ms step_avg:185.18ms\nstep:3955/4500 train_loss:8.0892 train_time:730541ms step_avg:185.18ms\nstep:3956/4500 train_loss:8.1094 train_time:730727ms step_avg:185.18ms\nstep:3957/4500 train_loss:8.1464 train_time:730910ms step_avg:185.18ms\nstep:3958/4500 train_loss:7.9678 train_time:731094ms step_avg:185.18ms\nstep:3959/4500 train_loss:8.1029 train_time:731278ms step_avg:185.18ms\nstep:3960/4500 train_loss:8.1765 train_time:731462ms step_avg:185.18ms\nstep:3961/4500 train_loss:8.1472 train_time:731648ms step_avg:185.18ms\nstep:3962/4500 train_loss:7.9990 train_time:731834ms step_avg:185.18ms\nstep:3963/4500 train_loss:7.9679 train_time:732019ms step_avg:185.18ms\nstep:3964/4500 train_loss:8.1139 train_time:732204ms step_avg:185.18ms\nstep:3965/4500 train_loss:8.1712 train_time:732388ms step_avg:185.18ms\nstep:3966/4500 train_loss:8.0370 train_time:732572ms step_avg:185.18ms\nstep:3967/4500 train_loss:8.1098 train_time:732756ms step_avg:185.18ms\nstep:3968/4500 train_loss:7.9558 train_time:732941ms step_avg:185.18ms\nstep:3969/4500 train_loss:8.0227 train_time:733130ms step_avg:185.18ms\nstep:3970/4500 train_loss:8.1135 train_time:733315ms step_avg:185.18ms\nstep:3971/4500 train_loss:8.1326 train_time:733499ms step_avg:185.18ms\nstep:3972/4500 train_loss:8.0860 train_time:733684ms step_avg:185.18ms\nstep:3973/4500 train_loss:7.9660 train_time:733870ms step_avg:185.18ms\nstep:3974/4500 train_loss:8.1183 train_time:734054ms step_avg:185.18ms\nstep:3975/4500 train_loss:8.0649 train_time:734240ms step_avg:185.18ms\nstep:3976/4500 train_loss:7.9723 train_time:734427ms step_avg:185.18ms\nstep:3977/4500 train_loss:8.0874 train_time:734615ms step_avg:185.18ms\nstep:3978/4500 train_loss:7.7671 train_time:734797ms step_avg:185.18ms\nstep:3979/4500 train_loss:7.9727 train_time:734982ms step_avg:185.18ms\nstep:3980/4500 train_loss:8.2184 train_time:735168ms step_avg:185.18ms\nstep:3981/4500 train_loss:8.0466 train_time:735352ms step_avg:185.18ms\nstep:3982/4500 train_loss:8.1222 train_time:735536ms step_avg:185.18ms\nstep:3983/4500 train_loss:8.1603 train_time:735724ms step_avg:185.18ms\nstep:3984/4500 train_loss:8.0276 train_time:735909ms step_avg:185.18ms\nstep:3985/4500 train_loss:8.1664 train_time:736092ms step_avg:185.18ms\nstep:3986/4500 train_loss:8.1209 train_time:736276ms step_avg:185.18ms\nstep:3987/4500 train_loss:8.0571 train_time:736460ms step_avg:185.18ms\nstep:3988/4500 train_loss:8.1000 train_time:736646ms step_avg:185.18ms\nstep:3989/4500 train_loss:7.9738 train_time:736831ms step_avg:185.18ms\nstep:3990/4500 train_loss:8.0435 train_time:737018ms step_avg:185.18ms\nstep:3991/4500 train_loss:8.1448 train_time:737206ms step_avg:185.18ms\nstep:3992/4500 train_loss:8.0100 train_time:737391ms step_avg:185.18ms\nstep:3993/4500 train_loss:8.1218 train_time:737577ms step_avg:185.18ms\nstep:3994/4500 train_loss:8.1508 train_time:737761ms step_avg:185.18ms\nstep:3995/4500 train_loss:8.0345 train_time:737947ms step_avg:185.18ms\nstep:3996/4500 train_loss:8.3232 train_time:738131ms step_avg:185.18ms\nstep:3997/4500 train_loss:8.0119 train_time:738317ms step_avg:185.18ms\nstep:3998/4500 train_loss:8.0598 train_time:738503ms step_avg:185.18ms\nstep:3999/4500 train_loss:8.0967 train_time:738688ms step_avg:185.18ms\nstep:4000/4500 train_loss:8.0133 train_time:738872ms step_avg:185.18ms\nstep:4001/4500 train_loss:8.1348 train_time:739057ms step_avg:185.18ms\nstep:4002/4500 train_loss:8.1326 train_time:739246ms step_avg:185.18ms\nstep:4003/4500 train_loss:8.0527 train_time:739432ms step_avg:185.18ms\nstep:4004/4500 train_loss:8.1364 train_time:739617ms step_avg:185.18ms\nstep:4005/4500 train_loss:8.1321 train_time:739801ms step_avg:185.18ms\nstep:4006/4500 train_loss:7.9711 train_time:739992ms step_avg:185.18ms\nstep:4007/4500 train_loss:8.2269 train_time:740171ms step_avg:185.18ms\nstep:4008/4500 train_loss:8.0659 train_time:740353ms step_avg:185.18ms\nstep:4009/4500 train_loss:8.2010 train_time:740536ms step_avg:185.18ms\nstep:4010/4500 train_loss:8.1574 train_time:740722ms step_avg:185.18ms\nstep:4011/4500 train_loss:8.1403 train_time:740906ms step_avg:185.18ms\nstep:4012/4500 train_loss:8.1341 train_time:741093ms step_avg:185.18ms\nstep:4013/4500 train_loss:8.0968 train_time:741277ms step_avg:185.18ms\nstep:4014/4500 train_loss:8.1444 train_time:741463ms step_avg:185.18ms\nstep:4015/4500 train_loss:8.0559 train_time:741648ms step_avg:185.18ms\nstep:4016/4500 train_loss:8.0953 train_time:741833ms step_avg:185.18ms\nstep:4017/4500 train_loss:8.0453 train_time:742018ms step_avg:185.18ms\nstep:4018/4500 train_loss:8.0937 train_time:742203ms step_avg:185.18ms\nstep:4019/4500 train_loss:8.1938 train_time:742389ms step_avg:185.18ms\nstep:4020/4500 train_loss:8.0924 train_time:742572ms step_avg:185.18ms\nstep:4021/4500 train_loss:8.0762 train_time:742756ms step_avg:185.18ms\nstep:4022/4500 train_loss:8.0914 train_time:742940ms step_avg:185.18ms\nstep:4023/4500 train_loss:8.1259 train_time:743124ms step_avg:185.18ms\nstep:4024/4500 train_loss:8.1971 train_time:743308ms step_avg:185.18ms\nstep:4025/4500 train_loss:8.1350 train_time:743493ms step_avg:185.18ms\nstep:4026/4500 train_loss:8.0060 train_time:743676ms step_avg:185.18ms\nstep:4027/4500 train_loss:8.3867 train_time:743864ms step_avg:185.18ms\nstep:4028/4500 train_loss:8.0242 train_time:744052ms step_avg:185.18ms\nstep:4029/4500 train_loss:8.1267 train_time:744236ms step_avg:185.18ms\nstep:4030/4500 train_loss:8.0536 train_time:744425ms step_avg:185.18ms\nstep:4031/4500 train_loss:8.1185 train_time:744606ms step_avg:185.18ms\nstep:4032/4500 train_loss:8.0424 train_time:744791ms step_avg:185.18ms\nstep:4033/4500 train_loss:8.0717 train_time:744976ms step_avg:185.18ms\nstep:4034/4500 train_loss:8.1067 train_time:745161ms step_avg:185.18ms\nstep:4035/4500 train_loss:8.0539 train_time:745345ms step_avg:185.18ms\nstep:4036/4500 train_loss:8.0058 train_time:745532ms step_avg:185.18ms\nstep:4037/4500 train_loss:8.2474 train_time:745715ms step_avg:185.18ms\nstep:4038/4500 train_loss:8.0246 train_time:745899ms step_avg:185.18ms\nstep:4039/4500 train_loss:8.1755 train_time:746085ms step_avg:185.18ms\nstep:4040/4500 train_loss:8.0732 train_time:746270ms step_avg:185.18ms\nstep:4041/4500 train_loss:8.1609 train_time:746454ms step_avg:185.18ms\nstep:4042/4500 train_loss:8.1077 train_time:746637ms step_avg:185.18ms\nstep:4043/4500 train_loss:8.1104 train_time:746822ms step_avg:185.18ms\nstep:4044/4500 train_loss:8.1131 train_time:747005ms step_avg:185.18ms\nstep:4045/4500 train_loss:8.1468 train_time:747189ms step_avg:185.18ms\nstep:4046/4500 train_loss:8.1282 train_time:747374ms step_avg:185.18ms\nstep:4047/4500 train_loss:8.2293 train_time:747560ms step_avg:185.18ms\nstep:4048/4500 train_loss:8.0334 train_time:747747ms step_avg:185.18ms\nstep:4049/4500 train_loss:8.0675 train_time:747933ms step_avg:185.18ms\nstep:4050/4500 train_loss:8.0199 train_time:748119ms step_avg:185.18ms\nstep:4051/4500 train_loss:8.1066 train_time:748307ms step_avg:185.18ms\nstep:4052/4500 train_loss:8.0229 train_time:748493ms step_avg:185.18ms\nstep:4053/4500 train_loss:8.1349 train_time:748680ms step_avg:185.18ms\nstep:4054/4500 train_loss:7.9804 train_time:748864ms step_avg:185.18ms\nstep:4055/4500 train_loss:8.2068 train_time:749049ms step_avg:185.18ms\nstep:4056/4500 train_loss:7.9576 train_time:749233ms step_avg:185.18ms\nstep:4057/4500 train_loss:7.9585 train_time:749417ms step_avg:185.18ms\nstep:4058/4500 train_loss:8.1429 train_time:749604ms step_avg:185.18ms\nstep:4059/4500 train_loss:8.0981 train_time:749791ms step_avg:185.18ms\nstep:4060/4500 train_loss:8.1437 train_time:749974ms step_avg:185.18ms\nstep:4061/4500 train_loss:8.2056 train_time:750159ms step_avg:185.18ms\nstep:4062/4500 train_loss:8.0687 train_time:750341ms step_avg:185.18ms\nstep:4063/4500 train_loss:8.1341 train_time:750525ms step_avg:185.18ms\nstep:4064/4500 train_loss:8.2223 train_time:750709ms step_avg:185.18ms\nstep:4065/4500 train_loss:7.9607 train_time:750896ms step_avg:185.18ms\nstep:4066/4500 train_loss:8.1223 train_time:751081ms step_avg:185.18ms\nstep:4067/4500 train_loss:8.1175 train_time:751269ms step_avg:185.18ms\nstep:4068/4500 train_loss:8.0415 train_time:751454ms step_avg:185.18ms\nstep:4069/4500 train_loss:8.2199 train_time:751639ms step_avg:185.18ms\nstep:4070/4500 train_loss:8.0757 train_time:751823ms step_avg:185.18ms\nstep:4071/4500 train_loss:8.0921 train_time:752006ms step_avg:185.18ms\nstep:4072/4500 train_loss:8.1579 train_time:752190ms step_avg:185.18ms\nstep:4073/4500 train_loss:7.9827 train_time:752374ms step_avg:185.18ms\nstep:4074/4500 train_loss:8.1373 train_time:752561ms step_avg:185.18ms\nstep:4075/4500 train_loss:8.0879 train_time:752748ms step_avg:185.18ms\nstep:4076/4500 train_loss:8.2507 train_time:752934ms step_avg:185.18ms\nstep:4077/4500 train_loss:8.2081 train_time:753119ms step_avg:185.18ms\nstep:4078/4500 train_loss:8.0319 train_time:753303ms step_avg:185.18ms\nstep:4079/4500 train_loss:8.1330 train_time:753487ms step_avg:185.18ms\nstep:4080/4500 train_loss:8.0971 train_time:753672ms step_avg:185.18ms\nstep:4081/4500 train_loss:8.0756 train_time:753856ms step_avg:185.18ms\nstep:4082/4500 train_loss:8.0941 train_time:754044ms step_avg:185.18ms\nstep:4083/4500 train_loss:8.0474 train_time:754229ms step_avg:185.18ms\nstep:4084/4500 train_loss:8.0885 train_time:754413ms step_avg:185.18ms\nstep:4085/4500 train_loss:8.1678 train_time:754598ms step_avg:185.18ms\nstep:4086/4500 train_loss:8.0898 train_time:754786ms step_avg:185.18ms\nstep:4087/4500 train_loss:8.0679 train_time:754969ms step_avg:185.18ms\nstep:4088/4500 train_loss:8.0919 train_time:755154ms step_avg:185.18ms\nstep:4089/4500 train_loss:8.1587 train_time:755339ms step_avg:185.18ms\nstep:4090/4500 train_loss:8.1033 train_time:755522ms step_avg:185.18ms\nstep:4091/4500 train_loss:8.0503 train_time:755710ms step_avg:185.18ms\nstep:4092/4500 train_loss:8.1479 train_time:755894ms step_avg:185.18ms\nstep:4093/4500 train_loss:8.0000 train_time:756079ms step_avg:185.18ms\nstep:4094/4500 train_loss:8.0676 train_time:756263ms step_avg:185.18ms\nstep:4095/4500 train_loss:8.1825 train_time:756447ms step_avg:185.18ms\nstep:4096/4500 train_loss:8.0976 train_time:756629ms step_avg:185.18ms\nstep:4097/4500 train_loss:8.1038 train_time:756812ms step_avg:185.18ms\nstep:4098/4500 train_loss:7.9359 train_time:756998ms step_avg:185.18ms\nstep:4099/4500 train_loss:8.1585 train_time:757182ms step_avg:185.18ms\nstep:4100/4500 train_loss:8.0812 train_time:757369ms step_avg:185.18ms\nstep:4101/4500 train_loss:8.0383 train_time:757555ms step_avg:185.18ms\nstep:4102/4500 train_loss:8.0527 train_time:757741ms step_avg:185.18ms\nstep:4103/4500 train_loss:8.0388 train_time:757925ms step_avg:185.18ms\nstep:4104/4500 train_loss:7.9843 train_time:758109ms step_avg:185.18ms\nstep:4105/4500 train_loss:8.0905 train_time:758294ms step_avg:185.18ms\nstep:4106/4500 train_loss:8.1047 train_time:758479ms step_avg:185.18ms\nstep:4107/4500 train_loss:8.0628 train_time:758666ms step_avg:185.18ms\nstep:4108/4500 train_loss:8.2021 train_time:758849ms step_avg:185.18ms\nstep:4109/4500 train_loss:8.0844 train_time:759031ms step_avg:185.17ms\nstep:4110/4500 train_loss:8.0644 train_time:759216ms step_avg:185.17ms\nstep:4111/4500 train_loss:8.0904 train_time:759401ms step_avg:185.17ms\nstep:4112/4500 train_loss:8.3074 train_time:759586ms step_avg:185.17ms\nstep:4113/4500 train_loss:8.0019 train_time:759770ms step_avg:185.17ms\nstep:4114/4500 train_loss:8.0068 train_time:759954ms step_avg:185.17ms\nstep:4115/4500 train_loss:8.0802 train_time:760139ms step_avg:185.17ms\nstep:4116/4500 train_loss:8.0834 train_time:760325ms step_avg:185.17ms\nstep:4117/4500 train_loss:7.9553 train_time:760508ms step_avg:185.17ms\nstep:4118/4500 train_loss:8.1602 train_time:760694ms step_avg:185.17ms\nstep:4119/4500 train_loss:8.1107 train_time:760878ms step_avg:185.17ms\nstep:4120/4500 train_loss:7.9789 train_time:761066ms step_avg:185.17ms\nstep:4121/4500 train_loss:8.1142 train_time:761250ms step_avg:185.17ms\nstep:4122/4500 train_loss:8.0016 train_time:761433ms step_avg:185.17ms\nstep:4123/4500 train_loss:7.9988 train_time:761618ms step_avg:185.17ms\nstep:4124/4500 train_loss:8.0907 train_time:761805ms step_avg:185.17ms\nstep:4125/4500 train_loss:8.0777 train_time:761989ms step_avg:185.17ms\nstep:4126/4500 train_loss:8.1620 train_time:762173ms step_avg:185.17ms\nstep:4127/4500 train_loss:7.8929 train_time:762357ms step_avg:185.17ms\nstep:4128/4500 train_loss:7.9670 train_time:762541ms step_avg:185.17ms\nstep:4129/4500 train_loss:8.0696 train_time:762727ms step_avg:185.17ms\nstep:4130/4500 train_loss:8.0868 train_time:762912ms step_avg:185.17ms\nstep:4131/4500 train_loss:8.0569 train_time:763099ms step_avg:185.17ms\nstep:4132/4500 train_loss:8.1936 train_time:763288ms step_avg:185.17ms\nstep:4133/4500 train_loss:8.0231 train_time:763471ms step_avg:185.17ms\nstep:4134/4500 train_loss:8.0353 train_time:763654ms step_avg:185.17ms\nstep:4135/4500 train_loss:8.1443 train_time:763836ms step_avg:185.17ms\nstep:4136/4500 train_loss:8.0176 train_time:764020ms step_avg:185.17ms\nstep:4137/4500 train_loss:8.0858 train_time:764204ms step_avg:185.17ms\nstep:4138/4500 train_loss:8.1769 train_time:764387ms step_avg:185.17ms\nstep:4139/4500 train_loss:8.1043 train_time:764573ms step_avg:185.17ms\nstep:4140/4500 train_loss:8.0408 train_time:764759ms step_avg:185.17ms\nstep:4141/4500 train_loss:8.1247 train_time:764944ms step_avg:185.17ms\nstep:4142/4500 train_loss:8.1860 train_time:765127ms step_avg:185.17ms\nstep:4143/4500 train_loss:8.0968 train_time:765311ms step_avg:185.17ms\nstep:4144/4500 train_loss:8.0485 train_time:765493ms step_avg:185.17ms\nstep:4145/4500 train_loss:8.0380 train_time:765679ms step_avg:185.17ms\nstep:4146/4500 train_loss:7.9999 train_time:765868ms step_avg:185.17ms\nstep:4147/4500 train_loss:8.0819 train_time:766052ms step_avg:185.17ms\nstep:4148/4500 train_loss:7.9728 train_time:766238ms step_avg:185.17ms\nstep:4149/4500 train_loss:7.9860 train_time:766424ms step_avg:185.17ms\nstep:4150/4500 train_loss:7.9864 train_time:766610ms step_avg:185.17ms\nstep:4151/4500 train_loss:8.0747 train_time:766794ms step_avg:185.17ms\nstep:4152/4500 train_loss:8.1022 train_time:766979ms step_avg:185.17ms\nstep:4153/4500 train_loss:8.0472 train_time:767163ms step_avg:185.17ms\nstep:4154/4500 train_loss:8.0652 train_time:767346ms step_avg:185.17ms\nstep:4155/4500 train_loss:8.2019 train_time:767530ms step_avg:185.17ms\nstep:4156/4500 train_loss:8.1852 train_time:767714ms step_avg:185.17ms\nstep:4157/4500 train_loss:8.0778 train_time:767900ms step_avg:185.17ms\nstep:4158/4500 train_loss:7.9930 train_time:768088ms step_avg:185.17ms\nstep:4159/4500 train_loss:8.0614 train_time:768272ms step_avg:185.17ms\nstep:4160/4500 train_loss:8.0679 train_time:768456ms step_avg:185.17ms\nstep:4161/4500 train_loss:7.9993 train_time:768640ms step_avg:185.17ms\nstep:4162/4500 train_loss:7.9882 train_time:768824ms step_avg:185.17ms\nstep:4163/4500 train_loss:8.0521 train_time:769007ms step_avg:185.17ms\nstep:4164/4500 train_loss:8.0350 train_time:769193ms step_avg:185.17ms\nstep:4165/4500 train_loss:7.9428 train_time:769377ms step_avg:185.17ms\nstep:4166/4500 train_loss:8.1120 train_time:769563ms step_avg:185.17ms\nstep:4167/4500 train_loss:8.0028 train_time:769748ms step_avg:185.17ms\nstep:4168/4500 train_loss:8.1119 train_time:769933ms step_avg:185.17ms\nstep:4169/4500 train_loss:8.0452 train_time:770119ms step_avg:185.17ms\nstep:4170/4500 train_loss:8.2539 train_time:770305ms step_avg:185.17ms\nstep:4171/4500 train_loss:8.0572 train_time:770491ms step_avg:185.17ms\nstep:4172/4500 train_loss:7.9088 train_time:770675ms step_avg:185.17ms\nstep:4173/4500 train_loss:8.1908 train_time:770859ms step_avg:185.17ms\nstep:4174/4500 train_loss:8.0175 train_time:771046ms step_avg:185.17ms\nstep:4175/4500 train_loss:8.0888 train_time:771231ms step_avg:185.17ms\nstep:4176/4500 train_loss:8.1043 train_time:771415ms step_avg:185.17ms\nstep:4177/4500 train_loss:8.1646 train_time:771599ms step_avg:185.17ms\nstep:4178/4500 train_loss:8.1106 train_time:771789ms step_avg:185.17ms\nstep:4179/4500 train_loss:8.1457 train_time:771972ms step_avg:185.17ms\nstep:4180/4500 train_loss:7.9886 train_time:772155ms step_avg:185.17ms\nstep:4181/4500 train_loss:8.0371 train_time:772341ms step_avg:185.17ms\nstep:4182/4500 train_loss:8.0542 train_time:772530ms step_avg:185.17ms\nstep:4183/4500 train_loss:7.9618 train_time:772712ms step_avg:185.17ms\nstep:4184/4500 train_loss:7.9893 train_time:772902ms step_avg:185.17ms\nstep:4185/4500 train_loss:8.0049 train_time:773084ms step_avg:185.17ms\nstep:4186/4500 train_loss:7.9784 train_time:773271ms step_avg:185.17ms\nstep:4187/4500 train_loss:8.0679 train_time:773454ms step_avg:185.17ms\nstep:4188/4500 train_loss:8.1223 train_time:773639ms step_avg:185.17ms\nstep:4189/4500 train_loss:8.1105 train_time:773826ms step_avg:185.17ms\nstep:4190/4500 train_loss:8.1937 train_time:774011ms step_avg:185.17ms\nstep:4191/4500 train_loss:8.1994 train_time:774195ms step_avg:185.17ms\nstep:4192/4500 train_loss:8.0684 train_time:774381ms step_avg:185.17ms\nstep:4193/4500 train_loss:8.0942 train_time:774564ms step_avg:185.17ms\nstep:4194/4500 train_loss:7.9247 train_time:774750ms step_avg:185.17ms\nstep:4195/4500 train_loss:8.0719 train_time:774934ms step_avg:185.17ms\nstep:4196/4500 train_loss:8.2777 train_time:775119ms step_avg:185.17ms\nstep:4197/4500 train_loss:8.0866 train_time:775304ms step_avg:185.17ms\nstep:4198/4500 train_loss:8.2608 train_time:775493ms step_avg:185.17ms\nstep:4199/4500 train_loss:8.0525 train_time:775679ms step_avg:185.17ms\nstep:4200/4500 train_loss:8.0432 train_time:775864ms step_avg:185.17ms\nstep:4201/4500 train_loss:8.0386 train_time:776051ms step_avg:185.17ms\nstep:4202/4500 train_loss:8.2905 train_time:776235ms step_avg:185.17ms\nstep:4203/4500 train_loss:8.0394 train_time:776419ms step_avg:185.17ms\nstep:4204/4500 train_loss:8.0664 train_time:776602ms step_avg:185.17ms\nstep:4205/4500 train_loss:8.1704 train_time:776785ms step_avg:185.17ms\nstep:4206/4500 train_loss:8.0565 train_time:776968ms step_avg:185.17ms\nstep:4207/4500 train_loss:8.0270 train_time:777151ms step_avg:185.17ms\nstep:4208/4500 train_loss:7.9922 train_time:777337ms step_avg:185.17ms\nstep:4209/4500 train_loss:8.1421 train_time:777523ms step_avg:185.17ms\nstep:4210/4500 train_loss:7.8977 train_time:777708ms step_avg:185.17ms\nstep:4211/4500 train_loss:8.0304 train_time:777891ms step_avg:185.17ms\nstep:4212/4500 train_loss:8.0728 train_time:778074ms step_avg:185.17ms\nstep:4213/4500 train_loss:7.9698 train_time:778259ms step_avg:185.17ms\nstep:4214/4500 train_loss:7.9645 train_time:778448ms step_avg:185.17ms\nstep:4215/4500 train_loss:8.1822 train_time:778635ms step_avg:185.17ms\nstep:4216/4500 train_loss:8.1072 train_time:778820ms step_avg:185.17ms\nstep:4217/4500 train_loss:8.0414 train_time:779004ms step_avg:185.17ms\nstep:4218/4500 train_loss:8.1245 train_time:779188ms step_avg:185.17ms\nstep:4219/4500 train_loss:8.1094 train_time:779374ms step_avg:185.17ms\nstep:4220/4500 train_loss:8.1451 train_time:779560ms step_avg:185.17ms\nstep:4221/4500 train_loss:8.0014 train_time:779750ms step_avg:185.17ms\nstep:4222/4500 train_loss:8.0163 train_time:779936ms step_avg:185.17ms\nstep:4223/4500 train_loss:8.0694 train_time:780120ms step_avg:185.17ms\nstep:4224/4500 train_loss:8.0431 train_time:780305ms step_avg:185.17ms\nstep:4225/4500 train_loss:8.2410 train_time:780488ms step_avg:185.17ms\nstep:4226/4500 train_loss:8.0463 train_time:780673ms step_avg:185.17ms\nstep:4227/4500 train_loss:8.0192 train_time:780859ms step_avg:185.17ms\nstep:4228/4500 train_loss:8.1207 train_time:781045ms step_avg:185.17ms\nstep:4229/4500 train_loss:8.0955 train_time:781230ms step_avg:185.17ms\nstep:4230/4500 train_loss:8.0671 train_time:781415ms step_avg:185.17ms\nstep:4231/4500 train_loss:8.0173 train_time:781598ms step_avg:185.17ms\nstep:4232/4500 train_loss:8.1571 train_time:781785ms step_avg:185.17ms\nstep:4233/4500 train_loss:7.9822 train_time:781971ms step_avg:185.17ms\nstep:4234/4500 train_loss:8.1782 train_time:782155ms step_avg:185.17ms\nstep:4235/4500 train_loss:8.0032 train_time:782340ms step_avg:185.17ms\nstep:4236/4500 train_loss:8.0235 train_time:782525ms step_avg:185.17ms\nstep:4237/4500 train_loss:8.0313 train_time:782713ms step_avg:185.17ms\nstep:4238/4500 train_loss:8.0578 train_time:782897ms step_avg:185.17ms\nstep:4239/4500 train_loss:7.9677 train_time:783084ms step_avg:185.17ms\nstep:4240/4500 train_loss:8.0101 train_time:783268ms step_avg:185.17ms\nstep:4241/4500 train_loss:8.1102 train_time:783452ms step_avg:185.17ms\nstep:4242/4500 train_loss:7.9933 train_time:783638ms step_avg:185.17ms\nstep:4243/4500 train_loss:8.2044 train_time:783823ms step_avg:185.17ms\nstep:4244/4500 train_loss:8.2391 train_time:784008ms step_avg:185.17ms\nstep:4245/4500 train_loss:8.0726 train_time:784192ms step_avg:185.17ms\nstep:4246/4500 train_loss:8.1600 train_time:784377ms step_avg:185.17ms\nstep:4247/4500 train_loss:8.0700 train_time:784563ms step_avg:185.17ms\nstep:4248/4500 train_loss:8.0834 train_time:784749ms step_avg:185.17ms\nstep:4249/4500 train_loss:8.0768 train_time:784935ms step_avg:185.17ms\nstep:4250/4500 train_loss:8.0960 train_time:785120ms step_avg:185.17ms\nstep:4251/4500 train_loss:7.9010 train_time:785305ms step_avg:185.17ms\nstep:4252/4500 train_loss:8.1139 train_time:785489ms step_avg:185.17ms\nstep:4253/4500 train_loss:8.0034 train_time:785674ms step_avg:185.17ms\nstep:4254/4500 train_loss:8.0775 train_time:785858ms step_avg:185.17ms\nstep:4255/4500 train_loss:8.1745 train_time:786047ms step_avg:185.17ms\nstep:4256/4500 train_loss:8.0299 train_time:786232ms step_avg:185.17ms\nstep:4257/4500 train_loss:7.9846 train_time:786416ms step_avg:185.17ms\nstep:4258/4500 train_loss:8.0982 train_time:786600ms step_avg:185.17ms\nstep:4259/4500 train_loss:8.1197 train_time:786784ms step_avg:185.17ms\nstep:4260/4500 train_loss:8.1208 train_time:786970ms step_avg:185.17ms\nstep:4261/4500 train_loss:8.1590 train_time:787153ms step_avg:185.17ms\nstep:4262/4500 train_loss:8.0934 train_time:787337ms step_avg:185.17ms\nstep:4263/4500 train_loss:7.9887 train_time:787521ms step_avg:185.17ms\nstep:4264/4500 train_loss:8.0594 train_time:787705ms step_avg:185.17ms\nstep:4265/4500 train_loss:8.0347 train_time:787892ms step_avg:185.17ms\nstep:4266/4500 train_loss:8.0873 train_time:788077ms step_avg:185.17ms\nstep:4267/4500 train_loss:8.0710 train_time:788263ms step_avg:185.17ms\nstep:4268/4500 train_loss:7.9722 train_time:788448ms step_avg:185.17ms\nstep:4269/4500 train_loss:8.0885 train_time:788634ms step_avg:185.17ms\nstep:4270/4500 train_loss:8.3747 train_time:788818ms step_avg:185.17ms\nstep:4271/4500 train_loss:8.1367 train_time:789002ms step_avg:185.17ms\nstep:4272/4500 train_loss:7.9028 train_time:789187ms step_avg:185.17ms\nstep:4273/4500 train_loss:8.1407 train_time:789371ms step_avg:185.17ms\nstep:4274/4500 train_loss:8.0089 train_time:789554ms step_avg:185.17ms\nstep:4275/4500 train_loss:8.0594 train_time:789738ms step_avg:185.17ms\nstep:4276/4500 train_loss:8.0631 train_time:789925ms step_avg:185.17ms\nstep:4277/4500 train_loss:7.9843 train_time:790112ms step_avg:185.17ms\nstep:4278/4500 train_loss:7.9503 train_time:790296ms step_avg:185.17ms\nstep:4279/4500 train_loss:8.0661 train_time:790481ms step_avg:185.17ms\nstep:4280/4500 train_loss:8.0017 train_time:790665ms step_avg:185.17ms\nstep:4281/4500 train_loss:8.0686 train_time:790853ms step_avg:185.17ms\nstep:4282/4500 train_loss:8.1089 train_time:791040ms step_avg:185.17ms\nstep:4283/4500 train_loss:8.1503 train_time:791228ms step_avg:185.17ms\nstep:4284/4500 train_loss:7.9027 train_time:791416ms step_avg:185.17ms\nstep:4285/4500 train_loss:8.0952 train_time:791601ms step_avg:185.17ms\nstep:4286/4500 train_loss:8.0163 train_time:791788ms step_avg:185.17ms\nstep:4287/4500 train_loss:8.1214 train_time:791973ms step_avg:185.17ms\nstep:4288/4500 train_loss:7.9933 train_time:792158ms step_avg:185.17ms\nstep:4289/4500 train_loss:8.0965 train_time:792347ms step_avg:185.17ms\nstep:4290/4500 train_loss:7.9760 train_time:792531ms step_avg:185.17ms\nstep:4291/4500 train_loss:7.9457 train_time:792715ms step_avg:185.17ms\nstep:4292/4500 train_loss:7.9908 train_time:792898ms step_avg:185.17ms\nstep:4293/4500 train_loss:7.9748 train_time:793082ms step_avg:185.17ms\nstep:4294/4500 train_loss:7.9753 train_time:793265ms step_avg:185.17ms\nstep:4295/4500 train_loss:7.9681 train_time:793450ms step_avg:185.17ms\nstep:4296/4500 train_loss:8.0700 train_time:793633ms step_avg:185.17ms\nstep:4297/4500 train_loss:8.0431 train_time:793817ms step_avg:185.17ms\nstep:4298/4500 train_loss:8.0303 train_time:794002ms step_avg:185.17ms\nstep:4299/4500 train_loss:8.0797 train_time:794188ms step_avg:185.17ms\nstep:4300/4500 train_loss:8.0427 train_time:794372ms step_avg:185.17ms\nstep:4301/4500 train_loss:8.1190 train_time:794555ms step_avg:185.17ms\nstep:4302/4500 train_loss:8.0948 train_time:794739ms step_avg:185.17ms\nstep:4303/4500 train_loss:8.1266 train_time:794926ms step_avg:185.17ms\nstep:4304/4500 train_loss:8.1227 train_time:795109ms step_avg:185.17ms\nstep:4305/4500 train_loss:8.0654 train_time:795293ms step_avg:185.17ms\nstep:4306/4500 train_loss:8.0265 train_time:795479ms step_avg:185.17ms\nstep:4307/4500 train_loss:8.0084 train_time:795663ms step_avg:185.17ms\nstep:4308/4500 train_loss:7.9869 train_time:795849ms step_avg:185.17ms\nstep:4309/4500 train_loss:8.0994 train_time:796032ms step_avg:185.17ms\nstep:4310/4500 train_loss:7.9977 train_time:796217ms step_avg:185.17ms\nstep:4311/4500 train_loss:8.0588 train_time:796401ms step_avg:185.17ms\nstep:4312/4500 train_loss:8.1106 train_time:796587ms step_avg:185.17ms\nstep:4313/4500 train_loss:8.0244 train_time:796772ms step_avg:185.17ms\nstep:4314/4500 train_loss:7.9881 train_time:796957ms step_avg:185.17ms\nstep:4315/4500 train_loss:8.0273 train_time:797143ms step_avg:185.17ms\nstep:4316/4500 train_loss:8.2516 train_time:797332ms step_avg:185.17ms\nstep:4317/4500 train_loss:7.9554 train_time:797517ms step_avg:185.17ms\nstep:4318/4500 train_loss:8.0720 train_time:797702ms step_avg:185.17ms\nstep:4319/4500 train_loss:7.9978 train_time:797888ms step_avg:185.17ms\nstep:4320/4500 train_loss:8.1005 train_time:798074ms step_avg:185.17ms\nstep:4321/4500 train_loss:8.0366 train_time:798261ms step_avg:185.17ms\nstep:4322/4500 train_loss:8.0214 train_time:798449ms step_avg:185.17ms\nstep:4323/4500 train_loss:8.0538 train_time:798633ms step_avg:185.17ms\nstep:4324/4500 train_loss:7.9602 train_time:798818ms step_avg:185.17ms\nstep:4325/4500 train_loss:7.9831 train_time:799005ms step_avg:185.17ms\nstep:4326/4500 train_loss:8.1803 train_time:799189ms step_avg:185.17ms\nstep:4327/4500 train_loss:8.0912 train_time:799375ms step_avg:185.17ms\nstep:4328/4500 train_loss:7.9958 train_time:799562ms step_avg:185.17ms\nstep:4329/4500 train_loss:8.0043 train_time:799746ms step_avg:185.17ms\nstep:4330/4500 train_loss:8.0581 train_time:799930ms step_avg:185.17ms\nstep:4331/4500 train_loss:8.0555 train_time:800116ms step_avg:185.17ms\nstep:4332/4500 train_loss:8.1756 train_time:800302ms step_avg:185.17ms\nstep:4333/4500 train_loss:8.0100 train_time:800488ms step_avg:185.17ms\nstep:4334/4500 train_loss:8.0711 train_time:800674ms step_avg:185.17ms\nstep:4335/4500 train_loss:8.0593 train_time:800860ms step_avg:185.17ms\nstep:4336/4500 train_loss:8.1810 train_time:801044ms step_avg:185.17ms\nstep:4337/4500 train_loss:7.7097 train_time:801230ms step_avg:185.17ms\nstep:4338/4500 train_loss:7.9568 train_time:801414ms step_avg:185.17ms\nstep:4339/4500 train_loss:8.1475 train_time:801599ms step_avg:185.17ms\nstep:4340/4500 train_loss:8.0572 train_time:801787ms step_avg:185.17ms\nstep:4341/4500 train_loss:7.9426 train_time:801972ms step_avg:185.17ms\nstep:4342/4500 train_loss:7.9725 train_time:802156ms step_avg:185.17ms\nstep:4343/4500 train_loss:8.0189 train_time:802340ms step_avg:185.17ms\nstep:4344/4500 train_loss:8.0381 train_time:802526ms step_avg:185.17ms\nstep:4345/4500 train_loss:8.0914 train_time:802711ms step_avg:185.17ms\nstep:4346/4500 train_loss:8.0558 train_time:802895ms step_avg:185.17ms\nstep:4347/4500 train_loss:7.9856 train_time:803083ms step_avg:185.17ms\nstep:4348/4500 train_loss:8.0855 train_time:803270ms step_avg:185.17ms\nstep:4349/4500 train_loss:8.0091 train_time:803455ms step_avg:185.17ms\nstep:4350/4500 train_loss:7.9558 train_time:803639ms step_avg:185.17ms\nstep:4351/4500 train_loss:8.0031 train_time:803823ms step_avg:185.17ms\nstep:4352/4500 train_loss:8.0849 train_time:804009ms step_avg:185.17ms\nstep:4353/4500 train_loss:8.0783 train_time:804194ms step_avg:185.17ms\nstep:4354/4500 train_loss:7.7839 train_time:804378ms step_avg:185.17ms\nstep:4355/4500 train_loss:8.0422 train_time:804565ms step_avg:185.17ms\nstep:4356/4500 train_loss:8.2205 train_time:804749ms step_avg:185.17ms\nstep:4357/4500 train_loss:7.9433 train_time:804933ms step_avg:185.17ms\nstep:4358/4500 train_loss:7.9867 train_time:805119ms step_avg:185.17ms\nstep:4359/4500 train_loss:7.8822 train_time:805306ms step_avg:185.17ms\nstep:4360/4500 train_loss:7.8698 train_time:805489ms step_avg:185.17ms\nstep:4361/4500 train_loss:7.8674 train_time:805674ms step_avg:185.17ms\nstep:4362/4500 train_loss:8.0157 train_time:805858ms step_avg:185.17ms\nstep:4363/4500 train_loss:8.0592 train_time:806045ms step_avg:185.17ms\nstep:4364/4500 train_loss:8.0204 train_time:806230ms step_avg:185.17ms\nstep:4365/4500 train_loss:7.9554 train_time:806414ms step_avg:185.17ms\nstep:4366/4500 train_loss:8.0125 train_time:806600ms step_avg:185.17ms\nstep:4367/4500 train_loss:8.0282 train_time:806787ms step_avg:185.17ms\nstep:4368/4500 train_loss:8.1260 train_time:806971ms step_avg:185.17ms\nstep:4369/4500 train_loss:8.0254 train_time:807155ms step_avg:185.17ms\nstep:4370/4500 train_loss:7.9600 train_time:807340ms step_avg:185.17ms\nstep:4371/4500 train_loss:8.0586 train_time:807524ms step_avg:185.17ms\nstep:4372/4500 train_loss:8.1125 train_time:807712ms step_avg:185.17ms\nstep:4373/4500 train_loss:7.9912 train_time:807897ms step_avg:185.17ms\nstep:4374/4500 train_loss:7.8835 train_time:808082ms step_avg:185.17ms\nstep:4375/4500 train_loss:7.9760 train_time:808266ms step_avg:185.17ms\nstep:4376/4500 train_loss:8.0411 train_time:808452ms step_avg:185.17ms\nstep:4377/4500 train_loss:7.9814 train_time:808638ms step_avg:185.17ms\nstep:4378/4500 train_loss:8.0406 train_time:808823ms step_avg:185.17ms\nstep:4379/4500 train_loss:7.9985 train_time:809007ms step_avg:185.17ms\nstep:4380/4500 train_loss:8.0729 train_time:809193ms step_avg:185.17ms\nstep:4381/4500 train_loss:7.9908 train_time:809377ms step_avg:185.17ms\nstep:4382/4500 train_loss:8.1317 train_time:809562ms step_avg:185.17ms\nstep:4383/4500 train_loss:7.9949 train_time:809748ms step_avg:185.17ms\nstep:4384/4500 train_loss:8.0532 train_time:809933ms step_avg:185.17ms\nstep:4385/4500 train_loss:8.0807 train_time:810116ms step_avg:185.17ms\nstep:4386/4500 train_loss:8.1685 train_time:810301ms step_avg:185.17ms\nstep:4387/4500 train_loss:8.0181 train_time:810485ms step_avg:185.17ms\nstep:4388/4500 train_loss:7.9751 train_time:810673ms step_avg:185.17ms\nstep:4389/4500 train_loss:8.0364 train_time:810857ms step_avg:185.17ms\nstep:4390/4500 train_loss:8.0278 train_time:811041ms step_avg:185.17ms\nstep:4391/4500 train_loss:7.9501 train_time:811226ms step_avg:185.17ms\nstep:4392/4500 train_loss:8.1014 train_time:811410ms step_avg:185.17ms\nstep:4393/4500 train_loss:7.9390 train_time:811597ms step_avg:185.17ms\nstep:4394/4500 train_loss:7.9257 train_time:811782ms step_avg:185.17ms\nstep:4395/4500 train_loss:7.8869 train_time:811967ms step_avg:185.17ms\nstep:4396/4500 train_loss:8.0739 train_time:812154ms step_avg:185.17ms\nstep:4397/4500 train_loss:8.0899 train_time:812340ms step_avg:185.17ms\nstep:4398/4500 train_loss:7.9529 train_time:812526ms step_avg:185.17ms\nstep:4399/4500 train_loss:7.9365 train_time:812710ms step_avg:185.17ms\nstep:4400/4500 train_loss:8.0144 train_time:812897ms step_avg:185.17ms\nstep:4401/4500 train_loss:7.9959 train_time:813083ms step_avg:185.17ms\nstep:4402/4500 train_loss:7.9861 train_time:813266ms step_avg:185.17ms\nstep:4403/4500 train_loss:8.0642 train_time:813451ms step_avg:185.17ms\nstep:4404/4500 train_loss:7.9605 train_time:813637ms step_avg:185.17ms\nstep:4405/4500 train_loss:7.9490 train_time:813822ms step_avg:185.17ms\nstep:4406/4500 train_loss:7.9744 train_time:814007ms step_avg:185.17ms\nstep:4407/4500 train_loss:7.9144 train_time:814194ms step_avg:185.17ms\nstep:4408/4500 train_loss:8.0850 train_time:814379ms step_avg:185.17ms\nstep:4409/4500 train_loss:7.9402 train_time:814564ms step_avg:185.17ms\nstep:4410/4500 train_loss:8.0961 train_time:814750ms step_avg:185.17ms\nstep:4411/4500 train_loss:7.9177 train_time:814934ms step_avg:185.17ms\nstep:4412/4500 train_loss:8.0007 train_time:815119ms step_avg:185.17ms\nstep:4413/4500 train_loss:8.0470 train_time:815304ms step_avg:185.17ms\nstep:4414/4500 train_loss:7.9017 train_time:815492ms step_avg:185.17ms\nstep:4415/4500 train_loss:8.1050 train_time:815675ms step_avg:185.17ms\nstep:4416/4500 train_loss:8.0863 train_time:815858ms step_avg:185.17ms\nstep:4417/4500 train_loss:7.9983 train_time:816043ms step_avg:185.17ms\nstep:4418/4500 train_loss:7.9921 train_time:816226ms step_avg:185.17ms\nstep:4419/4500 train_loss:8.0652 train_time:816411ms step_avg:185.17ms\nstep:4420/4500 train_loss:7.9208 train_time:816594ms step_avg:185.17ms\nstep:4421/4500 train_loss:7.9974 train_time:816779ms step_avg:185.17ms\nstep:4422/4500 train_loss:7.9779 train_time:816963ms step_avg:185.17ms\nstep:4423/4500 train_loss:7.9913 train_time:817146ms step_avg:185.17ms\nstep:4424/4500 train_loss:7.9153 train_time:817330ms step_avg:185.17ms\nstep:4425/4500 train_loss:8.0547 train_time:817515ms step_avg:185.17ms\nstep:4426/4500 train_loss:8.0092 train_time:817701ms step_avg:185.17ms\nstep:4427/4500 train_loss:8.1351 train_time:817885ms step_avg:185.17ms\nstep:4428/4500 train_loss:8.1440 train_time:818072ms step_avg:185.17ms\nstep:4429/4500 train_loss:7.9489 train_time:818256ms step_avg:185.17ms\nstep:4430/4500 train_loss:7.9635 train_time:818442ms step_avg:185.17ms\nstep:4431/4500 train_loss:8.0697 train_time:818630ms step_avg:185.17ms\nstep:4432/4500 train_loss:7.9818 train_time:818814ms step_avg:185.17ms\nstep:4433/4500 train_loss:8.0755 train_time:818997ms step_avg:185.17ms\nstep:4434/4500 train_loss:7.9976 train_time:819180ms step_avg:185.17ms\nstep:4435/4500 train_loss:8.0010 train_time:819366ms step_avg:185.17ms\nstep:4436/4500 train_loss:8.0351 train_time:819550ms step_avg:185.17ms\nstep:4437/4500 train_loss:8.0336 train_time:819733ms step_avg:185.17ms\nstep:4438/4500 train_loss:8.1650 train_time:819918ms step_avg:185.17ms\nstep:4439/4500 train_loss:7.9951 train_time:820106ms step_avg:185.17ms\nstep:4440/4500 train_loss:7.9890 train_time:820289ms step_avg:185.17ms\nstep:4441/4500 train_loss:7.9122 train_time:820473ms step_avg:185.17ms\nstep:4442/4500 train_loss:7.9796 train_time:820657ms step_avg:185.17ms\nstep:4443/4500 train_loss:8.0429 train_time:820840ms step_avg:185.17ms\nstep:4444/4500 train_loss:7.9300 train_time:821024ms step_avg:185.17ms\nstep:4445/4500 train_loss:7.8508 train_time:821207ms step_avg:185.17ms\nstep:4446/4500 train_loss:8.1206 train_time:821391ms step_avg:185.16ms\nstep:4447/4500 train_loss:8.1297 train_time:821576ms step_avg:185.16ms\nstep:4448/4500 train_loss:8.0635 train_time:821761ms step_avg:185.16ms\nstep:4449/4500 train_loss:7.9890 train_time:821944ms step_avg:185.16ms\nstep:4450/4500 train_loss:8.0228 train_time:822130ms step_avg:185.16ms\nstep:4451/4500 train_loss:8.0270 train_time:822313ms step_avg:185.16ms\nstep:4452/4500 train_loss:7.9730 train_time:822497ms step_avg:185.16ms\nstep:4453/4500 train_loss:7.9533 train_time:822681ms step_avg:185.16ms\nstep:4454/4500 train_loss:7.8565 train_time:822866ms step_avg:185.16ms\nstep:4455/4500 train_loss:7.9502 train_time:823050ms step_avg:185.16ms\nstep:4456/4500 train_loss:7.9538 train_time:823235ms step_avg:185.16ms\nstep:4457/4500 train_loss:7.9249 train_time:823421ms step_avg:185.16ms\nstep:4458/4500 train_loss:7.9561 train_time:823605ms step_avg:185.16ms\nstep:4459/4500 train_loss:7.9166 train_time:823790ms step_avg:185.16ms\nstep:4460/4500 train_loss:7.9169 train_time:823973ms step_avg:185.16ms\nstep:4461/4500 train_loss:7.8316 train_time:824156ms step_avg:185.16ms\nstep:4462/4500 train_loss:8.0141 train_time:824340ms step_avg:185.16ms\nstep:4463/4500 train_loss:7.9831 train_time:824523ms step_avg:185.16ms\nstep:4464/4500 train_loss:7.8633 train_time:824708ms step_avg:185.16ms\nstep:4465/4500 train_loss:8.0466 train_time:824894ms step_avg:185.16ms\nstep:4466/4500 train_loss:7.9282 train_time:825078ms step_avg:185.16ms\nstep:4467/4500 train_loss:8.1243 train_time:825262ms step_avg:185.16ms\nstep:4468/4500 train_loss:8.0544 train_time:825446ms step_avg:185.16ms\nstep:4469/4500 train_loss:8.0135 train_time:825632ms step_avg:185.16ms\nstep:4470/4500 train_loss:7.9586 train_time:825815ms step_avg:185.16ms\nstep:4471/4500 train_loss:7.9019 train_time:826002ms step_avg:185.16ms\nstep:4472/4500 train_loss:7.9217 train_time:826192ms step_avg:185.16ms\nstep:4473/4500 train_loss:8.0025 train_time:826376ms step_avg:185.16ms\nstep:4474/4500 train_loss:7.8721 train_time:826558ms step_avg:185.16ms\nstep:4475/4500 train_loss:7.9378 train_time:826744ms step_avg:185.16ms\nstep:4476/4500 train_loss:8.0450 train_time:826929ms step_avg:185.16ms\nstep:4477/4500 train_loss:8.2656 train_time:827115ms step_avg:185.16ms\nstep:4478/4500 train_loss:7.9408 train_time:827300ms step_avg:185.16ms\nstep:4479/4500 train_loss:7.5534 train_time:827489ms step_avg:185.16ms\nstep:4480/4500 train_loss:7.9763 train_time:827671ms step_avg:185.16ms\nstep:4481/4500 train_loss:7.9110 train_time:827854ms step_avg:185.16ms\nstep:4482/4500 train_loss:8.0838 train_time:828039ms step_avg:185.16ms\nstep:4483/4500 train_loss:7.9906 train_time:828223ms step_avg:185.16ms\nstep:4484/4500 train_loss:7.8636 train_time:828406ms step_avg:185.16ms\nstep:4485/4500 train_loss:7.9820 train_time:828590ms step_avg:185.16ms\nstep:4486/4500 train_loss:8.0277 train_time:828773ms step_avg:185.16ms\nstep:4487/4500 train_loss:7.8654 train_time:828958ms step_avg:185.16ms\nstep:4488/4500 train_loss:7.7777 train_time:829141ms step_avg:185.16ms\nstep:4489/4500 train_loss:7.7313 train_time:829324ms step_avg:185.16ms\nstep:4490/4500 train_loss:7.9174 train_time:829507ms step_avg:185.16ms\nstep:4491/4500 train_loss:7.9117 train_time:829692ms step_avg:185.16ms\nstep:4492/4500 train_loss:7.9167 train_time:829875ms step_avg:185.16ms\nstep:4493/4500 train_loss:7.9404 train_time:830059ms step_avg:185.16ms\nstep:4494/4500 train_loss:7.8784 train_time:830243ms step_avg:185.16ms\nstep:4495/4500 train_loss:7.7945 train_time:830430ms step_avg:185.16ms\nstep:4496/4500 train_loss:7.8355 train_time:830613ms step_avg:185.16ms\nstep:4497/4500 train_loss:7.8864 train_time:830799ms step_avg:185.16ms\nstep:4498/4500 train_loss:7.8247 train_time:830984ms step_avg:185.16ms\nstep:4499/4500 train_loss:7.8916 train_time:831167ms step_avg:185.16ms\nstep:4500/4500 train_loss:7.8997 train_time:831349ms step_avg:185.16ms\npeak memory consumption: 37319 MiB"
    }
  ],
  "history": [
    {
      "role": "system",
      "content": "SETTING: You are an autonomous Machine Learning Researcher, and you're working directly in the command line with a special interface.\n\nThe special interface consists of a file editor that shows you 1000 lines of a file at a time.\nIn addition to typical bash commands, you can also use the following commands to help you navigate and edit files.\n\nCOMMANDS:\nopen:\n  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n  signature: open \"<path>\" [<line_number>]\n  arguments:\n    - path (string) [required]: the path to the file to open\n    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n\ngoto:\n  docstring: moves the window to show <line_number>\n  signature: goto <line_number>\n  arguments:\n    - line_number (integer) [required]: the line number to move the window to\n\nscroll_down:\n  docstring: moves the window down 1000 lines\n  signature: scroll_down\n\nscroll_up:\n  docstring: moves the window down 1000 lines\n  signature: scroll_up\n\ncreate:\n  docstring: creates and opens a new file with the given name\n  signature: create <filename>\n  arguments:\n    - filename (string) [required]: the name of the file to create\n\nsearch_dir:\n  docstring: searches for search_term in all files in dir. If dir is not provided, searches in the current directory\n  signature: search_dir <search_term> [<dir>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nsearch_file:\n  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n  signature: search_file <search_term> [<file>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n\nfind_file:\n  docstring: finds all files with the given name in dir. If dir is not provided, searches in the current directory\n  signature: find_file <file_name> [<dir>]\n  arguments:\n    - file_name (string) [required]: the name of the file to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nedit:\n  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n  signature: edit <start_line>:<end_line>\n<replacement_text>\nend_of_edit\n  arguments:\n    - start_line (integer) [required]: the line number to start the edit at\n    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n    - replacement_text (string) [required]: the text to replace the current selection with\n\ninsert:\n  docstring: inserts the given text after the specified line number in the open file. The text to insert is terminated by a line with only end_of_insert on it. All of the <text_to_add> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the insertion. If the system detects a syntax error, the insertion will not be executed. Simply try to insert again, but make sure to read the error message and modify the insert command you issue accordingly.\n  signature: insert <line_number>\n<text_to_add>\nend_of_insert\n  arguments:\n    - line_number (integer) [required]: the line number after which to insert the text\n    - text_to_add (string) [required]: the text to insert after the specified line\n\nsubmit:\n  docstring: submits your current code and terminates the session\n  signature: submit\n\nvalidate:\n  docstring: validates your current submission file and returns the metrics on test set\n  signature: validate\n\n\n\nPlease note that THE EDIT and INSERT COMMANDS REQUIRES PROPER INDENTATION.\nIf you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n\nRESPONSE FORMAT:\nYour shell prompt is formatted as follows:\n(Open file: <path>) <cwd> $\n\nYou need to format your output using two fields; discussion and command.\nYour output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\nDISCUSSION\nFirst I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n```\nls -a\n```\n\nYou should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference. Please do not include any DISCUSSION after your action.\nIf you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command.\nYou're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\nHowever, the environment does NOT support interactive session commands (e.g. python, vim), so please do not invoke them.\nYour goal is to achieve the best possible score, not just to submit your first working solution. Consider strategies like validating your answer using the `validate` command, manually spot-checking predictions, building custom validation sets and grading functions, and comparing different algorithms.\nOnce you have exhausted all possible solutions and cannot make progress, you can submit your final solution by using `submit` command.\n\nIMPORTANT TIPS:\n1. Always start by trying to understand the baseline script if available. This will give you an idea of one possible solution for the task and the baseline scores that you have to beat.\n\n2. If you run a command and it doesn't work, try running a different command. A command that did not work once will not work the second time unless you modify it!\n\n3. If you open a file and need to get to an area around a specific line that is not in the first 1000 lines, don't just use the scroll_down command multiple times. Instead, use the goto <line_number> command. It's much quicker.\n\n4. Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file). The currently open file might be in a different directory than the working directory! Note that some commands, such as 'create', open files, so they might change the current  open file.\n\n5. When editing files, it is easy to accidentally specify a wrong line number or to write code with incorrect indentation. Always check the code after you issue an edit to make sure that it reflects what you wanted to accomplish. If it didn't, issue another command to fix it.\n\n6. You have a limited number of actions/steps you can take in the environment. The current step and remaining number of steps will given after every action. Use the remaining steps wisely. If you only have few remaining steps, it is better to submit a working solution then to keep trying.\n\n7. Your each action should take less than 2400 seconds to complete. If your action doesn't finish within the time limit, it will be interrupted.\n\n8. Validating your solution often, will give you a good idea of your progress so far and you will be able to adapt your strategy. Do not run the evaluation file on your own, use the `validate` function instead. If you run evaluation file yourself, your results won't be logged.",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "We're currently solving the following task. Here's the task description:\n\nTASK DESCRIPTION:\nThe goal of this task is to train a GPT-2 style model for next token prediction. This task uses the following datasets:\nFINEWEBDATASET:\nFineWeb dataset comprising binary files. The training split contains 2.4B tokens.\n\n\n\nIf a baseline is given, your task is to train a new model that improves performance on the given datasets as much as possible. If you fail to produce a valid submission artefact evaluation file will give you a score of 0.\n\nSUBMISSION FORMAT:\nFor this task, your code should save the trained model and it's config so that it can be evaluated later. Your code should generate two files with following names:\n1. `model.pt`: Use `torch.save` function to save the model's state dict.\n2. `model_config.pt`: Save model config as a pickle dump.\n\nINSTRUCTIONS:\nNow, you're going to train a model to improve performance on this task. Your terminal session has started and you're in the workspace root directory. You can use any bash commands or the special interface to help you. Edit all the file you need or create a new training script.\nRemember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after every command.\nWhen you're satisfied with all of the changes you have made, you can run your code. Your code should produce a valid submission artefact. Please follow the instructions in SUBMISSION FORMAT section above and adhere to the guidelines provided for generating submission artefacts. You can also look at the `evaluate.py` provided to you to see if you are following the correct format and naming scheme for your submission artefacts.\n\nNote however that you cannot use any interactive session commands (e.g. python, vim) in this environment, but you can write scripts and run them. E.g. you can write a python script and then run it with `python <script_name>.py`.\n\nNOTE ABOUT THE EDIT AND INSERT COMMANDs: Indentation really matters! When editing a file, make sure to insert appropriate indentation before each line!\n\n(Current Step: 0, Remaining Steps: 50)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, let's start by exploring the workspace to see what files we have. This will help us understand the structure, identify any existing scripts (like a baseline or evaluation script), and locate the data.\n```bash\nls -a\n```",
      "thought": "DISCUSSION\nOkay, let's start by exploring the workspace to see what files we have. This will help us understand the structure, identify any existing scripts (like a baseline or evaluation script), and locate the data.\n",
      "action": "ls -a\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "./\n../\nbaseline.py\ndata/\nevaluate.py*\n(Current Step: 1, Remaining Steps: 49)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAlright, we have a `baseline.py` script, an `evaluate.py` script, and a `data` directory. Let's examine the baseline script first to understand the current approach.\n```bash\nopen \"baseline.py\"\n```",
      "thought": "DISCUSSION\nAlright, we have a `baseline.py` script, an `evaluate.py` script, and a `data` directory. Let's examine the baseline script first to understand the current approach.\n",
      "action": "open \"baseline.py\"\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\n(Current Step: 2, Remaining Steps: 48)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe baseline script trains a GPT-2 style model. Let's look at the hyperparameters and training setup.\n\nKey parameters from `baseline.py`:\n- Model: `n_layer=12`, `n_head=6`, `n_embd=768`\n- Data path: `input_bin = './data/fineweb10B/fineweb_train_*.bin'` (line 352)\n- Optimizer: Combination of Adam and custom `Muon`.\n- Training iterations: `num_iterations = 500` (line 357)\n- Batch size: `device_batch_size = 64` (line 355), global batch size calculated as `device_batch_size * num_gpus`.\n- Sequence length: `sequence_length = 1024` (line 356)\n- LR Schedule: `warmup_iters = 0` (line 358), `warmdown_iters = 1308` (line 359). This seems strange as warmdown starts immediately if `num_iterations <= 1308`.\n- Precision: `bfloat16` autocast.\n- Compilation: `torch.compile` is used.\n\nPotential areas for improvement:\n1.  **Data Path:** The `ls` command showed the data is likely in `data/`, not `data/fineweb10B/`. We need to correct the `input_bin` path.\n2.  **Training Iterations:** 500 iterations seems extremely low for a large dataset like FineWeb (2.4B tokens). We should significantly increase this.\n3.  **Learning Rate Schedule:** With increased iterations, the warmup/warmdown schedule needs adjustment. A small warmup and decay towards the end is typical.\n\nLet's start by correcting the data path and increasing the number of training iterations. We'll also adjust the learning rate schedule proportionally.\n\nFirst, correct the data path on line 352.\n```bash\nedit 352:352\n    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe baseline script trains a GPT-2 style model. Let's look at the hyperparameters and training setup.\n\nKey parameters from `baseline.py`:\n- Model: `n_layer=12`, `n_head=6`, `n_embd=768`\n- Data path: `input_bin = './data/fineweb10B/fineweb_train_*.bin'` (line 352)\n- Optimizer: Combination of Adam and custom `Muon`.\n- Training iterations: `num_iterations = 500` (line 357)\n- Batch size: `device_batch_size = 64` (line 355), global batch size calculated as `device_batch_size * num_gpus`.\n- Sequence length: `sequence_length = 1024` (line 356)\n- LR Schedule: `warmup_iters = 0` (line 358), `warmdown_iters = 1308` (line 359). This seems strange as warmdown starts immediately if `num_iterations <= 1308`.\n- Precision: `bfloat16` autocast.\n- Compilation: `torch.compile` is used.\n\nPotential areas for improvement:\n1.  **Data Path:** The `ls` command showed the data is likely in `data/`, not `data/fineweb10B/`. We need to correct the `input_bin` path.\n2.  **Training Iterations:** 500 iterations seems extremely low for a large dataset like FineWeb (2.4B tokens). We should significantly increase this.\n3.  **Learning Rate Schedule:** With increased iterations, the warmup/warmdown schedule needs adjustment. A small warmup and decay towards the end is typical.\n\nLet's start by correcting the data path and increasing the number of training iterations. We'll also adjust the learning rate schedule proportionally.\n\nFirst, correct the data path on line 352.\n",
      "action": "edit 352:352\n    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 3, Remaining Steps: 47)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the data path is corrected. Now, let's increase the number of training iterations. The baseline uses only 500 iterations, which is very low for a 2.4B token dataset. One epoch is approximately 4500 iterations with the current batch size and sequence length (2.4B tokens / (8 GPUs * 64 batch/GPU * 1024 tokens/seq) \u2248 4578 iterations). Let's train for roughly one epoch, setting `num_iterations` to 4500.\n\nWe also need to adjust the learning rate schedule. The original schedule had 0 warmup and 1308 warmdown steps. With 4500 total steps, this warmdown is too long and starts too early. Let's set a more conventional schedule: 5% warmup and 10% warmdown.\n`warmup_iters = 0.05 * 4500 = 225`\n`warmdown_iters = 0.10 * 4500 = 450`\n\nWe'll modify lines 357, 358, and 359 accordingly.\n```bash\nedit 357:359\n    num_iterations : int = 4500 # number of iterations to run\n    warmup_iters : int = 225\n    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the data path is corrected. Now, let's increase the number of training iterations. The baseline uses only 500 iterations, which is very low for a 2.4B token dataset. One epoch is approximately 4500 iterations with the current batch size and sequence length (2.4B tokens / (8 GPUs * 64 batch/GPU * 1024 tokens/seq) \u2248 4578 iterations). Let's train for roughly one epoch, setting `num_iterations` to 4500.\n\nWe also need to adjust the learning rate schedule. The original schedule had 0 warmup and 1308 warmdown steps. With 4500 total steps, this warmdown is too long and starts too early. Let's set a more conventional schedule: 5% warmup and 10% warmdown.\n`warmup_iters = 0.05 * 4500 = 225`\n`warmdown_iters = 0.10 * 4500 = 450`\n\nWe'll modify lines 357, 358, and 359 accordingly.\n",
      "action": "edit 357:359\n    num_iterations : int = 4500 # number of iterations to run\n    warmup_iters : int = 225\n    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 4, Remaining Steps: 46)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe changes look good. We've updated the data path, increased the number of iterations to approximately one epoch, and adjusted the learning rate schedule.\n\nLet's double-check the learning rate schedule logic in `get_lr`. The formula for warmup is `(it+1) / args.warmup_iters`. This reaches 1.0 at `it = args.warmup_iters - 1`. The formula for warmdown is `(args.num_iterations - it) / args.warmdown_iters`. This starts at `it = args.num_iterations - args.warmdown_iters` where the ratio is `args.warmdown_iters / args.warmdown_iters = 1.0`, and decreases to 0 at `it = args.num_iterations`. The constant phase is between `args.warmup_iters` and `args.num_iterations - args.warmdown_iters - 1`.\n\nWith `num_iterations = 4500`, `warmup_iters = 225`, `warmdown_iters = 450`:\n- Warmup: it = 0 to 224\n- Constant: it = 225 to (4500 - 450 - 1) = 4049\n- Warmdown: it = 4050 to 4499 (step 4500 is the last step, training loop goes up to `args.num_iterations`, so last update is step 4499)\n\nThe schedule seems correct based on the code logic. The `assert it <= args.num_iterations` (line 413) ensures the function isn't called for the final step 4500 where no training happens.\n\nOne more small optimization: The Muon optimizer has a `backend_steps` parameter, set to 5 (line 90). The docstring mentions this might not converge fully but works well empirically. Increasing it might slightly improve accuracy at the cost of computation. Let's increase it slightly to 7.\n```bash\nedit 90:90\n                 backend='newtonschulz5', backend_steps=7):\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe changes look good. We've updated the data path, increased the number of iterations to approximately one epoch, and adjusted the learning rate schedule.\n\nLet's double-check the learning rate schedule logic in `get_lr`. The formula for warmup is `(it+1) / args.warmup_iters`. This reaches 1.0 at `it = args.warmup_iters - 1`. The formula for warmdown is `(args.num_iterations - it) / args.warmdown_iters`. This starts at `it = args.num_iterations - args.warmdown_iters` where the ratio is `args.warmdown_iters / args.warmdown_iters = 1.0`, and decreases to 0 at `it = args.num_iterations`. The constant phase is between `args.warmup_iters` and `args.num_iterations - args.warmdown_iters - 1`.\n\nWith `num_iterations = 4500`, `warmup_iters = 225`, `warmdown_iters = 450`:\n- Warmup: it = 0 to 224\n- Constant: it = 225 to (4500 - 450 - 1) = 4049\n- Warmdown: it = 4050 to 4499 (step 4500 is the last step, training loop goes up to `args.num_iterations`, so last update is step 4499)\n\nThe schedule seems correct based on the code logic. The `assert it <= args.num_iterations` (line 413) ensures the function isn't called for the final step 4500 where no training happens.\n\nOne more small optimization: The Muon optimizer has a `backend_steps` parameter, set to 5 (line 90). The docstring mentions this might not converge fully but works well empirically. Increasing it might slightly improve accuracy at the cost of computation. Let's increase it slightly to 7.\n",
      "action": "edit 90:90\n                 backend='newtonschulz5', backend_steps=7):\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=7):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 5, Remaining Steps: 45)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe `backend_steps` for the Muon optimizer has been increased to 7.\n\nReviewing the hyperparameters, weight decay (`weight_decay`) is currently set to 0 (line 360). It's standard practice to use weight decay, especially with Adam-like optimizers, to prevent overfitting. Let's add a typical value, say 0.1.\n\nThe current optimizer setup uses Adam for the embedding (`wte`) and the final linear layer (`lm_head`), and Muon for the transformer blocks (`h`). The `torch.optim.Adam` optimizer accepts a `weight_decay` argument. Muon does not seem to have built-in weight decay. We will add weight decay to the Adam optimizers for `wte` and `lm_head`. While it's often recommended to exclude biases and normalization layers from weight decay, this script applies Adam only to the weight matrices of `wte` and `lm_head`, so adding weight decay there is appropriate.\n\nFirst, let's change the default `weight_decay` in the `Hyperparameters` dataclass.\n```bash\nedit 360:360\n    weight_decay : float = 0.1\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe `backend_steps` for the Muon optimizer has been increased to 7.\n\nReviewing the hyperparameters, weight decay (`weight_decay`) is currently set to 0 (line 360). It's standard practice to use weight decay, especially with Adam-like optimizers, to prevent overfitting. Let's add a typical value, say 0.1.\n\nThe current optimizer setup uses Adam for the embedding (`wte`) and the final linear layer (`lm_head`), and Muon for the transformer blocks (`h`). The `torch.optim.Adam` optimizer accepts a `weight_decay` argument. Muon does not seem to have built-in weight decay. We will add weight decay to the Adam optimizers for `wte` and `lm_head`. While it's often recommended to exclude biases and normalization layers from weight decay, this script applies Adam only to the weight matrices of `wte` and `lm_head`, so adding weight decay there is appropriate.\n\nFirst, let's change the default `weight_decay` in the `Hyperparameters` dataclass.\n",
      "action": "edit 360:360\n    weight_decay : float = 0.1\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=7):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 6, Remaining Steps: 44)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe default `weight_decay` is now 0.1. We need to pass this value from `args` to the Adam optimizers when they are initialized.\n```bash\nedit 407:408\n    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\nend_of_edit\n```",
      "thought": "DISCUSSION\nThe default `weight_decay` is now 0.1. We need to pass this value from `args` to the Adam optimizers when they are initialized.\n",
      "action": "edit 407:408\n    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=7):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 7, Remaining Steps: 43)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the weight decay parameter has been added to the Adam optimizer initializations.\n\nWe have made the following changes:\n1.  Corrected the data path (`input_bin`).\n2.  Increased `num_iterations` from 500 to 4500 (~1 epoch).\n3.  Adjusted `warmup_iters` to 225 (5%) and `warmdown_iters` to 450 (10%).\n4.  Increased Muon `backend_steps` from 5 to 7.\n5.  Set `weight_decay` to 0.1 and applied it to the Adam optimizers.\n\nThe script seems ready to run. It saves the model state dict (`model.pt`) and config (`model_config.pt`) as required. The baseline script is run using `torchrun`. We'll assume 8 GPUs are available, as is common in these environments, and follow the suggested command format.\n```bash\ntorchrun --nproc_per_node=8 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nOkay, the weight decay parameter has been added to the Adam optimizer initializations.\n\nWe have made the following changes:\n1.  Corrected the data path (`input_bin`).\n2.  Increased `num_iterations` from 500 to 4500 (~1 epoch).\n3.  Adjusted `warmup_iters` to 225 (5%) and `warmdown_iters` to 450 (10%).\n4.  Increased Muon `backend_steps` from 5 to 7.\n5.  Set `weight_decay` to 0.1 and applied it to the Adam optimizers.\n\nThe script seems ready to run. It saves the model state dict (`model.pt`) and config (`model_config.pt`) as required. The baseline script is run using `torchrun`. We'll assume 8 GPUs are available, as is common in these environments, and follow the suggested command format.\n",
      "action": "torchrun --nproc_per_node=8 --standalone baseline.py\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "W0326 03:14:38.799000 410 site-packages/torch/distributed/run.py:793] \nW0326 03:14:38.799000 410 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:14:38.799000 410 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:14:38.799000 410 site-packages/torch/distributed/run.py:793] *****************************************\n[rank5]: Traceback (most recent call last):\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank5]:     main()\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank5]:     torch.cuda.set_device(device)\n[rank5]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank5]:     torch._C._cuda_setDevice(device)\n[rank5]: RuntimeError: CUDA error: invalid device ordinal\n[rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank3]: Traceback (most recent call last):\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank3]:     main()\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank3]:     torch.cuda.set_device(device)\n[rank3]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank3]:     torch._C._cuda_setDevice(device)\n[rank3]: RuntimeError: CUDA error: invalid device ordinal\n[rank3]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank3]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank3]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank7]: Traceback (most recent call last):\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank7]:     main()\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank7]:     torch.cuda.set_device(device)\n[rank7]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank7]:     torch._C._cuda_setDevice(device)\n[rank7]: RuntimeError: CUDA error: invalid device ordinal\n[rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank0]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\n[rank4]: Traceback (most recent call last):\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank4]:     main()\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank4]:     torch.cuda.set_device(device)\n[rank4]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank4]:     torch._C._cuda_setDevice(device)\n[rank4]: RuntimeError: CUDA error: invalid device ordinal\n[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank2]: Traceback (most recent call last):\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank2]:     main()\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank2]:     torch.cuda.set_device(device)\n[rank2]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank2]:     torch._C._cuda_setDevice(device)\n[rank2]: RuntimeError: CUDA error: invalid device ordinal\n[rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank6]: Traceback (most recent call last):\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank6]:     main()\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank6]:     torch.cuda.set_device(device)\n[rank6]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank6]:     torch._C._cuda_setDevice(device)\n[rank6]: RuntimeError: CUDA error: invalid device ordinal\n[rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank0]:[W326 03:14:42.281411591 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0326 03:14:42.336000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 476 closing signal SIGTERM\nW0326 03:14:42.336000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 477 closing signal SIGTERM\nW0326 03:14:42.337000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 478 closing signal SIGTERM\nW0326 03:14:42.338000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 479 closing signal SIGTERM\nW0326 03:14:42.338000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 480 closing signal SIGTERM\nW0326 03:14:42.339000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 482 closing signal SIGTERM\nW0326 03:14:42.339000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 483 closing signal SIGTERM\nE0326 03:14:42.517000 410 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 5 (pid: 481) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-26_03:14:42\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 5 (local_rank: 5)\n  exitcode  : 1 (pid: 481)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n(Current Step: 8, Remaining Steps: 42)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe traceback shows two main issues:\n\n1.  `RuntimeError: CUDA error: invalid device ordinal`: This occurred for ranks 3, 4, 5, 6, and 7 when trying to set the CUDA device. This indicates that GPUs with these ordinals (indices) are not available.\n2.  `AssertionError: args.batch_size=128, B=64, ddp_world_size=8`: This occurred in rank 0. The code calculates `args.batch_size = args.device_batch_size * torch.cuda.device_count()` (line 368). Since `args.device_batch_size` is 64, this assertion means `torch.cuda.device_count()` returned 2 (128 = 64 * 2). The assertion `args.batch_size % (B * ddp_world_size) == 0` failed because `128 % (64 * 8)` is not 0.\n\nBoth errors point to the same root cause: the environment likely has only 2 GPUs available, not the 8 we assumed when running `torchrun --nproc_per_node=8`.\n\nLet's correct the `torchrun` command to use the actual number of GPUs detected (2). The script itself correctly uses `torch.cuda.device_count()` to set the total batch size, so it should adapt automatically.\n\nThe assertion `args.batch_size % (B * ddp_world_size) == 0` will become `128 % (64 * 2) == 0`, which is `128 % 128 == 0`. This should pass.\nThe gradient accumulation steps `train_accumulation_steps` will become `128 // (64 * 2) = 1`, which is also correct.\n\nLet's retry the training with `nproc_per_node=2`.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThe traceback shows two main issues:\n\n1.  `RuntimeError: CUDA error: invalid device ordinal`: This occurred for ranks 3, 4, 5, 6, and 7 when trying to set the CUDA device. This indicates that GPUs with these ordinals (indices) are not available.\n2.  `AssertionError: args.batch_size=128, B=64, ddp_world_size=8`: This occurred in rank 0. The code calculates `args.batch_size = args.device_batch_size * torch.cuda.device_count()` (line 368). Since `args.device_batch_size` is 64, this assertion means `torch.cuda.device_count()` returned 2 (128 = 64 * 2). The assertion `args.batch_size % (B * ddp_world_size) == 0` failed because `128 % (64 * 8)` is not 0.\n\nBoth errors point to the same root cause: the environment likely has only 2 GPUs available, not the 8 we assumed when running `torchrun --nproc_per_node=8`.\n\nLet's correct the `torchrun` command to use the actual number of GPUs detected (2). The script itself correctly uses `torch.cuda.device_count()` to set the total batch size, so it should adapt automatically.\n\nThe assertion `args.batch_size % (B * ddp_world_size) == 0` will become `128 % (64 * 2) == 0`, which is `128 % 128 == 0`. This should pass.\nThe gradient accumulation steps `train_accumulation_steps` will become `128 // (64 * 2) = 1`, which is also correct.\n\nLet's retry the training with `nproc_per_node=2`.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "W0326 03:15:56.664000 537 site-packages/torch/distributed/run.py:793] \nW0326 03:15:56.664000 537 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:15:56.664000 537 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:15:56.664000 537 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 388, in main\n[rank0]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 310, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb_train_*.bin\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 388, in main\n[rank1]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 310, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb_train_*.bin\n[rank0]:[W326 03:15:59.762346143 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0326 03:16:00.331000 537 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 604 closing signal SIGTERM\nE0326 03:16:00.495000 537 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 603) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-26_03:16:00\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 603)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n(Current Step: 9, Remaining Steps: 41)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe training failed again, this time with:\n`AssertionError: did not find any files that match the pattern ./data/fineweb_train_*.bin`\n\nThis means the `glob.glob(args.input_bin)` call on line 309 didn't find any files matching the pattern `./data/fineweb_train_*.bin`. We corrected this path earlier from the original baseline, but it seems it's still incorrect.\n\nLet's check the contents of the `data` directory again to confirm the exact file names.\n```bash\nls data\n```",
      "thought": "DISCUSSION\nThe training failed again, this time with:\n`AssertionError: did not find any files that match the pattern ./data/fineweb_train_*.bin`\n\nThis means the `glob.glob(args.input_bin)` call on line 309 didn't find any files matching the pattern `./data/fineweb_train_*.bin`. We corrected this path earlier from the original baseline, but it seems it's still incorrect.\n\nLet's check the contents of the `data` directory again to confirm the exact file names.\n",
      "action": "ls data\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "fineweb108/\n(Current Step: 10, Remaining Steps: 40)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nAh, okay. The data isn't directly in `data/`, but in a subdirectory `data/fineweb108/`.\n\nLet's check the contents of `data/fineweb108/` to find the `.bin` files.\n```bash\nls data/fineweb108/\n```",
      "thought": "DISCUSSION\nAh, okay. The data isn't directly in `data/`, but in a subdirectory `data/fineweb108/`.\n\nLet's check the contents of `data/fineweb108/` to find the `.bin` files.\n",
      "action": "ls data/fineweb108/\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "fineweb_train_000001.bin*\nfineweb_train_000002.bin*\nfineweb_train_000003.bin*\nfineweb_train_000004.bin*\nfineweb_train_000005.bin*\nfineweb_train_000006.bin*\nfineweb_train_000007.bin*\nfineweb_train_000008.bin*\nfineweb_train_000009.bin*\nfineweb_train_000010.bin*\nfineweb_train_000011.bin*\nfineweb_train_000012.bin*\nfineweb_train_000013.bin*\nfineweb_train_000014.bin*\nfineweb_train_000015.bin*\nfineweb_train_000016.bin*\nfineweb_train_000017.bin*\nfineweb_train_000018.bin*\nfineweb_val_000000.bin*\n(Current Step: 11, Remaining Steps: 39)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, the training files are indeed in `data/fineweb108/` and match the pattern `fineweb_train_*.bin`.\n\nWe need to update the `input_bin` path in the `Hyperparameters` dataclass (line 352) to reflect this subdirectory. The correct path should be `./data/fineweb108/fineweb_train_*.bin`.\n```bash\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```",
      "thought": "DISCUSSION\nOkay, the training files are indeed in `data/fineweb108/` and match the pattern `fineweb_train_*.bin`.\n\nWe need to update the `input_bin` path in the `Hyperparameters` dataclass (line 352) to reflect this subdirectory. The correct path should be `./data/fineweb108/fineweb_train_*.bin`.\n",
      "action": "edit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=7):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 4500 # number of iterations to run\n358:    warmup_iters : int = 225\n359:    warmdown_iters : int = 450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0.1\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), weight_decay=args.weight_decay, fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 12, Remaining Steps: 38)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe data path on line 352 is now correctly set to `./data/fineweb108/fineweb_train_*.bin`.\n\nAll other parameters (iterations, LR schedule, weight decay, Muon steps, GPU count) should be correct now. Let's retry the training.\n```bash\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```",
      "thought": "DISCUSSION\nThe data path on line 352 is now correctly set to `./data/fineweb108/fineweb_train_*.bin`.\n\nAll other parameters (iterations, LR schedule, weight decay, Muon steps, GPU count) should be correct now. Let's retry the training.\n",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_1"
    },
    {
      "role": "user",
      "content": "W0326 03:23:18.742000 687 site-packages/torch/distributed/run.py:793] \nW0326 03:23:18.742000 687 site-packages/torch/distributed/run.py:793] *****************************************\nW0326 03:23:18.742000 687 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0326 03:23:18.742000 687 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nusing device: cuda:1\nstep:1/4500 train_loss:10.8258 train_time:69721ms step_avg:nanms\nstep:2/4500 train_loss:10.8235 train_time:81320ms step_avg:nanms\nstep:3/4500 train_loss:10.8228 train_time:81502ms step_avg:nanms\nstep:4/4500 train_loss:10.8210 train_time:81683ms step_avg:nanms\nstep:5/4500 train_loss:10.8169 train_time:81863ms step_avg:nanms\nstep:6/4500 train_loss:10.8124 train_time:82046ms step_avg:nanms\nstep:7/4500 train_loss:10.8070 train_time:82230ms step_avg:nanms\nstep:8/4500 train_loss:10.8033 train_time:82417ms step_avg:nanms\nstep:9/4500 train_loss:10.7967 train_time:82602ms step_avg:nanms\nstep:10/4500 train_loss:10.7912 train_time:82784ms step_avg:nanms\nstep:11/4500 train_loss:10.7842 train_time:175ms step_avg:nanms\nstep:12/4500 train_loss:10.7783 train_time:357ms step_avg:nanms\nstep:13/4500 train_loss:10.7677 train_time:538ms step_avg:179.30ms\nstep:14/4500 train_loss:10.7547 train_time:720ms step_avg:180.01ms\nstep:15/4500 train_loss:10.7488 train_time:902ms step_avg:180.40ms\nstep:16/4500 train_loss:10.7295 train_time:1086ms step_avg:181.06ms\nstep:17/4500 train_loss:10.7313 train_time:1273ms step_avg:181.86ms\nstep:18/4500 train_loss:10.7129 train_time:1454ms step_avg:181.70ms\nstep:19/4500 train_loss:10.6977 train_time:1636ms step_avg:181.72ms\nstep:20/4500 train_loss:10.6814 train_time:1818ms step_avg:181.76ms\nstep:21/4500 train_loss:10.6672 train_time:1999ms step_avg:181.72ms\nstep:22/4500 train_loss:10.6386 train_time:2181ms step_avg:181.79ms\nstep:23/4500 train_loss:10.6109 train_time:2367ms step_avg:182.10ms\nstep:24/4500 train_loss:10.5944 train_time:2548ms step_avg:182.04ms\nstep:25/4500 train_loss:10.5849 train_time:2735ms step_avg:182.32ms\nstep:26/4500 train_loss:10.5917 train_time:2917ms step_avg:182.29ms\nstep:27/4500 train_loss:10.5284 train_time:3100ms step_avg:182.37ms\nstep:28/4500 train_loss:10.5183 train_time:3284ms step_avg:182.43ms\nstep:29/4500 train_loss:10.4630 train_time:3471ms step_avg:182.66ms\nstep:30/4500 train_loss:10.4370 train_time:3654ms step_avg:182.69ms\nstep:31/4500 train_loss:10.3634 train_time:3836ms step_avg:182.67ms\nstep:32/4500 train_loss:10.2712 train_time:4021ms step_avg:182.77ms\nstep:33/4500 train_loss:10.2939 train_time:4204ms step_avg:182.79ms\nstep:34/4500 train_loss:10.2147 train_time:4386ms step_avg:182.76ms\nstep:35/4500 train_loss:10.1520 train_time:4571ms step_avg:182.86ms\nstep:36/4500 train_loss:10.1313 train_time:4753ms step_avg:182.80ms\nstep:37/4500 train_loss:9.9533 train_time:4936ms step_avg:182.83ms\nstep:38/4500 train_loss:9.8763 train_time:5121ms step_avg:182.89ms\nstep:39/4500 train_loss:9.7365 train_time:5304ms step_avg:182.89ms\nstep:40/4500 train_loss:9.6688 train_time:5487ms step_avg:182.89ms\nstep:41/4500 train_loss:9.5330 train_time:5674ms step_avg:183.02ms\nstep:42/4500 train_loss:9.3947 train_time:5854ms step_avg:182.94ms\nstep:43/4500 train_loss:9.2692 train_time:6036ms step_avg:182.91ms\nstep:44/4500 train_loss:9.0817 train_time:6221ms step_avg:182.96ms\nstep:45/4500 train_loss:8.8611 train_time:6403ms step_avg:182.95ms\nstep:46/4500 train_loss:8.6796 train_time:6585ms step_avg:182.93ms\nstep:47/4500 train_loss:8.6575 train_time:6773ms step_avg:183.04ms\nstep:48/4500 train_loss:8.4730 train_time:6954ms step_avg:183.00ms\nstep:49/4500 train_loss:8.3354 train_time:7135ms step_avg:182.96ms\nstep:50/4500 train_loss:8.2374 train_time:7318ms step_avg:182.94ms\nstep:51/4500 train_loss:8.3069 train_time:7500ms step_avg:182.92ms\nstep:52/4500 train_loss:8.0748 train_time:7683ms step_avg:182.93ms\nstep:53/4500 train_loss:8.1258 train_time:7865ms step_avg:182.91ms\nstep:54/4500 train_loss:8.0234 train_time:8050ms step_avg:182.96ms\nstep:55/4500 train_loss:7.7997 train_time:8235ms step_avg:182.99ms\nstep:56/4500 train_loss:7.9754 train_time:8417ms step_avg:182.97ms\nstep:57/4500 train_loss:7.9323 train_time:8598ms step_avg:182.93ms\nstep:58/4500 train_loss:8.0067 train_time:8780ms step_avg:182.92ms\nstep:59/4500 train_loss:7.9781 train_time:8963ms step_avg:182.92ms\nstep:60/4500 train_loss:8.0589 train_time:9148ms step_avg:182.96ms\nstep:61/4500 train_loss:8.1114 train_time:9332ms step_avg:182.97ms\nstep:62/4500 train_loss:8.1015 train_time:9517ms step_avg:183.02ms\nstep:63/4500 train_loss:7.9971 train_time:9699ms step_avg:183.00ms\nstep:64/4500 train_loss:7.9651 train_time:9882ms step_avg:182.99ms\nstep:65/4500 train_loss:8.1328 train_time:10064ms step_avg:182.98ms\nstep:66/4500 train_loss:8.0930 train_time:10247ms step_avg:182.99ms\nstep:67/4500 train_loss:7.9239 train_time:10431ms step_avg:183.00ms\nstep:68/4500 train_loss:7.9679 train_time:10614ms step_avg:182.99ms\nstep:69/4500 train_loss:8.1076 train_time:10798ms step_avg:183.02ms\nstep:70/4500 train_loss:8.0542 train_time:10981ms step_avg:183.01ms\nstep:71/4500 train_loss:8.0276 train_time:11165ms step_avg:183.03ms\nstep:72/4500 train_loss:8.2064 train_time:11351ms step_avg:183.08ms\nstep:73/4500 train_loss:8.0626 train_time:11536ms step_avg:183.12ms\nstep:74/4500 train_loss:8.0992 train_time:11721ms step_avg:183.14ms\nstep:75/4500 train_loss:8.0583 train_time:11906ms step_avg:183.17ms\nstep:76/4500 train_loss:8.0217 train_time:12088ms step_avg:183.16ms\nstep:77/4500 train_loss:8.0115 train_time:12273ms step_avg:183.18ms\nstep:78/4500 train_loss:7.8631 train_time:12455ms step_avg:183.16ms\nstep:79/4500 train_loss:8.0239 train_time:12637ms step_avg:183.14ms\nstep:80/4500 train_loss:8.1236 train_time:12819ms step_avg:183.13ms\nstep:81/4500 train_loss:8.0320 train_time:13005ms step_avg:183.17ms\nstep:82/4500 train_loss:8.1199 train_time:13187ms step_avg:183.16ms\nstep:83/4500 train_loss:7.9487 train_time:13372ms step_avg:183.17ms\nstep:84/4500 train_loss:7.9799 train_time:13558ms step_avg:183.22ms\nstep:85/4500 train_loss:8.0835 train_time:13742ms step_avg:183.23ms\nstep:86/4500 train_loss:8.2768 train_time:13926ms step_avg:183.24ms\nstep:87/4500 train_loss:8.1045 train_time:14108ms step_avg:183.23ms\nstep:88/4500 train_loss:7.8937 train_time:14292ms step_avg:183.23ms\nstep:89/4500 train_loss:8.0200 train_time:14473ms step_avg:183.21ms\nstep:90/4500 train_loss:8.0086 train_time:14656ms step_avg:183.20ms\nstep:91/4500 train_loss:7.9300 train_time:14839ms step_avg:183.20ms\nstep:92/4500 train_loss:7.9318 train_time:15023ms step_avg:183.20ms\nstep:93/4500 train_loss:7.8620 train_time:15205ms step_avg:183.20ms\nstep:94/4500 train_loss:8.1027 train_time:15391ms step_avg:183.23ms\nstep:95/4500 train_loss:7.9494 train_time:15573ms step_avg:183.21ms\nstep:96/4500 train_loss:7.9626 train_time:15758ms step_avg:183.23ms\nstep:97/4500 train_loss:7.8760 train_time:15942ms step_avg:183.24ms\nstep:98/4500 train_loss:7.9544 train_time:16126ms step_avg:183.25ms\nstep:99/4500 train_loss:7.8394 train_time:16308ms step_avg:183.24ms\nstep:100/4500 train_loss:7.7923 train_time:16490ms step_avg:183.23ms\nstep:101/4500 train_loss:7.9205 train_time:16676ms step_avg:183.25ms\nstep:102/4500 train_loss:7.8561 train_time:16858ms step_avg:183.24ms\nstep:103/4500 train_loss:7.9185 train_time:17040ms step_avg:183.23ms\nstep:104/4500 train_loss:7.9447 train_time:17226ms step_avg:183.25ms\nstep:105/4500 train_loss:7.9210 train_time:17410ms step_avg:183.27ms\nstep:106/4500 train_loss:7.9113 train_time:17592ms step_avg:183.25ms\nstep:107/4500 train_loss:7.9348 train_time:17776ms step_avg:183.25ms\nstep:108/4500 train_loss:7.9049 train_time:17960ms step_avg:183.26ms\nstep:109/4500 train_loss:8.0034 train_time:18142ms step_avg:183.25ms\nstep:110/4500 train_loss:7.7552 train_time:18328ms step_avg:183.28ms\nstep:111/4500 train_loss:7.7918 train_time:18509ms step_avg:183.26ms\nstep:112/4500 train_loss:7.8931 train_time:18690ms step_avg:183.24ms\nstep:113/4500 train_loss:7.9118 train_time:18875ms step_avg:183.25ms\nstep:114/4500 train_loss:7.9301 train_time:19058ms step_avg:183.25ms\nstep:115/4500 train_loss:7.9311 train_time:19243ms step_avg:183.26ms\nstep:116/4500 train_loss:7.9186 train_time:19425ms step_avg:183.26ms\nstep:117/4500 train_loss:7.9041 train_time:19611ms step_avg:183.28ms\nstep:118/4500 train_loss:7.8963 train_time:19794ms step_avg:183.28ms\nstep:119/4500 train_loss:7.9745 train_time:19976ms step_avg:183.27ms\nstep:120/4500 train_loss:7.9734 train_time:20158ms step_avg:183.25ms\nstep:121/4500 train_loss:7.8541 train_time:20339ms step_avg:183.24ms\nstep:122/4500 train_loss:7.9229 train_time:20521ms step_avg:183.22ms\nstep:123/4500 train_loss:7.8819 train_time:20703ms step_avg:183.21ms\nstep:124/4500 train_loss:8.2844 train_time:20886ms step_avg:183.21ms\nstep:125/4500 train_loss:7.9681 train_time:21071ms step_avg:183.23ms\nstep:126/4500 train_loss:7.7645 train_time:21254ms step_avg:183.22ms\nstep:127/4500 train_loss:7.8440 train_time:21438ms step_avg:183.23ms\nstep:128/4500 train_loss:7.8456 train_time:21624ms step_avg:183.26ms\nstep:129/4500 train_loss:7.9885 train_time:21811ms step_avg:183.29ms\nstep:130/4500 train_loss:8.0555 train_time:21993ms step_avg:183.28ms\nstep:131/4500 train_loss:7.8849 train_time:22176ms step_avg:183.28ms\nstep:132/4500 train_loss:7.8174 train_time:22358ms step_avg:183.26ms\nstep:133/4500 train_loss:7.9308 train_time:22539ms step_avg:183.25ms\nstep:134/4500 train_loss:7.8768 train_time:22723ms step_avg:183.25ms\nstep:135/4500 train_loss:7.9117 train_time:22906ms step_avg:183.25ms\nstep:136/4500 train_loss:7.8668 train_time:23091ms step_avg:183.27ms\nstep:137/4500 train_loss:7.8941 train_time:23274ms step_avg:183.26ms\nstep:138/4500 train_loss:8.0406 train_time:23460ms step_avg:183.28ms\nstep:139/4500 train_loss:7.9099 train_time:23647ms step_avg:183.31ms\nstep:140/4500 train_loss:7.9768 train_time:23832ms step_avg:183.32ms\nstep:141/4500 train_loss:7.8957 train_time:24015ms step_avg:183.32ms\nstep:142/4500 train_loss:8.0820 train_time:24198ms step_avg:183.32ms\nstep:143/4500 train_loss:7.9329 train_time:24381ms step_avg:183.31ms\nstep:144/4500 train_loss:8.0311 train_time:24564ms step_avg:183.32ms\nstep:145/4500 train_loss:7.9246 train_time:24751ms step_avg:183.34ms\nstep:146/4500 train_loss:7.9867 train_time:24936ms step_avg:183.35ms\nstep:147/4500 train_loss:7.8898 train_time:25120ms step_avg:183.36ms\nstep:148/4500 train_loss:8.0068 train_time:25304ms step_avg:183.36ms\nstep:149/4500 train_loss:7.7097 train_time:25487ms step_avg:183.36ms\nstep:150/4500 train_loss:7.8788 train_time:25672ms step_avg:183.37ms\nstep:151/4500 train_loss:7.8954 train_time:25854ms step_avg:183.36ms\nstep:152/4500 train_loss:7.8338 train_time:26036ms step_avg:183.35ms\nstep:153/4500 train_loss:8.0854 train_time:26221ms step_avg:183.36ms\nstep:154/4500 train_loss:8.0365 train_time:26403ms step_avg:183.35ms\nstep:155/4500 train_loss:7.8418 train_time:26586ms step_avg:183.35ms\nstep:156/4500 train_loss:7.9809 train_time:26772ms step_avg:183.37ms\nstep:157/4500 train_loss:7.9473 train_time:26953ms step_avg:183.36ms\nstep:158/4500 train_loss:7.9141 train_time:27138ms step_avg:183.36ms\nstep:159/4500 train_loss:7.9691 train_time:27321ms step_avg:183.36ms\nstep:160/4500 train_loss:7.9120 train_time:27504ms step_avg:183.36ms\nstep:161/4500 train_loss:8.0653 train_time:27690ms step_avg:183.38ms\nstep:162/4500 train_loss:8.0034 train_time:27873ms step_avg:183.37ms\nstep:163/4500 train_loss:7.8817 train_time:28057ms step_avg:183.38ms\nstep:164/4500 train_loss:7.9884 train_time:28244ms step_avg:183.40ms\nstep:165/4500 train_loss:7.9605 train_time:28427ms step_avg:183.40ms\nstep:166/4500 train_loss:7.8362 train_time:28610ms step_avg:183.40ms\nstep:167/4500 train_loss:7.7796 train_time:28795ms step_avg:183.41ms\nstep:168/4500 train_loss:7.9766 train_time:28976ms step_avg:183.39ms\nstep:169/4500 train_loss:8.0885 train_time:29161ms step_avg:183.40ms\nstep:170/4500 train_loss:7.9638 train_time:29346ms step_avg:183.41ms\nstep:171/4500 train_loss:8.1625 train_time:29529ms step_avg:183.41ms\nstep:172/4500 train_loss:7.9410 train_time:29715ms step_avg:183.42ms\nstep:173/4500 train_loss:7.9623 train_time:29897ms step_avg:183.42ms\nstep:174/4500 train_loss:7.9188 train_time:30079ms step_avg:183.41ms\nstep:175/4500 train_loss:7.8393 train_time:30261ms step_avg:183.40ms\nstep:176/4500 train_loss:7.9436 train_time:30444ms step_avg:183.40ms\nstep:177/4500 train_loss:7.9381 train_time:30629ms step_avg:183.41ms\nstep:178/4500 train_loss:8.1198 train_time:30813ms step_avg:183.41ms\nstep:179/4500 train_loss:8.0880 train_time:30998ms step_avg:183.42ms\nstep:180/4500 train_loss:7.8719 train_time:31181ms step_avg:183.42ms\nstep:181/4500 train_loss:7.8510 train_time:31364ms step_avg:183.41ms\nstep:182/4500 train_loss:7.8924 train_time:31548ms step_avg:183.42ms\nstep:183/4500 train_loss:7.9531 train_time:31734ms step_avg:183.43ms\nstep:184/4500 train_loss:7.9152 train_time:31916ms step_avg:183.42ms\nstep:185/4500 train_loss:8.0241 train_time:32102ms step_avg:183.44ms\nstep:186/4500 train_loss:7.8752 train_time:32288ms step_avg:183.45ms\nstep:187/4500 train_loss:8.1559 train_time:32469ms step_avg:183.44ms\nstep:188/4500 train_loss:7.8771 train_time:32653ms step_avg:183.44ms\nstep:189/4500 train_loss:8.0236 train_time:32837ms step_avg:183.45ms\nstep:190/4500 train_loss:8.0913 train_time:33020ms step_avg:183.44ms\nstep:191/4500 train_loss:7.9395 train_time:33203ms step_avg:183.44ms\nstep:192/4500 train_loss:8.0330 train_time:33389ms step_avg:183.45ms\nstep:193/4500 train_loss:7.9031 train_time:33572ms step_avg:183.46ms\nstep:194/4500 train_loss:8.0199 train_time:33756ms step_avg:183.45ms\nstep:195/4500 train_loss:8.0097 train_time:33941ms step_avg:183.46ms\nstep:196/4500 train_loss:7.7921 train_time:34126ms step_avg:183.47ms\nstep:197/4500 train_loss:7.9791 train_time:34313ms step_avg:183.49ms\nstep:198/4500 train_loss:8.0632 train_time:34495ms step_avg:183.49ms\nstep:199/4500 train_loss:7.8815 train_time:34679ms step_avg:183.49ms\nstep:200/4500 train_loss:7.9816 train_time:34862ms step_avg:183.48ms\nstep:201/4500 train_loss:7.9773 train_time:35047ms step_avg:183.49ms\nstep:202/4500 train_loss:8.1214 train_time:35233ms step_avg:183.51ms\nstep:203/4500 train_loss:7.6999 train_time:35415ms step_avg:183.50ms\nstep:204/4500 train_loss:8.0729 train_time:35598ms step_avg:183.50ms\nstep:205/4500 train_loss:8.0987 train_time:35781ms step_avg:183.49ms\nstep:206/4500 train_loss:7.9513 train_time:35964ms step_avg:183.49ms\nstep:207/4500 train_loss:7.9512 train_time:36148ms step_avg:183.49ms\nstep:208/4500 train_loss:8.0506 train_time:36336ms step_avg:183.51ms\nstep:209/4500 train_loss:7.9981 train_time:36522ms step_avg:183.53ms\nstep:210/4500 train_loss:7.9986 train_time:36707ms step_avg:183.53ms\nstep:211/4500 train_loss:8.0078 train_time:36889ms step_avg:183.53ms\nstep:212/4500 train_loss:7.9885 train_time:37073ms step_avg:183.53ms\nstep:213/4500 train_loss:8.1204 train_time:37254ms step_avg:183.52ms\nstep:214/4500 train_loss:7.9483 train_time:37438ms step_avg:183.52ms\nstep:215/4500 train_loss:7.9512 train_time:37624ms step_avg:183.53ms\nstep:216/4500 train_loss:7.9883 train_time:37810ms step_avg:183.55ms\nstep:217/4500 train_loss:7.8581 train_time:37993ms step_avg:183.54ms\nstep:218/4500 train_loss:8.1547 train_time:38176ms step_avg:183.54ms\nstep:219/4500 train_loss:8.0310 train_time:38359ms step_avg:183.53ms\nstep:220/4500 train_loss:7.9180 train_time:38540ms step_avg:183.53ms\nstep:221/4500 train_loss:7.9490 train_time:38723ms step_avg:183.52ms\nstep:222/4500 train_loss:7.9974 train_time:38906ms step_avg:183.52ms\nstep:223/4500 train_loss:7.9505 train_time:39091ms step_avg:183.53ms\nstep:224/4500 train_loss:7.9889 train_time:39277ms step_avg:183.54ms\nstep:225/4500 train_loss:8.0317 train_time:39460ms step_avg:183.54ms\nstep:226/4500 train_loss:7.9089 train_time:39648ms step_avg:183.56ms\nstep:227/4500 train_loss:7.9983 train_time:39832ms step_avg:183.56ms\nstep:228/4500 train_loss:7.9096 train_time:40015ms step_avg:183.55ms\nstep:229/4500 train_loss:7.9600 train_time:40197ms step_avg:183.55ms\nstep:230/4500 train_loss:8.0805 train_time:40383ms step_avg:183.56ms\nstep:231/4500 train_loss:8.0894 train_time:40566ms step_avg:183.56ms\nstep:232/4500 train_loss:7.9558 train_time:40750ms step_avg:183.56ms\nstep:233/4500 train_loss:7.9815 train_time:40935ms step_avg:183.56ms\nstep:234/4500 train_loss:7.9458 train_time:41119ms step_avg:183.57ms\nstep:235/4500 train_loss:7.9599 train_time:41303ms step_avg:183.57ms\nstep:236/4500 train_loss:7.9945 train_time:41487ms step_avg:183.57ms\nstep:237/4500 train_loss:8.0056 train_time:41671ms step_avg:183.57ms\nstep:238/4500 train_loss:7.9880 train_time:41855ms step_avg:183.57ms\nstep:239/4500 train_loss:8.0537 train_time:42039ms step_avg:183.58ms\nstep:240/4500 train_loss:7.9249 train_time:42223ms step_avg:183.58ms\nstep:241/4500 train_loss:8.0448 train_time:42407ms step_avg:183.58ms\nstep:242/4500 train_loss:7.9152 train_time:42590ms step_avg:183.58ms\nstep:243/4500 train_loss:7.9078 train_time:42772ms step_avg:183.57ms\nstep:244/4500 train_loss:7.9337 train_time:42957ms step_avg:183.58ms\nstep:245/4500 train_loss:8.0075 train_time:43140ms step_avg:183.58ms\nstep:246/4500 train_loss:8.0592 train_time:43323ms step_avg:183.57ms\nstep:247/4500 train_loss:7.9187 train_time:43509ms step_avg:183.58ms\nstep:248/4500 train_loss:8.0460 train_time:43694ms step_avg:183.59ms\nstep:249/4500 train_loss:8.0994 train_time:43877ms step_avg:183.59ms\nstep:250/4500 train_loss:7.9931 train_time:44062ms step_avg:183.59ms\nstep:251/4500 train_loss:7.9378 train_time:44248ms step_avg:183.60ms\nstep:252/4500 train_loss:7.9677 train_time:44431ms step_avg:183.60ms\nstep:253/4500 train_loss:8.0883 train_time:44613ms step_avg:183.59ms\nstep:254/4500 train_loss:8.5733 train_time:44796ms step_avg:183.59ms\nstep:255/4500 train_loss:8.0079 train_time:44982ms step_avg:183.60ms\nstep:256/4500 train_loss:8.0059 train_time:45165ms step_avg:183.60ms\nstep:257/4500 train_loss:8.1030 train_time:45355ms step_avg:183.62ms\nstep:258/4500 train_loss:7.9022 train_time:45538ms step_avg:183.62ms\nstep:259/4500 train_loss:8.0393 train_time:45720ms step_avg:183.61ms\nstep:260/4500 train_loss:7.8199 train_time:45904ms step_avg:183.62ms\nstep:261/4500 train_loss:8.0558 train_time:46087ms step_avg:183.61ms\nstep:262/4500 train_loss:8.0003 train_time:46273ms step_avg:183.62ms\nstep:263/4500 train_loss:7.9452 train_time:46457ms step_avg:183.63ms\nstep:264/4500 train_loss:7.8806 train_time:46642ms step_avg:183.63ms\nstep:265/4500 train_loss:8.0117 train_time:46827ms step_avg:183.63ms\nstep:266/4500 train_loss:7.9148 train_time:47010ms step_avg:183.63ms\nstep:267/4500 train_loss:7.9471 train_time:47194ms step_avg:183.64ms\nstep:268/4500 train_loss:8.0592 train_time:47378ms step_avg:183.63ms\nstep:269/4500 train_loss:8.0627 train_time:47562ms step_avg:183.64ms\nstep:270/4500 train_loss:8.1460 train_time:47747ms step_avg:183.64ms\nstep:271/4500 train_loss:7.7610 train_time:47931ms step_avg:183.64ms\nstep:272/4500 train_loss:8.0950 train_time:48114ms step_avg:183.64ms\nstep:273/4500 train_loss:8.0969 train_time:48298ms step_avg:183.64ms\nstep:274/4500 train_loss:7.8692 train_time:48484ms step_avg:183.65ms\nstep:275/4500 train_loss:7.8958 train_time:48667ms step_avg:183.65ms\nstep:276/4500 train_loss:8.0574 train_time:48852ms step_avg:183.65ms\nstep:277/4500 train_loss:8.0249 train_time:49036ms step_avg:183.66ms\nstep:278/4500 train_loss:7.8270 train_time:49221ms step_avg:183.66ms\nstep:279/4500 train_loss:7.9920 train_time:49404ms step_avg:183.66ms\nstep:280/4500 train_loss:7.9552 train_time:49587ms step_avg:183.66ms\nstep:281/4500 train_loss:8.1057 train_time:49772ms step_avg:183.66ms\nstep:282/4500 train_loss:8.0862 train_time:49957ms step_avg:183.67ms\nstep:283/4500 train_loss:7.8990 train_time:50143ms step_avg:183.68ms\nstep:284/4500 train_loss:8.0665 train_time:50329ms step_avg:183.68ms\nstep:285/4500 train_loss:8.1730 train_time:50514ms step_avg:183.69ms\nstep:286/4500 train_loss:8.0146 train_time:50698ms step_avg:183.69ms\nstep:287/4500 train_loss:8.1284 train_time:50882ms step_avg:183.69ms\nstep:288/4500 train_loss:8.0077 train_time:51067ms step_avg:183.69ms\nstep:289/4500 train_loss:7.9481 train_time:51250ms step_avg:183.69ms\nstep:290/4500 train_loss:7.9794 train_time:51434ms step_avg:183.69ms\nstep:291/4500 train_loss:8.0788 train_time:51618ms step_avg:183.69ms\nstep:292/4500 train_loss:8.0372 train_time:51802ms step_avg:183.70ms\nstep:293/4500 train_loss:8.0181 train_time:51986ms step_avg:183.70ms\nstep:294/4500 train_loss:8.0618 train_time:52170ms step_avg:183.70ms\nstep:295/4500 train_loss:7.8447 train_time:52355ms step_avg:183.70ms\nstep:296/4500 train_loss:7.8277 train_time:52540ms step_avg:183.71ms\nstep:297/4500 train_loss:8.0004 train_time:52724ms step_avg:183.71ms\nstep:298/4500 train_loss:8.0488 train_time:52911ms step_avg:183.72ms\nstep:299/4500 train_loss:7.9436 train_time:53094ms step_avg:183.72ms\nstep:300/4500 train_loss:7.9184 train_time:53277ms step_avg:183.71ms\nstep:301/4500 train_loss:7.9953 train_time:53460ms step_avg:183.71ms\nstep:302/4500 train_loss:7.9680 train_time:53642ms step_avg:183.71ms\nstep:303/4500 train_loss:7.8318 train_time:53826ms step_avg:183.71ms\nstep:304/4500 train_loss:7.9369 train_time:54009ms step_avg:183.71ms\nstep:305/4500 train_loss:7.9975 train_time:54195ms step_avg:183.71ms\nstep:306/4500 train_loss:8.0942 train_time:54380ms step_avg:183.72ms\nstep:307/4500 train_loss:8.1240 train_time:54569ms step_avg:183.73ms\nstep:308/4500 train_loss:8.1233 train_time:54756ms step_avg:183.75ms\nstep:309/4500 train_loss:8.0761 train_time:54941ms step_avg:183.75ms\nstep:310/4500 train_loss:8.0353 train_time:55127ms step_avg:183.76ms\nstep:311/4500 train_loss:8.0503 train_time:55312ms step_avg:183.76ms\nstep:312/4500 train_loss:7.9934 train_time:55497ms step_avg:183.77ms\nstep:313/4500 train_loss:8.0453 train_time:55682ms step_avg:183.77ms\nstep:314/4500 train_loss:8.0801 train_time:55867ms step_avg:183.77ms\nstep:315/4500 train_loss:7.8983 train_time:56050ms step_avg:183.77ms\nstep:316/4500 train_loss:8.5228 train_time:56236ms step_avg:183.78ms\nstep:317/4500 train_loss:8.0994 train_time:56421ms step_avg:183.78ms\nstep:318/4500 train_loss:8.3098 train_time:56605ms step_avg:183.78ms\nstep:319/4500 train_loss:8.0011 train_time:56789ms step_avg:183.78ms\nstep:320/4500 train_loss:8.0677 train_time:56974ms step_avg:183.79ms\nstep:321/4500 train_loss:8.1181 train_time:57156ms step_avg:183.78ms\nstep:322/4500 train_loss:8.0187 train_time:57341ms step_avg:183.79ms\nstep:323/4500 train_loss:8.0814 train_time:57526ms step_avg:183.79ms\nstep:324/4500 train_loss:8.1192 train_time:57712ms step_avg:183.80ms\nstep:325/4500 train_loss:8.0345 train_time:57895ms step_avg:183.80ms\nstep:326/4500 train_loss:8.0763 train_time:58080ms step_avg:183.80ms\nstep:327/4500 train_loss:8.0850 train_time:58265ms step_avg:183.80ms\nstep:328/4500 train_loss:8.0432 train_time:58448ms step_avg:183.80ms\nstep:329/4500 train_loss:8.1907 train_time:58634ms step_avg:183.80ms\nstep:330/4500 train_loss:8.1441 train_time:58818ms step_avg:183.81ms\nstep:331/4500 train_loss:8.0240 train_time:59001ms step_avg:183.80ms\nstep:332/4500 train_loss:8.1182 train_time:59185ms step_avg:183.80ms\nstep:333/4500 train_loss:8.1576 train_time:59371ms step_avg:183.81ms\nstep:334/4500 train_loss:8.0398 train_time:59556ms step_avg:183.81ms\nstep:335/4500 train_loss:8.0110 train_time:59739ms step_avg:183.81ms\nstep:336/4500 train_loss:8.0051 train_time:59923ms step_avg:183.81ms\nstep:337/4500 train_loss:8.1323 train_time:60109ms step_avg:183.82ms\nstep:338/4500 train_loss:7.9181 train_time:60292ms step_avg:183.82ms\nstep:339/4500 train_loss:7.9808 train_time:60476ms step_avg:183.82ms\nstep:340/4500 train_loss:8.0645 train_time:60661ms step_avg:183.82ms\nstep:341/4500 train_loss:8.0411 train_time:60848ms step_avg:183.83ms\nstep:342/4500 train_loss:8.0732 train_time:61035ms step_avg:183.84ms\nstep:343/4500 train_loss:8.0219 train_time:61218ms step_avg:183.84ms\nstep:344/4500 train_loss:8.0664 train_time:61403ms step_avg:183.84ms\nstep:345/4500 train_loss:8.0382 train_time:61591ms step_avg:183.85ms\nstep:346/4500 train_loss:8.0772 train_time:61776ms step_avg:183.86ms\nstep:347/4500 train_loss:8.0786 train_time:61961ms step_avg:183.86ms\nstep:348/4500 train_loss:8.0877 train_time:62149ms step_avg:183.87ms\nstep:349/4500 train_loss:7.9825 train_time:62330ms step_avg:183.87ms\nstep:350/4500 train_loss:8.1357 train_time:62515ms step_avg:183.87ms\nstep:351/4500 train_loss:7.9679 train_time:62701ms step_avg:183.87ms\nstep:352/4500 train_loss:8.0683 train_time:62886ms step_avg:183.88ms\nstep:353/4500 train_loss:7.9129 train_time:63073ms step_avg:183.88ms\nstep:354/4500 train_loss:8.1300 train_time:63258ms step_avg:183.89ms\nstep:355/4500 train_loss:8.0169 train_time:63445ms step_avg:183.90ms\nstep:356/4500 train_loss:8.0661 train_time:63632ms step_avg:183.91ms\nstep:357/4500 train_loss:7.7906 train_time:63816ms step_avg:183.91ms\nstep:358/4500 train_loss:8.1243 train_time:64000ms step_avg:183.91ms\nstep:359/4500 train_loss:8.1093 train_time:64184ms step_avg:183.91ms\nstep:360/4500 train_loss:7.8286 train_time:64370ms step_avg:183.91ms\nstep:361/4500 train_loss:7.9449 train_time:64552ms step_avg:183.91ms\nstep:362/4500 train_loss:8.0688 train_time:64738ms step_avg:183.91ms\nstep:363/4500 train_loss:8.0202 train_time:64924ms step_avg:183.92ms\nstep:364/4500 train_loss:8.0522 train_time:65108ms step_avg:183.92ms\nstep:365/4500 train_loss:8.0637 train_time:65292ms step_avg:183.92ms\nstep:366/4500 train_loss:8.2160 train_time:65476ms step_avg:183.92ms\nstep:367/4500 train_loss:8.4067 train_time:65659ms step_avg:183.92ms\nstep:368/4500 train_loss:8.0586 train_time:65843ms step_avg:183.92ms\nstep:369/4500 train_loss:8.2342 train_time:66029ms step_avg:183.93ms\nstep:370/4500 train_loss:8.0745 train_time:66215ms step_avg:183.93ms\nstep:371/4500 train_loss:8.0640 train_time:66400ms step_avg:183.93ms\nstep:372/4500 train_loss:8.2697 train_time:66586ms step_avg:183.94ms\nstep:373/4500 train_loss:8.0390 train_time:66770ms step_avg:183.94ms\nstep:374/4500 train_loss:8.0672 train_time:66956ms step_avg:183.94ms\nstep:375/4500 train_loss:8.0864 train_time:67141ms step_avg:183.95ms\nstep:376/4500 train_loss:8.1994 train_time:67326ms step_avg:183.95ms\nstep:377/4500 train_loss:8.0000 train_time:67513ms step_avg:183.96ms\nstep:378/4500 train_loss:8.0860 train_time:67698ms step_avg:183.96ms\nstep:379/4500 train_loss:8.1000 train_time:67884ms step_avg:183.97ms\nstep:380/4500 train_loss:7.9639 train_time:68070ms step_avg:183.97ms\nstep:381/4500 train_loss:8.2275 train_time:68253ms step_avg:183.97ms\nstep:382/4500 train_loss:7.9662 train_time:68438ms step_avg:183.97ms\nstep:383/4500 train_loss:8.0812 train_time:68621ms step_avg:183.97ms\nstep:384/4500 train_loss:8.0710 train_time:68804ms step_avg:183.97ms\nstep:385/4500 train_loss:7.9449 train_time:68989ms step_avg:183.97ms\nstep:386/4500 train_loss:8.1003 train_time:69173ms step_avg:183.97ms\nstep:387/4500 train_loss:8.0310 train_time:69357ms step_avg:183.97ms\nstep:388/4500 train_loss:8.1524 train_time:69544ms step_avg:183.98ms\nstep:389/4500 train_loss:8.0525 train_time:69728ms step_avg:183.98ms\nstep:390/4500 train_loss:7.9808 train_time:69913ms step_avg:183.98ms\nstep:391/4500 train_loss:8.1512 train_time:70096ms step_avg:183.98ms\nstep:392/4500 train_loss:8.0538 train_time:70280ms step_avg:183.98ms\nstep:393/4500 train_loss:8.0321 train_time:70466ms step_avg:183.98ms\nstep:394/4500 train_loss:8.1753 train_time:70653ms step_avg:183.99ms\nstep:395/4500 train_loss:8.0008 train_time:70839ms step_avg:184.00ms\nstep:396/4500 train_loss:8.0247 train_time:71025ms step_avg:184.00ms\nstep:397/4500 train_loss:8.1337 train_time:71208ms step_avg:184.00ms\nstep:398/4500 train_loss:8.1071 train_time:71391ms step_avg:184.00ms\nstep:399/4500 train_loss:7.9928 train_time:71576ms step_avg:184.00ms\nstep:400/4500 train_loss:7.9538 train_time:71765ms step_avg:184.01ms\nstep:401/4500 train_loss:7.9643 train_time:71950ms step_avg:184.01ms\nstep:402/4500 train_loss:8.0550 train_time:72135ms step_avg:184.02ms\nstep:403/4500 train_loss:8.3929 train_time:72319ms step_avg:184.02ms\nstep:404/4500 train_loss:8.0241 train_time:72502ms step_avg:184.02ms\nstep:405/4500 train_loss:8.1575 train_time:72687ms step_avg:184.02ms\nstep:406/4500 train_loss:8.1120 train_time:72873ms step_avg:184.02ms\nstep:407/4500 train_loss:8.0955 train_time:73057ms step_avg:184.02ms\nstep:408/4500 train_loss:8.0553 train_time:73243ms step_avg:184.03ms\nstep:409/4500 train_loss:8.1627 train_time:73428ms step_avg:184.03ms\nstep:410/4500 train_loss:8.1044 train_time:73610ms step_avg:184.03ms\nstep:411/4500 train_loss:8.0915 train_time:73794ms step_avg:184.03ms\nstep:412/4500 train_loss:7.9035 train_time:73977ms step_avg:184.02ms\nstep:413/4500 train_loss:8.0568 train_time:74162ms step_avg:184.02ms\nstep:414/4500 train_loss:7.9190 train_time:74347ms step_avg:184.03ms\nstep:415/4500 train_loss:8.3285 train_time:74531ms step_avg:184.03ms\nstep:416/4500 train_loss:8.0564 train_time:74716ms step_avg:184.03ms\nstep:417/4500 train_loss:7.9612 train_time:74902ms step_avg:184.03ms\nstep:418/4500 train_loss:8.0999 train_time:75089ms step_avg:184.04ms\nstep:419/4500 train_loss:8.1413 train_time:75272ms step_avg:184.04ms\nstep:420/4500 train_loss:8.1733 train_time:75456ms step_avg:184.04ms\nstep:421/4500 train_loss:8.0209 train_time:75641ms step_avg:184.04ms\nstep:422/4500 train_loss:8.3654 train_time:75826ms step_avg:184.04ms\nstep:423/4500 train_loss:8.1700 train_time:76012ms step_avg:184.05ms\nstep:424/4500 train_loss:8.2363 train_time:76197ms step_avg:184.05ms\nstep:425/4500 train_loss:8.0863 train_time:76381ms step_avg:184.05ms\nstep:426/4500 train_loss:8.0896 train_time:76565ms step_avg:184.05ms\nstep:427/4500 train_loss:8.2419 train_time:76752ms step_avg:184.06ms\nstep:428/4500 train_loss:8.0686 train_time:76934ms step_avg:184.05ms\nstep:429/4500 train_loss:8.1123 train_time:77117ms step_avg:184.05ms\nstep:430/4500 train_loss:7.8643 train_time:77302ms step_avg:184.05ms\nstep:431/4500 train_loss:8.0213 train_time:77488ms step_avg:184.06ms\nstep:432/4500 train_loss:8.0388 train_time:77673ms step_avg:184.06ms\nstep:433/4500 train_loss:8.2513 train_time:77858ms step_avg:184.06ms\nstep:434/4500 train_loss:8.1579 train_time:78042ms step_avg:184.06ms\nstep:435/4500 train_loss:8.1569 train_time:78229ms step_avg:184.07ms\nstep:436/4500 train_loss:8.1806 train_time:78413ms step_avg:184.07ms\nstep:437/4500 train_loss:8.1045 train_time:78597ms step_avg:184.07ms\nstep:438/4500 train_loss:8.1580 train_time:78781ms step_avg:184.07ms\nstep:439/4500 train_loss:8.0825 train_time:78965ms step_avg:184.07ms\nstep:440/4500 train_loss:8.1958 train_time:79150ms step_avg:184.07ms\nstep:441/4500 train_loss:8.1353 train_time:79334ms step_avg:184.07ms\nstep:442/4500 train_loss:8.1398 train_time:79519ms step_avg:184.07ms\nstep:443/4500 train_loss:8.0810 train_time:79710ms step_avg:184.09ms\nstep:444/4500 train_loss:8.1294 train_time:79893ms step_avg:184.08ms\nstep:445/4500 train_loss:8.1811 train_time:80075ms step_avg:184.08ms\nstep:446/4500 train_loss:8.1638 train_time:80259ms step_avg:184.08ms\nstep:447/4500 train_loss:8.1691 train_time:80445ms step_avg:184.08ms\nstep:448/4500 train_loss:8.0827 train_time:80628ms step_avg:184.08ms\nstep:449/4500 train_loss:8.1581 train_time:80813ms step_avg:184.08ms\nstep:450/4500 train_loss:8.1671 train_time:80998ms step_avg:184.09ms\nstep:451/4500 train_loss:8.1489 train_time:81182ms step_avg:184.09ms\nstep:452/4500 train_loss:7.9558 train_time:81366ms step_avg:184.09ms\nstep:453/4500 train_loss:8.1846 train_time:81553ms step_avg:184.09ms\nstep:454/4500 train_loss:8.0457 train_time:81739ms step_avg:184.10ms\nstep:455/4500 train_loss:8.1201 train_time:81926ms step_avg:184.10ms\nstep:456/4500 train_loss:8.1594 train_time:82110ms step_avg:184.10ms\nstep:457/4500 train_loss:8.1056 train_time:82296ms step_avg:184.11ms\nstep:458/4500 train_loss:8.2077 train_time:82482ms step_avg:184.11ms\nstep:459/4500 train_loss:8.1162 train_time:82666ms step_avg:184.11ms\nstep:460/4500 train_loss:8.0615 train_time:82851ms step_avg:184.11ms\nstep:461/4500 train_loss:8.0723 train_time:83035ms step_avg:184.11ms\nstep:462/4500 train_loss:8.0019 train_time:83220ms step_avg:184.12ms\nstep:463/4500 train_loss:8.0765 train_time:83404ms step_avg:184.12ms\nstep:464/4500 train_loss:8.1901 train_time:83591ms step_avg:184.12ms\nstep:465/4500 train_loss:8.1576 train_time:83775ms step_avg:184.12ms\nstep:466/4500 train_loss:8.0749 train_time:83960ms step_avg:184.12ms\nstep:467/4500 train_loss:8.1533 train_time:84143ms step_avg:184.12ms\nstep:468/4500 train_loss:8.0447 train_time:84329ms step_avg:184.12ms\nstep:469/4500 train_loss:8.0570 train_time:84515ms step_avg:184.13ms\nstep:470/4500 train_loss:8.0078 train_time:84701ms step_avg:184.13ms\nstep:471/4500 train_loss:8.1228 train_time:84885ms step_avg:184.13ms\nstep:472/4500 train_loss:8.0860 train_time:85072ms step_avg:184.14ms\nstep:473/4500 train_loss:7.9986 train_time:85256ms step_avg:184.14ms\nstep:474/4500 train_loss:8.2057 train_time:85439ms step_avg:184.14ms\nstep:475/4500 train_loss:8.1495 train_time:85623ms step_avg:184.14ms\nstep:476/4500 train_loss:8.1772 train_time:85811ms step_avg:184.14ms\nstep:477/4500 train_loss:8.2046 train_time:85995ms step_avg:184.14ms\nstep:478/4500 train_loss:8.1307 train_time:86179ms step_avg:184.14ms\nstep:479/4500 train_loss:8.0934 train_time:86363ms step_avg:184.14ms\nstep:480/4500 train_loss:8.1670 train_time:86548ms step_avg:184.14ms\nstep:481/4500 train_loss:8.0555 train_time:86733ms step_avg:184.15ms\nstep:482/4500 train_loss:8.0580 train_time:86917ms step_avg:184.15ms\nstep:483/4500 train_loss:8.1024 train_time:87102ms step_avg:184.15ms\nstep:484/4500 train_loss:8.1458 train_time:87286ms step_avg:184.15ms\nstep:485/4500 train_loss:8.1417 train_time:87471ms step_avg:184.15ms\nstep:486/4500 train_loss:8.0854 train_time:87656ms step_avg:184.15ms\nstep:487/4500 train_loss:8.0786 train_time:87844ms step_avg:184.16ms\nstep:488/4500 train_loss:8.0868 train_time:88033ms step_avg:184.17ms\nstep:489/4500 train_loss:7.9817 train_time:88218ms step_avg:184.17ms\nstep:490/4500 train_loss:7.9848 train_time:88402ms step_avg:184.17ms\nstep:491/4500 train_loss:8.0811 train_time:88586ms step_avg:184.17ms\nstep:492/4500 train_loss:8.1616 train_time:88769ms step_avg:184.17ms\nstep:493/4500 train_loss:8.1297 train_time:88953ms step_avg:184.17ms\nstep:494/4500 train_loss:8.2568 train_time:89139ms step_avg:184.17ms\nstep:495/4500 train_loss:8.0870 train_time:89324ms step_avg:184.17ms\nstep:496/4500 train_loss:8.1439 train_time:89507ms step_avg:184.17ms\nstep:497/4500 train_loss:8.1496 train_time:89691ms step_avg:184.17ms\nstep:498/4500 train_loss:8.0893 train_time:89874ms step_avg:184.17ms\nstep:499/4500 train_loss:8.1082 train_time:90059ms step_avg:184.17ms\nstep:500/4500 train_loss:7.9786 train_time:90248ms step_avg:184.18ms\nstep:501/4500 train_loss:8.0639 train_time:90432ms step_avg:184.18ms\nstep:502/4500 train_loss:8.1542 train_time:90617ms step_avg:184.18ms\nstep:503/4500 train_loss:8.0197 train_time:90804ms step_avg:184.19ms\nstep:504/4500 train_loss:7.9996 train_time:90987ms step_avg:184.18ms\nstep:505/4500 train_loss:8.1452 train_time:91169ms step_avg:184.18ms\nstep:506/4500 train_loss:8.1633 train_time:91352ms step_avg:184.18ms\nstep:507/4500 train_loss:8.2188 train_time:91535ms step_avg:184.18ms\nstep:508/4500 train_loss:8.1053 train_time:91721ms step_avg:184.18ms\nstep:509/4500 train_loss:8.2613 train_time:91903ms step_avg:184.18ms\nstep:510/4500 train_loss:8.1186 train_time:92087ms step_avg:184.17ms\nstep:511/4500 train_loss:8.1441 train_time:92274ms step_avg:184.18ms\nstep:512/4500 train_loss:8.1633 train_time:92458ms step_avg:184.18ms\nstep:513/4500 train_loss:8.1388 train_time:92645ms step_avg:184.18ms\nstep:514/4500 train_loss:8.1943 train_time:92829ms step_avg:184.19ms\nstep:515/4500 train_loss:8.1012 train_time:93016ms step_avg:184.19ms\nstep:516/4500 train_loss:8.1583 train_time:93200ms step_avg:184.19ms\nstep:517/4500 train_loss:8.0673 train_time:93385ms step_avg:184.19ms\nstep:518/4500 train_loss:8.1085 train_time:93572ms step_avg:184.20ms\nstep:519/4500 train_loss:8.0331 train_time:93756ms step_avg:184.20ms\nstep:520/4500 train_loss:8.1438 train_time:93942ms step_avg:184.20ms\nstep:521/4500 train_loss:8.0834 train_time:94128ms step_avg:184.20ms\nstep:522/4500 train_loss:8.2539 train_time:94314ms step_avg:184.21ms\nstep:523/4500 train_loss:8.1510 train_time:94498ms step_avg:184.21ms\nstep:524/4500 train_loss:8.2806 train_time:94682ms step_avg:184.21ms\nstep:525/4500 train_loss:8.4331 train_time:94868ms step_avg:184.21ms\nstep:526/4500 train_loss:8.1520 train_time:95053ms step_avg:184.21ms\nstep:527/4500 train_loss:8.1237 train_time:95236ms step_avg:184.21ms\nstep:528/4500 train_loss:8.0717 train_time:95424ms step_avg:184.22ms\nstep:529/4500 train_loss:8.1023 train_time:95614ms step_avg:184.23ms\nstep:530/4500 train_loss:8.1035 train_time:95798ms step_avg:184.23ms\nstep:531/4500 train_loss:8.1330 train_time:95983ms step_avg:184.23ms\nstep:532/4500 train_loss:8.0866 train_time:96167ms step_avg:184.23ms\nstep:533/4500 train_loss:8.0886 train_time:96354ms step_avg:184.23ms\nstep:534/4500 train_loss:8.1219 train_time:96537ms step_avg:184.23ms\nstep:535/4500 train_loss:8.1164 train_time:96723ms step_avg:184.23ms\nstep:536/4500 train_loss:8.0861 train_time:96908ms step_avg:184.24ms\nstep:537/4500 train_loss:8.2090 train_time:97095ms step_avg:184.24ms\nstep:538/4500 train_loss:8.1252 train_time:97278ms step_avg:184.24ms\nstep:539/4500 train_loss:8.1497 train_time:97462ms step_avg:184.24ms\nstep:540/4500 train_loss:8.1366 train_time:97647ms step_avg:184.24ms\nstep:541/4500 train_loss:8.0689 train_time:97830ms step_avg:184.24ms\nstep:542/4500 train_loss:8.0870 train_time:98016ms step_avg:184.24ms\nstep:543/4500 train_loss:8.1505 train_time:98200ms step_avg:184.24ms\nstep:544/4500 train_loss:8.0641 train_time:98385ms step_avg:184.24ms\nstep:545/4500 train_loss:8.0863 train_time:98569ms step_avg:184.24ms\nstep:546/4500 train_loss:8.0868 train_time:98753ms step_avg:184.24ms\nstep:547/4500 train_loss:8.0490 train_time:98937ms step_avg:184.24ms\nstep:548/4500 train_loss:8.0982 train_time:99120ms step_avg:184.24ms\nstep:549/4500 train_loss:8.0737 train_time:99304ms step_avg:184.24ms\nstep:550/4500 train_loss:8.1144 train_time:99489ms step_avg:184.24ms\nstep:551/4500 train_loss:8.1387 train_time:99673ms step_avg:184.24ms\nstep:552/4500 train_loss:8.0942 train_time:99859ms step_avg:184.24ms\nstep:553/4500 train_loss:8.0584 train_time:100046ms step_avg:184.25ms\nstep:554/4500 train_loss:8.1696 train_time:100232ms step_avg:184.25ms\nstep:555/4500 train_loss:8.0498 train_time:100417ms step_avg:184.25ms\nstep:556/4500 train_loss:8.0686 train_time:100602ms step_avg:184.25ms\nstep:557/4500 train_loss:8.0479 train_time:100787ms step_avg:184.25ms\nstep:558/4500 train_loss:8.0280 train_time:100975ms step_avg:184.26ms\nstep:559/4500 train_loss:8.0915 train_time:101161ms step_avg:184.26ms\nstep:560/4500 train_loss:8.0408 train_time:101347ms step_avg:184.27ms\nstep:561/4500 train_loss:8.1786 train_time:101534ms step_avg:184.27ms\nstep:562/4500 train_loss:8.0260 train_time:101719ms step_avg:184.27ms\nstep:563/4500 train_loss:8.0031 train_time:101905ms step_avg:184.28ms\nstep:564/4500 train_loss:8.0635 train_time:102091ms step_avg:184.28ms\nstep:565/4500 train_loss:8.2048 train_time:102274ms step_avg:184.28ms\nstep:566/4500 train_loss:8.1715 train_time:102458ms step_avg:184.28ms\nstep:567/4500 train_loss:8.2078 train_time:102642ms step_avg:184.28ms\nstep:568/4500 train_loss:8.0204 train_time:102827ms step_avg:184.28ms\nstep:569/4500 train_loss:8.2158 train_time:103014ms step_avg:184.28ms\nstep:570/4500 train_loss:7.9945 train_time:103199ms step_avg:184.28ms\nstep:571/4500 train_loss:8.1230 train_time:103386ms step_avg:184.29ms\nstep:572/4500 train_loss:8.1947 train_time:103580ms step_avg:184.31ms\nstep:573/4500 train_loss:8.2004 train_time:103768ms step_avg:184.31ms\nstep:574/4500 train_loss:8.1045 train_time:103954ms step_avg:184.32ms\nstep:575/4500 train_loss:8.0692 train_time:104139ms step_avg:184.32ms\nstep:576/4500 train_loss:8.4048 train_time:104325ms step_avg:184.32ms\nstep:577/4500 train_loss:8.1163 train_time:104510ms step_avg:184.32ms\nstep:578/4500 train_loss:8.0940 train_time:104695ms step_avg:184.32ms\nstep:579/4500 train_loss:8.1492 train_time:104879ms step_avg:184.32ms\nstep:580/4500 train_loss:8.1151 train_time:105064ms step_avg:184.32ms\nstep:581/4500 train_loss:8.1011 train_time:105248ms step_avg:184.32ms\nstep:582/4500 train_loss:8.1132 train_time:105433ms step_avg:184.32ms\nstep:583/4500 train_loss:8.0958 train_time:105618ms step_avg:184.32ms\nstep:584/4500 train_loss:8.1691 train_time:105806ms step_avg:184.33ms\nstep:585/4500 train_loss:8.0370 train_time:105990ms step_avg:184.33ms\nstep:586/4500 train_loss:8.1086 train_time:106174ms step_avg:184.33ms\nstep:587/4500 train_loss:8.0889 train_time:106358ms step_avg:184.33ms\nstep:588/4500 train_loss:8.1288 train_time:106543ms step_avg:184.33ms\nstep:589/4500 train_loss:8.0590 train_time:106727ms step_avg:184.33ms\nstep:590/4500 train_loss:8.0679 train_time:106911ms step_avg:184.33ms\nstep:591/4500 train_loss:8.1036 train_time:107095ms step_avg:184.33ms\nstep:592/4500 train_loss:7.9841 train_time:107280ms step_avg:184.33ms\nstep:593/4500 train_loss:8.1741 train_time:107465ms step_avg:184.33ms\nstep:594/4500 train_loss:8.1952 train_time:107649ms step_avg:184.33ms\nstep:595/4500 train_loss:8.0623 train_time:107834ms step_avg:184.33ms\nstep:596/4500 train_loss:8.2374 train_time:108019ms step_avg:184.33ms\nstep:597/4500 train_loss:8.0992 train_time:108204ms step_avg:184.33ms\nstep:598/4500 train_loss:8.1972 train_time:108388ms step_avg:184.33ms\nstep:599/4500 train_loss:8.1636 train_time:108573ms step_avg:184.33ms\nstep:600/4500 train_loss:8.2630 train_time:108757ms step_avg:184.33ms\nstep:601/4500 train_loss:8.1477 train_time:108945ms step_avg:184.34ms\nstep:602/4500 train_loss:8.1340 train_time:109133ms step_avg:184.35ms\nstep:603/4500 train_loss:8.1811 train_time:109318ms step_avg:184.35ms\nstep:604/4500 train_loss:8.0668 train_time:109503ms step_avg:184.35ms\nstep:605/4500 train_loss:8.0674 train_time:109690ms step_avg:184.35ms\nstep:606/4500 train_loss:8.1282 train_time:109873ms step_avg:184.35ms\nstep:607/4500 train_loss:8.1499 train_time:110057ms step_avg:184.35ms\nstep:608/4500 train_loss:8.1920 train_time:110241ms step_avg:184.35ms\nstep:609/4500 train_loss:8.0626 train_time:110427ms step_avg:184.35ms\nstep:610/4500 train_loss:8.0764 train_time:110610ms step_avg:184.35ms\nstep:611/4500 train_loss:8.2452 train_time:110797ms step_avg:184.35ms\nstep:612/4500 train_loss:8.0552 train_time:110983ms step_avg:184.36ms\nstep:613/4500 train_loss:7.9313 train_time:111173ms step_avg:184.37ms\nstep:614/4500 train_loss:8.1138 train_time:111355ms step_avg:184.36ms\nstep:615/4500 train_loss:8.1034 train_time:111541ms step_avg:184.36ms\nstep:616/4500 train_loss:8.1175 train_time:111727ms step_avg:184.37ms\nstep:617/4500 train_loss:8.0990 train_time:111911ms step_avg:184.37ms\nstep:618/4500 train_loss:8.1764 train_time:112095ms step_avg:184.37ms\nstep:619/4500 train_loss:8.1875 train_time:112280ms step_avg:184.37ms\nstep:620/4500 train_loss:8.1071 train_time:112464ms step_avg:184.37ms\nstep:621/4500 train_loss:8.1417 train_time:112651ms step_avg:184.37ms\nstep:622/4500 train_loss:8.1192 train_time:112837ms step_avg:184.37ms\nstep:623/4500 train_loss:8.1166 train_time:113027ms step_avg:184.38ms\nstep:624/4500 train_loss:8.1549 train_time:113211ms step_avg:184.38ms\nstep:625/4500 train_loss:8.1792 train_time:113394ms step_avg:184.38ms\nstep:626/4500 train_loss:8.2368 train_time:113578ms step_avg:184.38ms\nstep:627/4500 train_loss:8.0625 train_time:113764ms step_avg:184.38ms\nstep:628/4500 train_loss:8.1046 train_time:113952ms step_avg:184.39ms\nstep:629/4500 train_loss:8.1335 train_time:114137ms step_avg:184.39ms\nstep:630/4500 train_loss:8.0583 train_time:114323ms step_avg:184.39ms\nstep:631/4500 train_loss:8.0599 train_time:114508ms step_avg:184.39ms\nstep:632/4500 train_loss:8.1742 train_time:114692ms step_avg:184.39ms\nstep:633/4500 train_loss:8.0692 train_time:114876ms step_avg:184.39ms\nstep:634/4500 train_loss:8.1336 train_time:115060ms step_avg:184.39ms\nstep:635/4500 train_loss:8.1743 train_time:115247ms step_avg:184.40ms\nstep:636/4500 train_loss:8.0247 train_time:115433ms step_avg:184.40ms\nstep:637/4500 train_loss:8.0081 train_time:115617ms step_avg:184.40ms\nstep:638/4500 train_loss:8.0842 train_time:115803ms step_avg:184.40ms\nstep:639/4500 train_loss:8.2227 train_time:115988ms step_avg:184.40ms\nstep:640/4500 train_loss:8.0884 train_time:116172ms step_avg:184.40ms\nstep:641/4500 train_loss:8.1208 train_time:116355ms step_avg:184.40ms\nstep:642/4500 train_loss:8.0707 train_time:116542ms step_avg:184.40ms\nstep:643/4500 train_loss:8.0946 train_time:116729ms step_avg:184.41ms\nstep:644/4500 train_loss:8.1620 train_time:116913ms step_avg:184.41ms\nstep:645/4500 train_loss:8.0173 train_time:117097ms step_avg:184.40ms\nstep:646/4500 train_loss:8.1403 train_time:117282ms step_avg:184.41ms\nstep:647/4500 train_loss:8.1767 train_time:117466ms step_avg:184.41ms\nstep:648/4500 train_loss:8.1463 train_time:117652ms step_avg:184.41ms\nstep:649/4500 train_loss:8.0779 train_time:117838ms step_avg:184.41ms\nstep:650/4500 train_loss:8.1587 train_time:118025ms step_avg:184.41ms\nstep:651/4500 train_loss:8.2945 train_time:118210ms step_avg:184.42ms\nstep:652/4500 train_loss:8.0739 train_time:118393ms step_avg:184.41ms\nstep:653/4500 train_loss:8.0802 train_time:118576ms step_avg:184.41ms\nstep:654/4500 train_loss:8.0531 train_time:118762ms step_avg:184.41ms\nstep:655/4500 train_loss:8.0845 train_time:118946ms step_avg:184.41ms\nstep:656/4500 train_loss:8.0321 train_time:119133ms step_avg:184.42ms\nstep:657/4500 train_loss:8.0288 train_time:119318ms step_avg:184.42ms\nstep:658/4500 train_loss:8.1385 train_time:119506ms step_avg:184.42ms\nstep:659/4500 train_loss:8.1478 train_time:119690ms step_avg:184.42ms\nstep:660/4500 train_loss:8.1357 train_time:119877ms step_avg:184.43ms\nstep:661/4500 train_loss:8.0074 train_time:120061ms step_avg:184.43ms\nstep:662/4500 train_loss:8.2367 train_time:120245ms step_avg:184.43ms\nstep:663/4500 train_loss:8.0929 train_time:120429ms step_avg:184.42ms\nstep:664/4500 train_loss:8.1719 train_time:120612ms step_avg:184.42ms\nstep:665/4500 train_loss:8.1589 train_time:120796ms step_avg:184.42ms\nstep:666/4500 train_loss:8.1182 train_time:120981ms step_avg:184.42ms\nstep:667/4500 train_loss:8.1499 train_time:121166ms step_avg:184.42ms\nstep:668/4500 train_loss:7.9577 train_time:121350ms step_avg:184.42ms\nstep:669/4500 train_loss:7.9923 train_time:121537ms step_avg:184.43ms\nstep:670/4500 train_loss:8.0902 train_time:121722ms step_avg:184.43ms\nstep:671/4500 train_loss:8.1452 train_time:121910ms step_avg:184.43ms\nstep:672/4500 train_loss:8.1341 train_time:122093ms step_avg:184.43ms\nstep:673/4500 train_loss:8.1018 train_time:122278ms step_avg:184.43ms\nstep:674/4500 train_loss:8.0778 train_time:122463ms step_avg:184.43ms\nstep:675/4500 train_loss:8.0419 train_time:122647ms step_avg:184.43ms\nstep:676/4500 train_loss:8.1039 train_time:122831ms step_avg:184.43ms\nstep:677/4500 train_loss:8.1463 train_time:123017ms step_avg:184.43ms\nstep:678/4500 train_loss:8.1612 train_time:123202ms step_avg:184.43ms\nstep:679/4500 train_loss:8.1082 train_time:123388ms step_avg:184.44ms\nstep:680/4500 train_loss:8.1261 train_time:123573ms step_avg:184.44ms\nstep:681/4500 train_loss:8.1386 train_time:123756ms step_avg:184.43ms\nstep:682/4500 train_loss:8.1061 train_time:123940ms step_avg:184.43ms\nstep:683/4500 train_loss:8.2569 train_time:124125ms step_avg:184.44ms\nstep:684/4500 train_loss:7.6260 train_time:124312ms step_avg:184.44ms\nstep:685/4500 train_loss:8.0455 train_time:124497ms step_avg:184.44ms\nstep:686/4500 train_loss:8.1303 train_time:124685ms step_avg:184.44ms\nstep:687/4500 train_loss:8.2172 train_time:124870ms step_avg:184.45ms\nstep:688/4500 train_loss:8.1226 train_time:125052ms step_avg:184.44ms\nstep:689/4500 train_loss:8.1703 train_time:125236ms step_avg:184.44ms\nstep:690/4500 train_loss:8.1444 train_time:125420ms step_avg:184.44ms\nstep:691/4500 train_loss:8.1487 train_time:125604ms step_avg:184.44ms\nstep:692/4500 train_loss:8.0527 train_time:125788ms step_avg:184.44ms\nstep:693/4500 train_loss:8.0512 train_time:125973ms step_avg:184.44ms\nstep:694/4500 train_loss:8.1734 train_time:126158ms step_avg:184.44ms\nstep:695/4500 train_loss:8.0118 train_time:126346ms step_avg:184.45ms\nstep:696/4500 train_loss:8.1647 train_time:126531ms step_avg:184.45ms\nstep:697/4500 train_loss:8.1763 train_time:126715ms step_avg:184.45ms\nstep:698/4500 train_loss:8.1241 train_time:126898ms step_avg:184.45ms\nstep:699/4500 train_loss:8.1064 train_time:127084ms step_avg:184.45ms\nstep:700/4500 train_loss:8.2233 train_time:127269ms step_avg:184.45ms\nstep:701/4500 train_loss:8.2921 train_time:127452ms step_avg:184.45ms\nstep:702/4500 train_loss:8.0747 train_time:127636ms step_avg:184.44ms\nstep:703/4500 train_loss:8.1934 train_time:127820ms step_avg:184.44ms\nstep:704/4500 train_loss:8.1285 train_time:128007ms step_avg:184.45ms\nstep:705/4500 train_loss:8.1013 train_time:128189ms step_avg:184.44ms\nstep:706/4500 train_loss:8.0679 train_time:128372ms step_avg:184.44ms\nstep:707/4500 train_loss:8.0250 train_time:128557ms step_avg:184.44ms\nstep:708/4500 train_loss:8.2637 train_time:128742ms step_avg:184.44ms\nstep:709/4500 train_loss:8.0924 train_time:128926ms step_avg:184.44ms\nstep:710/4500 train_loss:8.0412 train_time:129110ms step_avg:184.44ms\nstep:711/4500 train_loss:7.9604 train_time:129295ms step_avg:184.44ms\nstep:712/4500 train_loss:8.0462 train_time:129479ms step_avg:184.44ms\nstep:713/4500 train_loss:8.3100 train_time:129665ms step_avg:184.44ms\nstep:714/4500 train_loss:8.1528 train_time:129850ms step_avg:184.45ms\nstep:715/4500 train_loss:8.1817 train_time:130034ms step_avg:184.45ms\nstep:716/4500 train_loss:8.0942 train_time:130219ms step_avg:184.45ms\nstep:717/4500 train_loss:8.0910 train_time:130405ms step_avg:184.45ms\nstep:718/4500 train_loss:8.1594 train_time:130590ms step_avg:184.45ms\nstep:719/4500 train_loss:8.2047 train_time:130776ms step_avg:184.45ms\nstep:720/4500 train_loss:8.1425 train_time:130963ms step_avg:184.45ms\nstep:721/4500 train_loss:8.1621 train_time:131147ms step_avg:184.45ms\nstep:722/4500 train_loss:8.1310 train_time:131330ms step_avg:184.45ms\nstep:723/4500 train_loss:8.1015 train_time:131516ms step_avg:184.45ms\nstep:724/4500 train_loss:8.2961 train_time:131702ms step_avg:184.46ms\nstep:725/4500 train_loss:8.0071 train_time:131888ms step_avg:184.46ms\nstep:726/4500 train_loss:8.1287 train_time:132071ms step_avg:184.46ms\nstep:727/4500 train_loss:8.1033 train_time:132253ms step_avg:184.45ms\nstep:728/4500 train_loss:7.9894 train_time:132435ms step_avg:184.45ms\nstep:729/4500 train_loss:8.1659 train_time:132620ms step_avg:184.45ms\nstep:730/4500 train_loss:8.0554 train_time:132804ms step_avg:184.45ms\nstep:731/4500 train_loss:8.0343 train_time:132989ms step_avg:184.45ms\nstep:732/4500 train_loss:8.2024 train_time:133172ms step_avg:184.45ms\nstep:733/4500 train_loss:8.0902 train_time:133357ms step_avg:184.45ms\nstep:734/4500 train_loss:8.0688 train_time:133543ms step_avg:184.45ms\nstep:735/4500 train_loss:8.1081 train_time:133732ms step_avg:184.46ms\nstep:736/4500 train_loss:8.2693 train_time:133916ms step_avg:184.46ms\nstep:737/4500 train_loss:8.0674 train_time:134100ms step_avg:184.46ms\nstep:738/4500 train_loss:8.1479 train_time:134286ms step_avg:184.46ms\nstep:739/4500 train_loss:8.0373 train_time:134472ms step_avg:184.46ms\nstep:740/4500 train_loss:8.1250 train_time:134659ms step_avg:184.46ms\nstep:741/4500 train_loss:8.0808 train_time:134846ms step_avg:184.47ms\nstep:742/4500 train_loss:8.0425 train_time:135031ms step_avg:184.47ms\nstep:743/4500 train_loss:8.0825 train_time:135214ms step_avg:184.47ms\nstep:744/4500 train_loss:8.0640 train_time:135397ms step_avg:184.47ms\nstep:745/4500 train_loss:8.0269 train_time:135581ms step_avg:184.46ms\nstep:746/4500 train_loss:8.3563 train_time:135765ms step_avg:184.46ms\nstep:747/4500 train_loss:8.0965 train_time:135950ms step_avg:184.46ms\nstep:748/4500 train_loss:8.1587 train_time:136133ms step_avg:184.46ms\nstep:749/4500 train_loss:8.2013 train_time:136318ms step_avg:184.46ms\nstep:750/4500 train_loss:8.1028 train_time:136504ms step_avg:184.46ms\nstep:751/4500 train_loss:8.0821 train_time:136688ms step_avg:184.46ms\nstep:752/4500 train_loss:8.1994 train_time:136870ms step_avg:184.46ms\nstep:753/4500 train_loss:8.1835 train_time:137055ms step_avg:184.46ms\nstep:754/4500 train_loss:8.0879 train_time:137238ms step_avg:184.46ms\nstep:755/4500 train_loss:8.1817 train_time:137422ms step_avg:184.46ms\nstep:756/4500 train_loss:7.8842 train_time:137608ms step_avg:184.46ms\nstep:757/4500 train_loss:7.7935 train_time:137792ms step_avg:184.46ms\nstep:758/4500 train_loss:8.2401 train_time:137978ms step_avg:184.46ms\nstep:759/4500 train_loss:8.1512 train_time:138163ms step_avg:184.46ms\nstep:760/4500 train_loss:8.1053 train_time:138347ms step_avg:184.46ms\nstep:761/4500 train_loss:8.0426 train_time:138533ms step_avg:184.47ms\nstep:762/4500 train_loss:8.2037 train_time:138916ms step_avg:184.73ms\nstep:763/4500 train_loss:8.1454 train_time:139099ms step_avg:184.73ms\nstep:764/4500 train_loss:8.0599 train_time:139283ms step_avg:184.73ms\nstep:765/4500 train_loss:8.0573 train_time:139465ms step_avg:184.72ms\nstep:766/4500 train_loss:8.2219 train_time:139650ms step_avg:184.72ms\nstep:767/4500 train_loss:8.1274 train_time:139836ms step_avg:184.72ms\nstep:768/4500 train_loss:8.0756 train_time:140023ms step_avg:184.73ms\nstep:769/4500 train_loss:8.1335 train_time:140208ms step_avg:184.73ms\nstep:770/4500 train_loss:8.0877 train_time:140391ms step_avg:184.73ms\nstep:771/4500 train_loss:8.1683 train_time:140574ms step_avg:184.72ms\nstep:772/4500 train_loss:8.0688 train_time:140758ms step_avg:184.72ms\nstep:773/4500 train_loss:7.8194 train_time:140944ms step_avg:184.72ms\nstep:774/4500 train_loss:8.0938 train_time:141130ms step_avg:184.73ms\nstep:775/4500 train_loss:8.1884 train_time:141315ms step_avg:184.73ms\nstep:776/4500 train_loss:8.1235 train_time:141500ms step_avg:184.73ms\nstep:777/4500 train_loss:8.1826 train_time:141685ms step_avg:184.73ms\nstep:778/4500 train_loss:8.1803 train_time:141870ms step_avg:184.73ms\nstep:779/4500 train_loss:8.1205 train_time:142058ms step_avg:184.73ms\nstep:780/4500 train_loss:8.2866 train_time:142245ms step_avg:184.73ms\nstep:781/4500 train_loss:8.0929 train_time:142434ms step_avg:184.74ms\nstep:782/4500 train_loss:8.0550 train_time:142617ms step_avg:184.74ms\nstep:783/4500 train_loss:8.1019 train_time:142802ms step_avg:184.74ms\nstep:784/4500 train_loss:8.1998 train_time:142988ms step_avg:184.74ms\nstep:785/4500 train_loss:8.0395 train_time:143170ms step_avg:184.74ms\nstep:786/4500 train_loss:8.1966 train_time:143357ms step_avg:184.74ms\nstep:787/4500 train_loss:8.1902 train_time:143544ms step_avg:184.74ms\nstep:788/4500 train_loss:8.1125 train_time:143727ms step_avg:184.74ms\nstep:789/4500 train_loss:8.0724 train_time:143910ms step_avg:184.74ms\nstep:790/4500 train_loss:8.1033 train_time:144093ms step_avg:184.73ms\nstep:791/4500 train_loss:8.2247 train_time:144277ms step_avg:184.73ms\nstep:792/4500 train_loss:8.0994 train_time:144461ms step_avg:184.73ms\nstep:793/4500 train_loss:8.0109 train_time:144649ms step_avg:184.74ms\nstep:794/4500 train_loss:8.1981 train_time:144832ms step_avg:184.73ms\nstep:795/4500 train_loss:8.1012 train_time:145015ms step_avg:184.73ms\nstep:796/4500 train_loss:8.0705 train_time:145200ms step_avg:184.73ms\nstep:797/4500 train_loss:8.0858 train_time:145386ms step_avg:184.73ms\nstep:798/4500 train_loss:8.1330 train_time:145570ms step_avg:184.73ms\nstep:799/4500 train_loss:8.2735 train_time:145756ms step_avg:184.73ms\nstep:800/4500 train_loss:8.1454 train_time:145941ms step_avg:184.74ms\nstep:801/4500 train_loss:8.0222 train_time:146129ms step_avg:184.74ms\nstep:802/4500 train_loss:8.3427 train_time:146313ms step_avg:184.74ms\nstep:803/4500 train_loss:8.2394 train_time:146497ms step_avg:184.74ms\nstep:804/4500 train_loss:8.1359 train_time:146682ms step_avg:184.74ms\nstep:805/4500 train_loss:8.2090 train_time:146868ms step_avg:184.74ms\nstep:806/4500 train_loss:8.1796 train_time:147052ms step_avg:184.74ms\nstep:807/4500 train_loss:8.1220 train_time:147237ms step_avg:184.74ms\nstep:808/4500 train_loss:8.1686 train_time:147424ms step_avg:184.74ms\nstep:809/4500 train_loss:8.1901 train_time:147613ms step_avg:184.75ms\nstep:810/4500 train_loss:8.1086 train_time:147797ms step_avg:184.75ms\nstep:811/4500 train_loss:8.1258 train_time:147981ms step_avg:184.75ms\nstep:812/4500 train_loss:8.1897 train_time:148165ms step_avg:184.74ms\nstep:813/4500 train_loss:8.2171 train_time:148351ms step_avg:184.75ms\nstep:814/4500 train_loss:8.0874 train_time:148536ms step_avg:184.75ms\nstep:815/4500 train_loss:8.0937 train_time:148722ms step_avg:184.75ms\nstep:816/4500 train_loss:8.0663 train_time:148906ms step_avg:184.75ms\nstep:817/4500 train_loss:8.0841 train_time:149091ms step_avg:184.75ms\nstep:818/4500 train_loss:8.0828 train_time:149276ms step_avg:184.75ms\nstep:819/4500 train_loss:8.1696 train_time:149460ms step_avg:184.75ms\nstep:820/4500 train_loss:8.2856 train_time:149649ms step_avg:184.75ms\nstep:821/4500 train_loss:8.1899 train_time:149834ms step_avg:184.75ms\nstep:822/4500 train_loss:8.1041 train_time:150018ms step_avg:184.75ms\nstep:823/4500 train_loss:8.1189 train_time:150202ms step_avg:184.75ms\nstep:824/4500 train_loss:8.0628 train_time:150387ms step_avg:184.75ms\nstep:825/4500 train_loss:8.1335 train_time:150570ms step_avg:184.75ms\nstep:826/4500 train_loss:8.2433 train_time:150754ms step_avg:184.75ms\nstep:827/4500 train_loss:8.2015 train_time:150940ms step_avg:184.75ms\nstep:828/4500 train_loss:8.1308 train_time:151126ms step_avg:184.75ms\nstep:829/4500 train_loss:8.0333 train_time:151310ms step_avg:184.75ms\nstep:830/4500 train_loss:8.0696 train_time:151495ms step_avg:184.75ms\nstep:831/4500 train_loss:8.0438 train_time:151680ms step_avg:184.75ms\nstep:832/4500 train_loss:8.0950 train_time:151864ms step_avg:184.75ms\nstep:833/4500 train_loss:8.0851 train_time:152048ms step_avg:184.75ms\nstep:834/4500 train_loss:8.1000 train_time:152234ms step_avg:184.75ms\nstep:835/4500 train_loss:7.9519 train_time:152418ms step_avg:184.75ms\nstep:836/4500 train_loss:8.2125 train_time:152602ms step_avg:184.75ms\nstep:837/4500 train_loss:8.0938 train_time:152792ms step_avg:184.75ms\nstep:838/4500 train_loss:8.1008 train_time:152977ms step_avg:184.75ms\nstep:839/4500 train_loss:8.1954 train_time:153161ms step_avg:184.75ms\nstep:840/4500 train_loss:7.9489 train_time:153347ms step_avg:184.76ms\nstep:841/4500 train_loss:8.1463 train_time:153530ms step_avg:184.75ms\nstep:842/4500 train_loss:8.0426 train_time:153713ms step_avg:184.75ms\nstep:843/4500 train_loss:8.1326 train_time:153897ms step_avg:184.75ms\nstep:844/4500 train_loss:8.0769 train_time:154083ms step_avg:184.75ms\nstep:845/4500 train_loss:8.1630 train_time:154268ms step_avg:184.75ms\nstep:846/4500 train_loss:8.0608 train_time:154455ms step_avg:184.75ms\nstep:847/4500 train_loss:8.1855 train_time:154641ms step_avg:184.76ms\nstep:848/4500 train_loss:8.4010 train_time:154827ms step_avg:184.76ms\nstep:849/4500 train_loss:8.0701 train_time:155015ms step_avg:184.76ms\nstep:850/4500 train_loss:8.0698 train_time:155199ms step_avg:184.76ms\nstep:851/4500 train_loss:8.3041 train_time:155383ms step_avg:184.76ms\nstep:852/4500 train_loss:8.0423 train_time:155570ms step_avg:184.76ms\nstep:853/4500 train_loss:8.1213 train_time:155754ms step_avg:184.76ms\nstep:854/4500 train_loss:8.1298 train_time:155937ms step_avg:184.76ms\nstep:855/4500 train_loss:8.1830 train_time:156122ms step_avg:184.76ms\nstep:856/4500 train_loss:8.1435 train_time:156307ms step_avg:184.76ms\nstep:857/4500 train_loss:8.0072 train_time:156494ms step_avg:184.76ms\nstep:858/4500 train_loss:8.1706 train_time:156679ms step_avg:184.76ms\nstep:859/4500 train_loss:8.2117 train_time:156868ms step_avg:184.77ms\nstep:860/4500 train_loss:8.0289 train_time:157052ms step_avg:184.77ms\nstep:861/4500 train_loss:8.3464 train_time:157235ms step_avg:184.76ms\nstep:862/4500 train_loss:8.0679 train_time:157418ms step_avg:184.76ms\nstep:863/4500 train_loss:8.1048 train_time:157602ms step_avg:184.76ms\nstep:864/4500 train_loss:8.1924 train_time:157786ms step_avg:184.76ms\nstep:865/4500 train_loss:8.0674 train_time:157973ms step_avg:184.76ms\nstep:866/4500 train_loss:8.1586 train_time:158156ms step_avg:184.76ms\nstep:867/4500 train_loss:8.0911 train_time:158342ms step_avg:184.76ms\nstep:868/4500 train_loss:8.1038 train_time:158528ms step_avg:184.76ms\nstep:869/4500 train_loss:8.0445 train_time:158714ms step_avg:184.77ms\nstep:870/4500 train_loss:8.1081 train_time:158899ms step_avg:184.77ms\nstep:871/4500 train_loss:8.0679 train_time:159084ms step_avg:184.77ms\nstep:872/4500 train_loss:8.0513 train_time:159269ms step_avg:184.77ms\nstep:873/4500 train_loss:8.0087 train_time:159453ms step_avg:184.77ms\nstep:874/4500 train_loss:7.9856 train_time:159636ms step_avg:184.76ms\nstep:875/4500 train_loss:8.1825 train_time:159821ms step_avg:184.76ms\nstep:876/4500 train_loss:8.1226 train_time:160007ms step_avg:184.77ms\nstep:877/4500 train_loss:8.1583 train_time:160190ms step_avg:184.76ms\nstep:878/4500 train_loss:8.0750 train_time:160375ms step_avg:184.76ms\nstep:879/4500 train_loss:7.9606 train_time:160562ms step_avg:184.77ms\nstep:880/4500 train_loss:8.0810 train_time:160746ms step_avg:184.77ms\nstep:881/4500 train_loss:8.0939 train_time:160929ms step_avg:184.76ms\nstep:882/4500 train_loss:8.2202 train_time:161115ms step_avg:184.77ms\nstep:883/4500 train_loss:8.0799 train_time:161301ms step_avg:184.77ms\nstep:884/4500 train_loss:8.1849 train_time:161486ms step_avg:184.77ms\nstep:885/4500 train_loss:8.0440 train_time:161670ms step_avg:184.77ms\nstep:886/4500 train_loss:8.1579 train_time:161854ms step_avg:184.76ms\nstep:887/4500 train_loss:8.0940 train_time:162036ms step_avg:184.76ms\nstep:888/4500 train_loss:8.1924 train_time:162220ms step_avg:184.76ms\nstep:889/4500 train_loss:8.1390 train_time:162406ms step_avg:184.76ms\nstep:890/4500 train_loss:8.1521 train_time:162590ms step_avg:184.76ms\nstep:891/4500 train_loss:8.1142 train_time:162774ms step_avg:184.76ms\nstep:892/4500 train_loss:8.1808 train_time:162962ms step_avg:184.76ms\nstep:893/4500 train_loss:8.1633 train_time:163150ms step_avg:184.77ms\nstep:894/4500 train_loss:8.0768 train_time:163334ms step_avg:184.77ms\nstep:895/4500 train_loss:8.1904 train_time:163517ms step_avg:184.77ms\nstep:896/4500 train_loss:8.1449 train_time:163701ms step_avg:184.76ms\nstep:897/4500 train_loss:8.0702 train_time:163886ms step_avg:184.76ms\nstep:898/4500 train_loss:8.1743 train_time:164069ms step_avg:184.76ms\nstep:899/4500 train_loss:8.1899 train_time:164254ms step_avg:184.76ms\nstep:900/4500 train_loss:7.9670 train_time:164439ms step_avg:184.76ms\nstep:901/4500 train_loss:8.1430 train_time:164624ms step_avg:184.76ms\nstep:902/4500 train_loss:8.0198 train_time:164809ms step_avg:184.76ms\nstep:903/4500 train_loss:8.2179 train_time:164993ms step_avg:184.76ms\nstep:904/4500 train_loss:8.0627 train_time:165177ms step_avg:184.76ms\nstep:905/4500 train_loss:8.1469 train_time:165362ms step_avg:184.76ms\nstep:906/4500 train_loss:8.0482 train_time:165546ms step_avg:184.76ms\nstep:907/4500 train_loss:8.0607 train_time:165729ms step_avg:184.76ms\nstep:908/4500 train_loss:8.0386 train_time:165914ms step_avg:184.76ms\nstep:909/4500 train_loss:8.1581 train_time:166097ms step_avg:184.76ms\nstep:910/4500 train_loss:8.0651 train_time:166281ms step_avg:184.76ms\nstep:911/4500 train_loss:8.1654 train_time:166465ms step_avg:184.76ms\nstep:912/4500 train_loss:8.1274 train_time:166652ms step_avg:184.76ms\nstep:913/4500 train_loss:8.0949 train_time:166835ms step_avg:184.76ms\nstep:914/4500 train_loss:8.1928 train_time:167020ms step_avg:184.76ms\nstep:915/4500 train_loss:8.1610 train_time:167205ms step_avg:184.76ms\nstep:916/4500 train_loss:8.1418 train_time:167389ms step_avg:184.76ms\nstep:917/4500 train_loss:8.2036 train_time:167572ms step_avg:184.75ms\nstep:918/4500 train_loss:8.1798 train_time:167756ms step_avg:184.75ms\nstep:919/4500 train_loss:8.3501 train_time:167942ms step_avg:184.75ms\nstep:920/4500 train_loss:8.2216 train_time:168129ms step_avg:184.76ms\nstep:921/4500 train_loss:8.1613 train_time:168314ms step_avg:184.76ms\nstep:922/4500 train_loss:8.0908 train_time:168498ms step_avg:184.76ms\nstep:923/4500 train_loss:8.0231 train_time:168683ms step_avg:184.76ms\nstep:924/4500 train_loss:8.2051 train_time:168868ms step_avg:184.76ms\nstep:925/4500 train_loss:8.0080 train_time:169051ms step_avg:184.76ms\nstep:926/4500 train_loss:8.1969 train_time:169234ms step_avg:184.75ms\nstep:927/4500 train_loss:8.0696 train_time:169421ms step_avg:184.76ms\nstep:928/4500 train_loss:8.0081 train_time:169610ms step_avg:184.76ms\nstep:929/4500 train_loss:8.1298 train_time:169795ms step_avg:184.76ms\nstep:930/4500 train_loss:8.0530 train_time:169980ms step_avg:184.76ms\nstep:931/4500 train_loss:8.0819 train_time:170165ms step_avg:184.76ms\nstep:932/4500 train_loss:8.1511 train_time:170352ms step_avg:184.76ms\nstep:933/4500 train_loss:8.1651 train_time:170535ms step_avg:184.76ms\nstep:934/4500 train_loss:8.0421 train_time:170720ms step_avg:184.76ms\nstep:935/4500 train_loss:8.0749 train_time:170907ms step_avg:184.76ms\nstep:936/4500 train_loss:8.0939 train_time:171094ms step_avg:184.77ms\nstep:937/4500 train_loss:8.1204 train_time:171278ms step_avg:184.77ms\nstep:938/4500 train_loss:8.2941 train_time:171462ms step_avg:184.77ms\nstep:939/4500 train_loss:8.2280 train_time:171646ms step_avg:184.76ms\nstep:940/4500 train_loss:7.8436 train_time:171832ms step_avg:184.77ms\nstep:941/4500 train_loss:8.1766 train_time:172017ms step_avg:184.77ms\nstep:942/4500 train_loss:8.1791 train_time:172205ms step_avg:184.77ms\nstep:943/4500 train_loss:8.0141 train_time:172391ms step_avg:184.77ms\nstep:944/4500 train_loss:8.2454 train_time:172577ms step_avg:184.77ms\nstep:945/4500 train_loss:7.9978 train_time:172762ms step_avg:184.77ms\nstep:946/4500 train_loss:8.2171 train_time:172945ms step_avg:184.77ms\nstep:947/4500 train_loss:8.1689 train_time:173131ms step_avg:184.77ms\nstep:948/4500 train_loss:8.0977 train_time:173316ms step_avg:184.77ms\nstep:949/4500 train_loss:7.9046 train_time:173501ms step_avg:184.77ms\nstep:950/4500 train_loss:8.1524 train_time:173689ms step_avg:184.78ms\nstep:951/4500 train_loss:8.1858 train_time:173874ms step_avg:184.78ms\nstep:952/4500 train_loss:8.0585 train_time:174059ms step_avg:184.78ms\nstep:953/4500 train_loss:8.2861 train_time:174243ms step_avg:184.77ms\nstep:954/4500 train_loss:8.0175 train_time:174429ms step_avg:184.78ms\nstep:955/4500 train_loss:8.1380 train_time:174612ms step_avg:184.77ms\nstep:956/4500 train_loss:8.1593 train_time:174795ms step_avg:184.77ms\nstep:957/4500 train_loss:8.0587 train_time:174979ms step_avg:184.77ms\nstep:958/4500 train_loss:8.1439 train_time:175163ms step_avg:184.77ms\nstep:959/4500 train_loss:8.0649 train_time:175347ms step_avg:184.77ms\nstep:960/4500 train_loss:8.2089 train_time:175533ms step_avg:184.77ms\nstep:961/4500 train_loss:8.0542 train_time:175719ms step_avg:184.77ms\nstep:962/4500 train_loss:8.2994 train_time:175907ms step_avg:184.78ms\nstep:963/4500 train_loss:8.3165 train_time:176092ms step_avg:184.78ms\nstep:964/4500 train_loss:8.1454 train_time:176279ms step_avg:184.78ms\nstep:965/4500 train_loss:8.1687 train_time:176463ms step_avg:184.78ms\nstep:966/4500 train_loss:8.0856 train_time:176648ms step_avg:184.78ms\nstep:967/4500 train_loss:8.1840 train_time:176834ms step_avg:184.78ms\nstep:968/4500 train_loss:8.2602 train_time:177020ms step_avg:184.78ms\nstep:969/4500 train_loss:8.1978 train_time:177205ms step_avg:184.78ms\nstep:970/4500 train_loss:8.2297 train_time:177389ms step_avg:184.78ms\nstep:971/4500 train_loss:8.1542 train_time:177574ms step_avg:184.78ms\nstep:972/4500 train_loss:8.0894 train_time:177757ms step_avg:184.78ms\nstep:973/4500 train_loss:8.0872 train_time:177943ms step_avg:184.78ms\nstep:974/4500 train_loss:8.0581 train_time:178129ms step_avg:184.78ms\nstep:975/4500 train_loss:8.0201 train_time:178315ms step_avg:184.78ms\nstep:976/4500 train_loss:8.0315 train_time:178500ms step_avg:184.78ms\nstep:977/4500 train_loss:8.1293 train_time:178684ms step_avg:184.78ms\nstep:978/4500 train_loss:8.0641 train_time:178867ms step_avg:184.78ms\nstep:979/4500 train_loss:8.1679 train_time:179050ms step_avg:184.78ms\nstep:980/4500 train_loss:8.1182 train_time:179233ms step_avg:184.78ms\nstep:981/4500 train_loss:8.1748 train_time:179418ms step_avg:184.78ms\nstep:982/4500 train_loss:8.0823 train_time:179604ms step_avg:184.78ms\nstep:983/4500 train_loss:8.2154 train_time:179792ms step_avg:184.78ms\nstep:984/4500 train_loss:8.0896 train_time:179975ms step_avg:184.78ms\nstep:985/4500 train_loss:8.0912 train_time:180159ms step_avg:184.78ms\nstep:986/4500 train_loss:8.1520 train_time:180346ms step_avg:184.78ms\nstep:987/4500 train_loss:8.1930 train_time:180530ms step_avg:184.78ms\nstep:988/4500 train_loss:8.1164 train_time:180716ms step_avg:184.78ms\nstep:989/4500 train_loss:8.0140 train_time:180902ms step_avg:184.78ms\nstep:990/4500 train_loss:8.1491 train_time:181090ms step_avg:184.79ms\nstep:991/4500 train_loss:8.1537 train_time:181274ms step_avg:184.79ms\nstep:992/4500 train_loss:8.2835 train_time:181461ms step_avg:184.79ms\nstep:993/4500 train_loss:8.1277 train_time:181645ms step_avg:184.79ms\nstep:994/4500 train_loss:8.0602 train_time:181828ms step_avg:184.78ms\nstep:995/4500 train_loss:8.1574 train_time:182016ms step_avg:184.79ms\nstep:996/4500 train_loss:8.0856 train_time:182200ms step_avg:184.79ms\nstep:997/4500 train_loss:8.1439 train_time:182387ms step_avg:184.79ms\nstep:998/4500 train_loss:8.1521 train_time:182571ms step_avg:184.79ms\nstep:999/4500 train_loss:8.0917 train_time:182755ms step_avg:184.79ms\nstep:1000/4500 train_loss:8.0606 train_time:182939ms step_avg:184.79ms\nstep:1001/4500 train_loss:8.0881 train_time:183126ms step_avg:184.79ms\nstep:1002/4500 train_loss:8.1214 train_time:183314ms step_avg:184.79ms\nstep:1003/4500 train_loss:7.9985 train_time:183498ms step_avg:184.79ms\nstep:1004/4500 train_loss:8.1494 train_time:183682ms step_avg:184.79ms\nstep:1005/4500 train_loss:8.1656 train_time:183867ms step_avg:184.79ms\nstep:1006/4500 train_loss:8.0610 train_time:184051ms step_avg:184.79ms\nstep:1007/4500 train_loss:8.2847 train_time:184235ms step_avg:184.79ms\nstep:1008/4500 train_loss:8.0450 train_time:184421ms step_avg:184.79ms\nstep:1009/4500 train_loss:8.1165 train_time:184610ms step_avg:184.80ms\nstep:1010/4500 train_loss:8.0200 train_time:184795ms step_avg:184.79ms\nstep:1011/4500 train_loss:8.1235 train_time:184981ms step_avg:184.80ms\nstep:1012/4500 train_loss:8.0448 train_time:185169ms step_avg:184.80ms\nstep:1013/4500 train_loss:7.9990 train_time:185353ms step_avg:184.80ms\nstep:1014/4500 train_loss:8.0816 train_time:185538ms step_avg:184.80ms\nstep:1015/4500 train_loss:8.1033 train_time:185725ms step_avg:184.80ms\nstep:1016/4500 train_loss:8.1336 train_time:185909ms step_avg:184.80ms\nstep:1017/4500 train_loss:8.0697 train_time:186094ms step_avg:184.80ms\nstep:1018/4500 train_loss:8.1171 train_time:186278ms step_avg:184.80ms\nstep:1019/4500 train_loss:8.0194 train_time:186464ms step_avg:184.80ms\nstep:1020/4500 train_loss:8.1473 train_time:186647ms step_avg:184.80ms\nstep:1021/4500 train_loss:8.1987 train_time:186832ms step_avg:184.80ms\nstep:1022/4500 train_loss:8.0466 train_time:187016ms step_avg:184.80ms\nstep:1023/4500 train_loss:8.0711 train_time:187199ms step_avg:184.80ms\nstep:1024/4500 train_loss:8.2340 train_time:187383ms step_avg:184.80ms\nstep:1025/4500 train_loss:8.1290 train_time:187568ms step_avg:184.80ms\nstep:1026/4500 train_loss:8.1476 train_time:187753ms step_avg:184.80ms\nstep:1027/4500 train_loss:8.0213 train_time:187936ms step_avg:184.79ms\nstep:1028/4500 train_loss:8.2073 train_time:188124ms step_avg:184.80ms\nstep:1029/4500 train_loss:8.1240 train_time:188311ms step_avg:184.80ms\nstep:1030/4500 train_loss:8.2834 train_time:188496ms step_avg:184.80ms\nstep:1031/4500 train_loss:8.1029 train_time:188680ms step_avg:184.80ms\nstep:1032/4500 train_loss:8.0464 train_time:188865ms step_avg:184.80ms\nstep:1033/4500 train_loss:8.2347 train_time:189051ms step_avg:184.80ms\nstep:1034/4500 train_loss:8.0643 train_time:189238ms step_avg:184.80ms\nstep:1035/4500 train_loss:8.0273 train_time:189423ms step_avg:184.80ms\nstep:1036/4500 train_loss:8.0460 train_time:189607ms step_avg:184.80ms\nstep:1037/4500 train_loss:8.0478 train_time:189791ms step_avg:184.80ms\nstep:1038/4500 train_loss:8.0895 train_time:189974ms step_avg:184.80ms\nstep:1039/4500 train_loss:8.1034 train_time:190160ms step_avg:184.80ms\nstep:1040/4500 train_loss:8.0677 train_time:190345ms step_avg:184.80ms\nstep:1041/4500 train_loss:8.1958 train_time:190533ms step_avg:184.80ms\nstep:1042/4500 train_loss:8.0278 train_time:190718ms step_avg:184.80ms\nstep:1043/4500 train_loss:8.1790 train_time:190903ms step_avg:184.80ms\nstep:1044/4500 train_loss:8.1443 train_time:191087ms step_avg:184.80ms\nstep:1045/4500 train_loss:7.9515 train_time:191271ms step_avg:184.80ms\nstep:1046/4500 train_loss:8.0406 train_time:191455ms step_avg:184.80ms\nstep:1047/4500 train_loss:8.0482 train_time:191638ms step_avg:184.80ms\nstep:1048/4500 train_loss:8.0862 train_time:191823ms step_avg:184.80ms\nstep:1049/4500 train_loss:8.2611 train_time:192006ms step_avg:184.80ms\nstep:1050/4500 train_loss:8.1127 train_time:192190ms step_avg:184.80ms\nstep:1051/4500 train_loss:8.2019 train_time:192375ms step_avg:184.80ms\nstep:1052/4500 train_loss:8.1614 train_time:192562ms step_avg:184.80ms\nstep:1053/4500 train_loss:8.1619 train_time:192747ms step_avg:184.80ms\nstep:1054/4500 train_loss:8.2140 train_time:192935ms step_avg:184.80ms\nstep:1055/4500 train_loss:8.1407 train_time:193119ms step_avg:184.80ms\nstep:1056/4500 train_loss:8.0432 train_time:193305ms step_avg:184.80ms\nstep:1057/4500 train_loss:8.1241 train_time:193489ms step_avg:184.80ms\nstep:1058/4500 train_loss:8.3307 train_time:193676ms step_avg:184.81ms\nstep:1059/4500 train_loss:8.1195 train_time:193861ms step_avg:184.81ms\nstep:1060/4500 train_loss:8.0051 train_time:194045ms step_avg:184.81ms\nstep:1061/4500 train_loss:7.9432 train_time:194229ms step_avg:184.80ms\nstep:1062/4500 train_loss:8.1622 train_time:194413ms step_avg:184.80ms\nstep:1063/4500 train_loss:8.3402 train_time:194597ms step_avg:184.80ms\nstep:1064/4500 train_loss:8.0458 train_time:194782ms step_avg:184.80ms\nstep:1065/4500 train_loss:8.0013 train_time:194969ms step_avg:184.81ms\nstep:1066/4500 train_loss:8.1379 train_time:195152ms step_avg:184.80ms\nstep:1067/4500 train_loss:8.1550 train_time:195336ms step_avg:184.80ms\nstep:1068/4500 train_loss:8.1383 train_time:195522ms step_avg:184.80ms\nstep:1069/4500 train_loss:8.0298 train_time:195706ms step_avg:184.80ms\nstep:1070/4500 train_loss:8.0297 train_time:195894ms step_avg:184.81ms\nstep:1071/4500 train_loss:8.1592 train_time:196080ms step_avg:184.81ms\nstep:1072/4500 train_loss:7.9884 train_time:196262ms step_avg:184.80ms\nstep:1073/4500 train_loss:8.1687 train_time:196450ms step_avg:184.81ms\nstep:1074/4500 train_loss:8.0224 train_time:196636ms step_avg:184.81ms\nstep:1075/4500 train_loss:8.0742 train_time:196822ms step_avg:184.81ms\nstep:1076/4500 train_loss:8.1468 train_time:197006ms step_avg:184.81ms\nstep:1077/4500 train_loss:8.1848 train_time:197189ms step_avg:184.81ms\nstep:1078/4500 train_loss:8.1463 train_time:197372ms step_avg:184.81ms\nstep:1079/4500 train_loss:8.0497 train_time:197556ms step_avg:184.80ms\nstep:1080/4500 train_loss:8.1740 train_time:197740ms step_avg:184.80ms\nstep:1081/4500 train_loss:8.0433 train_time:197927ms step_avg:184.81ms\nstep:1082/4500 train_loss:8.1076 train_time:198115ms step_avg:184.81ms\nstep:1083/4500 train_loss:8.1013 train_time:198300ms step_avg:184.81ms\nstep:1084/4500 train_loss:8.1359 train_time:198483ms step_avg:184.81ms\nstep:1085/4500 train_loss:8.1422 train_time:198668ms step_avg:184.81ms\nstep:1086/4500 train_loss:8.1231 train_time:198854ms step_avg:184.81ms\nstep:1087/4500 train_loss:8.2141 train_time:199039ms step_avg:184.81ms\nstep:1088/4500 train_loss:7.9993 train_time:199229ms step_avg:184.81ms\nstep:1089/4500 train_loss:8.0443 train_time:199413ms step_avg:184.81ms\nstep:1090/4500 train_loss:8.0276 train_time:199596ms step_avg:184.81ms\nstep:1091/4500 train_loss:8.2148 train_time:199779ms step_avg:184.81ms\nstep:1092/4500 train_loss:8.0532 train_time:199963ms step_avg:184.81ms\nstep:1093/4500 train_loss:8.1639 train_time:200150ms step_avg:184.81ms\nstep:1094/4500 train_loss:8.3255 train_time:200336ms step_avg:184.81ms\nstep:1095/4500 train_loss:8.0076 train_time:200523ms step_avg:184.81ms\nstep:1096/4500 train_loss:8.1626 train_time:200708ms step_avg:184.81ms\nstep:1097/4500 train_loss:8.1802 train_time:200893ms step_avg:184.81ms\nstep:1098/4500 train_loss:8.0245 train_time:201075ms step_avg:184.81ms\nstep:1099/4500 train_loss:7.9876 train_time:201260ms step_avg:184.81ms\nstep:1100/4500 train_loss:8.1147 train_time:201447ms step_avg:184.81ms\nstep:1101/4500 train_loss:7.9148 train_time:201634ms step_avg:184.82ms\nstep:1102/4500 train_loss:8.1374 train_time:201820ms step_avg:184.82ms\nstep:1103/4500 train_loss:8.1566 train_time:202008ms step_avg:184.82ms\nstep:1104/4500 train_loss:8.1942 train_time:202193ms step_avg:184.82ms\nstep:1105/4500 train_loss:8.1325 train_time:202377ms step_avg:184.82ms\nstep:1106/4500 train_loss:8.1165 train_time:202562ms step_avg:184.82ms\nstep:1107/4500 train_loss:8.0805 train_time:202748ms step_avg:184.82ms\nstep:1108/4500 train_loss:8.0881 train_time:202935ms step_avg:184.82ms\nstep:1109/4500 train_loss:8.1207 train_time:203121ms step_avg:184.82ms\nstep:1110/4500 train_loss:8.0633 train_time:203306ms step_avg:184.82ms\nstep:1111/4500 train_loss:8.0213 train_time:203490ms step_avg:184.82ms\nstep:1112/4500 train_loss:8.1666 train_time:203674ms step_avg:184.82ms\nstep:1113/4500 train_loss:7.9941 train_time:203857ms step_avg:184.82ms\nstep:1114/4500 train_loss:8.1982 train_time:204042ms step_avg:184.82ms\nstep:1115/4500 train_loss:8.0939 train_time:204226ms step_avg:184.82ms\nstep:1116/4500 train_loss:8.0963 train_time:204410ms step_avg:184.82ms\nstep:1117/4500 train_loss:8.1079 train_time:204595ms step_avg:184.82ms\nstep:1118/4500 train_loss:8.0980 train_time:204780ms step_avg:184.82ms\nstep:1119/4500 train_loss:8.0928 train_time:204964ms step_avg:184.82ms\nstep:1120/4500 train_loss:7.9983 train_time:205147ms step_avg:184.82ms\nstep:1121/4500 train_loss:8.1683 train_time:205331ms step_avg:184.82ms\nstep:1122/4500 train_loss:8.0325 train_time:205517ms step_avg:184.82ms\nstep:1123/4500 train_loss:8.0078 train_time:205701ms step_avg:184.82ms\nstep:1124/4500 train_loss:8.1008 train_time:205886ms step_avg:184.82ms\nstep:1125/4500 train_loss:8.1264 train_time:206069ms step_avg:184.82ms\nstep:1126/4500 train_loss:8.0868 train_time:206252ms step_avg:184.81ms\nstep:1127/4500 train_loss:8.1122 train_time:206435ms step_avg:184.81ms\nstep:1128/4500 train_loss:8.2647 train_time:206620ms step_avg:184.81ms\nstep:1129/4500 train_loss:8.1074 train_time:206804ms step_avg:184.81ms\nstep:1130/4500 train_loss:8.2021 train_time:206989ms step_avg:184.81ms\nstep:1131/4500 train_loss:8.2754 train_time:207173ms step_avg:184.81ms\nstep:1132/4500 train_loss:8.1047 train_time:207359ms step_avg:184.81ms\nstep:1133/4500 train_loss:8.1098 train_time:207544ms step_avg:184.81ms\nstep:1134/4500 train_loss:8.1009 train_time:207728ms step_avg:184.81ms\nstep:1135/4500 train_loss:8.1334 train_time:207912ms step_avg:184.81ms\nstep:1136/4500 train_loss:8.1546 train_time:208096ms step_avg:184.81ms\nstep:1137/4500 train_loss:8.0281 train_time:208280ms step_avg:184.81ms\nstep:1138/4500 train_loss:8.0480 train_time:208464ms step_avg:184.81ms\nstep:1139/4500 train_loss:8.1713 train_time:208648ms step_avg:184.81ms\nstep:1140/4500 train_loss:8.1248 train_time:208832ms step_avg:184.81ms\nstep:1141/4500 train_loss:8.0826 train_time:209016ms step_avg:184.81ms\nstep:1142/4500 train_loss:8.0580 train_time:209204ms step_avg:184.81ms\nstep:1143/4500 train_loss:8.0590 train_time:209390ms step_avg:184.81ms\nstep:1144/4500 train_loss:8.1701 train_time:209571ms step_avg:184.81ms\nstep:1145/4500 train_loss:8.1663 train_time:209755ms step_avg:184.81ms\nstep:1146/4500 train_loss:8.1032 train_time:209940ms step_avg:184.81ms\nstep:1147/4500 train_loss:8.1102 train_time:210124ms step_avg:184.81ms\nstep:1148/4500 train_loss:8.1441 train_time:210309ms step_avg:184.81ms\nstep:1149/4500 train_loss:8.0799 train_time:210495ms step_avg:184.81ms\nstep:1150/4500 train_loss:8.0543 train_time:210679ms step_avg:184.81ms\nstep:1151/4500 train_loss:8.0782 train_time:210864ms step_avg:184.81ms\nstep:1152/4500 train_loss:8.1226 train_time:211049ms step_avg:184.81ms\nstep:1153/4500 train_loss:8.1274 train_time:211234ms step_avg:184.81ms\nstep:1154/4500 train_loss:8.2556 train_time:211420ms step_avg:184.81ms\nstep:1155/4500 train_loss:8.1160 train_time:211605ms step_avg:184.81ms\nstep:1156/4500 train_loss:8.0682 train_time:211792ms step_avg:184.81ms\nstep:1157/4500 train_loss:8.0134 train_time:211975ms step_avg:184.81ms\nstep:1158/4500 train_loss:8.2518 train_time:212161ms step_avg:184.81ms\nstep:1159/4500 train_loss:8.0472 train_time:212348ms step_avg:184.81ms\nstep:1160/4500 train_loss:8.0684 train_time:212535ms step_avg:184.81ms\nstep:1161/4500 train_loss:8.1485 train_time:212720ms step_avg:184.81ms\nstep:1162/4500 train_loss:8.1048 train_time:212908ms step_avg:184.82ms\nstep:1163/4500 train_loss:8.1162 train_time:213092ms step_avg:184.82ms\nstep:1164/4500 train_loss:8.1833 train_time:213277ms step_avg:184.82ms\nstep:1165/4500 train_loss:8.0676 train_time:213462ms step_avg:184.82ms\nstep:1166/4500 train_loss:8.0776 train_time:213649ms step_avg:184.82ms\nstep:1167/4500 train_loss:8.1575 train_time:213833ms step_avg:184.82ms\nstep:1168/4500 train_loss:8.0929 train_time:214016ms step_avg:184.82ms\nstep:1169/4500 train_loss:8.1705 train_time:214201ms step_avg:184.82ms\nstep:1170/4500 train_loss:8.1278 train_time:214386ms step_avg:184.82ms\nstep:1171/4500 train_loss:8.0582 train_time:214571ms step_avg:184.82ms\nstep:1172/4500 train_loss:8.1071 train_time:214755ms step_avg:184.81ms\nstep:1173/4500 train_loss:8.1076 train_time:214940ms step_avg:184.81ms\nstep:1174/4500 train_loss:8.0942 train_time:215123ms step_avg:184.81ms\nstep:1175/4500 train_loss:8.0651 train_time:215310ms step_avg:184.82ms\nstep:1176/4500 train_loss:8.0945 train_time:215494ms step_avg:184.81ms\nstep:1177/4500 train_loss:8.2365 train_time:215678ms step_avg:184.81ms\nstep:1178/4500 train_loss:8.0770 train_time:215866ms step_avg:184.82ms\nstep:1179/4500 train_loss:8.1247 train_time:216054ms step_avg:184.82ms\nstep:1180/4500 train_loss:8.0744 train_time:216239ms step_avg:184.82ms\nstep:1181/4500 train_loss:7.9592 train_time:216424ms step_avg:184.82ms\nstep:1182/4500 train_loss:8.2089 train_time:216609ms step_avg:184.82ms\nstep:1183/4500 train_loss:8.0842 train_time:216793ms step_avg:184.82ms\nstep:1184/4500 train_loss:8.0182 train_time:216978ms step_avg:184.82ms\nstep:1185/4500 train_loss:8.1266 train_time:217163ms step_avg:184.82ms\nstep:1186/4500 train_loss:8.1365 train_time:217346ms step_avg:184.82ms\nstep:1187/4500 train_loss:8.2033 train_time:217533ms step_avg:184.82ms\nstep:1188/4500 train_loss:8.0280 train_time:217717ms step_avg:184.82ms\nstep:1189/4500 train_loss:8.3096 train_time:217903ms step_avg:184.82ms\nstep:1190/4500 train_loss:8.2382 train_time:218090ms step_avg:184.82ms\nstep:1191/4500 train_loss:8.1559 train_time:218274ms step_avg:184.82ms\nstep:1192/4500 train_loss:8.0890 train_time:218458ms step_avg:184.82ms\nstep:1193/4500 train_loss:8.2271 train_time:218644ms step_avg:184.82ms\nstep:1194/4500 train_loss:8.1019 train_time:218832ms step_avg:184.82ms\nstep:1195/4500 train_loss:8.0914 train_time:219016ms step_avg:184.82ms\nstep:1196/4500 train_loss:8.0799 train_time:219200ms step_avg:184.82ms\nstep:1197/4500 train_loss:8.0947 train_time:219383ms step_avg:184.82ms\nstep:1198/4500 train_loss:8.2866 train_time:219568ms step_avg:184.82ms\nstep:1199/4500 train_loss:8.0067 train_time:219753ms step_avg:184.82ms\nstep:1200/4500 train_loss:8.1869 train_time:219937ms step_avg:184.82ms\nstep:1201/4500 train_loss:8.1454 train_time:220126ms step_avg:184.82ms\nstep:1202/4500 train_loss:8.1362 train_time:220308ms step_avg:184.82ms\nstep:1203/4500 train_loss:8.0825 train_time:220494ms step_avg:184.82ms\nstep:1204/4500 train_loss:8.1250 train_time:220678ms step_avg:184.82ms\nstep:1205/4500 train_loss:8.1161 train_time:220862ms step_avg:184.82ms\nstep:1206/4500 train_loss:8.0947 train_time:221046ms step_avg:184.82ms\nstep:1207/4500 train_loss:8.1566 train_time:221231ms step_avg:184.82ms\nstep:1208/4500 train_loss:8.0834 train_time:221416ms step_avg:184.82ms\nstep:1209/4500 train_loss:8.1920 train_time:221601ms step_avg:184.82ms\nstep:1210/4500 train_loss:8.1147 train_time:221786ms step_avg:184.82ms\nstep:1211/4500 train_loss:8.1370 train_time:221973ms step_avg:184.82ms\nstep:1212/4500 train_loss:8.1478 train_time:222157ms step_avg:184.82ms\nstep:1213/4500 train_loss:8.0207 train_time:222342ms step_avg:184.82ms\nstep:1214/4500 train_loss:8.1658 train_time:222527ms step_avg:184.82ms\nstep:1215/4500 train_loss:8.0897 train_time:222712ms step_avg:184.82ms\nstep:1216/4500 train_loss:8.1561 train_time:222898ms step_avg:184.82ms\nstep:1217/4500 train_loss:8.1801 train_time:223085ms step_avg:184.83ms\nstep:1218/4500 train_loss:8.0972 train_time:223271ms step_avg:184.83ms\nstep:1219/4500 train_loss:8.1903 train_time:223455ms step_avg:184.83ms\nstep:1220/4500 train_loss:8.1883 train_time:223640ms step_avg:184.83ms\nstep:1221/4500 train_loss:8.0496 train_time:223829ms step_avg:184.83ms\nstep:1222/4500 train_loss:8.1971 train_time:224012ms step_avg:184.83ms\nstep:1223/4500 train_loss:7.9546 train_time:224197ms step_avg:184.83ms\nstep:1224/4500 train_loss:8.0783 train_time:224383ms step_avg:184.83ms\nstep:1225/4500 train_loss:8.0741 train_time:224568ms step_avg:184.83ms\nstep:1226/4500 train_loss:8.1142 train_time:224753ms step_avg:184.83ms\nstep:1227/4500 train_loss:8.2343 train_time:224938ms step_avg:184.83ms\nstep:1228/4500 train_loss:8.0429 train_time:225126ms step_avg:184.83ms\nstep:1229/4500 train_loss:8.1164 train_time:225311ms step_avg:184.83ms\nstep:1230/4500 train_loss:8.1333 train_time:225496ms step_avg:184.83ms\nstep:1231/4500 train_loss:8.2325 train_time:225680ms step_avg:184.83ms\nstep:1232/4500 train_loss:8.2086 train_time:225864ms step_avg:184.83ms\nstep:1233/4500 train_loss:8.0286 train_time:226051ms step_avg:184.83ms\nstep:1234/4500 train_loss:8.2116 train_time:226234ms step_avg:184.83ms\nstep:1235/4500 train_loss:8.0367 train_time:226417ms step_avg:184.83ms\nstep:1236/4500 train_loss:8.0025 train_time:226602ms step_avg:184.83ms\nstep:1237/4500 train_loss:8.1338 train_time:226790ms step_avg:184.83ms\nstep:1238/4500 train_loss:8.1038 train_time:226973ms step_avg:184.83ms\nstep:1239/4500 train_loss:8.1827 train_time:227156ms step_avg:184.83ms\nstep:1240/4500 train_loss:8.0977 train_time:227341ms step_avg:184.83ms\nstep:1241/4500 train_loss:8.1381 train_time:227527ms step_avg:184.83ms\nstep:1242/4500 train_loss:8.1113 train_time:227710ms step_avg:184.83ms\nstep:1243/4500 train_loss:8.2128 train_time:227898ms step_avg:184.83ms\nstep:1244/4500 train_loss:8.1752 train_time:228086ms step_avg:184.83ms\nstep:1245/4500 train_loss:8.1514 train_time:228270ms step_avg:184.83ms\nstep:1246/4500 train_loss:8.1143 train_time:228454ms step_avg:184.83ms\nstep:1247/4500 train_loss:8.2252 train_time:228640ms step_avg:184.83ms\nstep:1248/4500 train_loss:8.1141 train_time:228825ms step_avg:184.83ms\nstep:1249/4500 train_loss:8.1293 train_time:229012ms step_avg:184.84ms\nstep:1250/4500 train_loss:8.0536 train_time:229195ms step_avg:184.84ms\nstep:1251/4500 train_loss:8.1279 train_time:229381ms step_avg:184.84ms\nstep:1252/4500 train_loss:8.3242 train_time:229567ms step_avg:184.84ms\nstep:1253/4500 train_loss:8.1617 train_time:229751ms step_avg:184.84ms\nstep:1254/4500 train_loss:8.0891 train_time:229935ms step_avg:184.84ms\nstep:1255/4500 train_loss:8.2172 train_time:230120ms step_avg:184.84ms\nstep:1256/4500 train_loss:8.2681 train_time:230306ms step_avg:184.84ms\nstep:1257/4500 train_loss:8.1512 train_time:230493ms step_avg:184.84ms\nstep:1258/4500 train_loss:8.1499 train_time:230681ms step_avg:184.84ms\nstep:1259/4500 train_loss:8.1348 train_time:230885ms step_avg:184.86ms\nstep:1260/4500 train_loss:8.1637 train_time:231068ms step_avg:184.85ms\nstep:1261/4500 train_loss:8.2306 train_time:231251ms step_avg:184.85ms\nstep:1262/4500 train_loss:8.0078 train_time:231435ms step_avg:184.85ms\nstep:1263/4500 train_loss:8.2061 train_time:231621ms step_avg:184.85ms\nstep:1264/4500 train_loss:8.1567 train_time:231806ms step_avg:184.85ms\nstep:1265/4500 train_loss:8.1280 train_time:231993ms step_avg:184.86ms\nstep:1266/4500 train_loss:8.1829 train_time:232177ms step_avg:184.85ms\nstep:1267/4500 train_loss:8.0979 train_time:232360ms step_avg:184.85ms\nstep:1268/4500 train_loss:8.1110 train_time:232544ms step_avg:184.85ms\nstep:1269/4500 train_loss:8.0094 train_time:232730ms step_avg:184.85ms\nstep:1270/4500 train_loss:8.1364 train_time:232914ms step_avg:184.85ms\nstep:1271/4500 train_loss:8.1179 train_time:233100ms step_avg:184.85ms\nstep:1272/4500 train_loss:8.0311 train_time:233287ms step_avg:184.85ms\nstep:1273/4500 train_loss:8.0152 train_time:233471ms step_avg:184.85ms\nstep:1274/4500 train_loss:8.0584 train_time:233653ms step_avg:184.85ms\nstep:1275/4500 train_loss:8.1194 train_time:233837ms step_avg:184.85ms\nstep:1276/4500 train_loss:8.0443 train_time:234021ms step_avg:184.85ms\nstep:1277/4500 train_loss:8.1944 train_time:234207ms step_avg:184.85ms\nstep:1278/4500 train_loss:8.1527 train_time:234395ms step_avg:184.85ms\nstep:1279/4500 train_loss:8.1652 train_time:234580ms step_avg:184.85ms\nstep:1280/4500 train_loss:7.9978 train_time:234763ms step_avg:184.85ms\nstep:1281/4500 train_loss:8.1217 train_time:234947ms step_avg:184.85ms\nstep:1282/4500 train_loss:8.0109 train_time:235131ms step_avg:184.85ms\nstep:1283/4500 train_loss:8.0981 train_time:235313ms step_avg:184.85ms\nstep:1284/4500 train_loss:8.1181 train_time:235497ms step_avg:184.85ms\nstep:1285/4500 train_loss:8.0498 train_time:235681ms step_avg:184.85ms\nstep:1286/4500 train_loss:8.1811 train_time:235867ms step_avg:184.85ms\nstep:1287/4500 train_loss:8.1885 train_time:236052ms step_avg:184.85ms\nstep:1288/4500 train_loss:8.0178 train_time:236235ms step_avg:184.85ms\nstep:1289/4500 train_loss:8.0969 train_time:236419ms step_avg:184.85ms\nstep:1290/4500 train_loss:8.0829 train_time:236604ms step_avg:184.85ms\nstep:1291/4500 train_loss:8.0640 train_time:236788ms step_avg:184.85ms\nstep:1292/4500 train_loss:8.1550 train_time:236973ms step_avg:184.85ms\nstep:1293/4500 train_loss:8.1338 train_time:237155ms step_avg:184.84ms\nstep:1294/4500 train_loss:8.1547 train_time:237339ms step_avg:184.84ms\nstep:1295/4500 train_loss:8.1454 train_time:237523ms step_avg:184.84ms\nstep:1296/4500 train_loss:8.3640 train_time:237707ms step_avg:184.84ms\nstep:1297/4500 train_loss:8.0665 train_time:237893ms step_avg:184.84ms\nstep:1298/4500 train_loss:8.1292 train_time:238078ms step_avg:184.84ms\nstep:1299/4500 train_loss:8.1603 train_time:238265ms step_avg:184.84ms\nstep:1300/4500 train_loss:8.2019 train_time:238448ms step_avg:184.84ms\nstep:1301/4500 train_loss:8.1410 train_time:238632ms step_avg:184.84ms\nstep:1302/4500 train_loss:8.0798 train_time:238815ms step_avg:184.84ms\nstep:1303/4500 train_loss:8.2478 train_time:239000ms step_avg:184.84ms\nstep:1304/4500 train_loss:8.1127 train_time:239185ms step_avg:184.84ms\nstep:1305/4500 train_loss:8.0764 train_time:239370ms step_avg:184.84ms\nstep:1306/4500 train_loss:8.1536 train_time:239554ms step_avg:184.84ms\nstep:1307/4500 train_loss:8.1830 train_time:239738ms step_avg:184.84ms\nstep:1308/4500 train_loss:8.2552 train_time:239923ms step_avg:184.84ms\nstep:1309/4500 train_loss:7.9779 train_time:240107ms step_avg:184.84ms\nstep:1310/4500 train_loss:7.9848 train_time:240290ms step_avg:184.84ms\nstep:1311/4500 train_loss:8.1216 train_time:240476ms step_avg:184.84ms\nstep:1312/4500 train_loss:8.2199 train_time:240664ms step_avg:184.84ms\nstep:1313/4500 train_loss:8.2181 train_time:240849ms step_avg:184.84ms\nstep:1314/4500 train_loss:8.2283 train_time:241037ms step_avg:184.84ms\nstep:1315/4500 train_loss:8.1199 train_time:241221ms step_avg:184.84ms\nstep:1316/4500 train_loss:8.1345 train_time:241405ms step_avg:184.84ms\nstep:1317/4500 train_loss:8.0459 train_time:241589ms step_avg:184.84ms\nstep:1318/4500 train_loss:8.0301 train_time:241772ms step_avg:184.84ms\nstep:1319/4500 train_loss:8.1178 train_time:241957ms step_avg:184.84ms\nstep:1320/4500 train_loss:8.0958 train_time:242141ms step_avg:184.84ms\nstep:1321/4500 train_loss:8.1592 train_time:242328ms step_avg:184.84ms\nstep:1322/4500 train_loss:8.1432 train_time:242512ms step_avg:184.84ms\nstep:1323/4500 train_loss:8.1147 train_time:242698ms step_avg:184.84ms\nstep:1324/4500 train_loss:8.1144 train_time:242883ms step_avg:184.84ms\nstep:1325/4500 train_loss:8.0642 train_time:243067ms step_avg:184.84ms\nstep:1326/4500 train_loss:8.3987 train_time:243252ms step_avg:184.84ms\nstep:1327/4500 train_loss:8.0927 train_time:243437ms step_avg:184.84ms\nstep:1328/4500 train_loss:7.9949 train_time:243622ms step_avg:184.84ms\nstep:1329/4500 train_loss:8.0777 train_time:243807ms step_avg:184.84ms\nstep:1330/4500 train_loss:8.0678 train_time:243991ms step_avg:184.84ms\nstep:1331/4500 train_loss:8.0277 train_time:244174ms step_avg:184.84ms\nstep:1332/4500 train_loss:8.0743 train_time:244358ms step_avg:184.84ms\nstep:1333/4500 train_loss:8.0402 train_time:244542ms step_avg:184.84ms\nstep:1334/4500 train_loss:8.1000 train_time:244728ms step_avg:184.84ms\nstep:1335/4500 train_loss:8.0968 train_time:244911ms step_avg:184.84ms\nstep:1336/4500 train_loss:8.0548 train_time:245096ms step_avg:184.84ms\nstep:1337/4500 train_loss:8.1376 train_time:245280ms step_avg:184.84ms\nstep:1338/4500 train_loss:8.1365 train_time:245464ms step_avg:184.84ms\nstep:1339/4500 train_loss:8.1039 train_time:245650ms step_avg:184.84ms\nstep:1340/4500 train_loss:8.2261 train_time:245834ms step_avg:184.84ms\nstep:1341/4500 train_loss:8.1948 train_time:246018ms step_avg:184.84ms\nstep:1342/4500 train_loss:8.1335 train_time:246203ms step_avg:184.84ms\nstep:1343/4500 train_loss:8.0286 train_time:246388ms step_avg:184.84ms\nstep:1344/4500 train_loss:8.1560 train_time:246574ms step_avg:184.84ms\nstep:1345/4500 train_loss:8.0311 train_time:246759ms step_avg:184.84ms\nstep:1346/4500 train_loss:8.0696 train_time:246945ms step_avg:184.84ms\nstep:1347/4500 train_loss:8.0529 train_time:247132ms step_avg:184.84ms\nstep:1348/4500 train_loss:8.3101 train_time:247314ms step_avg:184.84ms\nstep:1349/4500 train_loss:8.6110 train_time:247497ms step_avg:184.84ms\nstep:1350/4500 train_loss:8.1668 train_time:247681ms step_avg:184.84ms\nstep:1351/4500 train_loss:8.1514 train_time:247867ms step_avg:184.84ms\nstep:1352/4500 train_loss:8.1587 train_time:248053ms step_avg:184.84ms\nstep:1353/4500 train_loss:8.1722 train_time:248237ms step_avg:184.84ms\nstep:1354/4500 train_loss:8.0364 train_time:248421ms step_avg:184.84ms\nstep:1355/4500 train_loss:8.1940 train_time:248605ms step_avg:184.84ms\nstep:1356/4500 train_loss:8.1795 train_time:248789ms step_avg:184.84ms\nstep:1357/4500 train_loss:8.1183 train_time:248972ms step_avg:184.83ms\nstep:1358/4500 train_loss:8.1063 train_time:249158ms step_avg:184.84ms\nstep:1359/4500 train_loss:8.0948 train_time:249346ms step_avg:184.84ms\nstep:1360/4500 train_loss:8.0228 train_time:249532ms step_avg:184.84ms\nstep:1361/4500 train_loss:8.0362 train_time:249716ms step_avg:184.84ms\nstep:1362/4500 train_loss:7.9974 train_time:249901ms step_avg:184.84ms\nstep:1363/4500 train_loss:8.0219 train_time:250084ms step_avg:184.84ms\nstep:1364/4500 train_loss:8.0744 train_time:250270ms step_avg:184.84ms\nstep:1365/4500 train_loss:8.1289 train_time:250455ms step_avg:184.84ms\nstep:1366/4500 train_loss:8.1477 train_time:250639ms step_avg:184.84ms\nstep:1367/4500 train_loss:7.9847 train_time:250826ms step_avg:184.84ms\nstep:1368/4500 train_loss:8.0982 train_time:251011ms step_avg:184.84ms\nstep:1369/4500 train_loss:8.1305 train_time:251194ms step_avg:184.84ms\nstep:1370/4500 train_loss:8.0720 train_time:251379ms step_avg:184.84ms\nstep:1371/4500 train_loss:8.1427 train_time:251567ms step_avg:184.84ms\nstep:1372/4500 train_loss:8.1407 train_time:251750ms step_avg:184.84ms\nstep:1373/4500 train_loss:8.0087 train_time:251935ms step_avg:184.84ms\nstep:1374/4500 train_loss:8.2090 train_time:252120ms step_avg:184.84ms\nstep:1375/4500 train_loss:8.0648 train_time:252306ms step_avg:184.84ms\nstep:1376/4500 train_loss:8.1588 train_time:252490ms step_avg:184.84ms\nstep:1377/4500 train_loss:7.9535 train_time:252674ms step_avg:184.84ms\nstep:1378/4500 train_loss:8.1120 train_time:252859ms step_avg:184.84ms\nstep:1379/4500 train_loss:8.2112 train_time:253043ms step_avg:184.84ms\nstep:1380/4500 train_loss:8.0805 train_time:253227ms step_avg:184.84ms\nstep:1381/4500 train_loss:8.0355 train_time:253410ms step_avg:184.84ms\nstep:1382/4500 train_loss:8.1154 train_time:253593ms step_avg:184.83ms\nstep:1383/4500 train_loss:8.0921 train_time:253778ms step_avg:184.83ms\nstep:1384/4500 train_loss:8.1453 train_time:253965ms step_avg:184.84ms\nstep:1385/4500 train_loss:8.0422 train_time:254149ms step_avg:184.84ms\nstep:1386/4500 train_loss:8.1621 train_time:254333ms step_avg:184.84ms\nstep:1387/4500 train_loss:7.9862 train_time:254517ms step_avg:184.83ms\nstep:1388/4500 train_loss:8.1741 train_time:254701ms step_avg:184.83ms\nstep:1389/4500 train_loss:8.0301 train_time:254885ms step_avg:184.83ms\nstep:1390/4500 train_loss:7.9936 train_time:255070ms step_avg:184.83ms\nstep:1391/4500 train_loss:8.0548 train_time:255254ms step_avg:184.83ms\nstep:1392/4500 train_loss:8.0943 train_time:255440ms step_avg:184.83ms\nstep:1393/4500 train_loss:8.1032 train_time:255627ms step_avg:184.84ms\nstep:1394/4500 train_loss:8.1481 train_time:255812ms step_avg:184.84ms\nstep:1395/4500 train_loss:7.9892 train_time:255998ms step_avg:184.84ms\nstep:1396/4500 train_loss:8.0120 train_time:256182ms step_avg:184.84ms\nstep:1397/4500 train_loss:7.9040 train_time:256367ms step_avg:184.84ms\nstep:1398/4500 train_loss:8.1581 train_time:256553ms step_avg:184.84ms\nstep:1399/4500 train_loss:7.8982 train_time:256738ms step_avg:184.84ms\nstep:1400/4500 train_loss:7.7841 train_time:256923ms step_avg:184.84ms\nstep:1401/4500 train_loss:8.0573 train_time:257107ms step_avg:184.84ms\nstep:1402/4500 train_loss:8.1259 train_time:257293ms step_avg:184.84ms\nstep:1403/4500 train_loss:8.0312 train_time:257476ms step_avg:184.84ms\nstep:1404/4500 train_loss:8.0306 train_time:257660ms step_avg:184.83ms\nstep:1405/4500 train_loss:8.1688 train_time:257845ms step_avg:184.83ms\nstep:1406/4500 train_loss:8.1169 train_time:258028ms step_avg:184.83ms\nstep:1407/4500 train_loss:8.1344 train_time:258214ms step_avg:184.83ms\nstep:1408/4500 train_loss:8.1777 train_time:258401ms step_avg:184.84ms\nstep:1409/4500 train_loss:8.0762 train_time:258587ms step_avg:184.84ms\nstep:1410/4500 train_loss:8.1128 train_time:258773ms step_avg:184.84ms\nstep:1411/4500 train_loss:8.0082 train_time:258958ms step_avg:184.84ms\nstep:1412/4500 train_loss:8.0640 train_time:259142ms step_avg:184.84ms\nstep:1413/4500 train_loss:8.1478 train_time:259328ms step_avg:184.84ms\nstep:1414/4500 train_loss:8.1063 train_time:259515ms step_avg:184.84ms\nstep:1415/4500 train_loss:8.1152 train_time:259701ms step_avg:184.84ms\nstep:1416/4500 train_loss:8.0938 train_time:259889ms step_avg:184.84ms\nstep:1417/4500 train_loss:8.0522 train_time:260075ms step_avg:184.84ms\nstep:1418/4500 train_loss:8.1554 train_time:260259ms step_avg:184.84ms\nstep:1419/4500 train_loss:8.0505 train_time:260444ms step_avg:184.84ms\nstep:1420/4500 train_loss:8.1474 train_time:260628ms step_avg:184.84ms\nstep:1421/4500 train_loss:8.1258 train_time:260814ms step_avg:184.84ms\nstep:1422/4500 train_loss:8.0823 train_time:261001ms step_avg:184.84ms\nstep:1423/4500 train_loss:8.1583 train_time:261186ms step_avg:184.84ms\nstep:1424/4500 train_loss:8.0727 train_time:261368ms step_avg:184.84ms\nstep:1425/4500 train_loss:8.0573 train_time:261553ms step_avg:184.84ms\nstep:1426/4500 train_loss:8.0914 train_time:261736ms step_avg:184.84ms\nstep:1427/4500 train_loss:7.9926 train_time:261923ms step_avg:184.84ms\nstep:1428/4500 train_loss:8.2276 train_time:262110ms step_avg:184.85ms\nstep:1429/4500 train_loss:7.9996 train_time:262295ms step_avg:184.85ms\nstep:1430/4500 train_loss:8.1142 train_time:262480ms step_avg:184.85ms\nstep:1431/4500 train_loss:8.1858 train_time:262664ms step_avg:184.84ms\nstep:1432/4500 train_loss:8.1279 train_time:262849ms step_avg:184.84ms\nstep:1433/4500 train_loss:8.0762 train_time:263033ms step_avg:184.84ms\nstep:1434/4500 train_loss:8.1566 train_time:263218ms step_avg:184.84ms\nstep:1435/4500 train_loss:8.1977 train_time:263404ms step_avg:184.85ms\nstep:1436/4500 train_loss:8.4532 train_time:263594ms step_avg:184.85ms\nstep:1437/4500 train_loss:8.1274 train_time:263782ms step_avg:184.85ms\nstep:1438/4500 train_loss:7.9809 train_time:263962ms step_avg:184.85ms\nstep:1439/4500 train_loss:8.1573 train_time:264146ms step_avg:184.85ms\nstep:1440/4500 train_loss:8.0083 train_time:264334ms step_avg:184.85ms\nstep:1441/4500 train_loss:8.1382 train_time:264518ms step_avg:184.85ms\nstep:1442/4500 train_loss:8.2322 train_time:264702ms step_avg:184.85ms\nstep:1443/4500 train_loss:8.1762 train_time:264887ms step_avg:184.85ms\nstep:1444/4500 train_loss:8.2094 train_time:265074ms step_avg:184.85ms\nstep:1445/4500 train_loss:8.1424 train_time:265256ms step_avg:184.85ms\nstep:1446/4500 train_loss:8.1363 train_time:265440ms step_avg:184.85ms\nstep:1447/4500 train_loss:8.0307 train_time:265627ms step_avg:184.85ms\nstep:1448/4500 train_loss:8.2222 train_time:265812ms step_avg:184.85ms\nstep:1449/4500 train_loss:8.1665 train_time:265996ms step_avg:184.85ms\nstep:1450/4500 train_loss:8.1474 train_time:266180ms step_avg:184.85ms\nstep:1451/4500 train_loss:8.1296 train_time:266367ms step_avg:184.85ms\nstep:1452/4500 train_loss:8.0812 train_time:266554ms step_avg:184.85ms\nstep:1453/4500 train_loss:8.0597 train_time:266738ms step_avg:184.85ms\nstep:1454/4500 train_loss:8.1416 train_time:266925ms step_avg:184.85ms\nstep:1455/4500 train_loss:8.0518 train_time:267110ms step_avg:184.85ms\nstep:1456/4500 train_loss:8.1566 train_time:267294ms step_avg:184.85ms\nstep:1457/4500 train_loss:8.8087 train_time:267478ms step_avg:184.85ms\nstep:1458/4500 train_loss:8.1682 train_time:267662ms step_avg:184.85ms\nstep:1459/4500 train_loss:8.1535 train_time:267846ms step_avg:184.85ms\nstep:1460/4500 train_loss:8.1422 train_time:268034ms step_avg:184.85ms\nstep:1461/4500 train_loss:8.0744 train_time:268217ms step_avg:184.85ms\nstep:1462/4500 train_loss:8.2186 train_time:268401ms step_avg:184.85ms\nstep:1463/4500 train_loss:8.1506 train_time:268586ms step_avg:184.85ms\nstep:1464/4500 train_loss:8.0833 train_time:268768ms step_avg:184.85ms\nstep:1465/4500 train_loss:8.0596 train_time:268952ms step_avg:184.85ms\nstep:1466/4500 train_loss:8.1010 train_time:269137ms step_avg:184.85ms\nstep:1467/4500 train_loss:8.1446 train_time:269323ms step_avg:184.85ms\nstep:1468/4500 train_loss:8.0876 train_time:269509ms step_avg:184.85ms\nstep:1469/4500 train_loss:8.1491 train_time:269696ms step_avg:184.85ms\nstep:1470/4500 train_loss:8.1459 train_time:269881ms step_avg:184.85ms\nstep:1471/4500 train_loss:8.0883 train_time:270069ms step_avg:184.85ms\nstep:1472/4500 train_loss:8.1106 train_time:270254ms step_avg:184.85ms\nstep:1473/4500 train_loss:8.0078 train_time:270438ms step_avg:184.85ms\nstep:1474/4500 train_loss:8.1453 train_time:270622ms step_avg:184.85ms\nstep:1475/4500 train_loss:8.1021 train_time:270806ms step_avg:184.85ms\nstep:1476/4500 train_loss:8.0291 train_time:270989ms step_avg:184.85ms\nstep:1477/4500 train_loss:8.2283 train_time:271175ms step_avg:184.85ms\nstep:1478/4500 train_loss:8.0677 train_time:271365ms step_avg:184.85ms\nstep:1479/4500 train_loss:8.0123 train_time:271547ms step_avg:184.85ms\nstep:1480/4500 train_loss:8.0553 train_time:271728ms step_avg:184.85ms\nstep:1481/4500 train_loss:8.0762 train_time:271912ms step_avg:184.85ms\nstep:1482/4500 train_loss:8.0235 train_time:272095ms step_avg:184.85ms\nstep:1483/4500 train_loss:8.1322 train_time:272280ms step_avg:184.85ms\nstep:1484/4500 train_loss:8.2552 train_time:272468ms step_avg:184.85ms\nstep:1485/4500 train_loss:8.1118 train_time:272656ms step_avg:184.85ms\nstep:1486/4500 train_loss:8.1113 train_time:272841ms step_avg:184.85ms\nstep:1487/4500 train_loss:8.0875 train_time:273028ms step_avg:184.85ms\nstep:1488/4500 train_loss:8.0396 train_time:273215ms step_avg:184.85ms\nstep:1489/4500 train_loss:8.0354 train_time:273400ms step_avg:184.85ms\nstep:1490/4500 train_loss:8.0548 train_time:273589ms step_avg:184.86ms\nstep:1491/4500 train_loss:8.3200 train_time:273773ms step_avg:184.86ms\nstep:1492/4500 train_loss:8.1012 train_time:273956ms step_avg:184.86ms\nstep:1493/4500 train_loss:8.1138 train_time:274140ms step_avg:184.86ms\nstep:1494/4500 train_loss:8.1796 train_time:274325ms step_avg:184.86ms\nstep:1495/4500 train_loss:8.1426 train_time:274511ms step_avg:184.86ms\nstep:1496/4500 train_loss:8.0949 train_time:274696ms step_avg:184.86ms\nstep:1497/4500 train_loss:8.0173 train_time:274882ms step_avg:184.86ms\nstep:1498/4500 train_loss:8.0542 train_time:275066ms step_avg:184.86ms\nstep:1499/4500 train_loss:8.0369 train_time:275250ms step_avg:184.86ms\nstep:1500/4500 train_loss:8.0966 train_time:275433ms step_avg:184.85ms\nstep:1501/4500 train_loss:8.0514 train_time:275617ms step_avg:184.85ms\nstep:1502/4500 train_loss:8.2213 train_time:275803ms step_avg:184.85ms\nstep:1503/4500 train_loss:8.1259 train_time:275990ms step_avg:184.86ms\nstep:1504/4500 train_loss:8.1798 train_time:276175ms step_avg:184.86ms\nstep:1505/4500 train_loss:8.2055 train_time:276360ms step_avg:184.86ms\nstep:1506/4500 train_loss:8.1095 train_time:276545ms step_avg:184.86ms\nstep:1507/4500 train_loss:8.0043 train_time:276729ms step_avg:184.86ms\nstep:1508/4500 train_loss:8.0598 train_time:276913ms step_avg:184.86ms\nstep:1509/4500 train_loss:8.0588 train_time:277096ms step_avg:184.85ms\nstep:1510/4500 train_loss:7.4424 train_time:277281ms step_avg:184.85ms\nstep:1511/4500 train_loss:8.0035 train_time:277467ms step_avg:184.85ms\nstep:1512/4500 train_loss:8.1255 train_time:277650ms step_avg:184.85ms\nstep:1513/4500 train_loss:8.1564 train_time:277836ms step_avg:184.85ms\nstep:1514/4500 train_loss:7.9957 train_time:278023ms step_avg:184.86ms\nstep:1515/4500 train_loss:7.9981 train_time:278211ms step_avg:184.86ms\nstep:1516/4500 train_loss:8.2266 train_time:278395ms step_avg:184.86ms\nstep:1517/4500 train_loss:8.1068 train_time:278580ms step_avg:184.86ms\nstep:1518/4500 train_loss:8.1188 train_time:278765ms step_avg:184.86ms\nstep:1519/4500 train_loss:8.2836 train_time:278949ms step_avg:184.86ms\nstep:1520/4500 train_loss:8.1978 train_time:279135ms step_avg:184.86ms\nstep:1521/4500 train_loss:8.0628 train_time:279320ms step_avg:184.86ms\nstep:1522/4500 train_loss:8.1524 train_time:279505ms step_avg:184.86ms\nstep:1523/4500 train_loss:8.1824 train_time:279689ms step_avg:184.86ms\nstep:1524/4500 train_loss:8.0753 train_time:280102ms step_avg:185.01ms\nstep:1525/4500 train_loss:8.0300 train_time:280285ms step_avg:185.01ms\nstep:1526/4500 train_loss:8.1944 train_time:280469ms step_avg:185.01ms\nstep:1527/4500 train_loss:8.2161 train_time:280654ms step_avg:185.01ms\nstep:1528/4500 train_loss:8.1844 train_time:280837ms step_avg:185.00ms\nstep:1529/4500 train_loss:8.0326 train_time:281028ms step_avg:185.01ms\nstep:1530/4500 train_loss:8.2767 train_time:281212ms step_avg:185.01ms\nstep:1531/4500 train_loss:8.1033 train_time:281394ms step_avg:185.01ms\nstep:1532/4500 train_loss:8.1124 train_time:281577ms step_avg:185.00ms\nstep:1533/4500 train_loss:8.1616 train_time:281761ms step_avg:185.00ms\nstep:1534/4500 train_loss:8.0028 train_time:281946ms step_avg:185.00ms\nstep:1535/4500 train_loss:8.1042 train_time:282129ms step_avg:185.00ms\nstep:1536/4500 train_loss:8.1061 train_time:282312ms step_avg:185.00ms\nstep:1537/4500 train_loss:8.1412 train_time:282495ms step_avg:185.00ms\nstep:1538/4500 train_loss:8.0710 train_time:282682ms step_avg:185.00ms\nstep:1539/4500 train_loss:8.0606 train_time:282868ms step_avg:185.00ms\nstep:1540/4500 train_loss:8.0624 train_time:283054ms step_avg:185.00ms\nstep:1541/4500 train_loss:8.1832 train_time:283238ms step_avg:185.00ms\nstep:1542/4500 train_loss:8.0820 train_time:283423ms step_avg:185.00ms\nstep:1543/4500 train_loss:8.1244 train_time:283607ms step_avg:185.00ms\nstep:1544/4500 train_loss:8.1669 train_time:283792ms step_avg:185.00ms\nstep:1545/4500 train_loss:8.1452 train_time:283975ms step_avg:185.00ms\nstep:1546/4500 train_loss:8.1692 train_time:284160ms step_avg:185.00ms\nstep:1547/4500 train_loss:8.0981 train_time:284344ms step_avg:185.00ms\nstep:1548/4500 train_loss:8.0630 train_time:284529ms step_avg:185.00ms\nstep:1549/4500 train_loss:8.0820 train_time:284713ms step_avg:185.00ms\nstep:1550/4500 train_loss:8.2680 train_time:284897ms step_avg:185.00ms\nstep:1551/4500 train_loss:8.1222 train_time:285082ms step_avg:185.00ms\nstep:1552/4500 train_loss:8.0387 train_time:285266ms step_avg:185.00ms\nstep:1553/4500 train_loss:8.1793 train_time:285451ms step_avg:185.00ms\nstep:1554/4500 train_loss:8.1306 train_time:285635ms step_avg:185.00ms\nstep:1555/4500 train_loss:8.0652 train_time:285821ms step_avg:185.00ms\nstep:1556/4500 train_loss:8.1401 train_time:286010ms step_avg:185.00ms\nstep:1557/4500 train_loss:8.1292 train_time:286194ms step_avg:185.00ms\nstep:1558/4500 train_loss:8.0091 train_time:286377ms step_avg:185.00ms\nstep:1559/4500 train_loss:8.1500 train_time:286561ms step_avg:185.00ms\nstep:1560/4500 train_loss:8.0633 train_time:286748ms step_avg:185.00ms\nstep:1561/4500 train_loss:8.0553 train_time:286934ms step_avg:185.00ms\nstep:1562/4500 train_loss:8.1383 train_time:287119ms step_avg:185.00ms\nstep:1563/4500 train_loss:8.1763 train_time:287306ms step_avg:185.00ms\nstep:1564/4500 train_loss:8.1813 train_time:287496ms step_avg:185.00ms\nstep:1565/4500 train_loss:8.0817 train_time:287680ms step_avg:185.00ms\nstep:1566/4500 train_loss:8.0735 train_time:287865ms step_avg:185.00ms\nstep:1567/4500 train_loss:8.1198 train_time:288049ms step_avg:185.00ms\nstep:1568/4500 train_loss:8.1283 train_time:288233ms step_avg:185.00ms\nstep:1569/4500 train_loss:8.0708 train_time:288418ms step_avg:185.00ms\nstep:1570/4500 train_loss:8.1507 train_time:288602ms step_avg:185.00ms\nstep:1571/4500 train_loss:8.2219 train_time:288790ms step_avg:185.00ms\nstep:1572/4500 train_loss:8.0270 train_time:288973ms step_avg:185.00ms\nstep:1573/4500 train_loss:8.0815 train_time:289158ms step_avg:185.00ms\nstep:1574/4500 train_loss:8.0351 train_time:289343ms step_avg:185.00ms\nstep:1575/4500 train_loss:8.1464 train_time:289530ms step_avg:185.00ms\nstep:1576/4500 train_loss:8.1356 train_time:289715ms step_avg:185.00ms\nstep:1577/4500 train_loss:8.1856 train_time:289901ms step_avg:185.00ms\nstep:1578/4500 train_loss:8.0941 train_time:290090ms step_avg:185.01ms\nstep:1579/4500 train_loss:8.1149 train_time:290274ms step_avg:185.01ms\nstep:1580/4500 train_loss:8.0665 train_time:290458ms step_avg:185.00ms\nstep:1581/4500 train_loss:8.1490 train_time:290642ms step_avg:185.00ms\nstep:1582/4500 train_loss:8.0970 train_time:290828ms step_avg:185.00ms\nstep:1583/4500 train_loss:8.0928 train_time:291012ms step_avg:185.00ms\nstep:1584/4500 train_loss:8.1856 train_time:291195ms step_avg:185.00ms\nstep:1585/4500 train_loss:8.0566 train_time:291381ms step_avg:185.00ms\nstep:1586/4500 train_loss:8.0436 train_time:291566ms step_avg:185.00ms\nstep:1587/4500 train_loss:8.0602 train_time:291751ms step_avg:185.00ms\nstep:1588/4500 train_loss:7.9677 train_time:291935ms step_avg:185.00ms\nstep:1589/4500 train_loss:8.0858 train_time:292120ms step_avg:185.00ms\nstep:1590/4500 train_loss:8.2812 train_time:292306ms step_avg:185.00ms\nstep:1591/4500 train_loss:8.1525 train_time:292494ms step_avg:185.01ms\nstep:1592/4500 train_loss:8.0276 train_time:292679ms step_avg:185.01ms\nstep:1593/4500 train_loss:8.2132 train_time:292863ms step_avg:185.00ms\nstep:1594/4500 train_loss:8.3445 train_time:293051ms step_avg:185.01ms\nstep:1595/4500 train_loss:8.0838 train_time:293236ms step_avg:185.01ms\nstep:1596/4500 train_loss:8.0845 train_time:293420ms step_avg:185.01ms\nstep:1597/4500 train_loss:8.1509 train_time:293605ms step_avg:185.01ms\nstep:1598/4500 train_loss:8.1047 train_time:293788ms step_avg:185.01ms\nstep:1599/4500 train_loss:8.0850 train_time:293971ms step_avg:185.00ms\nstep:1600/4500 train_loss:8.0970 train_time:294155ms step_avg:185.00ms\nstep:1601/4500 train_loss:8.3300 train_time:294340ms step_avg:185.00ms\nstep:1602/4500 train_loss:8.2886 train_time:294527ms step_avg:185.00ms\nstep:1603/4500 train_loss:8.1747 train_time:294715ms step_avg:185.01ms\nstep:1604/4500 train_loss:8.2004 train_time:294904ms step_avg:185.01ms\nstep:1605/4500 train_loss:8.0399 train_time:295084ms step_avg:185.01ms\nstep:1606/4500 train_loss:8.1100 train_time:295268ms step_avg:185.01ms\nstep:1607/4500 train_loss:8.2867 train_time:295453ms step_avg:185.00ms\nstep:1608/4500 train_loss:8.1447 train_time:295637ms step_avg:185.00ms\nstep:1609/4500 train_loss:8.1004 train_time:295824ms step_avg:185.01ms\nstep:1610/4500 train_loss:8.1004 train_time:296008ms step_avg:185.01ms\nstep:1611/4500 train_loss:8.1353 train_time:296193ms step_avg:185.00ms\nstep:1612/4500 train_loss:8.2111 train_time:296377ms step_avg:185.00ms\nstep:1613/4500 train_loss:8.1456 train_time:296561ms step_avg:185.00ms\nstep:1614/4500 train_loss:8.1040 train_time:296747ms step_avg:185.00ms\nstep:1615/4500 train_loss:7.9998 train_time:296930ms step_avg:185.00ms\nstep:1616/4500 train_loss:8.2175 train_time:297114ms step_avg:185.00ms\nstep:1617/4500 train_loss:8.1822 train_time:297301ms step_avg:185.00ms\nstep:1618/4500 train_loss:8.3352 train_time:297486ms step_avg:185.00ms\nstep:1619/4500 train_loss:8.0921 train_time:297672ms step_avg:185.00ms\nstep:1620/4500 train_loss:8.1308 train_time:297858ms step_avg:185.00ms\nstep:1621/4500 train_loss:8.1437 train_time:298042ms step_avg:185.00ms\nstep:1622/4500 train_loss:8.0874 train_time:298228ms step_avg:185.01ms\nstep:1623/4500 train_loss:8.1299 train_time:298413ms step_avg:185.01ms\nstep:1624/4500 train_loss:8.1732 train_time:298597ms step_avg:185.00ms\nstep:1625/4500 train_loss:8.2163 train_time:298783ms step_avg:185.00ms\nstep:1626/4500 train_loss:8.1849 train_time:298970ms step_avg:185.01ms\nstep:1627/4500 train_loss:8.0812 train_time:299153ms step_avg:185.00ms\nstep:1628/4500 train_loss:8.0617 train_time:299336ms step_avg:185.00ms\nstep:1629/4500 train_loss:8.2170 train_time:299520ms step_avg:185.00ms\nstep:1630/4500 train_loss:8.0955 train_time:299705ms step_avg:185.00ms\nstep:1631/4500 train_loss:8.1339 train_time:299890ms step_avg:185.00ms\nstep:1632/4500 train_loss:8.0629 train_time:300077ms step_avg:185.00ms\nstep:1633/4500 train_loss:8.2110 train_time:300266ms step_avg:185.01ms\nstep:1634/4500 train_loss:8.1282 train_time:300450ms step_avg:185.01ms\nstep:1635/4500 train_loss:8.0942 train_time:300632ms step_avg:185.00ms\nstep:1636/4500 train_loss:8.1814 train_time:300821ms step_avg:185.01ms\nstep:1637/4500 train_loss:8.1614 train_time:301002ms step_avg:185.00ms\nstep:1638/4500 train_loss:8.1049 train_time:301188ms step_avg:185.01ms\nstep:1639/4500 train_loss:8.1082 train_time:301374ms step_avg:185.01ms\nstep:1640/4500 train_loss:8.0912 train_time:301559ms step_avg:185.01ms\nstep:1641/4500 train_loss:8.0359 train_time:301744ms step_avg:185.01ms\nstep:1642/4500 train_loss:8.0757 train_time:301927ms step_avg:185.00ms\nstep:1643/4500 train_loss:8.0175 train_time:302113ms step_avg:185.00ms\nstep:1644/4500 train_loss:8.1050 train_time:302298ms step_avg:185.00ms\nstep:1645/4500 train_loss:8.0699 train_time:302485ms step_avg:185.01ms\nstep:1646/4500 train_loss:8.0995 train_time:302674ms step_avg:185.01ms\nstep:1647/4500 train_loss:8.0762 train_time:302859ms step_avg:185.01ms\nstep:1648/4500 train_loss:8.0911 train_time:303044ms step_avg:185.01ms\nstep:1649/4500 train_loss:8.1449 train_time:303228ms step_avg:185.01ms\nstep:1650/4500 train_loss:8.2482 train_time:303413ms step_avg:185.01ms\nstep:1651/4500 train_loss:8.1320 train_time:303598ms step_avg:185.01ms\nstep:1652/4500 train_loss:8.1259 train_time:303783ms step_avg:185.01ms\nstep:1653/4500 train_loss:8.2135 train_time:303969ms step_avg:185.01ms\nstep:1654/4500 train_loss:8.1287 train_time:304153ms step_avg:185.01ms\nstep:1655/4500 train_loss:8.0756 train_time:304338ms step_avg:185.01ms\nstep:1656/4500 train_loss:8.0443 train_time:304522ms step_avg:185.01ms\nstep:1657/4500 train_loss:8.0644 train_time:304708ms step_avg:185.01ms\nstep:1658/4500 train_loss:8.2312 train_time:304891ms step_avg:185.01ms\nstep:1659/4500 train_loss:8.1739 train_time:305076ms step_avg:185.01ms\nstep:1660/4500 train_loss:8.1944 train_time:305262ms step_avg:185.01ms\nstep:1661/4500 train_loss:8.1593 train_time:305449ms step_avg:185.01ms\nstep:1662/4500 train_loss:8.1110 train_time:305634ms step_avg:185.01ms\nstep:1663/4500 train_loss:8.0758 train_time:305818ms step_avg:185.01ms\nstep:1664/4500 train_loss:8.0705 train_time:306004ms step_avg:185.01ms\nstep:1665/4500 train_loss:8.2804 train_time:306190ms step_avg:185.01ms\nstep:1666/4500 train_loss:8.0758 train_time:306376ms step_avg:185.01ms\nstep:1667/4500 train_loss:7.9044 train_time:306561ms step_avg:185.01ms\nstep:1668/4500 train_loss:8.0336 train_time:306746ms step_avg:185.01ms\nstep:1669/4500 train_loss:8.1805 train_time:306931ms step_avg:185.01ms\nstep:1670/4500 train_loss:8.1143 train_time:307114ms step_avg:185.01ms\nstep:1671/4500 train_loss:8.1092 train_time:307298ms step_avg:185.01ms\nstep:1672/4500 train_loss:7.9598 train_time:307482ms step_avg:185.01ms\nstep:1673/4500 train_loss:8.0949 train_time:307667ms step_avg:185.01ms\nstep:1674/4500 train_loss:8.0587 train_time:307850ms step_avg:185.01ms\nstep:1675/4500 train_loss:8.0857 train_time:308034ms step_avg:185.01ms\nstep:1676/4500 train_loss:8.2180 train_time:308216ms step_avg:185.00ms\nstep:1677/4500 train_loss:7.9104 train_time:308401ms step_avg:185.00ms\nstep:1678/4500 train_loss:8.0881 train_time:308586ms step_avg:185.00ms\nstep:1679/4500 train_loss:8.0701 train_time:308772ms step_avg:185.00ms\nstep:1680/4500 train_loss:8.0074 train_time:308956ms step_avg:185.00ms\nstep:1681/4500 train_loss:8.0770 train_time:309141ms step_avg:185.00ms\nstep:1682/4500 train_loss:8.1884 train_time:309329ms step_avg:185.01ms\nstep:1683/4500 train_loss:8.1886 train_time:309516ms step_avg:185.01ms\nstep:1684/4500 train_loss:8.1613 train_time:309700ms step_avg:185.01ms\nstep:1685/4500 train_loss:8.1088 train_time:309885ms step_avg:185.01ms\nstep:1686/4500 train_loss:8.0378 train_time:310071ms step_avg:185.01ms\nstep:1687/4500 train_loss:8.0116 train_time:310255ms step_avg:185.01ms\nstep:1688/4500 train_loss:8.1436 train_time:310442ms step_avg:185.01ms\nstep:1689/4500 train_loss:8.1868 train_time:310631ms step_avg:185.01ms\nstep:1690/4500 train_loss:7.9677 train_time:310817ms step_avg:185.01ms\nstep:1691/4500 train_loss:8.1664 train_time:311002ms step_avg:185.01ms\nstep:1692/4500 train_loss:8.0828 train_time:311186ms step_avg:185.01ms\nstep:1693/4500 train_loss:8.0500 train_time:311372ms step_avg:185.01ms\nstep:1694/4500 train_loss:8.0965 train_time:311559ms step_avg:185.01ms\nstep:1695/4500 train_loss:8.1108 train_time:311744ms step_avg:185.01ms\nstep:1696/4500 train_loss:8.0111 train_time:311927ms step_avg:185.01ms\nstep:1697/4500 train_loss:8.1720 train_time:312114ms step_avg:185.01ms\nstep:1698/4500 train_loss:8.1262 train_time:312297ms step_avg:185.01ms\nstep:1699/4500 train_loss:8.1431 train_time:312483ms step_avg:185.01ms\nstep:1700/4500 train_loss:8.0766 train_time:312668ms step_avg:185.01ms\nstep:1701/4500 train_loss:8.0687 train_time:312854ms step_avg:185.01ms\nstep:1702/4500 train_loss:8.0632 train_time:313040ms step_avg:185.01ms\nstep:1703/4500 train_loss:8.1110 train_time:313226ms step_avg:185.01ms\nstep:1704/4500 train_loss:8.1696 train_time:313414ms step_avg:185.01ms\nstep:1705/4500 train_loss:8.1060 train_time:313599ms step_avg:185.01ms\nstep:1706/4500 train_loss:8.0759 train_time:313784ms step_avg:185.01ms\nstep:1707/4500 train_loss:8.1858 train_time:313969ms step_avg:185.01ms\nstep:1708/4500 train_loss:8.1390 train_time:314155ms step_avg:185.01ms\nstep:1709/4500 train_loss:8.0637 train_time:314338ms step_avg:185.01ms\nstep:1710/4500 train_loss:8.1894 train_time:314523ms step_avg:185.01ms\nstep:1711/4500 train_loss:8.1373 train_time:314708ms step_avg:185.01ms\nstep:1712/4500 train_loss:7.9606 train_time:314894ms step_avg:185.01ms\nstep:1713/4500 train_loss:8.0903 train_time:315076ms step_avg:185.01ms\nstep:1714/4500 train_loss:8.2388 train_time:315264ms step_avg:185.01ms\nstep:1715/4500 train_loss:8.2067 train_time:315447ms step_avg:185.01ms\nstep:1716/4500 train_loss:8.0647 train_time:315631ms step_avg:185.01ms\nstep:1717/4500 train_loss:7.9580 train_time:315815ms step_avg:185.01ms\nstep:1718/4500 train_loss:8.0462 train_time:315998ms step_avg:185.01ms\nstep:1719/4500 train_loss:8.0448 train_time:316182ms step_avg:185.01ms\nstep:1720/4500 train_loss:8.1130 train_time:316370ms step_avg:185.01ms\nstep:1721/4500 train_loss:8.1284 train_time:316556ms step_avg:185.01ms\nstep:1722/4500 train_loss:8.0588 train_time:316741ms step_avg:185.01ms\nstep:1723/4500 train_loss:8.0756 train_time:316925ms step_avg:185.01ms\nstep:1724/4500 train_loss:8.0610 train_time:317110ms step_avg:185.01ms\nstep:1725/4500 train_loss:7.9724 train_time:317295ms step_avg:185.01ms\nstep:1726/4500 train_loss:8.1970 train_time:317481ms step_avg:185.01ms\nstep:1727/4500 train_loss:8.3212 train_time:317669ms step_avg:185.01ms\nstep:1728/4500 train_loss:8.1328 train_time:317855ms step_avg:185.01ms\nstep:1729/4500 train_loss:8.2086 train_time:318038ms step_avg:185.01ms\nstep:1730/4500 train_loss:8.1102 train_time:318221ms step_avg:185.01ms\nstep:1731/4500 train_loss:8.1576 train_time:318405ms step_avg:185.01ms\nstep:1732/4500 train_loss:8.2012 train_time:318591ms step_avg:185.01ms\nstep:1733/4500 train_loss:8.1465 train_time:318774ms step_avg:185.01ms\nstep:1734/4500 train_loss:8.1996 train_time:318957ms step_avg:185.01ms\nstep:1735/4500 train_loss:8.0577 train_time:319142ms step_avg:185.01ms\nstep:1736/4500 train_loss:8.1428 train_time:319328ms step_avg:185.01ms\nstep:1737/4500 train_loss:8.0433 train_time:319514ms step_avg:185.01ms\nstep:1738/4500 train_loss:8.1234 train_time:319699ms step_avg:185.01ms\nstep:1739/4500 train_loss:8.1287 train_time:319885ms step_avg:185.01ms\nstep:1740/4500 train_loss:8.0452 train_time:320069ms step_avg:185.01ms\nstep:1741/4500 train_loss:8.1172 train_time:320256ms step_avg:185.01ms\nstep:1742/4500 train_loss:8.1096 train_time:320440ms step_avg:185.01ms\nstep:1743/4500 train_loss:8.1487 train_time:320627ms step_avg:185.01ms\nstep:1744/4500 train_loss:8.1358 train_time:320810ms step_avg:185.01ms\nstep:1745/4500 train_loss:8.0885 train_time:320997ms step_avg:185.01ms\nstep:1746/4500 train_loss:8.1219 train_time:321181ms step_avg:185.01ms\nstep:1747/4500 train_loss:8.1452 train_time:321366ms step_avg:185.01ms\nstep:1748/4500 train_loss:8.2400 train_time:321551ms step_avg:185.01ms\nstep:1749/4500 train_loss:8.1234 train_time:321737ms step_avg:185.01ms\nstep:1750/4500 train_loss:8.1364 train_time:321923ms step_avg:185.01ms\nstep:1751/4500 train_loss:8.2620 train_time:322107ms step_avg:185.01ms\nstep:1752/4500 train_loss:8.1320 train_time:322292ms step_avg:185.01ms\nstep:1753/4500 train_loss:8.1510 train_time:322477ms step_avg:185.01ms\nstep:1754/4500 train_loss:8.1029 train_time:322661ms step_avg:185.01ms\nstep:1755/4500 train_loss:8.0652 train_time:322847ms step_avg:185.01ms\nstep:1756/4500 train_loss:8.1610 train_time:323035ms step_avg:185.01ms\nstep:1757/4500 train_loss:8.1454 train_time:323219ms step_avg:185.01ms\nstep:1758/4500 train_loss:8.0207 train_time:323402ms step_avg:185.01ms\nstep:1759/4500 train_loss:8.1748 train_time:323585ms step_avg:185.01ms\nstep:1760/4500 train_loss:8.0614 train_time:323769ms step_avg:185.01ms\nstep:1761/4500 train_loss:8.1705 train_time:323956ms step_avg:185.01ms\nstep:1762/4500 train_loss:8.1404 train_time:324143ms step_avg:185.01ms\nstep:1763/4500 train_loss:8.0286 train_time:324329ms step_avg:185.01ms\nstep:1764/4500 train_loss:8.0408 train_time:324514ms step_avg:185.01ms\nstep:1765/4500 train_loss:8.0026 train_time:324697ms step_avg:185.01ms\nstep:1766/4500 train_loss:8.1252 train_time:324882ms step_avg:185.01ms\nstep:1767/4500 train_loss:8.0926 train_time:325067ms step_avg:185.01ms\nstep:1768/4500 train_loss:8.1619 train_time:325253ms step_avg:185.01ms\nstep:1769/4500 train_loss:8.3217 train_time:325438ms step_avg:185.01ms\nstep:1770/4500 train_loss:8.1539 train_time:325625ms step_avg:185.01ms\nstep:1771/4500 train_loss:8.1115 train_time:325808ms step_avg:185.01ms\nstep:1772/4500 train_loss:8.1117 train_time:325994ms step_avg:185.01ms\nstep:1773/4500 train_loss:8.1130 train_time:326177ms step_avg:185.01ms\nstep:1774/4500 train_loss:8.0479 train_time:326361ms step_avg:185.01ms\nstep:1775/4500 train_loss:8.0824 train_time:326546ms step_avg:185.01ms\nstep:1776/4500 train_loss:8.1451 train_time:326733ms step_avg:185.01ms\nstep:1777/4500 train_loss:8.1512 train_time:326918ms step_avg:185.01ms\nstep:1778/4500 train_loss:8.0933 train_time:327102ms step_avg:185.01ms\nstep:1779/4500 train_loss:8.1286 train_time:327288ms step_avg:185.01ms\nstep:1780/4500 train_loss:8.1100 train_time:327472ms step_avg:185.01ms\nstep:1781/4500 train_loss:8.1202 train_time:327656ms step_avg:185.01ms\nstep:1782/4500 train_loss:8.0901 train_time:327842ms step_avg:185.01ms\nstep:1783/4500 train_loss:8.1460 train_time:328029ms step_avg:185.01ms\nstep:1784/4500 train_loss:8.0706 train_time:328213ms step_avg:185.01ms\nstep:1785/4500 train_loss:8.1349 train_time:328397ms step_avg:185.01ms\nstep:1786/4500 train_loss:8.1458 train_time:328581ms step_avg:185.01ms\nstep:1787/4500 train_loss:8.1039 train_time:328766ms step_avg:185.01ms\nstep:1788/4500 train_loss:8.1074 train_time:328950ms step_avg:185.01ms\nstep:1789/4500 train_loss:8.2142 train_time:329137ms step_avg:185.01ms\nstep:1790/4500 train_loss:8.1121 train_time:329323ms step_avg:185.01ms\nstep:1791/4500 train_loss:8.1221 train_time:329508ms step_avg:185.01ms\nstep:1792/4500 train_loss:8.0892 train_time:329694ms step_avg:185.01ms\nstep:1793/4500 train_loss:8.1505 train_time:329878ms step_avg:185.01ms\nstep:1794/4500 train_loss:8.0784 train_time:330062ms step_avg:185.01ms\nstep:1795/4500 train_loss:8.0680 train_time:330247ms step_avg:185.01ms\nstep:1796/4500 train_loss:8.0053 train_time:330433ms step_avg:185.01ms\nstep:1797/4500 train_loss:8.1919 train_time:330618ms step_avg:185.01ms\nstep:1798/4500 train_loss:7.9943 train_time:330804ms step_avg:185.01ms\nstep:1799/4500 train_loss:8.1620 train_time:330989ms step_avg:185.01ms\nstep:1800/4500 train_loss:8.1078 train_time:331171ms step_avg:185.01ms\nstep:1801/4500 train_loss:8.2280 train_time:331355ms step_avg:185.01ms\nstep:1802/4500 train_loss:7.9352 train_time:331540ms step_avg:185.01ms\nstep:1803/4500 train_loss:8.1861 train_time:331725ms step_avg:185.01ms\nstep:1804/4500 train_loss:8.1583 train_time:331911ms step_avg:185.01ms\nstep:1805/4500 train_loss:8.0976 train_time:332097ms step_avg:185.01ms\nstep:1806/4500 train_loss:8.0538 train_time:332281ms step_avg:185.01ms\nstep:1807/4500 train_loss:8.0622 train_time:332468ms step_avg:185.01ms\nstep:1808/4500 train_loss:8.1540 train_time:332654ms step_avg:185.01ms\nstep:1809/4500 train_loss:8.0997 train_time:332840ms step_avg:185.01ms\nstep:1810/4500 train_loss:8.0760 train_time:333027ms step_avg:185.01ms\nstep:1811/4500 train_loss:8.1416 train_time:333215ms step_avg:185.02ms\nstep:1812/4500 train_loss:8.2110 train_time:333401ms step_avg:185.02ms\nstep:1813/4500 train_loss:8.0802 train_time:333585ms step_avg:185.02ms\nstep:1814/4500 train_loss:8.0444 train_time:333772ms step_avg:185.02ms\nstep:1815/4500 train_loss:8.0715 train_time:333955ms step_avg:185.02ms\nstep:1816/4500 train_loss:7.9939 train_time:334140ms step_avg:185.02ms\nstep:1817/4500 train_loss:8.1002 train_time:334324ms step_avg:185.02ms\nstep:1818/4500 train_loss:8.0569 train_time:334509ms step_avg:185.02ms\nstep:1819/4500 train_loss:7.9449 train_time:334694ms step_avg:185.02ms\nstep:1820/4500 train_loss:8.0652 train_time:334878ms step_avg:185.02ms\nstep:1821/4500 train_loss:8.1959 train_time:335063ms step_avg:185.02ms\nstep:1822/4500 train_loss:8.1045 train_time:335247ms step_avg:185.02ms\nstep:1823/4500 train_loss:8.0241 train_time:335433ms step_avg:185.02ms\nstep:1824/4500 train_loss:8.1581 train_time:335616ms step_avg:185.01ms\nstep:1825/4500 train_loss:8.1661 train_time:335801ms step_avg:185.01ms\nstep:1826/4500 train_loss:8.0763 train_time:335988ms step_avg:185.02ms\nstep:1827/4500 train_loss:8.1132 train_time:336175ms step_avg:185.02ms\nstep:1828/4500 train_loss:8.1618 train_time:336360ms step_avg:185.02ms\nstep:1829/4500 train_loss:8.1799 train_time:336545ms step_avg:185.02ms\nstep:1830/4500 train_loss:8.1435 train_time:336729ms step_avg:185.02ms\nstep:1831/4500 train_loss:7.9971 train_time:336915ms step_avg:185.02ms\nstep:1832/4500 train_loss:8.1510 train_time:337100ms step_avg:185.02ms\nstep:1833/4500 train_loss:8.0359 train_time:337288ms step_avg:185.02ms\nstep:1834/4500 train_loss:8.1624 train_time:337473ms step_avg:185.02ms\nstep:1835/4500 train_loss:8.0850 train_time:337656ms step_avg:185.02ms\nstep:1836/4500 train_loss:8.1494 train_time:337841ms step_avg:185.02ms\nstep:1837/4500 train_loss:8.0707 train_time:338029ms step_avg:185.02ms\nstep:1838/4500 train_loss:8.0119 train_time:338213ms step_avg:185.02ms\nstep:1839/4500 train_loss:8.0476 train_time:338397ms step_avg:185.02ms\nstep:1840/4500 train_loss:8.1795 train_time:338583ms step_avg:185.02ms\nstep:1841/4500 train_loss:8.1129 train_time:338769ms step_avg:185.02ms\nstep:1842/4500 train_loss:8.0545 train_time:338951ms step_avg:185.02ms\nstep:1843/4500 train_loss:8.1085 train_time:339134ms step_avg:185.02ms\nstep:1844/4500 train_loss:8.1830 train_time:339317ms step_avg:185.01ms\nstep:1845/4500 train_loss:8.1278 train_time:339502ms step_avg:185.01ms\nstep:1846/4500 train_loss:8.1342 train_time:339689ms step_avg:185.02ms\nstep:1847/4500 train_loss:8.0964 train_time:339874ms step_avg:185.02ms\nstep:1848/4500 train_loss:8.1105 train_time:340058ms step_avg:185.02ms\nstep:1849/4500 train_loss:8.0988 train_time:340244ms step_avg:185.02ms\nstep:1850/4500 train_loss:7.9457 train_time:340431ms step_avg:185.02ms\nstep:1851/4500 train_loss:8.2194 train_time:340615ms step_avg:185.02ms\nstep:1852/4500 train_loss:7.9649 train_time:340801ms step_avg:185.02ms\nstep:1853/4500 train_loss:8.0110 train_time:340987ms step_avg:185.02ms\nstep:1854/4500 train_loss:8.1628 train_time:341171ms step_avg:185.02ms\nstep:1855/4500 train_loss:8.2996 train_time:341354ms step_avg:185.02ms\nstep:1856/4500 train_loss:8.0760 train_time:341539ms step_avg:185.02ms\nstep:1857/4500 train_loss:8.1698 train_time:341726ms step_avg:185.02ms\nstep:1858/4500 train_loss:8.1144 train_time:341912ms step_avg:185.02ms\nstep:1859/4500 train_loss:8.1005 train_time:342096ms step_avg:185.02ms\nstep:1860/4500 train_loss:8.2078 train_time:342283ms step_avg:185.02ms\nstep:1861/4500 train_loss:8.1675 train_time:342469ms step_avg:185.02ms\nstep:1862/4500 train_loss:8.1045 train_time:342656ms step_avg:185.02ms\nstep:1863/4500 train_loss:8.0700 train_time:342843ms step_avg:185.02ms\nstep:1864/4500 train_loss:8.0108 train_time:343031ms step_avg:185.02ms\nstep:1865/4500 train_loss:8.1576 train_time:343218ms step_avg:185.02ms\nstep:1866/4500 train_loss:8.1850 train_time:343402ms step_avg:185.02ms\nstep:1867/4500 train_loss:8.1589 train_time:343587ms step_avg:185.02ms\nstep:1868/4500 train_loss:8.1259 train_time:343770ms step_avg:185.02ms\nstep:1869/4500 train_loss:8.1996 train_time:343954ms step_avg:185.02ms\nstep:1870/4500 train_loss:8.3102 train_time:344138ms step_avg:185.02ms\nstep:1871/4500 train_loss:8.1187 train_time:344322ms step_avg:185.02ms\nstep:1872/4500 train_loss:8.0789 train_time:344506ms step_avg:185.02ms\nstep:1873/4500 train_loss:8.1568 train_time:344689ms step_avg:185.02ms\nstep:1874/4500 train_loss:8.1284 train_time:344873ms step_avg:185.02ms\nstep:1875/4500 train_loss:8.1530 train_time:345056ms step_avg:185.02ms\nstep:1876/4500 train_loss:8.1697 train_time:345242ms step_avg:185.02ms\nstep:1877/4500 train_loss:8.1954 train_time:345427ms step_avg:185.02ms\nstep:1878/4500 train_loss:8.1307 train_time:345611ms step_avg:185.02ms\nstep:1879/4500 train_loss:8.1305 train_time:345796ms step_avg:185.02ms\nstep:1880/4500 train_loss:8.1386 train_time:345982ms step_avg:185.02ms\nstep:1881/4500 train_loss:8.0145 train_time:346170ms step_avg:185.02ms\nstep:1882/4500 train_loss:8.1584 train_time:346353ms step_avg:185.02ms\nstep:1883/4500 train_loss:8.1995 train_time:346536ms step_avg:185.02ms\nstep:1884/4500 train_loss:8.0328 train_time:346721ms step_avg:185.02ms\nstep:1885/4500 train_loss:7.9448 train_time:346906ms step_avg:185.02ms\nstep:1886/4500 train_loss:8.1257 train_time:347091ms step_avg:185.02ms\nstep:1887/4500 train_loss:8.4668 train_time:347276ms step_avg:185.02ms\nstep:1888/4500 train_loss:8.1020 train_time:347462ms step_avg:185.02ms\nstep:1889/4500 train_loss:8.0784 train_time:347646ms step_avg:185.02ms\nstep:1890/4500 train_loss:8.0881 train_time:347833ms step_avg:185.02ms\nstep:1891/4500 train_loss:8.1793 train_time:348018ms step_avg:185.02ms\nstep:1892/4500 train_loss:8.1354 train_time:348203ms step_avg:185.02ms\nstep:1893/4500 train_loss:8.1937 train_time:348389ms step_avg:185.02ms\nstep:1894/4500 train_loss:8.1067 train_time:348573ms step_avg:185.02ms\nstep:1895/4500 train_loss:8.0836 train_time:348757ms step_avg:185.02ms\nstep:1896/4500 train_loss:8.1956 train_time:348941ms step_avg:185.02ms\nstep:1897/4500 train_loss:8.1076 train_time:349126ms step_avg:185.02ms\nstep:1898/4500 train_loss:8.0532 train_time:349311ms step_avg:185.02ms\nstep:1899/4500 train_loss:8.2849 train_time:349493ms step_avg:185.01ms\nstep:1900/4500 train_loss:8.0735 train_time:349680ms step_avg:185.02ms\nstep:1901/4500 train_loss:8.1194 train_time:349866ms step_avg:185.02ms\nstep:1902/4500 train_loss:8.1734 train_time:350049ms step_avg:185.02ms\nstep:1903/4500 train_loss:8.0029 train_time:350234ms step_avg:185.02ms\nstep:1904/4500 train_loss:8.0107 train_time:350417ms step_avg:185.01ms\nstep:1905/4500 train_loss:8.1401 train_time:350602ms step_avg:185.01ms\nstep:1906/4500 train_loss:8.1362 train_time:350788ms step_avg:185.01ms\nstep:1907/4500 train_loss:8.2054 train_time:350975ms step_avg:185.02ms\nstep:1908/4500 train_loss:8.1774 train_time:351160ms step_avg:185.02ms\nstep:1909/4500 train_loss:7.9521 train_time:351346ms step_avg:185.02ms\nstep:1910/4500 train_loss:8.1410 train_time:351533ms step_avg:185.02ms\nstep:1911/4500 train_loss:8.2143 train_time:351716ms step_avg:185.02ms\nstep:1912/4500 train_loss:8.1769 train_time:351900ms step_avg:185.02ms\nstep:1913/4500 train_loss:7.9949 train_time:352085ms step_avg:185.02ms\nstep:1914/4500 train_loss:8.0507 train_time:352270ms step_avg:185.02ms\nstep:1915/4500 train_loss:8.0597 train_time:352455ms step_avg:185.02ms\nstep:1916/4500 train_loss:8.1409 train_time:352641ms step_avg:185.02ms\nstep:1917/4500 train_loss:8.0902 train_time:352827ms step_avg:185.02ms\nstep:1918/4500 train_loss:8.1334 train_time:353010ms step_avg:185.02ms\nstep:1919/4500 train_loss:8.1223 train_time:353193ms step_avg:185.01ms\nstep:1920/4500 train_loss:8.0446 train_time:353375ms step_avg:185.01ms\nstep:1921/4500 train_loss:8.2145 train_time:353561ms step_avg:185.01ms\nstep:1922/4500 train_loss:8.1788 train_time:353744ms step_avg:185.01ms\nstep:1923/4500 train_loss:8.0677 train_time:353933ms step_avg:185.01ms\nstep:1924/4500 train_loss:8.1990 train_time:354118ms step_avg:185.01ms\nstep:1925/4500 train_loss:8.0482 train_time:354305ms step_avg:185.02ms\nstep:1926/4500 train_loss:8.1145 train_time:354489ms step_avg:185.02ms\nstep:1927/4500 train_loss:8.0309 train_time:354674ms step_avg:185.02ms\nstep:1928/4500 train_loss:8.1891 train_time:354858ms step_avg:185.01ms\nstep:1929/4500 train_loss:8.0905 train_time:355043ms step_avg:185.01ms\nstep:1930/4500 train_loss:8.0018 train_time:355227ms step_avg:185.01ms\nstep:1931/4500 train_loss:8.0868 train_time:355411ms step_avg:185.01ms\nstep:1932/4500 train_loss:8.0286 train_time:355595ms step_avg:185.01ms\nstep:1933/4500 train_loss:7.9350 train_time:355781ms step_avg:185.01ms\nstep:1934/4500 train_loss:8.1240 train_time:355965ms step_avg:185.01ms\nstep:1935/4500 train_loss:8.0574 train_time:356148ms step_avg:185.01ms\nstep:1936/4500 train_loss:8.0940 train_time:356335ms step_avg:185.01ms\nstep:1937/4500 train_loss:8.0857 train_time:356517ms step_avg:185.01ms\nstep:1938/4500 train_loss:8.1264 train_time:356701ms step_avg:185.01ms\nstep:1939/4500 train_loss:8.1202 train_time:356886ms step_avg:185.01ms\nstep:1940/4500 train_loss:8.1142 train_time:357070ms step_avg:185.01ms\nstep:1941/4500 train_loss:8.1602 train_time:357254ms step_avg:185.01ms\nstep:1942/4500 train_loss:8.1238 train_time:357438ms step_avg:185.01ms\nstep:1943/4500 train_loss:8.1035 train_time:357624ms step_avg:185.01ms\nstep:1944/4500 train_loss:8.0402 train_time:357808ms step_avg:185.01ms\nstep:1945/4500 train_loss:8.0860 train_time:357995ms step_avg:185.01ms\nstep:1946/4500 train_loss:8.1313 train_time:358180ms step_avg:185.01ms\nstep:1947/4500 train_loss:8.1090 train_time:358369ms step_avg:185.01ms\nstep:1948/4500 train_loss:8.2597 train_time:358557ms step_avg:185.01ms\nstep:1949/4500 train_loss:8.0755 train_time:358742ms step_avg:185.01ms\nstep:1950/4500 train_loss:8.0198 train_time:358928ms step_avg:185.01ms\nstep:1951/4500 train_loss:8.1167 train_time:359112ms step_avg:185.01ms\nstep:1952/4500 train_loss:8.1521 train_time:359295ms step_avg:185.01ms\nstep:1953/4500 train_loss:8.0770 train_time:359479ms step_avg:185.01ms\nstep:1954/4500 train_loss:8.2290 train_time:359664ms step_avg:185.01ms\nstep:1955/4500 train_loss:8.1304 train_time:359847ms step_avg:185.01ms\nstep:1956/4500 train_loss:8.0278 train_time:360030ms step_avg:185.01ms\nstep:1957/4500 train_loss:8.1071 train_time:360214ms step_avg:185.01ms\nstep:1958/4500 train_loss:8.1004 train_time:360397ms step_avg:185.01ms\nstep:1959/4500 train_loss:8.0651 train_time:360583ms step_avg:185.01ms\nstep:1960/4500 train_loss:8.1540 train_time:360766ms step_avg:185.01ms\nstep:1961/4500 train_loss:8.0737 train_time:360951ms step_avg:185.01ms\nstep:1962/4500 train_loss:8.1163 train_time:361136ms step_avg:185.01ms\nstep:1963/4500 train_loss:8.0554 train_time:361323ms step_avg:185.01ms\nstep:1964/4500 train_loss:8.1182 train_time:361511ms step_avg:185.01ms\nstep:1965/4500 train_loss:7.9627 train_time:361695ms step_avg:185.01ms\nstep:1966/4500 train_loss:8.0588 train_time:361880ms step_avg:185.01ms\nstep:1967/4500 train_loss:8.1948 train_time:362065ms step_avg:185.01ms\nstep:1968/4500 train_loss:8.0878 train_time:362257ms step_avg:185.01ms\nstep:1969/4500 train_loss:8.0275 train_time:362439ms step_avg:185.01ms\nstep:1970/4500 train_loss:8.1478 train_time:362626ms step_avg:185.01ms\nstep:1971/4500 train_loss:8.0853 train_time:362810ms step_avg:185.01ms\nstep:1972/4500 train_loss:8.1632 train_time:362994ms step_avg:185.01ms\nstep:1973/4500 train_loss:8.3069 train_time:363176ms step_avg:185.01ms\nstep:1974/4500 train_loss:8.0261 train_time:363360ms step_avg:185.01ms\nstep:1975/4500 train_loss:8.0573 train_time:363544ms step_avg:185.01ms\nstep:1976/4500 train_loss:8.0849 train_time:363729ms step_avg:185.01ms\nstep:1977/4500 train_loss:8.0976 train_time:363912ms step_avg:185.01ms\nstep:1978/4500 train_loss:8.1440 train_time:364097ms step_avg:185.01ms\nstep:1979/4500 train_loss:8.0669 train_time:364282ms step_avg:185.01ms\nstep:1980/4500 train_loss:8.1216 train_time:364466ms step_avg:185.01ms\nstep:1981/4500 train_loss:8.0900 train_time:364650ms step_avg:185.01ms\nstep:1982/4500 train_loss:7.9910 train_time:364835ms step_avg:185.01ms\nstep:1983/4500 train_loss:8.1745 train_time:365021ms step_avg:185.01ms\nstep:1984/4500 train_loss:8.1477 train_time:365210ms step_avg:185.01ms\nstep:1985/4500 train_loss:8.0919 train_time:365393ms step_avg:185.01ms\nstep:1986/4500 train_loss:8.2825 train_time:365576ms step_avg:185.01ms\nstep:1987/4500 train_loss:8.3036 train_time:365759ms step_avg:185.01ms\nstep:1988/4500 train_loss:8.0992 train_time:365945ms step_avg:185.01ms\nstep:1989/4500 train_loss:8.1086 train_time:366129ms step_avg:185.01ms\nstep:1990/4500 train_loss:8.1558 train_time:366312ms step_avg:185.01ms\nstep:1991/4500 train_loss:8.0972 train_time:366496ms step_avg:185.01ms\nstep:1992/4500 train_loss:8.0131 train_time:366680ms step_avg:185.00ms\nstep:1993/4500 train_loss:8.4514 train_time:366863ms step_avg:185.00ms\nstep:1994/4500 train_loss:8.2224 train_time:367046ms step_avg:185.00ms\nstep:1995/4500 train_loss:8.2087 train_time:367230ms step_avg:185.00ms\nstep:1996/4500 train_loss:8.0843 train_time:367413ms step_avg:185.00ms\nstep:1997/4500 train_loss:8.0988 train_time:367595ms step_avg:185.00ms\nstep:1998/4500 train_loss:8.1741 train_time:367780ms step_avg:185.00ms\nstep:1999/4500 train_loss:8.0893 train_time:367966ms step_avg:185.00ms\nstep:2000/4500 train_loss:8.1770 train_time:368149ms step_avg:185.00ms\nstep:2001/4500 train_loss:8.2192 train_time:368332ms step_avg:185.00ms\nstep:2002/4500 train_loss:8.1887 train_time:368516ms step_avg:185.00ms\nstep:2003/4500 train_loss:8.1538 train_time:368702ms step_avg:185.00ms\nstep:2004/4500 train_loss:8.1466 train_time:368887ms step_avg:185.00ms\nstep:2005/4500 train_loss:8.1170 train_time:369071ms step_avg:185.00ms\nstep:2006/4500 train_loss:8.0124 train_time:369255ms step_avg:185.00ms\nstep:2007/4500 train_loss:8.2359 train_time:369441ms step_avg:185.00ms\nstep:2008/4500 train_loss:8.0108 train_time:369626ms step_avg:185.00ms\nstep:2009/4500 train_loss:8.1389 train_time:369813ms step_avg:185.00ms\nstep:2010/4500 train_loss:8.0968 train_time:369998ms step_avg:185.00ms\nstep:2011/4500 train_loss:8.0537 train_time:370186ms step_avg:185.00ms\nstep:2012/4500 train_loss:8.0347 train_time:370369ms step_avg:185.00ms\nstep:2013/4500 train_loss:7.9588 train_time:370556ms step_avg:185.00ms\nstep:2014/4500 train_loss:8.0665 train_time:370741ms step_avg:185.00ms\nstep:2015/4500 train_loss:8.0555 train_time:370926ms step_avg:185.00ms\nstep:2016/4500 train_loss:8.1348 train_time:371113ms step_avg:185.00ms\nstep:2017/4500 train_loss:8.0126 train_time:371299ms step_avg:185.00ms\nstep:2018/4500 train_loss:8.1667 train_time:371483ms step_avg:185.00ms\nstep:2019/4500 train_loss:8.1511 train_time:371667ms step_avg:185.00ms\nstep:2020/4500 train_loss:8.3220 train_time:371853ms step_avg:185.00ms\nstep:2021/4500 train_loss:7.9185 train_time:372036ms step_avg:185.00ms\nstep:2022/4500 train_loss:8.2781 train_time:372220ms step_avg:185.00ms\nstep:2023/4500 train_loss:7.9536 train_time:372404ms step_avg:185.00ms\nstep:2024/4500 train_loss:8.1563 train_time:372588ms step_avg:185.00ms\nstep:2025/4500 train_loss:8.1197 train_time:372774ms step_avg:185.00ms\nstep:2026/4500 train_loss:7.9911 train_time:372961ms step_avg:185.00ms\nstep:2027/4500 train_loss:8.0295 train_time:373146ms step_avg:185.00ms\nstep:2028/4500 train_loss:8.1334 train_time:373333ms step_avg:185.00ms\nstep:2029/4500 train_loss:7.9978 train_time:373519ms step_avg:185.00ms\nstep:2030/4500 train_loss:8.1457 train_time:373704ms step_avg:185.00ms\nstep:2031/4500 train_loss:8.1122 train_time:373889ms step_avg:185.00ms\nstep:2032/4500 train_loss:8.0760 train_time:374076ms step_avg:185.00ms\nstep:2033/4500 train_loss:8.1512 train_time:374260ms step_avg:185.00ms\nstep:2034/4500 train_loss:8.2326 train_time:374443ms step_avg:185.00ms\nstep:2035/4500 train_loss:8.1240 train_time:374628ms step_avg:185.00ms\nstep:2036/4500 train_loss:8.0510 train_time:374812ms step_avg:185.00ms\nstep:2037/4500 train_loss:8.2103 train_time:374996ms step_avg:185.00ms\nstep:2038/4500 train_loss:8.1412 train_time:375181ms step_avg:185.00ms\nstep:2039/4500 train_loss:8.1642 train_time:375366ms step_avg:185.00ms\nstep:2040/4500 train_loss:8.0933 train_time:375550ms step_avg:185.00ms\nstep:2041/4500 train_loss:8.1285 train_time:375736ms step_avg:185.00ms\nstep:2042/4500 train_loss:8.0801 train_time:375922ms step_avg:185.00ms\nstep:2043/4500 train_loss:8.0676 train_time:376107ms step_avg:185.00ms\nstep:2044/4500 train_loss:8.1324 train_time:376292ms step_avg:185.00ms\nstep:2045/4500 train_loss:8.1394 train_time:376476ms step_avg:185.00ms\nstep:2046/4500 train_loss:8.1895 train_time:376660ms step_avg:185.00ms\nstep:2047/4500 train_loss:8.0117 train_time:376846ms step_avg:185.00ms\nstep:2048/4500 train_loss:7.9576 train_time:377033ms step_avg:185.00ms\nstep:2049/4500 train_loss:8.0964 train_time:377219ms step_avg:185.00ms\nstep:2050/4500 train_loss:8.0133 train_time:377406ms step_avg:185.00ms\nstep:2051/4500 train_loss:8.1185 train_time:377595ms step_avg:185.00ms\nstep:2052/4500 train_loss:8.0627 train_time:377779ms step_avg:185.00ms\nstep:2053/4500 train_loss:8.1379 train_time:377965ms step_avg:185.00ms\nstep:2054/4500 train_loss:8.1884 train_time:378147ms step_avg:185.00ms\nstep:2055/4500 train_loss:8.1382 train_time:378333ms step_avg:185.00ms\nstep:2056/4500 train_loss:8.1772 train_time:378518ms step_avg:185.00ms\nstep:2057/4500 train_loss:8.0961 train_time:378706ms step_avg:185.01ms\nstep:2058/4500 train_loss:8.0857 train_time:378895ms step_avg:185.01ms\nstep:2059/4500 train_loss:7.9842 train_time:379080ms step_avg:185.01ms\nstep:2060/4500 train_loss:8.3465 train_time:379269ms step_avg:185.01ms\nstep:2061/4500 train_loss:7.9499 train_time:379452ms step_avg:185.01ms\nstep:2062/4500 train_loss:8.2783 train_time:379637ms step_avg:185.01ms\nstep:2063/4500 train_loss:8.1353 train_time:379821ms step_avg:185.01ms\nstep:2064/4500 train_loss:8.1171 train_time:380008ms step_avg:185.01ms\nstep:2065/4500 train_loss:8.0597 train_time:380193ms step_avg:185.01ms\nstep:2066/4500 train_loss:8.5935 train_time:380377ms step_avg:185.01ms\nstep:2067/4500 train_loss:8.0958 train_time:380565ms step_avg:185.01ms\nstep:2068/4500 train_loss:8.0335 train_time:380751ms step_avg:185.01ms\nstep:2069/4500 train_loss:8.1055 train_time:380935ms step_avg:185.01ms\nstep:2070/4500 train_loss:8.1643 train_time:381119ms step_avg:185.01ms\nstep:2071/4500 train_loss:8.1020 train_time:381304ms step_avg:185.01ms\nstep:2072/4500 train_loss:8.1242 train_time:381488ms step_avg:185.01ms\nstep:2073/4500 train_loss:8.0850 train_time:381672ms step_avg:185.01ms\nstep:2074/4500 train_loss:8.2191 train_time:381857ms step_avg:185.01ms\nstep:2075/4500 train_loss:8.0284 train_time:382042ms step_avg:185.01ms\nstep:2076/4500 train_loss:8.0562 train_time:382226ms step_avg:185.01ms\nstep:2077/4500 train_loss:8.0951 train_time:382414ms step_avg:185.01ms\nstep:2078/4500 train_loss:8.1457 train_time:382598ms step_avg:185.01ms\nstep:2079/4500 train_loss:8.0290 train_time:382784ms step_avg:185.01ms\nstep:2080/4500 train_loss:8.0828 train_time:382970ms step_avg:185.01ms\nstep:2081/4500 train_loss:8.1115 train_time:383155ms step_avg:185.01ms\nstep:2082/4500 train_loss:8.2073 train_time:383339ms step_avg:185.01ms\nstep:2083/4500 train_loss:8.0862 train_time:383523ms step_avg:185.01ms\nstep:2084/4500 train_loss:8.0793 train_time:383711ms step_avg:185.01ms\nstep:2085/4500 train_loss:8.0908 train_time:383894ms step_avg:185.01ms\nstep:2086/4500 train_loss:8.0249 train_time:384079ms step_avg:185.01ms\nstep:2087/4500 train_loss:8.1493 train_time:384263ms step_avg:185.01ms\nstep:2088/4500 train_loss:8.0395 train_time:384448ms step_avg:185.01ms\nstep:2089/4500 train_loss:8.1080 train_time:384634ms step_avg:185.01ms\nstep:2090/4500 train_loss:8.0824 train_time:384819ms step_avg:185.01ms\nstep:2091/4500 train_loss:8.1046 train_time:385008ms step_avg:185.01ms\nstep:2092/4500 train_loss:8.1391 train_time:385193ms step_avg:185.01ms\nstep:2093/4500 train_loss:8.1682 train_time:385375ms step_avg:185.01ms\nstep:2094/4500 train_loss:8.4318 train_time:385558ms step_avg:185.01ms\nstep:2095/4500 train_loss:8.0920 train_time:385743ms step_avg:185.01ms\nstep:2096/4500 train_loss:8.0286 train_time:385928ms step_avg:185.01ms\nstep:2097/4500 train_loss:8.1270 train_time:386112ms step_avg:185.01ms\nstep:2098/4500 train_loss:8.1732 train_time:386296ms step_avg:185.01ms\nstep:2099/4500 train_loss:8.0926 train_time:386482ms step_avg:185.01ms\nstep:2100/4500 train_loss:8.0784 train_time:386666ms step_avg:185.01ms\nstep:2101/4500 train_loss:8.1711 train_time:386853ms step_avg:185.01ms\nstep:2102/4500 train_loss:8.1681 train_time:387039ms step_avg:185.01ms\nstep:2103/4500 train_loss:8.0887 train_time:387226ms step_avg:185.01ms\nstep:2104/4500 train_loss:8.0977 train_time:387409ms step_avg:185.01ms\nstep:2105/4500 train_loss:8.0300 train_time:387593ms step_avg:185.01ms\nstep:2106/4500 train_loss:8.1323 train_time:387777ms step_avg:185.01ms\nstep:2107/4500 train_loss:8.2252 train_time:387960ms step_avg:185.01ms\nstep:2108/4500 train_loss:8.1612 train_time:388144ms step_avg:185.01ms\nstep:2109/4500 train_loss:8.0686 train_time:388328ms step_avg:185.01ms\nstep:2110/4500 train_loss:8.0427 train_time:388512ms step_avg:185.01ms\nstep:2111/4500 train_loss:8.1035 train_time:388697ms step_avg:185.01ms\nstep:2112/4500 train_loss:7.9793 train_time:388886ms step_avg:185.01ms\nstep:2113/4500 train_loss:8.1194 train_time:389069ms step_avg:185.01ms\nstep:2114/4500 train_loss:8.0821 train_time:389256ms step_avg:185.01ms\nstep:2115/4500 train_loss:8.2176 train_time:389441ms step_avg:185.01ms\nstep:2116/4500 train_loss:8.0414 train_time:389625ms step_avg:185.01ms\nstep:2117/4500 train_loss:8.0712 train_time:389810ms step_avg:185.01ms\nstep:2118/4500 train_loss:8.0869 train_time:389993ms step_avg:185.01ms\nstep:2119/4500 train_loss:8.1885 train_time:390177ms step_avg:185.01ms\nstep:2120/4500 train_loss:8.1443 train_time:390361ms step_avg:185.01ms\nstep:2121/4500 train_loss:8.1394 train_time:390545ms step_avg:185.00ms\nstep:2122/4500 train_loss:8.0694 train_time:390728ms step_avg:185.00ms\nstep:2123/4500 train_loss:8.0348 train_time:390912ms step_avg:185.00ms\nstep:2124/4500 train_loss:8.1896 train_time:391095ms step_avg:185.00ms\nstep:2125/4500 train_loss:8.1779 train_time:391279ms step_avg:185.00ms\nstep:2126/4500 train_loss:8.1979 train_time:391465ms step_avg:185.00ms\nstep:2127/4500 train_loss:8.1086 train_time:391649ms step_avg:185.00ms\nstep:2128/4500 train_loss:8.0752 train_time:391832ms step_avg:185.00ms\nstep:2129/4500 train_loss:8.0550 train_time:392016ms step_avg:185.00ms\nstep:2130/4500 train_loss:8.0933 train_time:392200ms step_avg:185.00ms\nstep:2131/4500 train_loss:8.1480 train_time:392389ms step_avg:185.00ms\nstep:2132/4500 train_loss:8.1400 train_time:392573ms step_avg:185.00ms\nstep:2133/4500 train_loss:7.6799 train_time:392757ms step_avg:185.00ms\nstep:2134/4500 train_loss:8.2418 train_time:392942ms step_avg:185.00ms\nstep:2135/4500 train_loss:8.0411 train_time:393126ms step_avg:185.00ms\nstep:2136/4500 train_loss:8.1572 train_time:393312ms step_avg:185.00ms\nstep:2137/4500 train_loss:8.2121 train_time:393496ms step_avg:185.00ms\nstep:2138/4500 train_loss:8.0653 train_time:393682ms step_avg:185.00ms\nstep:2139/4500 train_loss:8.1292 train_time:393869ms step_avg:185.00ms\nstep:2140/4500 train_loss:8.1544 train_time:394052ms step_avg:185.00ms\nstep:2141/4500 train_loss:8.1098 train_time:394236ms step_avg:185.00ms\nstep:2142/4500 train_loss:8.0805 train_time:394421ms step_avg:185.00ms\nstep:2143/4500 train_loss:7.8744 train_time:394606ms step_avg:185.00ms\nstep:2144/4500 train_loss:8.2427 train_time:394794ms step_avg:185.00ms\nstep:2145/4500 train_loss:8.2901 train_time:394981ms step_avg:185.00ms\nstep:2146/4500 train_loss:8.0839 train_time:395168ms step_avg:185.00ms\nstep:2147/4500 train_loss:8.0807 train_time:395352ms step_avg:185.00ms\nstep:2148/4500 train_loss:8.1871 train_time:395536ms step_avg:185.00ms\nstep:2149/4500 train_loss:8.0708 train_time:395720ms step_avg:185.00ms\nstep:2150/4500 train_loss:8.1090 train_time:395906ms step_avg:185.00ms\nstep:2151/4500 train_loss:8.4222 train_time:396090ms step_avg:185.00ms\nstep:2152/4500 train_loss:8.0143 train_time:396275ms step_avg:185.00ms\nstep:2153/4500 train_loss:8.0703 train_time:396459ms step_avg:185.00ms\nstep:2154/4500 train_loss:8.1399 train_time:396643ms step_avg:185.00ms\nstep:2155/4500 train_loss:8.1113 train_time:396829ms step_avg:185.00ms\nstep:2156/4500 train_loss:7.9729 train_time:397014ms step_avg:185.00ms\nstep:2157/4500 train_loss:8.1156 train_time:397200ms step_avg:185.00ms\nstep:2158/4500 train_loss:8.2069 train_time:397387ms step_avg:185.00ms\nstep:2159/4500 train_loss:8.0320 train_time:397570ms step_avg:185.00ms\nstep:2160/4500 train_loss:8.0154 train_time:397755ms step_avg:185.00ms\nstep:2161/4500 train_loss:8.1090 train_time:397941ms step_avg:185.00ms\nstep:2162/4500 train_loss:8.0957 train_time:398127ms step_avg:185.00ms\nstep:2163/4500 train_loss:8.1983 train_time:398313ms step_avg:185.00ms\nstep:2164/4500 train_loss:8.1520 train_time:398496ms step_avg:185.00ms\nstep:2165/4500 train_loss:7.8016 train_time:398681ms step_avg:185.00ms\nstep:2166/4500 train_loss:8.1162 train_time:398868ms step_avg:185.00ms\nstep:2167/4500 train_loss:8.4817 train_time:399051ms step_avg:185.00ms\nstep:2168/4500 train_loss:7.9955 train_time:399235ms step_avg:185.00ms\nstep:2169/4500 train_loss:8.1032 train_time:399420ms step_avg:185.00ms\nstep:2170/4500 train_loss:8.1082 train_time:399606ms step_avg:185.00ms\nstep:2171/4500 train_loss:8.2913 train_time:399792ms step_avg:185.00ms\nstep:2172/4500 train_loss:8.1280 train_time:399974ms step_avg:185.00ms\nstep:2173/4500 train_loss:8.1538 train_time:400159ms step_avg:185.00ms\nstep:2174/4500 train_loss:8.1333 train_time:400346ms step_avg:185.00ms\nstep:2175/4500 train_loss:8.1273 train_time:400533ms step_avg:185.00ms\nstep:2176/4500 train_loss:8.1647 train_time:400716ms step_avg:185.00ms\nstep:2177/4500 train_loss:8.1101 train_time:400900ms step_avg:185.00ms\nstep:2178/4500 train_loss:8.2425 train_time:401084ms step_avg:185.00ms\nstep:2179/4500 train_loss:8.0370 train_time:401268ms step_avg:185.00ms\nstep:2180/4500 train_loss:8.2141 train_time:401452ms step_avg:185.00ms\nstep:2181/4500 train_loss:8.3113 train_time:401636ms step_avg:185.00ms\nstep:2182/4500 train_loss:8.1438 train_time:401822ms step_avg:185.00ms\nstep:2183/4500 train_loss:8.0117 train_time:402006ms step_avg:185.00ms\nstep:2184/4500 train_loss:8.0836 train_time:402191ms step_avg:185.00ms\nstep:2185/4500 train_loss:8.1236 train_time:402375ms step_avg:185.00ms\nstep:2186/4500 train_loss:8.1397 train_time:402561ms step_avg:185.00ms\nstep:2187/4500 train_loss:8.0503 train_time:402746ms step_avg:185.00ms\nstep:2188/4500 train_loss:7.9759 train_time:402932ms step_avg:185.00ms\nstep:2189/4500 train_loss:8.0868 train_time:403115ms step_avg:185.00ms\nstep:2190/4500 train_loss:8.1734 train_time:403299ms step_avg:185.00ms\nstep:2191/4500 train_loss:8.0086 train_time:403482ms step_avg:185.00ms\nstep:2192/4500 train_loss:8.1531 train_time:403668ms step_avg:185.00ms\nstep:2193/4500 train_loss:8.3872 train_time:403852ms step_avg:185.00ms\nstep:2194/4500 train_loss:8.2114 train_time:404036ms step_avg:185.00ms\nstep:2195/4500 train_loss:8.1361 train_time:404222ms step_avg:185.00ms\nstep:2196/4500 train_loss:8.3585 train_time:404406ms step_avg:185.00ms\nstep:2197/4500 train_loss:8.0903 train_time:404589ms step_avg:185.00ms\nstep:2198/4500 train_loss:8.1260 train_time:404774ms step_avg:185.00ms\nstep:2199/4500 train_loss:8.0961 train_time:404960ms step_avg:185.00ms\nstep:2200/4500 train_loss:8.2323 train_time:405150ms step_avg:185.00ms\nstep:2201/4500 train_loss:8.0360 train_time:405336ms step_avg:185.00ms\nstep:2202/4500 train_loss:8.0545 train_time:405519ms step_avg:185.00ms\nstep:2203/4500 train_loss:8.1706 train_time:405705ms step_avg:185.00ms\nstep:2204/4500 train_loss:8.0782 train_time:405892ms step_avg:185.00ms\nstep:2205/4500 train_loss:8.1287 train_time:406077ms step_avg:185.00ms\nstep:2206/4500 train_loss:8.1114 train_time:406262ms step_avg:185.00ms\nstep:2207/4500 train_loss:8.1080 train_time:406447ms step_avg:185.00ms\nstep:2208/4500 train_loss:8.0889 train_time:406630ms step_avg:185.00ms\nstep:2209/4500 train_loss:8.0781 train_time:406814ms step_avg:185.00ms\nstep:2210/4500 train_loss:8.1493 train_time:406998ms step_avg:185.00ms\nstep:2211/4500 train_loss:8.0348 train_time:407183ms step_avg:185.00ms\nstep:2212/4500 train_loss:8.3015 train_time:407368ms step_avg:185.00ms\nstep:2213/4500 train_loss:8.2241 train_time:407554ms step_avg:185.00ms\nstep:2214/4500 train_loss:8.1243 train_time:407737ms step_avg:185.00ms\nstep:2215/4500 train_loss:8.1282 train_time:407923ms step_avg:185.00ms\nstep:2216/4500 train_loss:8.1860 train_time:408109ms step_avg:185.00ms\nstep:2217/4500 train_loss:8.1413 train_time:408296ms step_avg:185.00ms\nstep:2218/4500 train_loss:8.1021 train_time:408481ms step_avg:185.00ms\nstep:2219/4500 train_loss:8.1675 train_time:408666ms step_avg:185.00ms\nstep:2220/4500 train_loss:8.1697 train_time:408850ms step_avg:185.00ms\nstep:2221/4500 train_loss:8.1105 train_time:409035ms step_avg:185.00ms\nstep:2222/4500 train_loss:8.3573 train_time:409218ms step_avg:185.00ms\nstep:2223/4500 train_loss:8.2385 train_time:409403ms step_avg:185.00ms\nstep:2224/4500 train_loss:8.0849 train_time:409590ms step_avg:185.00ms\nstep:2225/4500 train_loss:8.0951 train_time:409776ms step_avg:185.00ms\nstep:2226/4500 train_loss:8.1633 train_time:409961ms step_avg:185.00ms\nstep:2227/4500 train_loss:8.1491 train_time:410146ms step_avg:185.00ms\nstep:2228/4500 train_loss:8.1460 train_time:410331ms step_avg:185.00ms\nstep:2229/4500 train_loss:8.0268 train_time:410516ms step_avg:185.00ms\nstep:2230/4500 train_loss:8.3551 train_time:410701ms step_avg:185.00ms\nstep:2231/4500 train_loss:8.1522 train_time:410887ms step_avg:185.00ms\nstep:2232/4500 train_loss:8.0828 train_time:411073ms step_avg:185.00ms\nstep:2233/4500 train_loss:8.1135 train_time:411257ms step_avg:185.00ms\nstep:2234/4500 train_loss:8.0212 train_time:411443ms step_avg:185.00ms\nstep:2235/4500 train_loss:8.1429 train_time:411627ms step_avg:185.00ms\nstep:2236/4500 train_loss:8.1041 train_time:411814ms step_avg:185.00ms\nstep:2237/4500 train_loss:8.2343 train_time:411999ms step_avg:185.00ms\nstep:2238/4500 train_loss:8.1686 train_time:412187ms step_avg:185.00ms\nstep:2239/4500 train_loss:8.2159 train_time:412371ms step_avg:185.00ms\nstep:2240/4500 train_loss:8.0502 train_time:412555ms step_avg:185.00ms\nstep:2241/4500 train_loss:8.2166 train_time:412739ms step_avg:185.00ms\nstep:2242/4500 train_loss:8.2302 train_time:412923ms step_avg:185.00ms\nstep:2243/4500 train_loss:8.1793 train_time:413107ms step_avg:185.00ms\nstep:2244/4500 train_loss:8.1074 train_time:413293ms step_avg:185.00ms\nstep:2245/4500 train_loss:8.0363 train_time:413476ms step_avg:185.00ms\nstep:2246/4500 train_loss:8.0869 train_time:413661ms step_avg:185.00ms\nstep:2247/4500 train_loss:8.1217 train_time:413850ms step_avg:185.00ms\nstep:2248/4500 train_loss:8.0778 train_time:414035ms step_avg:185.00ms\nstep:2249/4500 train_loss:8.0711 train_time:414221ms step_avg:185.00ms\nstep:2250/4500 train_loss:8.0301 train_time:414408ms step_avg:185.00ms\nstep:2251/4500 train_loss:8.0984 train_time:414593ms step_avg:185.00ms\nstep:2252/4500 train_loss:8.1100 train_time:414777ms step_avg:185.00ms\nstep:2253/4500 train_loss:8.1459 train_time:414962ms step_avg:185.00ms\nstep:2254/4500 train_loss:8.1113 train_time:415149ms step_avg:185.00ms\nstep:2255/4500 train_loss:8.0505 train_time:415334ms step_avg:185.00ms\nstep:2256/4500 train_loss:8.1459 train_time:415517ms step_avg:185.00ms\nstep:2257/4500 train_loss:8.0642 train_time:415701ms step_avg:185.00ms\nstep:2258/4500 train_loss:8.1808 train_time:415885ms step_avg:185.00ms\nstep:2259/4500 train_loss:8.1578 train_time:416069ms step_avg:185.00ms\nstep:2260/4500 train_loss:8.1196 train_time:416253ms step_avg:185.00ms\nstep:2261/4500 train_loss:8.0456 train_time:416438ms step_avg:185.00ms\nstep:2262/4500 train_loss:8.1850 train_time:416623ms step_avg:185.00ms\nstep:2263/4500 train_loss:8.0085 train_time:416807ms step_avg:185.00ms\nstep:2264/4500 train_loss:8.1085 train_time:416994ms step_avg:185.00ms\nstep:2265/4500 train_loss:8.1631 train_time:417179ms step_avg:185.00ms\nstep:2266/4500 train_loss:8.0841 train_time:417366ms step_avg:185.00ms\nstep:2267/4500 train_loss:8.0982 train_time:417554ms step_avg:185.00ms\nstep:2268/4500 train_loss:8.1573 train_time:417740ms step_avg:185.00ms\nstep:2269/4500 train_loss:8.0897 train_time:417925ms step_avg:185.00ms\nstep:2270/4500 train_loss:8.0745 train_time:418112ms step_avg:185.01ms\nstep:2271/4500 train_loss:8.1030 train_time:418296ms step_avg:185.00ms\nstep:2272/4500 train_loss:8.3052 train_time:418482ms step_avg:185.01ms\nstep:2273/4500 train_loss:8.0649 train_time:418670ms step_avg:185.01ms\nstep:2274/4500 train_loss:8.1026 train_time:418853ms step_avg:185.01ms\nstep:2275/4500 train_loss:8.0729 train_time:419036ms step_avg:185.00ms\nstep:2276/4500 train_loss:8.0451 train_time:419221ms step_avg:185.00ms\nstep:2277/4500 train_loss:8.1414 train_time:419405ms step_avg:185.00ms\nstep:2278/4500 train_loss:8.1105 train_time:419590ms step_avg:185.00ms\nstep:2279/4500 train_loss:8.1269 train_time:419774ms step_avg:185.00ms\nstep:2280/4500 train_loss:8.0401 train_time:419958ms step_avg:185.00ms\nstep:2281/4500 train_loss:8.0847 train_time:420142ms step_avg:185.00ms\nstep:2282/4500 train_loss:8.1187 train_time:420327ms step_avg:185.00ms\nstep:2283/4500 train_loss:8.0560 train_time:420509ms step_avg:185.00ms\nstep:2284/4500 train_loss:8.0611 train_time:420694ms step_avg:185.00ms\nstep:2285/4500 train_loss:8.0527 train_time:420881ms step_avg:185.00ms\nstep:2286/4500 train_loss:8.1467 train_time:421299ms step_avg:185.10ms\nstep:2287/4500 train_loss:8.0488 train_time:421483ms step_avg:185.10ms\nstep:2288/4500 train_loss:8.1989 train_time:421667ms step_avg:185.10ms\nstep:2289/4500 train_loss:8.0931 train_time:421850ms step_avg:185.10ms\nstep:2290/4500 train_loss:7.9996 train_time:422034ms step_avg:185.10ms\nstep:2291/4500 train_loss:8.1547 train_time:422221ms step_avg:185.10ms\nstep:2292/4500 train_loss:8.0503 train_time:422409ms step_avg:185.10ms\nstep:2293/4500 train_loss:8.3620 train_time:422591ms step_avg:185.10ms\nstep:2294/4500 train_loss:8.1328 train_time:422775ms step_avg:185.10ms\nstep:2295/4500 train_loss:8.0798 train_time:422960ms step_avg:185.10ms\nstep:2296/4500 train_loss:8.0375 train_time:423145ms step_avg:185.10ms\nstep:2297/4500 train_loss:8.1109 train_time:423331ms step_avg:185.10ms\nstep:2298/4500 train_loss:8.0505 train_time:423518ms step_avg:185.10ms\nstep:2299/4500 train_loss:8.1487 train_time:423702ms step_avg:185.10ms\nstep:2300/4500 train_loss:8.1240 train_time:423886ms step_avg:185.10ms\nstep:2301/4500 train_loss:8.1059 train_time:424071ms step_avg:185.10ms\nstep:2302/4500 train_loss:8.1493 train_time:424253ms step_avg:185.10ms\nstep:2303/4500 train_loss:8.1237 train_time:424436ms step_avg:185.10ms\nstep:2304/4500 train_loss:8.1824 train_time:424622ms step_avg:185.10ms\nstep:2305/4500 train_loss:8.0727 train_time:424807ms step_avg:185.10ms\nstep:2306/4500 train_loss:8.1298 train_time:424990ms step_avg:185.10ms\nstep:2307/4500 train_loss:8.1951 train_time:425174ms step_avg:185.10ms\nstep:2308/4500 train_loss:8.1340 train_time:425358ms step_avg:185.10ms\nstep:2309/4500 train_loss:8.2767 train_time:425544ms step_avg:185.10ms\nstep:2310/4500 train_loss:8.1958 train_time:425729ms step_avg:185.10ms\nstep:2311/4500 train_loss:8.1036 train_time:425913ms step_avg:185.10ms\nstep:2312/4500 train_loss:8.1607 train_time:426098ms step_avg:185.10ms\nstep:2313/4500 train_loss:8.1657 train_time:426284ms step_avg:185.10ms\nstep:2314/4500 train_loss:8.0898 train_time:426470ms step_avg:185.10ms\nstep:2315/4500 train_loss:8.0320 train_time:426653ms step_avg:185.10ms\nstep:2316/4500 train_loss:8.1003 train_time:426836ms step_avg:185.10ms\nstep:2317/4500 train_loss:8.1709 train_time:427020ms step_avg:185.10ms\nstep:2318/4500 train_loss:7.9976 train_time:427202ms step_avg:185.10ms\nstep:2319/4500 train_loss:8.2704 train_time:427387ms step_avg:185.10ms\nstep:2320/4500 train_loss:8.0339 train_time:427574ms step_avg:185.10ms\nstep:2321/4500 train_loss:8.2409 train_time:427759ms step_avg:185.10ms\nstep:2322/4500 train_loss:8.1745 train_time:427946ms step_avg:185.10ms\nstep:2323/4500 train_loss:8.0171 train_time:428133ms step_avg:185.10ms\nstep:2324/4500 train_loss:8.1063 train_time:428317ms step_avg:185.10ms\nstep:2325/4500 train_loss:8.0372 train_time:428503ms step_avg:185.10ms\nstep:2326/4500 train_loss:8.1494 train_time:428690ms step_avg:185.10ms\nstep:2327/4500 train_loss:8.0933 train_time:428875ms step_avg:185.10ms\nstep:2328/4500 train_loss:8.1025 train_time:429057ms step_avg:185.10ms\nstep:2329/4500 train_loss:8.0897 train_time:429241ms step_avg:185.10ms\nstep:2330/4500 train_loss:8.1348 train_time:429426ms step_avg:185.10ms\nstep:2331/4500 train_loss:8.0853 train_time:429613ms step_avg:185.10ms\nstep:2332/4500 train_loss:8.2410 train_time:429800ms step_avg:185.10ms\nstep:2333/4500 train_loss:8.0639 train_time:429986ms step_avg:185.10ms\nstep:2334/4500 train_loss:8.2710 train_time:430170ms step_avg:185.10ms\nstep:2335/4500 train_loss:7.9695 train_time:430352ms step_avg:185.10ms\nstep:2336/4500 train_loss:8.0638 train_time:430534ms step_avg:185.10ms\nstep:2337/4500 train_loss:8.1280 train_time:430718ms step_avg:185.10ms\nstep:2338/4500 train_loss:8.0021 train_time:430904ms step_avg:185.10ms\nstep:2339/4500 train_loss:8.1129 train_time:431092ms step_avg:185.10ms\nstep:2340/4500 train_loss:8.0559 train_time:431282ms step_avg:185.10ms\nstep:2341/4500 train_loss:8.0299 train_time:431463ms step_avg:185.10ms\nstep:2342/4500 train_loss:8.1463 train_time:431648ms step_avg:185.10ms\nstep:2343/4500 train_loss:8.1600 train_time:431836ms step_avg:185.10ms\nstep:2344/4500 train_loss:8.1083 train_time:432018ms step_avg:185.10ms\nstep:2345/4500 train_loss:8.2005 train_time:432203ms step_avg:185.10ms\nstep:2346/4500 train_loss:8.1113 train_time:432389ms step_avg:185.10ms\nstep:2347/4500 train_loss:8.0629 train_time:432572ms step_avg:185.10ms\nstep:2348/4500 train_loss:8.0631 train_time:432761ms step_avg:185.10ms\nstep:2349/4500 train_loss:8.0922 train_time:432945ms step_avg:185.10ms\nstep:2350/4500 train_loss:8.1589 train_time:433129ms step_avg:185.10ms\nstep:2351/4500 train_loss:8.0016 train_time:433317ms step_avg:185.10ms\nstep:2352/4500 train_loss:8.1579 train_time:433503ms step_avg:185.10ms\nstep:2353/4500 train_loss:8.1019 train_time:433688ms step_avg:185.10ms\nstep:2354/4500 train_loss:7.9985 train_time:433875ms step_avg:185.10ms\nstep:2355/4500 train_loss:8.1318 train_time:434060ms step_avg:185.10ms\nstep:2356/4500 train_loss:8.1744 train_time:434245ms step_avg:185.10ms\nstep:2357/4500 train_loss:8.0169 train_time:434428ms step_avg:185.10ms\nstep:2358/4500 train_loss:8.2040 train_time:434611ms step_avg:185.10ms\nstep:2359/4500 train_loss:8.1212 train_time:434796ms step_avg:185.10ms\nstep:2360/4500 train_loss:8.2339 train_time:434982ms step_avg:185.10ms\nstep:2361/4500 train_loss:8.1283 train_time:435167ms step_avg:185.10ms\nstep:2362/4500 train_loss:8.1634 train_time:435352ms step_avg:185.10ms\nstep:2363/4500 train_loss:8.1763 train_time:435536ms step_avg:185.10ms\nstep:2364/4500 train_loss:8.1494 train_time:435720ms step_avg:185.10ms\nstep:2365/4500 train_loss:8.1419 train_time:435906ms step_avg:185.10ms\nstep:2366/4500 train_loss:8.0409 train_time:436089ms step_avg:185.10ms\nstep:2367/4500 train_loss:8.0979 train_time:436276ms step_avg:185.10ms\nstep:2368/4500 train_loss:8.0205 train_time:436461ms step_avg:185.10ms\nstep:2369/4500 train_loss:8.1911 train_time:436647ms step_avg:185.10ms\nstep:2370/4500 train_loss:7.9731 train_time:436829ms step_avg:185.10ms\nstep:2371/4500 train_loss:8.1585 train_time:437015ms step_avg:185.10ms\nstep:2372/4500 train_loss:8.1279 train_time:437199ms step_avg:185.10ms\nstep:2373/4500 train_loss:8.1369 train_time:437383ms step_avg:185.10ms\nstep:2374/4500 train_loss:8.1587 train_time:437570ms step_avg:185.10ms\nstep:2375/4500 train_loss:8.1570 train_time:437754ms step_avg:185.10ms\nstep:2376/4500 train_loss:8.1414 train_time:437938ms step_avg:185.10ms\nstep:2377/4500 train_loss:8.1292 train_time:438123ms step_avg:185.10ms\nstep:2378/4500 train_loss:8.1072 train_time:438308ms step_avg:185.10ms\nstep:2379/4500 train_loss:8.1425 train_time:438492ms step_avg:185.10ms\nstep:2380/4500 train_loss:8.2442 train_time:438675ms step_avg:185.09ms\nstep:2381/4500 train_loss:8.0534 train_time:438860ms step_avg:185.09ms\nstep:2382/4500 train_loss:8.0642 train_time:439044ms step_avg:185.09ms\nstep:2383/4500 train_loss:8.2427 train_time:439230ms step_avg:185.10ms\nstep:2384/4500 train_loss:7.9275 train_time:439415ms step_avg:185.09ms\nstep:2385/4500 train_loss:8.1632 train_time:439601ms step_avg:185.10ms\nstep:2386/4500 train_loss:8.1210 train_time:439788ms step_avg:185.10ms\nstep:2387/4500 train_loss:8.0954 train_time:439973ms step_avg:185.10ms\nstep:2388/4500 train_loss:8.0882 train_time:440157ms step_avg:185.10ms\nstep:2389/4500 train_loss:8.1676 train_time:440342ms step_avg:185.10ms\nstep:2390/4500 train_loss:8.0581 train_time:440526ms step_avg:185.10ms\nstep:2391/4500 train_loss:8.0404 train_time:440712ms step_avg:185.10ms\nstep:2392/4500 train_loss:8.1128 train_time:440896ms step_avg:185.10ms\nstep:2393/4500 train_loss:8.1201 train_time:441081ms step_avg:185.09ms\nstep:2394/4500 train_loss:8.1962 train_time:441267ms step_avg:185.10ms\nstep:2395/4500 train_loss:8.1380 train_time:441450ms step_avg:185.09ms\nstep:2396/4500 train_loss:8.1799 train_time:441636ms step_avg:185.09ms\nstep:2397/4500 train_loss:8.1437 train_time:441822ms step_avg:185.09ms\nstep:2398/4500 train_loss:8.1421 train_time:442012ms step_avg:185.10ms\nstep:2399/4500 train_loss:8.0699 train_time:442196ms step_avg:185.10ms\nstep:2400/4500 train_loss:8.0597 train_time:442383ms step_avg:185.10ms\nstep:2401/4500 train_loss:8.1777 train_time:442567ms step_avg:185.10ms\nstep:2402/4500 train_loss:8.1401 train_time:442751ms step_avg:185.10ms\nstep:2403/4500 train_loss:8.1696 train_time:442936ms step_avg:185.10ms\nstep:2404/4500 train_loss:8.0427 train_time:443123ms step_avg:185.10ms\nstep:2405/4500 train_loss:8.1555 train_time:443308ms step_avg:185.10ms\nstep:2406/4500 train_loss:7.9861 train_time:443492ms step_avg:185.10ms\nstep:2407/4500 train_loss:8.0388 train_time:443678ms step_avg:185.10ms\nstep:2408/4500 train_loss:8.0199 train_time:443860ms step_avg:185.10ms\nstep:2409/4500 train_loss:8.0534 train_time:444046ms step_avg:185.10ms\nstep:2410/4500 train_loss:8.0775 train_time:444232ms step_avg:185.10ms\nstep:2411/4500 train_loss:8.1048 train_time:444415ms step_avg:185.10ms\nstep:2412/4500 train_loss:8.0405 train_time:444598ms step_avg:185.09ms\nstep:2413/4500 train_loss:8.0182 train_time:444786ms step_avg:185.10ms\nstep:2414/4500 train_loss:8.0635 train_time:444968ms step_avg:185.09ms\nstep:2415/4500 train_loss:8.2231 train_time:445151ms step_avg:185.09ms\nstep:2416/4500 train_loss:8.1263 train_time:445335ms step_avg:185.09ms\nstep:2417/4500 train_loss:8.0189 train_time:445521ms step_avg:185.09ms\nstep:2418/4500 train_loss:8.1386 train_time:445706ms step_avg:185.09ms\nstep:2419/4500 train_loss:8.1990 train_time:445891ms step_avg:185.09ms\nstep:2420/4500 train_loss:8.2152 train_time:446075ms step_avg:185.09ms\nstep:2421/4500 train_loss:8.1211 train_time:446259ms step_avg:185.09ms\nstep:2422/4500 train_loss:8.1683 train_time:446444ms step_avg:185.09ms\nstep:2423/4500 train_loss:8.1255 train_time:446627ms step_avg:185.09ms\nstep:2424/4500 train_loss:8.1272 train_time:446814ms step_avg:185.09ms\nstep:2425/4500 train_loss:8.1361 train_time:447000ms step_avg:185.09ms\nstep:2426/4500 train_loss:8.1624 train_time:447184ms step_avg:185.09ms\nstep:2427/4500 train_loss:8.0130 train_time:447371ms step_avg:185.09ms\nstep:2428/4500 train_loss:8.0558 train_time:447555ms step_avg:185.09ms\nstep:2429/4500 train_loss:8.0416 train_time:447738ms step_avg:185.09ms\nstep:2430/4500 train_loss:8.1269 train_time:447923ms step_avg:185.09ms\nstep:2431/4500 train_loss:8.0952 train_time:448109ms step_avg:185.09ms\nstep:2432/4500 train_loss:8.0881 train_time:448295ms step_avg:185.09ms\nstep:2433/4500 train_loss:8.0102 train_time:448481ms step_avg:185.09ms\nstep:2434/4500 train_loss:8.1131 train_time:448668ms step_avg:185.09ms\nstep:2435/4500 train_loss:7.9457 train_time:448856ms step_avg:185.10ms\nstep:2436/4500 train_loss:8.0839 train_time:449040ms step_avg:185.09ms\nstep:2437/4500 train_loss:8.0460 train_time:449225ms step_avg:185.09ms\nstep:2438/4500 train_loss:8.0784 train_time:449411ms step_avg:185.10ms\nstep:2439/4500 train_loss:8.0706 train_time:449593ms step_avg:185.09ms\nstep:2440/4500 train_loss:8.0936 train_time:449777ms step_avg:185.09ms\nstep:2441/4500 train_loss:8.0523 train_time:449960ms step_avg:185.09ms\nstep:2442/4500 train_loss:8.0751 train_time:450145ms step_avg:185.09ms\nstep:2443/4500 train_loss:8.0125 train_time:450332ms step_avg:185.09ms\nstep:2444/4500 train_loss:8.1257 train_time:450515ms step_avg:185.09ms\nstep:2445/4500 train_loss:8.0363 train_time:450701ms step_avg:185.09ms\nstep:2446/4500 train_loss:8.1023 train_time:450888ms step_avg:185.09ms\nstep:2447/4500 train_loss:8.0922 train_time:451070ms step_avg:185.09ms\nstep:2448/4500 train_loss:8.1097 train_time:451254ms step_avg:185.09ms\nstep:2449/4500 train_loss:8.2105 train_time:451438ms step_avg:185.09ms\nstep:2450/4500 train_loss:8.1900 train_time:451623ms step_avg:185.09ms\nstep:2451/4500 train_loss:8.1168 train_time:451808ms step_avg:185.09ms\nstep:2452/4500 train_loss:8.0923 train_time:451993ms step_avg:185.09ms\nstep:2453/4500 train_loss:7.9897 train_time:452178ms step_avg:185.09ms\nstep:2454/4500 train_loss:7.9651 train_time:452364ms step_avg:185.09ms\nstep:2455/4500 train_loss:8.0890 train_time:452547ms step_avg:185.09ms\nstep:2456/4500 train_loss:8.1403 train_time:452731ms step_avg:185.09ms\nstep:2457/4500 train_loss:8.1175 train_time:452915ms step_avg:185.09ms\nstep:2458/4500 train_loss:8.2173 train_time:453100ms step_avg:185.09ms\nstep:2459/4500 train_loss:8.1916 train_time:453285ms step_avg:185.09ms\nstep:2460/4500 train_loss:8.2879 train_time:453472ms step_avg:185.09ms\nstep:2461/4500 train_loss:8.1687 train_time:453656ms step_avg:185.09ms\nstep:2462/4500 train_loss:8.1301 train_time:453840ms step_avg:185.09ms\nstep:2463/4500 train_loss:8.2174 train_time:454025ms step_avg:185.09ms\nstep:2464/4500 train_loss:8.1664 train_time:454212ms step_avg:185.09ms\nstep:2465/4500 train_loss:8.1865 train_time:454395ms step_avg:185.09ms\nstep:2466/4500 train_loss:8.1311 train_time:454581ms step_avg:185.09ms\nstep:2467/4500 train_loss:8.1454 train_time:454768ms step_avg:185.09ms\nstep:2468/4500 train_loss:7.9842 train_time:454952ms step_avg:185.09ms\nstep:2469/4500 train_loss:8.1643 train_time:455139ms step_avg:185.09ms\nstep:2470/4500 train_loss:8.1308 train_time:455322ms step_avg:185.09ms\nstep:2471/4500 train_loss:8.0581 train_time:455507ms step_avg:185.09ms\nstep:2472/4500 train_loss:8.2270 train_time:455692ms step_avg:185.09ms\nstep:2473/4500 train_loss:8.0439 train_time:455878ms step_avg:185.09ms\nstep:2474/4500 train_loss:8.0576 train_time:456062ms step_avg:185.09ms\nstep:2475/4500 train_loss:8.0862 train_time:456247ms step_avg:185.09ms\nstep:2476/4500 train_loss:8.0960 train_time:456430ms step_avg:185.09ms\nstep:2477/4500 train_loss:8.2584 train_time:456613ms step_avg:185.09ms\nstep:2478/4500 train_loss:8.1929 train_time:456797ms step_avg:185.09ms\nstep:2479/4500 train_loss:8.1155 train_time:456983ms step_avg:185.09ms\nstep:2480/4500 train_loss:8.2317 train_time:457167ms step_avg:185.09ms\nstep:2481/4500 train_loss:8.0682 train_time:457350ms step_avg:185.09ms\nstep:2482/4500 train_loss:8.1617 train_time:457535ms step_avg:185.09ms\nstep:2483/4500 train_loss:7.8968 train_time:457721ms step_avg:185.09ms\nstep:2484/4500 train_loss:8.1811 train_time:457905ms step_avg:185.09ms\nstep:2485/4500 train_loss:8.2309 train_time:458092ms step_avg:185.09ms\nstep:2486/4500 train_loss:8.1068 train_time:458275ms step_avg:185.09ms\nstep:2487/4500 train_loss:8.1132 train_time:458461ms step_avg:185.09ms\nstep:2488/4500 train_loss:8.1384 train_time:458648ms step_avg:185.09ms\nstep:2489/4500 train_loss:8.3093 train_time:458832ms step_avg:185.09ms\nstep:2490/4500 train_loss:8.0799 train_time:459017ms step_avg:185.09ms\nstep:2491/4500 train_loss:8.1062 train_time:459201ms step_avg:185.09ms\nstep:2492/4500 train_loss:8.1229 train_time:459385ms step_avg:185.09ms\nstep:2493/4500 train_loss:8.0988 train_time:459569ms step_avg:185.09ms\nstep:2494/4500 train_loss:8.1958 train_time:459756ms step_avg:185.09ms\nstep:2495/4500 train_loss:8.1059 train_time:459944ms step_avg:185.09ms\nstep:2496/4500 train_loss:8.2186 train_time:460130ms step_avg:185.09ms\nstep:2497/4500 train_loss:8.1025 train_time:460315ms step_avg:185.09ms\nstep:2498/4500 train_loss:8.1136 train_time:460500ms step_avg:185.09ms\nstep:2499/4500 train_loss:8.0550 train_time:460684ms step_avg:185.09ms\nstep:2500/4500 train_loss:8.1567 train_time:460869ms step_avg:185.09ms\nstep:2501/4500 train_loss:8.1690 train_time:461052ms step_avg:185.09ms\nstep:2502/4500 train_loss:8.0940 train_time:461235ms step_avg:185.09ms\nstep:2503/4500 train_loss:8.0912 train_time:461420ms step_avg:185.09ms\nstep:2504/4500 train_loss:8.3161 train_time:461605ms step_avg:185.09ms\nstep:2505/4500 train_loss:8.1404 train_time:461790ms step_avg:185.09ms\nstep:2506/4500 train_loss:8.0829 train_time:461974ms step_avg:185.09ms\nstep:2507/4500 train_loss:8.1767 train_time:462159ms step_avg:185.09ms\nstep:2508/4500 train_loss:8.2039 train_time:462345ms step_avg:185.09ms\nstep:2509/4500 train_loss:8.0615 train_time:462531ms step_avg:185.09ms\nstep:2510/4500 train_loss:8.0556 train_time:462714ms step_avg:185.09ms\nstep:2511/4500 train_loss:8.0204 train_time:462898ms step_avg:185.09ms\nstep:2512/4500 train_loss:8.0438 train_time:463082ms step_avg:185.08ms\nstep:2513/4500 train_loss:8.1807 train_time:463268ms step_avg:185.08ms\nstep:2514/4500 train_loss:8.1391 train_time:463451ms step_avg:185.08ms\nstep:2515/4500 train_loss:8.1170 train_time:463635ms step_avg:185.08ms\nstep:2516/4500 train_loss:8.2121 train_time:463821ms step_avg:185.08ms\nstep:2517/4500 train_loss:8.0579 train_time:464005ms step_avg:185.08ms\nstep:2518/4500 train_loss:8.1359 train_time:464188ms step_avg:185.08ms\nstep:2519/4500 train_loss:8.1140 train_time:464373ms step_avg:185.08ms\nstep:2520/4500 train_loss:8.0277 train_time:464558ms step_avg:185.08ms\nstep:2521/4500 train_loss:8.1066 train_time:464747ms step_avg:185.08ms\nstep:2522/4500 train_loss:8.1644 train_time:464932ms step_avg:185.08ms\nstep:2523/4500 train_loss:8.1679 train_time:465117ms step_avg:185.08ms\nstep:2524/4500 train_loss:8.0789 train_time:465301ms step_avg:185.08ms\nstep:2525/4500 train_loss:8.1146 train_time:465485ms step_avg:185.08ms\nstep:2526/4500 train_loss:8.1265 train_time:465672ms step_avg:185.08ms\nstep:2527/4500 train_loss:8.0656 train_time:465854ms step_avg:185.08ms\nstep:2528/4500 train_loss:8.1929 train_time:466039ms step_avg:185.08ms\nstep:2529/4500 train_loss:8.1206 train_time:466222ms step_avg:185.08ms\nstep:2530/4500 train_loss:8.1235 train_time:466406ms step_avg:185.08ms\nstep:2531/4500 train_loss:8.1291 train_time:466591ms step_avg:185.08ms\nstep:2532/4500 train_loss:8.1603 train_time:466776ms step_avg:185.08ms\nstep:2533/4500 train_loss:8.0872 train_time:466963ms step_avg:185.08ms\nstep:2534/4500 train_loss:8.2761 train_time:467150ms step_avg:185.08ms\nstep:2535/4500 train_loss:8.1506 train_time:467335ms step_avg:185.08ms\nstep:2536/4500 train_loss:8.0205 train_time:467521ms step_avg:185.08ms\nstep:2537/4500 train_loss:8.1006 train_time:467706ms step_avg:185.08ms\nstep:2538/4500 train_loss:8.1354 train_time:467894ms step_avg:185.08ms\nstep:2539/4500 train_loss:8.2075 train_time:468078ms step_avg:185.08ms\nstep:2540/4500 train_loss:7.9974 train_time:468262ms step_avg:185.08ms\nstep:2541/4500 train_loss:8.0683 train_time:468450ms step_avg:185.08ms\nstep:2542/4500 train_loss:8.1642 train_time:468633ms step_avg:185.08ms\nstep:2543/4500 train_loss:8.2603 train_time:468817ms step_avg:185.08ms\nstep:2544/4500 train_loss:8.1160 train_time:469001ms step_avg:185.08ms\nstep:2545/4500 train_loss:8.1952 train_time:469186ms step_avg:185.08ms\nstep:2546/4500 train_loss:8.0348 train_time:469371ms step_avg:185.08ms\nstep:2547/4500 train_loss:8.0974 train_time:469554ms step_avg:185.08ms\nstep:2548/4500 train_loss:8.0339 train_time:469737ms step_avg:185.08ms\nstep:2549/4500 train_loss:8.0852 train_time:469922ms step_avg:185.08ms\nstep:2550/4500 train_loss:8.0065 train_time:470110ms step_avg:185.08ms\nstep:2551/4500 train_loss:8.1894 train_time:470293ms step_avg:185.08ms\nstep:2552/4500 train_loss:8.3211 train_time:470477ms step_avg:185.08ms\nstep:2553/4500 train_loss:8.1278 train_time:470665ms step_avg:185.08ms\nstep:2554/4500 train_loss:8.0112 train_time:470848ms step_avg:185.08ms\nstep:2555/4500 train_loss:8.0918 train_time:471033ms step_avg:185.08ms\nstep:2556/4500 train_loss:8.0972 train_time:471219ms step_avg:185.08ms\nstep:2557/4500 train_loss:8.1161 train_time:471407ms step_avg:185.08ms\nstep:2558/4500 train_loss:8.0900 train_time:471591ms step_avg:185.08ms\nstep:2559/4500 train_loss:8.1092 train_time:471775ms step_avg:185.08ms\nstep:2560/4500 train_loss:8.0247 train_time:471958ms step_avg:185.08ms\nstep:2561/4500 train_loss:8.1917 train_time:472141ms step_avg:185.08ms\nstep:2562/4500 train_loss:8.0661 train_time:472327ms step_avg:185.08ms\nstep:2563/4500 train_loss:8.0870 train_time:472512ms step_avg:185.08ms\nstep:2564/4500 train_loss:8.2256 train_time:472696ms step_avg:185.08ms\nstep:2565/4500 train_loss:8.0757 train_time:472882ms step_avg:185.08ms\nstep:2566/4500 train_loss:8.1447 train_time:473067ms step_avg:185.08ms\nstep:2567/4500 train_loss:8.1772 train_time:473250ms step_avg:185.08ms\nstep:2568/4500 train_loss:8.1400 train_time:473432ms step_avg:185.08ms\nstep:2569/4500 train_loss:8.1382 train_time:473616ms step_avg:185.08ms\nstep:2570/4500 train_loss:8.1112 train_time:473800ms step_avg:185.08ms\nstep:2571/4500 train_loss:8.0814 train_time:473988ms step_avg:185.08ms\nstep:2572/4500 train_loss:8.1429 train_time:474173ms step_avg:185.08ms\nstep:2573/4500 train_loss:8.2402 train_time:474356ms step_avg:185.08ms\nstep:2574/4500 train_loss:8.0587 train_time:474541ms step_avg:185.08ms\nstep:2575/4500 train_loss:8.0838 train_time:474730ms step_avg:185.08ms\nstep:2576/4500 train_loss:8.2059 train_time:474915ms step_avg:185.08ms\nstep:2577/4500 train_loss:8.1176 train_time:475100ms step_avg:185.08ms\nstep:2578/4500 train_loss:8.1205 train_time:475288ms step_avg:185.08ms\nstep:2579/4500 train_loss:8.1784 train_time:475472ms step_avg:185.08ms\nstep:2580/4500 train_loss:8.1525 train_time:475656ms step_avg:185.08ms\nstep:2581/4500 train_loss:8.0939 train_time:475840ms step_avg:185.08ms\nstep:2582/4500 train_loss:7.9845 train_time:476024ms step_avg:185.08ms\nstep:2583/4500 train_loss:8.1417 train_time:476212ms step_avg:185.08ms\nstep:2584/4500 train_loss:8.1868 train_time:476395ms step_avg:185.08ms\nstep:2585/4500 train_loss:8.0953 train_time:476580ms step_avg:185.08ms\nstep:2586/4500 train_loss:8.0849 train_time:476765ms step_avg:185.08ms\nstep:2587/4500 train_loss:8.0119 train_time:476948ms step_avg:185.08ms\nstep:2588/4500 train_loss:8.1323 train_time:477133ms step_avg:185.08ms\nstep:2589/4500 train_loss:8.0596 train_time:477318ms step_avg:185.08ms\nstep:2590/4500 train_loss:8.0642 train_time:477503ms step_avg:185.08ms\nstep:2591/4500 train_loss:8.0878 train_time:477691ms step_avg:185.08ms\nstep:2592/4500 train_loss:8.1537 train_time:477874ms step_avg:185.08ms\nstep:2593/4500 train_loss:8.1030 train_time:478058ms step_avg:185.08ms\nstep:2594/4500 train_loss:8.2472 train_time:478242ms step_avg:185.08ms\nstep:2595/4500 train_loss:8.1781 train_time:478427ms step_avg:185.08ms\nstep:2596/4500 train_loss:8.0483 train_time:478610ms step_avg:185.08ms\nstep:2597/4500 train_loss:8.2482 train_time:478795ms step_avg:185.08ms\nstep:2598/4500 train_loss:8.1755 train_time:478982ms step_avg:185.08ms\nstep:2599/4500 train_loss:8.1588 train_time:479166ms step_avg:185.08ms\nstep:2600/4500 train_loss:8.2178 train_time:479350ms step_avg:185.08ms\nstep:2601/4500 train_loss:8.2659 train_time:479533ms step_avg:185.08ms\nstep:2602/4500 train_loss:8.4685 train_time:479716ms step_avg:185.08ms\nstep:2603/4500 train_loss:8.1436 train_time:479903ms step_avg:185.08ms\nstep:2604/4500 train_loss:8.0977 train_time:480088ms step_avg:185.08ms\nstep:2605/4500 train_loss:8.2538 train_time:480272ms step_avg:185.08ms\nstep:2606/4500 train_loss:7.9969 train_time:480456ms step_avg:185.08ms\nstep:2607/4500 train_loss:8.1038 train_time:480640ms step_avg:185.08ms\nstep:2608/4500 train_loss:8.1419 train_time:480827ms step_avg:185.08ms\nstep:2609/4500 train_loss:8.2011 train_time:481011ms step_avg:185.08ms\nstep:2610/4500 train_loss:8.1210 train_time:481194ms step_avg:185.07ms\nstep:2611/4500 train_loss:8.0529 train_time:481378ms step_avg:185.07ms\nstep:2612/4500 train_loss:8.1430 train_time:481564ms step_avg:185.07ms\nstep:2613/4500 train_loss:8.1520 train_time:481751ms step_avg:185.08ms\nstep:2614/4500 train_loss:8.1202 train_time:481939ms step_avg:185.08ms\nstep:2615/4500 train_loss:8.0992 train_time:482125ms step_avg:185.08ms\nstep:2616/4500 train_loss:8.0600 train_time:482310ms step_avg:185.08ms\nstep:2617/4500 train_loss:8.2445 train_time:482494ms step_avg:185.08ms\nstep:2618/4500 train_loss:8.1255 train_time:482678ms step_avg:185.08ms\nstep:2619/4500 train_loss:8.0740 train_time:482863ms step_avg:185.08ms\nstep:2620/4500 train_loss:7.9944 train_time:483048ms step_avg:185.08ms\nstep:2621/4500 train_loss:8.1249 train_time:483232ms step_avg:185.08ms\nstep:2622/4500 train_loss:8.2310 train_time:483416ms step_avg:185.08ms\nstep:2623/4500 train_loss:8.1090 train_time:483601ms step_avg:185.07ms\nstep:2624/4500 train_loss:8.0605 train_time:483789ms step_avg:185.08ms\nstep:2625/4500 train_loss:8.0420 train_time:483975ms step_avg:185.08ms\nstep:2626/4500 train_loss:8.0670 train_time:484161ms step_avg:185.08ms\nstep:2627/4500 train_loss:8.1459 train_time:484345ms step_avg:185.08ms\nstep:2628/4500 train_loss:8.0737 train_time:484529ms step_avg:185.08ms\nstep:2629/4500 train_loss:8.0902 train_time:484712ms step_avg:185.08ms\nstep:2630/4500 train_loss:8.1733 train_time:484897ms step_avg:185.08ms\nstep:2631/4500 train_loss:8.1433 train_time:485080ms step_avg:185.07ms\nstep:2632/4500 train_loss:8.1420 train_time:485266ms step_avg:185.07ms\nstep:2633/4500 train_loss:7.9646 train_time:485450ms step_avg:185.07ms\nstep:2634/4500 train_loss:8.1393 train_time:485635ms step_avg:185.07ms\nstep:2635/4500 train_loss:8.0673 train_time:485819ms step_avg:185.07ms\nstep:2636/4500 train_loss:8.1675 train_time:486007ms step_avg:185.08ms\nstep:2637/4500 train_loss:8.1029 train_time:486191ms step_avg:185.07ms\nstep:2638/4500 train_loss:8.1044 train_time:486375ms step_avg:185.07ms\nstep:2639/4500 train_loss:8.1439 train_time:486559ms step_avg:185.07ms\nstep:2640/4500 train_loss:8.1472 train_time:486745ms step_avg:185.07ms\nstep:2641/4500 train_loss:8.1153 train_time:486933ms step_avg:185.08ms\nstep:2642/4500 train_loss:8.2216 train_time:487118ms step_avg:185.08ms\nstep:2643/4500 train_loss:8.0798 train_time:487303ms step_avg:185.08ms\nstep:2644/4500 train_loss:8.1063 train_time:487488ms step_avg:185.08ms\nstep:2645/4500 train_loss:8.1597 train_time:487672ms step_avg:185.07ms\nstep:2646/4500 train_loss:8.0657 train_time:487856ms step_avg:185.07ms\nstep:2647/4500 train_loss:8.1210 train_time:488044ms step_avg:185.08ms\nstep:2648/4500 train_loss:8.0541 train_time:488230ms step_avg:185.08ms\nstep:2649/4500 train_loss:8.1439 train_time:488413ms step_avg:185.08ms\nstep:2650/4500 train_loss:8.1478 train_time:488597ms step_avg:185.07ms\nstep:2651/4500 train_loss:8.1555 train_time:488781ms step_avg:185.07ms\nstep:2652/4500 train_loss:8.1387 train_time:488966ms step_avg:185.07ms\nstep:2653/4500 train_loss:8.0841 train_time:489149ms step_avg:185.07ms\nstep:2654/4500 train_loss:8.1019 train_time:489336ms step_avg:185.07ms\nstep:2655/4500 train_loss:8.1046 train_time:489520ms step_avg:185.07ms\nstep:2656/4500 train_loss:8.1001 train_time:489705ms step_avg:185.07ms\nstep:2657/4500 train_loss:8.0972 train_time:489893ms step_avg:185.07ms\nstep:2658/4500 train_loss:7.9847 train_time:490076ms step_avg:185.07ms\nstep:2659/4500 train_loss:8.0829 train_time:490260ms step_avg:185.07ms\nstep:2660/4500 train_loss:8.2415 train_time:490445ms step_avg:185.07ms\nstep:2661/4500 train_loss:8.1011 train_time:490629ms step_avg:185.07ms\nstep:2662/4500 train_loss:8.0327 train_time:490815ms step_avg:185.07ms\nstep:2663/4500 train_loss:8.1690 train_time:491000ms step_avg:185.07ms\nstep:2664/4500 train_loss:8.0476 train_time:491189ms step_avg:185.07ms\nstep:2665/4500 train_loss:8.1883 train_time:491373ms step_avg:185.07ms\nstep:2666/4500 train_loss:8.1816 train_time:491556ms step_avg:185.07ms\nstep:2667/4500 train_loss:8.0688 train_time:491740ms step_avg:185.07ms\nstep:2668/4500 train_loss:8.2295 train_time:491924ms step_avg:185.07ms\nstep:2669/4500 train_loss:8.1616 train_time:492110ms step_avg:185.07ms\nstep:2670/4500 train_loss:8.1070 train_time:492295ms step_avg:185.07ms\nstep:2671/4500 train_loss:8.1903 train_time:492483ms step_avg:185.07ms\nstep:2672/4500 train_loss:8.1170 train_time:492667ms step_avg:185.07ms\nstep:2673/4500 train_loss:8.0562 train_time:492853ms step_avg:185.07ms\nstep:2674/4500 train_loss:8.1684 train_time:493036ms step_avg:185.07ms\nstep:2675/4500 train_loss:8.2719 train_time:493220ms step_avg:185.07ms\nstep:2676/4500 train_loss:8.0848 train_time:493406ms step_avg:185.07ms\nstep:2677/4500 train_loss:8.1330 train_time:493590ms step_avg:185.07ms\nstep:2678/4500 train_loss:8.1226 train_time:493773ms step_avg:185.07ms\nstep:2679/4500 train_loss:8.1109 train_time:493956ms step_avg:185.07ms\nstep:2680/4500 train_loss:8.0045 train_time:494140ms step_avg:185.07ms\nstep:2681/4500 train_loss:8.0888 train_time:494327ms step_avg:185.07ms\nstep:2682/4500 train_loss:8.0173 train_time:494511ms step_avg:185.07ms\nstep:2683/4500 train_loss:8.0172 train_time:494696ms step_avg:185.07ms\nstep:2684/4500 train_loss:8.0571 train_time:494882ms step_avg:185.07ms\nstep:2685/4500 train_loss:8.1498 train_time:495068ms step_avg:185.07ms\nstep:2686/4500 train_loss:8.1161 train_time:495253ms step_avg:185.07ms\nstep:2687/4500 train_loss:8.2374 train_time:495439ms step_avg:185.07ms\nstep:2688/4500 train_loss:8.0522 train_time:495627ms step_avg:185.07ms\nstep:2689/4500 train_loss:8.0318 train_time:495814ms step_avg:185.07ms\nstep:2690/4500 train_loss:7.9833 train_time:495999ms step_avg:185.07ms\nstep:2691/4500 train_loss:8.0888 train_time:496184ms step_avg:185.07ms\nstep:2692/4500 train_loss:8.1282 train_time:496367ms step_avg:185.07ms\nstep:2693/4500 train_loss:8.1472 train_time:496553ms step_avg:185.07ms\nstep:2694/4500 train_loss:8.1600 train_time:496737ms step_avg:185.07ms\nstep:2695/4500 train_loss:8.1290 train_time:496920ms step_avg:185.07ms\nstep:2696/4500 train_loss:8.1984 train_time:497107ms step_avg:185.07ms\nstep:2697/4500 train_loss:8.0857 train_time:497289ms step_avg:185.07ms\nstep:2698/4500 train_loss:8.2971 train_time:497473ms step_avg:185.07ms\nstep:2699/4500 train_loss:8.0880 train_time:497658ms step_avg:185.07ms\nstep:2700/4500 train_loss:8.2089 train_time:497844ms step_avg:185.07ms\nstep:2701/4500 train_loss:8.1013 train_time:498029ms step_avg:185.07ms\nstep:2702/4500 train_loss:8.1559 train_time:498214ms step_avg:185.07ms\nstep:2703/4500 train_loss:8.0879 train_time:498400ms step_avg:185.07ms\nstep:2704/4500 train_loss:8.0756 train_time:498588ms step_avg:185.07ms\nstep:2705/4500 train_loss:8.1042 train_time:498774ms step_avg:185.07ms\nstep:2706/4500 train_loss:8.0026 train_time:498960ms step_avg:185.07ms\nstep:2707/4500 train_loss:8.0595 train_time:499145ms step_avg:185.07ms\nstep:2708/4500 train_loss:8.1085 train_time:499329ms step_avg:185.07ms\nstep:2709/4500 train_loss:8.0724 train_time:499513ms step_avg:185.07ms\nstep:2710/4500 train_loss:8.1244 train_time:499698ms step_avg:185.07ms\nstep:2711/4500 train_loss:8.0289 train_time:499885ms step_avg:185.07ms\nstep:2712/4500 train_loss:7.9891 train_time:500073ms step_avg:185.08ms\nstep:2713/4500 train_loss:8.1184 train_time:500259ms step_avg:185.08ms\nstep:2714/4500 train_loss:8.1854 train_time:500444ms step_avg:185.08ms\nstep:2715/4500 train_loss:8.0726 train_time:500627ms step_avg:185.07ms\nstep:2716/4500 train_loss:8.0254 train_time:500811ms step_avg:185.07ms\nstep:2717/4500 train_loss:8.0034 train_time:500994ms step_avg:185.07ms\nstep:2718/4500 train_loss:8.2798 train_time:501179ms step_avg:185.07ms\nstep:2719/4500 train_loss:8.1174 train_time:501368ms step_avg:185.07ms\nstep:2720/4500 train_loss:8.1518 train_time:501554ms step_avg:185.08ms\nstep:2721/4500 train_loss:8.1508 train_time:501739ms step_avg:185.08ms\nstep:2722/4500 train_loss:8.1249 train_time:501923ms step_avg:185.07ms\nstep:2723/4500 train_loss:8.0383 train_time:502106ms step_avg:185.07ms\nstep:2724/4500 train_loss:8.1037 train_time:502291ms step_avg:185.07ms\nstep:2725/4500 train_loss:8.1461 train_time:502475ms step_avg:185.07ms\nstep:2726/4500 train_loss:8.1185 train_time:502660ms step_avg:185.07ms\nstep:2727/4500 train_loss:8.2633 train_time:502844ms step_avg:185.07ms\nstep:2728/4500 train_loss:8.1279 train_time:503027ms step_avg:185.07ms\nstep:2729/4500 train_loss:8.1174 train_time:503212ms step_avg:185.07ms\nstep:2730/4500 train_loss:8.1310 train_time:503395ms step_avg:185.07ms\nstep:2731/4500 train_loss:8.0013 train_time:503581ms step_avg:185.07ms\nstep:2732/4500 train_loss:8.0540 train_time:503770ms step_avg:185.07ms\nstep:2733/4500 train_loss:8.1258 train_time:503955ms step_avg:185.07ms\nstep:2734/4500 train_loss:8.0956 train_time:504140ms step_avg:185.07ms\nstep:2735/4500 train_loss:8.0923 train_time:504329ms step_avg:185.07ms\nstep:2736/4500 train_loss:8.1637 train_time:504513ms step_avg:185.07ms\nstep:2737/4500 train_loss:8.2423 train_time:504697ms step_avg:185.07ms\nstep:2738/4500 train_loss:8.0592 train_time:504882ms step_avg:185.07ms\nstep:2739/4500 train_loss:7.9803 train_time:505067ms step_avg:185.07ms\nstep:2740/4500 train_loss:8.1185 train_time:505253ms step_avg:185.07ms\nstep:2741/4500 train_loss:8.3471 train_time:505438ms step_avg:185.07ms\nstep:2742/4500 train_loss:8.0823 train_time:505624ms step_avg:185.07ms\nstep:2743/4500 train_loss:8.1464 train_time:505812ms step_avg:185.08ms\nstep:2744/4500 train_loss:8.1450 train_time:505996ms step_avg:185.08ms\nstep:2745/4500 train_loss:8.0728 train_time:506180ms step_avg:185.08ms\nstep:2746/4500 train_loss:8.1558 train_time:506364ms step_avg:185.07ms\nstep:2747/4500 train_loss:8.1294 train_time:506552ms step_avg:185.08ms\nstep:2748/4500 train_loss:8.1871 train_time:506737ms step_avg:185.08ms\nstep:2749/4500 train_loss:8.0564 train_time:506923ms step_avg:185.08ms\nstep:2750/4500 train_loss:8.1193 train_time:507108ms step_avg:185.08ms\nstep:2751/4500 train_loss:8.1742 train_time:507293ms step_avg:185.08ms\nstep:2752/4500 train_loss:8.1278 train_time:507477ms step_avg:185.08ms\nstep:2753/4500 train_loss:8.1064 train_time:507662ms step_avg:185.08ms\nstep:2754/4500 train_loss:8.1228 train_time:507846ms step_avg:185.08ms\nstep:2755/4500 train_loss:8.1022 train_time:508030ms step_avg:185.07ms\nstep:2756/4500 train_loss:7.8160 train_time:508217ms step_avg:185.08ms\nstep:2757/4500 train_loss:8.0396 train_time:508401ms step_avg:185.08ms\nstep:2758/4500 train_loss:8.1195 train_time:508587ms step_avg:185.08ms\nstep:2759/4500 train_loss:8.2469 train_time:508771ms step_avg:185.07ms\nstep:2760/4500 train_loss:8.0551 train_time:508955ms step_avg:185.07ms\nstep:2761/4500 train_loss:8.0576 train_time:509140ms step_avg:185.07ms\nstep:2762/4500 train_loss:8.0150 train_time:509325ms step_avg:185.07ms\nstep:2763/4500 train_loss:8.0854 train_time:509512ms step_avg:185.08ms\nstep:2764/4500 train_loss:8.0488 train_time:509695ms step_avg:185.07ms\nstep:2765/4500 train_loss:8.1125 train_time:509880ms step_avg:185.07ms\nstep:2766/4500 train_loss:8.2223 train_time:510065ms step_avg:185.07ms\nstep:2767/4500 train_loss:8.0933 train_time:510248ms step_avg:185.07ms\nstep:2768/4500 train_loss:8.2113 train_time:510434ms step_avg:185.07ms\nstep:2769/4500 train_loss:8.1295 train_time:510619ms step_avg:185.07ms\nstep:2770/4500 train_loss:8.1098 train_time:510805ms step_avg:185.07ms\nstep:2771/4500 train_loss:8.0582 train_time:510988ms step_avg:185.07ms\nstep:2772/4500 train_loss:8.1087 train_time:511173ms step_avg:185.07ms\nstep:2773/4500 train_loss:8.1353 train_time:511356ms step_avg:185.07ms\nstep:2774/4500 train_loss:8.1343 train_time:511541ms step_avg:185.07ms\nstep:2775/4500 train_loss:8.1035 train_time:511724ms step_avg:185.07ms\nstep:2776/4500 train_loss:8.1271 train_time:511909ms step_avg:185.07ms\nstep:2777/4500 train_loss:7.8381 train_time:512092ms step_avg:185.07ms\nstep:2778/4500 train_loss:8.0862 train_time:512278ms step_avg:185.07ms\nstep:2779/4500 train_loss:8.1368 train_time:512463ms step_avg:185.07ms\nstep:2780/4500 train_loss:8.1432 train_time:512647ms step_avg:185.07ms\nstep:2781/4500 train_loss:7.9914 train_time:512832ms step_avg:185.07ms\nstep:2782/4500 train_loss:8.1188 train_time:513017ms step_avg:185.07ms\nstep:2783/4500 train_loss:8.1205 train_time:513201ms step_avg:185.07ms\nstep:2784/4500 train_loss:8.0929 train_time:513386ms step_avg:185.07ms\nstep:2785/4500 train_loss:8.1051 train_time:513570ms step_avg:185.07ms\nstep:2786/4500 train_loss:7.9851 train_time:513753ms step_avg:185.07ms\nstep:2787/4500 train_loss:8.1280 train_time:513937ms step_avg:185.07ms\nstep:2788/4500 train_loss:8.1363 train_time:514123ms step_avg:185.07ms\nstep:2789/4500 train_loss:8.1134 train_time:514307ms step_avg:185.07ms\nstep:2790/4500 train_loss:8.3083 train_time:514490ms step_avg:185.07ms\nstep:2791/4500 train_loss:8.0532 train_time:514674ms step_avg:185.07ms\nstep:2792/4500 train_loss:8.2096 train_time:514860ms step_avg:185.07ms\nstep:2793/4500 train_loss:8.0511 train_time:515044ms step_avg:185.07ms\nstep:2794/4500 train_loss:8.1364 train_time:515229ms step_avg:185.07ms\nstep:2795/4500 train_loss:8.1460 train_time:515413ms step_avg:185.07ms\nstep:2796/4500 train_loss:8.1137 train_time:515597ms step_avg:185.07ms\nstep:2797/4500 train_loss:8.0448 train_time:515783ms step_avg:185.07ms\nstep:2798/4500 train_loss:8.1422 train_time:515967ms step_avg:185.07ms\nstep:2799/4500 train_loss:8.0301 train_time:516150ms step_avg:185.07ms\nstep:2800/4500 train_loss:8.2738 train_time:516333ms step_avg:185.07ms\nstep:2801/4500 train_loss:8.1424 train_time:516519ms step_avg:185.07ms\nstep:2802/4500 train_loss:8.0446 train_time:516703ms step_avg:185.07ms\nstep:2803/4500 train_loss:8.2320 train_time:516889ms step_avg:185.07ms\nstep:2804/4500 train_loss:8.1267 train_time:517073ms step_avg:185.07ms\nstep:2805/4500 train_loss:8.0833 train_time:517258ms step_avg:185.07ms\nstep:2806/4500 train_loss:8.0063 train_time:517444ms step_avg:185.07ms\nstep:2807/4500 train_loss:8.0405 train_time:517632ms step_avg:185.07ms\nstep:2808/4500 train_loss:7.9890 train_time:517817ms step_avg:185.07ms\nstep:2809/4500 train_loss:8.2276 train_time:518001ms step_avg:185.07ms\nstep:2810/4500 train_loss:8.4128 train_time:518187ms step_avg:185.07ms\nstep:2811/4500 train_loss:8.1027 train_time:518372ms step_avg:185.07ms\nstep:2812/4500 train_loss:8.0574 train_time:518556ms step_avg:185.07ms\nstep:2813/4500 train_loss:8.2516 train_time:518742ms step_avg:185.07ms\nstep:2814/4500 train_loss:8.1014 train_time:518927ms step_avg:185.07ms\nstep:2815/4500 train_loss:8.0961 train_time:519110ms step_avg:185.07ms\nstep:2816/4500 train_loss:7.9316 train_time:519295ms step_avg:185.07ms\nstep:2817/4500 train_loss:8.1081 train_time:519481ms step_avg:185.07ms\nstep:2818/4500 train_loss:8.1046 train_time:519665ms step_avg:185.07ms\nstep:2819/4500 train_loss:8.0623 train_time:519853ms step_avg:185.07ms\nstep:2820/4500 train_loss:8.0588 train_time:520037ms step_avg:185.07ms\nstep:2821/4500 train_loss:8.1710 train_time:520221ms step_avg:185.07ms\nstep:2822/4500 train_loss:7.9603 train_time:520408ms step_avg:185.07ms\nstep:2823/4500 train_loss:8.0909 train_time:520595ms step_avg:185.07ms\nstep:2824/4500 train_loss:8.1776 train_time:520780ms step_avg:185.07ms\nstep:2825/4500 train_loss:8.0983 train_time:520967ms step_avg:185.07ms\nstep:2826/4500 train_loss:7.9948 train_time:521152ms step_avg:185.07ms\nstep:2827/4500 train_loss:8.1005 train_time:521337ms step_avg:185.07ms\nstep:2828/4500 train_loss:8.0781 train_time:521520ms step_avg:185.07ms\nstep:2829/4500 train_loss:8.0783 train_time:521707ms step_avg:185.07ms\nstep:2830/4500 train_loss:8.2139 train_time:521889ms step_avg:185.07ms\nstep:2831/4500 train_loss:8.1370 train_time:522074ms step_avg:185.07ms\nstep:2832/4500 train_loss:8.0749 train_time:522258ms step_avg:185.07ms\nstep:2833/4500 train_loss:8.0945 train_time:522441ms step_avg:185.07ms\nstep:2834/4500 train_loss:8.0882 train_time:522628ms step_avg:185.07ms\nstep:2835/4500 train_loss:8.6216 train_time:522813ms step_avg:185.07ms\nstep:2836/4500 train_loss:8.0749 train_time:522996ms step_avg:185.07ms\nstep:2837/4500 train_loss:8.0587 train_time:523180ms step_avg:185.07ms\nstep:2838/4500 train_loss:8.1458 train_time:523365ms step_avg:185.07ms\nstep:2839/4500 train_loss:8.0968 train_time:523551ms step_avg:185.07ms\nstep:2840/4500 train_loss:8.0170 train_time:523734ms step_avg:185.06ms\nstep:2841/4500 train_loss:8.0623 train_time:523919ms step_avg:185.06ms\nstep:2842/4500 train_loss:8.0534 train_time:524104ms step_avg:185.07ms\nstep:2843/4500 train_loss:8.0880 train_time:524289ms step_avg:185.06ms\nstep:2844/4500 train_loss:8.1733 train_time:524472ms step_avg:185.06ms\nstep:2845/4500 train_loss:8.1669 train_time:524657ms step_avg:185.06ms\nstep:2846/4500 train_loss:8.2338 train_time:524843ms step_avg:185.06ms\nstep:2847/4500 train_loss:8.1414 train_time:525027ms step_avg:185.06ms\nstep:2848/4500 train_loss:8.1506 train_time:525210ms step_avg:185.06ms\nstep:2849/4500 train_loss:7.8813 train_time:525392ms step_avg:185.06ms\nstep:2850/4500 train_loss:8.0396 train_time:525575ms step_avg:185.06ms\nstep:2851/4500 train_loss:8.1548 train_time:525758ms step_avg:185.06ms\nstep:2852/4500 train_loss:8.1058 train_time:525944ms step_avg:185.06ms\nstep:2853/4500 train_loss:7.9816 train_time:526130ms step_avg:185.06ms\nstep:2854/4500 train_loss:8.1901 train_time:526315ms step_avg:185.06ms\nstep:2855/4500 train_loss:8.0169 train_time:526500ms step_avg:185.06ms\nstep:2856/4500 train_loss:8.1946 train_time:526688ms step_avg:185.06ms\nstep:2857/4500 train_loss:8.0776 train_time:526873ms step_avg:185.06ms\nstep:2858/4500 train_loss:8.1754 train_time:527056ms step_avg:185.06ms\nstep:2859/4500 train_loss:8.1897 train_time:527239ms step_avg:185.06ms\nstep:2860/4500 train_loss:8.1734 train_time:527424ms step_avg:185.06ms\nstep:2861/4500 train_loss:8.1149 train_time:527611ms step_avg:185.06ms\nstep:2862/4500 train_loss:8.0459 train_time:527794ms step_avg:185.06ms\nstep:2863/4500 train_loss:8.0572 train_time:527978ms step_avg:185.06ms\nstep:2864/4500 train_loss:8.2055 train_time:528164ms step_avg:185.06ms\nstep:2865/4500 train_loss:8.1363 train_time:528348ms step_avg:185.06ms\nstep:2866/4500 train_loss:8.1585 train_time:528533ms step_avg:185.06ms\nstep:2867/4500 train_loss:8.1019 train_time:528718ms step_avg:185.06ms\nstep:2868/4500 train_loss:8.1852 train_time:528908ms step_avg:185.06ms\nstep:2869/4500 train_loss:7.9126 train_time:529092ms step_avg:185.06ms\nstep:2870/4500 train_loss:8.0992 train_time:529276ms step_avg:185.06ms\nstep:2871/4500 train_loss:7.9881 train_time:529459ms step_avg:185.06ms\nstep:2872/4500 train_loss:8.2526 train_time:529642ms step_avg:185.06ms\nstep:2873/4500 train_loss:8.1122 train_time:529825ms step_avg:185.06ms\nstep:2874/4500 train_loss:8.1915 train_time:530011ms step_avg:185.06ms\nstep:2875/4500 train_loss:8.1251 train_time:530195ms step_avg:185.06ms\nstep:2876/4500 train_loss:8.0985 train_time:530381ms step_avg:185.06ms\nstep:2877/4500 train_loss:8.1457 train_time:530569ms step_avg:185.06ms\nstep:2878/4500 train_loss:8.1183 train_time:530755ms step_avg:185.06ms\nstep:2879/4500 train_loss:8.2034 train_time:530940ms step_avg:185.06ms\nstep:2880/4500 train_loss:8.1047 train_time:531124ms step_avg:185.06ms\nstep:2881/4500 train_loss:8.1404 train_time:531307ms step_avg:185.06ms\nstep:2882/4500 train_loss:8.1078 train_time:531492ms step_avg:185.06ms\nstep:2883/4500 train_loss:8.0044 train_time:531678ms step_avg:185.06ms\nstep:2884/4500 train_loss:8.0480 train_time:531865ms step_avg:185.06ms\nstep:2885/4500 train_loss:8.1147 train_time:532053ms step_avg:185.06ms\nstep:2886/4500 train_loss:8.1307 train_time:532237ms step_avg:185.06ms\nstep:2887/4500 train_loss:7.9933 train_time:532422ms step_avg:185.06ms\nstep:2888/4500 train_loss:8.0253 train_time:532607ms step_avg:185.06ms\nstep:2889/4500 train_loss:8.1670 train_time:532791ms step_avg:185.06ms\nstep:2890/4500 train_loss:8.0060 train_time:532974ms step_avg:185.06ms\nstep:2891/4500 train_loss:8.2300 train_time:533157ms step_avg:185.06ms\nstep:2892/4500 train_loss:8.1556 train_time:533341ms step_avg:185.06ms\nstep:2893/4500 train_loss:8.1842 train_time:533525ms step_avg:185.06ms\nstep:2894/4500 train_loss:8.1097 train_time:533707ms step_avg:185.06ms\nstep:2895/4500 train_loss:8.2217 train_time:533890ms step_avg:185.06ms\nstep:2896/4500 train_loss:8.1461 train_time:534074ms step_avg:185.06ms\nstep:2897/4500 train_loss:8.0804 train_time:534258ms step_avg:185.06ms\nstep:2898/4500 train_loss:8.1089 train_time:534443ms step_avg:185.06ms\nstep:2899/4500 train_loss:8.0766 train_time:534631ms step_avg:185.06ms\nstep:2900/4500 train_loss:8.1295 train_time:534818ms step_avg:185.06ms\nstep:2901/4500 train_loss:8.1013 train_time:535002ms step_avg:185.06ms\nstep:2902/4500 train_loss:8.0958 train_time:535189ms step_avg:185.06ms\nstep:2903/4500 train_loss:8.1058 train_time:535373ms step_avg:185.06ms\nstep:2904/4500 train_loss:8.1344 train_time:535556ms step_avg:185.06ms\nstep:2905/4500 train_loss:8.0504 train_time:535739ms step_avg:185.06ms\nstep:2906/4500 train_loss:8.0798 train_time:535927ms step_avg:185.06ms\nstep:2907/4500 train_loss:8.0665 train_time:536110ms step_avg:185.06ms\nstep:2908/4500 train_loss:8.1208 train_time:536294ms step_avg:185.06ms\nstep:2909/4500 train_loss:8.2793 train_time:536481ms step_avg:185.06ms\nstep:2910/4500 train_loss:8.0461 train_time:536669ms step_avg:185.06ms\nstep:2911/4500 train_loss:8.1718 train_time:536854ms step_avg:185.06ms\nstep:2912/4500 train_loss:7.9426 train_time:537037ms step_avg:185.06ms\nstep:2913/4500 train_loss:8.1255 train_time:537223ms step_avg:185.06ms\nstep:2914/4500 train_loss:8.1549 train_time:537408ms step_avg:185.06ms\nstep:2915/4500 train_loss:8.1176 train_time:537592ms step_avg:185.06ms\nstep:2916/4500 train_loss:8.1407 train_time:537778ms step_avg:185.06ms\nstep:2917/4500 train_loss:8.0294 train_time:537963ms step_avg:185.06ms\nstep:2918/4500 train_loss:7.9851 train_time:538149ms step_avg:185.06ms\nstep:2919/4500 train_loss:7.9379 train_time:538332ms step_avg:185.06ms\nstep:2920/4500 train_loss:8.2541 train_time:538517ms step_avg:185.06ms\nstep:2921/4500 train_loss:8.1778 train_time:538702ms step_avg:185.06ms\nstep:2922/4500 train_loss:7.9750 train_time:538890ms step_avg:185.06ms\nstep:2923/4500 train_loss:8.1794 train_time:539073ms step_avg:185.06ms\nstep:2924/4500 train_loss:8.1708 train_time:539257ms step_avg:185.06ms\nstep:2925/4500 train_loss:8.0902 train_time:539443ms step_avg:185.06ms\nstep:2926/4500 train_loss:8.0524 train_time:539628ms step_avg:185.06ms\nstep:2927/4500 train_loss:8.1658 train_time:539813ms step_avg:185.06ms\nstep:2928/4500 train_loss:7.9904 train_time:539998ms step_avg:185.06ms\nstep:2929/4500 train_loss:8.0718 train_time:540188ms step_avg:185.06ms\nstep:2930/4500 train_loss:8.0506 train_time:540375ms step_avg:185.06ms\nstep:2931/4500 train_loss:8.1728 train_time:540560ms step_avg:185.06ms\nstep:2932/4500 train_loss:8.1693 train_time:540744ms step_avg:185.06ms\nstep:2933/4500 train_loss:8.0439 train_time:540932ms step_avg:185.06ms\nstep:2934/4500 train_loss:8.1294 train_time:541116ms step_avg:185.06ms\nstep:2935/4500 train_loss:8.0673 train_time:541302ms step_avg:185.06ms\nstep:2936/4500 train_loss:8.2933 train_time:541491ms step_avg:185.06ms\nstep:2937/4500 train_loss:8.0012 train_time:541675ms step_avg:185.06ms\nstep:2938/4500 train_loss:8.1215 train_time:541859ms step_avg:185.06ms\nstep:2939/4500 train_loss:8.1384 train_time:542044ms step_avg:185.06ms\nstep:2940/4500 train_loss:8.0291 train_time:542230ms step_avg:185.06ms\nstep:2941/4500 train_loss:8.1181 train_time:542415ms step_avg:185.06ms\nstep:2942/4500 train_loss:8.3228 train_time:542601ms step_avg:185.06ms\nstep:2943/4500 train_loss:8.0475 train_time:542787ms step_avg:185.06ms\nstep:2944/4500 train_loss:8.1528 train_time:542971ms step_avg:185.06ms\nstep:2945/4500 train_loss:8.1977 train_time:543155ms step_avg:185.06ms\nstep:2946/4500 train_loss:8.1487 train_time:543341ms step_avg:185.06ms\nstep:2947/4500 train_loss:8.0476 train_time:543527ms step_avg:185.06ms\nstep:2948/4500 train_loss:8.1801 train_time:543711ms step_avg:185.06ms\nstep:2949/4500 train_loss:7.9955 train_time:543895ms step_avg:185.06ms\nstep:2950/4500 train_loss:8.0938 train_time:544081ms step_avg:185.06ms\nstep:2951/4500 train_loss:8.2034 train_time:544266ms step_avg:185.06ms\nstep:2952/4500 train_loss:8.0775 train_time:544450ms step_avg:185.06ms\nstep:2953/4500 train_loss:8.0702 train_time:544636ms step_avg:185.06ms\nstep:2954/4500 train_loss:8.0561 train_time:544821ms step_avg:185.06ms\nstep:2955/4500 train_loss:8.1369 train_time:545006ms step_avg:185.06ms\nstep:2956/4500 train_loss:8.0972 train_time:545190ms step_avg:185.06ms\nstep:2957/4500 train_loss:8.0744 train_time:545374ms step_avg:185.06ms\nstep:2958/4500 train_loss:8.1731 train_time:545559ms step_avg:185.06ms\nstep:2959/4500 train_loss:8.1927 train_time:545745ms step_avg:185.06ms\nstep:2960/4500 train_loss:8.0191 train_time:545929ms step_avg:185.06ms\nstep:2961/4500 train_loss:8.1263 train_time:546113ms step_avg:185.06ms\nstep:2962/4500 train_loss:8.1349 train_time:546298ms step_avg:185.06ms\nstep:2963/4500 train_loss:8.0969 train_time:546485ms step_avg:185.06ms\nstep:2964/4500 train_loss:7.9820 train_time:546670ms step_avg:185.06ms\nstep:2965/4500 train_loss:8.0794 train_time:546856ms step_avg:185.06ms\nstep:2966/4500 train_loss:8.1524 train_time:547041ms step_avg:185.06ms\nstep:2967/4500 train_loss:8.1073 train_time:547229ms step_avg:185.06ms\nstep:2968/4500 train_loss:8.1280 train_time:547415ms step_avg:185.06ms\nstep:2969/4500 train_loss:8.1398 train_time:547599ms step_avg:185.06ms\nstep:2970/4500 train_loss:8.1700 train_time:547785ms step_avg:185.06ms\nstep:2971/4500 train_loss:8.1176 train_time:547972ms step_avg:185.06ms\nstep:2972/4500 train_loss:8.2694 train_time:548155ms step_avg:185.06ms\nstep:2973/4500 train_loss:8.3236 train_time:548340ms step_avg:185.06ms\nstep:2974/4500 train_loss:8.1369 train_time:548523ms step_avg:185.06ms\nstep:2975/4500 train_loss:8.1289 train_time:548709ms step_avg:185.06ms\nstep:2976/4500 train_loss:8.1649 train_time:548896ms step_avg:185.06ms\nstep:2977/4500 train_loss:8.1382 train_time:549083ms step_avg:185.06ms\nstep:2978/4500 train_loss:8.1806 train_time:549267ms step_avg:185.06ms\nstep:2979/4500 train_loss:8.1450 train_time:549451ms step_avg:185.06ms\nstep:2980/4500 train_loss:8.1175 train_time:549635ms step_avg:185.06ms\nstep:2981/4500 train_loss:8.0928 train_time:549820ms step_avg:185.06ms\nstep:2982/4500 train_loss:8.0856 train_time:550008ms step_avg:185.06ms\nstep:2983/4500 train_loss:8.0120 train_time:550190ms step_avg:185.06ms\nstep:2984/4500 train_loss:8.1371 train_time:550374ms step_avg:185.06ms\nstep:2985/4500 train_loss:8.0714 train_time:550558ms step_avg:185.06ms\nstep:2986/4500 train_loss:8.1060 train_time:550744ms step_avg:185.06ms\nstep:2987/4500 train_loss:8.0270 train_time:550930ms step_avg:185.06ms\nstep:2988/4500 train_loss:8.1111 train_time:551116ms step_avg:185.06ms\nstep:2989/4500 train_loss:8.1678 train_time:551302ms step_avg:185.06ms\nstep:2990/4500 train_loss:8.1600 train_time:551490ms step_avg:185.06ms\nstep:2991/4500 train_loss:8.0284 train_time:551675ms step_avg:185.06ms\nstep:2992/4500 train_loss:8.1236 train_time:551860ms step_avg:185.06ms\nstep:2993/4500 train_loss:8.0527 train_time:552046ms step_avg:185.06ms\nstep:2994/4500 train_loss:8.0868 train_time:552229ms step_avg:185.06ms\nstep:2995/4500 train_loss:8.1826 train_time:552413ms step_avg:185.06ms\nstep:2996/4500 train_loss:8.0192 train_time:552596ms step_avg:185.06ms\nstep:2997/4500 train_loss:8.1237 train_time:552780ms step_avg:185.06ms\nstep:2998/4500 train_loss:8.1679 train_time:552964ms step_avg:185.06ms\nstep:2999/4500 train_loss:8.1438 train_time:553150ms step_avg:185.06ms\nstep:3000/4500 train_loss:8.1039 train_time:553338ms step_avg:185.06ms\nstep:3001/4500 train_loss:8.1113 train_time:553522ms step_avg:185.06ms\nstep:3002/4500 train_loss:7.9947 train_time:553707ms step_avg:185.06ms\nstep:3003/4500 train_loss:8.0743 train_time:553891ms step_avg:185.06ms\nstep:3004/4500 train_loss:8.0921 train_time:554073ms step_avg:185.06ms\nstep:3005/4500 train_loss:8.1508 train_time:554257ms step_avg:185.06ms\nstep:3006/4500 train_loss:8.1654 train_time:554441ms step_avg:185.06ms\nstep:3007/4500 train_loss:7.9335 train_time:554626ms step_avg:185.06ms\nstep:3008/4500 train_loss:8.0885 train_time:554810ms step_avg:185.06ms\nstep:3009/4500 train_loss:8.1606 train_time:554997ms step_avg:185.06ms\nstep:3010/4500 train_loss:7.9553 train_time:555183ms step_avg:185.06ms\nstep:3011/4500 train_loss:8.0415 train_time:555370ms step_avg:185.06ms\nstep:3012/4500 train_loss:8.1079 train_time:555554ms step_avg:185.06ms\nstep:3013/4500 train_loss:8.1477 train_time:555741ms step_avg:185.06ms\nstep:3014/4500 train_loss:8.1346 train_time:555929ms step_avg:185.06ms\nstep:3015/4500 train_loss:8.0036 train_time:556115ms step_avg:185.06ms\nstep:3016/4500 train_loss:8.0788 train_time:556302ms step_avg:185.06ms\nstep:3017/4500 train_loss:8.0637 train_time:556486ms step_avg:185.06ms\nstep:3018/4500 train_loss:8.0289 train_time:556672ms step_avg:185.06ms\nstep:3019/4500 train_loss:8.1153 train_time:556856ms step_avg:185.06ms\nstep:3020/4500 train_loss:8.0582 train_time:557040ms step_avg:185.06ms\nstep:3021/4500 train_loss:8.1530 train_time:557226ms step_avg:185.06ms\nstep:3022/4500 train_loss:8.0359 train_time:557409ms step_avg:185.06ms\nstep:3023/4500 train_loss:7.9959 train_time:557595ms step_avg:185.06ms\nstep:3024/4500 train_loss:8.1515 train_time:557781ms step_avg:185.06ms\nstep:3025/4500 train_loss:8.1601 train_time:557967ms step_avg:185.06ms\nstep:3026/4500 train_loss:8.0933 train_time:558150ms step_avg:185.06ms\nstep:3027/4500 train_loss:8.1706 train_time:558336ms step_avg:185.06ms\nstep:3028/4500 train_loss:8.1365 train_time:558520ms step_avg:185.06ms\nstep:3029/4500 train_loss:8.1235 train_time:558706ms step_avg:185.06ms\nstep:3030/4500 train_loss:8.2514 train_time:558893ms step_avg:185.06ms\nstep:3031/4500 train_loss:8.2156 train_time:559079ms step_avg:185.06ms\nstep:3032/4500 train_loss:8.1227 train_time:559265ms step_avg:185.06ms\nstep:3033/4500 train_loss:8.1244 train_time:559452ms step_avg:185.07ms\nstep:3034/4500 train_loss:8.1821 train_time:559636ms step_avg:185.06ms\nstep:3035/4500 train_loss:8.0623 train_time:559821ms step_avg:185.06ms\nstep:3036/4500 train_loss:8.1009 train_time:560007ms step_avg:185.07ms\nstep:3037/4500 train_loss:8.1327 train_time:560190ms step_avg:185.06ms\nstep:3038/4500 train_loss:8.1324 train_time:560374ms step_avg:185.06ms\nstep:3039/4500 train_loss:8.0421 train_time:560559ms step_avg:185.06ms\nstep:3040/4500 train_loss:7.9195 train_time:560745ms step_avg:185.06ms\nstep:3041/4500 train_loss:8.1024 train_time:560933ms step_avg:185.07ms\nstep:3042/4500 train_loss:8.1024 train_time:561119ms step_avg:185.07ms\nstep:3043/4500 train_loss:8.1266 train_time:561304ms step_avg:185.07ms\nstep:3044/4500 train_loss:8.0716 train_time:561491ms step_avg:185.07ms\nstep:3045/4500 train_loss:8.1438 train_time:561675ms step_avg:185.07ms\nstep:3046/4500 train_loss:8.3157 train_time:561860ms step_avg:185.07ms\nstep:3047/4500 train_loss:8.1463 train_time:562045ms step_avg:185.07ms\nstep:3048/4500 train_loss:8.1201 train_time:562456ms step_avg:185.14ms\nstep:3049/4500 train_loss:8.1373 train_time:562640ms step_avg:185.14ms\nstep:3050/4500 train_loss:8.0379 train_time:562823ms step_avg:185.14ms\nstep:3051/4500 train_loss:8.0350 train_time:563005ms step_avg:185.14ms\nstep:3052/4500 train_loss:8.2389 train_time:563190ms step_avg:185.14ms\nstep:3053/4500 train_loss:8.0210 train_time:563373ms step_avg:185.14ms\nstep:3054/4500 train_loss:8.2306 train_time:563558ms step_avg:185.14ms\nstep:3055/4500 train_loss:8.2494 train_time:563743ms step_avg:185.14ms\nstep:3056/4500 train_loss:7.9195 train_time:563932ms step_avg:185.14ms\nstep:3057/4500 train_loss:8.1133 train_time:564116ms step_avg:185.14ms\nstep:3058/4500 train_loss:8.1365 train_time:564304ms step_avg:185.14ms\nstep:3059/4500 train_loss:8.1657 train_time:564492ms step_avg:185.14ms\nstep:3060/4500 train_loss:8.2129 train_time:564677ms step_avg:185.14ms\nstep:3061/4500 train_loss:8.1121 train_time:564862ms step_avg:185.14ms\nstep:3062/4500 train_loss:8.1564 train_time:565046ms step_avg:185.14ms\nstep:3063/4500 train_loss:8.1161 train_time:565230ms step_avg:185.14ms\nstep:3064/4500 train_loss:8.1308 train_time:565413ms step_avg:185.14ms\nstep:3065/4500 train_loss:8.0914 train_time:565597ms step_avg:185.14ms\nstep:3066/4500 train_loss:8.1150 train_time:565785ms step_avg:185.14ms\nstep:3067/4500 train_loss:8.2271 train_time:565969ms step_avg:185.14ms\nstep:3068/4500 train_loss:8.2512 train_time:566151ms step_avg:185.14ms\nstep:3069/4500 train_loss:8.1046 train_time:566335ms step_avg:185.14ms\nstep:3070/4500 train_loss:8.2179 train_time:566520ms step_avg:185.14ms\nstep:3071/4500 train_loss:8.0361 train_time:566704ms step_avg:185.14ms\nstep:3072/4500 train_loss:8.0362 train_time:566890ms step_avg:185.14ms\nstep:3073/4500 train_loss:8.1195 train_time:567074ms step_avg:185.14ms\nstep:3074/4500 train_loss:8.1339 train_time:567259ms step_avg:185.14ms\nstep:3075/4500 train_loss:8.1933 train_time:567447ms step_avg:185.14ms\nstep:3076/4500 train_loss:8.2196 train_time:567634ms step_avg:185.14ms\nstep:3077/4500 train_loss:8.1085 train_time:567820ms step_avg:185.14ms\nstep:3078/4500 train_loss:8.0848 train_time:568004ms step_avg:185.14ms\nstep:3079/4500 train_loss:8.0885 train_time:568190ms step_avg:185.14ms\nstep:3080/4500 train_loss:8.0849 train_time:568373ms step_avg:185.14ms\nstep:3081/4500 train_loss:8.0953 train_time:568557ms step_avg:185.14ms\nstep:3082/4500 train_loss:8.1924 train_time:568740ms step_avg:185.14ms\nstep:3083/4500 train_loss:8.1344 train_time:568928ms step_avg:185.14ms\nstep:3084/4500 train_loss:8.3288 train_time:569113ms step_avg:185.14ms\nstep:3085/4500 train_loss:7.8448 train_time:569295ms step_avg:185.14ms\nstep:3086/4500 train_loss:8.1832 train_time:569482ms step_avg:185.14ms\nstep:3087/4500 train_loss:8.0750 train_time:569667ms step_avg:185.14ms\nstep:3088/4500 train_loss:8.2155 train_time:569851ms step_avg:185.14ms\nstep:3089/4500 train_loss:8.1166 train_time:570036ms step_avg:185.14ms\nstep:3090/4500 train_loss:8.2311 train_time:570221ms step_avg:185.14ms\nstep:3091/4500 train_loss:8.0919 train_time:570408ms step_avg:185.14ms\nstep:3092/4500 train_loss:8.1027 train_time:570595ms step_avg:185.14ms\nstep:3093/4500 train_loss:8.1563 train_time:570781ms step_avg:185.14ms\nstep:3094/4500 train_loss:8.0576 train_time:570967ms step_avg:185.14ms\nstep:3095/4500 train_loss:8.0373 train_time:571150ms step_avg:185.14ms\nstep:3096/4500 train_loss:8.1761 train_time:571334ms step_avg:185.14ms\nstep:3097/4500 train_loss:8.0539 train_time:571520ms step_avg:185.14ms\nstep:3098/4500 train_loss:8.2372 train_time:571705ms step_avg:185.14ms\nstep:3099/4500 train_loss:8.1057 train_time:571890ms step_avg:185.14ms\nstep:3100/4500 train_loss:8.1852 train_time:572076ms step_avg:185.14ms\nstep:3101/4500 train_loss:8.1033 train_time:572261ms step_avg:185.14ms\nstep:3102/4500 train_loss:8.0793 train_time:572445ms step_avg:185.14ms\nstep:3103/4500 train_loss:8.2843 train_time:572629ms step_avg:185.14ms\nstep:3104/4500 train_loss:8.0701 train_time:572814ms step_avg:185.14ms\nstep:3105/4500 train_loss:8.1471 train_time:573000ms step_avg:185.14ms\nstep:3106/4500 train_loss:8.1025 train_time:573185ms step_avg:185.14ms\nstep:3107/4500 train_loss:8.0701 train_time:573368ms step_avg:185.14ms\nstep:3108/4500 train_loss:8.0334 train_time:573552ms step_avg:185.14ms\nstep:3109/4500 train_loss:8.1018 train_time:573737ms step_avg:185.14ms\nstep:3110/4500 train_loss:8.2359 train_time:573922ms step_avg:185.14ms\nstep:3111/4500 train_loss:8.1800 train_time:574107ms step_avg:185.14ms\nstep:3112/4500 train_loss:8.1058 train_time:574291ms step_avg:185.14ms\nstep:3113/4500 train_loss:8.1538 train_time:574475ms step_avg:185.14ms\nstep:3114/4500 train_loss:8.0566 train_time:574660ms step_avg:185.14ms\nstep:3115/4500 train_loss:8.1295 train_time:574844ms step_avg:185.13ms\nstep:3116/4500 train_loss:8.1272 train_time:575028ms step_avg:185.13ms\nstep:3117/4500 train_loss:8.0742 train_time:575212ms step_avg:185.13ms\nstep:3118/4500 train_loss:8.0471 train_time:575397ms step_avg:185.13ms\nstep:3119/4500 train_loss:8.1008 train_time:575585ms step_avg:185.13ms\nstep:3120/4500 train_loss:8.0778 train_time:575769ms step_avg:185.13ms\nstep:3121/4500 train_loss:8.1336 train_time:575953ms step_avg:185.13ms\nstep:3122/4500 train_loss:8.0083 train_time:576137ms step_avg:185.13ms\nstep:3123/4500 train_loss:8.1851 train_time:576323ms step_avg:185.13ms\nstep:3124/4500 train_loss:7.8816 train_time:576511ms step_avg:185.14ms\nstep:3125/4500 train_loss:8.0782 train_time:576694ms step_avg:185.13ms\nstep:3126/4500 train_loss:8.1426 train_time:576878ms step_avg:185.13ms\nstep:3127/4500 train_loss:8.1719 train_time:577064ms step_avg:185.13ms\nstep:3128/4500 train_loss:7.9831 train_time:577253ms step_avg:185.14ms\nstep:3129/4500 train_loss:8.0900 train_time:577438ms step_avg:185.14ms\nstep:3130/4500 train_loss:8.1497 train_time:577626ms step_avg:185.14ms\nstep:3131/4500 train_loss:8.0765 train_time:577808ms step_avg:185.14ms\nstep:3132/4500 train_loss:8.1670 train_time:577992ms step_avg:185.14ms\nstep:3133/4500 train_loss:8.1749 train_time:578177ms step_avg:185.13ms\nstep:3134/4500 train_loss:7.9944 train_time:578361ms step_avg:185.13ms\nstep:3135/4500 train_loss:8.0581 train_time:578546ms step_avg:185.13ms\nstep:3136/4500 train_loss:8.1294 train_time:578732ms step_avg:185.14ms\nstep:3137/4500 train_loss:8.0495 train_time:578917ms step_avg:185.13ms\nstep:3138/4500 train_loss:8.2013 train_time:579101ms step_avg:185.13ms\nstep:3139/4500 train_loss:8.0517 train_time:579288ms step_avg:185.14ms\nstep:3140/4500 train_loss:8.0067 train_time:579472ms step_avg:185.13ms\nstep:3141/4500 train_loss:8.1021 train_time:579656ms step_avg:185.13ms\nstep:3142/4500 train_loss:7.9696 train_time:579840ms step_avg:185.13ms\nstep:3143/4500 train_loss:8.0755 train_time:580024ms step_avg:185.13ms\nstep:3144/4500 train_loss:8.2852 train_time:580210ms step_avg:185.13ms\nstep:3145/4500 train_loss:8.0777 train_time:580393ms step_avg:185.13ms\nstep:3146/4500 train_loss:8.0827 train_time:580577ms step_avg:185.13ms\nstep:3147/4500 train_loss:8.1394 train_time:580761ms step_avg:185.13ms\nstep:3148/4500 train_loss:8.1091 train_time:580949ms step_avg:185.13ms\nstep:3149/4500 train_loss:7.8691 train_time:581133ms step_avg:185.13ms\nstep:3150/4500 train_loss:8.0335 train_time:581318ms step_avg:185.13ms\nstep:3151/4500 train_loss:8.0951 train_time:581503ms step_avg:185.13ms\nstep:3152/4500 train_loss:8.0793 train_time:581686ms step_avg:185.13ms\nstep:3153/4500 train_loss:8.2091 train_time:581868ms step_avg:185.13ms\nstep:3154/4500 train_loss:8.2629 train_time:582054ms step_avg:185.13ms\nstep:3155/4500 train_loss:8.1536 train_time:582240ms step_avg:185.13ms\nstep:3156/4500 train_loss:8.2907 train_time:582425ms step_avg:185.13ms\nstep:3157/4500 train_loss:8.1194 train_time:582611ms step_avg:185.13ms\nstep:3158/4500 train_loss:8.0118 train_time:582794ms step_avg:185.13ms\nstep:3159/4500 train_loss:7.9581 train_time:582978ms step_avg:185.13ms\nstep:3160/4500 train_loss:8.1096 train_time:583164ms step_avg:185.13ms\nstep:3161/4500 train_loss:8.1219 train_time:583350ms step_avg:185.13ms\nstep:3162/4500 train_loss:8.1356 train_time:583534ms step_avg:185.13ms\nstep:3163/4500 train_loss:8.1037 train_time:583720ms step_avg:185.13ms\nstep:3164/4500 train_loss:8.1801 train_time:583904ms step_avg:185.13ms\nstep:3165/4500 train_loss:8.4969 train_time:584087ms step_avg:185.13ms\nstep:3166/4500 train_loss:8.1283 train_time:584270ms step_avg:185.13ms\nstep:3167/4500 train_loss:7.9566 train_time:584455ms step_avg:185.13ms\nstep:3168/4500 train_loss:8.1627 train_time:584639ms step_avg:185.13ms\nstep:3169/4500 train_loss:8.1708 train_time:584824ms step_avg:185.13ms\nstep:3170/4500 train_loss:8.1280 train_time:585007ms step_avg:185.13ms\nstep:3171/4500 train_loss:8.1448 train_time:585192ms step_avg:185.13ms\nstep:3172/4500 train_loss:8.1894 train_time:585376ms step_avg:185.13ms\nstep:3173/4500 train_loss:8.0529 train_time:585561ms step_avg:185.13ms\nstep:3174/4500 train_loss:8.1603 train_time:585747ms step_avg:185.13ms\nstep:3175/4500 train_loss:8.1497 train_time:585935ms step_avg:185.13ms\nstep:3176/4500 train_loss:8.0336 train_time:586119ms step_avg:185.13ms\nstep:3177/4500 train_loss:8.2137 train_time:586304ms step_avg:185.13ms\nstep:3178/4500 train_loss:8.0977 train_time:586488ms step_avg:185.13ms\nstep:3179/4500 train_loss:8.0182 train_time:586672ms step_avg:185.13ms\nstep:3180/4500 train_loss:8.0966 train_time:586857ms step_avg:185.13ms\nstep:3181/4500 train_loss:8.0193 train_time:587045ms step_avg:185.13ms\nstep:3182/4500 train_loss:8.0801 train_time:587227ms step_avg:185.13ms\nstep:3183/4500 train_loss:8.0643 train_time:587413ms step_avg:185.13ms\nstep:3184/4500 train_loss:8.0467 train_time:587600ms step_avg:185.13ms\nstep:3185/4500 train_loss:8.2693 train_time:587785ms step_avg:185.13ms\nstep:3186/4500 train_loss:8.0567 train_time:587973ms step_avg:185.13ms\nstep:3187/4500 train_loss:8.0717 train_time:588160ms step_avg:185.13ms\nstep:3188/4500 train_loss:8.1207 train_time:588344ms step_avg:185.13ms\nstep:3189/4500 train_loss:8.1543 train_time:588531ms step_avg:185.13ms\nstep:3190/4500 train_loss:8.1274 train_time:588717ms step_avg:185.13ms\nstep:3191/4500 train_loss:8.1824 train_time:588904ms step_avg:185.13ms\nstep:3192/4500 train_loss:7.9985 train_time:589089ms step_avg:185.13ms\nstep:3193/4500 train_loss:8.2206 train_time:589273ms step_avg:185.13ms\nstep:3194/4500 train_loss:8.1215 train_time:589458ms step_avg:185.13ms\nstep:3195/4500 train_loss:8.1086 train_time:589645ms step_avg:185.13ms\nstep:3196/4500 train_loss:7.9917 train_time:589828ms step_avg:185.13ms\nstep:3197/4500 train_loss:8.0877 train_time:590012ms step_avg:185.13ms\nstep:3198/4500 train_loss:8.0818 train_time:590195ms step_avg:185.13ms\nstep:3199/4500 train_loss:8.0903 train_time:590379ms step_avg:185.13ms\nstep:3200/4500 train_loss:8.8270 train_time:590563ms step_avg:185.13ms\nstep:3201/4500 train_loss:8.0471 train_time:590749ms step_avg:185.13ms\nstep:3202/4500 train_loss:8.0466 train_time:590934ms step_avg:185.13ms\nstep:3203/4500 train_loss:8.1696 train_time:591120ms step_avg:185.13ms\nstep:3204/4500 train_loss:8.1017 train_time:591308ms step_avg:185.13ms\nstep:3205/4500 train_loss:8.0992 train_time:591495ms step_avg:185.13ms\nstep:3206/4500 train_loss:8.2057 train_time:591679ms step_avg:185.13ms\nstep:3207/4500 train_loss:8.1203 train_time:591862ms step_avg:185.13ms\nstep:3208/4500 train_loss:8.1118 train_time:592046ms step_avg:185.13ms\nstep:3209/4500 train_loss:8.0993 train_time:592229ms step_avg:185.13ms\nstep:3210/4500 train_loss:8.3077 train_time:592413ms step_avg:185.13ms\nstep:3211/4500 train_loss:8.2992 train_time:592600ms step_avg:185.13ms\nstep:3212/4500 train_loss:8.1173 train_time:592789ms step_avg:185.13ms\nstep:3213/4500 train_loss:8.2176 train_time:592974ms step_avg:185.13ms\nstep:3214/4500 train_loss:8.0857 train_time:593158ms step_avg:185.13ms\nstep:3215/4500 train_loss:8.1487 train_time:593343ms step_avg:185.13ms\nstep:3216/4500 train_loss:8.1331 train_time:593527ms step_avg:185.13ms\nstep:3217/4500 train_loss:8.1322 train_time:593711ms step_avg:185.13ms\nstep:3218/4500 train_loss:8.1140 train_time:593896ms step_avg:185.13ms\nstep:3219/4500 train_loss:8.1343 train_time:594080ms step_avg:185.13ms\nstep:3220/4500 train_loss:8.0406 train_time:594265ms step_avg:185.13ms\nstep:3221/4500 train_loss:8.0923 train_time:594449ms step_avg:185.13ms\nstep:3222/4500 train_loss:8.1748 train_time:594635ms step_avg:185.13ms\nstep:3223/4500 train_loss:8.0428 train_time:594823ms step_avg:185.13ms\nstep:3224/4500 train_loss:7.9306 train_time:595012ms step_avg:185.13ms\nstep:3225/4500 train_loss:8.2287 train_time:595196ms step_avg:185.13ms\nstep:3226/4500 train_loss:8.0886 train_time:595380ms step_avg:185.13ms\nstep:3227/4500 train_loss:8.4151 train_time:595566ms step_avg:185.13ms\nstep:3228/4500 train_loss:8.0767 train_time:595753ms step_avg:185.13ms\nstep:3229/4500 train_loss:8.1472 train_time:595939ms step_avg:185.13ms\nstep:3230/4500 train_loss:8.1201 train_time:596128ms step_avg:185.13ms\nstep:3231/4500 train_loss:8.1711 train_time:596312ms step_avg:185.13ms\nstep:3232/4500 train_loss:8.1261 train_time:596496ms step_avg:185.13ms\nstep:3233/4500 train_loss:8.1932 train_time:596681ms step_avg:185.13ms\nstep:3234/4500 train_loss:8.1786 train_time:596868ms step_avg:185.13ms\nstep:3235/4500 train_loss:8.3113 train_time:597054ms step_avg:185.13ms\nstep:3236/4500 train_loss:8.1664 train_time:597240ms step_avg:185.13ms\nstep:3237/4500 train_loss:8.1431 train_time:597428ms step_avg:185.13ms\nstep:3238/4500 train_loss:8.1120 train_time:597614ms step_avg:185.13ms\nstep:3239/4500 train_loss:8.0815 train_time:597797ms step_avg:185.13ms\nstep:3240/4500 train_loss:8.1411 train_time:597982ms step_avg:185.13ms\nstep:3241/4500 train_loss:8.1210 train_time:598169ms step_avg:185.13ms\nstep:3242/4500 train_loss:8.1074 train_time:598353ms step_avg:185.13ms\nstep:3243/4500 train_loss:8.3865 train_time:598540ms step_avg:185.13ms\nstep:3244/4500 train_loss:8.1500 train_time:598728ms step_avg:185.14ms\nstep:3245/4500 train_loss:8.1042 train_time:598912ms step_avg:185.14ms\nstep:3246/4500 train_loss:8.1309 train_time:599097ms step_avg:185.13ms\nstep:3247/4500 train_loss:8.1116 train_time:599281ms step_avg:185.13ms\nstep:3248/4500 train_loss:8.1045 train_time:599466ms step_avg:185.13ms\nstep:3249/4500 train_loss:8.1399 train_time:599653ms step_avg:185.14ms\nstep:3250/4500 train_loss:8.1383 train_time:599838ms step_avg:185.14ms\nstep:3251/4500 train_loss:8.1689 train_time:600024ms step_avg:185.14ms\nstep:3252/4500 train_loss:8.0709 train_time:600208ms step_avg:185.14ms\nstep:3253/4500 train_loss:8.1626 train_time:600392ms step_avg:185.13ms\nstep:3254/4500 train_loss:8.1138 train_time:600575ms step_avg:185.13ms\nstep:3255/4500 train_loss:8.1240 train_time:600761ms step_avg:185.13ms\nstep:3256/4500 train_loss:8.1667 train_time:600945ms step_avg:185.13ms\nstep:3257/4500 train_loss:8.0570 train_time:601128ms step_avg:185.13ms\nstep:3258/4500 train_loss:8.1483 train_time:601314ms step_avg:185.13ms\nstep:3259/4500 train_loss:8.1895 train_time:601499ms step_avg:185.13ms\nstep:3260/4500 train_loss:8.0382 train_time:601685ms step_avg:185.13ms\nstep:3261/4500 train_loss:8.2139 train_time:601874ms step_avg:185.14ms\nstep:3262/4500 train_loss:8.1446 train_time:602058ms step_avg:185.13ms\nstep:3263/4500 train_loss:8.2269 train_time:602244ms step_avg:185.13ms\nstep:3264/4500 train_loss:8.2436 train_time:602428ms step_avg:185.13ms\nstep:3265/4500 train_loss:8.0771 train_time:602615ms step_avg:185.14ms\nstep:3266/4500 train_loss:8.1162 train_time:602797ms step_avg:185.13ms\nstep:3267/4500 train_loss:8.0971 train_time:602981ms step_avg:185.13ms\nstep:3268/4500 train_loss:7.9944 train_time:603166ms step_avg:185.13ms\nstep:3269/4500 train_loss:8.0446 train_time:603352ms step_avg:185.13ms\nstep:3270/4500 train_loss:8.0550 train_time:603538ms step_avg:185.13ms\nstep:3271/4500 train_loss:8.0469 train_time:603725ms step_avg:185.13ms\nstep:3272/4500 train_loss:8.0920 train_time:603909ms step_avg:185.13ms\nstep:3273/4500 train_loss:8.0400 train_time:604093ms step_avg:185.13ms\nstep:3274/4500 train_loss:8.1107 train_time:604277ms step_avg:185.13ms\nstep:3275/4500 train_loss:8.1385 train_time:604462ms step_avg:185.13ms\nstep:3276/4500 train_loss:8.0316 train_time:604649ms step_avg:185.13ms\nstep:3277/4500 train_loss:8.0870 train_time:604834ms step_avg:185.13ms\nstep:3278/4500 train_loss:8.1620 train_time:605020ms step_avg:185.13ms\nstep:3279/4500 train_loss:8.0245 train_time:605207ms step_avg:185.14ms\nstep:3280/4500 train_loss:8.1897 train_time:605391ms step_avg:185.13ms\nstep:3281/4500 train_loss:7.9378 train_time:605574ms step_avg:185.13ms\nstep:3282/4500 train_loss:8.2344 train_time:605758ms step_avg:185.13ms\nstep:3283/4500 train_loss:8.2867 train_time:605944ms step_avg:185.13ms\nstep:3284/4500 train_loss:7.9428 train_time:606129ms step_avg:185.13ms\nstep:3285/4500 train_loss:8.0743 train_time:606315ms step_avg:185.13ms\nstep:3286/4500 train_loss:8.1420 train_time:606501ms step_avg:185.13ms\nstep:3287/4500 train_loss:8.0506 train_time:606688ms step_avg:185.14ms\nstep:3288/4500 train_loss:8.1169 train_time:606873ms step_avg:185.14ms\nstep:3289/4500 train_loss:8.1305 train_time:607060ms step_avg:185.14ms\nstep:3290/4500 train_loss:8.0872 train_time:607245ms step_avg:185.14ms\nstep:3291/4500 train_loss:8.0287 train_time:607429ms step_avg:185.14ms\nstep:3292/4500 train_loss:8.1400 train_time:607616ms step_avg:185.14ms\nstep:3293/4500 train_loss:8.0093 train_time:607799ms step_avg:185.14ms\nstep:3294/4500 train_loss:8.0407 train_time:607985ms step_avg:185.14ms\nstep:3295/4500 train_loss:8.1053 train_time:608171ms step_avg:185.14ms\nstep:3296/4500 train_loss:7.9859 train_time:608357ms step_avg:185.14ms\nstep:3297/4500 train_loss:8.1440 train_time:608540ms step_avg:185.14ms\nstep:3298/4500 train_loss:8.0776 train_time:608728ms step_avg:185.14ms\nstep:3299/4500 train_loss:8.1308 train_time:608912ms step_avg:185.14ms\nstep:3300/4500 train_loss:8.0786 train_time:609095ms step_avg:185.14ms\nstep:3301/4500 train_loss:7.9865 train_time:609281ms step_avg:185.14ms\nstep:3302/4500 train_loss:8.0394 train_time:609466ms step_avg:185.14ms\nstep:3303/4500 train_loss:8.0503 train_time:609649ms step_avg:185.13ms\nstep:3304/4500 train_loss:8.1242 train_time:609834ms step_avg:185.13ms\nstep:3305/4500 train_loss:7.9806 train_time:610021ms step_avg:185.14ms\nstep:3306/4500 train_loss:8.0336 train_time:610207ms step_avg:185.14ms\nstep:3307/4500 train_loss:8.1699 train_time:610392ms step_avg:185.14ms\nstep:3308/4500 train_loss:8.1159 train_time:610578ms step_avg:185.14ms\nstep:3309/4500 train_loss:8.1005 train_time:610761ms step_avg:185.14ms\nstep:3310/4500 train_loss:8.1031 train_time:610949ms step_avg:185.14ms\nstep:3311/4500 train_loss:8.1515 train_time:611131ms step_avg:185.14ms\nstep:3312/4500 train_loss:8.1672 train_time:611315ms step_avg:185.13ms\nstep:3313/4500 train_loss:8.1622 train_time:611501ms step_avg:185.13ms\nstep:3314/4500 train_loss:8.2284 train_time:611684ms step_avg:185.13ms\nstep:3315/4500 train_loss:8.1524 train_time:611872ms step_avg:185.14ms\nstep:3316/4500 train_loss:8.1638 train_time:612055ms step_avg:185.13ms\nstep:3317/4500 train_loss:8.1402 train_time:612239ms step_avg:185.13ms\nstep:3318/4500 train_loss:8.1000 train_time:612430ms step_avg:185.14ms\nstep:3319/4500 train_loss:8.1903 train_time:612611ms step_avg:185.13ms\nstep:3320/4500 train_loss:8.1958 train_time:612795ms step_avg:185.13ms\nstep:3321/4500 train_loss:8.1452 train_time:612982ms step_avg:185.14ms\nstep:3322/4500 train_loss:8.2206 train_time:613166ms step_avg:185.13ms\nstep:3323/4500 train_loss:8.1604 train_time:613354ms step_avg:185.14ms\nstep:3324/4500 train_loss:8.0207 train_time:613539ms step_avg:185.14ms\nstep:3325/4500 train_loss:8.1039 train_time:613727ms step_avg:185.14ms\nstep:3326/4500 train_loss:8.1198 train_time:613912ms step_avg:185.14ms\nstep:3327/4500 train_loss:8.1829 train_time:614096ms step_avg:185.14ms\nstep:3328/4500 train_loss:8.1549 train_time:614280ms step_avg:185.14ms\nstep:3329/4500 train_loss:8.1005 train_time:614465ms step_avg:185.14ms\nstep:3330/4500 train_loss:8.0241 train_time:614650ms step_avg:185.14ms\nstep:3331/4500 train_loss:8.2481 train_time:614834ms step_avg:185.14ms\nstep:3332/4500 train_loss:8.0919 train_time:615019ms step_avg:185.14ms\nstep:3333/4500 train_loss:8.1517 train_time:615212ms step_avg:185.14ms\nstep:3334/4500 train_loss:8.1494 train_time:615391ms step_avg:185.14ms\nstep:3335/4500 train_loss:8.1894 train_time:615576ms step_avg:185.14ms\nstep:3336/4500 train_loss:8.1280 train_time:615762ms step_avg:185.14ms\nstep:3337/4500 train_loss:8.1769 train_time:615948ms step_avg:185.14ms\nstep:3338/4500 train_loss:8.1059 train_time:616132ms step_avg:185.14ms\nstep:3339/4500 train_loss:8.2289 train_time:616317ms step_avg:185.14ms\nstep:3340/4500 train_loss:8.1599 train_time:616501ms step_avg:185.14ms\nstep:3341/4500 train_loss:8.1497 train_time:616689ms step_avg:185.14ms\nstep:3342/4500 train_loss:8.2148 train_time:616872ms step_avg:185.14ms\nstep:3343/4500 train_loss:8.1428 train_time:617054ms step_avg:185.13ms\nstep:3344/4500 train_loss:8.1393 train_time:617238ms step_avg:185.13ms\nstep:3345/4500 train_loss:8.0635 train_time:617423ms step_avg:185.13ms\nstep:3346/4500 train_loss:8.1102 train_time:617610ms step_avg:185.13ms\nstep:3347/4500 train_loss:8.1425 train_time:617796ms step_avg:185.14ms\nstep:3348/4500 train_loss:7.9965 train_time:617983ms step_avg:185.14ms\nstep:3349/4500 train_loss:8.0667 train_time:618168ms step_avg:185.14ms\nstep:3350/4500 train_loss:8.3229 train_time:618354ms step_avg:185.14ms\nstep:3351/4500 train_loss:8.1040 train_time:618539ms step_avg:185.14ms\nstep:3352/4500 train_loss:8.0873 train_time:618727ms step_avg:185.14ms\nstep:3353/4500 train_loss:7.8938 train_time:618912ms step_avg:185.14ms\nstep:3354/4500 train_loss:8.0928 train_time:619096ms step_avg:185.14ms\nstep:3355/4500 train_loss:8.0649 train_time:619280ms step_avg:185.14ms\nstep:3356/4500 train_loss:8.0648 train_time:619467ms step_avg:185.14ms\nstep:3357/4500 train_loss:8.1205 train_time:619654ms step_avg:185.14ms\nstep:3358/4500 train_loss:8.1349 train_time:619838ms step_avg:185.14ms\nstep:3359/4500 train_loss:8.1443 train_time:620026ms step_avg:185.14ms\nstep:3360/4500 train_loss:8.0749 train_time:620209ms step_avg:185.14ms\nstep:3361/4500 train_loss:8.0627 train_time:620394ms step_avg:185.14ms\nstep:3362/4500 train_loss:8.1186 train_time:620578ms step_avg:185.14ms\nstep:3363/4500 train_loss:8.0906 train_time:620766ms step_avg:185.14ms\nstep:3364/4500 train_loss:8.1063 train_time:620949ms step_avg:185.14ms\nstep:3365/4500 train_loss:8.0914 train_time:621134ms step_avg:185.14ms\nstep:3366/4500 train_loss:8.0949 train_time:621320ms step_avg:185.14ms\nstep:3367/4500 train_loss:8.1588 train_time:621504ms step_avg:185.14ms\nstep:3368/4500 train_loss:8.1176 train_time:621689ms step_avg:185.14ms\nstep:3369/4500 train_loss:8.1127 train_time:621875ms step_avg:185.14ms\nstep:3370/4500 train_loss:8.1898 train_time:622060ms step_avg:185.14ms\nstep:3371/4500 train_loss:8.1994 train_time:622248ms step_avg:185.14ms\nstep:3372/4500 train_loss:7.9739 train_time:622433ms step_avg:185.14ms\nstep:3373/4500 train_loss:8.0576 train_time:622618ms step_avg:185.14ms\nstep:3374/4500 train_loss:8.0899 train_time:622802ms step_avg:185.14ms\nstep:3375/4500 train_loss:8.0929 train_time:622988ms step_avg:185.14ms\nstep:3376/4500 train_loss:8.0126 train_time:623170ms step_avg:185.14ms\nstep:3377/4500 train_loss:8.1090 train_time:623354ms step_avg:185.14ms\nstep:3378/4500 train_loss:8.1114 train_time:623542ms step_avg:185.14ms\nstep:3379/4500 train_loss:8.2553 train_time:623727ms step_avg:185.14ms\nstep:3380/4500 train_loss:8.1995 train_time:623913ms step_avg:185.14ms\nstep:3381/4500 train_loss:7.9632 train_time:624100ms step_avg:185.14ms\nstep:3382/4500 train_loss:7.9988 train_time:624286ms step_avg:185.14ms\nstep:3383/4500 train_loss:8.1204 train_time:624469ms step_avg:185.14ms\nstep:3384/4500 train_loss:8.0406 train_time:624654ms step_avg:185.14ms\nstep:3385/4500 train_loss:8.1784 train_time:624839ms step_avg:185.14ms\nstep:3386/4500 train_loss:8.1421 train_time:625024ms step_avg:185.14ms\nstep:3387/4500 train_loss:8.0699 train_time:625209ms step_avg:185.14ms\nstep:3388/4500 train_loss:8.1376 train_time:625393ms step_avg:185.14ms\nstep:3389/4500 train_loss:8.1380 train_time:625577ms step_avg:185.14ms\nstep:3390/4500 train_loss:8.1051 train_time:625763ms step_avg:185.14ms\nstep:3391/4500 train_loss:8.1521 train_time:625947ms step_avg:185.14ms\nstep:3392/4500 train_loss:8.1391 train_time:626131ms step_avg:185.14ms\nstep:3393/4500 train_loss:8.1445 train_time:626313ms step_avg:185.14ms\nstep:3394/4500 train_loss:8.1036 train_time:626497ms step_avg:185.14ms\nstep:3395/4500 train_loss:8.2239 train_time:626683ms step_avg:185.14ms\nstep:3396/4500 train_loss:8.2400 train_time:626870ms step_avg:185.14ms\nstep:3397/4500 train_loss:8.2541 train_time:627053ms step_avg:185.14ms\nstep:3398/4500 train_loss:8.1098 train_time:627237ms step_avg:185.13ms\nstep:3399/4500 train_loss:8.1652 train_time:627423ms step_avg:185.14ms\nstep:3400/4500 train_loss:8.2292 train_time:627611ms step_avg:185.14ms\nstep:3401/4500 train_loss:8.1965 train_time:627794ms step_avg:185.14ms\nstep:3402/4500 train_loss:8.0598 train_time:627980ms step_avg:185.14ms\nstep:3403/4500 train_loss:8.0814 train_time:628168ms step_avg:185.14ms\nstep:3404/4500 train_loss:8.1355 train_time:628352ms step_avg:185.14ms\nstep:3405/4500 train_loss:8.0627 train_time:628535ms step_avg:185.14ms\nstep:3406/4500 train_loss:8.1432 train_time:628721ms step_avg:185.14ms\nstep:3407/4500 train_loss:8.2427 train_time:628906ms step_avg:185.14ms\nstep:3408/4500 train_loss:8.0873 train_time:629091ms step_avg:185.14ms\nstep:3409/4500 train_loss:8.1761 train_time:629275ms step_avg:185.14ms\nstep:3410/4500 train_loss:8.3306 train_time:629461ms step_avg:185.14ms\nstep:3411/4500 train_loss:8.1171 train_time:629645ms step_avg:185.14ms\nstep:3412/4500 train_loss:7.9937 train_time:629828ms step_avg:185.13ms\nstep:3413/4500 train_loss:8.1524 train_time:630011ms step_avg:185.13ms\nstep:3414/4500 train_loss:8.0517 train_time:630196ms step_avg:185.13ms\nstep:3415/4500 train_loss:8.1592 train_time:630380ms step_avg:185.13ms\nstep:3416/4500 train_loss:8.1739 train_time:630566ms step_avg:185.13ms\nstep:3417/4500 train_loss:8.0756 train_time:630751ms step_avg:185.13ms\nstep:3418/4500 train_loss:8.1276 train_time:630937ms step_avg:185.13ms\nstep:3419/4500 train_loss:8.1338 train_time:631123ms step_avg:185.13ms\nstep:3420/4500 train_loss:7.9863 train_time:631309ms step_avg:185.13ms\nstep:3421/4500 train_loss:8.1265 train_time:631494ms step_avg:185.13ms\nstep:3422/4500 train_loss:8.2089 train_time:631680ms step_avg:185.13ms\nstep:3423/4500 train_loss:8.1807 train_time:631867ms step_avg:185.14ms\nstep:3424/4500 train_loss:7.9110 train_time:632052ms step_avg:185.14ms\nstep:3425/4500 train_loss:8.1450 train_time:632238ms step_avg:185.14ms\nstep:3426/4500 train_loss:8.1799 train_time:632424ms step_avg:185.14ms\nstep:3427/4500 train_loss:7.9987 train_time:632607ms step_avg:185.14ms\nstep:3428/4500 train_loss:8.0540 train_time:632794ms step_avg:185.14ms\nstep:3429/4500 train_loss:8.1817 train_time:632977ms step_avg:185.14ms\nstep:3430/4500 train_loss:8.1189 train_time:633162ms step_avg:185.14ms\nstep:3431/4500 train_loss:8.0662 train_time:633347ms step_avg:185.14ms\nstep:3432/4500 train_loss:8.1563 train_time:633535ms step_avg:185.14ms\nstep:3433/4500 train_loss:8.0773 train_time:633720ms step_avg:185.14ms\nstep:3434/4500 train_loss:8.0621 train_time:633905ms step_avg:185.14ms\nstep:3435/4500 train_loss:8.0749 train_time:634090ms step_avg:185.14ms\nstep:3436/4500 train_loss:8.1594 train_time:634273ms step_avg:185.14ms\nstep:3437/4500 train_loss:8.2437 train_time:634458ms step_avg:185.14ms\nstep:3438/4500 train_loss:8.1049 train_time:634642ms step_avg:185.13ms\nstep:3439/4500 train_loss:8.1240 train_time:634826ms step_avg:185.13ms\nstep:3440/4500 train_loss:8.0980 train_time:635010ms step_avg:185.13ms\nstep:3441/4500 train_loss:8.3492 train_time:635195ms step_avg:185.13ms\nstep:3442/4500 train_loss:8.1453 train_time:635380ms step_avg:185.13ms\nstep:3443/4500 train_loss:8.1149 train_time:635566ms step_avg:185.13ms\nstep:3444/4500 train_loss:8.1924 train_time:635753ms step_avg:185.13ms\nstep:3445/4500 train_loss:8.1542 train_time:635936ms step_avg:185.13ms\nstep:3446/4500 train_loss:8.1599 train_time:636121ms step_avg:185.13ms\nstep:3447/4500 train_loss:8.1881 train_time:636309ms step_avg:185.13ms\nstep:3448/4500 train_loss:8.0710 train_time:636494ms step_avg:185.13ms\nstep:3449/4500 train_loss:8.2214 train_time:636680ms step_avg:185.14ms\nstep:3450/4500 train_loss:8.2141 train_time:636865ms step_avg:185.14ms\nstep:3451/4500 train_loss:8.1474 train_time:637050ms step_avg:185.14ms\nstep:3452/4500 train_loss:8.0512 train_time:637232ms step_avg:185.13ms\nstep:3453/4500 train_loss:8.2262 train_time:637415ms step_avg:185.13ms\nstep:3454/4500 train_loss:8.0367 train_time:637601ms step_avg:185.13ms\nstep:3455/4500 train_loss:8.1934 train_time:637786ms step_avg:185.13ms\nstep:3456/4500 train_loss:8.2301 train_time:637973ms step_avg:185.13ms\nstep:3457/4500 train_loss:8.1110 train_time:638157ms step_avg:185.13ms\nstep:3458/4500 train_loss:8.0503 train_time:638342ms step_avg:185.13ms\nstep:3459/4500 train_loss:8.1309 train_time:638526ms step_avg:185.13ms\nstep:3460/4500 train_loss:8.2623 train_time:638710ms step_avg:185.13ms\nstep:3461/4500 train_loss:8.0230 train_time:638895ms step_avg:185.13ms\nstep:3462/4500 train_loss:8.2328 train_time:639080ms step_avg:185.13ms\nstep:3463/4500 train_loss:8.1763 train_time:639266ms step_avg:185.13ms\nstep:3464/4500 train_loss:8.1320 train_time:639451ms step_avg:185.13ms\nstep:3465/4500 train_loss:8.2049 train_time:639635ms step_avg:185.13ms\nstep:3466/4500 train_loss:8.1917 train_time:639819ms step_avg:185.13ms\nstep:3467/4500 train_loss:8.0603 train_time:640002ms step_avg:185.13ms\nstep:3468/4500 train_loss:8.1228 train_time:640187ms step_avg:185.13ms\nstep:3469/4500 train_loss:7.9930 train_time:640370ms step_avg:185.13ms\nstep:3470/4500 train_loss:8.0318 train_time:640556ms step_avg:185.13ms\nstep:3471/4500 train_loss:8.0288 train_time:640740ms step_avg:185.13ms\nstep:3472/4500 train_loss:8.2249 train_time:640926ms step_avg:185.13ms\nstep:3473/4500 train_loss:8.1173 train_time:641111ms step_avg:185.13ms\nstep:3474/4500 train_loss:8.0975 train_time:641296ms step_avg:185.13ms\nstep:3475/4500 train_loss:8.0988 train_time:641480ms step_avg:185.13ms\nstep:3476/4500 train_loss:8.1440 train_time:641666ms step_avg:185.13ms\nstep:3477/4500 train_loss:8.0443 train_time:641849ms step_avg:185.13ms\nstep:3478/4500 train_loss:8.0813 train_time:642033ms step_avg:185.13ms\nstep:3479/4500 train_loss:8.1243 train_time:642218ms step_avg:185.13ms\nstep:3480/4500 train_loss:7.9734 train_time:642407ms step_avg:185.13ms\nstep:3481/4500 train_loss:8.1762 train_time:642594ms step_avg:185.13ms\nstep:3482/4500 train_loss:8.1026 train_time:642781ms step_avg:185.13ms\nstep:3483/4500 train_loss:8.1676 train_time:642964ms step_avg:185.13ms\nstep:3484/4500 train_loss:8.2770 train_time:643150ms step_avg:185.13ms\nstep:3485/4500 train_loss:8.2449 train_time:643334ms step_avg:185.13ms\nstep:3486/4500 train_loss:8.0461 train_time:643522ms step_avg:185.13ms\nstep:3487/4500 train_loss:7.9170 train_time:643711ms step_avg:185.13ms\nstep:3488/4500 train_loss:8.1539 train_time:643896ms step_avg:185.13ms\nstep:3489/4500 train_loss:8.0255 train_time:644080ms step_avg:185.13ms\nstep:3490/4500 train_loss:8.0517 train_time:644266ms step_avg:185.13ms\nstep:3491/4500 train_loss:8.0812 train_time:644451ms step_avg:185.13ms\nstep:3492/4500 train_loss:8.1939 train_time:644636ms step_avg:185.13ms\nstep:3493/4500 train_loss:8.1119 train_time:644821ms step_avg:185.13ms\nstep:3494/4500 train_loss:8.1771 train_time:645005ms step_avg:185.13ms\nstep:3495/4500 train_loss:8.1083 train_time:645192ms step_avg:185.13ms\nstep:3496/4500 train_loss:8.1568 train_time:645377ms step_avg:185.13ms\nstep:3497/4500 train_loss:8.0677 train_time:645562ms step_avg:185.13ms\nstep:3498/4500 train_loss:8.0958 train_time:645750ms step_avg:185.13ms\nstep:3499/4500 train_loss:8.0549 train_time:645937ms step_avg:185.14ms\nstep:3500/4500 train_loss:8.0293 train_time:646123ms step_avg:185.14ms\nstep:3501/4500 train_loss:8.1108 train_time:646307ms step_avg:185.14ms\nstep:3502/4500 train_loss:7.6203 train_time:646491ms step_avg:185.13ms\nstep:3503/4500 train_loss:8.1957 train_time:646678ms step_avg:185.14ms\nstep:3504/4500 train_loss:8.1148 train_time:646862ms step_avg:185.14ms\nstep:3505/4500 train_loss:8.0790 train_time:647048ms step_avg:185.14ms\nstep:3506/4500 train_loss:8.0810 train_time:647230ms step_avg:185.13ms\nstep:3507/4500 train_loss:8.0536 train_time:647414ms step_avg:185.13ms\nstep:3508/4500 train_loss:8.1318 train_time:647597ms step_avg:185.13ms\nstep:3509/4500 train_loss:8.5087 train_time:647783ms step_avg:185.13ms\nstep:3510/4500 train_loss:8.1964 train_time:647967ms step_avg:185.13ms\nstep:3511/4500 train_loss:7.9655 train_time:648152ms step_avg:185.13ms\nstep:3512/4500 train_loss:8.0849 train_time:648337ms step_avg:185.13ms\nstep:3513/4500 train_loss:8.1354 train_time:648523ms step_avg:185.13ms\nstep:3514/4500 train_loss:8.1416 train_time:648709ms step_avg:185.13ms\nstep:3515/4500 train_loss:8.1117 train_time:648893ms step_avg:185.13ms\nstep:3516/4500 train_loss:8.1270 train_time:649077ms step_avg:185.13ms\nstep:3517/4500 train_loss:8.0647 train_time:649262ms step_avg:185.13ms\nstep:3518/4500 train_loss:8.0394 train_time:649449ms step_avg:185.13ms\nstep:3519/4500 train_loss:8.0770 train_time:649633ms step_avg:185.13ms\nstep:3520/4500 train_loss:8.3264 train_time:649819ms step_avg:185.13ms\nstep:3521/4500 train_loss:8.0727 train_time:650005ms step_avg:185.13ms\nstep:3522/4500 train_loss:8.2182 train_time:650190ms step_avg:185.13ms\nstep:3523/4500 train_loss:8.1744 train_time:650374ms step_avg:185.13ms\nstep:3524/4500 train_loss:8.0577 train_time:650560ms step_avg:185.13ms\nstep:3525/4500 train_loss:8.1189 train_time:650746ms step_avg:185.13ms\nstep:3526/4500 train_loss:8.1042 train_time:650932ms step_avg:185.13ms\nstep:3527/4500 train_loss:8.0998 train_time:651117ms step_avg:185.13ms\nstep:3528/4500 train_loss:8.1111 train_time:651304ms step_avg:185.13ms\nstep:3529/4500 train_loss:8.0311 train_time:651488ms step_avg:185.13ms\nstep:3530/4500 train_loss:8.1508 train_time:651672ms step_avg:185.13ms\nstep:3531/4500 train_loss:8.1251 train_time:651857ms step_avg:185.13ms\nstep:3532/4500 train_loss:8.0498 train_time:652043ms step_avg:185.13ms\nstep:3533/4500 train_loss:8.1678 train_time:652232ms step_avg:185.14ms\nstep:3534/4500 train_loss:8.2078 train_time:652417ms step_avg:185.14ms\nstep:3535/4500 train_loss:8.0302 train_time:652601ms step_avg:185.13ms\nstep:3536/4500 train_loss:8.0318 train_time:652785ms step_avg:185.13ms\nstep:3537/4500 train_loss:8.0053 train_time:652968ms step_avg:185.13ms\nstep:3538/4500 train_loss:8.2494 train_time:653153ms step_avg:185.13ms\nstep:3539/4500 train_loss:8.0712 train_time:653342ms step_avg:185.14ms\nstep:3540/4500 train_loss:8.0644 train_time:653527ms step_avg:185.14ms\nstep:3541/4500 train_loss:8.1889 train_time:653713ms step_avg:185.14ms\nstep:3542/4500 train_loss:8.2106 train_time:653896ms step_avg:185.13ms\nstep:3543/4500 train_loss:8.0496 train_time:654080ms step_avg:185.13ms\nstep:3544/4500 train_loss:8.0587 train_time:654265ms step_avg:185.13ms\nstep:3545/4500 train_loss:8.0363 train_time:654449ms step_avg:185.13ms\nstep:3546/4500 train_loss:8.1017 train_time:654635ms step_avg:185.13ms\nstep:3547/4500 train_loss:8.0577 train_time:654821ms step_avg:185.13ms\nstep:3548/4500 train_loss:8.0631 train_time:655008ms step_avg:185.14ms\nstep:3549/4500 train_loss:8.2430 train_time:655192ms step_avg:185.13ms\nstep:3550/4500 train_loss:8.1752 train_time:655376ms step_avg:185.13ms\nstep:3551/4500 train_loss:8.1749 train_time:655560ms step_avg:185.13ms\nstep:3552/4500 train_loss:8.1143 train_time:655744ms step_avg:185.13ms\nstep:3553/4500 train_loss:8.1425 train_time:655927ms step_avg:185.13ms\nstep:3554/4500 train_loss:8.2482 train_time:656110ms step_avg:185.13ms\nstep:3555/4500 train_loss:8.0754 train_time:656295ms step_avg:185.13ms\nstep:3556/4500 train_loss:8.2116 train_time:656479ms step_avg:185.13ms\nstep:3557/4500 train_loss:8.1263 train_time:656663ms step_avg:185.13ms\nstep:3558/4500 train_loss:8.0233 train_time:656847ms step_avg:185.13ms\nstep:3559/4500 train_loss:8.0596 train_time:657034ms step_avg:185.13ms\nstep:3560/4500 train_loss:8.1517 train_time:657220ms step_avg:185.13ms\nstep:3561/4500 train_loss:8.1050 train_time:657405ms step_avg:185.13ms\nstep:3562/4500 train_loss:8.0739 train_time:657593ms step_avg:185.13ms\nstep:3563/4500 train_loss:8.0129 train_time:657778ms step_avg:185.13ms\nstep:3564/4500 train_loss:8.0649 train_time:657962ms step_avg:185.13ms\nstep:3565/4500 train_loss:8.0519 train_time:658146ms step_avg:185.13ms\nstep:3566/4500 train_loss:8.0892 train_time:658334ms step_avg:185.13ms\nstep:3567/4500 train_loss:8.2448 train_time:658518ms step_avg:185.13ms\nstep:3568/4500 train_loss:8.1179 train_time:658707ms step_avg:185.13ms\nstep:3569/4500 train_loss:8.1558 train_time:658895ms step_avg:185.13ms\nstep:3570/4500 train_loss:8.0746 train_time:659080ms step_avg:185.13ms\nstep:3571/4500 train_loss:8.0048 train_time:659266ms step_avg:185.13ms\nstep:3572/4500 train_loss:8.0733 train_time:659454ms step_avg:185.14ms\nstep:3573/4500 train_loss:8.1878 train_time:659639ms step_avg:185.14ms\nstep:3574/4500 train_loss:8.1163 train_time:659824ms step_avg:185.14ms\nstep:3575/4500 train_loss:8.1134 train_time:660012ms step_avg:185.14ms\nstep:3576/4500 train_loss:8.0714 train_time:660197ms step_avg:185.14ms\nstep:3577/4500 train_loss:8.0258 train_time:660382ms step_avg:185.14ms\nstep:3578/4500 train_loss:8.1342 train_time:660567ms step_avg:185.14ms\nstep:3579/4500 train_loss:8.0772 train_time:660751ms step_avg:185.14ms\nstep:3580/4500 train_loss:8.0519 train_time:660936ms step_avg:185.14ms\nstep:3581/4500 train_loss:8.0955 train_time:661123ms step_avg:185.14ms\nstep:3582/4500 train_loss:8.2867 train_time:661306ms step_avg:185.14ms\nstep:3583/4500 train_loss:8.1842 train_time:661490ms step_avg:185.14ms\nstep:3584/4500 train_loss:7.9710 train_time:661674ms step_avg:185.14ms\nstep:3585/4500 train_loss:8.0970 train_time:661857ms step_avg:185.13ms\nstep:3586/4500 train_loss:8.2355 train_time:662041ms step_avg:185.13ms\nstep:3587/4500 train_loss:8.0535 train_time:662228ms step_avg:185.14ms\nstep:3588/4500 train_loss:8.0581 train_time:662414ms step_avg:185.14ms\nstep:3589/4500 train_loss:8.0854 train_time:662599ms step_avg:185.14ms\nstep:3590/4500 train_loss:8.1561 train_time:662786ms step_avg:185.14ms\nstep:3591/4500 train_loss:7.9854 train_time:662970ms step_avg:185.14ms\nstep:3592/4500 train_loss:8.2113 train_time:663154ms step_avg:185.14ms\nstep:3593/4500 train_loss:8.1401 train_time:663339ms step_avg:185.14ms\nstep:3594/4500 train_loss:8.0579 train_time:663526ms step_avg:185.14ms\nstep:3595/4500 train_loss:8.2143 train_time:663711ms step_avg:185.14ms\nstep:3596/4500 train_loss:8.0505 train_time:663895ms step_avg:185.14ms\nstep:3597/4500 train_loss:8.2130 train_time:664079ms step_avg:185.13ms\nstep:3598/4500 train_loss:7.9341 train_time:664265ms step_avg:185.14ms\nstep:3599/4500 train_loss:8.0082 train_time:664449ms step_avg:185.13ms\nstep:3600/4500 train_loss:8.0513 train_time:664634ms step_avg:185.13ms\nstep:3601/4500 train_loss:8.5786 train_time:664821ms step_avg:185.14ms\nstep:3602/4500 train_loss:8.0945 train_time:665009ms step_avg:185.14ms\nstep:3603/4500 train_loss:8.1433 train_time:665195ms step_avg:185.14ms\nstep:3604/4500 train_loss:8.1365 train_time:665382ms step_avg:185.14ms\nstep:3605/4500 train_loss:8.0129 train_time:665568ms step_avg:185.14ms\nstep:3606/4500 train_loss:8.1150 train_time:665751ms step_avg:185.14ms\nstep:3607/4500 train_loss:8.1233 train_time:665936ms step_avg:185.14ms\nstep:3608/4500 train_loss:8.0815 train_time:666119ms step_avg:185.14ms\nstep:3609/4500 train_loss:8.0476 train_time:666306ms step_avg:185.14ms\nstep:3610/4500 train_loss:8.0680 train_time:666492ms step_avg:185.14ms\nstep:3611/4500 train_loss:8.0474 train_time:666675ms step_avg:185.14ms\nstep:3612/4500 train_loss:8.0965 train_time:666859ms step_avg:185.14ms\nstep:3613/4500 train_loss:8.1317 train_time:667044ms step_avg:185.14ms\nstep:3614/4500 train_loss:8.1712 train_time:667231ms step_avg:185.14ms\nstep:3615/4500 train_loss:8.1915 train_time:667415ms step_avg:185.14ms\nstep:3616/4500 train_loss:8.2536 train_time:667600ms step_avg:185.14ms\nstep:3617/4500 train_loss:8.0937 train_time:667786ms step_avg:185.14ms\nstep:3618/4500 train_loss:8.0532 train_time:667970ms step_avg:185.14ms\nstep:3619/4500 train_loss:8.1241 train_time:668156ms step_avg:185.14ms\nstep:3620/4500 train_loss:8.0751 train_time:668346ms step_avg:185.14ms\nstep:3621/4500 train_loss:8.1080 train_time:668527ms step_avg:185.14ms\nstep:3622/4500 train_loss:8.1521 train_time:668710ms step_avg:185.14ms\nstep:3623/4500 train_loss:8.0796 train_time:668895ms step_avg:185.14ms\nstep:3624/4500 train_loss:8.2166 train_time:669080ms step_avg:185.14ms\nstep:3625/4500 train_loss:8.0947 train_time:669264ms step_avg:185.14ms\nstep:3626/4500 train_loss:8.0781 train_time:669449ms step_avg:185.14ms\nstep:3627/4500 train_loss:8.0352 train_time:669635ms step_avg:185.14ms\nstep:3628/4500 train_loss:8.0456 train_time:669821ms step_avg:185.14ms\nstep:3629/4500 train_loss:8.0786 train_time:670008ms step_avg:185.14ms\nstep:3630/4500 train_loss:8.3426 train_time:670195ms step_avg:185.14ms\nstep:3631/4500 train_loss:8.0950 train_time:670379ms step_avg:185.14ms\nstep:3632/4500 train_loss:8.1278 train_time:670568ms step_avg:185.14ms\nstep:3633/4500 train_loss:8.0903 train_time:670751ms step_avg:185.14ms\nstep:3634/4500 train_loss:8.1136 train_time:670939ms step_avg:185.14ms\nstep:3635/4500 train_loss:8.2431 train_time:671125ms step_avg:185.14ms\nstep:3636/4500 train_loss:8.0529 train_time:671307ms step_avg:185.14ms\nstep:3637/4500 train_loss:8.1589 train_time:671491ms step_avg:185.14ms\nstep:3638/4500 train_loss:8.0804 train_time:671676ms step_avg:185.14ms\nstep:3639/4500 train_loss:8.0710 train_time:671861ms step_avg:185.14ms\nstep:3640/4500 train_loss:8.0473 train_time:672045ms step_avg:185.14ms\nstep:3641/4500 train_loss:8.1235 train_time:672229ms step_avg:185.14ms\nstep:3642/4500 train_loss:8.0688 train_time:672414ms step_avg:185.14ms\nstep:3643/4500 train_loss:8.1413 train_time:672599ms step_avg:185.14ms\nstep:3644/4500 train_loss:8.0913 train_time:672783ms step_avg:185.14ms\nstep:3645/4500 train_loss:8.2128 train_time:672965ms step_avg:185.13ms\nstep:3646/4500 train_loss:8.1949 train_time:673152ms step_avg:185.14ms\nstep:3647/4500 train_loss:8.0427 train_time:673336ms step_avg:185.13ms\nstep:3648/4500 train_loss:8.1507 train_time:673521ms step_avg:185.13ms\nstep:3649/4500 train_loss:8.1157 train_time:673707ms step_avg:185.14ms\nstep:3650/4500 train_loss:8.0843 train_time:673893ms step_avg:185.14ms\nstep:3651/4500 train_loss:8.1006 train_time:674076ms step_avg:185.13ms\nstep:3652/4500 train_loss:8.1553 train_time:674261ms step_avg:185.13ms\nstep:3653/4500 train_loss:8.1200 train_time:674446ms step_avg:185.13ms\nstep:3654/4500 train_loss:8.4497 train_time:674630ms step_avg:185.13ms\nstep:3655/4500 train_loss:8.1048 train_time:674814ms step_avg:185.13ms\nstep:3656/4500 train_loss:8.2311 train_time:675001ms step_avg:185.13ms\nstep:3657/4500 train_loss:8.1456 train_time:675186ms step_avg:185.13ms\nstep:3658/4500 train_loss:8.1612 train_time:675369ms step_avg:185.13ms\nstep:3659/4500 train_loss:8.0895 train_time:675555ms step_avg:185.13ms\nstep:3660/4500 train_loss:8.1286 train_time:675739ms step_avg:185.13ms\nstep:3661/4500 train_loss:8.0283 train_time:675923ms step_avg:185.13ms\nstep:3662/4500 train_loss:8.1412 train_time:676108ms step_avg:185.13ms\nstep:3663/4500 train_loss:8.0355 train_time:676292ms step_avg:185.13ms\nstep:3664/4500 train_loss:8.3172 train_time:676476ms step_avg:185.13ms\nstep:3665/4500 train_loss:8.0936 train_time:676662ms step_avg:185.13ms\nstep:3666/4500 train_loss:8.1691 train_time:676847ms step_avg:185.13ms\nstep:3667/4500 train_loss:8.1073 train_time:677034ms step_avg:185.13ms\nstep:3668/4500 train_loss:8.0612 train_time:677219ms step_avg:185.13ms\nstep:3669/4500 train_loss:8.1071 train_time:677407ms step_avg:185.13ms\nstep:3670/4500 train_loss:8.0825 train_time:677593ms step_avg:185.13ms\nstep:3671/4500 train_loss:8.0528 train_time:677777ms step_avg:185.13ms\nstep:3672/4500 train_loss:8.1042 train_time:677961ms step_avg:185.13ms\nstep:3673/4500 train_loss:8.3354 train_time:678146ms step_avg:185.13ms\nstep:3674/4500 train_loss:8.2048 train_time:678331ms step_avg:185.13ms\nstep:3675/4500 train_loss:8.2493 train_time:678516ms step_avg:185.13ms\nstep:3676/4500 train_loss:8.0958 train_time:678703ms step_avg:185.13ms\nstep:3677/4500 train_loss:8.1539 train_time:678886ms step_avg:185.13ms\nstep:3678/4500 train_loss:8.2331 train_time:679070ms step_avg:185.13ms\nstep:3679/4500 train_loss:8.1007 train_time:679253ms step_avg:185.13ms\nstep:3680/4500 train_loss:8.1071 train_time:679437ms step_avg:185.13ms\nstep:3681/4500 train_loss:7.7726 train_time:679622ms step_avg:185.13ms\nstep:3682/4500 train_loss:8.2196 train_time:679807ms step_avg:185.13ms\nstep:3683/4500 train_loss:7.8967 train_time:679992ms step_avg:185.13ms\nstep:3684/4500 train_loss:8.1093 train_time:680176ms step_avg:185.13ms\nstep:3685/4500 train_loss:8.1074 train_time:680360ms step_avg:185.13ms\nstep:3686/4500 train_loss:8.0676 train_time:680543ms step_avg:185.13ms\nstep:3687/4500 train_loss:8.0604 train_time:680730ms step_avg:185.13ms\nstep:3688/4500 train_loss:8.0004 train_time:680912ms step_avg:185.13ms\nstep:3689/4500 train_loss:8.1199 train_time:681097ms step_avg:185.13ms\nstep:3690/4500 train_loss:8.0368 train_time:681282ms step_avg:185.13ms\nstep:3691/4500 train_loss:8.0801 train_time:681467ms step_avg:185.13ms\nstep:3692/4500 train_loss:7.9665 train_time:681650ms step_avg:185.13ms\nstep:3693/4500 train_loss:8.1415 train_time:681834ms step_avg:185.13ms\nstep:3694/4500 train_loss:8.1571 train_time:682018ms step_avg:185.13ms\nstep:3695/4500 train_loss:8.1676 train_time:682205ms step_avg:185.13ms\nstep:3696/4500 train_loss:8.1630 train_time:682390ms step_avg:185.13ms\nstep:3697/4500 train_loss:8.0693 train_time:682575ms step_avg:185.13ms\nstep:3698/4500 train_loss:8.1574 train_time:682759ms step_avg:185.13ms\nstep:3699/4500 train_loss:8.0322 train_time:682944ms step_avg:185.13ms\nstep:3700/4500 train_loss:7.8887 train_time:683129ms step_avg:185.13ms\nstep:3701/4500 train_loss:8.0820 train_time:683314ms step_avg:185.13ms\nstep:3702/4500 train_loss:8.0767 train_time:683500ms step_avg:185.13ms\nstep:3703/4500 train_loss:8.0352 train_time:683686ms step_avg:185.13ms\nstep:3704/4500 train_loss:7.9959 train_time:683873ms step_avg:185.13ms\nstep:3705/4500 train_loss:8.0715 train_time:684057ms step_avg:185.13ms\nstep:3706/4500 train_loss:8.1221 train_time:684241ms step_avg:185.13ms\nstep:3707/4500 train_loss:8.0837 train_time:684425ms step_avg:185.13ms\nstep:3708/4500 train_loss:8.0252 train_time:684608ms step_avg:185.13ms\nstep:3709/4500 train_loss:7.9508 train_time:684791ms step_avg:185.13ms\nstep:3710/4500 train_loss:8.0369 train_time:684975ms step_avg:185.13ms\nstep:3711/4500 train_loss:8.1698 train_time:685161ms step_avg:185.13ms\nstep:3712/4500 train_loss:8.1392 train_time:685349ms step_avg:185.13ms\nstep:3713/4500 train_loss:8.1814 train_time:685534ms step_avg:185.13ms\nstep:3714/4500 train_loss:8.1248 train_time:685720ms step_avg:185.13ms\nstep:3715/4500 train_loss:8.0998 train_time:685904ms step_avg:185.13ms\nstep:3716/4500 train_loss:8.0876 train_time:686092ms step_avg:185.13ms\nstep:3717/4500 train_loss:8.1293 train_time:686275ms step_avg:185.13ms\nstep:3718/4500 train_loss:8.0735 train_time:686460ms step_avg:185.13ms\nstep:3719/4500 train_loss:8.1270 train_time:686645ms step_avg:185.13ms\nstep:3720/4500 train_loss:7.9790 train_time:686833ms step_avg:185.13ms\nstep:3721/4500 train_loss:8.1462 train_time:687017ms step_avg:185.13ms\nstep:3722/4500 train_loss:8.1600 train_time:687201ms step_avg:185.13ms\nstep:3723/4500 train_loss:8.1694 train_time:687385ms step_avg:185.13ms\nstep:3724/4500 train_loss:8.1257 train_time:687569ms step_avg:185.13ms\nstep:3725/4500 train_loss:8.2114 train_time:687752ms step_avg:185.13ms\nstep:3726/4500 train_loss:8.0061 train_time:687937ms step_avg:185.13ms\nstep:3727/4500 train_loss:8.1038 train_time:688126ms step_avg:185.13ms\nstep:3728/4500 train_loss:8.2986 train_time:688308ms step_avg:185.13ms\nstep:3729/4500 train_loss:8.1458 train_time:688494ms step_avg:185.13ms\nstep:3730/4500 train_loss:8.0600 train_time:688679ms step_avg:185.13ms\nstep:3731/4500 train_loss:8.1142 train_time:688865ms step_avg:185.13ms\nstep:3732/4500 train_loss:8.1658 train_time:689050ms step_avg:185.13ms\nstep:3733/4500 train_loss:8.0140 train_time:689235ms step_avg:185.13ms\nstep:3734/4500 train_loss:8.1468 train_time:689419ms step_avg:185.13ms\nstep:3735/4500 train_loss:8.1205 train_time:689606ms step_avg:185.13ms\nstep:3736/4500 train_loss:8.0318 train_time:689793ms step_avg:185.13ms\nstep:3737/4500 train_loss:8.1152 train_time:689979ms step_avg:185.13ms\nstep:3738/4500 train_loss:8.1223 train_time:690167ms step_avg:185.13ms\nstep:3739/4500 train_loss:8.2053 train_time:690352ms step_avg:185.13ms\nstep:3740/4500 train_loss:8.1446 train_time:690535ms step_avg:185.13ms\nstep:3741/4500 train_loss:8.1301 train_time:690719ms step_avg:185.13ms\nstep:3742/4500 train_loss:8.0823 train_time:690903ms step_avg:185.13ms\nstep:3743/4500 train_loss:8.1901 train_time:691089ms step_avg:185.13ms\nstep:3744/4500 train_loss:8.1569 train_time:691274ms step_avg:185.13ms\nstep:3745/4500 train_loss:8.2518 train_time:691459ms step_avg:185.13ms\nstep:3746/4500 train_loss:8.1700 train_time:691647ms step_avg:185.13ms\nstep:3747/4500 train_loss:8.0175 train_time:691832ms step_avg:185.13ms\nstep:3748/4500 train_loss:8.2234 train_time:692015ms step_avg:185.13ms\nstep:3749/4500 train_loss:8.1734 train_time:692198ms step_avg:185.13ms\nstep:3750/4500 train_loss:8.0548 train_time:692383ms step_avg:185.13ms\nstep:3751/4500 train_loss:8.1971 train_time:692570ms step_avg:185.13ms\nstep:3752/4500 train_loss:8.1195 train_time:692756ms step_avg:185.13ms\nstep:3753/4500 train_loss:8.0255 train_time:692942ms step_avg:185.13ms\nstep:3754/4500 train_loss:8.1288 train_time:693128ms step_avg:185.13ms\nstep:3755/4500 train_loss:8.1631 train_time:693313ms step_avg:185.13ms\nstep:3756/4500 train_loss:8.1439 train_time:693497ms step_avg:185.13ms\nstep:3757/4500 train_loss:8.0012 train_time:693683ms step_avg:185.13ms\nstep:3758/4500 train_loss:8.0735 train_time:693871ms step_avg:185.13ms\nstep:3759/4500 train_loss:8.1501 train_time:694055ms step_avg:185.13ms\nstep:3760/4500 train_loss:8.0822 train_time:694239ms step_avg:185.13ms\nstep:3761/4500 train_loss:8.0455 train_time:694424ms step_avg:185.13ms\nstep:3762/4500 train_loss:8.1904 train_time:694610ms step_avg:185.13ms\nstep:3763/4500 train_loss:8.0110 train_time:694794ms step_avg:185.13ms\nstep:3764/4500 train_loss:8.1234 train_time:694979ms step_avg:185.13ms\nstep:3765/4500 train_loss:8.0852 train_time:695168ms step_avg:185.13ms\nstep:3766/4500 train_loss:8.1211 train_time:695355ms step_avg:185.13ms\nstep:3767/4500 train_loss:8.0464 train_time:695538ms step_avg:185.13ms\nstep:3768/4500 train_loss:7.9379 train_time:695722ms step_avg:185.13ms\nstep:3769/4500 train_loss:8.0897 train_time:695910ms step_avg:185.13ms\nstep:3770/4500 train_loss:8.1094 train_time:696095ms step_avg:185.13ms\nstep:3771/4500 train_loss:8.2664 train_time:696280ms step_avg:185.13ms\nstep:3772/4500 train_loss:8.0640 train_time:696468ms step_avg:185.13ms\nstep:3773/4500 train_loss:8.0119 train_time:696654ms step_avg:185.13ms\nstep:3774/4500 train_loss:8.1085 train_time:696837ms step_avg:185.13ms\nstep:3775/4500 train_loss:8.0430 train_time:697022ms step_avg:185.13ms\nstep:3776/4500 train_loss:8.0429 train_time:697207ms step_avg:185.13ms\nstep:3777/4500 train_loss:8.1685 train_time:697391ms step_avg:185.13ms\nstep:3778/4500 train_loss:8.1285 train_time:697574ms step_avg:185.13ms\nstep:3779/4500 train_loss:8.1157 train_time:697762ms step_avg:185.13ms\nstep:3780/4500 train_loss:8.1408 train_time:697948ms step_avg:185.13ms\nstep:3781/4500 train_loss:8.0090 train_time:698132ms step_avg:185.13ms\nstep:3782/4500 train_loss:8.0234 train_time:698318ms step_avg:185.13ms\nstep:3783/4500 train_loss:8.1008 train_time:698502ms step_avg:185.13ms\nstep:3784/4500 train_loss:8.4333 train_time:698686ms step_avg:185.13ms\nstep:3785/4500 train_loss:8.1243 train_time:698868ms step_avg:185.13ms\nstep:3786/4500 train_loss:8.1629 train_time:699054ms step_avg:185.13ms\nstep:3787/4500 train_loss:8.1388 train_time:699240ms step_avg:185.13ms\nstep:3788/4500 train_loss:8.1836 train_time:699425ms step_avg:185.13ms\nstep:3789/4500 train_loss:7.9790 train_time:699610ms step_avg:185.13ms\nstep:3790/4500 train_loss:8.5159 train_time:699794ms step_avg:185.13ms\nstep:3791/4500 train_loss:8.0704 train_time:699978ms step_avg:185.13ms\nstep:3792/4500 train_loss:8.1511 train_time:700163ms step_avg:185.13ms\nstep:3793/4500 train_loss:8.0935 train_time:700346ms step_avg:185.13ms\nstep:3794/4500 train_loss:8.0723 train_time:700532ms step_avg:185.13ms\nstep:3795/4500 train_loss:8.3597 train_time:700718ms step_avg:185.13ms\nstep:3796/4500 train_loss:8.1365 train_time:700906ms step_avg:185.13ms\nstep:3797/4500 train_loss:8.1671 train_time:701094ms step_avg:185.13ms\nstep:3798/4500 train_loss:8.0720 train_time:701280ms step_avg:185.13ms\nstep:3799/4500 train_loss:8.0818 train_time:701466ms step_avg:185.13ms\nstep:3800/4500 train_loss:7.9365 train_time:701651ms step_avg:185.13ms\nstep:3801/4500 train_loss:8.1706 train_time:701835ms step_avg:185.13ms\nstep:3802/4500 train_loss:8.1624 train_time:702019ms step_avg:185.13ms\nstep:3803/4500 train_loss:8.0634 train_time:702205ms step_avg:185.13ms\nstep:3804/4500 train_loss:8.0344 train_time:702389ms step_avg:185.13ms\nstep:3805/4500 train_loss:8.0135 train_time:702573ms step_avg:185.13ms\nstep:3806/4500 train_loss:8.0805 train_time:702757ms step_avg:185.13ms\nstep:3807/4500 train_loss:7.9430 train_time:702944ms step_avg:185.13ms\nstep:3808/4500 train_loss:8.0788 train_time:703129ms step_avg:185.13ms\nstep:3809/4500 train_loss:8.2917 train_time:703312ms step_avg:185.13ms\nstep:3810/4500 train_loss:8.1260 train_time:703730ms step_avg:185.19ms\nstep:3811/4500 train_loss:8.1972 train_time:703912ms step_avg:185.19ms\nstep:3812/4500 train_loss:8.0839 train_time:704095ms step_avg:185.19ms\nstep:3813/4500 train_loss:8.0713 train_time:704279ms step_avg:185.19ms\nstep:3814/4500 train_loss:8.1068 train_time:704463ms step_avg:185.19ms\nstep:3815/4500 train_loss:8.1311 train_time:704648ms step_avg:185.19ms\nstep:3816/4500 train_loss:7.9963 train_time:704833ms step_avg:185.19ms\nstep:3817/4500 train_loss:8.0668 train_time:705018ms step_avg:185.19ms\nstep:3818/4500 train_loss:7.9740 train_time:705202ms step_avg:185.19ms\nstep:3819/4500 train_loss:8.0768 train_time:705387ms step_avg:185.19ms\nstep:3820/4500 train_loss:8.0011 train_time:705574ms step_avg:185.19ms\nstep:3821/4500 train_loss:8.0541 train_time:705760ms step_avg:185.19ms\nstep:3822/4500 train_loss:8.1838 train_time:705945ms step_avg:185.19ms\nstep:3823/4500 train_loss:8.0580 train_time:706130ms step_avg:185.19ms\nstep:3824/4500 train_loss:7.9470 train_time:706315ms step_avg:185.19ms\nstep:3825/4500 train_loss:7.9450 train_time:706499ms step_avg:185.19ms\nstep:3826/4500 train_loss:8.1257 train_time:706684ms step_avg:185.19ms\nstep:3827/4500 train_loss:7.9946 train_time:706868ms step_avg:185.19ms\nstep:3828/4500 train_loss:8.1003 train_time:707051ms step_avg:185.19ms\nstep:3829/4500 train_loss:8.1685 train_time:707234ms step_avg:185.19ms\nstep:3830/4500 train_loss:8.1182 train_time:707419ms step_avg:185.19ms\nstep:3831/4500 train_loss:8.2815 train_time:707608ms step_avg:185.19ms\nstep:3832/4500 train_loss:7.9852 train_time:707792ms step_avg:185.19ms\nstep:3833/4500 train_loss:8.2308 train_time:707975ms step_avg:185.19ms\nstep:3834/4500 train_loss:8.1068 train_time:708159ms step_avg:185.19ms\nstep:3835/4500 train_loss:7.9914 train_time:708348ms step_avg:185.19ms\nstep:3836/4500 train_loss:8.1296 train_time:708531ms step_avg:185.19ms\nstep:3837/4500 train_loss:8.1514 train_time:708714ms step_avg:185.19ms\nstep:3838/4500 train_loss:8.2721 train_time:708899ms step_avg:185.19ms\nstep:3839/4500 train_loss:8.0889 train_time:709084ms step_avg:185.19ms\nstep:3840/4500 train_loss:8.0705 train_time:709270ms step_avg:185.19ms\nstep:3841/4500 train_loss:8.1685 train_time:709456ms step_avg:185.19ms\nstep:3842/4500 train_loss:8.1268 train_time:709646ms step_avg:185.19ms\nstep:3843/4500 train_loss:8.2155 train_time:709835ms step_avg:185.19ms\nstep:3844/4500 train_loss:7.9931 train_time:710014ms step_avg:185.19ms\nstep:3845/4500 train_loss:8.3009 train_time:710196ms step_avg:185.19ms\nstep:3846/4500 train_loss:8.1428 train_time:710383ms step_avg:185.19ms\nstep:3847/4500 train_loss:8.1212 train_time:710564ms step_avg:185.19ms\nstep:3848/4500 train_loss:7.9311 train_time:710749ms step_avg:185.19ms\nstep:3849/4500 train_loss:8.1734 train_time:710938ms step_avg:185.19ms\nstep:3850/4500 train_loss:8.1361 train_time:711124ms step_avg:185.19ms\nstep:3851/4500 train_loss:7.9994 train_time:711313ms step_avg:185.19ms\nstep:3852/4500 train_loss:8.2053 train_time:711498ms step_avg:185.19ms\nstep:3853/4500 train_loss:8.1727 train_time:711684ms step_avg:185.19ms\nstep:3854/4500 train_loss:8.0432 train_time:711871ms step_avg:185.19ms\nstep:3855/4500 train_loss:8.1642 train_time:712059ms step_avg:185.19ms\nstep:3856/4500 train_loss:8.1195 train_time:712246ms step_avg:185.19ms\nstep:3857/4500 train_loss:8.1268 train_time:712432ms step_avg:185.19ms\nstep:3858/4500 train_loss:8.0443 train_time:712616ms step_avg:185.19ms\nstep:3859/4500 train_loss:8.0388 train_time:712801ms step_avg:185.19ms\nstep:3860/4500 train_loss:8.0724 train_time:712987ms step_avg:185.19ms\nstep:3861/4500 train_loss:8.1367 train_time:713174ms step_avg:185.19ms\nstep:3862/4500 train_loss:8.0910 train_time:713357ms step_avg:185.19ms\nstep:3863/4500 train_loss:8.1147 train_time:713541ms step_avg:185.19ms\nstep:3864/4500 train_loss:8.0573 train_time:713725ms step_avg:185.19ms\nstep:3865/4500 train_loss:7.9883 train_time:713909ms step_avg:185.19ms\nstep:3866/4500 train_loss:8.1183 train_time:714092ms step_avg:185.19ms\nstep:3867/4500 train_loss:8.1046 train_time:714276ms step_avg:185.19ms\nstep:3868/4500 train_loss:8.0524 train_time:714459ms step_avg:185.19ms\nstep:3869/4500 train_loss:8.0498 train_time:714643ms step_avg:185.19ms\nstep:3870/4500 train_loss:8.0327 train_time:714827ms step_avg:185.19ms\nstep:3871/4500 train_loss:8.0678 train_time:715010ms step_avg:185.19ms\nstep:3872/4500 train_loss:8.0799 train_time:715193ms step_avg:185.19ms\nstep:3873/4500 train_loss:8.1038 train_time:715377ms step_avg:185.19ms\nstep:3874/4500 train_loss:8.2034 train_time:715564ms step_avg:185.19ms\nstep:3875/4500 train_loss:7.9560 train_time:715748ms step_avg:185.19ms\nstep:3876/4500 train_loss:8.1165 train_time:715932ms step_avg:185.19ms\nstep:3877/4500 train_loss:8.1233 train_time:716118ms step_avg:185.19ms\nstep:3878/4500 train_loss:8.1511 train_time:716303ms step_avg:185.19ms\nstep:3879/4500 train_loss:8.0747 train_time:716488ms step_avg:185.19ms\nstep:3880/4500 train_loss:8.1703 train_time:716672ms step_avg:185.19ms\nstep:3881/4500 train_loss:8.0705 train_time:716857ms step_avg:185.19ms\nstep:3882/4500 train_loss:8.1719 train_time:717042ms step_avg:185.19ms\nstep:3883/4500 train_loss:8.0479 train_time:717230ms step_avg:185.19ms\nstep:3884/4500 train_loss:8.0850 train_time:717412ms step_avg:185.19ms\nstep:3885/4500 train_loss:8.0681 train_time:717598ms step_avg:185.19ms\nstep:3886/4500 train_loss:8.0343 train_time:717782ms step_avg:185.19ms\nstep:3887/4500 train_loss:8.0708 train_time:717968ms step_avg:185.19ms\nstep:3888/4500 train_loss:8.1129 train_time:718153ms step_avg:185.19ms\nstep:3889/4500 train_loss:8.2263 train_time:718338ms step_avg:185.19ms\nstep:3890/4500 train_loss:8.1418 train_time:718526ms step_avg:185.19ms\nstep:3891/4500 train_loss:8.0894 train_time:718712ms step_avg:185.19ms\nstep:3892/4500 train_loss:8.1214 train_time:718897ms step_avg:185.19ms\nstep:3893/4500 train_loss:8.1104 train_time:719082ms step_avg:185.19ms\nstep:3894/4500 train_loss:8.1651 train_time:719265ms step_avg:185.19ms\nstep:3895/4500 train_loss:7.9814 train_time:719454ms step_avg:185.19ms\nstep:3896/4500 train_loss:8.1777 train_time:719637ms step_avg:185.19ms\nstep:3897/4500 train_loss:8.1861 train_time:719822ms step_avg:185.19ms\nstep:3898/4500 train_loss:8.1471 train_time:720007ms step_avg:185.19ms\nstep:3899/4500 train_loss:7.9983 train_time:720190ms step_avg:185.19ms\nstep:3900/4500 train_loss:8.1763 train_time:720373ms step_avg:185.19ms\nstep:3901/4500 train_loss:8.1270 train_time:720556ms step_avg:185.19ms\nstep:3902/4500 train_loss:8.1042 train_time:720741ms step_avg:185.19ms\nstep:3903/4500 train_loss:8.1259 train_time:720926ms step_avg:185.19ms\nstep:3904/4500 train_loss:8.1397 train_time:721110ms step_avg:185.18ms\nstep:3905/4500 train_loss:8.0214 train_time:721295ms step_avg:185.18ms\nstep:3906/4500 train_loss:8.1031 train_time:721481ms step_avg:185.19ms\nstep:3907/4500 train_loss:8.0107 train_time:721665ms step_avg:185.18ms\nstep:3908/4500 train_loss:8.0674 train_time:721849ms step_avg:185.18ms\nstep:3909/4500 train_loss:8.0693 train_time:722033ms step_avg:185.18ms\nstep:3910/4500 train_loss:8.2255 train_time:722217ms step_avg:185.18ms\nstep:3911/4500 train_loss:8.2242 train_time:722402ms step_avg:185.18ms\nstep:3912/4500 train_loss:8.1769 train_time:722590ms step_avg:185.18ms\nstep:3913/4500 train_loss:8.1306 train_time:722776ms step_avg:185.18ms\nstep:3914/4500 train_loss:8.0092 train_time:722960ms step_avg:185.18ms\nstep:3915/4500 train_loss:8.1255 train_time:723143ms step_avg:185.18ms\nstep:3916/4500 train_loss:8.1854 train_time:723329ms step_avg:185.18ms\nstep:3917/4500 train_loss:8.0147 train_time:723512ms step_avg:185.18ms\nstep:3918/4500 train_loss:8.1517 train_time:723695ms step_avg:185.18ms\nstep:3919/4500 train_loss:8.0872 train_time:723882ms step_avg:185.18ms\nstep:3920/4500 train_loss:8.1722 train_time:724067ms step_avg:185.18ms\nstep:3921/4500 train_loss:8.1423 train_time:724251ms step_avg:185.18ms\nstep:3922/4500 train_loss:8.0178 train_time:724435ms step_avg:185.18ms\nstep:3923/4500 train_loss:8.0156 train_time:724620ms step_avg:185.18ms\nstep:3924/4500 train_loss:8.3139 train_time:724805ms step_avg:185.18ms\nstep:3925/4500 train_loss:8.0567 train_time:724991ms step_avg:185.18ms\nstep:3926/4500 train_loss:7.9119 train_time:725176ms step_avg:185.18ms\nstep:3927/4500 train_loss:8.0275 train_time:725361ms step_avg:185.18ms\nstep:3928/4500 train_loss:8.1063 train_time:725547ms step_avg:185.18ms\nstep:3929/4500 train_loss:8.1637 train_time:725730ms step_avg:185.18ms\nstep:3930/4500 train_loss:7.9990 train_time:725913ms step_avg:185.18ms\nstep:3931/4500 train_loss:8.1414 train_time:726098ms step_avg:185.18ms\nstep:3932/4500 train_loss:8.0865 train_time:726283ms step_avg:185.18ms\nstep:3933/4500 train_loss:8.1611 train_time:726469ms step_avg:185.18ms\nstep:3934/4500 train_loss:8.1464 train_time:726653ms step_avg:185.18ms\nstep:3935/4500 train_loss:8.0903 train_time:726837ms step_avg:185.18ms\nstep:3936/4500 train_loss:8.1833 train_time:727022ms step_avg:185.18ms\nstep:3937/4500 train_loss:8.1182 train_time:727206ms step_avg:185.18ms\nstep:3938/4500 train_loss:8.1470 train_time:727391ms step_avg:185.18ms\nstep:3939/4500 train_loss:8.0349 train_time:727576ms step_avg:185.18ms\nstep:3940/4500 train_loss:8.3413 train_time:727760ms step_avg:185.18ms\nstep:3941/4500 train_loss:8.0939 train_time:727945ms step_avg:185.18ms\nstep:3942/4500 train_loss:8.1567 train_time:728133ms step_avg:185.18ms\nstep:3943/4500 train_loss:8.1288 train_time:728317ms step_avg:185.18ms\nstep:3944/4500 train_loss:8.0536 train_time:728503ms step_avg:185.18ms\nstep:3945/4500 train_loss:8.0845 train_time:728690ms step_avg:185.18ms\nstep:3946/4500 train_loss:8.0977 train_time:728874ms step_avg:185.18ms\nstep:3947/4500 train_loss:7.9550 train_time:729060ms step_avg:185.18ms\nstep:3948/4500 train_loss:8.0969 train_time:729246ms step_avg:185.18ms\nstep:3949/4500 train_loss:8.1862 train_time:729432ms step_avg:185.18ms\nstep:3950/4500 train_loss:8.0510 train_time:729616ms step_avg:185.18ms\nstep:3951/4500 train_loss:8.4405 train_time:729800ms step_avg:185.18ms\nstep:3952/4500 train_loss:8.0964 train_time:729985ms step_avg:185.18ms\nstep:3953/4500 train_loss:8.1338 train_time:730171ms step_avg:185.18ms\nstep:3954/4500 train_loss:8.2340 train_time:730355ms step_avg:185.18ms\nstep:3955/4500 train_loss:8.0892 train_time:730541ms step_avg:185.18ms\nstep:3956/4500 train_loss:8.1094 train_time:730727ms step_avg:185.18ms\nstep:3957/4500 train_loss:8.1464 train_time:730910ms step_avg:185.18ms\nstep:3958/4500 train_loss:7.9678 train_time:731094ms step_avg:185.18ms\nstep:3959/4500 train_loss:8.1029 train_time:731278ms step_avg:185.18ms\nstep:3960/4500 train_loss:8.1765 train_time:731462ms step_avg:185.18ms\nstep:3961/4500 train_loss:8.1472 train_time:731648ms step_avg:185.18ms\nstep:3962/4500 train_loss:7.9990 train_time:731834ms step_avg:185.18ms\nstep:3963/4500 train_loss:7.9679 train_time:732019ms step_avg:185.18ms\nstep:3964/4500 train_loss:8.1139 train_time:732204ms step_avg:185.18ms\nstep:3965/4500 train_loss:8.1712 train_time:732388ms step_avg:185.18ms\nstep:3966/4500 train_loss:8.0370 train_time:732572ms step_avg:185.18ms\nstep:3967/4500 train_loss:8.1098 train_time:732756ms step_avg:185.18ms\nstep:3968/4500 train_loss:7.9558 train_time:732941ms step_avg:185.18ms\nstep:3969/4500 train_loss:8.0227 train_time:733130ms step_avg:185.18ms\nstep:3970/4500 train_loss:8.1135 train_time:733315ms step_avg:185.18ms\nstep:3971/4500 train_loss:8.1326 train_time:733499ms step_avg:185.18ms\nstep:3972/4500 train_loss:8.0860 train_time:733684ms step_avg:185.18ms\nstep:3973/4500 train_loss:7.9660 train_time:733870ms step_avg:185.18ms\nstep:3974/4500 train_loss:8.1183 train_time:734054ms step_avg:185.18ms\nstep:3975/4500 train_loss:8.0649 train_time:734240ms step_avg:185.18ms\nstep:3976/4500 train_loss:7.9723 train_time:734427ms step_avg:185.18ms\nstep:3977/4500 train_loss:8.0874 train_time:734615ms step_avg:185.18ms\nstep:3978/4500 train_loss:7.7671 train_time:734797ms step_avg:185.18ms\nstep:3979/4500 train_loss:7.9727 train_time:734982ms step_avg:185.18ms\nstep:3980/4500 train_loss:8.2184 train_time:735168ms step_avg:185.18ms\nstep:3981/4500 train_loss:8.0466 train_time:735352ms step_avg:185.18ms\nstep:3982/4500 train_loss:8.1222 train_time:735536ms step_avg:185.18ms\nstep:3983/4500 train_loss:8.1603 train_time:735724ms step_avg:185.18ms\nstep:3984/4500 train_loss:8.0276 train_time:735909ms step_avg:185.18ms\nstep:3985/4500 train_loss:8.1664 train_time:736092ms step_avg:185.18ms\nstep:3986/4500 train_loss:8.1209 train_time:736276ms step_avg:185.18ms\nstep:3987/4500 train_loss:8.0571 train_time:736460ms step_avg:185.18ms\nstep:3988/4500 train_loss:8.1000 train_time:736646ms step_avg:185.18ms\nstep:3989/4500 train_loss:7.9738 train_time:736831ms step_avg:185.18ms\nstep:3990/4500 train_loss:8.0435 train_time:737018ms step_avg:185.18ms\nstep:3991/4500 train_loss:8.1448 train_time:737206ms step_avg:185.18ms\nstep:3992/4500 train_loss:8.0100 train_time:737391ms step_avg:185.18ms\nstep:3993/4500 train_loss:8.1218 train_time:737577ms step_avg:185.18ms\nstep:3994/4500 train_loss:8.1508 train_time:737761ms step_avg:185.18ms\nstep:3995/4500 train_loss:8.0345 train_time:737947ms step_avg:185.18ms\nstep:3996/4500 train_loss:8.3232 train_time:738131ms step_avg:185.18ms\nstep:3997/4500 train_loss:8.0119 train_time:738317ms step_avg:185.18ms\nstep:3998/4500 train_loss:8.0598 train_time:738503ms step_avg:185.18ms\nstep:3999/4500 train_loss:8.0967 train_time:738688ms step_avg:185.18ms\nstep:4000/4500 train_loss:8.0133 train_time:738872ms step_avg:185.18ms\nstep:4001/4500 train_loss:8.1348 train_time:739057ms step_avg:185.18ms\nstep:4002/4500 train_loss:8.1326 train_time:739246ms step_avg:185.18ms\nstep:4003/4500 train_loss:8.0527 train_time:739432ms step_avg:185.18ms\nstep:4004/4500 train_loss:8.1364 train_time:739617ms step_avg:185.18ms\nstep:4005/4500 train_loss:8.1321 train_time:739801ms step_avg:185.18ms\nstep:4006/4500 train_loss:7.9711 train_time:739992ms step_avg:185.18ms\nstep:4007/4500 train_loss:8.2269 train_time:740171ms step_avg:185.18ms\nstep:4008/4500 train_loss:8.0659 train_time:740353ms step_avg:185.18ms\nstep:4009/4500 train_loss:8.2010 train_time:740536ms step_avg:185.18ms\nstep:4010/4500 train_loss:8.1574 train_time:740722ms step_avg:185.18ms\nstep:4011/4500 train_loss:8.1403 train_time:740906ms step_avg:185.18ms\nstep:4012/4500 train_loss:8.1341 train_time:741093ms step_avg:185.18ms\nstep:4013/4500 train_loss:8.0968 train_time:741277ms step_avg:185.18ms\nstep:4014/4500 train_loss:8.1444 train_time:741463ms step_avg:185.18ms\nstep:4015/4500 train_loss:8.0559 train_time:741648ms step_avg:185.18ms\nstep:4016/4500 train_loss:8.0953 train_time:741833ms step_avg:185.18ms\nstep:4017/4500 train_loss:8.0453 train_time:742018ms step_avg:185.18ms\nstep:4018/4500 train_loss:8.0937 train_time:742203ms step_avg:185.18ms\nstep:4019/4500 train_loss:8.1938 train_time:742389ms step_avg:185.18ms\nstep:4020/4500 train_loss:8.0924 train_time:742572ms step_avg:185.18ms\nstep:4021/4500 train_loss:8.0762 train_time:742756ms step_avg:185.18ms\nstep:4022/4500 train_loss:8.0914 train_time:742940ms step_avg:185.18ms\nstep:4023/4500 train_loss:8.1259 train_time:743124ms step_avg:185.18ms\nstep:4024/4500 train_loss:8.1971 train_time:743308ms step_avg:185.18ms\nstep:4025/4500 train_loss:8.1350 train_time:743493ms step_avg:185.18ms\nstep:4026/4500 train_loss:8.0060 train_time:743676ms step_avg:185.18ms\nstep:4027/4500 train_loss:8.3867 train_time:743864ms step_avg:185.18ms\nstep:4028/4500 train_loss:8.0242 train_time:744052ms step_avg:185.18ms\nstep:4029/4500 train_loss:8.1267 train_time:744236ms step_avg:185.18ms\nstep:4030/4500 train_loss:8.0536 train_time:744425ms step_avg:185.18ms\nstep:4031/4500 train_loss:8.1185 train_time:744606ms step_avg:185.18ms\nstep:4032/4500 train_loss:8.0424 train_time:744791ms step_avg:185.18ms\nstep:4033/4500 train_loss:8.0717 train_time:744976ms step_avg:185.18ms\nstep:4034/4500 train_loss:8.1067 train_time:745161ms step_avg:185.18ms\nstep:4035/4500 train_loss:8.0539 train_time:745345ms step_avg:185.18ms\nstep:4036/4500 train_loss:8.0058 train_time:745532ms step_avg:185.18ms\nstep:4037/4500 train_loss:8.2474 train_time:745715ms step_avg:185.18ms\nstep:4038/4500 train_loss:8.0246 train_time:745899ms step_avg:185.18ms\nstep:4039/4500 train_loss:8.1755 train_time:746085ms step_avg:185.18ms\nstep:4040/4500 train_loss:8.0732 train_time:746270ms step_avg:185.18ms\nstep:4041/4500 train_loss:8.1609 train_time:746454ms step_avg:185.18ms\nstep:4042/4500 train_loss:8.1077 train_time:746637ms step_avg:185.18ms\nstep:4043/4500 train_loss:8.1104 train_time:746822ms step_avg:185.18ms\nstep:4044/4500 train_loss:8.1131 train_time:747005ms step_avg:185.18ms\nstep:4045/4500 train_loss:8.1468 train_time:747189ms step_avg:185.18ms\nstep:4046/4500 train_loss:8.1282 train_time:747374ms step_avg:185.18ms\nstep:4047/4500 train_loss:8.2293 train_time:747560ms step_avg:185.18ms\nstep:4048/4500 train_loss:8.0334 train_time:747747ms step_avg:185.18ms\nstep:4049/4500 train_loss:8.0675 train_time:747933ms step_avg:185.18ms\nstep:4050/4500 train_loss:8.0199 train_time:748119ms step_avg:185.18ms\nstep:4051/4500 train_loss:8.1066 train_time:748307ms step_avg:185.18ms\nstep:4052/4500 train_loss:8.0229 train_time:748493ms step_avg:185.18ms\nstep:4053/4500 train_loss:8.1349 train_time:748680ms step_avg:185.18ms\nstep:4054/4500 train_loss:7.9804 train_time:748864ms step_avg:185.18ms\nstep:4055/4500 train_loss:8.2068 train_time:749049ms step_avg:185.18ms\nstep:4056/4500 train_loss:7.9576 train_time:749233ms step_avg:185.18ms\nstep:4057/4500 train_loss:7.9585 train_time:749417ms step_avg:185.18ms\nstep:4058/4500 train_loss:8.1429 train_time:749604ms step_avg:185.18ms\nstep:4059/4500 train_loss:8.0981 train_time:749791ms step_avg:185.18ms\nstep:4060/4500 train_loss:8.1437 train_time:749974ms step_avg:185.18ms\nstep:4061/4500 train_loss:8.2056 train_time:750159ms step_avg:185.18ms\nstep:4062/4500 train_loss:8.0687 train_time:750341ms step_avg:185.18ms\nstep:4063/4500 train_loss:8.1341 train_time:750525ms step_avg:185.18ms\nstep:4064/4500 train_loss:8.2223 train_time:750709ms step_avg:185.18ms\nstep:4065/4500 train_loss:7.9607 train_time:750896ms step_avg:185.18ms\nstep:4066/4500 train_loss:8.1223 train_time:751081ms step_avg:185.18ms\nstep:4067/4500 train_loss:8.1175 train_time:751269ms step_avg:185.18ms\nstep:4068/4500 train_loss:8.0415 train_time:751454ms step_avg:185.18ms\nstep:4069/4500 train_loss:8.2199 train_time:751639ms step_avg:185.18ms\nstep:4070/4500 train_loss:8.0757 train_time:751823ms step_avg:185.18ms\nstep:4071/4500 train_loss:8.0921 train_time:752006ms step_avg:185.18ms\nstep:4072/4500 train_loss:8.1579 train_time:752190ms step_avg:185.18ms\nstep:4073/4500 train_loss:7.9827 train_time:752374ms step_avg:185.18ms\nstep:4074/4500 train_loss:8.1373 train_time:752561ms step_avg:185.18ms\nstep:4075/4500 train_loss:8.0879 train_time:752748ms step_avg:185.18ms\nstep:4076/4500 train_loss:8.2507 train_time:752934ms step_avg:185.18ms\nstep:4077/4500 train_loss:8.2081 train_time:753119ms step_avg:185.18ms\nstep:4078/4500 train_loss:8.0319 train_time:753303ms step_avg:185.18ms\nstep:4079/4500 train_loss:8.1330 train_time:753487ms step_avg:185.18ms\nstep:4080/4500 train_loss:8.0971 train_time:753672ms step_avg:185.18ms\nstep:4081/4500 train_loss:8.0756 train_time:753856ms step_avg:185.18ms\nstep:4082/4500 train_loss:8.0941 train_time:754044ms step_avg:185.18ms\nstep:4083/4500 train_loss:8.0474 train_time:754229ms step_avg:185.18ms\nstep:4084/4500 train_loss:8.0885 train_time:754413ms step_avg:185.18ms\nstep:4085/4500 train_loss:8.1678 train_time:754598ms step_avg:185.18ms\nstep:4086/4500 train_loss:8.0898 train_time:754786ms step_avg:185.18ms\nstep:4087/4500 train_loss:8.0679 train_time:754969ms step_avg:185.18ms\nstep:4088/4500 train_loss:8.0919 train_time:755154ms step_avg:185.18ms\nstep:4089/4500 train_loss:8.1587 train_time:755339ms step_avg:185.18ms\nstep:4090/4500 train_loss:8.1033 train_time:755522ms step_avg:185.18ms\nstep:4091/4500 train_loss:8.0503 train_time:755710ms step_avg:185.18ms\nstep:4092/4500 train_loss:8.1479 train_time:755894ms step_avg:185.18ms\nstep:4093/4500 train_loss:8.0000 train_time:756079ms step_avg:185.18ms\nstep:4094/4500 train_loss:8.0676 train_time:756263ms step_avg:185.18ms\nstep:4095/4500 train_loss:8.1825 train_time:756447ms step_avg:185.18ms\nstep:4096/4500 train_loss:8.0976 train_time:756629ms step_avg:185.18ms\nstep:4097/4500 train_loss:8.1038 train_time:756812ms step_avg:185.18ms\nstep:4098/4500 train_loss:7.9359 train_time:756998ms step_avg:185.18ms\nstep:4099/4500 train_loss:8.1585 train_time:757182ms step_avg:185.18ms\nstep:4100/4500 train_loss:8.0812 train_time:757369ms step_avg:185.18ms\nstep:4101/4500 train_loss:8.0383 train_time:757555ms step_avg:185.18ms\nstep:4102/4500 train_loss:8.0527 train_time:757741ms step_avg:185.18ms\nstep:4103/4500 train_loss:8.0388 train_time:757925ms step_avg:185.18ms\nstep:4104/4500 train_loss:7.9843 train_time:758109ms step_avg:185.18ms\nstep:4105/4500 train_loss:8.0905 train_time:758294ms step_avg:185.18ms\nstep:4106/4500 train_loss:8.1047 train_time:758479ms step_avg:185.18ms\nstep:4107/4500 train_loss:8.0628 train_time:758666ms step_avg:185.18ms\nstep:4108/4500 train_loss:8.2021 train_time:758849ms step_avg:185.18ms\nstep:4109/4500 train_loss:8.0844 train_time:759031ms step_avg:185.17ms\nstep:4110/4500 train_loss:8.0644 train_time:759216ms step_avg:185.17ms\nstep:4111/4500 train_loss:8.0904 train_time:759401ms step_avg:185.17ms\nstep:4112/4500 train_loss:8.3074 train_time:759586ms step_avg:185.17ms\nstep:4113/4500 train_loss:8.0019 train_time:759770ms step_avg:185.17ms\nstep:4114/4500 train_loss:8.0068 train_time:759954ms step_avg:185.17ms\nstep:4115/4500 train_loss:8.0802 train_time:760139ms step_avg:185.17ms\nstep:4116/4500 train_loss:8.0834 train_time:760325ms step_avg:185.17ms\nstep:4117/4500 train_loss:7.9553 train_time:760508ms step_avg:185.17ms\nstep:4118/4500 train_loss:8.1602 train_time:760694ms step_avg:185.17ms\nstep:4119/4500 train_loss:8.1107 train_time:760878ms step_avg:185.17ms\nstep:4120/4500 train_loss:7.9789 train_time:761066ms step_avg:185.17ms\nstep:4121/4500 train_loss:8.1142 train_time:761250ms step_avg:185.17ms\nstep:4122/4500 train_loss:8.0016 train_time:761433ms step_avg:185.17ms\nstep:4123/4500 train_loss:7.9988 train_time:761618ms step_avg:185.17ms\nstep:4124/4500 train_loss:8.0907 train_time:761805ms step_avg:185.17ms\nstep:4125/4500 train_loss:8.0777 train_time:761989ms step_avg:185.17ms\nstep:4126/4500 train_loss:8.1620 train_time:762173ms step_avg:185.17ms\nstep:4127/4500 train_loss:7.8929 train_time:762357ms step_avg:185.17ms\nstep:4128/4500 train_loss:7.9670 train_time:762541ms step_avg:185.17ms\nstep:4129/4500 train_loss:8.0696 train_time:762727ms step_avg:185.17ms\nstep:4130/4500 train_loss:8.0868 train_time:762912ms step_avg:185.17ms\nstep:4131/4500 train_loss:8.0569 train_time:763099ms step_avg:185.17ms\nstep:4132/4500 train_loss:8.1936 train_time:763288ms step_avg:185.17ms\nstep:4133/4500 train_loss:8.0231 train_time:763471ms step_avg:185.17ms\nstep:4134/4500 train_loss:8.0353 train_time:763654ms step_avg:185.17ms\nstep:4135/4500 train_loss:8.1443 train_time:763836ms step_avg:185.17ms\nstep:4136/4500 train_loss:8.0176 train_time:764020ms step_avg:185.17ms\nstep:4137/4500 train_loss:8.0858 train_time:764204ms step_avg:185.17ms\nstep:4138/4500 train_loss:8.1769 train_time:764387ms step_avg:185.17ms\nstep:4139/4500 train_loss:8.1043 train_time:764573ms step_avg:185.17ms\nstep:4140/4500 train_loss:8.0408 train_time:764759ms step_avg:185.17ms\nstep:4141/4500 train_loss:8.1247 train_time:764944ms step_avg:185.17ms\nstep:4142/4500 train_loss:8.1860 train_time:765127ms step_avg:185.17ms\nstep:4143/4500 train_loss:8.0968 train_time:765311ms step_avg:185.17ms\nstep:4144/4500 train_loss:8.0485 train_time:765493ms step_avg:185.17ms\nstep:4145/4500 train_loss:8.0380 train_time:765679ms step_avg:185.17ms\nstep:4146/4500 train_loss:7.9999 train_time:765868ms step_avg:185.17ms\nstep:4147/4500 train_loss:8.0819 train_time:766052ms step_avg:185.17ms\nstep:4148/4500 train_loss:7.9728 train_time:766238ms step_avg:185.17ms\nstep:4149/4500 train_loss:7.9860 train_time:766424ms step_avg:185.17ms\nstep:4150/4500 train_loss:7.9864 train_time:766610ms step_avg:185.17ms\nstep:4151/4500 train_loss:8.0747 train_time:766794ms step_avg:185.17ms\nstep:4152/4500 train_loss:8.1022 train_time:766979ms step_avg:185.17ms\nstep:4153/4500 train_loss:8.0472 train_time:767163ms step_avg:185.17ms\nstep:4154/4500 train_loss:8.0652 train_time:767346ms step_avg:185.17ms\nstep:4155/4500 train_loss:8.2019 train_time:767530ms step_avg:185.17ms\nstep:4156/4500 train_loss:8.1852 train_time:767714ms step_avg:185.17ms\nstep:4157/4500 train_loss:8.0778 train_time:767900ms step_avg:185.17ms\nstep:4158/4500 train_loss:7.9930 train_time:768088ms step_avg:185.17ms\nstep:4159/4500 train_loss:8.0614 train_time:768272ms step_avg:185.17ms\nstep:4160/4500 train_loss:8.0679 train_time:768456ms step_avg:185.17ms\nstep:4161/4500 train_loss:7.9993 train_time:768640ms step_avg:185.17ms\nstep:4162/4500 train_loss:7.9882 train_time:768824ms step_avg:185.17ms\nstep:4163/4500 train_loss:8.0521 train_time:769007ms step_avg:185.17ms\nstep:4164/4500 train_loss:8.0350 train_time:769193ms step_avg:185.17ms\nstep:4165/4500 train_loss:7.9428 train_time:769377ms step_avg:185.17ms\nstep:4166/4500 train_loss:8.1120 train_time:769563ms step_avg:185.17ms\nstep:4167/4500 train_loss:8.0028 train_time:769748ms step_avg:185.17ms\nstep:4168/4500 train_loss:8.1119 train_time:769933ms step_avg:185.17ms\nstep:4169/4500 train_loss:8.0452 train_time:770119ms step_avg:185.17ms\nstep:4170/4500 train_loss:8.2539 train_time:770305ms step_avg:185.17ms\nstep:4171/4500 train_loss:8.0572 train_time:770491ms step_avg:185.17ms\nstep:4172/4500 train_loss:7.9088 train_time:770675ms step_avg:185.17ms\nstep:4173/4500 train_loss:8.1908 train_time:770859ms step_avg:185.17ms\nstep:4174/4500 train_loss:8.0175 train_time:771046ms step_avg:185.17ms\nstep:4175/4500 train_loss:8.0888 train_time:771231ms step_avg:185.17ms\nstep:4176/4500 train_loss:8.1043 train_time:771415ms step_avg:185.17ms\nstep:4177/4500 train_loss:8.1646 train_time:771599ms step_avg:185.17ms\nstep:4178/4500 train_loss:8.1106 train_time:771789ms step_avg:185.17ms\nstep:4179/4500 train_loss:8.1457 train_time:771972ms step_avg:185.17ms\nstep:4180/4500 train_loss:7.9886 train_time:772155ms step_avg:185.17ms\nstep:4181/4500 train_loss:8.0371 train_time:772341ms step_avg:185.17ms\nstep:4182/4500 train_loss:8.0542 train_time:772530ms step_avg:185.17ms\nstep:4183/4500 train_loss:7.9618 train_time:772712ms step_avg:185.17ms\nstep:4184/4500 train_loss:7.9893 train_time:772902ms step_avg:185.17ms\nstep:4185/4500 train_loss:8.0049 train_time:773084ms step_avg:185.17ms\nstep:4186/4500 train_loss:7.9784 train_time:773271ms step_avg:185.17ms\nstep:4187/4500 train_loss:8.0679 train_time:773454ms step_avg:185.17ms\nstep:4188/4500 train_loss:8.1223 train_time:773639ms step_avg:185.17ms\nstep:4189/4500 train_loss:8.1105 train_time:773826ms step_avg:185.17ms\nstep:4190/4500 train_loss:8.1937 train_time:774011ms step_avg:185.17ms\nstep:4191/4500 train_loss:8.1994 train_time:774195ms step_avg:185.17ms\nstep:4192/4500 train_loss:8.0684 train_time:774381ms step_avg:185.17ms\nstep:4193/4500 train_loss:8.0942 train_time:774564ms step_avg:185.17ms\nstep:4194/4500 train_loss:7.9247 train_time:774750ms step_avg:185.17ms\nstep:4195/4500 train_loss:8.0719 train_time:774934ms step_avg:185.17ms\nstep:4196/4500 train_loss:8.2777 train_time:775119ms step_avg:185.17ms\nstep:4197/4500 train_loss:8.0866 train_time:775304ms step_avg:185.17ms\nstep:4198/4500 train_loss:8.2608 train_time:775493ms step_avg:185.17ms\nstep:4199/4500 train_loss:8.0525 train_time:775679ms step_avg:185.17ms\nstep:4200/4500 train_loss:8.0432 train_time:775864ms step_avg:185.17ms\nstep:4201/4500 train_loss:8.0386 train_time:776051ms step_avg:185.17ms\nstep:4202/4500 train_loss:8.2905 train_time:776235ms step_avg:185.17ms\nstep:4203/4500 train_loss:8.0394 train_time:776419ms step_avg:185.17ms\nstep:4204/4500 train_loss:8.0664 train_time:776602ms step_avg:185.17ms\nstep:4205/4500 train_loss:8.1704 train_time:776785ms step_avg:185.17ms\nstep:4206/4500 train_loss:8.0565 train_time:776968ms step_avg:185.17ms\nstep:4207/4500 train_loss:8.0270 train_time:777151ms step_avg:185.17ms\nstep:4208/4500 train_loss:7.9922 train_time:777337ms step_avg:185.17ms\nstep:4209/4500 train_loss:8.1421 train_time:777523ms step_avg:185.17ms\nstep:4210/4500 train_loss:7.8977 train_time:777708ms step_avg:185.17ms\nstep:4211/4500 train_loss:8.0304 train_time:777891ms step_avg:185.17ms\nstep:4212/4500 train_loss:8.0728 train_time:778074ms step_avg:185.17ms\nstep:4213/4500 train_loss:7.9698 train_time:778259ms step_avg:185.17ms\nstep:4214/4500 train_loss:7.9645 train_time:778448ms step_avg:185.17ms\nstep:4215/4500 train_loss:8.1822 train_time:778635ms step_avg:185.17ms\nstep:4216/4500 train_loss:8.1072 train_time:778820ms step_avg:185.17ms\nstep:4217/4500 train_loss:8.0414 train_time:779004ms step_avg:185.17ms\nstep:4218/4500 train_loss:8.1245 train_time:779188ms step_avg:185.17ms\nstep:4219/4500 train_loss:8.1094 train_time:779374ms step_avg:185.17ms\nstep:4220/4500 train_loss:8.1451 train_time:779560ms step_avg:185.17ms\nstep:4221/4500 train_loss:8.0014 train_time:779750ms step_avg:185.17ms\nstep:4222/4500 train_loss:8.0163 train_time:779936ms step_avg:185.17ms\nstep:4223/4500 train_loss:8.0694 train_time:780120ms step_avg:185.17ms\nstep:4224/4500 train_loss:8.0431 train_time:780305ms step_avg:185.17ms\nstep:4225/4500 train_loss:8.2410 train_time:780488ms step_avg:185.17ms\nstep:4226/4500 train_loss:8.0463 train_time:780673ms step_avg:185.17ms\nstep:4227/4500 train_loss:8.0192 train_time:780859ms step_avg:185.17ms\nstep:4228/4500 train_loss:8.1207 train_time:781045ms step_avg:185.17ms\nstep:4229/4500 train_loss:8.0955 train_time:781230ms step_avg:185.17ms\nstep:4230/4500 train_loss:8.0671 train_time:781415ms step_avg:185.17ms\nstep:4231/4500 train_loss:8.0173 train_time:781598ms step_avg:185.17ms\nstep:4232/4500 train_loss:8.1571 train_time:781785ms step_avg:185.17ms\nstep:4233/4500 train_loss:7.9822 train_time:781971ms step_avg:185.17ms\nstep:4234/4500 train_loss:8.1782 train_time:782155ms step_avg:185.17ms\nstep:4235/4500 train_loss:8.0032 train_time:782340ms step_avg:185.17ms\nstep:4236/4500 train_loss:8.0235 train_time:782525ms step_avg:185.17ms\nstep:4237/4500 train_loss:8.0313 train_time:782713ms step_avg:185.17ms\nstep:4238/4500 train_loss:8.0578 train_time:782897ms step_avg:185.17ms\nstep:4239/4500 train_loss:7.9677 train_time:783084ms step_avg:185.17ms\nstep:4240/4500 train_loss:8.0101 train_time:783268ms step_avg:185.17ms\nstep:4241/4500 train_loss:8.1102 train_time:783452ms step_avg:185.17ms\nstep:4242/4500 train_loss:7.9933 train_time:783638ms step_avg:185.17ms\nstep:4243/4500 train_loss:8.2044 train_time:783823ms step_avg:185.17ms\nstep:4244/4500 train_loss:8.2391 train_time:784008ms step_avg:185.17ms\nstep:4245/4500 train_loss:8.0726 train_time:784192ms step_avg:185.17ms\nstep:4246/4500 train_loss:8.1600 train_time:784377ms step_avg:185.17ms\nstep:4247/4500 train_loss:8.0700 train_time:784563ms step_avg:185.17ms\nstep:4248/4500 train_loss:8.0834 train_time:784749ms step_avg:185.17ms\nstep:4249/4500 train_loss:8.0768 train_time:784935ms step_avg:185.17ms\nstep:4250/4500 train_loss:8.0960 train_time:785120ms step_avg:185.17ms\nstep:4251/4500 train_loss:7.9010 train_time:785305ms step_avg:185.17ms\nstep:4252/4500 train_loss:8.1139 train_time:785489ms step_avg:185.17ms\nstep:4253/4500 train_loss:8.0034 train_time:785674ms step_avg:185.17ms\nstep:4254/4500 train_loss:8.0775 train_time:785858ms step_avg:185.17ms\nstep:4255/4500 train_loss:8.1745 train_time:786047ms step_avg:185.17ms\nstep:4256/4500 train_loss:8.0299 train_time:786232ms step_avg:185.17ms\nstep:4257/4500 train_loss:7.9846 train_time:786416ms step_avg:185.17ms\nstep:4258/4500 train_loss:8.0982 train_time:786600ms step_avg:185.17ms\nstep:4259/4500 train_loss:8.1197 train_time:786784ms step_avg:185.17ms\nstep:4260/4500 train_loss:8.1208 train_time:786970ms step_avg:185.17ms\nstep:4261/4500 train_loss:8.1590 train_time:787153ms step_avg:185.17ms\nstep:4262/4500 train_loss:8.0934 train_time:787337ms step_avg:185.17ms\nstep:4263/4500 train_loss:7.9887 train_time:787521ms step_avg:185.17ms\nstep:4264/4500 train_loss:8.0594 train_time:787705ms step_avg:185.17ms\nstep:4265/4500 train_loss:8.0347 train_time:787892ms step_avg:185.17ms\nstep:4266/4500 train_loss:8.0873 train_time:788077ms step_avg:185.17ms\nstep:4267/4500 train_loss:8.0710 train_time:788263ms step_avg:185.17ms\nstep:4268/4500 train_loss:7.9722 train_time:788448ms step_avg:185.17ms\nstep:4269/4500 train_loss:8.0885 train_time:788634ms step_avg:185.17ms\nstep:4270/4500 train_loss:8.3747 train_time:788818ms step_avg:185.17ms\nstep:4271/4500 train_loss:8.1367 train_time:789002ms step_avg:185.17ms\nstep:4272/4500 train_loss:7.9028 train_time:789187ms step_avg:185.17ms\nstep:4273/4500 train_loss:8.1407 train_time:789371ms step_avg:185.17ms\nstep:4274/4500 train_loss:8.0089 train_time:789554ms step_avg:185.17ms\nstep:4275/4500 train_loss:8.0594 train_time:789738ms step_avg:185.17ms\nstep:4276/4500 train_loss:8.0631 train_time:789925ms step_avg:185.17ms\nstep:4277/4500 train_loss:7.9843 train_time:790112ms step_avg:185.17ms\nstep:4278/4500 train_loss:7.9503 train_time:790296ms step_avg:185.17ms\nstep:4279/4500 train_loss:8.0661 train_time:790481ms step_avg:185.17ms\nstep:4280/4500 train_loss:8.0017 train_time:790665ms step_avg:185.17ms\nstep:4281/4500 train_loss:8.0686 train_time:790853ms step_avg:185.17ms\nstep:4282/4500 train_loss:8.1089 train_time:791040ms step_avg:185.17ms\nstep:4283/4500 train_loss:8.1503 train_time:791228ms step_avg:185.17ms\nstep:4284/4500 train_loss:7.9027 train_time:791416ms step_avg:185.17ms\nstep:4285/4500 train_loss:8.0952 train_time:791601ms step_avg:185.17ms\nstep:4286/4500 train_loss:8.0163 train_time:791788ms step_avg:185.17ms\nstep:4287/4500 train_loss:8.1214 train_time:791973ms step_avg:185.17ms\nstep:4288/4500 train_loss:7.9933 train_time:792158ms step_avg:185.17ms\nstep:4289/4500 train_loss:8.0965 train_time:792347ms step_avg:185.17ms\nstep:4290/4500 train_loss:7.9760 train_time:792531ms step_avg:185.17ms\nstep:4291/4500 train_loss:7.9457 train_time:792715ms step_avg:185.17ms\nstep:4292/4500 train_loss:7.9908 train_time:792898ms step_avg:185.17ms\nstep:4293/4500 train_loss:7.9748 train_time:793082ms step_avg:185.17ms\nstep:4294/4500 train_loss:7.9753 train_time:793265ms step_avg:185.17ms\nstep:4295/4500 train_loss:7.9681 train_time:793450ms step_avg:185.17ms\nstep:4296/4500 train_loss:8.0700 train_time:793633ms step_avg:185.17ms\nstep:4297/4500 train_loss:8.0431 train_time:793817ms step_avg:185.17ms\nstep:4298/4500 train_loss:8.0303 train_time:794002ms step_avg:185.17ms\nstep:4299/4500 train_loss:8.0797 train_time:794188ms step_avg:185.17ms\nstep:4300/4500 train_loss:8.0427 train_time:794372ms step_avg:185.17ms\nstep:4301/4500 train_loss:8.1190 train_time:794555ms step_avg:185.17ms\nstep:4302/4500 train_loss:8.0948 train_time:794739ms step_avg:185.17ms\nstep:4303/4500 train_loss:8.1266 train_time:794926ms step_avg:185.17ms\nstep:4304/4500 train_loss:8.1227 train_time:795109ms step_avg:185.17ms\nstep:4305/4500 train_loss:8.0654 train_time:795293ms step_avg:185.17ms\nstep:4306/4500 train_loss:8.0265 train_time:795479ms step_avg:185.17ms\nstep:4307/4500 train_loss:8.0084 train_time:795663ms step_avg:185.17ms\nstep:4308/4500 train_loss:7.9869 train_time:795849ms step_avg:185.17ms\nstep:4309/4500 train_loss:8.0994 train_time:796032ms step_avg:185.17ms\nstep:4310/4500 train_loss:7.9977 train_time:796217ms step_avg:185.17ms\nstep:4311/4500 train_loss:8.0588 train_time:796401ms step_avg:185.17ms\nstep:4312/4500 train_loss:8.1106 train_time:796587ms step_avg:185.17ms\nstep:4313/4500 train_loss:8.0244 train_time:796772ms step_avg:185.17ms\nstep:4314/4500 train_loss:7.9881 train_time:796957ms step_avg:185.17ms\nstep:4315/4500 train_loss:8.0273 train_time:797143ms step_avg:185.17ms\nstep:4316/4500 train_loss:8.2516 train_time:797332ms step_avg:185.17ms\nstep:4317/4500 train_loss:7.9554 train_time:797517ms step_avg:185.17ms\nstep:4318/4500 train_loss:8.0720 train_time:797702ms step_avg:185.17ms\nstep:4319/4500 train_loss:7.9978 train_time:797888ms step_avg:185.17ms\nstep:4320/4500 train_loss:8.1005 train_time:798074ms step_avg:185.17ms\nstep:4321/4500 train_loss:8.0366 train_time:798261ms step_avg:185.17ms\nstep:4322/4500 train_loss:8.0214 train_time:798449ms step_avg:185.17ms\nstep:4323/4500 train_loss:8.0538 train_time:798633ms step_avg:185.17ms\nstep:4324/4500 train_loss:7.9602 train_time:798818ms step_avg:185.17ms\nstep:4325/4500 train_loss:7.9831 train_time:799005ms step_avg:185.17ms\nstep:4326/4500 train_loss:8.1803 train_time:799189ms step_avg:185.17ms\nstep:4327/4500 train_loss:8.0912 train_time:799375ms step_avg:185.17ms\nstep:4328/4500 train_loss:7.9958 train_time:799562ms step_avg:185.17ms\nstep:4329/4500 train_loss:8.0043 train_time:799746ms step_avg:185.17ms\nstep:4330/4500 train_loss:8.0581 train_time:799930ms step_avg:185.17ms\nstep:4331/4500 train_loss:8.0555 train_time:800116ms step_avg:185.17ms\nstep:4332/4500 train_loss:8.1756 train_time:800302ms step_avg:185.17ms\nstep:4333/4500 train_loss:8.0100 train_time:800488ms step_avg:185.17ms\nstep:4334/4500 train_loss:8.0711 train_time:800674ms step_avg:185.17ms\nstep:4335/4500 train_loss:8.0593 train_time:800860ms step_avg:185.17ms\nstep:4336/4500 train_loss:8.1810 train_time:801044ms step_avg:185.17ms\nstep:4337/4500 train_loss:7.7097 train_time:801230ms step_avg:185.17ms\nstep:4338/4500 train_loss:7.9568 train_time:801414ms step_avg:185.17ms\nstep:4339/4500 train_loss:8.1475 train_time:801599ms step_avg:185.17ms\nstep:4340/4500 train_loss:8.0572 train_time:801787ms step_avg:185.17ms\nstep:4341/4500 train_loss:7.9426 train_time:801972ms step_avg:185.17ms\nstep:4342/4500 train_loss:7.9725 train_time:802156ms step_avg:185.17ms\nstep:4343/4500 train_loss:8.0189 train_time:802340ms step_avg:185.17ms\nstep:4344/4500 train_loss:8.0381 train_time:802526ms step_avg:185.17ms\nstep:4345/4500 train_loss:8.0914 train_time:802711ms step_avg:185.17ms\nstep:4346/4500 train_loss:8.0558 train_time:802895ms step_avg:185.17ms\nstep:4347/4500 train_loss:7.9856 train_time:803083ms step_avg:185.17ms\nstep:4348/4500 train_loss:8.0855 train_time:803270ms step_avg:185.17ms\nstep:4349/4500 train_loss:8.0091 train_time:803455ms step_avg:185.17ms\nstep:4350/4500 train_loss:7.9558 train_time:803639ms step_avg:185.17ms\nstep:4351/4500 train_loss:8.0031 train_time:803823ms step_avg:185.17ms\nstep:4352/4500 train_loss:8.0849 train_time:804009ms step_avg:185.17ms\nstep:4353/4500 train_loss:8.0783 train_time:804194ms step_avg:185.17ms\nstep:4354/4500 train_loss:7.7839 train_time:804378ms step_avg:185.17ms\nstep:4355/4500 train_loss:8.0422 train_time:804565ms step_avg:185.17ms\nstep:4356/4500 train_loss:8.2205 train_time:804749ms step_avg:185.17ms\nstep:4357/4500 train_loss:7.9433 train_time:804933ms step_avg:185.17ms\nstep:4358/4500 train_loss:7.9867 train_time:805119ms step_avg:185.17ms\nstep:4359/4500 train_loss:7.8822 train_time:805306ms step_avg:185.17ms\nstep:4360/4500 train_loss:7.8698 train_time:805489ms step_avg:185.17ms\nstep:4361/4500 train_loss:7.8674 train_time:805674ms step_avg:185.17ms\nstep:4362/4500 train_loss:8.0157 train_time:805858ms step_avg:185.17ms\nstep:4363/4500 train_loss:8.0592 train_time:806045ms step_avg:185.17ms\nstep:4364/4500 train_loss:8.0204 train_time:806230ms step_avg:185.17ms\nstep:4365/4500 train_loss:7.9554 train_time:806414ms step_avg:185.17ms\nstep:4366/4500 train_loss:8.0125 train_time:806600ms step_avg:185.17ms\nstep:4367/4500 train_loss:8.0282 train_time:806787ms step_avg:185.17ms\nstep:4368/4500 train_loss:8.1260 train_time:806971ms step_avg:185.17ms\nstep:4369/4500 train_loss:8.0254 train_time:807155ms step_avg:185.17ms\nstep:4370/4500 train_loss:7.9600 train_time:807340ms step_avg:185.17ms\nstep:4371/4500 train_loss:8.0586 train_time:807524ms step_avg:185.17ms\nstep:4372/4500 train_loss:8.1125 train_time:807712ms step_avg:185.17ms\nstep:4373/4500 train_loss:7.9912 train_time:807897ms step_avg:185.17ms\nstep:4374/4500 train_loss:7.8835 train_time:808082ms step_avg:185.17ms\nstep:4375/4500 train_loss:7.9760 train_time:808266ms step_avg:185.17ms\nstep:4376/4500 train_loss:8.0411 train_time:808452ms step_avg:185.17ms\nstep:4377/4500 train_loss:7.9814 train_time:808638ms step_avg:185.17ms\nstep:4378/4500 train_loss:8.0406 train_time:808823ms step_avg:185.17ms\nstep:4379/4500 train_loss:7.9985 train_time:809007ms step_avg:185.17ms\nstep:4380/4500 train_loss:8.0729 train_time:809193ms step_avg:185.17ms\nstep:4381/4500 train_loss:7.9908 train_time:809377ms step_avg:185.17ms\nstep:4382/4500 train_loss:8.1317 train_time:809562ms step_avg:185.17ms\nstep:4383/4500 train_loss:7.9949 train_time:809748ms step_avg:185.17ms\nstep:4384/4500 train_loss:8.0532 train_time:809933ms step_avg:185.17ms\nstep:4385/4500 train_loss:8.0807 train_time:810116ms step_avg:185.17ms\nstep:4386/4500 train_loss:8.1685 train_time:810301ms step_avg:185.17ms\nstep:4387/4500 train_loss:8.0181 train_time:810485ms step_avg:185.17ms\nstep:4388/4500 train_loss:7.9751 train_time:810673ms step_avg:185.17ms\nstep:4389/4500 train_loss:8.0364 train_time:810857ms step_avg:185.17ms\nstep:4390/4500 train_loss:8.0278 train_time:811041ms step_avg:185.17ms\nstep:4391/4500 train_loss:7.9501 train_time:811226ms step_avg:185.17ms\nstep:4392/4500 train_loss:8.1014 train_time:811410ms step_avg:185.17ms\nstep:4393/4500 train_loss:7.9390 train_time:811597ms step_avg:185.17ms\nstep:4394/4500 train_loss:7.9257 train_time:811782ms step_avg:185.17ms\nstep:4395/4500 train_loss:7.8869 train_time:811967ms step_avg:185.17ms\nstep:4396/4500 train_loss:8.0739 train_time:812154ms step_avg:185.17ms\nstep:4397/4500 train_loss:8.0899 train_time:812340ms step_avg:185.17ms\nstep:4398/4500 train_loss:7.9529 train_time:812526ms step_avg:185.17ms\nstep:4399/4500 train_loss:7.9365 train_time:812710ms step_avg:185.17ms\nstep:4400/4500 train_loss:8.0144 train_time:812897ms step_avg:185.17ms\nstep:4401/4500 train_loss:7.9959 train_time:813083ms step_avg:185.17ms\nstep:4402/4500 train_loss:7.9861 train_time:813266ms step_avg:185.17ms\nstep:4403/4500 train_loss:8.0642 train_time:813451ms step_avg:185.17ms\nstep:4404/4500 train_loss:7.9605 train_time:813637ms step_avg:185.17ms\nstep:4405/4500 train_loss:7.9490 train_time:813822ms step_avg:185.17ms\nstep:4406/4500 train_loss:7.9744 train_time:814007ms step_avg:185.17ms\nstep:4407/4500 train_loss:7.9144 train_time:814194ms step_avg:185.17ms\nstep:4408/4500 train_loss:8.0850 train_time:814379ms step_avg:185.17ms\nstep:4409/4500 train_loss:7.9402 train_time:814564ms step_avg:185.17ms\nstep:4410/4500 train_loss:8.0961 train_time:814750ms step_avg:185.17ms\nstep:4411/4500 train_loss:7.9177 train_time:814934ms step_avg:185.17ms\nstep:4412/4500 train_loss:8.0007 train_time:815119ms step_avg:185.17ms\nstep:4413/4500 train_loss:8.0470 train_time:815304ms step_avg:185.17ms\nstep:4414/4500 train_loss:7.9017 train_time:815492ms step_avg:185.17ms\nstep:4415/4500 train_loss:8.1050 train_time:815675ms step_avg:185.17ms\nstep:4416/4500 train_loss:8.0863 train_time:815858ms step_avg:185.17ms\nstep:4417/4500 train_loss:7.9983 train_time:816043ms step_avg:185.17ms\nstep:4418/4500 train_loss:7.9921 train_time:816226ms step_avg:185.17ms\nstep:4419/4500 train_loss:8.0652 train_time:816411ms step_avg:185.17ms\nstep:4420/4500 train_loss:7.9208 train_time:816594ms step_avg:185.17ms\nstep:4421/4500 train_loss:7.9974 train_time:816779ms step_avg:185.17ms\nstep:4422/4500 train_loss:7.9779 train_time:816963ms step_avg:185.17ms\nstep:4423/4500 train_loss:7.9913 train_time:817146ms step_avg:185.17ms\nstep:4424/4500 train_loss:7.9153 train_time:817330ms step_avg:185.17ms\nstep:4425/4500 train_loss:8.0547 train_time:817515ms step_avg:185.17ms\nstep:4426/4500 train_loss:8.0092 train_time:817701ms step_avg:185.17ms\nstep:4427/4500 train_loss:8.1351 train_time:817885ms step_avg:185.17ms\nstep:4428/4500 train_loss:8.1440 train_time:818072ms step_avg:185.17ms\nstep:4429/4500 train_loss:7.9489 train_time:818256ms step_avg:185.17ms\nstep:4430/4500 train_loss:7.9635 train_time:818442ms step_avg:185.17ms\nstep:4431/4500 train_loss:8.0697 train_time:818630ms step_avg:185.17ms\nstep:4432/4500 train_loss:7.9818 train_time:818814ms step_avg:185.17ms\nstep:4433/4500 train_loss:8.0755 train_time:818997ms step_avg:185.17ms\nstep:4434/4500 train_loss:7.9976 train_time:819180ms step_avg:185.17ms\nstep:4435/4500 train_loss:8.0010 train_time:819366ms step_avg:185.17ms\nstep:4436/4500 train_loss:8.0351 train_time:819550ms step_avg:185.17ms\nstep:4437/4500 train_loss:8.0336 train_time:819733ms step_avg:185.17ms\nstep:4438/4500 train_loss:8.1650 train_time:819918ms step_avg:185.17ms\nstep:4439/4500 train_loss:7.9951 train_time:820106ms step_avg:185.17ms\nstep:4440/4500 train_loss:7.9890 train_time:820289ms step_avg:185.17ms\nstep:4441/4500 train_loss:7.9122 train_time:820473ms step_avg:185.17ms\nstep:4442/4500 train_loss:7.9796 train_time:820657ms step_avg:185.17ms\nstep:4443/4500 train_loss:8.0429 train_time:820840ms step_avg:185.17ms\nstep:4444/4500 train_loss:7.9300 train_time:821024ms step_avg:185.17ms\nstep:4445/4500 train_loss:7.8508 train_time:821207ms step_avg:185.17ms\nstep:4446/4500 train_loss:8.1206 train_time:821391ms step_avg:185.16ms\nstep:4447/4500 train_loss:8.1297 train_time:821576ms step_avg:185.16ms\nstep:4448/4500 train_loss:8.0635 train_time:821761ms step_avg:185.16ms\nstep:4449/4500 train_loss:7.9890 train_time:821944ms step_avg:185.16ms\nstep:4450/4500 train_loss:8.0228 train_time:822130ms step_avg:185.16ms\nstep:4451/4500 train_loss:8.0270 train_time:822313ms step_avg:185.16ms\nstep:4452/4500 train_loss:7.9730 train_time:822497ms step_avg:185.16ms\nstep:4453/4500 train_loss:7.9533 train_time:822681ms step_avg:185.16ms\nstep:4454/4500 train_loss:7.8565 train_time:822866ms step_avg:185.16ms\nstep:4455/4500 train_loss:7.9502 train_time:823050ms step_avg:185.16ms\nstep:4456/4500 train_loss:7.9538 train_time:823235ms step_avg:185.16ms\nstep:4457/4500 train_loss:7.9249 train_time:823421ms step_avg:185.16ms\nstep:4458/4500 train_loss:7.9561 train_time:823605ms step_avg:185.16ms\nstep:4459/4500 train_loss:7.9166 train_time:823790ms step_avg:185.16ms\nstep:4460/4500 train_loss:7.9169 train_time:823973ms step_avg:185.16ms\nstep:4461/4500 train_loss:7.8316 train_time:824156ms step_avg:185.16ms\nstep:4462/4500 train_loss:8.0141 train_time:824340ms step_avg:185.16ms\nstep:4463/4500 train_loss:7.9831 train_time:824523ms step_avg:185.16ms\nstep:4464/4500 train_loss:7.8633 train_time:824708ms step_avg:185.16ms\nstep:4465/4500 train_loss:8.0466 train_time:824894ms step_avg:185.16ms\nstep:4466/4500 train_loss:7.9282 train_time:825078ms step_avg:185.16ms\nstep:4467/4500 train_loss:8.1243 train_time:825262ms step_avg:185.16ms\nstep:4468/4500 train_loss:8.0544 train_time:825446ms step_avg:185.16ms\nstep:4469/4500 train_loss:8.0135 train_time:825632ms step_avg:185.16ms\nstep:4470/4500 train_loss:7.9586 train_time:825815ms step_avg:185.16ms\nstep:4471/4500 train_loss:7.9019 train_time:826002ms step_avg:185.16ms\nstep:4472/4500 train_loss:7.9217 train_time:826192ms step_avg:185.16ms\nstep:4473/4500 train_loss:8.0025 train_time:826376ms step_avg:185.16ms\nstep:4474/4500 train_loss:7.8721 train_time:826558ms step_avg:185.16ms\nstep:4475/4500 train_loss:7.9378 train_time:826744ms step_avg:185.16ms\nstep:4476/4500 train_loss:8.0450 train_time:826929ms step_avg:185.16ms\nstep:4477/4500 train_loss:8.2656 train_time:827115ms step_avg:185.16ms\nstep:4478/4500 train_loss:7.9408 train_time:827300ms step_avg:185.16ms\nstep:4479/4500 train_loss:7.5534 train_time:827489ms step_avg:185.16ms\nstep:4480/4500 train_loss:7.9763 train_time:827671ms step_avg:185.16ms\nstep:4481/4500 train_loss:7.9110 train_time:827854ms step_avg:185.16ms\nstep:4482/4500 train_loss:8.0838 train_time:828039ms step_avg:185.16ms\nstep:4483/4500 train_loss:7.9906 train_time:828223ms step_avg:185.16ms\nstep:4484/4500 train_loss:7.8636 train_time:828406ms step_avg:185.16ms\nstep:4485/4500 train_loss:7.9820 train_time:828590ms step_avg:185.16ms\nstep:4486/4500 train_loss:8.0277 train_time:828773ms step_avg:185.16ms\nstep:4487/4500 train_loss:7.8654 train_time:828958ms step_avg:185.16ms\nstep:4488/4500 train_loss:7.7777 train_time:829141ms step_avg:185.16ms\nstep:4489/4500 train_loss:7.7313 train_time:829324ms step_avg:185.16ms\nstep:4490/4500 train_loss:7.9174 train_time:829507ms step_avg:185.16ms\nstep:4491/4500 train_loss:7.9117 train_time:829692ms step_avg:185.16ms\nstep:4492/4500 train_loss:7.9167 train_time:829875ms step_avg:185.16ms\nstep:4493/4500 train_loss:7.9404 train_time:830059ms step_avg:185.16ms\nstep:4494/4500 train_loss:7.8784 train_time:830243ms step_avg:185.16ms\nstep:4495/4500 train_loss:7.7945 train_time:830430ms step_avg:185.16ms\nstep:4496/4500 train_loss:7.8355 train_time:830613ms step_avg:185.16ms\nstep:4497/4500 train_loss:7.8864 train_time:830799ms step_avg:185.16ms\nstep:4498/4500 train_loss:7.8247 train_time:830984ms step_avg:185.16ms\nstep:4499/4500 train_loss:7.8916 train_time:831167ms step_avg:185.16ms\nstep:4500/4500 train_loss:7.8997 train_time:831349ms step_avg:185.16ms\npeak memory consumption: 37319 MiB\n(Current Step: 13, Remaining Steps: 37)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_1"
    }
  ],
  "info": {
    "model_stats": {
      "total_cost": 1.082022,
      "task_cost": 1.082022,
      "tokens_sent": 348734,
      "tokens_received": 2985,
      "api_calls": 13
    },
    "exit_status": "",
    "submission": null,
    "score": [],
    "summarizer": {}
  }
}