{
  "environment": "mlgym_main",
  "trajectory": [
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nFirst I'll start by using ls to see what files are in the current directory to see what's available.\n```\nls -a\n```\nCOMMAND\nls -a",
      "thought": "DISCUSSION\nFirst I'll start by using ls to see what files are in the current directory to see what's available.\nCOMMAND\nls -a",
      "action": "ls -a\n",
      "execution_time": 0.3008751254528761,
      "observation": "./\n../\nbaseline.py\ndata/\nevaluate.py*"
    },
    {
      "state": "{\"open_file\": \"n/a\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nOkay, I see `baseline.py`, `data/`, and `evaluate.py`. I should start by examining the `baseline.py` script to understand the current training process.\n```\nopen baseline.py\n```\nCOMMAND\nopen baseline.py",
      "thought": "DISCUSSION\nOkay, I see `baseline.py`, `data/`, and `evaluate.py`. I should start by examining the `baseline.py` script to understand the current training process.\nCOMMAND\nopen baseline.py",
      "action": "open baseline.py\n",
      "execution_time": 0.42216936219483614,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe `baseline.py` script seems to be a standard GPT-2 training script, adapted from NanoGPT. It uses distributed data parallel training and mixed precision (bfloat16). It also uses a custom optimizer called Muon, along with Adam optimizers for embedding and lm_head layers.\n\nSome things to consider for improvement:\n\n1. **Hyperparameter tuning**: The hyperparameters are defined in the `Hyperparameters` dataclass. We could try different values for batch size, sequence length, learning rate, number of iterations, etc.\n2. **Model architecture**: The model architecture is defined in the `ModelConfig` dataclass and `Model` class. We could try different values for `n_layer`, `n_head`, `n_embd`.\n3. **Optimizer**: We could experiment with different optimizers like AdamW or look into the parameters for Muon optimizer.\n4. **Activation function**: The MLP uses `relu().square()` activation. We could try other activation functions like GELU or SwiGLU.\n5. **Data loading**: The `DistributedDataLoader` seems custom-built. We can check if there are any improvements to be made here, but it seems relatively standard.\n6. **Regularization**: The script uses weight decay, but we could explore other regularization techniques like dropout or layer norm.\n\nLet's start by trying to increase the number of training iterations and see if that improves the performance. The current number of iterations is 500, which seems quite small for a large dataset like FineWeb. I'll increase it to 5000. I will also save checkpoints more frequently.\n\nI will edit the `baseline.py` file and change `num_iterations` to 5000 and `save_every` to 100.\n```\nedit 357:361\n    num_iterations : int = 5000 # number of iterations to run\n    warmup_iters : int = 0\n    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0\n    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\nend_of_edit\n```\nCOMMAND\nedit 357:361\n    num_iterations : int = 5000 # number of iterations to run\n    warmup_iters : int = 0\n    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0\n    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\nend_of_edit",
      "thought": "DISCUSSION\nThe `baseline.py` script seems to be a standard GPT-2 training script, adapted from NanoGPT. It uses distributed data parallel training and mixed precision (bfloat16). It also uses a custom optimizer called Muon, along with Adam optimizers for embedding and lm_head layers.\n\nSome things to consider for improvement:\n\n1. **Hyperparameter tuning**: The hyperparameters are defined in the `Hyperparameters` dataclass. We could try different values for batch size, sequence length, learning rate, number of iterations, etc.\n2. **Model architecture**: The model architecture is defined in the `ModelConfig` dataclass and `Model` class. We could try different values for `n_layer`, `n_head`, `n_embd`.\n3. **Optimizer**: We could experiment with different optimizers like AdamW or look into the parameters for Muon optimizer.\n4. **Activation function**: The MLP uses `relu().square()` activation. We could try other activation functions like GELU or SwiGLU.\n5. **Data loading**: The `DistributedDataLoader` seems custom-built. We can check if there are any improvements to be made here, but it seems relatively standard.\n6. **Regularization**: The script uses weight decay, but we could explore other regularization techniques like dropout or layer norm.\n\nLet's start by trying to increase the number of training iterations and see if that improves the performance. The current number of iterations is 500, which seems quite small for a large dataset like FineWeb. I'll increase it to 5000. I will also save checkpoints more frequently.\n\nI will edit the `baseline.py` file and change `num_iterations` to 5000 and `save_every` to 100.\nCOMMAND\nedit 357:361\n    num_iterations : int = 5000 # number of iterations to run\n    warmup_iters : int = 0\n    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0\n    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\nend_of_edit",
      "action": "edit 357:361\n    num_iterations : int = 5000 # number of iterations to run\n    warmup_iters : int = 0\n    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0\n    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\nend_of_edit\n",
      "execution_time": 0.7953600352630019,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe file has been updated. Let's quickly review the changes to ensure they are correct. I've increased `num_iterations` to 5000 and set `save_every` to 100. The indentation looks correct.\n\nNow, I'll run the `baseline.py` script with the modified hyperparameters.\n```\nbash baseline.py\n```\nCOMMAND\nbash baseline.py",
      "thought": "DISCUSSION\nThe file has been updated. Let's quickly review the changes to ensure they are correct. I've increased `num_iterations` to 5000 and set `save_every` to 100. The indentation looks correct.\n\nNow, I'll run the `baseline.py` script with the modified hyperparameters.\nCOMMAND\nbash baseline.py",
      "action": "bash baseline.py\n",
      "execution_time": 0.30125002562999725,
      "observation": "baseline.py: line 9: \nCopyright (c) Meta Platforms, Inc. and affiliates.\nAdapted from https://github.com/KellerJordan/modded-nanogpt\nThis file contains the code for the training a GPT-2 model on the fine-web dataset.\nRun this file with the following command:\ntorchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n: No such file or directory\nbaseline.py: line 11: import: command not found\nbaseline.py: line 12: import: command not found\nbaseline.py: line 13: import: command not found\nbaseline.py: line 14: import: command not found\nbaseline.py: line 15: import: command not found\nbaseline.py: line 16: import: command not found\nbaseline.py: line 17: from: command not found\nbaseline.py: line 19: import: command not found\nbaseline.py: line 20: import: command not found\nbaseline.py: line 21: import: command not found\nbaseline.py: line 22: import: command not found\nbaseline.py: line 23: import: command not found\nbaseline.py: line 24: import: command not found\nbaseline.py: line 25: from: command not found\nbaseline.py: line 26: from: command not found\nbaseline.py: line 28: torch._dynamo.config.suppress_errors: command not found\nbaseline.py: line 33: syntax error near unexpected token `('\nbaseline.py: line 33: `def zeropower_via_svd(G, steps=None):'"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nIt seems I ran `bash baseline.py` which tries to execute the python script using bash. I need to use `python baseline.py` or `torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py` as mentioned in the script's comments. Since it's a distributed training script, I should use `torchrun`. Let's assume we have 8 GPUs available, so I'll use `torchrun --nproc_per_node=8 --standalone baseline.py`.\n```\ntorchrun --nproc_per_node=8 --standalone baseline.py\n```\nCOMMAND\ntorchrun --nproc_per_node=8 --standalone baseline.py",
      "thought": "DISCUSSION\nIt seems I ran `bash baseline.py` which tries to execute the python script using bash. I need to use `python baseline.py` or `torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py` as mentioned in the script's comments. Since it's a distributed training script, I should use `torchrun`. Let's assume we have 8 GPUs available, so I'll use `torchrun --nproc_per_node=8 --standalone baseline.py`.\nCOMMAND\ntorchrun --nproc_per_node=8 --standalone baseline.py",
      "action": "torchrun --nproc_per_node=8 --standalone baseline.py\n",
      "execution_time": 6.069335733540356,
      "observation": "W0402 05:54:13.254000 264 site-packages/torch/distributed/run.py:793] \nW0402 05:54:13.254000 264 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 05:54:13.254000 264 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 05:54:13.254000 264 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank0]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\n[rank3]: Traceback (most recent call last):\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank3]:     main()\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank3]:     torch.cuda.set_device(device)\n[rank3]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank3]:     torch._C._cuda_setDevice(device)\n[rank3]: RuntimeError: CUDA error: invalid device ordinal\n[rank3]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank3]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank3]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank4]: Traceback (most recent call last):\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank4]:     main()\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank4]:     torch.cuda.set_device(device)\n[rank4]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank4]:     torch._C._cuda_setDevice(device)\n[rank4]: RuntimeError: CUDA error: invalid device ordinal\n[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank6]: Traceback (most recent call last):\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank6]:     main()\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank6]:     torch.cuda.set_device(device)\n[rank6]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank6]:     torch._C._cuda_setDevice(device)\n[rank6]: RuntimeError: CUDA error: invalid device ordinal\n[rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank7]: Traceback (most recent call last):\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank7]:     main()\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank7]:     torch.cuda.set_device(device)\n[rank7]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank7]:     torch._C._cuda_setDevice(device)\n[rank7]: RuntimeError: CUDA error: invalid device ordinal\n[rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank2]: Traceback (most recent call last):\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank2]:     main()\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank2]:     torch.cuda.set_device(device)\n[rank2]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank2]:     torch._C._cuda_setDevice(device)\n[rank2]: RuntimeError: CUDA error: invalid device ordinal\n[rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank5]: Traceback (most recent call last):\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank5]:     main()\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank5]:     torch.cuda.set_device(device)\n[rank5]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank5]:     torch._C._cuda_setDevice(device)\n[rank5]: RuntimeError: CUDA error: invalid device ordinal\n[rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank0]:[W402 05:54:16.744434882 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank1]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\n[rank4]:[W402 05:54:17.027045777 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[rank6]:[W402 05:54:17.113496948 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[rank2]:[W402 05:54:17.124658432 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0402 05:54:17.234000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 331 closing signal SIGTERM\nW0402 05:54:17.235000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 332 closing signal SIGTERM\nW0402 05:54:17.236000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 333 closing signal SIGTERM\nW0402 05:54:17.236000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 334 closing signal SIGTERM\nW0402 05:54:17.237000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 335 closing signal SIGTERM\nW0402 05:54:17.237000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 336 closing signal SIGTERM\nW0402 05:54:17.238000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 337 closing signal SIGTERM\nE0402 05:54:17.403000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 330) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-02_05:54:17\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 330)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe previous command failed because of an assertion error related to batch size. To debug this, I'll insert a print statement in `baseline.py` to check the values of `device_batch_size`, `torch.cuda.device_count()`, and the calculated `batch_size`. This will help me understand why the batch size is being calculated as 128 instead of the expected 512.\n\n```\nedit 370:370\n    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\nend_of_edit\n```\nCOMMAND\nedit 370:370\n    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\nend_of_edit",
      "thought": "DISCUSSION\nThe previous command failed because of an assertion error related to batch size. To debug this, I'll insert a print statement in `baseline.py` to check the values of `device_batch_size`, `torch.cuda.device_count()`, and the calculated `batch_size`. This will help me understand why the batch size is being calculated as 128 instead of the expected 512.\n\nCOMMAND\nedit 370:370\n    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\nend_of_edit",
      "action": "edit 370:370\n    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\nend_of_edit\n",
      "execution_time": 1.0974874896928668,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe print statement is inserted. Now let's run the training script again with `torchrun`.\n```\ntorchrun --nproc_per_node=8 --standalone baseline.py\n```\nCOMMAND\ntorchrun --nproc_per_node=8 --standalone baseline.py",
      "thought": "DISCUSSION\nThe print statement is inserted. Now let's run the training script again with `torchrun`.\nCOMMAND\ntorchrun --nproc_per_node=8 --standalone baseline.py",
      "action": "torchrun --nproc_per_node=8 --standalone baseline.py\n",
      "execution_time": 5.792430368252099,
      "observation": "W0402 05:54:28.139000 437 site-packages/torch/distributed/run.py:793] \nW0402 05:54:28.139000 437 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 05:54:28.139000 437 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 05:54:28.139000 437 site-packages/torch/distributed/run.py:793] *****************************************\ndevice_batch_size: 64, device_count: 2, batch_size: 128\n[rank7]: Traceback (most recent call last):\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank7]:     main()\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank7]:     torch.cuda.set_device(device)\n[rank7]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank7]:     torch._C._cuda_setDevice(device)\n[rank7]: RuntimeError: CUDA error: invalid device ordinal\n[rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\n[rank3]: Traceback (most recent call last):\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank3]:     main()\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank3]:     torch.cuda.set_device(device)\n[rank3]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank3]:     torch._C._cuda_setDevice(device)\n[rank3]: RuntimeError: CUDA error: invalid device ordinal\n[rank3]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank3]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank3]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank2]: Traceback (most recent call last):\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank2]:     main()\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank2]:     torch.cuda.set_device(device)\n[rank2]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank2]:     torch._C._cuda_setDevice(device)\n[rank2]: RuntimeError: CUDA error: invalid device ordinal\n[rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank4]: Traceback (most recent call last):\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank4]:     main()\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank4]:     torch.cuda.set_device(device)\n[rank4]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank4]:     torch._C._cuda_setDevice(device)\n[rank4]: RuntimeError: CUDA error: invalid device ordinal\n[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank5]: Traceback (most recent call last):\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank5]:     main()\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank5]:     torch.cuda.set_device(device)\n[rank5]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank5]:     torch._C._cuda_setDevice(device)\n[rank5]: RuntimeError: CUDA error: invalid device ordinal\n[rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank6]: Traceback (most recent call last):\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank6]:     main()\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank6]:     torch.cuda.set_device(device)\n[rank6]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank6]:     torch._C._cuda_setDevice(device)\n[rank6]: RuntimeError: CUDA error: invalid device ordinal\n[rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank0]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank1]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\nW0402 05:54:31.898000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 503 closing signal SIGTERM\nW0402 05:54:31.899000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 504 closing signal SIGTERM\nW0402 05:54:31.900000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 505 closing signal SIGTERM\nW0402 05:54:31.901000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 506 closing signal SIGTERM\nW0402 05:54:31.901000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 507 closing signal SIGTERM\nW0402 05:54:31.902000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 508 closing signal SIGTERM\nW0402 05:54:31.902000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 509 closing signal SIGTERM\nE0402 05:54:32.080000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 7 (pid: 510) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-02_05:54:31\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 7 (local_rank: 7)\n  exitcode  : 1 (pid: 510)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe output confirms that `device_count` is 2, but I specified `--nproc_per_node=8`. This is likely the reason for the \"invalid device ordinal\" CUDA error and the batch size assertion failure. It seems the environment only has 2 GPUs available, not 8.\n\nI should reduce `--nproc_per_node` to 2 to match the available GPUs. Let's try running the training script again with `torchrun --nproc_per_node=2 --standalone baseline.py`. I also need to fix the batch size assertion. The assertion expects `args.batch_size` to be divisible by `B * ddp_world_size`. Currently, `args.batch_size` is 128, `B` is 64, and `ddp_world_size` will be 2. So, 128 is divisible by (64 * 2 = 128). The assertion *should* pass now.\n\nLet's try running with `--nproc_per_node=2` first and see if the CUDA error is resolved.\n```\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```\nCOMMAND\ntorchrun --nproc_per_node=2 --standalone baseline.py",
      "thought": "DISCUSSION\nThe output confirms that `device_count` is 2, but I specified `--nproc_per_node=8`. This is likely the reason for the \"invalid device ordinal\" CUDA error and the batch size assertion failure. It seems the environment only has 2 GPUs available, not 8.\n\nI should reduce `--nproc_per_node` to 2 to match the available GPUs. Let's try running the training script again with `torchrun --nproc_per_node=2 --standalone baseline.py`. I also need to fix the batch size assertion. The assertion expects `args.batch_size` to be divisible by `B * ddp_world_size`. Currently, `args.batch_size` is 128, `B` is 64, and `ddp_world_size` will be 2. So, 128 is divisible by (64 * 2 = 128). The assertion *should* pass now.\n\nLet's try running with `--nproc_per_node=2` first and see if the CUDA error is resolved.\nCOMMAND\ntorchrun --nproc_per_node=2 --standalone baseline.py",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 5.036179807037115,
      "observation": "W0402 05:54:36.441000 561 site-packages/torch/distributed/run.py:793] \nW0402 05:54:36.441000 561 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 05:54:36.441000 561 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 05:54:36.441000 561 site-packages/torch/distributed/run.py:793] *****************************************\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 388, in main\n[rank0]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 310, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 388, in main\n[rank1]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 310, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\n[rank0]:[W402 05:54:39.191335696 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0402 05:54:39.595000 561 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 628 closing signal SIGTERM\nE0402 05:54:39.759000 561 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 627) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-02_05:54:39\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 627)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nNow the CUDA error is gone, but there's a new error: \"AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\". This means the data file path is incorrect.\n\nLet's check the `data/` directory to see the correct path.\n```\nls data/\n```\nCOMMAND\nls data/",
      "thought": "DISCUSSION\nNow the CUDA error is gone, but there's a new error: \"AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\". This means the data file path is incorrect.\n\nLet's check the `data/` directory to see the correct path.\nCOMMAND\nls data/",
      "action": "ls data/\n",
      "execution_time": 0.3007793081924319,
      "observation": "fineweb108/"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nIt seems there is a typo in the directory name. It's `fineweb108` instead of `fineweb10B`. Let's correct the `input_bin` path in `baseline.py`.\n```\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```\nCOMMAND\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit",
      "thought": "DISCUSSION\nIt seems there is a typo in the directory name. It's `fineweb108` instead of `fineweb10B`. Let's correct the `input_bin` path in `baseline.py`.\nCOMMAND\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit",
      "action": "edit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "execution_time": 0.8537621088325977,
      "observation": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe data path is corrected. Now let's run the training script again with the corrected path and `--nproc_per_node=2`.\n```\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```\nCOMMAND\ntorchrun --nproc_per_node=2 --standalone baseline.py",
      "thought": "DISCUSSION\nThe data path is corrected. Now let's run the training script again with the corrected path and `--nproc_per_node=2`.\nCOMMAND\ntorchrun --nproc_per_node=2 --standalone baseline.py",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "execution_time": 998.3290612390265,
      "observation": "W0402 05:55:47.993000 702 site-packages/torch/distributed/run.py:793] \nW0402 05:55:47.993000 702 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 05:55:47.993000 702 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 05:55:47.993000 702 site-packages/torch/distributed/run.py:793] *****************************************\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\nusing device: cuda:0\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nusing device: cuda:1\nstep:1/5000 train_loss:10.8258 train_time:72096ms step_avg:nanms\nstep:2/5000 train_loss:10.3189 train_time:85141ms step_avg:nanms\nstep:3/5000 train_loss:9.9885 train_time:85309ms step_avg:nanms\nstep:4/5000 train_loss:9.3618 train_time:85485ms step_avg:nanms\nstep:5/5000 train_loss:8.3247 train_time:85662ms step_avg:nanms\nstep:6/5000 train_loss:7.7346 train_time:85839ms step_avg:nanms\nstep:7/5000 train_loss:7.1489 train_time:86017ms step_avg:nanms\nstep:8/5000 train_loss:7.0549 train_time:86199ms step_avg:nanms\nstep:9/5000 train_loss:6.9054 train_time:86377ms step_avg:nanms\nstep:10/5000 train_loss:6.9395 train_time:86554ms step_avg:nanms\nstep:11/5000 train_loss:6.8656 train_time:172ms step_avg:nanms\nstep:12/5000 train_loss:6.8867 train_time:351ms step_avg:nanms\nstep:13/5000 train_loss:6.8008 train_time:531ms step_avg:176.91ms\nstep:14/5000 train_loss:6.6187 train_time:710ms step_avg:177.38ms\nstep:15/5000 train_loss:6.7522 train_time:889ms step_avg:177.71ms\nstep:16/5000 train_loss:6.5286 train_time:1066ms step_avg:177.75ms\nstep:17/5000 train_loss:6.7844 train_time:1245ms step_avg:177.79ms\nstep:18/5000 train_loss:6.6020 train_time:1423ms step_avg:177.83ms\nstep:19/5000 train_loss:6.5011 train_time:1602ms step_avg:177.99ms\nstep:20/5000 train_loss:6.5028 train_time:1781ms step_avg:178.15ms\nstep:21/5000 train_loss:6.5010 train_time:1960ms step_avg:178.22ms\nstep:22/5000 train_loss:6.3605 train_time:2140ms step_avg:178.30ms\nstep:23/5000 train_loss:6.3135 train_time:2318ms step_avg:178.30ms\nstep:24/5000 train_loss:6.3479 train_time:2497ms step_avg:178.39ms\nstep:25/5000 train_loss:6.4334 train_time:2676ms step_avg:178.40ms\nstep:26/5000 train_loss:6.6624 train_time:2854ms step_avg:178.40ms\nstep:27/5000 train_loss:6.3805 train_time:3033ms step_avg:178.39ms\nstep:28/5000 train_loss:6.5132 train_time:3214ms step_avg:178.56ms\nstep:29/5000 train_loss:6.2949 train_time:3393ms step_avg:178.58ms\nstep:30/5000 train_loss:6.4066 train_time:3573ms step_avg:178.63ms\nstep:31/5000 train_loss:6.2685 train_time:3752ms step_avg:178.66ms\nstep:32/5000 train_loss:6.0329 train_time:3930ms step_avg:178.66ms\nstep:33/5000 train_loss:6.3387 train_time:4109ms step_avg:178.67ms\nstep:34/5000 train_loss:6.2966 train_time:4290ms step_avg:178.74ms\nstep:35/5000 train_loss:6.2813 train_time:4470ms step_avg:178.79ms\nstep:36/5000 train_loss:6.4003 train_time:4651ms step_avg:178.87ms\nstep:37/5000 train_loss:6.1010 train_time:4832ms step_avg:178.96ms\nstep:38/5000 train_loss:6.2412 train_time:5012ms step_avg:178.99ms\nstep:39/5000 train_loss:6.1811 train_time:5190ms step_avg:178.97ms\nstep:40/5000 train_loss:6.2101 train_time:5369ms step_avg:178.98ms\nstep:41/5000 train_loss:6.1716 train_time:5552ms step_avg:179.08ms\nstep:42/5000 train_loss:6.1527 train_time:5730ms step_avg:179.08ms\nstep:43/5000 train_loss:6.1561 train_time:5911ms step_avg:179.13ms\nstep:44/5000 train_loss:6.1292 train_time:6091ms step_avg:179.13ms\nstep:45/5000 train_loss:6.0040 train_time:6270ms step_avg:179.15ms\nstep:46/5000 train_loss:6.0386 train_time:6450ms step_avg:179.16ms\nstep:47/5000 train_loss:6.0390 train_time:6630ms step_avg:179.20ms\nstep:48/5000 train_loss:6.0047 train_time:6810ms step_avg:179.21ms\nstep:49/5000 train_loss:6.0537 train_time:6992ms step_avg:179.28ms\nstep:50/5000 train_loss:5.9733 train_time:7171ms step_avg:179.28ms\nstep:51/5000 train_loss:6.1354 train_time:7349ms step_avg:179.24ms\nstep:52/5000 train_loss:5.9891 train_time:7528ms step_avg:179.24ms\nstep:53/5000 train_loss:6.0544 train_time:7707ms step_avg:179.23ms\nstep:54/5000 train_loss:5.9219 train_time:7887ms step_avg:179.24ms\nstep:55/5000 train_loss:5.7107 train_time:8065ms step_avg:179.23ms\nstep:56/5000 train_loss:5.9072 train_time:8247ms step_avg:179.28ms\nstep:57/5000 train_loss:5.8327 train_time:8425ms step_avg:179.27ms\nstep:58/5000 train_loss:5.8790 train_time:8604ms step_avg:179.24ms\nstep:59/5000 train_loss:5.8485 train_time:8783ms step_avg:179.24ms\nstep:60/5000 train_loss:5.8568 train_time:8962ms step_avg:179.24ms\nstep:61/5000 train_loss:5.9567 train_time:9142ms step_avg:179.25ms\nstep:62/5000 train_loss:5.8426 train_time:9322ms step_avg:179.27ms\nstep:63/5000 train_loss:5.7330 train_time:9502ms step_avg:179.28ms\nstep:64/5000 train_loss:5.6749 train_time:9682ms step_avg:179.29ms\nstep:65/5000 train_loss:5.8886 train_time:9861ms step_avg:179.30ms\nstep:66/5000 train_loss:5.8894 train_time:10040ms step_avg:179.29ms\nstep:67/5000 train_loss:5.6308 train_time:10220ms step_avg:179.30ms\nstep:68/5000 train_loss:5.6252 train_time:10399ms step_avg:179.30ms\nstep:69/5000 train_loss:5.7683 train_time:10577ms step_avg:179.28ms\nstep:70/5000 train_loss:5.6867 train_time:10756ms step_avg:179.27ms\nstep:71/5000 train_loss:5.7732 train_time:10935ms step_avg:179.26ms\nstep:72/5000 train_loss:6.1531 train_time:11114ms step_avg:179.25ms\nstep:73/5000 train_loss:5.7393 train_time:11292ms step_avg:179.23ms\nstep:74/5000 train_loss:5.7051 train_time:11471ms step_avg:179.24ms\nstep:75/5000 train_loss:5.7801 train_time:11650ms step_avg:179.23ms\nstep:76/5000 train_loss:5.6024 train_time:11829ms step_avg:179.23ms\nstep:77/5000 train_loss:5.5714 train_time:12011ms step_avg:179.27ms\nstep:78/5000 train_loss:5.3981 train_time:12192ms step_avg:179.29ms\nstep:79/5000 train_loss:5.6228 train_time:12371ms step_avg:179.30ms\nstep:80/5000 train_loss:5.7587 train_time:12550ms step_avg:179.29ms\nstep:81/5000 train_loss:5.5846 train_time:12729ms step_avg:179.28ms\nstep:82/5000 train_loss:5.6713 train_time:12910ms step_avg:179.31ms\nstep:83/5000 train_loss:5.5533 train_time:13091ms step_avg:179.33ms\nstep:84/5000 train_loss:5.6860 train_time:13273ms step_avg:179.36ms\nstep:85/5000 train_loss:5.7139 train_time:13451ms step_avg:179.35ms\nstep:86/5000 train_loss:5.8476 train_time:13630ms step_avg:179.34ms\nstep:87/5000 train_loss:5.5848 train_time:13810ms step_avg:179.35ms\nstep:88/5000 train_loss:5.4037 train_time:13989ms step_avg:179.35ms\nstep:89/5000 train_loss:5.7710 train_time:14168ms step_avg:179.35ms\nstep:90/5000 train_loss:5.5601 train_time:14347ms step_avg:179.34ms\nstep:91/5000 train_loss:5.5549 train_time:14527ms step_avg:179.35ms\nstep:92/5000 train_loss:5.4529 train_time:14706ms step_avg:179.35ms\nstep:93/5000 train_loss:5.3457 train_time:14887ms step_avg:179.36ms\nstep:94/5000 train_loss:5.7394 train_time:15067ms step_avg:179.36ms\nstep:95/5000 train_loss:5.4371 train_time:15246ms step_avg:179.36ms\nstep:96/5000 train_loss:5.4870 train_time:15426ms step_avg:179.37ms\nstep:97/5000 train_loss:5.3184 train_time:15605ms step_avg:179.36ms\nstep:98/5000 train_loss:5.3861 train_time:15784ms step_avg:179.36ms\nstep:99/5000 train_loss:5.2205 train_time:15963ms step_avg:179.36ms\nstep:100/5000 train_loss:5.2949 train_time:16142ms step_avg:179.36ms\nstep:101/5000 train_loss:5.4149 train_time:16322ms step_avg:179.36ms\nstep:102/5000 train_loss:5.3211 train_time:16500ms step_avg:179.35ms\nstep:103/5000 train_loss:5.3576 train_time:16679ms step_avg:179.35ms\nstep:104/5000 train_loss:5.4167 train_time:16859ms step_avg:179.35ms\nstep:105/5000 train_loss:5.3226 train_time:17038ms step_avg:179.35ms\nstep:106/5000 train_loss:5.4677 train_time:17217ms step_avg:179.35ms\nstep:107/5000 train_loss:5.3685 train_time:17396ms step_avg:179.34ms\nstep:108/5000 train_loss:5.3482 train_time:17575ms step_avg:179.33ms\nstep:109/5000 train_loss:5.4642 train_time:17754ms step_avg:179.33ms\nstep:110/5000 train_loss:5.1305 train_time:17932ms step_avg:179.32ms\nstep:111/5000 train_loss:5.1933 train_time:18111ms step_avg:179.32ms\nstep:112/5000 train_loss:5.3258 train_time:18289ms step_avg:179.30ms\nstep:113/5000 train_loss:5.2957 train_time:18468ms step_avg:179.30ms\nstep:114/5000 train_loss:5.3762 train_time:18648ms step_avg:179.31ms\nstep:115/5000 train_loss:5.3301 train_time:18830ms step_avg:179.34ms\nstep:116/5000 train_loss:5.3908 train_time:19010ms step_avg:179.34ms\nstep:117/5000 train_loss:5.3128 train_time:19191ms step_avg:179.36ms\nstep:118/5000 train_loss:5.2022 train_time:19370ms step_avg:179.35ms\nstep:119/5000 train_loss:5.3049 train_time:19549ms step_avg:179.35ms\nstep:120/5000 train_loss:5.3934 train_time:19729ms step_avg:179.35ms\nstep:121/5000 train_loss:5.2004 train_time:19909ms step_avg:179.36ms\nstep:122/5000 train_loss:5.1705 train_time:20089ms step_avg:179.37ms\nstep:123/5000 train_loss:5.1651 train_time:20268ms step_avg:179.37ms\nstep:124/5000 train_loss:5.7213 train_time:20447ms step_avg:179.36ms\nstep:125/5000 train_loss:5.2042 train_time:20626ms step_avg:179.35ms\nstep:126/5000 train_loss:5.0173 train_time:20806ms step_avg:179.36ms\nstep:127/5000 train_loss:5.1850 train_time:20989ms step_avg:179.39ms\nstep:128/5000 train_loss:5.2202 train_time:21168ms step_avg:179.39ms\nstep:129/5000 train_loss:5.2901 train_time:21347ms step_avg:179.39ms\nstep:130/5000 train_loss:5.3369 train_time:21528ms step_avg:179.40ms\nstep:131/5000 train_loss:5.2179 train_time:21708ms step_avg:179.40ms\nstep:132/5000 train_loss:5.0871 train_time:21887ms step_avg:179.41ms\nstep:133/5000 train_loss:5.2010 train_time:22069ms step_avg:179.42ms\nstep:134/5000 train_loss:5.2242 train_time:22248ms step_avg:179.42ms\nstep:135/5000 train_loss:5.2129 train_time:22428ms step_avg:179.43ms\nstep:136/5000 train_loss:5.1088 train_time:22609ms step_avg:179.43ms\nstep:137/5000 train_loss:5.1067 train_time:22789ms step_avg:179.44ms\nstep:138/5000 train_loss:5.3150 train_time:22969ms step_avg:179.45ms\nstep:139/5000 train_loss:5.2147 train_time:23149ms step_avg:179.45ms\nstep:140/5000 train_loss:5.3388 train_time:23329ms step_avg:179.45ms\nstep:141/5000 train_loss:5.0776 train_time:23509ms step_avg:179.46ms\nstep:142/5000 train_loss:5.2945 train_time:23688ms step_avg:179.46ms\nstep:143/5000 train_loss:4.9505 train_time:23867ms step_avg:179.45ms\nstep:144/5000 train_loss:5.2204 train_time:24047ms step_avg:179.46ms\nstep:145/5000 train_loss:5.1108 train_time:24226ms step_avg:179.45ms\nstep:146/5000 train_loss:5.1994 train_time:24408ms step_avg:179.47ms\nstep:147/5000 train_loss:5.0703 train_time:24587ms step_avg:179.47ms\nstep:148/5000 train_loss:5.2519 train_time:24766ms step_avg:179.46ms\nstep:149/5000 train_loss:4.8437 train_time:24945ms step_avg:179.46ms\nstep:150/5000 train_loss:5.0675 train_time:25124ms step_avg:179.45ms\nstep:151/5000 train_loss:5.0337 train_time:25304ms step_avg:179.46ms\nstep:152/5000 train_loss:5.0757 train_time:25484ms step_avg:179.46ms\nstep:153/5000 train_loss:5.1887 train_time:25664ms step_avg:179.47ms\nstep:154/5000 train_loss:5.0544 train_time:25843ms step_avg:179.47ms\nstep:155/5000 train_loss:5.0456 train_time:26023ms step_avg:179.47ms\nstep:156/5000 train_loss:5.1606 train_time:26202ms step_avg:179.46ms\nstep:157/5000 train_loss:5.1534 train_time:26380ms step_avg:179.45ms\nstep:158/5000 train_loss:4.9929 train_time:26559ms step_avg:179.45ms\nstep:159/5000 train_loss:4.9753 train_time:26737ms step_avg:179.45ms\nstep:160/5000 train_loss:4.9497 train_time:26920ms step_avg:179.46ms\nstep:161/5000 train_loss:5.0656 train_time:27099ms step_avg:179.46ms\nstep:162/5000 train_loss:4.9708 train_time:27274ms step_avg:179.44ms\nstep:163/5000 train_loss:4.9822 train_time:27452ms step_avg:179.43ms\nstep:164/5000 train_loss:5.1245 train_time:27631ms step_avg:179.42ms\nstep:165/5000 train_loss:4.9729 train_time:27811ms step_avg:179.43ms\nstep:166/5000 train_loss:4.9301 train_time:27990ms step_avg:179.42ms\nstep:167/5000 train_loss:4.8304 train_time:28169ms step_avg:179.42ms\nstep:168/5000 train_loss:5.0126 train_time:28349ms step_avg:179.42ms\nstep:169/5000 train_loss:5.1058 train_time:28529ms step_avg:179.43ms\nstep:170/5000 train_loss:5.0079 train_time:28707ms step_avg:179.42ms\nstep:171/5000 train_loss:5.2859 train_time:28889ms step_avg:179.44ms\nstep:172/5000 train_loss:5.0204 train_time:29069ms step_avg:179.44ms\nstep:173/5000 train_loss:5.0497 train_time:29249ms step_avg:179.44ms\nstep:174/5000 train_loss:4.9789 train_time:29429ms step_avg:179.45ms\nstep:175/5000 train_loss:4.9645 train_time:29608ms step_avg:179.44ms\nstep:176/5000 train_loss:4.9128 train_time:29787ms step_avg:179.44ms\nstep:177/5000 train_loss:4.9073 train_time:29966ms step_avg:179.44ms\nstep:178/5000 train_loss:5.1146 train_time:30146ms step_avg:179.44ms\nstep:179/5000 train_loss:5.0486 train_time:30325ms step_avg:179.44ms\nstep:180/5000 train_loss:4.8265 train_time:30504ms step_avg:179.44ms\nstep:181/5000 train_loss:4.8801 train_time:30684ms step_avg:179.44ms\nstep:182/5000 train_loss:4.9142 train_time:30864ms step_avg:179.44ms\nstep:183/5000 train_loss:4.9786 train_time:31044ms step_avg:179.45ms\nstep:184/5000 train_loss:4.9244 train_time:31224ms step_avg:179.45ms\nstep:185/5000 train_loss:5.0155 train_time:31404ms step_avg:179.45ms\nstep:186/5000 train_loss:4.8275 train_time:31584ms step_avg:179.45ms\nstep:187/5000 train_loss:5.0559 train_time:31764ms step_avg:179.45ms\nstep:188/5000 train_loss:4.8172 train_time:31944ms step_avg:179.46ms\nstep:189/5000 train_loss:4.9541 train_time:32124ms step_avg:179.46ms\nstep:190/5000 train_loss:4.9610 train_time:32304ms step_avg:179.46ms\nstep:191/5000 train_loss:4.9689 train_time:32484ms step_avg:179.47ms\nstep:192/5000 train_loss:4.9626 train_time:32663ms step_avg:179.47ms\nstep:193/5000 train_loss:4.8797 train_time:32843ms step_avg:179.47ms\nstep:194/5000 train_loss:4.8948 train_time:33023ms step_avg:179.47ms\nstep:195/5000 train_loss:4.9473 train_time:33203ms step_avg:179.47ms\nstep:196/5000 train_loss:4.7945 train_time:33383ms step_avg:179.48ms\nstep:197/5000 train_loss:4.8233 train_time:33563ms step_avg:179.48ms\nstep:198/5000 train_loss:5.0156 train_time:33743ms step_avg:179.48ms\nstep:199/5000 train_loss:4.9801 train_time:33923ms step_avg:179.49ms\nstep:200/5000 train_loss:4.8372 train_time:34102ms step_avg:179.48ms\nstep:201/5000 train_loss:4.8876 train_time:34282ms step_avg:179.49ms\nstep:202/5000 train_loss:4.8887 train_time:34462ms step_avg:179.49ms\nstep:203/5000 train_loss:4.6333 train_time:34641ms step_avg:179.49ms\nstep:204/5000 train_loss:4.8137 train_time:34821ms step_avg:179.49ms\nstep:205/5000 train_loss:4.9079 train_time:35000ms step_avg:179.49ms\nstep:206/5000 train_loss:4.7769 train_time:35180ms step_avg:179.49ms\nstep:207/5000 train_loss:4.8834 train_time:35358ms step_avg:179.48ms\nstep:208/5000 train_loss:4.9832 train_time:35537ms step_avg:179.48ms\nstep:209/5000 train_loss:4.9147 train_time:35716ms step_avg:179.48ms\nstep:210/5000 train_loss:4.9487 train_time:35895ms step_avg:179.48ms\nstep:211/5000 train_loss:4.9212 train_time:36074ms step_avg:179.47ms\nstep:212/5000 train_loss:4.7953 train_time:36253ms step_avg:179.47ms\nstep:213/5000 train_loss:5.0539 train_time:36432ms step_avg:179.47ms\nstep:214/5000 train_loss:4.7637 train_time:36612ms step_avg:179.47ms\nstep:215/5000 train_loss:4.8169 train_time:36792ms step_avg:179.47ms\nstep:216/5000 train_loss:4.8392 train_time:36971ms step_avg:179.47ms\nstep:217/5000 train_loss:4.7478 train_time:37151ms step_avg:179.48ms\nstep:218/5000 train_loss:5.1754 train_time:37332ms step_avg:179.48ms\nstep:219/5000 train_loss:4.8682 train_time:37511ms step_avg:179.48ms\nstep:220/5000 train_loss:4.7554 train_time:37690ms step_avg:179.48ms\nstep:221/5000 train_loss:4.7764 train_time:37870ms step_avg:179.48ms\nstep:222/5000 train_loss:4.7789 train_time:38049ms step_avg:179.48ms\nstep:223/5000 train_loss:4.8027 train_time:38230ms step_avg:179.48ms\nstep:224/5000 train_loss:4.7761 train_time:38409ms step_avg:179.48ms\nstep:225/5000 train_loss:4.8270 train_time:38589ms step_avg:179.48ms\nstep:226/5000 train_loss:4.6415 train_time:38768ms step_avg:179.48ms\nstep:227/5000 train_loss:4.9095 train_time:38948ms step_avg:179.48ms\nstep:228/5000 train_loss:4.7981 train_time:39129ms step_avg:179.49ms\nstep:229/5000 train_loss:4.8177 train_time:39310ms step_avg:179.50ms\nstep:230/5000 train_loss:4.7482 train_time:39491ms step_avg:179.50ms\nstep:231/5000 train_loss:4.8711 train_time:39670ms step_avg:179.50ms\nstep:232/5000 train_loss:4.8171 train_time:39850ms step_avg:179.50ms\nstep:233/5000 train_loss:4.6174 train_time:40029ms step_avg:179.50ms\nstep:234/5000 train_loss:4.7342 train_time:40208ms step_avg:179.50ms\nstep:235/5000 train_loss:4.7867 train_time:40390ms step_avg:179.51ms\nstep:236/5000 train_loss:4.8008 train_time:40571ms step_avg:179.52ms\nstep:237/5000 train_loss:4.8288 train_time:40751ms step_avg:179.52ms\nstep:238/5000 train_loss:4.8626 train_time:40931ms step_avg:179.52ms\nstep:239/5000 train_loss:4.7813 train_time:41111ms step_avg:179.52ms\nstep:240/5000 train_loss:4.6943 train_time:41291ms step_avg:179.53ms\nstep:241/5000 train_loss:4.8082 train_time:41472ms step_avg:179.53ms\nstep:242/5000 train_loss:4.6013 train_time:41650ms step_avg:179.52ms\nstep:243/5000 train_loss:4.6371 train_time:41828ms step_avg:179.52ms\nstep:244/5000 train_loss:4.7845 train_time:42007ms step_avg:179.52ms\nstep:245/5000 train_loss:4.6944 train_time:42187ms step_avg:179.52ms\nstep:246/5000 train_loss:4.7247 train_time:42366ms step_avg:179.52ms\nstep:247/5000 train_loss:4.7047 train_time:42547ms step_avg:179.53ms\nstep:248/5000 train_loss:4.7911 train_time:42727ms step_avg:179.52ms\nstep:249/5000 train_loss:4.9205 train_time:42906ms step_avg:179.53ms\nstep:250/5000 train_loss:4.7288 train_time:43086ms step_avg:179.53ms\nstep:251/5000 train_loss:4.6270 train_time:43264ms step_avg:179.52ms\nstep:252/5000 train_loss:4.7241 train_time:43443ms step_avg:179.52ms\nstep:253/5000 train_loss:4.6740 train_time:43621ms step_avg:179.51ms\nstep:254/5000 train_loss:4.5048 train_time:43801ms step_avg:179.51ms\nstep:255/5000 train_loss:4.6830 train_time:43980ms step_avg:179.51ms\nstep:256/5000 train_loss:4.6874 train_time:44158ms step_avg:179.50ms\nstep:257/5000 train_loss:4.8678 train_time:44337ms step_avg:179.50ms\nstep:258/5000 train_loss:4.5467 train_time:44516ms step_avg:179.50ms\nstep:259/5000 train_loss:4.7116 train_time:44694ms step_avg:179.49ms\nstep:260/5000 train_loss:4.5423 train_time:44874ms step_avg:179.50ms\nstep:261/5000 train_loss:4.6598 train_time:45054ms step_avg:179.50ms\nstep:262/5000 train_loss:4.6630 train_time:45233ms step_avg:179.50ms\nstep:263/5000 train_loss:4.6770 train_time:45412ms step_avg:179.49ms\nstep:264/5000 train_loss:4.6156 train_time:45591ms step_avg:179.49ms\nstep:265/5000 train_loss:4.8591 train_time:45771ms step_avg:179.49ms\nstep:266/5000 train_loss:4.5702 train_time:45951ms step_avg:179.50ms\nstep:267/5000 train_loss:4.6346 train_time:46131ms step_avg:179.50ms\nstep:268/5000 train_loss:4.6639 train_time:46310ms step_avg:179.50ms\nstep:269/5000 train_loss:4.6236 train_time:46489ms step_avg:179.50ms\nstep:270/5000 train_loss:4.8202 train_time:46669ms step_avg:179.50ms\nstep:271/5000 train_loss:4.8059 train_time:46849ms step_avg:179.50ms\nstep:272/5000 train_loss:4.6792 train_time:47029ms step_avg:179.50ms\nstep:273/5000 train_loss:4.8074 train_time:47209ms step_avg:179.50ms\nstep:274/5000 train_loss:4.4773 train_time:47389ms step_avg:179.51ms\nstep:275/5000 train_loss:4.6120 train_time:47569ms step_avg:179.51ms\nstep:276/5000 train_loss:4.6434 train_time:47749ms step_avg:179.51ms\nstep:277/5000 train_loss:4.6306 train_time:47930ms step_avg:179.51ms\nstep:278/5000 train_loss:4.5601 train_time:48111ms step_avg:179.52ms\nstep:279/5000 train_loss:4.6524 train_time:48290ms step_avg:179.52ms\nstep:280/5000 train_loss:4.5637 train_time:48469ms step_avg:179.51ms\nstep:281/5000 train_loss:4.6638 train_time:48648ms step_avg:179.51ms\nstep:282/5000 train_loss:4.6887 train_time:48828ms step_avg:179.51ms\nstep:283/5000 train_loss:4.5592 train_time:49007ms step_avg:179.51ms\nstep:284/5000 train_loss:4.5846 train_time:49187ms step_avg:179.52ms\nstep:285/5000 train_loss:4.7212 train_time:49368ms step_avg:179.52ms\nstep:286/5000 train_loss:4.5877 train_time:49547ms step_avg:179.52ms\nstep:287/5000 train_loss:4.6272 train_time:49726ms step_avg:179.52ms\nstep:288/5000 train_loss:4.6247 train_time:49906ms step_avg:179.52ms\nstep:289/5000 train_loss:4.5949 train_time:50087ms step_avg:179.52ms\nstep:290/5000 train_loss:4.4883 train_time:50265ms step_avg:179.52ms\nstep:291/5000 train_loss:4.7339 train_time:50444ms step_avg:179.51ms\nstep:292/5000 train_loss:4.5683 train_time:50625ms step_avg:179.52ms\nstep:293/5000 train_loss:4.6608 train_time:50804ms step_avg:179.52ms\nstep:294/5000 train_loss:4.6300 train_time:50982ms step_avg:179.51ms\nstep:295/5000 train_loss:4.4756 train_time:51162ms step_avg:179.51ms\nstep:296/5000 train_loss:4.4677 train_time:51340ms step_avg:179.51ms\nstep:297/5000 train_loss:4.6866 train_time:51520ms step_avg:179.51ms\nstep:298/5000 train_loss:4.5042 train_time:51699ms step_avg:179.51ms\nstep:299/5000 train_loss:4.4863 train_time:51879ms step_avg:179.51ms\nstep:300/5000 train_loss:4.4827 train_time:52057ms step_avg:179.51ms\nstep:301/5000 train_loss:4.5455 train_time:52235ms step_avg:179.50ms\nstep:302/5000 train_loss:4.5413 train_time:52416ms step_avg:179.51ms\nstep:303/5000 train_loss:4.4581 train_time:52594ms step_avg:179.50ms\nstep:304/5000 train_loss:4.4186 train_time:52773ms step_avg:179.50ms\nstep:305/5000 train_loss:4.5998 train_time:52951ms step_avg:179.50ms\nstep:306/5000 train_loss:4.6453 train_time:53130ms step_avg:179.49ms\nstep:307/5000 train_loss:4.4844 train_time:53310ms step_avg:179.49ms\nstep:308/5000 train_loss:4.6086 train_time:53490ms step_avg:179.50ms\nstep:309/5000 train_loss:4.5697 train_time:53668ms step_avg:179.49ms\nstep:310/5000 train_loss:4.9371 train_time:53849ms step_avg:179.50ms\nstep:311/5000 train_loss:4.5728 train_time:54029ms step_avg:179.50ms\nstep:312/5000 train_loss:4.5665 train_time:54210ms step_avg:179.50ms\nstep:313/5000 train_loss:4.6844 train_time:54389ms step_avg:179.50ms\nstep:314/5000 train_loss:4.5489 train_time:54569ms step_avg:179.50ms\nstep:315/5000 train_loss:4.4045 train_time:54749ms step_avg:179.50ms\nstep:316/5000 train_loss:4.2532 train_time:54929ms step_avg:179.51ms\nstep:317/5000 train_loss:4.5493 train_time:55110ms step_avg:179.51ms\nstep:318/5000 train_loss:4.5989 train_time:55289ms step_avg:179.51ms\nstep:319/5000 train_loss:4.4551 train_time:55467ms step_avg:179.51ms\nstep:320/5000 train_loss:4.5551 train_time:55647ms step_avg:179.51ms\nstep:321/5000 train_loss:4.5446 train_time:55827ms step_avg:179.51ms\nstep:322/5000 train_loss:4.4145 train_time:56006ms step_avg:179.51ms\nstep:323/5000 train_loss:4.4504 train_time:56187ms step_avg:179.51ms\nstep:324/5000 train_loss:4.4832 train_time:56367ms step_avg:179.51ms\nstep:325/5000 train_loss:4.5852 train_time:56546ms step_avg:179.51ms\nstep:326/5000 train_loss:4.5728 train_time:56726ms step_avg:179.51ms\nstep:327/5000 train_loss:4.4717 train_time:56907ms step_avg:179.52ms\nstep:328/5000 train_loss:4.5180 train_time:57087ms step_avg:179.52ms\nstep:329/5000 train_loss:5.5521 train_time:57267ms step_avg:179.52ms\nstep:330/5000 train_loss:4.5440 train_time:57446ms step_avg:179.52ms\nstep:331/5000 train_loss:4.4433 train_time:57626ms step_avg:179.52ms\nstep:332/5000 train_loss:4.5322 train_time:57805ms step_avg:179.52ms\nstep:333/5000 train_loss:4.4916 train_time:57987ms step_avg:179.53ms\nstep:334/5000 train_loss:4.4932 train_time:58166ms step_avg:179.53ms\nstep:335/5000 train_loss:4.4667 train_time:58347ms step_avg:179.53ms\nstep:336/5000 train_loss:4.4284 train_time:58527ms step_avg:179.53ms\nstep:337/5000 train_loss:4.5483 train_time:58706ms step_avg:179.53ms\nstep:338/5000 train_loss:4.3956 train_time:58886ms step_avg:179.53ms\nstep:339/5000 train_loss:4.4093 train_time:59065ms step_avg:179.53ms\nstep:340/5000 train_loss:4.4050 train_time:59245ms step_avg:179.53ms\nstep:341/5000 train_loss:4.5243 train_time:59428ms step_avg:179.54ms\nstep:342/5000 train_loss:4.6083 train_time:59611ms step_avg:179.55ms\nstep:343/5000 train_loss:4.5929 train_time:59791ms step_avg:179.55ms\nstep:344/5000 train_loss:4.5599 train_time:59971ms step_avg:179.56ms\nstep:345/5000 train_loss:4.6000 train_time:60152ms step_avg:179.56ms\nstep:346/5000 train_loss:4.5089 train_time:60332ms step_avg:179.56ms\nstep:347/5000 train_loss:4.4057 train_time:60510ms step_avg:179.56ms\nstep:348/5000 train_loss:4.5406 train_time:60690ms step_avg:179.56ms\nstep:349/5000 train_loss:4.5063 train_time:60868ms step_avg:179.55ms\nstep:350/5000 train_loss:4.4762 train_time:61047ms step_avg:179.55ms\nstep:351/5000 train_loss:4.4706 train_time:61227ms step_avg:179.55ms\nstep:352/5000 train_loss:4.4060 train_time:61408ms step_avg:179.55ms\nstep:353/5000 train_loss:4.5001 train_time:61587ms step_avg:179.56ms\nstep:354/5000 train_loss:4.4591 train_time:61766ms step_avg:179.55ms\nstep:355/5000 train_loss:4.5712 train_time:61945ms step_avg:179.55ms\nstep:356/5000 train_loss:4.4390 train_time:62125ms step_avg:179.55ms\nstep:357/5000 train_loss:4.3059 train_time:62306ms step_avg:179.56ms\nstep:358/5000 train_loss:4.4277 train_time:62487ms step_avg:179.56ms\nstep:359/5000 train_loss:4.4879 train_time:62668ms step_avg:179.56ms\nstep:360/5000 train_loss:4.3547 train_time:62847ms step_avg:179.56ms\nstep:361/5000 train_loss:4.4197 train_time:63027ms step_avg:179.56ms\nstep:362/5000 train_loss:4.4157 train_time:63208ms step_avg:179.57ms\nstep:363/5000 train_loss:4.4169 train_time:63388ms step_avg:179.57ms\nstep:364/5000 train_loss:4.5095 train_time:63568ms step_avg:179.57ms\nstep:365/5000 train_loss:4.3430 train_time:63747ms step_avg:179.57ms\nstep:366/5000 train_loss:4.5342 train_time:63927ms step_avg:179.57ms\nstep:367/5000 train_loss:4.5826 train_time:64106ms step_avg:179.57ms\nstep:368/5000 train_loss:4.4793 train_time:64287ms step_avg:179.57ms\nstep:369/5000 train_loss:4.4194 train_time:64467ms step_avg:179.57ms\nstep:370/5000 train_loss:4.3692 train_time:64647ms step_avg:179.58ms\nstep:371/5000 train_loss:4.4090 train_time:64826ms step_avg:179.57ms\nstep:372/5000 train_loss:4.5315 train_time:65005ms step_avg:179.57ms\nstep:373/5000 train_loss:4.3627 train_time:65184ms step_avg:179.57ms\nstep:374/5000 train_loss:4.4401 train_time:65362ms step_avg:179.57ms\nstep:375/5000 train_loss:4.4652 train_time:65542ms step_avg:179.57ms\nstep:376/5000 train_loss:4.6901 train_time:65722ms step_avg:179.57ms\nstep:377/5000 train_loss:4.3438 train_time:65901ms step_avg:179.57ms\nstep:378/5000 train_loss:4.4778 train_time:66082ms step_avg:179.57ms\nstep:379/5000 train_loss:4.4840 train_time:66262ms step_avg:179.57ms\nstep:380/5000 train_loss:4.4306 train_time:66441ms step_avg:179.57ms\nstep:381/5000 train_loss:4.4653 train_time:66621ms step_avg:179.57ms\nstep:382/5000 train_loss:4.3855 train_time:66800ms step_avg:179.57ms\nstep:383/5000 train_loss:4.4895 train_time:66979ms step_avg:179.57ms\nstep:384/5000 train_loss:4.3191 train_time:67157ms step_avg:179.56ms\nstep:385/5000 train_loss:4.3521 train_time:67336ms step_avg:179.56ms\nstep:386/5000 train_loss:4.4086 train_time:67514ms step_avg:179.56ms\nstep:387/5000 train_loss:4.3907 train_time:67693ms step_avg:179.56ms\nstep:388/5000 train_loss:4.4859 train_time:67871ms step_avg:179.55ms\nstep:389/5000 train_loss:4.4449 train_time:68051ms step_avg:179.55ms\nstep:390/5000 train_loss:4.3795 train_time:68231ms step_avg:179.56ms\nstep:391/5000 train_loss:4.4186 train_time:68412ms step_avg:179.56ms\nstep:392/5000 train_loss:4.3472 train_time:68593ms step_avg:179.56ms\nstep:393/5000 train_loss:4.3693 train_time:68771ms step_avg:179.56ms\nstep:394/5000 train_loss:4.4521 train_time:68951ms step_avg:179.56ms\nstep:395/5000 train_loss:4.2810 train_time:69131ms step_avg:179.56ms\nstep:396/5000 train_loss:4.3324 train_time:69311ms step_avg:179.56ms\nstep:397/5000 train_loss:4.4130 train_time:69490ms step_avg:179.56ms\nstep:398/5000 train_loss:4.4985 train_time:69670ms step_avg:179.56ms\nstep:399/5000 train_loss:4.2710 train_time:69850ms step_avg:179.56ms\nstep:400/5000 train_loss:4.3773 train_time:70030ms step_avg:179.56ms\nstep:401/5000 train_loss:4.2689 train_time:70210ms step_avg:179.56ms\nstep:402/5000 train_loss:4.3631 train_time:70390ms step_avg:179.57ms\nstep:403/5000 train_loss:4.2936 train_time:70571ms step_avg:179.57ms\nstep:404/5000 train_loss:4.2479 train_time:70751ms step_avg:179.57ms\nstep:405/5000 train_loss:4.3766 train_time:70931ms step_avg:179.57ms\nstep:406/5000 train_loss:4.5181 train_time:71110ms step_avg:179.57ms\nstep:407/5000 train_loss:4.3757 train_time:71289ms step_avg:179.57ms\nstep:408/5000 train_loss:4.4152 train_time:71469ms step_avg:179.57ms\nstep:409/5000 train_loss:4.4887 train_time:71649ms step_avg:179.57ms\nstep:410/5000 train_loss:4.3781 train_time:71829ms step_avg:179.57ms\nstep:411/5000 train_loss:4.4642 train_time:72009ms step_avg:179.57ms\nstep:412/5000 train_loss:4.3506 train_time:72189ms step_avg:179.58ms\nstep:413/5000 train_loss:4.4700 train_time:72369ms step_avg:179.58ms\nstep:414/5000 train_loss:4.3805 train_time:72550ms step_avg:179.58ms\nstep:415/5000 train_loss:5.4223 train_time:72731ms step_avg:179.58ms\nstep:416/5000 train_loss:4.4299 train_time:72912ms step_avg:179.59ms\nstep:417/5000 train_loss:4.3626 train_time:73091ms step_avg:179.58ms\nstep:418/5000 train_loss:4.3665 train_time:73269ms step_avg:179.58ms\nstep:419/5000 train_loss:4.4532 train_time:73451ms step_avg:179.59ms\nstep:420/5000 train_loss:4.3922 train_time:73629ms step_avg:179.58ms\nstep:421/5000 train_loss:4.4323 train_time:73810ms step_avg:179.59ms\nstep:422/5000 train_loss:4.3468 train_time:73989ms step_avg:179.58ms\nstep:423/5000 train_loss:4.5323 train_time:74169ms step_avg:179.59ms\nstep:424/5000 train_loss:4.5418 train_time:74348ms step_avg:179.59ms\nstep:425/5000 train_loss:4.4119 train_time:74529ms step_avg:179.59ms\nstep:426/5000 train_loss:4.3390 train_time:74710ms step_avg:179.59ms\nstep:427/5000 train_loss:4.4907 train_time:74890ms step_avg:179.59ms\nstep:428/5000 train_loss:4.3718 train_time:75070ms step_avg:179.59ms\nstep:429/5000 train_loss:4.4784 train_time:75250ms step_avg:179.59ms\nstep:430/5000 train_loss:4.1903 train_time:75429ms step_avg:179.59ms\nstep:431/5000 train_loss:4.2324 train_time:75610ms step_avg:179.60ms\nstep:432/5000 train_loss:4.2478 train_time:75789ms step_avg:179.60ms\nstep:433/5000 train_loss:4.3835 train_time:75969ms step_avg:179.60ms\nstep:434/5000 train_loss:4.3267 train_time:76149ms step_avg:179.60ms\nstep:435/5000 train_loss:4.3200 train_time:76330ms step_avg:179.60ms\nstep:436/5000 train_loss:4.6080 train_time:76509ms step_avg:179.60ms\nstep:437/5000 train_loss:4.2923 train_time:76688ms step_avg:179.60ms\nstep:438/5000 train_loss:4.3016 train_time:76868ms step_avg:179.60ms\nstep:439/5000 train_loss:4.3458 train_time:77048ms step_avg:179.60ms\nstep:440/5000 train_loss:4.3738 train_time:77227ms step_avg:179.60ms\nstep:441/5000 train_loss:4.5242 train_time:77406ms step_avg:179.60ms\nstep:442/5000 train_loss:4.2517 train_time:77585ms step_avg:179.59ms\nstep:443/5000 train_loss:4.3954 train_time:77764ms step_avg:179.59ms\nstep:444/5000 train_loss:4.3103 train_time:77942ms step_avg:179.59ms\nstep:445/5000 train_loss:4.3646 train_time:78121ms step_avg:179.59ms\nstep:446/5000 train_loss:4.3991 train_time:78300ms step_avg:179.59ms\nstep:447/5000 train_loss:4.3904 train_time:78479ms step_avg:179.59ms\nstep:448/5000 train_loss:4.5131 train_time:78658ms step_avg:179.59ms\nstep:449/5000 train_loss:4.2473 train_time:78837ms step_avg:179.58ms\nstep:450/5000 train_loss:4.3129 train_time:79015ms step_avg:179.58ms\nstep:451/5000 train_loss:4.4800 train_time:79192ms step_avg:179.57ms\nstep:452/5000 train_loss:4.1950 train_time:79372ms step_avg:179.57ms\nstep:453/5000 train_loss:4.6314 train_time:79551ms step_avg:179.57ms\nstep:454/5000 train_loss:4.1972 train_time:79731ms step_avg:179.57ms\nstep:455/5000 train_loss:4.3132 train_time:79911ms step_avg:179.58ms\nstep:456/5000 train_loss:4.4048 train_time:80091ms step_avg:179.58ms\nstep:457/5000 train_loss:4.3496 train_time:80270ms step_avg:179.57ms\nstep:458/5000 train_loss:4.3363 train_time:80450ms step_avg:179.58ms\nstep:459/5000 train_loss:4.3678 train_time:80630ms step_avg:179.58ms\nstep:460/5000 train_loss:4.3252 train_time:80810ms step_avg:179.58ms\nstep:461/5000 train_loss:4.2019 train_time:80992ms step_avg:179.58ms\nstep:462/5000 train_loss:4.3244 train_time:81171ms step_avg:179.58ms\nstep:463/5000 train_loss:4.3305 train_time:81350ms step_avg:179.58ms\nstep:464/5000 train_loss:4.3702 train_time:81531ms step_avg:179.58ms\nstep:465/5000 train_loss:4.2746 train_time:81711ms step_avg:179.58ms\nstep:466/5000 train_loss:4.2209 train_time:81892ms step_avg:179.59ms\nstep:467/5000 train_loss:4.3511 train_time:82072ms step_avg:179.59ms\nstep:468/5000 train_loss:4.1607 train_time:82251ms step_avg:179.59ms\nstep:469/5000 train_loss:4.3278 train_time:82431ms step_avg:179.59ms\nstep:470/5000 train_loss:4.4534 train_time:82610ms step_avg:179.59ms\nstep:471/5000 train_loss:4.4234 train_time:82790ms step_avg:179.59ms\nstep:472/5000 train_loss:4.4072 train_time:82969ms step_avg:179.59ms\nstep:473/5000 train_loss:4.2906 train_time:83149ms step_avg:179.59ms\nstep:474/5000 train_loss:4.3131 train_time:83327ms step_avg:179.58ms\nstep:475/5000 train_loss:4.3488 train_time:83506ms step_avg:179.58ms\nstep:476/5000 train_loss:4.2257 train_time:83685ms step_avg:179.58ms\nstep:477/5000 train_loss:4.3158 train_time:83864ms step_avg:179.58ms\nstep:478/5000 train_loss:4.1790 train_time:84043ms step_avg:179.58ms\nstep:479/5000 train_loss:4.2975 train_time:84223ms step_avg:179.58ms\nstep:480/5000 train_loss:4.3315 train_time:84402ms step_avg:179.58ms\nstep:481/5000 train_loss:4.3494 train_time:84581ms step_avg:179.58ms\nstep:482/5000 train_loss:4.1687 train_time:84760ms step_avg:179.58ms\nstep:483/5000 train_loss:4.2707 train_time:84938ms step_avg:179.57ms\nstep:484/5000 train_loss:4.3136 train_time:85117ms step_avg:179.57ms\nstep:485/5000 train_loss:4.3920 train_time:85296ms step_avg:179.57ms\nstep:486/5000 train_loss:4.3191 train_time:85474ms step_avg:179.57ms\nstep:487/5000 train_loss:4.2463 train_time:85652ms step_avg:179.56ms\nstep:488/5000 train_loss:4.2125 train_time:85835ms step_avg:179.57ms\nstep:489/5000 train_loss:4.1435 train_time:86012ms step_avg:179.57ms\nstep:490/5000 train_loss:4.1270 train_time:86192ms step_avg:179.57ms\nstep:491/5000 train_loss:4.3413 train_time:86371ms step_avg:179.56ms\nstep:492/5000 train_loss:4.2932 train_time:86550ms step_avg:179.56ms\nstep:493/5000 train_loss:4.1879 train_time:86730ms step_avg:179.56ms\nstep:494/5000 train_loss:4.4568 train_time:86911ms step_avg:179.57ms\nstep:495/5000 train_loss:4.2750 train_time:87091ms step_avg:179.57ms\nstep:496/5000 train_loss:4.1593 train_time:87270ms step_avg:179.57ms\nstep:497/5000 train_loss:4.2714 train_time:87450ms step_avg:179.57ms\nstep:498/5000 train_loss:4.3268 train_time:87629ms step_avg:179.57ms\nstep:499/5000 train_loss:4.3562 train_time:87809ms step_avg:179.57ms\nstep:500/5000 train_loss:4.1733 train_time:87988ms step_avg:179.57ms\nstep:501/5000 train_loss:4.3055 train_time:88168ms step_avg:179.57ms\nstep:502/5000 train_loss:4.2862 train_time:88347ms step_avg:179.57ms\nstep:503/5000 train_loss:4.3120 train_time:88527ms step_avg:179.57ms\nstep:504/5000 train_loss:4.1171 train_time:88706ms step_avg:179.57ms\nstep:505/5000 train_loss:4.2153 train_time:88886ms step_avg:179.57ms\nstep:506/5000 train_loss:4.3053 train_time:89066ms step_avg:179.57ms\nstep:507/5000 train_loss:4.3966 train_time:89244ms step_avg:179.57ms\nstep:508/5000 train_loss:4.3229 train_time:89423ms step_avg:179.56ms\nstep:509/5000 train_loss:4.4962 train_time:89603ms step_avg:179.57ms\nstep:510/5000 train_loss:4.2575 train_time:89782ms step_avg:179.56ms\nstep:511/5000 train_loss:4.1592 train_time:89961ms step_avg:179.56ms\nstep:512/5000 train_loss:4.3183 train_time:90141ms step_avg:179.56ms\nstep:513/5000 train_loss:4.3241 train_time:90320ms step_avg:179.56ms\nstep:514/5000 train_loss:4.5658 train_time:90498ms step_avg:179.56ms\nstep:515/5000 train_loss:4.4050 train_time:90677ms step_avg:179.56ms\nstep:516/5000 train_loss:4.3417 train_time:90856ms step_avg:179.56ms\nstep:517/5000 train_loss:4.2675 train_time:91034ms step_avg:179.55ms\nstep:518/5000 train_loss:4.2381 train_time:91213ms step_avg:179.55ms\nstep:519/5000 train_loss:4.0796 train_time:91393ms step_avg:179.55ms\nstep:520/5000 train_loss:4.2364 train_time:91570ms step_avg:179.55ms\nstep:521/5000 train_loss:4.1259 train_time:91749ms step_avg:179.55ms\nstep:522/5000 train_loss:4.2922 train_time:91928ms step_avg:179.55ms\nstep:523/5000 train_loss:4.2185 train_time:92106ms step_avg:179.54ms\nstep:524/5000 train_loss:4.4329 train_time:92288ms step_avg:179.55ms\nstep:525/5000 train_loss:4.2970 train_time:92468ms step_avg:179.55ms\nstep:526/5000 train_loss:4.2306 train_time:92645ms step_avg:179.55ms\nstep:527/5000 train_loss:4.1793 train_time:92824ms step_avg:179.54ms\nstep:528/5000 train_loss:4.1933 train_time:93003ms step_avg:179.54ms\nstep:529/5000 train_loss:4.2956 train_time:93181ms step_avg:179.54ms\nstep:530/5000 train_loss:4.3360 train_time:93361ms step_avg:179.54ms\nstep:531/5000 train_loss:4.2371 train_time:93540ms step_avg:179.54ms\nstep:532/5000 train_loss:4.1790 train_time:93718ms step_avg:179.54ms\nstep:533/5000 train_loss:4.2505 train_time:93897ms step_avg:179.53ms\nstep:534/5000 train_loss:4.1607 train_time:94075ms step_avg:179.53ms\nstep:535/5000 train_loss:4.2551 train_time:94253ms step_avg:179.53ms\nstep:536/5000 train_loss:4.3254 train_time:94432ms step_avg:179.53ms\nstep:537/5000 train_loss:4.1880 train_time:94611ms step_avg:179.53ms\nstep:538/5000 train_loss:4.3392 train_time:94791ms step_avg:179.53ms\nstep:539/5000 train_loss:4.3793 train_time:94970ms step_avg:179.53ms\nstep:540/5000 train_loss:4.1652 train_time:95149ms step_avg:179.53ms\nstep:541/5000 train_loss:4.1770 train_time:95328ms step_avg:179.53ms\nstep:542/5000 train_loss:4.0932 train_time:95507ms step_avg:179.53ms\nstep:543/5000 train_loss:4.3035 train_time:95687ms step_avg:179.53ms\nstep:544/5000 train_loss:4.1630 train_time:95865ms step_avg:179.52ms\nstep:545/5000 train_loss:4.3249 train_time:96045ms step_avg:179.52ms\nstep:546/5000 train_loss:4.2746 train_time:96225ms step_avg:179.52ms\nstep:547/5000 train_loss:4.1148 train_time:96407ms step_avg:179.53ms\nstep:548/5000 train_loss:4.2015 train_time:96588ms step_avg:179.53ms\nstep:549/5000 train_loss:4.1656 train_time:96768ms step_avg:179.53ms\nstep:550/5000 train_loss:4.1894 train_time:96948ms step_avg:179.53ms\nstep:551/5000 train_loss:4.2336 train_time:97128ms step_avg:179.53ms\nstep:552/5000 train_loss:4.2611 train_time:97307ms step_avg:179.53ms\nstep:553/5000 train_loss:4.2699 train_time:97486ms step_avg:179.53ms\nstep:554/5000 train_loss:4.2679 train_time:97665ms step_avg:179.53ms\nstep:555/5000 train_loss:4.2164 train_time:97845ms step_avg:179.53ms\nstep:556/5000 train_loss:4.3547 train_time:98024ms step_avg:179.53ms\nstep:557/5000 train_loss:4.3332 train_time:98206ms step_avg:179.53ms\nstep:558/5000 train_loss:4.3572 train_time:98382ms step_avg:179.53ms\nstep:559/5000 train_loss:4.2233 train_time:98560ms step_avg:179.53ms\nstep:560/5000 train_loss:4.2493 train_time:98738ms step_avg:179.52ms\nstep:561/5000 train_loss:4.3364 train_time:98917ms step_avg:179.52ms\nstep:562/5000 train_loss:4.2146 train_time:99096ms step_avg:179.52ms\nstep:563/5000 train_loss:4.1356 train_time:99274ms step_avg:179.52ms\nstep:564/5000 train_loss:4.1413 train_time:99453ms step_avg:179.52ms\nstep:565/5000 train_loss:4.2841 train_time:99631ms step_avg:179.52ms\nstep:566/5000 train_loss:4.1939 train_time:99810ms step_avg:179.51ms\nstep:567/5000 train_loss:4.5146 train_time:99988ms step_avg:179.51ms\nstep:568/5000 train_loss:4.2619 train_time:100168ms step_avg:179.51ms\nstep:569/5000 train_loss:4.2487 train_time:100347ms step_avg:179.51ms\nstep:570/5000 train_loss:4.1580 train_time:100528ms step_avg:179.51ms\nstep:571/5000 train_loss:4.1773 train_time:100706ms step_avg:179.51ms\nstep:572/5000 train_loss:4.3244 train_time:100887ms step_avg:179.51ms\nstep:573/5000 train_loss:4.2036 train_time:101067ms step_avg:179.51ms\nstep:574/5000 train_loss:4.2607 train_time:101246ms step_avg:179.51ms\nstep:575/5000 train_loss:4.2439 train_time:101426ms step_avg:179.51ms\nstep:576/5000 train_loss:4.4269 train_time:101605ms step_avg:179.51ms\nstep:577/5000 train_loss:4.1206 train_time:101784ms step_avg:179.51ms\nstep:578/5000 train_loss:4.2154 train_time:101963ms step_avg:179.51ms\nstep:579/5000 train_loss:4.2041 train_time:102142ms step_avg:179.51ms\nstep:580/5000 train_loss:4.1969 train_time:102321ms step_avg:179.51ms\nstep:581/5000 train_loss:4.2398 train_time:102500ms step_avg:179.51ms\nstep:582/5000 train_loss:4.0790 train_time:102680ms step_avg:179.51ms\nstep:583/5000 train_loss:4.2228 train_time:102858ms step_avg:179.51ms\nstep:584/5000 train_loss:4.2200 train_time:103035ms step_avg:179.50ms\nstep:585/5000 train_loss:4.1424 train_time:103213ms step_avg:179.50ms\nstep:586/5000 train_loss:4.2299 train_time:103391ms step_avg:179.50ms\nstep:587/5000 train_loss:4.1399 train_time:103569ms step_avg:179.50ms\nstep:588/5000 train_loss:4.2560 train_time:103748ms step_avg:179.49ms\nstep:589/5000 train_loss:4.1757 train_time:103928ms step_avg:179.50ms\nstep:590/5000 train_loss:4.2723 train_time:104107ms step_avg:179.49ms\nstep:591/5000 train_loss:4.1307 train_time:104286ms step_avg:179.49ms\nstep:592/5000 train_loss:4.0549 train_time:104466ms step_avg:179.49ms\nstep:593/5000 train_loss:4.1872 train_time:104645ms step_avg:179.49ms\nstep:594/5000 train_loss:4.2175 train_time:104824ms step_avg:179.49ms\nstep:595/5000 train_loss:4.1904 train_time:105006ms step_avg:179.50ms\nstep:596/5000 train_loss:4.2001 train_time:105185ms step_avg:179.50ms\nstep:597/5000 train_loss:4.3078 train_time:105363ms step_avg:179.49ms\nstep:598/5000 train_loss:4.3373 train_time:105542ms step_avg:179.49ms\nstep:599/5000 train_loss:4.5223 train_time:105720ms step_avg:179.49ms\nstep:600/5000 train_loss:4.1839 train_time:105899ms step_avg:179.49ms\nstep:601/5000 train_loss:4.2962 train_time:106078ms step_avg:179.49ms\nstep:602/5000 train_loss:4.1867 train_time:106255ms step_avg:179.49ms\nstep:603/5000 train_loss:4.3326 train_time:106434ms step_avg:179.48ms\nstep:604/5000 train_loss:4.1833 train_time:106612ms step_avg:179.48ms\nstep:605/5000 train_loss:4.1414 train_time:106790ms step_avg:179.48ms\nstep:606/5000 train_loss:4.1925 train_time:106970ms step_avg:179.48ms\nstep:607/5000 train_loss:4.0958 train_time:107153ms step_avg:179.49ms\nstep:608/5000 train_loss:4.0337 train_time:107331ms step_avg:179.48ms\nstep:609/5000 train_loss:4.0914 train_time:107511ms step_avg:179.48ms\nstep:610/5000 train_loss:4.2833 train_time:107689ms step_avg:179.48ms\nstep:611/5000 train_loss:4.1198 train_time:107867ms step_avg:179.48ms\nstep:612/5000 train_loss:4.2660 train_time:108046ms step_avg:179.48ms\nstep:613/5000 train_loss:4.0497 train_time:108227ms step_avg:179.48ms\nstep:614/5000 train_loss:4.2714 train_time:108409ms step_avg:179.48ms\nstep:615/5000 train_loss:4.2303 train_time:108587ms step_avg:179.48ms\nstep:616/5000 train_loss:4.0636 train_time:108768ms step_avg:179.49ms\nstep:617/5000 train_loss:4.2455 train_time:108949ms step_avg:179.49ms\nstep:618/5000 train_loss:4.1836 train_time:109128ms step_avg:179.49ms\nstep:619/5000 train_loss:4.3432 train_time:109308ms step_avg:179.49ms\nstep:620/5000 train_loss:4.0655 train_time:109489ms step_avg:179.49ms\nstep:621/5000 train_loss:4.3144 train_time:109668ms step_avg:179.49ms\nstep:622/5000 train_loss:4.2655 train_time:109846ms step_avg:179.49ms\nstep:623/5000 train_loss:4.1369 train_time:110026ms step_avg:179.49ms\nstep:624/5000 train_loss:4.1837 train_time:110204ms step_avg:179.49ms\nstep:625/5000 train_loss:4.2207 train_time:110383ms step_avg:179.49ms\nstep:626/5000 train_loss:4.3304 train_time:110563ms step_avg:179.49ms\nstep:627/5000 train_loss:4.2270 train_time:110743ms step_avg:179.49ms\nstep:628/5000 train_loss:4.2716 train_time:110922ms step_avg:179.49ms\nstep:629/5000 train_loss:4.2230 train_time:111101ms step_avg:179.49ms\nstep:630/5000 train_loss:4.1617 train_time:111281ms step_avg:179.49ms\nstep:631/5000 train_loss:4.1228 train_time:111460ms step_avg:179.48ms\nstep:632/5000 train_loss:4.1766 train_time:111639ms step_avg:179.48ms\nstep:633/5000 train_loss:4.1009 train_time:111817ms step_avg:179.48ms\nstep:634/5000 train_loss:4.2311 train_time:111995ms step_avg:179.48ms\nstep:635/5000 train_loss:4.2875 train_time:112174ms step_avg:179.48ms\nstep:636/5000 train_loss:4.1286 train_time:112353ms step_avg:179.48ms\nstep:637/5000 train_loss:4.2177 train_time:112533ms step_avg:179.48ms\nstep:638/5000 train_loss:4.0344 train_time:112715ms step_avg:179.48ms\nstep:639/5000 train_loss:4.2788 train_time:112893ms step_avg:179.48ms\nstep:640/5000 train_loss:4.0698 train_time:113071ms step_avg:179.48ms\nstep:641/5000 train_loss:4.1785 train_time:113251ms step_avg:179.48ms\nstep:642/5000 train_loss:4.2738 train_time:113430ms step_avg:179.48ms\nstep:643/5000 train_loss:4.2779 train_time:113613ms step_avg:179.48ms\nstep:644/5000 train_loss:4.1419 train_time:113792ms step_avg:179.48ms\nstep:645/5000 train_loss:4.1146 train_time:113968ms step_avg:179.48ms\nstep:646/5000 train_loss:4.2819 train_time:114146ms step_avg:179.47ms\nstep:647/5000 train_loss:4.1958 train_time:114325ms step_avg:179.47ms\nstep:648/5000 train_loss:4.1662 train_time:114504ms step_avg:179.47ms\nstep:649/5000 train_loss:4.1361 train_time:114684ms step_avg:179.47ms\nstep:650/5000 train_loss:4.2606 train_time:114863ms step_avg:179.47ms\nstep:651/5000 train_loss:4.2506 train_time:115042ms step_avg:179.47ms\nstep:652/5000 train_loss:4.1331 train_time:115221ms step_avg:179.47ms\nstep:653/5000 train_loss:4.0378 train_time:115400ms step_avg:179.47ms\nstep:654/5000 train_loss:4.1110 train_time:115578ms step_avg:179.47ms\nstep:655/5000 train_loss:4.3979 train_time:115757ms step_avg:179.47ms\nstep:656/5000 train_loss:4.0690 train_time:115934ms step_avg:179.46ms\nstep:657/5000 train_loss:4.0758 train_time:116113ms step_avg:179.46ms\nstep:658/5000 train_loss:4.2045 train_time:116291ms step_avg:179.46ms\nstep:659/5000 train_loss:4.1279 train_time:116470ms step_avg:179.46ms\nstep:660/5000 train_loss:4.1489 train_time:116648ms step_avg:179.46ms\nstep:661/5000 train_loss:4.0620 train_time:116828ms step_avg:179.46ms\nstep:662/5000 train_loss:4.2765 train_time:117007ms step_avg:179.46ms\nstep:663/5000 train_loss:4.1458 train_time:117187ms step_avg:179.46ms\nstep:664/5000 train_loss:4.2801 train_time:117364ms step_avg:179.46ms\nstep:665/5000 train_loss:4.2196 train_time:117543ms step_avg:179.45ms\nstep:666/5000 train_loss:4.1181 train_time:117721ms step_avg:179.45ms\nstep:667/5000 train_loss:4.0356 train_time:117901ms step_avg:179.45ms\nstep:668/5000 train_loss:4.1894 train_time:118081ms step_avg:179.45ms\nstep:669/5000 train_loss:3.9922 train_time:118260ms step_avg:179.45ms\nstep:670/5000 train_loss:4.1985 train_time:118438ms step_avg:179.45ms\nstep:671/5000 train_loss:4.3013 train_time:118616ms step_avg:179.45ms\nstep:672/5000 train_loss:4.1339 train_time:118794ms step_avg:179.45ms\nstep:673/5000 train_loss:4.0301 train_time:118972ms step_avg:179.44ms\nstep:674/5000 train_loss:4.0895 train_time:119151ms step_avg:179.44ms\nstep:675/5000 train_loss:4.0714 train_time:119331ms step_avg:179.45ms\nstep:676/5000 train_loss:4.1730 train_time:119510ms step_avg:179.44ms\nstep:677/5000 train_loss:4.1075 train_time:119690ms step_avg:179.44ms\nstep:678/5000 train_loss:3.9595 train_time:119868ms step_avg:179.44ms\nstep:679/5000 train_loss:4.3192 train_time:120048ms step_avg:179.44ms\nstep:680/5000 train_loss:4.2284 train_time:120227ms step_avg:179.44ms\nstep:681/5000 train_loss:4.0974 train_time:120407ms step_avg:179.44ms\nstep:682/5000 train_loss:4.1349 train_time:120587ms step_avg:179.45ms\nstep:683/5000 train_loss:4.2316 train_time:120766ms step_avg:179.44ms\nstep:684/5000 train_loss:3.6911 train_time:120945ms step_avg:179.44ms\nstep:685/5000 train_loss:4.1661 train_time:121125ms step_avg:179.44ms\nstep:686/5000 train_loss:4.1618 train_time:121303ms step_avg:179.44ms\nstep:687/5000 train_loss:4.2986 train_time:121482ms step_avg:179.44ms\nstep:688/5000 train_loss:4.0762 train_time:121662ms step_avg:179.44ms\nstep:689/5000 train_loss:4.3970 train_time:121841ms step_avg:179.44ms\nstep:690/5000 train_loss:4.2079 train_time:122021ms step_avg:179.44ms\nstep:691/5000 train_loss:4.2418 train_time:122200ms step_avg:179.44ms\nstep:692/5000 train_loss:4.0660 train_time:122379ms step_avg:179.44ms\nstep:693/5000 train_loss:4.0334 train_time:122557ms step_avg:179.44ms\nstep:694/5000 train_loss:4.3604 train_time:122735ms step_avg:179.44ms\nstep:695/5000 train_loss:4.0599 train_time:122914ms step_avg:179.44ms\nstep:696/5000 train_loss:4.2780 train_time:123092ms step_avg:179.43ms\nstep:697/5000 train_loss:4.0870 train_time:123270ms step_avg:179.43ms\nstep:698/5000 train_loss:4.1867 train_time:123449ms step_avg:179.43ms\nstep:699/5000 train_loss:4.0265 train_time:123629ms step_avg:179.43ms\nstep:700/5000 train_loss:4.0965 train_time:123808ms step_avg:179.43ms\nstep:701/5000 train_loss:4.2962 train_time:123989ms step_avg:179.43ms\nstep:702/5000 train_loss:4.0438 train_time:124167ms step_avg:179.43ms\nstep:703/5000 train_loss:4.1407 train_time:124346ms step_avg:179.43ms\nstep:704/5000 train_loss:4.1774 train_time:124525ms step_avg:179.43ms\nstep:705/5000 train_loss:4.1818 train_time:124704ms step_avg:179.43ms\nstep:706/5000 train_loss:4.0140 train_time:124884ms step_avg:179.43ms\nstep:707/5000 train_loss:4.0895 train_time:125063ms step_avg:179.43ms\nstep:708/5000 train_loss:4.3023 train_time:125241ms step_avg:179.43ms\nstep:709/5000 train_loss:4.2127 train_time:125421ms step_avg:179.43ms\nstep:710/5000 train_loss:4.1250 train_time:125601ms step_avg:179.43ms\nstep:711/5000 train_loss:4.0316 train_time:125780ms step_avg:179.43ms\nstep:712/5000 train_loss:4.1598 train_time:125959ms step_avg:179.43ms\nstep:713/5000 train_loss:4.2098 train_time:126137ms step_avg:179.43ms\nstep:714/5000 train_loss:4.0901 train_time:126316ms step_avg:179.43ms\nstep:715/5000 train_loss:4.2623 train_time:126494ms step_avg:179.42ms\nstep:716/5000 train_loss:4.1002 train_time:126671ms step_avg:179.42ms\nstep:717/5000 train_loss:4.1005 train_time:126850ms step_avg:179.42ms\nstep:718/5000 train_loss:4.0668 train_time:127030ms step_avg:179.42ms\nstep:719/5000 train_loss:4.1754 train_time:127209ms step_avg:179.42ms\nstep:720/5000 train_loss:4.1667 train_time:127388ms step_avg:179.42ms\nstep:721/5000 train_loss:4.1577 train_time:127567ms step_avg:179.42ms\nstep:722/5000 train_loss:4.2367 train_time:127747ms step_avg:179.42ms\nstep:723/5000 train_loss:4.2180 train_time:127927ms step_avg:179.42ms\nstep:724/5000 train_loss:4.0610 train_time:128105ms step_avg:179.42ms\nstep:725/5000 train_loss:4.0921 train_time:128284ms step_avg:179.42ms\nstep:726/5000 train_loss:4.0933 train_time:128463ms step_avg:179.42ms\nstep:727/5000 train_loss:4.1035 train_time:128643ms step_avg:179.42ms\nstep:728/5000 train_loss:4.0967 train_time:128821ms step_avg:179.42ms\nstep:729/5000 train_loss:4.1927 train_time:129000ms step_avg:179.42ms\nstep:730/5000 train_loss:4.0876 train_time:129177ms step_avg:179.41ms\nstep:731/5000 train_loss:4.0404 train_time:129355ms step_avg:179.41ms\nstep:732/5000 train_loss:4.0719 train_time:129534ms step_avg:179.41ms\nstep:733/5000 train_loss:4.0928 train_time:129712ms step_avg:179.41ms\nstep:734/5000 train_loss:4.0722 train_time:129891ms step_avg:179.41ms\nstep:735/5000 train_loss:4.1090 train_time:130072ms step_avg:179.41ms\nstep:736/5000 train_loss:4.2266 train_time:130250ms step_avg:179.41ms\nstep:737/5000 train_loss:4.0877 train_time:130429ms step_avg:179.41ms\nstep:738/5000 train_loss:4.1625 train_time:130611ms step_avg:179.41ms\nstep:739/5000 train_loss:3.9107 train_time:130789ms step_avg:179.41ms\nstep:740/5000 train_loss:4.1247 train_time:130969ms step_avg:179.41ms\nstep:741/5000 train_loss:4.1714 train_time:131148ms step_avg:179.41ms\nstep:742/5000 train_loss:4.1344 train_time:131328ms step_avg:179.41ms\nstep:743/5000 train_loss:4.1085 train_time:131507ms step_avg:179.41ms\nstep:744/5000 train_loss:4.2130 train_time:131688ms step_avg:179.41ms\nstep:745/5000 train_loss:4.0956 train_time:131866ms step_avg:179.41ms\nstep:746/5000 train_loss:4.2102 train_time:132045ms step_avg:179.41ms\nstep:747/5000 train_loss:4.1694 train_time:132226ms step_avg:179.41ms\nstep:748/5000 train_loss:4.1430 train_time:132409ms step_avg:179.42ms\nstep:749/5000 train_loss:4.2512 train_time:132589ms step_avg:179.42ms\nstep:750/5000 train_loss:4.1228 train_time:132769ms step_avg:179.42ms\nstep:751/5000 train_loss:3.9743 train_time:132949ms step_avg:179.42ms\nstep:752/5000 train_loss:4.1191 train_time:133131ms step_avg:179.42ms\nstep:753/5000 train_loss:4.0581 train_time:133311ms step_avg:179.42ms\nstep:754/5000 train_loss:4.0587 train_time:133489ms step_avg:179.42ms\nstep:755/5000 train_loss:4.2115 train_time:133667ms step_avg:179.42ms\nstep:756/5000 train_loss:3.9431 train_time:133847ms step_avg:179.42ms\nstep:757/5000 train_loss:3.9110 train_time:134026ms step_avg:179.42ms\nstep:758/5000 train_loss:4.1673 train_time:134207ms step_avg:179.42ms\nstep:759/5000 train_loss:4.0729 train_time:134387ms step_avg:179.42ms\nstep:760/5000 train_loss:4.0592 train_time:134566ms step_avg:179.42ms\nstep:761/5000 train_loss:3.9873 train_time:134743ms step_avg:179.42ms\nstep:762/5000 train_loss:4.2431 train_time:135154ms step_avg:179.73ms\nstep:763/5000 train_loss:3.9588 train_time:135331ms step_avg:179.72ms\nstep:764/5000 train_loss:4.0573 train_time:135509ms step_avg:179.72ms\nstep:765/5000 train_loss:4.0446 train_time:135687ms step_avg:179.72ms\nstep:766/5000 train_loss:4.0431 train_time:135865ms step_avg:179.72ms\nstep:767/5000 train_loss:4.0550 train_time:136052ms step_avg:179.73ms\nstep:768/5000 train_loss:3.9942 train_time:136231ms step_avg:179.72ms\nstep:769/5000 train_loss:4.2144 train_time:136408ms step_avg:179.72ms\nstep:770/5000 train_loss:4.1112 train_time:136587ms step_avg:179.72ms\nstep:771/5000 train_loss:4.2772 train_time:136764ms step_avg:179.72ms\nstep:772/5000 train_loss:4.2121 train_time:136943ms step_avg:179.72ms\nstep:773/5000 train_loss:4.2111 train_time:137125ms step_avg:179.72ms\nstep:774/5000 train_loss:4.0867 train_time:137308ms step_avg:179.72ms\nstep:775/5000 train_loss:4.1517 train_time:137487ms step_avg:179.72ms\nstep:776/5000 train_loss:4.1354 train_time:137666ms step_avg:179.72ms\nstep:777/5000 train_loss:4.1120 train_time:137844ms step_avg:179.72ms\nstep:778/5000 train_loss:4.0422 train_time:138023ms step_avg:179.72ms\nstep:779/5000 train_loss:4.2672 train_time:138203ms step_avg:179.72ms\nstep:780/5000 train_loss:4.3362 train_time:138383ms step_avg:179.72ms\nstep:781/5000 train_loss:4.1073 train_time:138562ms step_avg:179.72ms\nstep:782/5000 train_loss:4.0418 train_time:138741ms step_avg:179.72ms\nstep:783/5000 train_loss:4.0953 train_time:138920ms step_avg:179.72ms\nstep:784/5000 train_loss:4.1520 train_time:139099ms step_avg:179.71ms\nstep:785/5000 train_loss:4.1109 train_time:139278ms step_avg:179.71ms\nstep:786/5000 train_loss:4.0800 train_time:139457ms step_avg:179.71ms\nstep:787/5000 train_loss:4.1093 train_time:139636ms step_avg:179.71ms\nstep:788/5000 train_loss:4.0064 train_time:139814ms step_avg:179.71ms\nstep:789/5000 train_loss:4.0531 train_time:139992ms step_avg:179.71ms\nstep:790/5000 train_loss:3.9877 train_time:140171ms step_avg:179.71ms\nstep:791/5000 train_loss:4.0205 train_time:140353ms step_avg:179.71ms\nstep:792/5000 train_loss:4.1453 train_time:140532ms step_avg:179.71ms\nstep:793/5000 train_loss:4.1533 train_time:140711ms step_avg:179.71ms\nstep:794/5000 train_loss:4.2200 train_time:140889ms step_avg:179.71ms\nstep:795/5000 train_loss:3.9950 train_time:141067ms step_avg:179.70ms\nstep:796/5000 train_loss:4.0017 train_time:141249ms step_avg:179.71ms\nstep:797/5000 train_loss:4.0682 train_time:141428ms step_avg:179.71ms\nstep:798/5000 train_loss:4.1415 train_time:141608ms step_avg:179.71ms\nstep:799/5000 train_loss:4.0617 train_time:141789ms step_avg:179.71ms\nstep:800/5000 train_loss:4.0854 train_time:141968ms step_avg:179.71ms\nstep:801/5000 train_loss:4.0772 train_time:142147ms step_avg:179.71ms\nstep:802/5000 train_loss:4.8842 train_time:142326ms step_avg:179.70ms\nstep:803/5000 train_loss:4.0618 train_time:142506ms step_avg:179.71ms\nstep:804/5000 train_loss:3.9662 train_time:142686ms step_avg:179.71ms\nstep:805/5000 train_loss:4.1515 train_time:142865ms step_avg:179.70ms\nstep:806/5000 train_loss:4.3547 train_time:143044ms step_avg:179.70ms\nstep:807/5000 train_loss:4.0803 train_time:143223ms step_avg:179.70ms\nstep:808/5000 train_loss:4.2071 train_time:143403ms step_avg:179.70ms\nstep:809/5000 train_loss:4.1381 train_time:143583ms step_avg:179.70ms\nstep:810/5000 train_loss:4.1089 train_time:143763ms step_avg:179.70ms\nstep:811/5000 train_loss:3.9644 train_time:143943ms step_avg:179.70ms\nstep:812/5000 train_loss:4.0348 train_time:144122ms step_avg:179.70ms\nstep:813/5000 train_loss:4.2230 train_time:144301ms step_avg:179.70ms\nstep:814/5000 train_loss:4.0317 train_time:144481ms step_avg:179.70ms\nstep:815/5000 train_loss:4.1775 train_time:144660ms step_avg:179.70ms\nstep:816/5000 train_loss:4.1569 train_time:144839ms step_avg:179.70ms\nstep:817/5000 train_loss:4.0226 train_time:145017ms step_avg:179.70ms\nstep:818/5000 train_loss:4.0897 train_time:145197ms step_avg:179.70ms\nstep:819/5000 train_loss:4.1060 train_time:145375ms step_avg:179.70ms\nstep:820/5000 train_loss:4.2252 train_time:145554ms step_avg:179.70ms\nstep:821/5000 train_loss:4.0643 train_time:145733ms step_avg:179.70ms\nstep:822/5000 train_loss:4.0528 train_time:145912ms step_avg:179.69ms\nstep:823/5000 train_loss:4.0834 train_time:146091ms step_avg:179.69ms\nstep:824/5000 train_loss:3.9149 train_time:146269ms step_avg:179.69ms\nstep:825/5000 train_loss:4.0145 train_time:146449ms step_avg:179.69ms\nstep:826/5000 train_loss:4.0498 train_time:146631ms step_avg:179.69ms\nstep:827/5000 train_loss:4.1565 train_time:146809ms step_avg:179.69ms\nstep:828/5000 train_loss:4.0627 train_time:146987ms step_avg:179.69ms\nstep:829/5000 train_loss:4.0881 train_time:147166ms step_avg:179.69ms\nstep:830/5000 train_loss:4.0046 train_time:147345ms step_avg:179.69ms\nstep:831/5000 train_loss:4.0886 train_time:147525ms step_avg:179.69ms\nstep:832/5000 train_loss:4.0686 train_time:147706ms step_avg:179.69ms\nstep:833/5000 train_loss:4.1772 train_time:147885ms step_avg:179.69ms\nstep:834/5000 train_loss:4.1720 train_time:148063ms step_avg:179.69ms\nstep:835/5000 train_loss:4.0135 train_time:148242ms step_avg:179.69ms\nstep:836/5000 train_loss:4.2167 train_time:148422ms step_avg:179.69ms\nstep:837/5000 train_loss:4.0197 train_time:148602ms step_avg:179.69ms\nstep:838/5000 train_loss:3.9927 train_time:148783ms step_avg:179.69ms\nstep:839/5000 train_loss:4.0084 train_time:148964ms step_avg:179.69ms\nstep:840/5000 train_loss:4.1563 train_time:149143ms step_avg:179.69ms\nstep:841/5000 train_loss:4.2246 train_time:149321ms step_avg:179.69ms\nstep:842/5000 train_loss:4.1050 train_time:149500ms step_avg:179.69ms\nstep:843/5000 train_loss:4.2187 train_time:149681ms step_avg:179.69ms\nstep:844/5000 train_loss:4.0397 train_time:149860ms step_avg:179.69ms\nstep:845/5000 train_loss:4.1014 train_time:150039ms step_avg:179.69ms\nstep:846/5000 train_loss:4.0959 train_time:150217ms step_avg:179.68ms\nstep:847/5000 train_loss:4.0340 train_time:150395ms step_avg:179.68ms\nstep:848/5000 train_loss:4.0693 train_time:150573ms step_avg:179.68ms\nstep:849/5000 train_loss:4.0631 train_time:150752ms step_avg:179.68ms\nstep:850/5000 train_loss:4.0448 train_time:150931ms step_avg:179.68ms\nstep:851/5000 train_loss:4.2049 train_time:151111ms step_avg:179.68ms\nstep:852/5000 train_loss:4.0216 train_time:151289ms step_avg:179.68ms\nstep:853/5000 train_loss:4.1085 train_time:151469ms step_avg:179.68ms\nstep:854/5000 train_loss:3.9994 train_time:151647ms step_avg:179.68ms\nstep:855/5000 train_loss:4.1624 train_time:151826ms step_avg:179.68ms\nstep:856/5000 train_loss:4.0883 train_time:152006ms step_avg:179.68ms\nstep:857/5000 train_loss:4.0389 train_time:152185ms step_avg:179.68ms\nstep:858/5000 train_loss:4.1236 train_time:152364ms step_avg:179.67ms\nstep:859/5000 train_loss:4.1710 train_time:152544ms step_avg:179.67ms\nstep:860/5000 train_loss:3.9200 train_time:152723ms step_avg:179.67ms\nstep:861/5000 train_loss:4.2418 train_time:152904ms step_avg:179.68ms\nstep:862/5000 train_loss:4.0655 train_time:153083ms step_avg:179.67ms\nstep:863/5000 train_loss:4.0644 train_time:153261ms step_avg:179.67ms\nstep:864/5000 train_loss:3.9787 train_time:153440ms step_avg:179.67ms\nstep:865/5000 train_loss:3.9985 train_time:153620ms step_avg:179.67ms\nstep:866/5000 train_loss:4.0566 train_time:153799ms step_avg:179.67ms\nstep:867/5000 train_loss:3.9778 train_time:153977ms step_avg:179.67ms\nstep:868/5000 train_loss:3.9837 train_time:154156ms step_avg:179.67ms\nstep:869/5000 train_loss:4.0876 train_time:154334ms step_avg:179.67ms\nstep:870/5000 train_loss:4.0635 train_time:154512ms step_avg:179.67ms\nstep:871/5000 train_loss:4.1089 train_time:154691ms step_avg:179.66ms\nstep:872/5000 train_loss:4.0603 train_time:154869ms step_avg:179.66ms\nstep:873/5000 train_loss:4.0444 train_time:155049ms step_avg:179.66ms\nstep:874/5000 train_loss:3.9945 train_time:155228ms step_avg:179.66ms\nstep:875/5000 train_loss:4.0841 train_time:155408ms step_avg:179.66ms\nstep:876/5000 train_loss:4.0453 train_time:155587ms step_avg:179.66ms\nstep:877/5000 train_loss:4.1143 train_time:155770ms step_avg:179.67ms\nstep:878/5000 train_loss:4.1053 train_time:155949ms step_avg:179.67ms\nstep:879/5000 train_loss:3.9804 train_time:156129ms step_avg:179.67ms\nstep:880/5000 train_loss:4.0653 train_time:156308ms step_avg:179.66ms\nstep:881/5000 train_loss:3.9988 train_time:156488ms step_avg:179.66ms\nstep:882/5000 train_loss:4.0185 train_time:156668ms step_avg:179.66ms\nstep:883/5000 train_loss:3.9801 train_time:156849ms step_avg:179.67ms\nstep:884/5000 train_loss:4.0858 train_time:157028ms step_avg:179.67ms\nstep:885/5000 train_loss:4.0370 train_time:157207ms step_avg:179.67ms\nstep:886/5000 train_loss:4.0971 train_time:157387ms step_avg:179.67ms\nstep:887/5000 train_loss:3.9577 train_time:157568ms step_avg:179.67ms\nstep:888/5000 train_loss:3.9959 train_time:157748ms step_avg:179.67ms\nstep:889/5000 train_loss:4.0132 train_time:157927ms step_avg:179.67ms\nstep:890/5000 train_loss:4.0194 train_time:158109ms step_avg:179.67ms\nstep:891/5000 train_loss:4.0247 train_time:158288ms step_avg:179.67ms\nstep:892/5000 train_loss:3.9699 train_time:158468ms step_avg:179.67ms\nstep:893/5000 train_loss:3.9799 train_time:158647ms step_avg:179.67ms\nstep:894/5000 train_loss:4.0305 train_time:158827ms step_avg:179.67ms\nstep:895/5000 train_loss:4.1767 train_time:159007ms step_avg:179.67ms\nstep:896/5000 train_loss:4.2814 train_time:159187ms step_avg:179.67ms\nstep:897/5000 train_loss:3.9355 train_time:159365ms step_avg:179.67ms\nstep:898/5000 train_loss:4.0091 train_time:159543ms step_avg:179.67ms\nstep:899/5000 train_loss:4.1581 train_time:159721ms step_avg:179.66ms\nstep:900/5000 train_loss:3.8973 train_time:159901ms step_avg:179.66ms\nstep:901/5000 train_loss:4.0033 train_time:160081ms step_avg:179.66ms\nstep:902/5000 train_loss:4.0309 train_time:160261ms step_avg:179.66ms\nstep:903/5000 train_loss:4.0388 train_time:160440ms step_avg:179.66ms\nstep:904/5000 train_loss:3.9674 train_time:160619ms step_avg:179.66ms\nstep:905/5000 train_loss:3.9706 train_time:160797ms step_avg:179.66ms\nstep:906/5000 train_loss:4.0235 train_time:160976ms step_avg:179.66ms\nstep:907/5000 train_loss:4.0802 train_time:161156ms step_avg:179.66ms\nstep:908/5000 train_loss:3.9669 train_time:161334ms step_avg:179.66ms\nstep:909/5000 train_loss:4.0693 train_time:161514ms step_avg:179.66ms\nstep:910/5000 train_loss:3.9487 train_time:161692ms step_avg:179.66ms\nstep:911/5000 train_loss:4.1309 train_time:161871ms step_avg:179.66ms\nstep:912/5000 train_loss:4.1277 train_time:162049ms step_avg:179.66ms\nstep:913/5000 train_loss:4.1318 train_time:162228ms step_avg:179.65ms\nstep:914/5000 train_loss:4.1668 train_time:162407ms step_avg:179.65ms\nstep:915/5000 train_loss:4.0179 train_time:162589ms step_avg:179.66ms\nstep:916/5000 train_loss:3.9191 train_time:162769ms step_avg:179.66ms\nstep:917/5000 train_loss:3.9973 train_time:162949ms step_avg:179.66ms\nstep:918/5000 train_loss:4.0418 train_time:163127ms step_avg:179.66ms\nstep:919/5000 train_loss:4.1344 train_time:163309ms step_avg:179.66ms\nstep:920/5000 train_loss:4.0407 train_time:163488ms step_avg:179.66ms\nstep:921/5000 train_loss:3.9744 train_time:163667ms step_avg:179.66ms\nstep:922/5000 train_loss:3.9298 train_time:163847ms step_avg:179.66ms\nstep:923/5000 train_loss:3.9051 train_time:164027ms step_avg:179.66ms\nstep:924/5000 train_loss:3.8966 train_time:164207ms step_avg:179.66ms\nstep:925/5000 train_loss:3.9599 train_time:164387ms step_avg:179.66ms\nstep:926/5000 train_loss:4.1375 train_time:164568ms step_avg:179.66ms\nstep:927/5000 train_loss:4.0343 train_time:164748ms step_avg:179.66ms\nstep:928/5000 train_loss:3.9687 train_time:164928ms step_avg:179.66ms\nstep:929/5000 train_loss:4.1608 train_time:165108ms step_avg:179.66ms\nstep:930/5000 train_loss:3.9744 train_time:165288ms step_avg:179.66ms\nstep:931/5000 train_loss:4.0104 train_time:165468ms step_avg:179.66ms\nstep:932/5000 train_loss:4.0913 train_time:165647ms step_avg:179.66ms\nstep:933/5000 train_loss:4.1355 train_time:165827ms step_avg:179.66ms\nstep:934/5000 train_loss:4.0092 train_time:166008ms step_avg:179.66ms\nstep:935/5000 train_loss:4.0068 train_time:166189ms step_avg:179.66ms\nstep:936/5000 train_loss:4.0152 train_time:166368ms step_avg:179.66ms\nstep:937/5000 train_loss:4.2104 train_time:166548ms step_avg:179.66ms\nstep:938/5000 train_loss:4.1715 train_time:166726ms step_avg:179.66ms\nstep:939/5000 train_loss:4.2223 train_time:166905ms step_avg:179.66ms\nstep:940/5000 train_loss:3.9085 train_time:167085ms step_avg:179.66ms\nstep:941/5000 train_loss:4.2426 train_time:167267ms step_avg:179.66ms\nstep:942/5000 train_loss:4.0624 train_time:167446ms step_avg:179.66ms\nstep:943/5000 train_loss:4.0242 train_time:167627ms step_avg:179.66ms\nstep:944/5000 train_loss:4.0736 train_time:167808ms step_avg:179.67ms\nstep:945/5000 train_loss:3.9357 train_time:167988ms step_avg:179.67ms\nstep:946/5000 train_loss:4.1613 train_time:168168ms step_avg:179.67ms\nstep:947/5000 train_loss:4.1759 train_time:168348ms step_avg:179.67ms\nstep:948/5000 train_loss:4.0687 train_time:168530ms step_avg:179.67ms\nstep:949/5000 train_loss:4.0723 train_time:168708ms step_avg:179.67ms\nstep:950/5000 train_loss:4.0940 train_time:168889ms step_avg:179.67ms\nstep:951/5000 train_loss:4.0223 train_time:169068ms step_avg:179.67ms\nstep:952/5000 train_loss:3.9380 train_time:169245ms step_avg:179.67ms\nstep:953/5000 train_loss:4.0447 train_time:169425ms step_avg:179.67ms\nstep:954/5000 train_loss:4.0935 train_time:169604ms step_avg:179.67ms\nstep:955/5000 train_loss:4.0383 train_time:169785ms step_avg:179.67ms\nstep:956/5000 train_loss:4.0769 train_time:169965ms step_avg:179.67ms\nstep:957/5000 train_loss:3.9333 train_time:170146ms step_avg:179.67ms\nstep:958/5000 train_loss:4.0679 train_time:170324ms step_avg:179.67ms\nstep:959/5000 train_loss:3.9614 train_time:170503ms step_avg:179.67ms\nstep:960/5000 train_loss:4.1133 train_time:170683ms step_avg:179.67ms\nstep:961/5000 train_loss:3.9530 train_time:170862ms step_avg:179.67ms\nstep:962/5000 train_loss:4.2715 train_time:171043ms step_avg:179.67ms\nstep:963/5000 train_loss:3.8560 train_time:171225ms step_avg:179.67ms\nstep:964/5000 train_loss:3.9615 train_time:171407ms step_avg:179.67ms\nstep:965/5000 train_loss:4.1275 train_time:171588ms step_avg:179.67ms\nstep:966/5000 train_loss:4.0854 train_time:171767ms step_avg:179.67ms\nstep:967/5000 train_loss:4.2513 train_time:171947ms step_avg:179.67ms\nstep:968/5000 train_loss:4.1601 train_time:172127ms step_avg:179.67ms\nstep:969/5000 train_loss:4.1099 train_time:172306ms step_avg:179.67ms\nstep:970/5000 train_loss:4.4303 train_time:172486ms step_avg:179.67ms\nstep:971/5000 train_loss:3.9910 train_time:172667ms step_avg:179.67ms\nstep:972/5000 train_loss:4.0190 train_time:172846ms step_avg:179.67ms\nstep:973/5000 train_loss:3.9497 train_time:173026ms step_avg:179.67ms\nstep:974/5000 train_loss:4.0359 train_time:173206ms step_avg:179.67ms\nstep:975/5000 train_loss:3.9176 train_time:173385ms step_avg:179.67ms\nstep:976/5000 train_loss:4.0758 train_time:173565ms step_avg:179.67ms\nstep:977/5000 train_loss:4.0881 train_time:173744ms step_avg:179.67ms\nstep:978/5000 train_loss:4.0538 train_time:173924ms step_avg:179.67ms\nstep:979/5000 train_loss:4.1645 train_time:174105ms step_avg:179.67ms\nstep:980/5000 train_loss:4.1415 train_time:174283ms step_avg:179.67ms\nstep:981/5000 train_loss:4.0362 train_time:174463ms step_avg:179.67ms\nstep:982/5000 train_loss:4.0911 train_time:174642ms step_avg:179.67ms\nstep:983/5000 train_loss:3.8006 train_time:174821ms step_avg:179.67ms\nstep:984/5000 train_loss:4.0643 train_time:175000ms step_avg:179.67ms\nstep:985/5000 train_loss:3.9180 train_time:175177ms step_avg:179.67ms\nstep:986/5000 train_loss:3.9971 train_time:175356ms step_avg:179.67ms\nstep:987/5000 train_loss:4.1858 train_time:175534ms step_avg:179.67ms\nstep:988/5000 train_loss:4.0042 train_time:175713ms step_avg:179.67ms\nstep:989/5000 train_loss:3.8935 train_time:175891ms step_avg:179.66ms\nstep:990/5000 train_loss:4.0261 train_time:176070ms step_avg:179.66ms\nstep:991/5000 train_loss:3.9790 train_time:176250ms step_avg:179.66ms\nstep:992/5000 train_loss:4.1279 train_time:176430ms step_avg:179.66ms\nstep:993/5000 train_loss:4.0005 train_time:176610ms step_avg:179.66ms\nstep:994/5000 train_loss:3.9487 train_time:176791ms step_avg:179.67ms\nstep:995/5000 train_loss:4.0910 train_time:176970ms step_avg:179.66ms\nstep:996/5000 train_loss:3.9235 train_time:177149ms step_avg:179.66ms\nstep:997/5000 train_loss:4.0987 train_time:177329ms step_avg:179.66ms\nstep:998/5000 train_loss:4.1070 train_time:177509ms step_avg:179.66ms\nstep:999/5000 train_loss:4.0469 train_time:177688ms step_avg:179.66ms\nstep:1000/5000 train_loss:3.9672 train_time:177869ms step_avg:179.67ms\nstep:1001/5000 train_loss:3.9987 train_time:178049ms step_avg:179.67ms\nstep:1002/5000 train_loss:3.9978 train_time:178229ms step_avg:179.67ms\nstep:1003/5000 train_loss:3.7828 train_time:178409ms step_avg:179.67ms\nstep:1004/5000 train_loss:4.0761 train_time:178588ms step_avg:179.67ms\nstep:1005/5000 train_loss:4.0885 train_time:178767ms step_avg:179.67ms\nstep:1006/5000 train_loss:3.9126 train_time:178947ms step_avg:179.67ms\nstep:1007/5000 train_loss:4.2301 train_time:179127ms step_avg:179.67ms\nstep:1008/5000 train_loss:4.1481 train_time:179309ms step_avg:179.67ms\nstep:1009/5000 train_loss:3.9371 train_time:179490ms step_avg:179.67ms\nstep:1010/5000 train_loss:3.9473 train_time:179669ms step_avg:179.67ms\nstep:1011/5000 train_loss:4.0761 train_time:179849ms step_avg:179.67ms\nstep:1012/5000 train_loss:3.9367 train_time:180029ms step_avg:179.67ms\nstep:1013/5000 train_loss:3.9379 train_time:180208ms step_avg:179.67ms\nstep:1014/5000 train_loss:4.0073 train_time:180389ms step_avg:179.67ms\nstep:1015/5000 train_loss:3.9688 train_time:180570ms step_avg:179.67ms\nstep:1016/5000 train_loss:3.8858 train_time:180751ms step_avg:179.67ms\nstep:1017/5000 train_loss:3.9799 train_time:180930ms step_avg:179.67ms\nstep:1018/5000 train_loss:3.9528 train_time:181111ms step_avg:179.67ms\nstep:1019/5000 train_loss:3.9060 train_time:181292ms step_avg:179.67ms\nstep:1020/5000 train_loss:4.0791 train_time:181470ms step_avg:179.67ms\nstep:1021/5000 train_loss:4.1476 train_time:181650ms step_avg:179.67ms\nstep:1022/5000 train_loss:3.8944 train_time:181828ms step_avg:179.67ms\nstep:1023/5000 train_loss:4.1404 train_time:182010ms step_avg:179.67ms\nstep:1024/5000 train_loss:3.9927 train_time:182189ms step_avg:179.67ms\nstep:1025/5000 train_loss:3.9992 train_time:182368ms step_avg:179.67ms\nstep:1026/5000 train_loss:3.9697 train_time:182551ms step_avg:179.68ms\nstep:1027/5000 train_loss:3.9202 train_time:182728ms step_avg:179.67ms\nstep:1028/5000 train_loss:3.9944 train_time:182908ms step_avg:179.67ms\nstep:1029/5000 train_loss:4.0625 train_time:183088ms step_avg:179.67ms\nstep:1030/5000 train_loss:4.0975 train_time:183267ms step_avg:179.67ms\nstep:1031/5000 train_loss:3.9723 train_time:183446ms step_avg:179.67ms\nstep:1032/5000 train_loss:3.9826 train_time:183625ms step_avg:179.67ms\nstep:1033/5000 train_loss:4.2459 train_time:183805ms step_avg:179.67ms\nstep:1034/5000 train_loss:3.8932 train_time:183986ms step_avg:179.67ms\nstep:1035/5000 train_loss:4.0209 train_time:184165ms step_avg:179.67ms\nstep:1036/5000 train_loss:4.0357 train_time:184345ms step_avg:179.67ms\nstep:1037/5000 train_loss:3.9908 train_time:184525ms step_avg:179.67ms\nstep:1038/5000 train_loss:4.0086 train_time:184705ms step_avg:179.67ms\nstep:1039/5000 train_loss:3.9233 train_time:184887ms step_avg:179.68ms\nstep:1040/5000 train_loss:4.0647 train_time:185067ms step_avg:179.68ms\nstep:1041/5000 train_loss:4.0441 train_time:185248ms step_avg:179.68ms\nstep:1042/5000 train_loss:3.9596 train_time:185427ms step_avg:179.68ms\nstep:1043/5000 train_loss:3.9510 train_time:185609ms step_avg:179.68ms\nstep:1044/5000 train_loss:4.0300 train_time:185788ms step_avg:179.68ms\nstep:1045/5000 train_loss:3.8763 train_time:185968ms step_avg:179.68ms\nstep:1046/5000 train_loss:3.9862 train_time:186147ms step_avg:179.68ms\nstep:1047/5000 train_loss:3.7407 train_time:186328ms step_avg:179.68ms\nstep:1048/5000 train_loss:4.0007 train_time:186507ms step_avg:179.68ms\nstep:1049/5000 train_loss:4.0803 train_time:186688ms step_avg:179.68ms\nstep:1050/5000 train_loss:4.0560 train_time:186868ms step_avg:179.68ms\nstep:1051/5000 train_loss:4.0167 train_time:187048ms step_avg:179.68ms\nstep:1052/5000 train_loss:3.9154 train_time:187229ms step_avg:179.68ms\nstep:1053/5000 train_loss:4.0081 train_time:187411ms step_avg:179.68ms\nstep:1054/5000 train_loss:4.1002 train_time:187592ms step_avg:179.69ms\nstep:1055/5000 train_loss:3.9775 train_time:187772ms step_avg:179.69ms\nstep:1056/5000 train_loss:3.9940 train_time:187951ms step_avg:179.69ms\nstep:1057/5000 train_loss:4.0460 train_time:188130ms step_avg:179.68ms\nstep:1058/5000 train_loss:4.4608 train_time:188309ms step_avg:179.68ms\nstep:1059/5000 train_loss:3.8887 train_time:188489ms step_avg:179.68ms\nstep:1060/5000 train_loss:3.8751 train_time:188670ms step_avg:179.69ms\nstep:1061/5000 train_loss:3.8028 train_time:188851ms step_avg:179.69ms\nstep:1062/5000 train_loss:3.9973 train_time:189030ms step_avg:179.69ms\nstep:1063/5000 train_loss:3.9295 train_time:189210ms step_avg:179.69ms\nstep:1064/5000 train_loss:3.9229 train_time:189389ms step_avg:179.69ms\nstep:1065/5000 train_loss:4.0244 train_time:189568ms step_avg:179.69ms\nstep:1066/5000 train_loss:4.1859 train_time:189748ms step_avg:179.69ms\nstep:1067/5000 train_loss:4.0545 train_time:189927ms step_avg:179.69ms\nstep:1068/5000 train_loss:3.9304 train_time:190107ms step_avg:179.69ms\nstep:1069/5000 train_loss:3.9070 train_time:190286ms step_avg:179.68ms\nstep:1070/5000 train_loss:3.9349 train_time:190464ms step_avg:179.68ms\nstep:1071/5000 train_loss:3.9999 train_time:190643ms step_avg:179.68ms\nstep:1072/5000 train_loss:3.9704 train_time:190824ms step_avg:179.68ms\nstep:1073/5000 train_loss:4.0285 train_time:191004ms step_avg:179.68ms\nstep:1074/5000 train_loss:3.9468 train_time:191183ms step_avg:179.68ms\nstep:1075/5000 train_loss:3.8836 train_time:191362ms step_avg:179.68ms\nstep:1076/5000 train_loss:3.8705 train_time:191541ms step_avg:179.68ms\nstep:1077/5000 train_loss:3.8958 train_time:191720ms step_avg:179.68ms\nstep:1078/5000 train_loss:4.0600 train_time:191898ms step_avg:179.68ms\nstep:1079/5000 train_loss:3.9436 train_time:192076ms step_avg:179.68ms\nstep:1080/5000 train_loss:4.1163 train_time:192254ms step_avg:179.68ms\nstep:1081/5000 train_loss:3.8141 train_time:192434ms step_avg:179.68ms\nstep:1082/5000 train_loss:4.1148 train_time:192613ms step_avg:179.68ms\nstep:1083/5000 train_loss:3.9940 train_time:192793ms step_avg:179.68ms\nstep:1084/5000 train_loss:3.9843 train_time:192972ms step_avg:179.68ms\nstep:1085/5000 train_loss:4.1272 train_time:193151ms step_avg:179.68ms\nstep:1086/5000 train_loss:3.9823 train_time:193330ms step_avg:179.68ms\nstep:1087/5000 train_loss:3.9416 train_time:193510ms step_avg:179.67ms\nstep:1088/5000 train_loss:3.9290 train_time:193688ms step_avg:179.67ms\nstep:1089/5000 train_loss:3.9643 train_time:193868ms step_avg:179.67ms\nstep:1090/5000 train_loss:3.9468 train_time:194047ms step_avg:179.67ms\nstep:1091/5000 train_loss:3.9319 train_time:194228ms step_avg:179.67ms\nstep:1092/5000 train_loss:3.9676 train_time:194407ms step_avg:179.67ms\nstep:1093/5000 train_loss:3.9209 train_time:194586ms step_avg:179.67ms\nstep:1094/5000 train_loss:3.9619 train_time:194766ms step_avg:179.67ms\nstep:1095/5000 train_loss:4.0111 train_time:194945ms step_avg:179.67ms\nstep:1096/5000 train_loss:4.0421 train_time:195127ms step_avg:179.68ms\nstep:1097/5000 train_loss:4.0579 train_time:195309ms step_avg:179.68ms\nstep:1098/5000 train_loss:3.9248 train_time:195488ms step_avg:179.68ms\nstep:1099/5000 train_loss:3.8833 train_time:195668ms step_avg:179.68ms\nstep:1100/5000 train_loss:4.0801 train_time:195848ms step_avg:179.68ms\nstep:1101/5000 train_loss:4.0571 train_time:196028ms step_avg:179.68ms\nstep:1102/5000 train_loss:4.0306 train_time:196208ms step_avg:179.68ms\nstep:1103/5000 train_loss:3.9817 train_time:196391ms step_avg:179.68ms\nstep:1104/5000 train_loss:4.2516 train_time:196569ms step_avg:179.68ms\nstep:1105/5000 train_loss:3.8980 train_time:196748ms step_avg:179.68ms\nstep:1106/5000 train_loss:4.0578 train_time:196927ms step_avg:179.68ms\nstep:1107/5000 train_loss:4.0892 train_time:197107ms step_avg:179.68ms\nstep:1108/5000 train_loss:4.0533 train_time:197287ms step_avg:179.68ms\nstep:1109/5000 train_loss:3.9431 train_time:197467ms step_avg:179.68ms\nstep:1110/5000 train_loss:3.8534 train_time:197646ms step_avg:179.68ms\nstep:1111/5000 train_loss:3.9518 train_time:197825ms step_avg:179.68ms\nstep:1112/5000 train_loss:4.0961 train_time:198004ms step_avg:179.68ms\nstep:1113/5000 train_loss:3.9325 train_time:198183ms step_avg:179.68ms\nstep:1114/5000 train_loss:4.1616 train_time:198363ms step_avg:179.68ms\nstep:1115/5000 train_loss:3.9360 train_time:198543ms step_avg:179.68ms\nstep:1116/5000 train_loss:4.0124 train_time:198723ms step_avg:179.68ms\nstep:1117/5000 train_loss:3.7555 train_time:198903ms step_avg:179.68ms\nstep:1118/5000 train_loss:4.0085 train_time:199083ms step_avg:179.68ms\nstep:1119/5000 train_loss:4.0451 train_time:199262ms step_avg:179.68ms\nstep:1120/5000 train_loss:4.1755 train_time:199441ms step_avg:179.68ms\nstep:1121/5000 train_loss:4.0568 train_time:199622ms step_avg:179.68ms\nstep:1122/5000 train_loss:4.0180 train_time:199802ms step_avg:179.68ms\nstep:1123/5000 train_loss:3.9180 train_time:199982ms step_avg:179.68ms\nstep:1124/5000 train_loss:3.9850 train_time:200162ms step_avg:179.68ms\nstep:1125/5000 train_loss:3.9053 train_time:200341ms step_avg:179.68ms\nstep:1126/5000 train_loss:3.9695 train_time:200521ms step_avg:179.68ms\nstep:1127/5000 train_loss:4.0221 train_time:200700ms step_avg:179.68ms\nstep:1128/5000 train_loss:3.9942 train_time:200880ms step_avg:179.68ms\nstep:1129/5000 train_loss:4.0532 train_time:201059ms step_avg:179.68ms\nstep:1130/5000 train_loss:4.1691 train_time:201237ms step_avg:179.68ms\nstep:1131/5000 train_loss:3.8900 train_time:201416ms step_avg:179.68ms\nstep:1132/5000 train_loss:3.9298 train_time:201594ms step_avg:179.67ms\nstep:1133/5000 train_loss:3.8948 train_time:201773ms step_avg:179.67ms\nstep:1134/5000 train_loss:3.9265 train_time:201952ms step_avg:179.67ms\nstep:1135/5000 train_loss:4.0107 train_time:202135ms step_avg:179.68ms\nstep:1136/5000 train_loss:4.0747 train_time:202313ms step_avg:179.67ms\nstep:1137/5000 train_loss:3.8662 train_time:202491ms step_avg:179.67ms\nstep:1138/5000 train_loss:3.8941 train_time:202669ms step_avg:179.67ms\nstep:1139/5000 train_loss:4.0526 train_time:202849ms step_avg:179.67ms\nstep:1140/5000 train_loss:4.0782 train_time:203029ms step_avg:179.67ms\nstep:1141/5000 train_loss:4.0594 train_time:203209ms step_avg:179.67ms\nstep:1142/5000 train_loss:4.0758 train_time:203389ms step_avg:179.67ms\nstep:1143/5000 train_loss:4.0621 train_time:203568ms step_avg:179.67ms\nstep:1144/5000 train_loss:4.1149 train_time:203746ms step_avg:179.67ms\nstep:1145/5000 train_loss:3.9545 train_time:203927ms step_avg:179.67ms\nstep:1146/5000 train_loss:4.0037 train_time:204110ms step_avg:179.67ms\nstep:1147/5000 train_loss:3.9418 train_time:204289ms step_avg:179.67ms\nstep:1148/5000 train_loss:3.9445 train_time:204469ms step_avg:179.67ms\nstep:1149/5000 train_loss:3.9947 train_time:204648ms step_avg:179.67ms\nstep:1150/5000 train_loss:4.0328 train_time:204827ms step_avg:179.67ms\nstep:1151/5000 train_loss:3.8826 train_time:205006ms step_avg:179.67ms\nstep:1152/5000 train_loss:4.0404 train_time:205187ms step_avg:179.67ms\nstep:1153/5000 train_loss:3.9317 train_time:205366ms step_avg:179.67ms\nstep:1154/5000 train_loss:4.0751 train_time:205545ms step_avg:179.67ms\nstep:1155/5000 train_loss:3.9977 train_time:205725ms step_avg:179.67ms\nstep:1156/5000 train_loss:3.8881 train_time:205906ms step_avg:179.67ms\nstep:1157/5000 train_loss:3.9404 train_time:206086ms step_avg:179.67ms\nstep:1158/5000 train_loss:4.0610 train_time:206266ms step_avg:179.67ms\nstep:1159/5000 train_loss:3.9403 train_time:206445ms step_avg:179.67ms\nstep:1160/5000 train_loss:3.8862 train_time:206626ms step_avg:179.67ms\nstep:1161/5000 train_loss:4.1069 train_time:206805ms step_avg:179.67ms\nstep:1162/5000 train_loss:3.9032 train_time:206985ms step_avg:179.67ms\nstep:1163/5000 train_loss:3.9120 train_time:207164ms step_avg:179.67ms\nstep:1164/5000 train_loss:3.9707 train_time:207345ms step_avg:179.67ms\nstep:1165/5000 train_loss:4.0066 train_time:207523ms step_avg:179.67ms\nstep:1166/5000 train_loss:3.8866 train_time:207703ms step_avg:179.67ms\nstep:1167/5000 train_loss:3.9312 train_time:207883ms step_avg:179.67ms\nstep:1168/5000 train_loss:3.8849 train_time:208062ms step_avg:179.67ms\nstep:1169/5000 train_loss:3.9810 train_time:208241ms step_avg:179.67ms\nstep:1170/5000 train_loss:4.0735 train_time:208421ms step_avg:179.67ms\nstep:1171/5000 train_loss:4.0467 train_time:208600ms step_avg:179.67ms\nstep:1172/5000 train_loss:3.9312 train_time:208779ms step_avg:179.67ms\nstep:1173/5000 train_loss:3.9859 train_time:208957ms step_avg:179.67ms\nstep:1174/5000 train_loss:3.8723 train_time:209136ms step_avg:179.67ms\nstep:1175/5000 train_loss:3.8646 train_time:209316ms step_avg:179.67ms\nstep:1176/5000 train_loss:3.9727 train_time:209494ms step_avg:179.67ms\nstep:1177/5000 train_loss:4.2535 train_time:209672ms step_avg:179.67ms\nstep:1178/5000 train_loss:4.1549 train_time:209852ms step_avg:179.67ms\nstep:1179/5000 train_loss:4.2116 train_time:210031ms step_avg:179.67ms\nstep:1180/5000 train_loss:3.8598 train_time:210210ms step_avg:179.67ms\nstep:1181/5000 train_loss:3.7423 train_time:210390ms step_avg:179.67ms\nstep:1182/5000 train_loss:4.0502 train_time:210569ms step_avg:179.67ms\nstep:1183/5000 train_loss:4.1207 train_time:210748ms step_avg:179.67ms\nstep:1184/5000 train_loss:3.8832 train_time:210927ms step_avg:179.67ms\nstep:1185/5000 train_loss:3.9255 train_time:211108ms step_avg:179.67ms\nstep:1186/5000 train_loss:3.8763 train_time:211287ms step_avg:179.67ms\nstep:1187/5000 train_loss:4.0079 train_time:211469ms step_avg:179.67ms\nstep:1188/5000 train_loss:3.8859 train_time:211648ms step_avg:179.67ms\nstep:1189/5000 train_loss:3.9198 train_time:211828ms step_avg:179.67ms\nstep:1190/5000 train_loss:3.9961 train_time:212008ms step_avg:179.67ms\nstep:1191/5000 train_loss:3.9626 train_time:212188ms step_avg:179.67ms\nstep:1192/5000 train_loss:4.0034 train_time:212368ms step_avg:179.67ms\nstep:1193/5000 train_loss:4.0798 train_time:212550ms step_avg:179.67ms\nstep:1194/5000 train_loss:3.9024 train_time:212731ms step_avg:179.67ms\nstep:1195/5000 train_loss:3.9324 train_time:212910ms step_avg:179.67ms\nstep:1196/5000 train_loss:3.8474 train_time:213089ms step_avg:179.67ms\nstep:1197/5000 train_loss:3.9272 train_time:213268ms step_avg:179.67ms\nstep:1198/5000 train_loss:4.0920 train_time:213449ms step_avg:179.67ms\nstep:1199/5000 train_loss:3.8844 train_time:213629ms step_avg:179.67ms\nstep:1200/5000 train_loss:3.9897 train_time:213809ms step_avg:179.67ms\nstep:1201/5000 train_loss:3.9732 train_time:213989ms step_avg:179.67ms\nstep:1202/5000 train_loss:3.8876 train_time:214168ms step_avg:179.67ms\nstep:1203/5000 train_loss:4.0243 train_time:214347ms step_avg:179.67ms\nstep:1204/5000 train_loss:4.0023 train_time:214527ms step_avg:179.67ms\nstep:1205/5000 train_loss:3.9119 train_time:214706ms step_avg:179.67ms\nstep:1206/5000 train_loss:3.9848 train_time:214885ms step_avg:179.67ms\nstep:1207/5000 train_loss:4.0309 train_time:215065ms step_avg:179.67ms\nstep:1208/5000 train_loss:3.9545 train_time:215243ms step_avg:179.67ms\nstep:1209/5000 train_loss:3.9879 train_time:215422ms step_avg:179.67ms\nstep:1210/5000 train_loss:3.9342 train_time:215602ms step_avg:179.67ms\nstep:1211/5000 train_loss:3.9581 train_time:215782ms step_avg:179.67ms\nstep:1212/5000 train_loss:4.0159 train_time:215961ms step_avg:179.67ms\nstep:1213/5000 train_loss:3.9732 train_time:216139ms step_avg:179.67ms\nstep:1214/5000 train_loss:3.9324 train_time:216319ms step_avg:179.67ms\nstep:1215/5000 train_loss:3.9235 train_time:216498ms step_avg:179.67ms\nstep:1216/5000 train_loss:3.9860 train_time:216677ms step_avg:179.67ms\nstep:1217/5000 train_loss:3.9296 train_time:216856ms step_avg:179.66ms\nstep:1218/5000 train_loss:3.9337 train_time:217034ms step_avg:179.66ms\nstep:1219/5000 train_loss:3.9367 train_time:217213ms step_avg:179.66ms\nstep:1220/5000 train_loss:4.4733 train_time:217391ms step_avg:179.66ms\nstep:1221/5000 train_loss:3.8180 train_time:217569ms step_avg:179.66ms\nstep:1222/5000 train_loss:4.0403 train_time:217749ms step_avg:179.66ms\nstep:1223/5000 train_loss:3.7896 train_time:217928ms step_avg:179.66ms\nstep:1224/5000 train_loss:3.9630 train_time:218110ms step_avg:179.66ms\nstep:1225/5000 train_loss:3.8948 train_time:218289ms step_avg:179.66ms\nstep:1226/5000 train_loss:3.9798 train_time:218469ms step_avg:179.66ms\nstep:1227/5000 train_loss:4.0314 train_time:218647ms step_avg:179.66ms\nstep:1228/5000 train_loss:3.8586 train_time:218826ms step_avg:179.66ms\nstep:1229/5000 train_loss:4.0270 train_time:219006ms step_avg:179.66ms\nstep:1230/5000 train_loss:3.8947 train_time:219184ms step_avg:179.66ms\nstep:1231/5000 train_loss:3.9128 train_time:219364ms step_avg:179.66ms\nstep:1232/5000 train_loss:4.0194 train_time:219545ms step_avg:179.66ms\nstep:1233/5000 train_loss:3.8784 train_time:219723ms step_avg:179.66ms\nstep:1234/5000 train_loss:4.0602 train_time:219903ms step_avg:179.66ms\nstep:1235/5000 train_loss:3.8639 train_time:220082ms step_avg:179.66ms\nstep:1236/5000 train_loss:3.8938 train_time:220262ms step_avg:179.66ms\nstep:1237/5000 train_loss:3.8390 train_time:220442ms step_avg:179.66ms\nstep:1238/5000 train_loss:3.9158 train_time:220622ms step_avg:179.66ms\nstep:1239/5000 train_loss:4.1391 train_time:220801ms step_avg:179.66ms\nstep:1240/5000 train_loss:4.1020 train_time:220981ms step_avg:179.66ms\nstep:1241/5000 train_loss:4.0008 train_time:221161ms step_avg:179.66ms\nstep:1242/5000 train_loss:3.9883 train_time:221340ms step_avg:179.66ms\nstep:1243/5000 train_loss:4.0620 train_time:221521ms step_avg:179.66ms\nstep:1244/5000 train_loss:3.9595 train_time:221701ms step_avg:179.66ms\nstep:1245/5000 train_loss:4.0331 train_time:221881ms step_avg:179.66ms\nstep:1246/5000 train_loss:3.9784 train_time:222061ms step_avg:179.66ms\nstep:1247/5000 train_loss:4.0141 train_time:222240ms step_avg:179.66ms\nstep:1248/5000 train_loss:3.8971 train_time:222420ms step_avg:179.66ms\nstep:1249/5000 train_loss:3.9879 train_time:222600ms step_avg:179.66ms\nstep:1250/5000 train_loss:3.8856 train_time:222780ms step_avg:179.66ms\nstep:1251/5000 train_loss:3.9490 train_time:222959ms step_avg:179.66ms\nstep:1252/5000 train_loss:3.9832 train_time:223139ms step_avg:179.66ms\nstep:1253/5000 train_loss:4.0046 train_time:223317ms step_avg:179.66ms\nstep:1254/5000 train_loss:3.9673 train_time:223496ms step_avg:179.66ms\nstep:1255/5000 train_loss:4.0837 train_time:223675ms step_avg:179.66ms\nstep:1256/5000 train_loss:4.1182 train_time:223854ms step_avg:179.66ms\nstep:1257/5000 train_loss:3.8753 train_time:224032ms step_avg:179.66ms\nstep:1258/5000 train_loss:3.9660 train_time:224212ms step_avg:179.66ms\nstep:1259/5000 train_loss:3.9619 train_time:224391ms step_avg:179.66ms\nstep:1260/5000 train_loss:3.9827 train_time:224571ms step_avg:179.66ms\nstep:1261/5000 train_loss:3.9386 train_time:224749ms step_avg:179.66ms\nstep:1262/5000 train_loss:3.6965 train_time:224930ms step_avg:179.66ms\nstep:1263/5000 train_loss:3.9330 train_time:225109ms step_avg:179.66ms\nstep:1264/5000 train_loss:3.8214 train_time:225290ms step_avg:179.66ms\nstep:1265/5000 train_loss:3.9716 train_time:225469ms step_avg:179.66ms\nstep:1266/5000 train_loss:3.9568 train_time:225648ms step_avg:179.66ms\nstep:1267/5000 train_loss:3.9530 train_time:225827ms step_avg:179.66ms\nstep:1268/5000 train_loss:3.9133 train_time:226006ms step_avg:179.66ms\nstep:1269/5000 train_loss:3.8883 train_time:226188ms step_avg:179.66ms\nstep:1270/5000 train_loss:3.9404 train_time:226368ms step_avg:179.66ms\nstep:1271/5000 train_loss:3.9216 train_time:226547ms step_avg:179.66ms\nstep:1272/5000 train_loss:3.9410 train_time:226727ms step_avg:179.66ms\nstep:1273/5000 train_loss:3.7764 train_time:226907ms step_avg:179.66ms\nstep:1274/5000 train_loss:3.8568 train_time:227087ms step_avg:179.66ms\nstep:1275/5000 train_loss:3.8713 train_time:227266ms step_avg:179.66ms\nstep:1276/5000 train_loss:3.9130 train_time:227448ms step_avg:179.66ms\nstep:1277/5000 train_loss:4.1234 train_time:227627ms step_avg:179.66ms\nstep:1278/5000 train_loss:3.8896 train_time:227807ms step_avg:179.66ms\nstep:1279/5000 train_loss:3.9126 train_time:227988ms step_avg:179.66ms\nstep:1280/5000 train_loss:4.0403 train_time:228169ms step_avg:179.66ms\nstep:1281/5000 train_loss:3.9896 train_time:228348ms step_avg:179.66ms\nstep:1282/5000 train_loss:3.9318 train_time:228528ms step_avg:179.66ms\nstep:1283/5000 train_loss:3.9816 train_time:228708ms step_avg:179.66ms\nstep:1284/5000 train_loss:3.9848 train_time:228889ms step_avg:179.66ms\nstep:1285/5000 train_loss:3.8092 train_time:229068ms step_avg:179.66ms\nstep:1286/5000 train_loss:3.9582 train_time:229247ms step_avg:179.66ms\nstep:1287/5000 train_loss:4.0505 train_time:229427ms step_avg:179.66ms\nstep:1288/5000 train_loss:3.9640 train_time:229608ms step_avg:179.66ms\nstep:1289/5000 train_loss:3.9994 train_time:229790ms step_avg:179.66ms\nstep:1290/5000 train_loss:3.8260 train_time:229967ms step_avg:179.66ms\nstep:1291/5000 train_loss:3.9577 train_time:230150ms step_avg:179.66ms\nstep:1292/5000 train_loss:4.0438 train_time:230330ms step_avg:179.66ms\nstep:1293/5000 train_loss:4.0567 train_time:230510ms step_avg:179.67ms\nstep:1294/5000 train_loss:3.9088 train_time:230691ms step_avg:179.67ms\nstep:1295/5000 train_loss:3.9927 train_time:230870ms step_avg:179.67ms\nstep:1296/5000 train_loss:3.9952 train_time:231050ms step_avg:179.67ms\nstep:1297/5000 train_loss:3.8858 train_time:231231ms step_avg:179.67ms\nstep:1298/5000 train_loss:3.9611 train_time:231410ms step_avg:179.67ms\nstep:1299/5000 train_loss:3.8462 train_time:231590ms step_avg:179.67ms\nstep:1300/5000 train_loss:4.0531 train_time:231771ms step_avg:179.67ms\nstep:1301/5000 train_loss:4.0147 train_time:231950ms step_avg:179.67ms\nstep:1302/5000 train_loss:3.9659 train_time:232130ms step_avg:179.67ms\nstep:1303/5000 train_loss:3.9917 train_time:232311ms step_avg:179.67ms\nstep:1304/5000 train_loss:3.9267 train_time:232490ms step_avg:179.67ms\nstep:1305/5000 train_loss:4.0337 train_time:232668ms step_avg:179.67ms\nstep:1306/5000 train_loss:3.9341 train_time:232848ms step_avg:179.67ms\nstep:1307/5000 train_loss:4.0056 train_time:233029ms step_avg:179.67ms\nstep:1308/5000 train_loss:4.4225 train_time:233208ms step_avg:179.67ms\nstep:1309/5000 train_loss:3.9227 train_time:233389ms step_avg:179.67ms\nstep:1310/5000 train_loss:4.1827 train_time:233569ms step_avg:179.67ms\nstep:1311/5000 train_loss:4.1521 train_time:233750ms step_avg:179.67ms\nstep:1312/5000 train_loss:4.1114 train_time:233930ms step_avg:179.67ms\nstep:1313/5000 train_loss:3.9618 train_time:234110ms step_avg:179.67ms\nstep:1314/5000 train_loss:3.9766 train_time:234289ms step_avg:179.67ms\nstep:1315/5000 train_loss:3.9432 train_time:234469ms step_avg:179.67ms\nstep:1316/5000 train_loss:3.8646 train_time:234648ms step_avg:179.67ms\nstep:1317/5000 train_loss:4.0655 train_time:234828ms step_avg:179.67ms\nstep:1318/5000 train_loss:4.0170 train_time:235008ms step_avg:179.67ms\nstep:1319/5000 train_loss:4.2026 train_time:235187ms step_avg:179.67ms\nstep:1320/5000 train_loss:3.7885 train_time:235368ms step_avg:179.67ms\nstep:1321/5000 train_loss:3.9990 train_time:235547ms step_avg:179.67ms\nstep:1322/5000 train_loss:3.8842 train_time:235726ms step_avg:179.67ms\nstep:1323/5000 train_loss:3.8740 train_time:235907ms step_avg:179.67ms\nstep:1324/5000 train_loss:4.0179 train_time:236088ms step_avg:179.67ms\nstep:1325/5000 train_loss:3.8889 train_time:236269ms step_avg:179.67ms\nstep:1326/5000 train_loss:4.4610 train_time:236449ms step_avg:179.67ms\nstep:1327/5000 train_loss:3.8889 train_time:236628ms step_avg:179.67ms\nstep:1328/5000 train_loss:3.9462 train_time:236808ms step_avg:179.67ms\nstep:1329/5000 train_loss:3.8069 train_time:236988ms step_avg:179.67ms\nstep:1330/5000 train_loss:3.8759 train_time:237168ms step_avg:179.67ms\nstep:1331/5000 train_loss:3.9172 train_time:237347ms step_avg:179.67ms\nstep:1332/5000 train_loss:3.9115 train_time:237526ms step_avg:179.67ms\nstep:1333/5000 train_loss:3.8587 train_time:237707ms step_avg:179.67ms\nstep:1334/5000 train_loss:3.7920 train_time:237885ms step_avg:179.67ms\nstep:1335/5000 train_loss:3.8248 train_time:238064ms step_avg:179.67ms\nstep:1336/5000 train_loss:3.8803 train_time:238243ms step_avg:179.67ms\nstep:1337/5000 train_loss:3.8299 train_time:238423ms step_avg:179.67ms\nstep:1338/5000 train_loss:3.9461 train_time:238602ms step_avg:179.67ms\nstep:1339/5000 train_loss:4.0331 train_time:238783ms step_avg:179.67ms\nstep:1340/5000 train_loss:4.0593 train_time:238962ms step_avg:179.67ms\nstep:1341/5000 train_loss:3.9024 train_time:239143ms step_avg:179.67ms\nstep:1342/5000 train_loss:3.9670 train_time:239325ms step_avg:179.67ms\nstep:1343/5000 train_loss:3.9544 train_time:239509ms step_avg:179.68ms\nstep:1344/5000 train_loss:4.0071 train_time:239689ms step_avg:179.68ms\nstep:1345/5000 train_loss:3.8904 train_time:239870ms step_avg:179.68ms\nstep:1346/5000 train_loss:3.9667 train_time:240051ms step_avg:179.68ms\nstep:1347/5000 train_loss:3.8518 train_time:240231ms step_avg:179.68ms\nstep:1348/5000 train_loss:4.4732 train_time:240410ms step_avg:179.68ms\nstep:1349/5000 train_loss:4.9337 train_time:240590ms step_avg:179.68ms\nstep:1350/5000 train_loss:3.9563 train_time:240770ms step_avg:179.68ms\nstep:1351/5000 train_loss:3.9841 train_time:240950ms step_avg:179.68ms\nstep:1352/5000 train_loss:3.9838 train_time:241129ms step_avg:179.68ms\nstep:1353/5000 train_loss:3.9365 train_time:241310ms step_avg:179.68ms\nstep:1354/5000 train_loss:3.9978 train_time:241489ms step_avg:179.68ms\nstep:1355/5000 train_loss:4.0315 train_time:241668ms step_avg:179.68ms\nstep:1356/5000 train_loss:3.9229 train_time:241847ms step_avg:179.68ms\nstep:1357/5000 train_loss:3.8688 train_time:242027ms step_avg:179.68ms\nstep:1358/5000 train_loss:3.8679 train_time:242207ms step_avg:179.68ms\nstep:1359/5000 train_loss:3.8644 train_time:242388ms step_avg:179.68ms\nstep:1360/5000 train_loss:3.9789 train_time:242567ms step_avg:179.68ms\nstep:1361/5000 train_loss:3.9361 train_time:242746ms step_avg:179.68ms\nstep:1362/5000 train_loss:3.8547 train_time:242928ms step_avg:179.68ms\nstep:1363/5000 train_loss:3.8946 train_time:243107ms step_avg:179.68ms\nstep:1364/5000 train_loss:3.9013 train_time:243286ms step_avg:179.68ms\nstep:1365/5000 train_loss:3.9367 train_time:243466ms step_avg:179.68ms\nstep:1366/5000 train_loss:3.8805 train_time:243645ms step_avg:179.68ms\nstep:1367/5000 train_loss:3.9353 train_time:243827ms step_avg:179.68ms\nstep:1368/5000 train_loss:3.8546 train_time:244007ms step_avg:179.68ms\nstep:1369/5000 train_loss:3.9144 train_time:244187ms step_avg:179.68ms\nstep:1370/5000 train_loss:3.9003 train_time:244367ms step_avg:179.68ms\nstep:1371/5000 train_loss:3.9511 train_time:244546ms step_avg:179.68ms\nstep:1372/5000 train_loss:3.8845 train_time:244725ms step_avg:179.68ms\nstep:1373/5000 train_loss:3.9479 train_time:244906ms step_avg:179.68ms\nstep:1374/5000 train_loss:3.9963 train_time:245085ms step_avg:179.68ms\nstep:1375/5000 train_loss:3.9561 train_time:245265ms step_avg:179.68ms\nstep:1376/5000 train_loss:4.0358 train_time:245444ms step_avg:179.68ms\nstep:1377/5000 train_loss:3.7062 train_time:245623ms step_avg:179.68ms\nstep:1378/5000 train_loss:3.9299 train_time:245803ms step_avg:179.68ms\nstep:1379/5000 train_loss:4.0244 train_time:245983ms step_avg:179.68ms\nstep:1380/5000 train_loss:3.8902 train_time:246162ms step_avg:179.68ms\nstep:1381/5000 train_loss:3.9267 train_time:246341ms step_avg:179.68ms\nstep:1382/5000 train_loss:3.8974 train_time:246521ms step_avg:179.68ms\nstep:1383/5000 train_loss:3.8226 train_time:246700ms step_avg:179.68ms\nstep:1384/5000 train_loss:3.8143 train_time:246879ms step_avg:179.68ms\nstep:1385/5000 train_loss:3.9110 train_time:247057ms step_avg:179.68ms\nstep:1386/5000 train_loss:3.9513 train_time:247236ms step_avg:179.68ms\nstep:1387/5000 train_loss:3.9094 train_time:247416ms step_avg:179.68ms\nstep:1388/5000 train_loss:3.8473 train_time:247594ms step_avg:179.68ms\nstep:1389/5000 train_loss:3.9260 train_time:247773ms step_avg:179.68ms\nstep:1390/5000 train_loss:3.8667 train_time:247952ms step_avg:179.68ms\nstep:1391/5000 train_loss:3.8980 train_time:248131ms step_avg:179.68ms\nstep:1392/5000 train_loss:3.9053 train_time:248310ms step_avg:179.67ms\nstep:1393/5000 train_loss:4.0519 train_time:248489ms step_avg:179.67ms\nstep:1394/5000 train_loss:3.8747 train_time:248669ms step_avg:179.67ms\nstep:1395/5000 train_loss:3.8934 train_time:248848ms step_avg:179.67ms\nstep:1396/5000 train_loss:3.8645 train_time:249027ms step_avg:179.67ms\nstep:1397/5000 train_loss:3.7415 train_time:249207ms step_avg:179.67ms\nstep:1398/5000 train_loss:3.9452 train_time:249387ms step_avg:179.67ms\nstep:1399/5000 train_loss:3.8485 train_time:249568ms step_avg:179.67ms\nstep:1400/5000 train_loss:3.6019 train_time:249747ms step_avg:179.67ms\nstep:1401/5000 train_loss:3.8480 train_time:249929ms step_avg:179.68ms\nstep:1402/5000 train_loss:4.1688 train_time:250108ms step_avg:179.68ms\nstep:1403/5000 train_loss:3.8917 train_time:250288ms step_avg:179.68ms\nstep:1404/5000 train_loss:3.8665 train_time:250468ms step_avg:179.68ms\nstep:1405/5000 train_loss:3.8761 train_time:250648ms step_avg:179.68ms\nstep:1406/5000 train_loss:3.8168 train_time:250829ms step_avg:179.68ms\nstep:1407/5000 train_loss:3.9352 train_time:251008ms step_avg:179.68ms\nstep:1408/5000 train_loss:4.2230 train_time:251187ms step_avg:179.68ms\nstep:1409/5000 train_loss:3.8512 train_time:251366ms step_avg:179.68ms\nstep:1410/5000 train_loss:3.8707 train_time:251546ms step_avg:179.68ms\nstep:1411/5000 train_loss:3.9408 train_time:251725ms step_avg:179.68ms\nstep:1412/5000 train_loss:3.7075 train_time:251907ms step_avg:179.68ms\nstep:1413/5000 train_loss:3.9532 train_time:252086ms step_avg:179.68ms\nstep:1414/5000 train_loss:3.8742 train_time:252266ms step_avg:179.68ms\nstep:1415/5000 train_loss:3.9290 train_time:252445ms step_avg:179.68ms\nstep:1416/5000 train_loss:3.9714 train_time:252628ms step_avg:179.68ms\nstep:1417/5000 train_loss:3.8175 train_time:252809ms step_avg:179.68ms\nstep:1418/5000 train_loss:3.7768 train_time:252987ms step_avg:179.68ms\nstep:1419/5000 train_loss:3.8272 train_time:253168ms step_avg:179.68ms\nstep:1420/5000 train_loss:3.8449 train_time:253348ms step_avg:179.68ms\nstep:1421/5000 train_loss:3.9460 train_time:253527ms step_avg:179.68ms\nstep:1422/5000 train_loss:3.9176 train_time:253707ms step_avg:179.68ms\nstep:1423/5000 train_loss:3.9421 train_time:253889ms step_avg:179.68ms\nstep:1424/5000 train_loss:3.9389 train_time:254070ms step_avg:179.68ms\nstep:1425/5000 train_loss:3.8660 train_time:254250ms step_avg:179.68ms\nstep:1426/5000 train_loss:3.8335 train_time:254429ms step_avg:179.68ms\nstep:1427/5000 train_loss:3.7778 train_time:254609ms step_avg:179.68ms\nstep:1428/5000 train_loss:3.8456 train_time:254788ms step_avg:179.68ms\nstep:1429/5000 train_loss:3.9043 train_time:254968ms step_avg:179.68ms\nstep:1430/5000 train_loss:3.9582 train_time:255148ms step_avg:179.68ms\nstep:1431/5000 train_loss:4.0777 train_time:255330ms step_avg:179.68ms\nstep:1432/5000 train_loss:3.8988 train_time:255509ms step_avg:179.68ms\nstep:1433/5000 train_loss:3.7562 train_time:255688ms step_avg:179.68ms\nstep:1434/5000 train_loss:3.9377 train_time:255869ms step_avg:179.68ms\nstep:1435/5000 train_loss:3.9952 train_time:256049ms step_avg:179.68ms\nstep:1436/5000 train_loss:3.9166 train_time:256227ms step_avg:179.68ms\nstep:1437/5000 train_loss:3.8807 train_time:256410ms step_avg:179.68ms\nstep:1438/5000 train_loss:3.8648 train_time:256589ms step_avg:179.68ms\nstep:1439/5000 train_loss:4.0250 train_time:256768ms step_avg:179.68ms\nstep:1440/5000 train_loss:3.5131 train_time:256948ms step_avg:179.68ms\nstep:1441/5000 train_loss:3.8589 train_time:257128ms step_avg:179.68ms\nstep:1442/5000 train_loss:4.1373 train_time:257308ms step_avg:179.68ms\nstep:1443/5000 train_loss:3.9741 train_time:257488ms step_avg:179.68ms\nstep:1444/5000 train_loss:4.0788 train_time:257668ms step_avg:179.68ms\nstep:1445/5000 train_loss:3.9235 train_time:257846ms step_avg:179.68ms\nstep:1446/5000 train_loss:4.1032 train_time:258026ms step_avg:179.68ms\nstep:1447/5000 train_loss:3.7429 train_time:258208ms step_avg:179.69ms\nstep:1448/5000 train_loss:3.9723 train_time:258388ms step_avg:179.69ms\nstep:1449/5000 train_loss:3.9309 train_time:258567ms step_avg:179.68ms\nstep:1450/5000 train_loss:3.9189 train_time:258746ms step_avg:179.69ms\nstep:1451/5000 train_loss:3.9426 train_time:258926ms step_avg:179.69ms\nstep:1452/5000 train_loss:3.8977 train_time:259106ms step_avg:179.69ms\nstep:1453/5000 train_loss:3.9372 train_time:259286ms step_avg:179.69ms\nstep:1454/5000 train_loss:3.8234 train_time:259465ms step_avg:179.69ms\nstep:1455/5000 train_loss:3.9190 train_time:259645ms step_avg:179.69ms\nstep:1456/5000 train_loss:3.7929 train_time:259825ms step_avg:179.69ms\nstep:1457/5000 train_loss:3.9323 train_time:260005ms step_avg:179.69ms\nstep:1458/5000 train_loss:3.9986 train_time:260186ms step_avg:179.69ms\nstep:1459/5000 train_loss:3.8508 train_time:260365ms step_avg:179.69ms\nstep:1460/5000 train_loss:3.9661 train_time:260545ms step_avg:179.69ms\nstep:1461/5000 train_loss:4.0015 train_time:260725ms step_avg:179.69ms\nstep:1462/5000 train_loss:3.8366 train_time:260903ms step_avg:179.69ms\nstep:1463/5000 train_loss:4.0402 train_time:261082ms step_avg:179.68ms\nstep:1464/5000 train_loss:3.9155 train_time:261262ms step_avg:179.68ms\nstep:1465/5000 train_loss:3.8554 train_time:261441ms step_avg:179.68ms\nstep:1466/5000 train_loss:3.9236 train_time:261622ms step_avg:179.69ms\nstep:1467/5000 train_loss:3.8711 train_time:261801ms step_avg:179.69ms\nstep:1468/5000 train_loss:3.9164 train_time:261981ms step_avg:179.69ms\nstep:1469/5000 train_loss:3.8736 train_time:262161ms step_avg:179.69ms\nstep:1470/5000 train_loss:3.8518 train_time:262341ms step_avg:179.69ms\nstep:1471/5000 train_loss:3.8959 train_time:262520ms step_avg:179.69ms\nstep:1472/5000 train_loss:3.8971 train_time:262700ms step_avg:179.69ms\nstep:1473/5000 train_loss:3.8850 train_time:262880ms step_avg:179.69ms\nstep:1474/5000 train_loss:3.9428 train_time:263059ms step_avg:179.69ms\nstep:1475/5000 train_loss:3.9079 train_time:263237ms step_avg:179.68ms\nstep:1476/5000 train_loss:3.7896 train_time:263417ms step_avg:179.68ms\nstep:1477/5000 train_loss:3.8107 train_time:263597ms step_avg:179.68ms\nstep:1478/5000 train_loss:3.9191 train_time:263775ms step_avg:179.68ms\nstep:1479/5000 train_loss:3.9063 train_time:263954ms step_avg:179.68ms\nstep:1480/5000 train_loss:3.9328 train_time:264133ms step_avg:179.68ms\nstep:1481/5000 train_loss:3.9111 train_time:264311ms step_avg:179.68ms\nstep:1482/5000 train_loss:3.7229 train_time:264490ms step_avg:179.68ms\nstep:1483/5000 train_loss:3.9804 train_time:264669ms step_avg:179.68ms\nstep:1484/5000 train_loss:3.8027 train_time:264848ms step_avg:179.68ms\nstep:1485/5000 train_loss:4.0029 train_time:265029ms step_avg:179.68ms\nstep:1486/5000 train_loss:3.8351 train_time:265209ms step_avg:179.68ms\nstep:1487/5000 train_loss:3.8662 train_time:265388ms step_avg:179.68ms\nstep:1488/5000 train_loss:3.7501 train_time:265567ms step_avg:179.68ms\nstep:1489/5000 train_loss:3.9114 train_time:265746ms step_avg:179.68ms\nstep:1490/5000 train_loss:3.9019 train_time:265927ms step_avg:179.68ms\nstep:1491/5000 train_loss:3.9205 train_time:266107ms step_avg:179.68ms\nstep:1492/5000 train_loss:3.9698 train_time:266286ms step_avg:179.68ms\nstep:1493/5000 train_loss:3.9479 train_time:266466ms step_avg:179.68ms\nstep:1494/5000 train_loss:3.8679 train_time:266647ms step_avg:179.68ms\nstep:1495/5000 train_loss:3.7440 train_time:266828ms step_avg:179.68ms\nstep:1496/5000 train_loss:3.8796 train_time:267009ms step_avg:179.68ms\nstep:1497/5000 train_loss:4.0437 train_time:267189ms step_avg:179.68ms\nstep:1498/5000 train_loss:3.9511 train_time:267369ms step_avg:179.68ms\nstep:1499/5000 train_loss:3.7941 train_time:267548ms step_avg:179.68ms\nstep:1500/5000 train_loss:3.8663 train_time:267728ms step_avg:179.68ms\nstep:1501/5000 train_loss:3.8955 train_time:267907ms step_avg:179.68ms\nstep:1502/5000 train_loss:3.8550 train_time:268088ms step_avg:179.68ms\nstep:1503/5000 train_loss:3.8709 train_time:268267ms step_avg:179.68ms\nstep:1504/5000 train_loss:3.9141 train_time:268447ms step_avg:179.68ms\nstep:1505/5000 train_loss:3.9001 train_time:268627ms step_avg:179.68ms\nstep:1506/5000 train_loss:3.7595 train_time:268808ms step_avg:179.68ms\nstep:1507/5000 train_loss:4.0195 train_time:268987ms step_avg:179.68ms\nstep:1508/5000 train_loss:3.8392 train_time:269166ms step_avg:179.68ms\nstep:1509/5000 train_loss:3.8023 train_time:269345ms step_avg:179.68ms\nstep:1510/5000 train_loss:3.3505 train_time:269525ms step_avg:179.68ms\nstep:1511/5000 train_loss:3.9312 train_time:269704ms step_avg:179.68ms\nstep:1512/5000 train_loss:3.8869 train_time:269885ms step_avg:179.68ms\nstep:1513/5000 train_loss:4.0172 train_time:270067ms step_avg:179.69ms\nstep:1514/5000 train_loss:3.7303 train_time:270246ms step_avg:179.68ms\nstep:1515/5000 train_loss:3.7650 train_time:270427ms step_avg:179.69ms\nstep:1516/5000 train_loss:3.9221 train_time:270609ms step_avg:179.69ms\nstep:1517/5000 train_loss:3.9711 train_time:270787ms step_avg:179.69ms\nstep:1518/5000 train_loss:3.7530 train_time:270966ms step_avg:179.69ms\nstep:1519/5000 train_loss:4.0284 train_time:271149ms step_avg:179.69ms\nstep:1520/5000 train_loss:4.0030 train_time:271328ms step_avg:179.69ms\nstep:1521/5000 train_loss:3.8569 train_time:271508ms step_avg:179.69ms\nstep:1522/5000 train_loss:3.8504 train_time:271690ms step_avg:179.69ms\nstep:1523/5000 train_loss:3.9187 train_time:271868ms step_avg:179.69ms\nstep:1524/5000 train_loss:3.7502 train_time:272282ms step_avg:179.84ms\nstep:1525/5000 train_loss:3.8501 train_time:272457ms step_avg:179.84ms\nstep:1526/5000 train_loss:3.9013 train_time:272634ms step_avg:179.84ms\nstep:1527/5000 train_loss:3.8243 train_time:272813ms step_avg:179.84ms\nstep:1528/5000 train_loss:3.8257 train_time:272991ms step_avg:179.84ms\nstep:1529/5000 train_loss:4.1900 train_time:273174ms step_avg:179.84ms\nstep:1530/5000 train_loss:3.8798 train_time:273352ms step_avg:179.84ms\nstep:1531/5000 train_loss:3.8278 train_time:273530ms step_avg:179.84ms\nstep:1532/5000 train_loss:3.8250 train_time:273710ms step_avg:179.84ms\nstep:1533/5000 train_loss:3.8081 train_time:273888ms step_avg:179.83ms\nstep:1534/5000 train_loss:3.8502 train_time:274068ms step_avg:179.83ms\nstep:1535/5000 train_loss:3.7826 train_time:274249ms step_avg:179.84ms\nstep:1536/5000 train_loss:3.9428 train_time:274430ms step_avg:179.84ms\nstep:1537/5000 train_loss:3.9929 train_time:274611ms step_avg:179.84ms\nstep:1538/5000 train_loss:3.9294 train_time:274789ms step_avg:179.84ms\nstep:1539/5000 train_loss:3.8760 train_time:274969ms step_avg:179.84ms\nstep:1540/5000 train_loss:3.7308 train_time:275151ms step_avg:179.84ms\nstep:1541/5000 train_loss:3.9098 train_time:275329ms step_avg:179.84ms\nstep:1542/5000 train_loss:3.8472 train_time:275508ms step_avg:179.84ms\nstep:1543/5000 train_loss:3.9643 train_time:275688ms step_avg:179.84ms\nstep:1544/5000 train_loss:3.9198 train_time:275866ms step_avg:179.83ms\nstep:1545/5000 train_loss:4.0151 train_time:276047ms step_avg:179.84ms\nstep:1546/5000 train_loss:3.9419 train_time:276227ms step_avg:179.84ms\nstep:1547/5000 train_loss:3.8451 train_time:276407ms step_avg:179.84ms\nstep:1548/5000 train_loss:3.8369 train_time:276586ms step_avg:179.83ms\nstep:1549/5000 train_loss:3.9439 train_time:276765ms step_avg:179.83ms\nstep:1550/5000 train_loss:4.1393 train_time:276944ms step_avg:179.83ms\nstep:1551/5000 train_loss:3.8595 train_time:277124ms step_avg:179.83ms\nstep:1552/5000 train_loss:4.0235 train_time:277305ms step_avg:179.83ms\nstep:1553/5000 train_loss:3.9694 train_time:277487ms step_avg:179.84ms\nstep:1554/5000 train_loss:3.8483 train_time:277668ms step_avg:179.84ms\nstep:1555/5000 train_loss:3.7863 train_time:277848ms step_avg:179.84ms\nstep:1556/5000 train_loss:3.8684 train_time:278027ms step_avg:179.84ms\nstep:1557/5000 train_loss:3.8817 train_time:278207ms step_avg:179.84ms\nstep:1558/5000 train_loss:3.8414 train_time:278386ms step_avg:179.84ms\nstep:1559/5000 train_loss:3.9044 train_time:278566ms step_avg:179.84ms\nstep:1560/5000 train_loss:3.9270 train_time:278746ms step_avg:179.84ms\nstep:1561/5000 train_loss:3.7546 train_time:278925ms step_avg:179.84ms\nstep:1562/5000 train_loss:3.9544 train_time:279103ms step_avg:179.83ms\nstep:1563/5000 train_loss:3.9044 train_time:279283ms step_avg:179.83ms\nstep:1564/5000 train_loss:3.7704 train_time:279463ms step_avg:179.83ms\nstep:1565/5000 train_loss:3.9104 train_time:279644ms step_avg:179.84ms\nstep:1566/5000 train_loss:3.8818 train_time:279825ms step_avg:179.84ms\nstep:1567/5000 train_loss:3.8566 train_time:280004ms step_avg:179.84ms\nstep:1568/5000 train_loss:3.8407 train_time:280183ms step_avg:179.84ms\nstep:1569/5000 train_loss:3.8671 train_time:280363ms step_avg:179.83ms\nstep:1570/5000 train_loss:3.9818 train_time:280541ms step_avg:179.83ms\nstep:1571/5000 train_loss:4.0084 train_time:280722ms step_avg:179.83ms\nstep:1572/5000 train_loss:3.8654 train_time:280901ms step_avg:179.83ms\nstep:1573/5000 train_loss:3.9589 train_time:281080ms step_avg:179.83ms\nstep:1574/5000 train_loss:3.8169 train_time:281260ms step_avg:179.83ms\nstep:1575/5000 train_loss:3.9465 train_time:281439ms step_avg:179.83ms\nstep:1576/5000 train_loss:3.8730 train_time:281618ms step_avg:179.83ms\nstep:1577/5000 train_loss:3.8703 train_time:281798ms step_avg:179.83ms\nstep:1578/5000 train_loss:3.9415 train_time:281977ms step_avg:179.83ms\nstep:1579/5000 train_loss:4.0058 train_time:282156ms step_avg:179.83ms\nstep:1580/5000 train_loss:3.8693 train_time:282334ms step_avg:179.83ms\nstep:1581/5000 train_loss:3.9099 train_time:282512ms step_avg:179.83ms\nstep:1582/5000 train_loss:3.6971 train_time:282692ms step_avg:179.83ms\nstep:1583/5000 train_loss:3.8685 train_time:282871ms step_avg:179.83ms\nstep:1584/5000 train_loss:3.8103 train_time:283049ms step_avg:179.83ms\nstep:1585/5000 train_loss:3.8666 train_time:283229ms step_avg:179.83ms\nstep:1586/5000 train_loss:3.8974 train_time:283408ms step_avg:179.83ms\nstep:1587/5000 train_loss:3.7127 train_time:283588ms step_avg:179.83ms\nstep:1588/5000 train_loss:3.6499 train_time:283768ms step_avg:179.83ms\nstep:1589/5000 train_loss:3.7515 train_time:283948ms step_avg:179.83ms\nstep:1590/5000 train_loss:3.9461 train_time:284127ms step_avg:179.83ms\nstep:1591/5000 train_loss:4.1723 train_time:284307ms step_avg:179.83ms\nstep:1592/5000 train_loss:3.8796 train_time:284489ms step_avg:179.83ms\nstep:1593/5000 train_loss:4.0119 train_time:284667ms step_avg:179.83ms\nstep:1594/5000 train_loss:3.9532 train_time:284846ms step_avg:179.83ms\nstep:1595/5000 train_loss:3.8346 train_time:285026ms step_avg:179.83ms\nstep:1596/5000 train_loss:3.8615 train_time:285205ms step_avg:179.83ms\nstep:1597/5000 train_loss:3.8978 train_time:285385ms step_avg:179.83ms\nstep:1598/5000 train_loss:3.8469 train_time:285566ms step_avg:179.83ms\nstep:1599/5000 train_loss:3.7982 train_time:285744ms step_avg:179.83ms\nstep:1600/5000 train_loss:3.7775 train_time:285923ms step_avg:179.83ms\nstep:1601/5000 train_loss:3.8428 train_time:286102ms step_avg:179.83ms\nstep:1602/5000 train_loss:3.9195 train_time:286281ms step_avg:179.83ms\nstep:1603/5000 train_loss:3.8317 train_time:286462ms step_avg:179.83ms\nstep:1604/5000 train_loss:3.8813 train_time:286641ms step_avg:179.83ms\nstep:1605/5000 train_loss:3.9004 train_time:286821ms step_avg:179.83ms\nstep:1606/5000 train_loss:3.9834 train_time:287002ms step_avg:179.83ms\nstep:1607/5000 train_loss:4.0264 train_time:287182ms step_avg:179.83ms\nstep:1608/5000 train_loss:3.7774 train_time:287361ms step_avg:179.83ms\nstep:1609/5000 train_loss:3.8206 train_time:287540ms step_avg:179.82ms\nstep:1610/5000 train_loss:3.9024 train_time:287719ms step_avg:179.82ms\nstep:1611/5000 train_loss:3.8854 train_time:287898ms step_avg:179.82ms\nstep:1612/5000 train_loss:4.0523 train_time:288076ms step_avg:179.82ms\nstep:1613/5000 train_loss:3.8328 train_time:288254ms step_avg:179.82ms\nstep:1614/5000 train_loss:4.0174 train_time:288431ms step_avg:179.82ms\nstep:1615/5000 train_loss:3.7965 train_time:288611ms step_avg:179.82ms\nstep:1616/5000 train_loss:3.9774 train_time:288790ms step_avg:179.82ms\nstep:1617/5000 train_loss:3.8855 train_time:288969ms step_avg:179.82ms\nstep:1618/5000 train_loss:3.7761 train_time:289149ms step_avg:179.82ms\nstep:1619/5000 train_loss:3.8477 train_time:289330ms step_avg:179.82ms\nstep:1620/5000 train_loss:3.9040 train_time:289510ms step_avg:179.82ms\nstep:1621/5000 train_loss:3.9159 train_time:289689ms step_avg:179.82ms\nstep:1622/5000 train_loss:3.8600 train_time:289869ms step_avg:179.82ms\nstep:1623/5000 train_loss:3.8811 train_time:290047ms step_avg:179.82ms\nstep:1624/5000 train_loss:3.9228 train_time:290227ms step_avg:179.82ms\nstep:1625/5000 train_loss:4.4452 train_time:290407ms step_avg:179.82ms\nstep:1626/5000 train_loss:4.1631 train_time:290587ms step_avg:179.82ms\nstep:1627/5000 train_loss:3.8184 train_time:290767ms step_avg:179.82ms\nstep:1628/5000 train_loss:3.9015 train_time:290948ms step_avg:179.82ms\nstep:1629/5000 train_loss:3.8964 train_time:291128ms step_avg:179.82ms\nstep:1630/5000 train_loss:3.8577 train_time:291309ms step_avg:179.82ms\nstep:1631/5000 train_loss:3.8787 train_time:291487ms step_avg:179.82ms\nstep:1632/5000 train_loss:3.8023 train_time:291667ms step_avg:179.82ms\nstep:1633/5000 train_loss:4.0511 train_time:291846ms step_avg:179.82ms\nstep:1634/5000 train_loss:3.8880 train_time:292027ms step_avg:179.82ms\nstep:1635/5000 train_loss:3.9009 train_time:292206ms step_avg:179.82ms\nstep:1636/5000 train_loss:3.8898 train_time:292387ms step_avg:179.82ms\nstep:1637/5000 train_loss:4.1045 train_time:292565ms step_avg:179.82ms\nstep:1638/5000 train_loss:3.9255 train_time:292745ms step_avg:179.82ms\nstep:1639/5000 train_loss:3.8528 train_time:292925ms step_avg:179.82ms\nstep:1640/5000 train_loss:3.8273 train_time:293105ms step_avg:179.82ms\nstep:1641/5000 train_loss:3.8648 train_time:293285ms step_avg:179.82ms\nstep:1642/5000 train_loss:3.8237 train_time:293467ms step_avg:179.82ms\nstep:1643/5000 train_loss:3.9375 train_time:293647ms step_avg:179.82ms\nstep:1644/5000 train_loss:3.8315 train_time:293826ms step_avg:179.82ms\nstep:1645/5000 train_loss:3.8821 train_time:294005ms step_avg:179.82ms\nstep:1646/5000 train_loss:3.8336 train_time:294184ms step_avg:179.82ms\nstep:1647/5000 train_loss:3.8319 train_time:294364ms step_avg:179.82ms\nstep:1648/5000 train_loss:3.8299 train_time:294545ms step_avg:179.82ms\nstep:1649/5000 train_loss:3.9201 train_time:294724ms step_avg:179.82ms\nstep:1650/5000 train_loss:4.2717 train_time:294905ms step_avg:179.82ms\nstep:1651/5000 train_loss:3.8931 train_time:295084ms step_avg:179.82ms\nstep:1652/5000 train_loss:3.8349 train_time:295265ms step_avg:179.82ms\nstep:1653/5000 train_loss:4.0085 train_time:295444ms step_avg:179.82ms\nstep:1654/5000 train_loss:3.7017 train_time:295624ms step_avg:179.82ms\nstep:1655/5000 train_loss:3.8217 train_time:295804ms step_avg:179.82ms\nstep:1656/5000 train_loss:3.9578 train_time:295985ms step_avg:179.82ms\nstep:1657/5000 train_loss:3.9538 train_time:296165ms step_avg:179.82ms\nstep:1658/5000 train_loss:4.0774 train_time:296346ms step_avg:179.82ms\nstep:1659/5000 train_loss:3.9564 train_time:296527ms step_avg:179.82ms\nstep:1660/5000 train_loss:3.7845 train_time:296707ms step_avg:179.82ms\nstep:1661/5000 train_loss:3.8134 train_time:296889ms step_avg:179.82ms\nstep:1662/5000 train_loss:3.8335 train_time:297069ms step_avg:179.82ms\nstep:1663/5000 train_loss:4.0132 train_time:297248ms step_avg:179.82ms\nstep:1664/5000 train_loss:3.8581 train_time:297427ms step_avg:179.82ms\nstep:1665/5000 train_loss:4.0196 train_time:297608ms step_avg:179.82ms\nstep:1666/5000 train_loss:3.8445 train_time:297787ms step_avg:179.82ms\nstep:1667/5000 train_loss:3.6352 train_time:297969ms step_avg:179.82ms\nstep:1668/5000 train_loss:3.9739 train_time:298150ms step_avg:179.83ms\nstep:1669/5000 train_loss:3.9064 train_time:298329ms step_avg:179.82ms\nstep:1670/5000 train_loss:4.0194 train_time:298509ms step_avg:179.82ms\nstep:1671/5000 train_loss:3.8399 train_time:298688ms step_avg:179.82ms\nstep:1672/5000 train_loss:3.7294 train_time:298868ms step_avg:179.82ms\nstep:1673/5000 train_loss:3.7603 train_time:299047ms step_avg:179.82ms\nstep:1674/5000 train_loss:3.7625 train_time:299227ms step_avg:179.82ms\nstep:1675/5000 train_loss:3.9699 train_time:299408ms step_avg:179.82ms\nstep:1676/5000 train_loss:4.0007 train_time:299588ms step_avg:179.82ms\nstep:1677/5000 train_loss:3.7250 train_time:299767ms step_avg:179.82ms\nstep:1678/5000 train_loss:3.8786 train_time:299948ms step_avg:179.82ms\nstep:1679/5000 train_loss:3.8136 train_time:300127ms step_avg:179.82ms\nstep:1680/5000 train_loss:4.0714 train_time:300305ms step_avg:179.82ms\nstep:1681/5000 train_loss:3.7106 train_time:300486ms step_avg:179.82ms\nstep:1682/5000 train_loss:3.7901 train_time:300666ms step_avg:179.82ms\nstep:1683/5000 train_loss:3.8417 train_time:300848ms step_avg:179.83ms\nstep:1684/5000 train_loss:3.8283 train_time:301027ms step_avg:179.82ms\nstep:1685/5000 train_loss:3.9298 train_time:301205ms step_avg:179.82ms\nstep:1686/5000 train_loss:3.7172 train_time:301384ms step_avg:179.82ms\nstep:1687/5000 train_loss:3.8559 train_time:301562ms step_avg:179.82ms\nstep:1688/5000 train_loss:3.9529 train_time:301742ms step_avg:179.82ms\nstep:1689/5000 train_loss:3.7809 train_time:301921ms step_avg:179.82ms\nstep:1690/5000 train_loss:3.8515 train_time:302101ms step_avg:179.82ms\nstep:1691/5000 train_loss:3.7673 train_time:302280ms step_avg:179.82ms\nstep:1692/5000 train_loss:3.6692 train_time:302459ms step_avg:179.82ms\nstep:1693/5000 train_loss:3.8755 train_time:302638ms step_avg:179.82ms\nstep:1694/5000 train_loss:3.9463 train_time:302816ms step_avg:179.82ms\nstep:1695/5000 train_loss:3.8431 train_time:302995ms step_avg:179.82ms\nstep:1696/5000 train_loss:3.8395 train_time:303175ms step_avg:179.82ms\nstep:1697/5000 train_loss:3.8803 train_time:303354ms step_avg:179.82ms\nstep:1698/5000 train_loss:3.7010 train_time:303533ms step_avg:179.82ms\nstep:1699/5000 train_loss:3.9333 train_time:303712ms step_avg:179.82ms\nstep:1700/5000 train_loss:3.7276 train_time:303891ms step_avg:179.82ms\nstep:1701/5000 train_loss:3.9491 train_time:304069ms step_avg:179.82ms\nstep:1702/5000 train_loss:3.8739 train_time:304248ms step_avg:179.82ms\nstep:1703/5000 train_loss:3.7290 train_time:304430ms step_avg:179.82ms\nstep:1704/5000 train_loss:3.9314 train_time:304610ms step_avg:179.82ms\nstep:1705/5000 train_loss:3.8006 train_time:304788ms step_avg:179.82ms\nstep:1706/5000 train_loss:3.7594 train_time:304967ms step_avg:179.82ms\nstep:1707/5000 train_loss:3.9096 train_time:305145ms step_avg:179.81ms\nstep:1708/5000 train_loss:3.9041 train_time:305325ms step_avg:179.81ms\nstep:1709/5000 train_loss:3.7559 train_time:305504ms step_avg:179.81ms\nstep:1710/5000 train_loss:3.9680 train_time:305684ms step_avg:179.81ms\nstep:1711/5000 train_loss:3.8763 train_time:305864ms step_avg:179.81ms\nstep:1712/5000 train_loss:3.8358 train_time:306045ms step_avg:179.81ms\nstep:1713/5000 train_loss:3.8414 train_time:306224ms step_avg:179.81ms\nstep:1714/5000 train_loss:3.8960 train_time:306407ms step_avg:179.82ms\nstep:1715/5000 train_loss:3.7783 train_time:306587ms step_avg:179.82ms\nstep:1716/5000 train_loss:3.9368 train_time:306766ms step_avg:179.82ms\nstep:1717/5000 train_loss:3.8216 train_time:306947ms step_avg:179.82ms\nstep:1718/5000 train_loss:3.8312 train_time:307127ms step_avg:179.82ms\nstep:1719/5000 train_loss:3.7522 train_time:307307ms step_avg:179.82ms\nstep:1720/5000 train_loss:3.7625 train_time:307487ms step_avg:179.82ms\nstep:1721/5000 train_loss:3.7768 train_time:307667ms step_avg:179.82ms\nstep:1722/5000 train_loss:3.7926 train_time:307846ms step_avg:179.82ms\nstep:1723/5000 train_loss:4.0317 train_time:308027ms step_avg:179.82ms\nstep:1724/5000 train_loss:3.6981 train_time:308207ms step_avg:179.82ms\nstep:1725/5000 train_loss:3.7996 train_time:308386ms step_avg:179.82ms\nstep:1726/5000 train_loss:3.7079 train_time:308566ms step_avg:179.82ms\nstep:1727/5000 train_loss:4.0629 train_time:308745ms step_avg:179.82ms\nstep:1728/5000 train_loss:3.9223 train_time:308927ms step_avg:179.82ms\nstep:1729/5000 train_loss:3.9878 train_time:309109ms step_avg:179.82ms\nstep:1730/5000 train_loss:3.8348 train_time:309288ms step_avg:179.82ms\nstep:1731/5000 train_loss:3.8494 train_time:309467ms step_avg:179.82ms\nstep:1732/5000 train_loss:3.9416 train_time:309649ms step_avg:179.82ms\nstep:1733/5000 train_loss:3.7004 train_time:309828ms step_avg:179.82ms\nstep:1734/5000 train_loss:3.8881 train_time:310009ms step_avg:179.82ms\nstep:1735/5000 train_loss:3.8411 train_time:310188ms step_avg:179.82ms\nstep:1736/5000 train_loss:3.9208 train_time:310367ms step_avg:179.82ms\nstep:1737/5000 train_loss:3.9492 train_time:310547ms step_avg:179.82ms\nstep:1738/5000 train_loss:3.8500 train_time:310725ms step_avg:179.82ms\nstep:1739/5000 train_loss:3.8692 train_time:310906ms step_avg:179.82ms\nstep:1740/5000 train_loss:3.8189 train_time:311085ms step_avg:179.82ms\nstep:1741/5000 train_loss:3.8515 train_time:311265ms step_avg:179.82ms\nstep:1742/5000 train_loss:3.8883 train_time:311445ms step_avg:179.82ms\nstep:1743/5000 train_loss:3.9448 train_time:311625ms step_avg:179.82ms\nstep:1744/5000 train_loss:3.9139 train_time:311804ms step_avg:179.82ms\nstep:1745/5000 train_loss:3.8822 train_time:311984ms step_avg:179.82ms\nstep:1746/5000 train_loss:3.9080 train_time:312163ms step_avg:179.82ms\nstep:1747/5000 train_loss:3.9755 train_time:312345ms step_avg:179.82ms\nstep:1748/5000 train_loss:3.9017 train_time:312526ms step_avg:179.82ms\nstep:1749/5000 train_loss:3.8982 train_time:312706ms step_avg:179.82ms\nstep:1750/5000 train_loss:3.7863 train_time:312885ms step_avg:179.82ms\nstep:1751/5000 train_loss:4.1788 train_time:313065ms step_avg:179.82ms\nstep:1752/5000 train_loss:3.8801 train_time:313245ms step_avg:179.82ms\nstep:1753/5000 train_loss:3.9337 train_time:313425ms step_avg:179.82ms\nstep:1754/5000 train_loss:3.7944 train_time:313608ms step_avg:179.82ms\nstep:1755/5000 train_loss:3.8897 train_time:313787ms step_avg:179.82ms\nstep:1756/5000 train_loss:3.9280 train_time:313966ms step_avg:179.82ms\nstep:1757/5000 train_loss:3.8831 train_time:314144ms step_avg:179.82ms\nstep:1758/5000 train_loss:3.7877 train_time:314324ms step_avg:179.82ms\nstep:1759/5000 train_loss:3.8997 train_time:314504ms step_avg:179.82ms\nstep:1760/5000 train_loss:3.7758 train_time:314684ms step_avg:179.82ms\nstep:1761/5000 train_loss:3.7059 train_time:314863ms step_avg:179.82ms\nstep:1762/5000 train_loss:3.9656 train_time:315042ms step_avg:179.82ms\nstep:1763/5000 train_loss:3.9277 train_time:315221ms step_avg:179.82ms\nstep:1764/5000 train_loss:3.8736 train_time:315400ms step_avg:179.82ms\nstep:1765/5000 train_loss:3.5863 train_time:315579ms step_avg:179.82ms\nstep:1766/5000 train_loss:3.8460 train_time:315758ms step_avg:179.82ms\nstep:1767/5000 train_loss:3.7627 train_time:315937ms step_avg:179.82ms\nstep:1768/5000 train_loss:3.8019 train_time:316116ms step_avg:179.82ms\nstep:1769/5000 train_loss:3.7484 train_time:316295ms step_avg:179.82ms\nstep:1770/5000 train_loss:3.8164 train_time:316474ms step_avg:179.81ms\nstep:1771/5000 train_loss:3.7758 train_time:316652ms step_avg:179.81ms\nstep:1772/5000 train_loss:3.7011 train_time:316832ms step_avg:179.81ms\nstep:1773/5000 train_loss:3.9406 train_time:317011ms step_avg:179.81ms\nstep:1774/5000 train_loss:3.7150 train_time:317190ms step_avg:179.81ms\nstep:1775/5000 train_loss:3.7828 train_time:317369ms step_avg:179.81ms\nstep:1776/5000 train_loss:3.8537 train_time:317550ms step_avg:179.81ms\nstep:1777/5000 train_loss:3.8388 train_time:317730ms step_avg:179.81ms\nstep:1778/5000 train_loss:3.9989 train_time:317910ms step_avg:179.81ms\nstep:1779/5000 train_loss:3.9013 train_time:318090ms step_avg:179.81ms\nstep:1780/5000 train_loss:4.0974 train_time:318269ms step_avg:179.81ms\nstep:1781/5000 train_loss:3.7738 train_time:318447ms step_avg:179.81ms\nstep:1782/5000 train_loss:3.9238 train_time:318628ms step_avg:179.81ms\nstep:1783/5000 train_loss:3.9885 train_time:318809ms step_avg:179.81ms\nstep:1784/5000 train_loss:3.7131 train_time:318988ms step_avg:179.81ms\nstep:1785/5000 train_loss:3.8553 train_time:319168ms step_avg:179.81ms\nstep:1786/5000 train_loss:3.9126 train_time:319350ms step_avg:179.81ms\nstep:1787/5000 train_loss:3.8365 train_time:319530ms step_avg:179.81ms\nstep:1788/5000 train_loss:3.9068 train_time:319710ms step_avg:179.81ms\nstep:1789/5000 train_loss:3.8992 train_time:319889ms step_avg:179.81ms\nstep:1790/5000 train_loss:3.8322 train_time:320070ms step_avg:179.81ms\nstep:1791/5000 train_loss:3.7039 train_time:320250ms step_avg:179.81ms\nstep:1792/5000 train_loss:3.9461 train_time:320429ms step_avg:179.81ms\nstep:1793/5000 train_loss:3.9036 train_time:320610ms step_avg:179.81ms\nstep:1794/5000 train_loss:3.9347 train_time:320790ms step_avg:179.81ms\nstep:1795/5000 train_loss:4.1810 train_time:320969ms step_avg:179.81ms\nstep:1796/5000 train_loss:3.7835 train_time:321148ms step_avg:179.81ms\nstep:1797/5000 train_loss:3.8887 train_time:321327ms step_avg:179.81ms\nstep:1798/5000 train_loss:3.7710 train_time:321506ms step_avg:179.81ms\nstep:1799/5000 train_loss:3.9180 train_time:321687ms step_avg:179.81ms\nstep:1800/5000 train_loss:3.7577 train_time:321868ms step_avg:179.81ms\nstep:1801/5000 train_loss:3.9611 train_time:322048ms step_avg:179.81ms\nstep:1802/5000 train_loss:3.7925 train_time:322228ms step_avg:179.81ms\nstep:1803/5000 train_loss:3.9255 train_time:322407ms step_avg:179.81ms\nstep:1804/5000 train_loss:3.8140 train_time:322586ms step_avg:179.81ms\nstep:1805/5000 train_loss:3.8542 train_time:322764ms step_avg:179.81ms\nstep:1806/5000 train_loss:3.6672 train_time:322944ms step_avg:179.81ms\nstep:1807/5000 train_loss:3.9222 train_time:323124ms step_avg:179.81ms\nstep:1808/5000 train_loss:4.1321 train_time:323303ms step_avg:179.81ms\nstep:1809/5000 train_loss:3.9492 train_time:323483ms step_avg:179.81ms\nstep:1810/5000 train_loss:3.7888 train_time:323665ms step_avg:179.81ms\nstep:1811/5000 train_loss:3.8423 train_time:323845ms step_avg:179.81ms\nstep:1812/5000 train_loss:4.0212 train_time:324025ms step_avg:179.81ms\nstep:1813/5000 train_loss:3.8973 train_time:324206ms step_avg:179.81ms\nstep:1814/5000 train_loss:3.7585 train_time:324386ms step_avg:179.81ms\nstep:1815/5000 train_loss:3.8772 train_time:324565ms step_avg:179.81ms\nstep:1816/5000 train_loss:3.8813 train_time:324744ms step_avg:179.81ms\nstep:1817/5000 train_loss:4.0836 train_time:324924ms step_avg:179.81ms\nstep:1818/5000 train_loss:3.7252 train_time:325104ms step_avg:179.81ms\nstep:1819/5000 train_loss:3.6381 train_time:325286ms step_avg:179.82ms\nstep:1820/5000 train_loss:3.7870 train_time:325468ms step_avg:179.82ms\nstep:1821/5000 train_loss:3.9023 train_time:325651ms step_avg:179.82ms\nstep:1822/5000 train_loss:3.9320 train_time:325828ms step_avg:179.82ms\nstep:1823/5000 train_loss:3.7069 train_time:326007ms step_avg:179.82ms\nstep:1824/5000 train_loss:3.8976 train_time:326190ms step_avg:179.82ms\nstep:1825/5000 train_loss:3.9091 train_time:326365ms step_avg:179.82ms\nstep:1826/5000 train_loss:3.8181 train_time:326545ms step_avg:179.82ms\nstep:1827/5000 train_loss:3.8158 train_time:326726ms step_avg:179.82ms\nstep:1828/5000 train_loss:3.8362 train_time:326905ms step_avg:179.82ms\nstep:1829/5000 train_loss:3.8666 train_time:327085ms step_avg:179.82ms\nstep:1830/5000 train_loss:3.8767 train_time:327264ms step_avg:179.82ms\nstep:1831/5000 train_loss:3.7668 train_time:327444ms step_avg:179.82ms\nstep:1832/5000 train_loss:3.8578 train_time:327624ms step_avg:179.82ms\nstep:1833/5000 train_loss:3.6198 train_time:327803ms step_avg:179.82ms\nstep:1834/5000 train_loss:3.9086 train_time:327986ms step_avg:179.82ms\nstep:1835/5000 train_loss:3.7883 train_time:328162ms step_avg:179.81ms\nstep:1836/5000 train_loss:3.9241 train_time:328342ms step_avg:179.81ms\nstep:1837/5000 train_loss:3.8237 train_time:328522ms step_avg:179.81ms\nstep:1838/5000 train_loss:3.7185 train_time:328703ms step_avg:179.82ms\nstep:1839/5000 train_loss:3.7583 train_time:328882ms step_avg:179.82ms\nstep:1840/5000 train_loss:3.7635 train_time:329062ms step_avg:179.82ms\nstep:1841/5000 train_loss:3.7980 train_time:329241ms step_avg:179.81ms\nstep:1842/5000 train_loss:3.8833 train_time:329420ms step_avg:179.81ms\nstep:1843/5000 train_loss:3.9052 train_time:329599ms step_avg:179.81ms\nstep:1844/5000 train_loss:3.8774 train_time:329779ms step_avg:179.81ms\nstep:1845/5000 train_loss:3.7890 train_time:329957ms step_avg:179.81ms\nstep:1846/5000 train_loss:3.7909 train_time:330136ms step_avg:179.81ms\nstep:1847/5000 train_loss:3.8979 train_time:330314ms step_avg:179.81ms\nstep:1848/5000 train_loss:3.8469 train_time:330492ms step_avg:179.81ms\nstep:1849/5000 train_loss:3.8156 train_time:330672ms step_avg:179.81ms\nstep:1850/5000 train_loss:3.8129 train_time:330850ms step_avg:179.81ms\nstep:1851/5000 train_loss:3.7999 train_time:331031ms step_avg:179.81ms\nstep:1852/5000 train_loss:3.6647 train_time:331209ms step_avg:179.81ms\nstep:1853/5000 train_loss:3.7147 train_time:331390ms step_avg:179.81ms\nstep:1854/5000 train_loss:3.8608 train_time:331569ms step_avg:179.81ms\nstep:1855/5000 train_loss:3.6813 train_time:331748ms step_avg:179.81ms\nstep:1856/5000 train_loss:3.8167 train_time:331927ms step_avg:179.81ms\nstep:1857/5000 train_loss:3.8656 train_time:332107ms step_avg:179.81ms\nstep:1858/5000 train_loss:3.7951 train_time:332286ms step_avg:179.81ms\nstep:1859/5000 train_loss:3.9522 train_time:332465ms step_avg:179.81ms\nstep:1860/5000 train_loss:3.9046 train_time:332645ms step_avg:179.81ms\nstep:1861/5000 train_loss:3.8544 train_time:332825ms step_avg:179.81ms\nstep:1862/5000 train_loss:3.8020 train_time:333006ms step_avg:179.81ms\nstep:1863/5000 train_loss:3.8378 train_time:333187ms step_avg:179.81ms\nstep:1864/5000 train_loss:3.8117 train_time:333365ms step_avg:179.81ms\nstep:1865/5000 train_loss:3.8905 train_time:333547ms step_avg:179.81ms\nstep:1866/5000 train_loss:3.8992 train_time:333724ms step_avg:179.81ms\nstep:1867/5000 train_loss:3.7713 train_time:333903ms step_avg:179.81ms\nstep:1868/5000 train_loss:3.8070 train_time:334082ms step_avg:179.81ms\nstep:1869/5000 train_loss:3.9406 train_time:334261ms step_avg:179.81ms\nstep:1870/5000 train_loss:3.9157 train_time:334441ms step_avg:179.81ms\nstep:1871/5000 train_loss:3.7163 train_time:334622ms step_avg:179.81ms\nstep:1872/5000 train_loss:3.7954 train_time:334801ms step_avg:179.81ms\nstep:1873/5000 train_loss:3.7889 train_time:334981ms step_avg:179.81ms\nstep:1874/5000 train_loss:3.8807 train_time:335160ms step_avg:179.81ms\nstep:1875/5000 train_loss:3.6871 train_time:335339ms step_avg:179.81ms\nstep:1876/5000 train_loss:3.9935 train_time:335517ms step_avg:179.81ms\nstep:1877/5000 train_loss:3.8353 train_time:335697ms step_avg:179.81ms\nstep:1878/5000 train_loss:3.7783 train_time:335876ms step_avg:179.80ms\nstep:1879/5000 train_loss:3.8085 train_time:336054ms step_avg:179.80ms\nstep:1880/5000 train_loss:3.8378 train_time:336233ms step_avg:179.80ms\nstep:1881/5000 train_loss:3.9125 train_time:336411ms step_avg:179.80ms\nstep:1882/5000 train_loss:3.8573 train_time:336590ms step_avg:179.80ms\nstep:1883/5000 train_loss:3.9492 train_time:336768ms step_avg:179.80ms\nstep:1884/5000 train_loss:3.6927 train_time:336949ms step_avg:179.80ms\nstep:1885/5000 train_loss:3.6514 train_time:337128ms step_avg:179.80ms\nstep:1886/5000 train_loss:3.9056 train_time:337307ms step_avg:179.80ms\nstep:1887/5000 train_loss:3.9427 train_time:337487ms step_avg:179.80ms\nstep:1888/5000 train_loss:3.9046 train_time:337666ms step_avg:179.80ms\nstep:1889/5000 train_loss:3.8390 train_time:337848ms step_avg:179.80ms\nstep:1890/5000 train_loss:3.8624 train_time:338030ms step_avg:179.80ms\nstep:1891/5000 train_loss:3.7604 train_time:338209ms step_avg:179.80ms\nstep:1892/5000 train_loss:3.7637 train_time:338387ms step_avg:179.80ms\nstep:1893/5000 train_loss:3.8382 train_time:338565ms step_avg:179.80ms\nstep:1894/5000 train_loss:3.8141 train_time:338746ms step_avg:179.80ms\nstep:1895/5000 train_loss:3.8896 train_time:338924ms step_avg:179.80ms\nstep:1896/5000 train_loss:3.8845 train_time:339104ms step_avg:179.80ms\nstep:1897/5000 train_loss:3.7584 train_time:339285ms step_avg:179.80ms\nstep:1898/5000 train_loss:3.6713 train_time:339465ms step_avg:179.80ms\nstep:1899/5000 train_loss:3.8505 train_time:339644ms step_avg:179.80ms\nstep:1900/5000 train_loss:3.9407 train_time:339823ms step_avg:179.80ms\nstep:1901/5000 train_loss:3.8472 train_time:340003ms step_avg:179.80ms\nstep:1902/5000 train_loss:4.1141 train_time:340182ms step_avg:179.80ms\nstep:1903/5000 train_loss:3.7595 train_time:340361ms step_avg:179.80ms\nstep:1904/5000 train_loss:4.1200 train_time:340540ms step_avg:179.80ms\nstep:1905/5000 train_loss:3.8931 train_time:340719ms step_avg:179.80ms\nstep:1906/5000 train_loss:3.8610 train_time:340900ms step_avg:179.80ms\nstep:1907/5000 train_loss:4.0845 train_time:341080ms step_avg:179.80ms\nstep:1908/5000 train_loss:3.9069 train_time:341260ms step_avg:179.80ms\nstep:1909/5000 train_loss:3.6270 train_time:341440ms step_avg:179.80ms\nstep:1910/5000 train_loss:3.6871 train_time:341620ms step_avg:179.80ms\nstep:1911/5000 train_loss:3.7232 train_time:341799ms step_avg:179.80ms\nstep:1912/5000 train_loss:3.8823 train_time:341978ms step_avg:179.80ms\nstep:1913/5000 train_loss:3.8057 train_time:342156ms step_avg:179.80ms\nstep:1914/5000 train_loss:3.9036 train_time:342335ms step_avg:179.80ms\nstep:1915/5000 train_loss:3.8057 train_time:342513ms step_avg:179.80ms\nstep:1916/5000 train_loss:3.8306 train_time:342692ms step_avg:179.80ms\nstep:1917/5000 train_loss:3.8594 train_time:342871ms step_avg:179.80ms\nstep:1918/5000 train_loss:3.8623 train_time:343050ms step_avg:179.80ms\nstep:1919/5000 train_loss:3.7658 train_time:343229ms step_avg:179.80ms\nstep:1920/5000 train_loss:3.7641 train_time:343409ms step_avg:179.80ms\nstep:1921/5000 train_loss:3.8971 train_time:343590ms step_avg:179.80ms\nstep:1922/5000 train_loss:4.0104 train_time:343768ms step_avg:179.80ms\nstep:1923/5000 train_loss:3.8053 train_time:343948ms step_avg:179.79ms\nstep:1924/5000 train_loss:3.8127 train_time:344128ms step_avg:179.79ms\nstep:1925/5000 train_loss:4.0141 train_time:344308ms step_avg:179.80ms\nstep:1926/5000 train_loss:3.8234 train_time:344487ms step_avg:179.79ms\nstep:1927/5000 train_loss:3.7585 train_time:344666ms step_avg:179.79ms\nstep:1928/5000 train_loss:3.9425 train_time:344846ms step_avg:179.79ms\nstep:1929/5000 train_loss:3.7621 train_time:345026ms step_avg:179.79ms\nstep:1930/5000 train_loss:3.6164 train_time:345205ms step_avg:179.79ms\nstep:1931/5000 train_loss:3.7307 train_time:345384ms step_avg:179.79ms\nstep:1932/5000 train_loss:3.8437 train_time:345564ms step_avg:179.79ms\nstep:1933/5000 train_loss:3.6641 train_time:345744ms step_avg:179.79ms\nstep:1934/5000 train_loss:3.9004 train_time:345924ms step_avg:179.79ms\nstep:1935/5000 train_loss:4.0077 train_time:346104ms step_avg:179.79ms\nstep:1936/5000 train_loss:3.9131 train_time:346289ms step_avg:179.80ms\nstep:1937/5000 train_loss:3.7889 train_time:346467ms step_avg:179.80ms\nstep:1938/5000 train_loss:3.7650 train_time:346646ms step_avg:179.80ms\nstep:1939/5000 train_loss:3.8326 train_time:346825ms step_avg:179.80ms\nstep:1940/5000 train_loss:3.8565 train_time:347006ms step_avg:179.80ms\nstep:1941/5000 train_loss:3.8989 train_time:347186ms step_avg:179.80ms\nstep:1942/5000 train_loss:3.7632 train_time:347366ms step_avg:179.80ms\nstep:1943/5000 train_loss:3.7569 train_time:347545ms step_avg:179.80ms\nstep:1944/5000 train_loss:3.6917 train_time:347724ms step_avg:179.80ms\nstep:1945/5000 train_loss:3.8704 train_time:347904ms step_avg:179.80ms\nstep:1946/5000 train_loss:3.6929 train_time:348083ms step_avg:179.79ms\nstep:1947/5000 train_loss:3.8452 train_time:348262ms step_avg:179.79ms\nstep:1948/5000 train_loss:3.8303 train_time:348442ms step_avg:179.79ms\nstep:1949/5000 train_loss:3.7811 train_time:348622ms step_avg:179.79ms\nstep:1950/5000 train_loss:3.7677 train_time:348802ms step_avg:179.79ms\nstep:1951/5000 train_loss:3.7686 train_time:348981ms step_avg:179.79ms\nstep:1952/5000 train_loss:3.8342 train_time:349161ms step_avg:179.79ms\nstep:1953/5000 train_loss:3.7449 train_time:349341ms step_avg:179.79ms\nstep:1954/5000 train_loss:3.9714 train_time:349521ms step_avg:179.79ms\nstep:1955/5000 train_loss:3.7717 train_time:349700ms step_avg:179.79ms\nstep:1956/5000 train_loss:3.8330 train_time:349880ms step_avg:179.79ms\nstep:1957/5000 train_loss:3.7163 train_time:350059ms step_avg:179.79ms\nstep:1958/5000 train_loss:3.8159 train_time:350238ms step_avg:179.79ms\nstep:1959/5000 train_loss:3.8730 train_time:350418ms step_avg:179.79ms\nstep:1960/5000 train_loss:4.0493 train_time:350598ms step_avg:179.79ms\nstep:1961/5000 train_loss:3.7538 train_time:350775ms step_avg:179.79ms\nstep:1962/5000 train_loss:3.6945 train_time:350954ms step_avg:179.79ms\nstep:1963/5000 train_loss:3.6673 train_time:351132ms step_avg:179.79ms\nstep:1964/5000 train_loss:3.7917 train_time:351311ms step_avg:179.79ms\nstep:1965/5000 train_loss:3.8822 train_time:351491ms step_avg:179.79ms\nstep:1966/5000 train_loss:3.7142 train_time:351671ms step_avg:179.79ms\nstep:1967/5000 train_loss:4.0456 train_time:351850ms step_avg:179.79ms\nstep:1968/5000 train_loss:3.7303 train_time:352028ms step_avg:179.79ms\nstep:1969/5000 train_loss:3.7445 train_time:352209ms step_avg:179.79ms\nstep:1970/5000 train_loss:3.8318 train_time:352389ms step_avg:179.79ms\nstep:1971/5000 train_loss:3.8855 train_time:352568ms step_avg:179.79ms\nstep:1972/5000 train_loss:3.8688 train_time:352749ms step_avg:179.79ms\nstep:1973/5000 train_loss:3.9083 train_time:352928ms step_avg:179.79ms\nstep:1974/5000 train_loss:3.6719 train_time:353108ms step_avg:179.79ms\nstep:1975/5000 train_loss:3.7868 train_time:353287ms step_avg:179.79ms\nstep:1976/5000 train_loss:3.6392 train_time:353469ms step_avg:179.79ms\nstep:1977/5000 train_loss:3.8960 train_time:353648ms step_avg:179.79ms\nstep:1978/5000 train_loss:3.8074 train_time:353828ms step_avg:179.79ms\nstep:1979/5000 train_loss:3.7783 train_time:354008ms step_avg:179.79ms\nstep:1980/5000 train_loss:3.8188 train_time:354190ms step_avg:179.79ms\nstep:1981/5000 train_loss:3.8269 train_time:354369ms step_avg:179.79ms\nstep:1982/5000 train_loss:3.7516 train_time:354550ms step_avg:179.79ms\nstep:1983/5000 train_loss:3.8195 train_time:354730ms step_avg:179.79ms\nstep:1984/5000 train_loss:3.9895 train_time:354909ms step_avg:179.79ms\nstep:1985/5000 train_loss:3.8819 train_time:355087ms step_avg:179.79ms\nstep:1986/5000 train_loss:3.6378 train_time:355266ms step_avg:179.79ms\nstep:1987/5000 train_loss:3.7571 train_time:355445ms step_avg:179.79ms\nstep:1988/5000 train_loss:3.8273 train_time:355625ms step_avg:179.79ms\nstep:1989/5000 train_loss:3.8087 train_time:355807ms step_avg:179.79ms\nstep:1990/5000 train_loss:3.8347 train_time:355988ms step_avg:179.79ms\nstep:1991/5000 train_loss:3.8226 train_time:356167ms step_avg:179.79ms\nstep:1992/5000 train_loss:3.8047 train_time:356347ms step_avg:179.79ms\nstep:1993/5000 train_loss:4.2391 train_time:356528ms step_avg:179.79ms\nstep:1994/5000 train_loss:3.8978 train_time:356708ms step_avg:179.79ms\nstep:1995/5000 train_loss:3.7307 train_time:356887ms step_avg:179.79ms\nstep:1996/5000 train_loss:3.7984 train_time:357066ms step_avg:179.79ms\nstep:1997/5000 train_loss:3.7226 train_time:357245ms step_avg:179.79ms\nstep:1998/5000 train_loss:3.9300 train_time:357424ms step_avg:179.79ms\nstep:1999/5000 train_loss:3.9352 train_time:357604ms step_avg:179.79ms\nstep:2000/5000 train_loss:3.8984 train_time:357784ms step_avg:179.79ms\nstep:2001/5000 train_loss:3.8455 train_time:357967ms step_avg:179.79ms\nstep:2002/5000 train_loss:3.8467 train_time:358146ms step_avg:179.79ms\nstep:2003/5000 train_loss:3.8654 train_time:358325ms step_avg:179.79ms\nstep:2004/5000 train_loss:3.7914 train_time:358506ms step_avg:179.79ms\nstep:2005/5000 train_loss:3.7946 train_time:358684ms step_avg:179.79ms\nstep:2006/5000 train_loss:3.8996 train_time:358864ms step_avg:179.79ms\nstep:2007/5000 train_loss:3.6723 train_time:359043ms step_avg:179.79ms\nstep:2008/5000 train_loss:3.7271 train_time:359223ms step_avg:179.79ms\nstep:2009/5000 train_loss:4.3432 train_time:359403ms step_avg:179.79ms\nstep:2010/5000 train_loss:3.8429 train_time:359583ms step_avg:179.79ms\nstep:2011/5000 train_loss:3.8454 train_time:359763ms step_avg:179.79ms\nstep:2012/5000 train_loss:3.8466 train_time:359943ms step_avg:179.79ms\nstep:2013/5000 train_loss:3.8838 train_time:360127ms step_avg:179.79ms\nstep:2014/5000 train_loss:3.8797 train_time:360310ms step_avg:179.80ms\nstep:2015/5000 train_loss:3.7457 train_time:360491ms step_avg:179.80ms\nstep:2016/5000 train_loss:3.7054 train_time:360670ms step_avg:179.80ms\nstep:2017/5000 train_loss:3.7392 train_time:360848ms step_avg:179.79ms\nstep:2018/5000 train_loss:3.8201 train_time:361028ms step_avg:179.79ms\nstep:2019/5000 train_loss:3.8836 train_time:361209ms step_avg:179.80ms\nstep:2020/5000 train_loss:4.1126 train_time:361390ms step_avg:179.80ms\nstep:2021/5000 train_loss:3.6824 train_time:361570ms step_avg:179.80ms\nstep:2022/5000 train_loss:3.9197 train_time:361749ms step_avg:179.80ms\nstep:2023/5000 train_loss:3.6976 train_time:361928ms step_avg:179.80ms\nstep:2024/5000 train_loss:3.7782 train_time:362109ms step_avg:179.80ms\nstep:2025/5000 train_loss:3.7538 train_time:362289ms step_avg:179.80ms\nstep:2026/5000 train_loss:3.7435 train_time:362469ms step_avg:179.80ms\nstep:2027/5000 train_loss:3.7733 train_time:362649ms step_avg:179.80ms\nstep:2028/5000 train_loss:3.8635 train_time:362827ms step_avg:179.80ms\nstep:2029/5000 train_loss:3.6744 train_time:363007ms step_avg:179.80ms\nstep:2030/5000 train_loss:3.8647 train_time:363188ms step_avg:179.80ms\nstep:2031/5000 train_loss:3.8602 train_time:363366ms step_avg:179.80ms\nstep:2032/5000 train_loss:4.0482 train_time:363546ms step_avg:179.80ms\nstep:2033/5000 train_loss:3.9439 train_time:363728ms step_avg:179.80ms\nstep:2034/5000 train_loss:3.9313 train_time:363909ms step_avg:179.80ms\nstep:2035/5000 train_loss:3.7333 train_time:364088ms step_avg:179.80ms\nstep:2036/5000 train_loss:3.7439 train_time:364268ms step_avg:179.80ms\nstep:2037/5000 train_loss:3.7809 train_time:364448ms step_avg:179.80ms\nstep:2038/5000 train_loss:3.8706 train_time:364630ms step_avg:179.80ms\nstep:2039/5000 train_loss:3.9363 train_time:364810ms step_avg:179.80ms\nstep:2040/5000 train_loss:3.8287 train_time:364990ms step_avg:179.80ms\nstep:2041/5000 train_loss:3.8183 train_time:365169ms step_avg:179.80ms\nstep:2042/5000 train_loss:3.6846 train_time:365349ms step_avg:179.80ms\nstep:2043/5000 train_loss:3.7873 train_time:365528ms step_avg:179.80ms\nstep:2044/5000 train_loss:3.8450 train_time:365711ms step_avg:179.80ms\nstep:2045/5000 train_loss:3.7738 train_time:365892ms step_avg:179.80ms\nstep:2046/5000 train_loss:3.8778 train_time:366071ms step_avg:179.80ms\nstep:2047/5000 train_loss:3.7946 train_time:366249ms step_avg:179.80ms\nstep:2048/5000 train_loss:3.6748 train_time:366428ms step_avg:179.80ms\nstep:2049/5000 train_loss:3.7598 train_time:366608ms step_avg:179.80ms\nstep:2050/5000 train_loss:3.8140 train_time:366789ms step_avg:179.80ms\nstep:2051/5000 train_loss:3.7708 train_time:366969ms step_avg:179.80ms\nstep:2052/5000 train_loss:3.6779 train_time:367149ms step_avg:179.80ms\nstep:2053/5000 train_loss:3.9001 train_time:367328ms step_avg:179.80ms\nstep:2054/5000 train_loss:3.7859 train_time:367508ms step_avg:179.80ms\nstep:2055/5000 train_loss:3.7633 train_time:367687ms step_avg:179.80ms\nstep:2056/5000 train_loss:3.8799 train_time:367866ms step_avg:179.80ms\nstep:2057/5000 train_loss:3.7747 train_time:368049ms step_avg:179.80ms\nstep:2058/5000 train_loss:3.8752 train_time:368227ms step_avg:179.80ms\nstep:2059/5000 train_loss:3.8084 train_time:368407ms step_avg:179.80ms\nstep:2060/5000 train_loss:4.0291 train_time:368587ms step_avg:179.80ms\nstep:2061/5000 train_loss:3.9390 train_time:368767ms step_avg:179.80ms\nstep:2062/5000 train_loss:3.8429 train_time:368945ms step_avg:179.80ms\nstep:2063/5000 train_loss:3.7597 train_time:369125ms step_avg:179.80ms\nstep:2064/5000 train_loss:3.8830 train_time:369305ms step_avg:179.80ms\nstep:2065/5000 train_loss:3.8706 train_time:369485ms step_avg:179.80ms\nstep:2066/5000 train_loss:4.2012 train_time:369665ms step_avg:179.80ms\nstep:2067/5000 train_loss:4.0048 train_time:369844ms step_avg:179.80ms\nstep:2068/5000 train_loss:3.7273 train_time:370024ms step_avg:179.80ms\nstep:2069/5000 train_loss:3.8249 train_time:370205ms step_avg:179.80ms\nstep:2070/5000 train_loss:3.7934 train_time:370385ms step_avg:179.80ms\nstep:2071/5000 train_loss:3.8111 train_time:370567ms step_avg:179.80ms\nstep:2072/5000 train_loss:3.8924 train_time:370746ms step_avg:179.80ms\nstep:2073/5000 train_loss:3.9098 train_time:370925ms step_avg:179.80ms\nstep:2074/5000 train_loss:3.8992 train_time:371106ms step_avg:179.80ms\nstep:2075/5000 train_loss:3.6776 train_time:371287ms step_avg:179.80ms\nstep:2076/5000 train_loss:3.6366 train_time:371466ms step_avg:179.80ms\nstep:2077/5000 train_loss:3.9674 train_time:371645ms step_avg:179.80ms\nstep:2078/5000 train_loss:3.7880 train_time:371826ms step_avg:179.80ms\nstep:2079/5000 train_loss:3.7725 train_time:372008ms step_avg:179.80ms\nstep:2080/5000 train_loss:3.8864 train_time:372188ms step_avg:179.80ms\nstep:2081/5000 train_loss:3.7811 train_time:372366ms step_avg:179.80ms\nstep:2082/5000 train_loss:3.8101 train_time:372545ms step_avg:179.80ms\nstep:2083/5000 train_loss:3.7974 train_time:372725ms step_avg:179.80ms\nstep:2084/5000 train_loss:3.7629 train_time:372906ms step_avg:179.80ms\nstep:2085/5000 train_loss:3.7809 train_time:373086ms step_avg:179.80ms\nstep:2086/5000 train_loss:3.7542 train_time:373264ms step_avg:179.80ms\nstep:2087/5000 train_loss:3.8184 train_time:373442ms step_avg:179.80ms\nstep:2088/5000 train_loss:3.6510 train_time:373623ms step_avg:179.80ms\nstep:2089/5000 train_loss:3.8295 train_time:373803ms step_avg:179.80ms\nstep:2090/5000 train_loss:3.7583 train_time:373985ms step_avg:179.80ms\nstep:2091/5000 train_loss:3.8118 train_time:374165ms step_avg:179.80ms\nstep:2092/5000 train_loss:3.9048 train_time:374344ms step_avg:179.80ms\nstep:2093/5000 train_loss:3.9474 train_time:374521ms step_avg:179.80ms\nstep:2094/5000 train_loss:4.3992 train_time:374701ms step_avg:179.80ms\nstep:2095/5000 train_loss:3.7102 train_time:374881ms step_avg:179.80ms\nstep:2096/5000 train_loss:3.7101 train_time:375061ms step_avg:179.80ms\nstep:2097/5000 train_loss:3.5582 train_time:375241ms step_avg:179.80ms\nstep:2098/5000 train_loss:3.8483 train_time:375421ms step_avg:179.80ms\nstep:2099/5000 train_loss:3.7841 train_time:375600ms step_avg:179.80ms\nstep:2100/5000 train_loss:3.7793 train_time:375779ms step_avg:179.80ms\nstep:2101/5000 train_loss:3.7665 train_time:375960ms step_avg:179.80ms\nstep:2102/5000 train_loss:3.7865 train_time:376139ms step_avg:179.80ms\nstep:2103/5000 train_loss:3.7492 train_time:376319ms step_avg:179.80ms\nstep:2104/5000 train_loss:3.8007 train_time:376497ms step_avg:179.80ms\nstep:2105/5000 train_loss:3.6208 train_time:376675ms step_avg:179.80ms\nstep:2106/5000 train_loss:3.8026 train_time:376854ms step_avg:179.80ms\nstep:2107/5000 train_loss:3.9671 train_time:377032ms step_avg:179.80ms\nstep:2108/5000 train_loss:4.0621 train_time:377212ms step_avg:179.80ms\nstep:2109/5000 train_loss:3.7067 train_time:377390ms step_avg:179.80ms\nstep:2110/5000 train_loss:3.7616 train_time:377569ms step_avg:179.79ms\nstep:2111/5000 train_loss:3.7575 train_time:377748ms step_avg:179.79ms\nstep:2112/5000 train_loss:3.7905 train_time:377927ms step_avg:179.79ms\nstep:2113/5000 train_loss:3.8182 train_time:378107ms step_avg:179.79ms\nstep:2114/5000 train_loss:3.7325 train_time:378287ms step_avg:179.79ms\nstep:2115/5000 train_loss:3.8814 train_time:378468ms step_avg:179.79ms\nstep:2116/5000 train_loss:3.7639 train_time:378647ms step_avg:179.79ms\nstep:2117/5000 train_loss:3.7872 train_time:378827ms step_avg:179.79ms\nstep:2118/5000 train_loss:3.9412 train_time:379006ms step_avg:179.79ms\nstep:2119/5000 train_loss:3.9193 train_time:379187ms step_avg:179.79ms\nstep:2120/5000 train_loss:3.7676 train_time:379367ms step_avg:179.79ms\nstep:2121/5000 train_loss:3.8397 train_time:379547ms step_avg:179.79ms\nstep:2122/5000 train_loss:3.7471 train_time:379727ms step_avg:179.79ms\nstep:2123/5000 train_loss:3.7896 train_time:379908ms step_avg:179.80ms\nstep:2124/5000 train_loss:3.8815 train_time:380089ms step_avg:179.80ms\nstep:2125/5000 train_loss:3.6862 train_time:380269ms step_avg:179.80ms\nstep:2126/5000 train_loss:4.0199 train_time:380448ms step_avg:179.80ms\nstep:2127/5000 train_loss:3.6577 train_time:380627ms step_avg:179.80ms\nstep:2128/5000 train_loss:3.8356 train_time:380808ms step_avg:179.80ms\nstep:2129/5000 train_loss:3.5971 train_time:380986ms step_avg:179.80ms\nstep:2130/5000 train_loss:3.8329 train_time:381166ms step_avg:179.80ms\nstep:2131/5000 train_loss:3.9241 train_time:381349ms step_avg:179.80ms\nstep:2132/5000 train_loss:3.8254 train_time:381527ms step_avg:179.80ms\nstep:2133/5000 train_loss:3.7206 train_time:381708ms step_avg:179.80ms\nstep:2134/5000 train_loss:3.7626 train_time:381888ms step_avg:179.80ms\nstep:2135/5000 train_loss:3.9156 train_time:382067ms step_avg:179.80ms\nstep:2136/5000 train_loss:3.9158 train_time:382246ms step_avg:179.80ms\nstep:2137/5000 train_loss:3.7434 train_time:382425ms step_avg:179.80ms\nstep:2138/5000 train_loss:3.7885 train_time:382604ms step_avg:179.80ms\nstep:2139/5000 train_loss:3.8469 train_time:382784ms step_avg:179.80ms\nstep:2140/5000 train_loss:3.8661 train_time:382963ms step_avg:179.79ms\nstep:2141/5000 train_loss:3.8163 train_time:383143ms step_avg:179.79ms\nstep:2142/5000 train_loss:3.7238 train_time:383323ms step_avg:179.80ms\nstep:2143/5000 train_loss:3.9636 train_time:383503ms step_avg:179.80ms\nstep:2144/5000 train_loss:3.7518 train_time:383682ms step_avg:179.79ms\nstep:2145/5000 train_loss:4.0208 train_time:383862ms step_avg:179.79ms\nstep:2146/5000 train_loss:3.8547 train_time:384040ms step_avg:179.79ms\nstep:2147/5000 train_loss:3.6596 train_time:384219ms step_avg:179.79ms\nstep:2148/5000 train_loss:3.8021 train_time:384398ms step_avg:179.79ms\nstep:2149/5000 train_loss:3.6620 train_time:384577ms step_avg:179.79ms\nstep:2150/5000 train_loss:3.8429 train_time:384756ms step_avg:179.79ms\nstep:2151/5000 train_loss:3.4992 train_time:384935ms step_avg:179.79ms\nstep:2152/5000 train_loss:3.7156 train_time:385113ms step_avg:179.79ms\nstep:2153/5000 train_loss:3.8902 train_time:385292ms step_avg:179.79ms\nstep:2154/5000 train_loss:3.7357 train_time:385470ms step_avg:179.79ms\nstep:2155/5000 train_loss:3.6370 train_time:385650ms step_avg:179.79ms\nstep:2156/5000 train_loss:3.7325 train_time:385830ms step_avg:179.79ms\nstep:2157/5000 train_loss:3.8677 train_time:386009ms step_avg:179.79ms\nstep:2158/5000 train_loss:4.2387 train_time:386189ms step_avg:179.79ms\nstep:2159/5000 train_loss:3.7386 train_time:386368ms step_avg:179.79ms\nstep:2160/5000 train_loss:3.7940 train_time:386548ms step_avg:179.79ms\nstep:2161/5000 train_loss:3.9164 train_time:386728ms step_avg:179.79ms\nstep:2162/5000 train_loss:3.7863 train_time:386907ms step_avg:179.79ms\nstep:2163/5000 train_loss:3.8905 train_time:387086ms step_avg:179.79ms\nstep:2164/5000 train_loss:3.7410 train_time:387265ms step_avg:179.79ms\nstep:2165/5000 train_loss:3.5856 train_time:387444ms step_avg:179.79ms\nstep:2166/5000 train_loss:3.8895 train_time:387626ms step_avg:179.79ms\nstep:2167/5000 train_loss:4.2871 train_time:387806ms step_avg:179.79ms\nstep:2168/5000 train_loss:3.7545 train_time:387986ms step_avg:179.79ms\nstep:2169/5000 train_loss:3.8077 train_time:388166ms step_avg:179.79ms\nstep:2170/5000 train_loss:3.7171 train_time:388345ms step_avg:179.79ms\nstep:2171/5000 train_loss:3.6627 train_time:388525ms step_avg:179.79ms\nstep:2172/5000 train_loss:3.8196 train_time:388705ms step_avg:179.79ms\nstep:2173/5000 train_loss:3.9399 train_time:388885ms step_avg:179.79ms\nstep:2174/5000 train_loss:3.6953 train_time:389064ms step_avg:179.79ms\nstep:2175/5000 train_loss:3.8919 train_time:389244ms step_avg:179.79ms\nstep:2176/5000 train_loss:3.8021 train_time:389425ms step_avg:179.79ms\nstep:2177/5000 train_loss:3.8628 train_time:389604ms step_avg:179.79ms\nstep:2178/5000 train_loss:3.7824 train_time:389784ms step_avg:179.79ms\nstep:2179/5000 train_loss:3.7505 train_time:389966ms step_avg:179.79ms\nstep:2180/5000 train_loss:3.8416 train_time:390146ms step_avg:179.79ms\nstep:2181/5000 train_loss:3.9510 train_time:390326ms step_avg:179.79ms\nstep:2182/5000 train_loss:3.6970 train_time:390507ms step_avg:179.79ms\nstep:2183/5000 train_loss:3.6855 train_time:390686ms step_avg:179.79ms\nstep:2184/5000 train_loss:3.8711 train_time:390866ms step_avg:179.79ms\nstep:2185/5000 train_loss:3.7976 train_time:391045ms step_avg:179.79ms\nstep:2186/5000 train_loss:3.7549 train_time:391225ms step_avg:179.79ms\nstep:2187/5000 train_loss:3.7431 train_time:391404ms step_avg:179.79ms\nstep:2188/5000 train_loss:3.6990 train_time:391583ms step_avg:179.79ms\nstep:2189/5000 train_loss:3.7796 train_time:391762ms step_avg:179.79ms\nstep:2190/5000 train_loss:3.7546 train_time:391942ms step_avg:179.79ms\nstep:2191/5000 train_loss:3.9045 train_time:392122ms step_avg:179.79ms\nstep:2192/5000 train_loss:3.9628 train_time:392303ms step_avg:179.79ms\nstep:2193/5000 train_loss:3.6649 train_time:392482ms step_avg:179.79ms\nstep:2194/5000 train_loss:3.7341 train_time:392660ms step_avg:179.79ms\nstep:2195/5000 train_loss:3.6285 train_time:392839ms step_avg:179.79ms\nstep:2196/5000 train_loss:3.4324 train_time:393019ms step_avg:179.79ms\nstep:2197/5000 train_loss:3.7626 train_time:393197ms step_avg:179.79ms\nstep:2198/5000 train_loss:3.8224 train_time:393376ms step_avg:179.79ms\nstep:2199/5000 train_loss:3.6522 train_time:393557ms step_avg:179.79ms\nstep:2200/5000 train_loss:3.8704 train_time:393745ms step_avg:179.79ms\nstep:2201/5000 train_loss:3.6539 train_time:393920ms step_avg:179.79ms\nstep:2202/5000 train_loss:3.7214 train_time:394098ms step_avg:179.79ms\nstep:2203/5000 train_loss:3.6937 train_time:394277ms step_avg:179.79ms\nstep:2204/5000 train_loss:3.8309 train_time:394456ms step_avg:179.79ms\nstep:2205/5000 train_loss:3.8078 train_time:394635ms step_avg:179.79ms\nstep:2206/5000 train_loss:3.7377 train_time:394814ms step_avg:179.79ms\nstep:2207/5000 train_loss:3.6944 train_time:394992ms step_avg:179.79ms\nstep:2208/5000 train_loss:3.7608 train_time:395170ms step_avg:179.79ms\nstep:2209/5000 train_loss:3.7385 train_time:395350ms step_avg:179.79ms\nstep:2210/5000 train_loss:4.0655 train_time:395529ms step_avg:179.79ms\nstep:2211/5000 train_loss:3.8167 train_time:395711ms step_avg:179.79ms\nstep:2212/5000 train_loss:3.8839 train_time:395890ms step_avg:179.79ms\nstep:2213/5000 train_loss:3.8220 train_time:396070ms step_avg:179.79ms\nstep:2214/5000 train_loss:3.8715 train_time:396250ms step_avg:179.79ms\nstep:2215/5000 train_loss:3.8548 train_time:396429ms step_avg:179.79ms\nstep:2216/5000 train_loss:3.7860 train_time:396609ms step_avg:179.79ms\nstep:2217/5000 train_loss:3.7141 train_time:396788ms step_avg:179.79ms\nstep:2218/5000 train_loss:3.8270 train_time:396968ms step_avg:179.79ms\nstep:2219/5000 train_loss:3.7635 train_time:397147ms step_avg:179.79ms\nstep:2220/5000 train_loss:3.7824 train_time:397327ms step_avg:179.79ms\nstep:2221/5000 train_loss:3.6044 train_time:397506ms step_avg:179.79ms\nstep:2222/5000 train_loss:3.7610 train_time:397686ms step_avg:179.79ms\nstep:2223/5000 train_loss:3.8111 train_time:397867ms step_avg:179.79ms\nstep:2224/5000 train_loss:3.9052 train_time:398048ms step_avg:179.79ms\nstep:2225/5000 train_loss:3.6982 train_time:398228ms step_avg:179.79ms\nstep:2226/5000 train_loss:3.8036 train_time:398406ms step_avg:179.79ms\nstep:2227/5000 train_loss:3.8054 train_time:398585ms step_avg:179.79ms\nstep:2228/5000 train_loss:3.8144 train_time:398765ms step_avg:179.79ms\nstep:2229/5000 train_loss:3.7074 train_time:398946ms step_avg:179.79ms\nstep:2230/5000 train_loss:3.4703 train_time:399128ms step_avg:179.79ms\nstep:2231/5000 train_loss:3.9088 train_time:399307ms step_avg:179.79ms\nstep:2232/5000 train_loss:3.7380 train_time:399486ms step_avg:179.79ms\nstep:2233/5000 train_loss:3.6756 train_time:399664ms step_avg:179.79ms\nstep:2234/5000 train_loss:3.7223 train_time:399843ms step_avg:179.79ms\nstep:2235/5000 train_loss:3.7396 train_time:400023ms step_avg:179.79ms\nstep:2236/5000 train_loss:3.8218 train_time:400203ms step_avg:179.79ms\nstep:2237/5000 train_loss:3.8932 train_time:400383ms step_avg:179.79ms\nstep:2238/5000 train_loss:3.7714 train_time:400564ms step_avg:179.79ms\nstep:2239/5000 train_loss:3.6996 train_time:400746ms step_avg:179.79ms\nstep:2240/5000 train_loss:3.7194 train_time:400927ms step_avg:179.79ms\nstep:2241/5000 train_loss:3.9172 train_time:401107ms step_avg:179.79ms\nstep:2242/5000 train_loss:3.8062 train_time:401287ms step_avg:179.79ms\nstep:2243/5000 train_loss:3.9909 train_time:401465ms step_avg:179.79ms\nstep:2244/5000 train_loss:3.7751 train_time:401644ms step_avg:179.79ms\nstep:2245/5000 train_loss:3.6983 train_time:401824ms step_avg:179.79ms\nstep:2246/5000 train_loss:3.7161 train_time:402006ms step_avg:179.79ms\nstep:2247/5000 train_loss:3.7727 train_time:402186ms step_avg:179.79ms\nstep:2248/5000 train_loss:3.7766 train_time:402366ms step_avg:179.79ms\nstep:2249/5000 train_loss:3.7647 train_time:402547ms step_avg:179.79ms\nstep:2250/5000 train_loss:3.6606 train_time:402727ms step_avg:179.79ms\nstep:2251/5000 train_loss:3.6759 train_time:402909ms step_avg:179.79ms\nstep:2252/5000 train_loss:3.5952 train_time:403088ms step_avg:179.79ms\nstep:2253/5000 train_loss:3.8613 train_time:403268ms step_avg:179.79ms\nstep:2254/5000 train_loss:3.8711 train_time:403448ms step_avg:179.79ms\nstep:2255/5000 train_loss:3.7345 train_time:403626ms step_avg:179.79ms\nstep:2256/5000 train_loss:3.8502 train_time:403806ms step_avg:179.79ms\nstep:2257/5000 train_loss:3.6912 train_time:403987ms step_avg:179.79ms\nstep:2258/5000 train_loss:3.6821 train_time:404167ms step_avg:179.79ms\nstep:2259/5000 train_loss:3.8249 train_time:404346ms step_avg:179.79ms\nstep:2260/5000 train_loss:3.7266 train_time:404527ms step_avg:179.79ms\nstep:2261/5000 train_loss:3.9263 train_time:404706ms step_avg:179.79ms\nstep:2262/5000 train_loss:3.7977 train_time:404885ms step_avg:179.79ms\nstep:2263/5000 train_loss:3.7330 train_time:405065ms step_avg:179.79ms\nstep:2264/5000 train_loss:3.7587 train_time:405246ms step_avg:179.79ms\nstep:2265/5000 train_loss:3.9218 train_time:405424ms step_avg:179.79ms\nstep:2266/5000 train_loss:3.7347 train_time:405604ms step_avg:179.79ms\nstep:2267/5000 train_loss:3.6942 train_time:405784ms step_avg:179.79ms\nstep:2268/5000 train_loss:3.8286 train_time:405963ms step_avg:179.79ms\nstep:2269/5000 train_loss:3.7375 train_time:406143ms step_avg:179.79ms\nstep:2270/5000 train_loss:3.7068 train_time:406322ms step_avg:179.79ms\nstep:2271/5000 train_loss:3.6835 train_time:406500ms step_avg:179.79ms\nstep:2272/5000 train_loss:3.7927 train_time:406679ms step_avg:179.79ms\nstep:2273/5000 train_loss:3.7994 train_time:406859ms step_avg:179.79ms\nstep:2274/5000 train_loss:3.7986 train_time:407038ms step_avg:179.79ms\nstep:2275/5000 train_loss:3.8249 train_time:407217ms step_avg:179.79ms\nstep:2276/5000 train_loss:4.0979 train_time:407396ms step_avg:179.79ms\nstep:2277/5000 train_loss:3.8089 train_time:407576ms step_avg:179.79ms\nstep:2278/5000 train_loss:3.7582 train_time:407755ms step_avg:179.79ms\nstep:2279/5000 train_loss:3.8884 train_time:407934ms step_avg:179.79ms\nstep:2280/5000 train_loss:3.7991 train_time:408114ms step_avg:179.79ms\nstep:2281/5000 train_loss:3.6213 train_time:408293ms step_avg:179.79ms\nstep:2282/5000 train_loss:3.8030 train_time:408471ms step_avg:179.78ms\nstep:2283/5000 train_loss:3.8503 train_time:408650ms step_avg:179.78ms\nstep:2284/5000 train_loss:3.7725 train_time:408829ms step_avg:179.78ms\nstep:2285/5000 train_loss:3.7517 train_time:409010ms step_avg:179.78ms\nstep:2286/5000 train_loss:3.6949 train_time:409409ms step_avg:179.88ms\nstep:2287/5000 train_loss:3.7658 train_time:409586ms step_avg:179.88ms\nstep:2288/5000 train_loss:3.8780 train_time:409764ms step_avg:179.88ms\nstep:2289/5000 train_loss:3.7106 train_time:409943ms step_avg:179.88ms\nstep:2290/5000 train_loss:3.6387 train_time:410121ms step_avg:179.88ms\nstep:2291/5000 train_loss:3.7875 train_time:410305ms step_avg:179.88ms\nstep:2292/5000 train_loss:3.8360 train_time:410489ms step_avg:179.88ms\nstep:2293/5000 train_loss:3.7699 train_time:410668ms step_avg:179.88ms\nstep:2294/5000 train_loss:3.6524 train_time:410846ms step_avg:179.88ms\nstep:2295/5000 train_loss:3.7624 train_time:411024ms step_avg:179.88ms\nstep:2296/5000 train_loss:3.8530 train_time:411204ms step_avg:179.88ms\nstep:2297/5000 train_loss:3.9139 train_time:411384ms step_avg:179.88ms\nstep:2298/5000 train_loss:4.1188 train_time:411568ms step_avg:179.88ms\nstep:2299/5000 train_loss:3.7806 train_time:411749ms step_avg:179.88ms\nstep:2300/5000 train_loss:3.8908 train_time:411927ms step_avg:179.88ms\nstep:2301/5000 train_loss:3.7577 train_time:412107ms step_avg:179.88ms\nstep:2302/5000 train_loss:3.6705 train_time:412287ms step_avg:179.88ms\nstep:2303/5000 train_loss:3.6608 train_time:412467ms step_avg:179.88ms\nstep:2304/5000 train_loss:3.8510 train_time:412647ms step_avg:179.88ms\nstep:2305/5000 train_loss:3.8237 train_time:412828ms step_avg:179.88ms\nstep:2306/5000 train_loss:3.7451 train_time:413007ms step_avg:179.88ms\nstep:2307/5000 train_loss:3.8259 train_time:413189ms step_avg:179.88ms\nstep:2308/5000 train_loss:3.8721 train_time:413367ms step_avg:179.88ms\nstep:2309/5000 train_loss:3.7030 train_time:413546ms step_avg:179.88ms\nstep:2310/5000 train_loss:3.8992 train_time:413724ms step_avg:179.88ms\nstep:2311/5000 train_loss:3.7592 train_time:413904ms step_avg:179.88ms\nstep:2312/5000 train_loss:3.7962 train_time:414082ms step_avg:179.88ms\nstep:2313/5000 train_loss:3.8788 train_time:414261ms step_avg:179.88ms\nstep:2314/5000 train_loss:3.6878 train_time:414439ms step_avg:179.88ms\nstep:2315/5000 train_loss:3.6810 train_time:414619ms step_avg:179.88ms\nstep:2316/5000 train_loss:3.7946 train_time:414799ms step_avg:179.88ms\nstep:2317/5000 train_loss:3.8591 train_time:414978ms step_avg:179.88ms\nstep:2318/5000 train_loss:3.7287 train_time:415157ms step_avg:179.88ms\nstep:2319/5000 train_loss:3.9894 train_time:415336ms step_avg:179.88ms\nstep:2320/5000 train_loss:3.7700 train_time:415515ms step_avg:179.88ms\nstep:2321/5000 train_loss:3.8549 train_time:415693ms step_avg:179.88ms\nstep:2322/5000 train_loss:3.8411 train_time:415871ms step_avg:179.87ms\nstep:2323/5000 train_loss:3.7281 train_time:416050ms step_avg:179.87ms\nstep:2324/5000 train_loss:3.7096 train_time:416231ms step_avg:179.88ms\nstep:2325/5000 train_loss:3.8387 train_time:416411ms step_avg:179.87ms\nstep:2326/5000 train_loss:3.8482 train_time:416590ms step_avg:179.87ms\nstep:2327/5000 train_loss:3.7267 train_time:416770ms step_avg:179.87ms\nstep:2328/5000 train_loss:3.7420 train_time:416948ms step_avg:179.87ms\nstep:2329/5000 train_loss:3.6867 train_time:417127ms step_avg:179.87ms\nstep:2330/5000 train_loss:3.7905 train_time:417306ms step_avg:179.87ms\nstep:2331/5000 train_loss:3.9245 train_time:417486ms step_avg:179.87ms\nstep:2332/5000 train_loss:3.9696 train_time:417665ms step_avg:179.87ms\nstep:2333/5000 train_loss:3.8105 train_time:417847ms step_avg:179.87ms\nstep:2334/5000 train_loss:3.9662 train_time:418027ms step_avg:179.87ms\nstep:2335/5000 train_loss:3.7068 train_time:418209ms step_avg:179.87ms\nstep:2336/5000 train_loss:3.7321 train_time:418388ms step_avg:179.87ms\nstep:2337/5000 train_loss:3.7242 train_time:418568ms step_avg:179.87ms\nstep:2338/5000 train_loss:3.7523 train_time:418748ms step_avg:179.87ms\nstep:2339/5000 train_loss:3.8386 train_time:418928ms step_avg:179.87ms\nstep:2340/5000 train_loss:3.6976 train_time:419106ms step_avg:179.87ms\nstep:2341/5000 train_loss:3.7667 train_time:419286ms step_avg:179.87ms\nstep:2342/5000 train_loss:3.7859 train_time:419465ms step_avg:179.87ms\nstep:2343/5000 train_loss:3.6432 train_time:419645ms step_avg:179.87ms\nstep:2344/5000 train_loss:3.8896 train_time:419823ms step_avg:179.87ms\nstep:2345/5000 train_loss:3.8035 train_time:420003ms step_avg:179.87ms\nstep:2346/5000 train_loss:3.8000 train_time:420183ms step_avg:179.87ms\nstep:2347/5000 train_loss:3.7464 train_time:420362ms step_avg:179.87ms\nstep:2348/5000 train_loss:3.6476 train_time:420543ms step_avg:179.87ms\nstep:2349/5000 train_loss:3.7317 train_time:420726ms step_avg:179.87ms\nstep:2350/5000 train_loss:4.1962 train_time:420906ms step_avg:179.87ms\nstep:2351/5000 train_loss:3.6899 train_time:421086ms step_avg:179.87ms\nstep:2352/5000 train_loss:3.7812 train_time:421265ms step_avg:179.87ms\nstep:2353/5000 train_loss:3.8206 train_time:421444ms step_avg:179.87ms\nstep:2354/5000 train_loss:3.6382 train_time:421624ms step_avg:179.87ms\nstep:2355/5000 train_loss:3.7259 train_time:421803ms step_avg:179.87ms\nstep:2356/5000 train_loss:3.7659 train_time:421984ms step_avg:179.87ms\nstep:2357/5000 train_loss:3.7965 train_time:422163ms step_avg:179.87ms\nstep:2358/5000 train_loss:3.9952 train_time:422342ms step_avg:179.87ms\nstep:2359/5000 train_loss:3.7666 train_time:422522ms step_avg:179.87ms\nstep:2360/5000 train_loss:4.1256 train_time:422702ms step_avg:179.87ms\nstep:2361/5000 train_loss:3.8264 train_time:422882ms step_avg:179.87ms\nstep:2362/5000 train_loss:3.8150 train_time:423062ms step_avg:179.87ms\nstep:2363/5000 train_loss:3.9239 train_time:423241ms step_avg:179.87ms\nstep:2364/5000 train_loss:3.9121 train_time:423421ms step_avg:179.87ms\nstep:2365/5000 train_loss:3.7851 train_time:423601ms step_avg:179.87ms\nstep:2366/5000 train_loss:3.6499 train_time:423781ms step_avg:179.87ms\nstep:2367/5000 train_loss:3.6491 train_time:423962ms step_avg:179.87ms\nstep:2368/5000 train_loss:3.6314 train_time:424141ms step_avg:179.87ms\nstep:2369/5000 train_loss:3.9479 train_time:424320ms step_avg:179.87ms\nstep:2370/5000 train_loss:3.7499 train_time:424501ms step_avg:179.87ms\nstep:2371/5000 train_loss:3.9018 train_time:424680ms step_avg:179.87ms\nstep:2372/5000 train_loss:3.6468 train_time:424859ms step_avg:179.87ms\nstep:2373/5000 train_loss:3.7621 train_time:425037ms step_avg:179.87ms\nstep:2374/5000 train_loss:3.8485 train_time:425216ms step_avg:179.87ms\nstep:2375/5000 train_loss:3.8309 train_time:425394ms step_avg:179.87ms\nstep:2376/5000 train_loss:3.6395 train_time:425573ms step_avg:179.87ms\nstep:2377/5000 train_loss:3.6618 train_time:425752ms step_avg:179.87ms\nstep:2378/5000 train_loss:3.7323 train_time:425930ms step_avg:179.87ms\nstep:2379/5000 train_loss:3.7901 train_time:426113ms step_avg:179.87ms\nstep:2380/5000 train_loss:3.6928 train_time:426292ms step_avg:179.87ms\nstep:2381/5000 train_loss:3.7279 train_time:426471ms step_avg:179.87ms\nstep:2382/5000 train_loss:3.7643 train_time:426650ms step_avg:179.87ms\nstep:2383/5000 train_loss:4.1597 train_time:426830ms step_avg:179.87ms\nstep:2384/5000 train_loss:4.0759 train_time:427010ms step_avg:179.87ms\nstep:2385/5000 train_loss:3.7760 train_time:427190ms step_avg:179.87ms\nstep:2386/5000 train_loss:3.7925 train_time:427369ms step_avg:179.87ms\nstep:2387/5000 train_loss:3.6321 train_time:427549ms step_avg:179.87ms\nstep:2388/5000 train_loss:3.7823 train_time:427729ms step_avg:179.87ms\nstep:2389/5000 train_loss:3.8449 train_time:427908ms step_avg:179.87ms\nstep:2390/5000 train_loss:3.8099 train_time:428086ms step_avg:179.87ms\nstep:2391/5000 train_loss:3.7856 train_time:428266ms step_avg:179.87ms\nstep:2392/5000 train_loss:3.7048 train_time:428447ms step_avg:179.87ms\nstep:2393/5000 train_loss:3.7594 train_time:428627ms step_avg:179.87ms\nstep:2394/5000 train_loss:3.8103 train_time:428808ms step_avg:179.87ms\nstep:2395/5000 train_loss:3.7908 train_time:428988ms step_avg:179.87ms\nstep:2396/5000 train_loss:3.7808 train_time:429168ms step_avg:179.87ms\nstep:2397/5000 train_loss:3.7884 train_time:429347ms step_avg:179.87ms\nstep:2398/5000 train_loss:3.9249 train_time:429526ms step_avg:179.87ms\nstep:2399/5000 train_loss:3.7916 train_time:429705ms step_avg:179.87ms\nstep:2400/5000 train_loss:3.6002 train_time:429889ms step_avg:179.87ms\nstep:2401/5000 train_loss:3.7565 train_time:430068ms step_avg:179.87ms\nstep:2402/5000 train_loss:3.7648 train_time:430247ms step_avg:179.87ms\nstep:2403/5000 train_loss:3.7080 train_time:430429ms step_avg:179.87ms\nstep:2404/5000 train_loss:3.7191 train_time:430608ms step_avg:179.87ms\nstep:2405/5000 train_loss:3.8468 train_time:430787ms step_avg:179.87ms\nstep:2406/5000 train_loss:3.7241 train_time:430969ms step_avg:179.87ms\nstep:2407/5000 train_loss:3.6888 train_time:431147ms step_avg:179.87ms\nstep:2408/5000 train_loss:3.7494 train_time:431326ms step_avg:179.87ms\nstep:2409/5000 train_loss:3.6697 train_time:431507ms step_avg:179.87ms\nstep:2410/5000 train_loss:3.8761 train_time:431688ms step_avg:179.87ms\nstep:2411/5000 train_loss:3.8096 train_time:431869ms step_avg:179.87ms\nstep:2412/5000 train_loss:3.7764 train_time:432048ms step_avg:179.87ms\nstep:2413/5000 train_loss:3.6810 train_time:432227ms step_avg:179.87ms\nstep:2414/5000 train_loss:3.6865 train_time:432406ms step_avg:179.87ms\nstep:2415/5000 train_loss:3.7782 train_time:432585ms step_avg:179.87ms\nstep:2416/5000 train_loss:3.9044 train_time:432765ms step_avg:179.87ms\nstep:2417/5000 train_loss:3.8317 train_time:432946ms step_avg:179.87ms\nstep:2418/5000 train_loss:3.7946 train_time:433127ms step_avg:179.87ms\nstep:2419/5000 train_loss:3.9008 train_time:433306ms step_avg:179.87ms\nstep:2420/5000 train_loss:3.7521 train_time:433486ms step_avg:179.87ms\nstep:2421/5000 train_loss:3.7926 train_time:433665ms step_avg:179.87ms\nstep:2422/5000 train_loss:4.0926 train_time:433846ms step_avg:179.87ms\nstep:2423/5000 train_loss:3.9059 train_time:434027ms step_avg:179.87ms\nstep:2424/5000 train_loss:3.7330 train_time:434205ms step_avg:179.87ms\nstep:2425/5000 train_loss:3.6432 train_time:434385ms step_avg:179.87ms\nstep:2426/5000 train_loss:3.8234 train_time:434564ms step_avg:179.87ms\nstep:2427/5000 train_loss:3.6108 train_time:434743ms step_avg:179.87ms\nstep:2428/5000 train_loss:3.6963 train_time:434923ms step_avg:179.87ms\nstep:2429/5000 train_loss:3.7086 train_time:435103ms step_avg:179.87ms\nstep:2430/5000 train_loss:3.8339 train_time:435280ms step_avg:179.87ms\nstep:2431/5000 train_loss:3.7642 train_time:435459ms step_avg:179.87ms\nstep:2432/5000 train_loss:3.9379 train_time:435638ms step_avg:179.87ms\nstep:2433/5000 train_loss:3.6732 train_time:435817ms step_avg:179.87ms\nstep:2434/5000 train_loss:3.7860 train_time:435996ms step_avg:179.87ms\nstep:2435/5000 train_loss:3.8202 train_time:436177ms step_avg:179.87ms\nstep:2436/5000 train_loss:3.7782 train_time:436356ms step_avg:179.87ms\nstep:2437/5000 train_loss:3.7410 train_time:436534ms step_avg:179.87ms\nstep:2438/5000 train_loss:3.7023 train_time:436712ms step_avg:179.86ms\nstep:2439/5000 train_loss:3.7011 train_time:436892ms step_avg:179.86ms\nstep:2440/5000 train_loss:3.7387 train_time:437071ms step_avg:179.86ms\nstep:2441/5000 train_loss:3.8064 train_time:437250ms step_avg:179.86ms\nstep:2442/5000 train_loss:3.7785 train_time:437430ms step_avg:179.86ms\nstep:2443/5000 train_loss:3.7869 train_time:437610ms step_avg:179.86ms\nstep:2444/5000 train_loss:3.8322 train_time:437788ms step_avg:179.86ms\nstep:2445/5000 train_loss:3.6992 train_time:437968ms step_avg:179.86ms\nstep:2446/5000 train_loss:3.7273 train_time:438148ms step_avg:179.86ms\nstep:2447/5000 train_loss:3.7614 train_time:438327ms step_avg:179.86ms\nstep:2448/5000 train_loss:3.7355 train_time:438507ms step_avg:179.86ms\nstep:2449/5000 train_loss:3.8115 train_time:438687ms step_avg:179.86ms\nstep:2450/5000 train_loss:3.8484 train_time:438869ms step_avg:179.86ms\nstep:2451/5000 train_loss:3.6599 train_time:439050ms step_avg:179.86ms\nstep:2452/5000 train_loss:3.7212 train_time:439229ms step_avg:179.86ms\nstep:2453/5000 train_loss:3.4941 train_time:439410ms step_avg:179.87ms\nstep:2454/5000 train_loss:3.6880 train_time:439589ms step_avg:179.86ms\nstep:2455/5000 train_loss:3.7322 train_time:439766ms step_avg:179.86ms\nstep:2456/5000 train_loss:3.8911 train_time:439946ms step_avg:179.86ms\nstep:2457/5000 train_loss:3.6219 train_time:440125ms step_avg:179.86ms\nstep:2458/5000 train_loss:3.7431 train_time:440306ms step_avg:179.86ms\nstep:2459/5000 train_loss:3.8813 train_time:440486ms step_avg:179.86ms\nstep:2460/5000 train_loss:3.8468 train_time:440667ms step_avg:179.86ms\nstep:2461/5000 train_loss:3.7739 train_time:440846ms step_avg:179.86ms\nstep:2462/5000 train_loss:3.6975 train_time:441026ms step_avg:179.86ms\nstep:2463/5000 train_loss:3.8906 train_time:441207ms step_avg:179.86ms\nstep:2464/5000 train_loss:3.8242 train_time:441387ms step_avg:179.86ms\nstep:2465/5000 train_loss:3.7009 train_time:441567ms step_avg:179.86ms\nstep:2466/5000 train_loss:3.7669 train_time:441746ms step_avg:179.86ms\nstep:2467/5000 train_loss:3.6832 train_time:441926ms step_avg:179.86ms\nstep:2468/5000 train_loss:3.7250 train_time:442106ms step_avg:179.86ms\nstep:2469/5000 train_loss:3.8588 train_time:442286ms step_avg:179.86ms\nstep:2470/5000 train_loss:3.8337 train_time:442465ms step_avg:179.86ms\nstep:2471/5000 train_loss:3.7089 train_time:442644ms step_avg:179.86ms\nstep:2472/5000 train_loss:3.6938 train_time:442823ms step_avg:179.86ms\nstep:2473/5000 train_loss:3.8602 train_time:443003ms step_avg:179.86ms\nstep:2474/5000 train_loss:3.6174 train_time:443183ms step_avg:179.86ms\nstep:2475/5000 train_loss:3.7498 train_time:443363ms step_avg:179.86ms\nstep:2476/5000 train_loss:3.7965 train_time:443543ms step_avg:179.86ms\nstep:2477/5000 train_loss:3.7957 train_time:443724ms step_avg:179.86ms\nstep:2478/5000 train_loss:3.8809 train_time:443903ms step_avg:179.86ms\nstep:2479/5000 train_loss:3.7711 train_time:444083ms step_avg:179.86ms\nstep:2480/5000 train_loss:3.7003 train_time:444262ms step_avg:179.86ms\nstep:2481/5000 train_loss:3.7284 train_time:444441ms step_avg:179.86ms\nstep:2482/5000 train_loss:3.7894 train_time:444619ms step_avg:179.86ms\nstep:2483/5000 train_loss:3.7692 train_time:444798ms step_avg:179.86ms\nstep:2484/5000 train_loss:3.7170 train_time:444977ms step_avg:179.86ms\nstep:2485/5000 train_loss:3.6988 train_time:445156ms step_avg:179.86ms\nstep:2486/5000 train_loss:3.7825 train_time:445334ms step_avg:179.86ms\nstep:2487/5000 train_loss:3.7933 train_time:445513ms step_avg:179.86ms\nstep:2488/5000 train_loss:4.0239 train_time:445692ms step_avg:179.86ms\nstep:2489/5000 train_loss:3.9500 train_time:445872ms step_avg:179.86ms\nstep:2490/5000 train_loss:3.6048 train_time:446050ms step_avg:179.86ms\nstep:2491/5000 train_loss:3.8054 train_time:446230ms step_avg:179.86ms\nstep:2492/5000 train_loss:3.7071 train_time:446409ms step_avg:179.86ms\nstep:2493/5000 train_loss:3.8788 train_time:446589ms step_avg:179.86ms\nstep:2494/5000 train_loss:3.7227 train_time:446770ms step_avg:179.86ms\nstep:2495/5000 train_loss:3.8266 train_time:446950ms step_avg:179.86ms\nstep:2496/5000 train_loss:3.7342 train_time:447129ms step_avg:179.86ms\nstep:2497/5000 train_loss:3.8017 train_time:447308ms step_avg:179.86ms\nstep:2498/5000 train_loss:3.8364 train_time:447487ms step_avg:179.86ms\nstep:2499/5000 train_loss:3.5829 train_time:447667ms step_avg:179.86ms\nstep:2500/5000 train_loss:3.8339 train_time:447847ms step_avg:179.86ms\nstep:2501/5000 train_loss:3.7787 train_time:448028ms step_avg:179.86ms\nstep:2502/5000 train_loss:3.7810 train_time:448206ms step_avg:179.86ms\nstep:2503/5000 train_loss:3.7772 train_time:448385ms step_avg:179.86ms\nstep:2504/5000 train_loss:3.8468 train_time:448565ms step_avg:179.86ms\nstep:2505/5000 train_loss:3.6970 train_time:448744ms step_avg:179.86ms\nstep:2506/5000 train_loss:3.7458 train_time:448923ms step_avg:179.86ms\nstep:2507/5000 train_loss:3.7834 train_time:449103ms step_avg:179.86ms\nstep:2508/5000 train_loss:3.8689 train_time:449282ms step_avg:179.86ms\nstep:2509/5000 train_loss:3.7537 train_time:449461ms step_avg:179.86ms\nstep:2510/5000 train_loss:3.6535 train_time:449639ms step_avg:179.86ms\nstep:2511/5000 train_loss:3.8225 train_time:449818ms step_avg:179.86ms\nstep:2512/5000 train_loss:3.8506 train_time:449996ms step_avg:179.85ms\nstep:2513/5000 train_loss:3.9143 train_time:450175ms step_avg:179.85ms\nstep:2514/5000 train_loss:3.8013 train_time:450354ms step_avg:179.85ms\nstep:2515/5000 train_loss:3.6512 train_time:450532ms step_avg:179.85ms\nstep:2516/5000 train_loss:3.8888 train_time:450711ms step_avg:179.85ms\nstep:2517/5000 train_loss:3.7184 train_time:450890ms step_avg:179.85ms\nstep:2518/5000 train_loss:3.6440 train_time:451073ms step_avg:179.85ms\nstep:2519/5000 train_loss:3.7268 train_time:451251ms step_avg:179.85ms\nstep:2520/5000 train_loss:3.7123 train_time:451431ms step_avg:179.85ms\nstep:2521/5000 train_loss:3.7793 train_time:451610ms step_avg:179.85ms\nstep:2522/5000 train_loss:3.8209 train_time:451788ms step_avg:179.85ms\nstep:2523/5000 train_loss:3.8382 train_time:451968ms step_avg:179.85ms\nstep:2524/5000 train_loss:3.8406 train_time:452148ms step_avg:179.85ms\nstep:2525/5000 train_loss:3.7233 train_time:452329ms step_avg:179.85ms\nstep:2526/5000 train_loss:3.6216 train_time:452509ms step_avg:179.85ms\nstep:2527/5000 train_loss:3.7577 train_time:452691ms step_avg:179.85ms\nstep:2528/5000 train_loss:3.8725 train_time:452870ms step_avg:179.85ms\nstep:2529/5000 train_loss:3.7699 train_time:453050ms step_avg:179.85ms\nstep:2530/5000 train_loss:3.7658 train_time:453229ms step_avg:179.85ms\nstep:2531/5000 train_loss:3.7140 train_time:453407ms step_avg:179.85ms\nstep:2532/5000 train_loss:3.7743 train_time:453587ms step_avg:179.85ms\nstep:2533/5000 train_loss:3.6232 train_time:453768ms step_avg:179.85ms\nstep:2534/5000 train_loss:3.9229 train_time:453947ms step_avg:179.85ms\nstep:2535/5000 train_loss:3.8924 train_time:454127ms step_avg:179.85ms\nstep:2536/5000 train_loss:3.7115 train_time:454307ms step_avg:179.85ms\nstep:2537/5000 train_loss:3.9361 train_time:454486ms step_avg:179.85ms\nstep:2538/5000 train_loss:3.7465 train_time:454666ms step_avg:179.85ms\nstep:2539/5000 train_loss:3.8022 train_time:454845ms step_avg:179.85ms\nstep:2540/5000 train_loss:3.8019 train_time:455024ms step_avg:179.85ms\nstep:2541/5000 train_loss:3.7434 train_time:455206ms step_avg:179.85ms\nstep:2542/5000 train_loss:3.6565 train_time:455386ms step_avg:179.85ms\nstep:2543/5000 train_loss:3.4868 train_time:455564ms step_avg:179.85ms\nstep:2544/5000 train_loss:4.0669 train_time:455746ms step_avg:179.85ms\nstep:2545/5000 train_loss:3.7143 train_time:455925ms step_avg:179.85ms\nstep:2546/5000 train_loss:3.7635 train_time:456104ms step_avg:179.85ms\nstep:2547/5000 train_loss:3.8369 train_time:456285ms step_avg:179.85ms\nstep:2548/5000 train_loss:3.6604 train_time:456465ms step_avg:179.85ms\nstep:2549/5000 train_loss:3.7313 train_time:456644ms step_avg:179.85ms\nstep:2550/5000 train_loss:3.6773 train_time:456824ms step_avg:179.85ms\nstep:2551/5000 train_loss:3.8712 train_time:457004ms step_avg:179.85ms\nstep:2552/5000 train_loss:3.4888 train_time:457184ms step_avg:179.85ms\nstep:2553/5000 train_loss:3.7846 train_time:457364ms step_avg:179.85ms\nstep:2554/5000 train_loss:3.7339 train_time:457543ms step_avg:179.85ms\nstep:2555/5000 train_loss:3.6932 train_time:457724ms step_avg:179.85ms\nstep:2556/5000 train_loss:3.7053 train_time:457903ms step_avg:179.85ms\nstep:2557/5000 train_loss:3.7578 train_time:458083ms step_avg:179.85ms\nstep:2558/5000 train_loss:3.7740 train_time:458262ms step_avg:179.85ms\nstep:2559/5000 train_loss:3.8587 train_time:458441ms step_avg:179.85ms\nstep:2560/5000 train_loss:3.7402 train_time:458619ms step_avg:179.85ms\nstep:2561/5000 train_loss:3.5959 train_time:458798ms step_avg:179.85ms\nstep:2562/5000 train_loss:3.6937 train_time:458978ms step_avg:179.85ms\nstep:2563/5000 train_loss:3.8666 train_time:459156ms step_avg:179.85ms\nstep:2564/5000 train_loss:3.6969 train_time:459335ms step_avg:179.85ms\nstep:2565/5000 train_loss:3.8498 train_time:459513ms step_avg:179.85ms\nstep:2566/5000 train_loss:3.6726 train_time:459691ms step_avg:179.85ms\nstep:2567/5000 train_loss:3.7573 train_time:459870ms step_avg:179.85ms\nstep:2568/5000 train_loss:3.7095 train_time:460049ms step_avg:179.85ms\nstep:2569/5000 train_loss:3.7887 train_time:460229ms step_avg:179.85ms\nstep:2570/5000 train_loss:3.8773 train_time:460409ms step_avg:179.85ms\nstep:2571/5000 train_loss:3.7444 train_time:460590ms step_avg:179.85ms\nstep:2572/5000 train_loss:3.7469 train_time:460770ms step_avg:179.85ms\nstep:2573/5000 train_loss:3.5687 train_time:460950ms step_avg:179.85ms\nstep:2574/5000 train_loss:3.5496 train_time:461129ms step_avg:179.85ms\nstep:2575/5000 train_loss:3.8044 train_time:461308ms step_avg:179.85ms\nstep:2576/5000 train_loss:3.7377 train_time:461488ms step_avg:179.85ms\nstep:2577/5000 train_loss:3.7514 train_time:461668ms step_avg:179.85ms\nstep:2578/5000 train_loss:3.7487 train_time:461848ms step_avg:179.85ms\nstep:2579/5000 train_loss:3.8770 train_time:462029ms step_avg:179.85ms\nstep:2580/5000 train_loss:3.6826 train_time:462209ms step_avg:179.85ms\nstep:2581/5000 train_loss:3.8827 train_time:462389ms step_avg:179.85ms\nstep:2582/5000 train_loss:3.6984 train_time:462568ms step_avg:179.85ms\nstep:2583/5000 train_loss:3.8444 train_time:462747ms step_avg:179.85ms\nstep:2584/5000 train_loss:3.8990 train_time:462929ms step_avg:179.85ms\nstep:2585/5000 train_loss:3.7103 train_time:463108ms step_avg:179.85ms\nstep:2586/5000 train_loss:3.8533 train_time:463288ms step_avg:179.85ms\nstep:2587/5000 train_loss:3.7448 train_time:463468ms step_avg:179.85ms\nstep:2588/5000 train_loss:3.8004 train_time:463647ms step_avg:179.85ms\nstep:2589/5000 train_loss:3.7878 train_time:463827ms step_avg:179.85ms\nstep:2590/5000 train_loss:3.7660 train_time:464009ms step_avg:179.85ms\nstep:2591/5000 train_loss:3.7181 train_time:464188ms step_avg:179.85ms\nstep:2592/5000 train_loss:3.7959 train_time:464368ms step_avg:179.85ms\nstep:2593/5000 train_loss:3.7414 train_time:464548ms step_avg:179.85ms\nstep:2594/5000 train_loss:3.9764 train_time:464727ms step_avg:179.85ms\nstep:2595/5000 train_loss:4.0502 train_time:464906ms step_avg:179.85ms\nstep:2596/5000 train_loss:3.8221 train_time:465085ms step_avg:179.85ms\nstep:2597/5000 train_loss:3.8440 train_time:465264ms step_avg:179.85ms\nstep:2598/5000 train_loss:3.7336 train_time:465443ms step_avg:179.85ms\nstep:2599/5000 train_loss:3.7747 train_time:465623ms step_avg:179.85ms\nstep:2600/5000 train_loss:3.8987 train_time:465803ms step_avg:179.85ms\nstep:2601/5000 train_loss:3.7597 train_time:465982ms step_avg:179.85ms\nstep:2602/5000 train_loss:3.6092 train_time:466162ms step_avg:179.85ms\nstep:2603/5000 train_loss:3.8176 train_time:466343ms step_avg:179.85ms\nstep:2604/5000 train_loss:3.7548 train_time:466525ms step_avg:179.85ms\nstep:2605/5000 train_loss:3.8645 train_time:466704ms step_avg:179.85ms\nstep:2606/5000 train_loss:3.6654 train_time:466884ms step_avg:179.85ms\nstep:2607/5000 train_loss:3.7743 train_time:467065ms step_avg:179.85ms\nstep:2608/5000 train_loss:3.8858 train_time:467243ms step_avg:179.85ms\nstep:2609/5000 train_loss:3.5590 train_time:467423ms step_avg:179.85ms\nstep:2610/5000 train_loss:3.7558 train_time:467602ms step_avg:179.85ms\nstep:2611/5000 train_loss:3.7412 train_time:467781ms step_avg:179.85ms\nstep:2612/5000 train_loss:3.7145 train_time:467960ms step_avg:179.85ms\nstep:2613/5000 train_loss:3.6522 train_time:468139ms step_avg:179.85ms\nstep:2614/5000 train_loss:3.7317 train_time:468317ms step_avg:179.85ms\nstep:2615/5000 train_loss:3.7098 train_time:468495ms step_avg:179.84ms\nstep:2616/5000 train_loss:3.7891 train_time:468674ms step_avg:179.84ms\nstep:2617/5000 train_loss:3.7017 train_time:468854ms step_avg:179.84ms\nstep:2618/5000 train_loss:3.8780 train_time:469032ms step_avg:179.84ms\nstep:2619/5000 train_loss:3.6821 train_time:469211ms step_avg:179.84ms\nstep:2620/5000 train_loss:3.5563 train_time:469389ms step_avg:179.84ms\nstep:2621/5000 train_loss:3.8440 train_time:469568ms step_avg:179.84ms\nstep:2622/5000 train_loss:3.8947 train_time:469747ms step_avg:179.84ms\nstep:2623/5000 train_loss:3.9046 train_time:469926ms step_avg:179.84ms\nstep:2624/5000 train_loss:3.6954 train_time:470106ms step_avg:179.84ms\nstep:2625/5000 train_loss:3.6170 train_time:470285ms step_avg:179.84ms\nstep:2626/5000 train_loss:3.8139 train_time:470464ms step_avg:179.84ms\nstep:2627/5000 train_loss:3.7798 train_time:470643ms step_avg:179.84ms\nstep:2628/5000 train_loss:3.6989 train_time:470823ms step_avg:179.84ms\nstep:2629/5000 train_loss:3.8194 train_time:471002ms step_avg:179.84ms\nstep:2630/5000 train_loss:3.7271 train_time:471182ms step_avg:179.84ms\nstep:2631/5000 train_loss:3.7816 train_time:471362ms step_avg:179.84ms\nstep:2632/5000 train_loss:3.6446 train_time:471543ms step_avg:179.84ms\nstep:2633/5000 train_loss:3.5315 train_time:471721ms step_avg:179.84ms\nstep:2634/5000 train_loss:3.5807 train_time:471900ms step_avg:179.84ms\nstep:2635/5000 train_loss:3.8053 train_time:472078ms step_avg:179.84ms\nstep:2636/5000 train_loss:3.8122 train_time:472258ms step_avg:179.84ms\nstep:2637/5000 train_loss:3.6950 train_time:472435ms step_avg:179.84ms\nstep:2638/5000 train_loss:3.7668 train_time:472614ms step_avg:179.84ms\nstep:2639/5000 train_loss:3.7331 train_time:472793ms step_avg:179.84ms\nstep:2640/5000 train_loss:3.7161 train_time:472970ms step_avg:179.84ms\nstep:2641/5000 train_loss:3.6612 train_time:473149ms step_avg:179.84ms\nstep:2642/5000 train_loss:3.8646 train_time:473330ms step_avg:179.84ms\nstep:2643/5000 train_loss:3.8504 train_time:473509ms step_avg:179.84ms\nstep:2644/5000 train_loss:3.8121 train_time:473689ms step_avg:179.84ms\nstep:2645/5000 train_loss:3.7606 train_time:473869ms step_avg:179.84ms\nstep:2646/5000 train_loss:3.8265 train_time:474048ms step_avg:179.84ms\nstep:2647/5000 train_loss:3.7487 train_time:474227ms step_avg:179.84ms\nstep:2648/5000 train_loss:3.8804 train_time:474406ms step_avg:179.84ms\nstep:2649/5000 train_loss:3.8896 train_time:474586ms step_avg:179.84ms\nstep:2650/5000 train_loss:3.6353 train_time:474766ms step_avg:179.84ms\nstep:2651/5000 train_loss:3.8571 train_time:474947ms step_avg:179.84ms\nstep:2652/5000 train_loss:3.7954 train_time:475126ms step_avg:179.84ms\nstep:2653/5000 train_loss:3.7067 train_time:475306ms step_avg:179.84ms\nstep:2654/5000 train_loss:3.7796 train_time:475485ms step_avg:179.84ms\nstep:2655/5000 train_loss:3.7299 train_time:475665ms step_avg:179.84ms\nstep:2656/5000 train_loss:3.6749 train_time:475844ms step_avg:179.84ms\nstep:2657/5000 train_loss:3.7149 train_time:476025ms step_avg:179.84ms\nstep:2658/5000 train_loss:3.7895 train_time:476205ms step_avg:179.84ms\nstep:2659/5000 train_loss:3.6848 train_time:476386ms step_avg:179.84ms\nstep:2660/5000 train_loss:3.7559 train_time:476564ms step_avg:179.84ms\nstep:2661/5000 train_loss:3.7123 train_time:476743ms step_avg:179.84ms\nstep:2662/5000 train_loss:3.6773 train_time:476925ms step_avg:179.84ms\nstep:2663/5000 train_loss:3.7420 train_time:477106ms step_avg:179.84ms\nstep:2664/5000 train_loss:3.6342 train_time:477287ms step_avg:179.84ms\nstep:2665/5000 train_loss:3.8960 train_time:477466ms step_avg:179.84ms\nstep:2666/5000 train_loss:3.7874 train_time:477646ms step_avg:179.84ms\nstep:2667/5000 train_loss:3.6431 train_time:477827ms step_avg:179.84ms\nstep:2668/5000 train_loss:3.9154 train_time:478006ms step_avg:179.84ms\nstep:2669/5000 train_loss:3.6408 train_time:478186ms step_avg:179.84ms\nstep:2670/5000 train_loss:3.8831 train_time:478365ms step_avg:179.84ms\nstep:2671/5000 train_loss:3.7774 train_time:478545ms step_avg:179.84ms\nstep:2672/5000 train_loss:3.7414 train_time:478725ms step_avg:179.84ms\nstep:2673/5000 train_loss:3.8056 train_time:478903ms step_avg:179.84ms\nstep:2674/5000 train_loss:3.8693 train_time:479082ms step_avg:179.84ms\nstep:2675/5000 train_loss:3.8613 train_time:479263ms step_avg:179.84ms\nstep:2676/5000 train_loss:3.7579 train_time:479444ms step_avg:179.84ms\nstep:2677/5000 train_loss:3.9144 train_time:479627ms step_avg:179.84ms\nstep:2678/5000 train_loss:3.6649 train_time:479807ms step_avg:179.84ms\nstep:2679/5000 train_loss:3.7526 train_time:479987ms step_avg:179.84ms\nstep:2680/5000 train_loss:3.6104 train_time:480166ms step_avg:179.84ms\nstep:2681/5000 train_loss:3.7195 train_time:480347ms step_avg:179.84ms\nstep:2682/5000 train_loss:3.8382 train_time:480527ms step_avg:179.84ms\nstep:2683/5000 train_loss:3.7003 train_time:480706ms step_avg:179.84ms\nstep:2684/5000 train_loss:3.7295 train_time:480886ms step_avg:179.84ms\nstep:2685/5000 train_loss:3.6846 train_time:481066ms step_avg:179.84ms\nstep:2686/5000 train_loss:3.7627 train_time:481246ms step_avg:179.84ms\nstep:2687/5000 train_loss:3.9375 train_time:481426ms step_avg:179.84ms\nstep:2688/5000 train_loss:3.6803 train_time:481606ms step_avg:179.84ms\nstep:2689/5000 train_loss:3.6994 train_time:481787ms step_avg:179.84ms\nstep:2690/5000 train_loss:3.5737 train_time:481967ms step_avg:179.84ms\nstep:2691/5000 train_loss:3.8396 train_time:482146ms step_avg:179.84ms\nstep:2692/5000 train_loss:3.7036 train_time:482327ms step_avg:179.84ms\nstep:2693/5000 train_loss:3.7306 train_time:482506ms step_avg:179.84ms\nstep:2694/5000 train_loss:4.0337 train_time:482688ms step_avg:179.84ms\nstep:2695/5000 train_loss:3.6895 train_time:482868ms step_avg:179.84ms\nstep:2696/5000 train_loss:3.9856 train_time:483047ms step_avg:179.84ms\nstep:2697/5000 train_loss:3.6556 train_time:483227ms step_avg:179.84ms\nstep:2698/5000 train_loss:3.8188 train_time:483407ms step_avg:179.84ms\nstep:2699/5000 train_loss:3.7032 train_time:483587ms step_avg:179.84ms\nstep:2700/5000 train_loss:3.7739 train_time:483769ms step_avg:179.84ms\nstep:2701/5000 train_loss:3.7402 train_time:483949ms step_avg:179.84ms\nstep:2702/5000 train_loss:3.7614 train_time:484128ms step_avg:179.84ms\nstep:2703/5000 train_loss:3.6179 train_time:484307ms step_avg:179.84ms\nstep:2704/5000 train_loss:3.8362 train_time:484486ms step_avg:179.84ms\nstep:2705/5000 train_loss:3.6953 train_time:484666ms step_avg:179.84ms\nstep:2706/5000 train_loss:3.8107 train_time:484846ms step_avg:179.84ms\nstep:2707/5000 train_loss:3.7561 train_time:485026ms step_avg:179.84ms\nstep:2708/5000 train_loss:3.6220 train_time:485206ms step_avg:179.84ms\nstep:2709/5000 train_loss:3.7692 train_time:485387ms step_avg:179.84ms\nstep:2710/5000 train_loss:3.7187 train_time:485568ms step_avg:179.84ms\nstep:2711/5000 train_loss:3.7754 train_time:485750ms step_avg:179.84ms\nstep:2712/5000 train_loss:3.7278 train_time:485930ms step_avg:179.84ms\nstep:2713/5000 train_loss:3.6683 train_time:486108ms step_avg:179.84ms\nstep:2714/5000 train_loss:3.5856 train_time:486288ms step_avg:179.84ms\nstep:2715/5000 train_loss:3.8032 train_time:486468ms step_avg:179.84ms\nstep:2716/5000 train_loss:3.6767 train_time:486647ms step_avg:179.84ms\nstep:2717/5000 train_loss:3.5542 train_time:486826ms step_avg:179.84ms\nstep:2718/5000 train_loss:3.6995 train_time:487007ms step_avg:179.84ms\nstep:2719/5000 train_loss:3.8048 train_time:487187ms step_avg:179.84ms\nstep:2720/5000 train_loss:3.8181 train_time:487367ms step_avg:179.84ms\nstep:2721/5000 train_loss:3.5511 train_time:487548ms step_avg:179.84ms\nstep:2722/5000 train_loss:3.8006 train_time:487728ms step_avg:179.84ms\nstep:2723/5000 train_loss:3.6869 train_time:487909ms step_avg:179.84ms\nstep:2724/5000 train_loss:3.7129 train_time:488088ms step_avg:179.84ms\nstep:2725/5000 train_loss:3.7115 train_time:488269ms step_avg:179.84ms\nstep:2726/5000 train_loss:3.8998 train_time:488448ms step_avg:179.84ms\nstep:2727/5000 train_loss:3.7190 train_time:488629ms step_avg:179.84ms\nstep:2728/5000 train_loss:3.7397 train_time:488809ms step_avg:179.84ms\nstep:2729/5000 train_loss:3.6603 train_time:488988ms step_avg:179.84ms\nstep:2730/5000 train_loss:3.6650 train_time:489166ms step_avg:179.84ms\nstep:2731/5000 train_loss:3.7132 train_time:489347ms step_avg:179.84ms\nstep:2732/5000 train_loss:3.8261 train_time:489527ms step_avg:179.84ms\nstep:2733/5000 train_loss:3.8741 train_time:489707ms step_avg:179.84ms\nstep:2734/5000 train_loss:3.7691 train_time:489889ms step_avg:179.84ms\nstep:2735/5000 train_loss:3.7093 train_time:490069ms step_avg:179.84ms\nstep:2736/5000 train_loss:3.8695 train_time:490248ms step_avg:179.84ms\nstep:2737/5000 train_loss:4.3707 train_time:490427ms step_avg:179.84ms\nstep:2738/5000 train_loss:3.8056 train_time:490607ms step_avg:179.84ms\nstep:2739/5000 train_loss:3.5514 train_time:490787ms step_avg:179.84ms\nstep:2740/5000 train_loss:3.7647 train_time:490966ms step_avg:179.84ms\nstep:2741/5000 train_loss:3.9477 train_time:491149ms step_avg:179.84ms\nstep:2742/5000 train_loss:3.7317 train_time:491329ms step_avg:179.84ms\nstep:2743/5000 train_loss:3.8115 train_time:491509ms step_avg:179.84ms\nstep:2744/5000 train_loss:3.8364 train_time:491688ms step_avg:179.84ms\nstep:2745/5000 train_loss:3.7456 train_time:491867ms step_avg:179.84ms\nstep:2746/5000 train_loss:3.8098 train_time:492046ms step_avg:179.84ms\nstep:2747/5000 train_loss:3.7187 train_time:492226ms step_avg:179.84ms\nstep:2748/5000 train_loss:3.7717 train_time:492406ms step_avg:179.84ms\nstep:2749/5000 train_loss:3.6701 train_time:492586ms step_avg:179.84ms\nstep:2750/5000 train_loss:3.7179 train_time:492765ms step_avg:179.84ms\nstep:2751/5000 train_loss:3.8717 train_time:492944ms step_avg:179.84ms\nstep:2752/5000 train_loss:3.8123 train_time:493124ms step_avg:179.84ms\nstep:2753/5000 train_loss:3.6640 train_time:493303ms step_avg:179.84ms\nstep:2754/5000 train_loss:3.6779 train_time:493483ms step_avg:179.84ms\nstep:2755/5000 train_loss:3.6915 train_time:493664ms step_avg:179.84ms\nstep:2756/5000 train_loss:3.4426 train_time:493843ms step_avg:179.84ms\nstep:2757/5000 train_loss:3.6602 train_time:494023ms step_avg:179.84ms\nstep:2758/5000 train_loss:3.7582 train_time:494204ms step_avg:179.84ms\nstep:2759/5000 train_loss:3.8334 train_time:494383ms step_avg:179.84ms\nstep:2760/5000 train_loss:3.5558 train_time:494563ms step_avg:179.84ms\nstep:2761/5000 train_loss:3.6812 train_time:494743ms step_avg:179.84ms\nstep:2762/5000 train_loss:3.6052 train_time:494923ms step_avg:179.84ms\nstep:2763/5000 train_loss:3.7153 train_time:495101ms step_avg:179.84ms\nstep:2764/5000 train_loss:3.6891 train_time:495281ms step_avg:179.84ms\nstep:2765/5000 train_loss:3.7820 train_time:495460ms step_avg:179.84ms\nstep:2766/5000 train_loss:3.8071 train_time:495639ms step_avg:179.84ms\nstep:2767/5000 train_loss:3.7084 train_time:495819ms step_avg:179.84ms\nstep:2768/5000 train_loss:3.5716 train_time:495999ms step_avg:179.84ms\nstep:2769/5000 train_loss:3.7728 train_time:496178ms step_avg:179.84ms\nstep:2770/5000 train_loss:3.8114 train_time:496357ms step_avg:179.84ms\nstep:2771/5000 train_loss:3.7255 train_time:496536ms step_avg:179.84ms\nstep:2772/5000 train_loss:3.7788 train_time:496715ms step_avg:179.84ms\nstep:2773/5000 train_loss:3.6911 train_time:496893ms step_avg:179.84ms\nstep:2774/5000 train_loss:3.5717 train_time:497072ms step_avg:179.84ms\nstep:2775/5000 train_loss:3.7198 train_time:497250ms step_avg:179.84ms\nstep:2776/5000 train_loss:3.8062 train_time:497430ms step_avg:179.84ms\nstep:2777/5000 train_loss:3.6289 train_time:497609ms step_avg:179.84ms\nstep:2778/5000 train_loss:3.8037 train_time:497788ms step_avg:179.84ms\nstep:2779/5000 train_loss:3.7602 train_time:497967ms step_avg:179.84ms\nstep:2780/5000 train_loss:3.6794 train_time:498147ms step_avg:179.84ms\nstep:2781/5000 train_loss:3.6883 train_time:498327ms step_avg:179.84ms\nstep:2782/5000 train_loss:3.7470 train_time:498507ms step_avg:179.84ms\nstep:2783/5000 train_loss:3.7983 train_time:498686ms step_avg:179.84ms\nstep:2784/5000 train_loss:3.6694 train_time:498866ms step_avg:179.84ms\nstep:2785/5000 train_loss:3.6577 train_time:499048ms step_avg:179.84ms\nstep:2786/5000 train_loss:3.5039 train_time:499227ms step_avg:179.84ms\nstep:2787/5000 train_loss:3.6898 train_time:499409ms step_avg:179.84ms\nstep:2788/5000 train_loss:4.0003 train_time:499589ms step_avg:179.84ms\nstep:2789/5000 train_loss:3.7385 train_time:499769ms step_avg:179.84ms\nstep:2790/5000 train_loss:3.8474 train_time:499948ms step_avg:179.84ms\nstep:2791/5000 train_loss:3.8207 train_time:500127ms step_avg:179.84ms\nstep:2792/5000 train_loss:3.7425 train_time:500308ms step_avg:179.84ms\nstep:2793/5000 train_loss:3.6210 train_time:500489ms step_avg:179.84ms\nstep:2794/5000 train_loss:3.7266 train_time:500669ms step_avg:179.84ms\nstep:2795/5000 train_loss:3.6423 train_time:500848ms step_avg:179.84ms\nstep:2796/5000 train_loss:3.7773 train_time:501027ms step_avg:179.84ms\nstep:2797/5000 train_loss:3.6037 train_time:501207ms step_avg:179.84ms\nstep:2798/5000 train_loss:3.7151 train_time:501387ms step_avg:179.84ms\nstep:2799/5000 train_loss:3.7015 train_time:501568ms step_avg:179.84ms\nstep:2800/5000 train_loss:3.9448 train_time:501747ms step_avg:179.84ms\nstep:2801/5000 train_loss:3.6818 train_time:501926ms step_avg:179.84ms\nstep:2802/5000 train_loss:3.6784 train_time:502105ms step_avg:179.84ms\nstep:2803/5000 train_loss:4.0270 train_time:502285ms step_avg:179.84ms\nstep:2804/5000 train_loss:3.7065 train_time:502466ms step_avg:179.84ms\nstep:2805/5000 train_loss:3.5857 train_time:502646ms step_avg:179.84ms\nstep:2806/5000 train_loss:3.7134 train_time:502828ms step_avg:179.84ms\nstep:2807/5000 train_loss:3.6082 train_time:503008ms step_avg:179.84ms\nstep:2808/5000 train_loss:3.6695 train_time:503188ms step_avg:179.84ms\nstep:2809/5000 train_loss:3.8232 train_time:503367ms step_avg:179.84ms\nstep:2810/5000 train_loss:3.2563 train_time:503546ms step_avg:179.84ms\nstep:2811/5000 train_loss:3.8080 train_time:503726ms step_avg:179.84ms\nstep:2812/5000 train_loss:3.6539 train_time:503905ms step_avg:179.84ms\nstep:2813/5000 train_loss:3.6692 train_time:504084ms step_avg:179.84ms\nstep:2814/5000 train_loss:3.7342 train_time:504263ms step_avg:179.84ms\nstep:2815/5000 train_loss:3.7071 train_time:504442ms step_avg:179.84ms\nstep:2816/5000 train_loss:3.6163 train_time:504622ms step_avg:179.84ms\nstep:2817/5000 train_loss:3.6779 train_time:504802ms step_avg:179.84ms\nstep:2818/5000 train_loss:3.5636 train_time:504982ms step_avg:179.84ms\nstep:2819/5000 train_loss:3.7495 train_time:505162ms step_avg:179.84ms\nstep:2820/5000 train_loss:3.6997 train_time:505341ms step_avg:179.84ms\nstep:2821/5000 train_loss:3.8179 train_time:505520ms step_avg:179.84ms\nstep:2822/5000 train_loss:3.8545 train_time:505700ms step_avg:179.84ms\nstep:2823/5000 train_loss:3.8342 train_time:505878ms step_avg:179.84ms\nstep:2824/5000 train_loss:3.7025 train_time:506057ms step_avg:179.84ms\nstep:2825/5000 train_loss:3.7988 train_time:506235ms step_avg:179.83ms\nstep:2826/5000 train_loss:3.7071 train_time:506415ms step_avg:179.83ms\nstep:2827/5000 train_loss:3.6800 train_time:506593ms step_avg:179.83ms\nstep:2828/5000 train_loss:3.7116 train_time:506773ms step_avg:179.83ms\nstep:2829/5000 train_loss:3.6050 train_time:506951ms step_avg:179.83ms\nstep:2830/5000 train_loss:3.7409 train_time:507130ms step_avg:179.83ms\nstep:2831/5000 train_loss:3.6851 train_time:507308ms step_avg:179.83ms\nstep:2832/5000 train_loss:3.7816 train_time:507488ms step_avg:179.83ms\nstep:2833/5000 train_loss:3.7077 train_time:507667ms step_avg:179.83ms\nstep:2834/5000 train_loss:3.7318 train_time:507847ms step_avg:179.83ms\nstep:2835/5000 train_loss:4.0988 train_time:508027ms step_avg:179.83ms\nstep:2836/5000 train_loss:3.7375 train_time:508206ms step_avg:179.83ms\nstep:2837/5000 train_loss:3.6366 train_time:508385ms step_avg:179.83ms\nstep:2838/5000 train_loss:3.7845 train_time:508564ms step_avg:179.83ms\nstep:2839/5000 train_loss:3.7483 train_time:508743ms step_avg:179.83ms\nstep:2840/5000 train_loss:3.7108 train_time:508923ms step_avg:179.83ms\nstep:2841/5000 train_loss:3.7938 train_time:509104ms step_avg:179.83ms\nstep:2842/5000 train_loss:3.8083 train_time:509284ms step_avg:179.83ms\nstep:2843/5000 train_loss:3.6946 train_time:509462ms step_avg:179.83ms\nstep:2844/5000 train_loss:3.6827 train_time:509641ms step_avg:179.83ms\nstep:2845/5000 train_loss:3.7324 train_time:509820ms step_avg:179.83ms\nstep:2846/5000 train_loss:3.6941 train_time:510000ms step_avg:179.83ms\nstep:2847/5000 train_loss:3.8794 train_time:510179ms step_avg:179.83ms\nstep:2848/5000 train_loss:3.7311 train_time:510358ms step_avg:179.83ms\nstep:2849/5000 train_loss:3.7645 train_time:510536ms step_avg:179.83ms\nstep:2850/5000 train_loss:3.6993 train_time:510715ms step_avg:179.83ms\nstep:2851/5000 train_loss:3.7747 train_time:510893ms step_avg:179.83ms\nstep:2852/5000 train_loss:3.7799 train_time:511072ms step_avg:179.83ms\nstep:2853/5000 train_loss:3.7363 train_time:511251ms step_avg:179.83ms\nstep:2854/5000 train_loss:3.6877 train_time:511430ms step_avg:179.83ms\nstep:2855/5000 train_loss:3.6700 train_time:511611ms step_avg:179.83ms\nstep:2856/5000 train_loss:3.8015 train_time:511790ms step_avg:179.83ms\nstep:2857/5000 train_loss:3.7638 train_time:511969ms step_avg:179.83ms\nstep:2858/5000 train_loss:3.7179 train_time:512149ms step_avg:179.83ms\nstep:2859/5000 train_loss:3.9089 train_time:512329ms step_avg:179.83ms\nstep:2860/5000 train_loss:3.7112 train_time:512509ms step_avg:179.83ms\nstep:2861/5000 train_loss:3.6715 train_time:512689ms step_avg:179.83ms\nstep:2862/5000 train_loss:3.5687 train_time:512868ms step_avg:179.83ms\nstep:2863/5000 train_loss:3.6454 train_time:513047ms step_avg:179.83ms\nstep:2864/5000 train_loss:3.7041 train_time:513227ms step_avg:179.83ms\nstep:2865/5000 train_loss:3.6481 train_time:513407ms step_avg:179.83ms\nstep:2866/5000 train_loss:3.9977 train_time:513587ms step_avg:179.83ms\nstep:2867/5000 train_loss:3.9322 train_time:513767ms step_avg:179.83ms\nstep:2868/5000 train_loss:3.7296 train_time:513946ms step_avg:179.83ms\nstep:2869/5000 train_loss:3.5677 train_time:514127ms step_avg:179.83ms\nstep:2870/5000 train_loss:3.8647 train_time:514307ms step_avg:179.83ms\nstep:2871/5000 train_loss:3.7068 train_time:514488ms step_avg:179.83ms\nstep:2872/5000 train_loss:3.8870 train_time:514667ms step_avg:179.83ms\nstep:2873/5000 train_loss:3.7049 train_time:514848ms step_avg:179.83ms\nstep:2874/5000 train_loss:3.8443 train_time:515027ms step_avg:179.83ms\nstep:2875/5000 train_loss:3.7617 train_time:515208ms step_avg:179.83ms\nstep:2876/5000 train_loss:3.7281 train_time:515387ms step_avg:179.83ms\nstep:2877/5000 train_loss:3.7695 train_time:515566ms step_avg:179.83ms\nstep:2878/5000 train_loss:3.8176 train_time:515746ms step_avg:179.83ms\nstep:2879/5000 train_loss:3.7459 train_time:515927ms step_avg:179.83ms\nstep:2880/5000 train_loss:3.8080 train_time:516106ms step_avg:179.83ms\nstep:2881/5000 train_loss:3.5847 train_time:516287ms step_avg:179.83ms\nstep:2882/5000 train_loss:3.7605 train_time:516467ms step_avg:179.83ms\nstep:2883/5000 train_loss:3.6282 train_time:516647ms step_avg:179.83ms\nstep:2884/5000 train_loss:3.9772 train_time:516827ms step_avg:179.83ms\nstep:2885/5000 train_loss:3.8924 train_time:517007ms step_avg:179.83ms\nstep:2886/5000 train_loss:3.6386 train_time:517186ms step_avg:179.83ms\nstep:2887/5000 train_loss:3.7504 train_time:517365ms step_avg:179.83ms\nstep:2888/5000 train_loss:3.5925 train_time:517544ms step_avg:179.83ms\nstep:2889/5000 train_loss:3.7895 train_time:517724ms step_avg:179.83ms\nstep:2890/5000 train_loss:3.7858 train_time:517904ms step_avg:179.83ms\nstep:2891/5000 train_loss:3.9012 train_time:518084ms step_avg:179.83ms\nstep:2892/5000 train_loss:3.8698 train_time:518266ms step_avg:179.83ms\nstep:2893/5000 train_loss:3.6832 train_time:518445ms step_avg:179.83ms\nstep:2894/5000 train_loss:3.7146 train_time:518626ms step_avg:179.83ms\nstep:2895/5000 train_loss:3.8384 train_time:518806ms step_avg:179.83ms\nstep:2896/5000 train_loss:3.9173 train_time:518987ms step_avg:179.83ms\nstep:2897/5000 train_loss:3.7617 train_time:519166ms step_avg:179.83ms\nstep:2898/5000 train_loss:3.7299 train_time:519347ms step_avg:179.83ms\nstep:2899/5000 train_loss:3.5090 train_time:519527ms step_avg:179.83ms\nstep:2900/5000 train_loss:3.7042 train_time:519708ms step_avg:179.83ms\nstep:2901/5000 train_loss:3.6912 train_time:519887ms step_avg:179.83ms\nstep:2902/5000 train_loss:3.7054 train_time:520066ms step_avg:179.83ms\nstep:2903/5000 train_loss:3.7589 train_time:520245ms step_avg:179.83ms\nstep:2904/5000 train_loss:3.7807 train_time:520425ms step_avg:179.83ms\nstep:2905/5000 train_loss:3.6863 train_time:520606ms step_avg:179.83ms\nstep:2906/5000 train_loss:3.6106 train_time:520786ms step_avg:179.83ms\nstep:2907/5000 train_loss:3.5185 train_time:520967ms step_avg:179.83ms\nstep:2908/5000 train_loss:3.6686 train_time:521149ms step_avg:179.83ms\nstep:2909/5000 train_loss:3.5812 train_time:521328ms step_avg:179.83ms\nstep:2910/5000 train_loss:3.7903 train_time:521508ms step_avg:179.83ms\nstep:2911/5000 train_loss:3.7467 train_time:521688ms step_avg:179.83ms\nstep:2912/5000 train_loss:3.7022 train_time:521868ms step_avg:179.83ms\nstep:2913/5000 train_loss:3.7754 train_time:522048ms step_avg:179.83ms\nstep:2914/5000 train_loss:3.7340 train_time:522227ms step_avg:179.83ms\nstep:2915/5000 train_loss:3.6746 train_time:522407ms step_avg:179.83ms\nstep:2916/5000 train_loss:3.8729 train_time:522586ms step_avg:179.83ms\nstep:2917/5000 train_loss:3.7118 train_time:522765ms step_avg:179.83ms\nstep:2918/5000 train_loss:3.5329 train_time:522945ms step_avg:179.83ms\nstep:2919/5000 train_loss:3.6041 train_time:523124ms step_avg:179.83ms\nstep:2920/5000 train_loss:3.8212 train_time:523305ms step_avg:179.83ms\nstep:2921/5000 train_loss:3.7307 train_time:523487ms step_avg:179.83ms\nstep:2922/5000 train_loss:3.7020 train_time:523666ms step_avg:179.83ms\nstep:2923/5000 train_loss:3.7551 train_time:523846ms step_avg:179.83ms\nstep:2924/5000 train_loss:3.8184 train_time:524025ms step_avg:179.83ms\nstep:2925/5000 train_loss:3.7513 train_time:524204ms step_avg:179.83ms\nstep:2926/5000 train_loss:3.8012 train_time:524383ms step_avg:179.83ms\nstep:2927/5000 train_loss:3.7722 train_time:524564ms step_avg:179.83ms\nstep:2928/5000 train_loss:3.7023 train_time:524744ms step_avg:179.83ms\nstep:2929/5000 train_loss:3.7882 train_time:524925ms step_avg:179.83ms\nstep:2930/5000 train_loss:3.7890 train_time:525107ms step_avg:179.83ms\nstep:2931/5000 train_loss:3.7750 train_time:525287ms step_avg:179.83ms\nstep:2932/5000 train_loss:3.7295 train_time:525465ms step_avg:179.83ms\nstep:2933/5000 train_loss:3.6210 train_time:525645ms step_avg:179.83ms\nstep:2934/5000 train_loss:3.8295 train_time:525825ms step_avg:179.83ms\nstep:2935/5000 train_loss:3.9187 train_time:526005ms step_avg:179.83ms\nstep:2936/5000 train_loss:3.9669 train_time:526187ms step_avg:179.83ms\nstep:2937/5000 train_loss:3.6707 train_time:526366ms step_avg:179.83ms\nstep:2938/5000 train_loss:3.6520 train_time:526546ms step_avg:179.83ms\nstep:2939/5000 train_loss:3.7787 train_time:526725ms step_avg:179.83ms\nstep:2940/5000 train_loss:3.7002 train_time:526906ms step_avg:179.83ms\nstep:2941/5000 train_loss:3.8086 train_time:527086ms step_avg:179.83ms\nstep:2942/5000 train_loss:3.6800 train_time:527265ms step_avg:179.83ms\nstep:2943/5000 train_loss:3.7388 train_time:527445ms step_avg:179.83ms\nstep:2944/5000 train_loss:3.7688 train_time:527624ms step_avg:179.83ms\nstep:2945/5000 train_loss:3.7742 train_time:527807ms step_avg:179.83ms\nstep:2946/5000 train_loss:3.7173 train_time:527987ms step_avg:179.83ms\nstep:2947/5000 train_loss:3.7164 train_time:528166ms step_avg:179.83ms\nstep:2948/5000 train_loss:3.8792 train_time:528344ms step_avg:179.83ms\nstep:2949/5000 train_loss:3.5957 train_time:528524ms step_avg:179.83ms\nstep:2950/5000 train_loss:3.8482 train_time:528705ms step_avg:179.83ms\nstep:2951/5000 train_loss:3.5830 train_time:528885ms step_avg:179.83ms\nstep:2952/5000 train_loss:3.7951 train_time:529064ms step_avg:179.83ms\nstep:2953/5000 train_loss:3.7201 train_time:529244ms step_avg:179.83ms\nstep:2954/5000 train_loss:3.7596 train_time:529424ms step_avg:179.83ms\nstep:2955/5000 train_loss:3.7844 train_time:529604ms step_avg:179.83ms\nstep:2956/5000 train_loss:3.7380 train_time:529785ms step_avg:179.83ms\nstep:2957/5000 train_loss:3.6761 train_time:529963ms step_avg:179.83ms\nstep:2958/5000 train_loss:3.7449 train_time:530143ms step_avg:179.83ms\nstep:2959/5000 train_loss:3.6299 train_time:530321ms step_avg:179.83ms\nstep:2960/5000 train_loss:3.6305 train_time:530501ms step_avg:179.83ms\nstep:2961/5000 train_loss:3.7472 train_time:530681ms step_avg:179.83ms\nstep:2962/5000 train_loss:3.7870 train_time:530861ms step_avg:179.83ms\nstep:2963/5000 train_loss:3.6497 train_time:531040ms step_avg:179.83ms\nstep:2964/5000 train_loss:4.2706 train_time:531222ms step_avg:179.83ms\nstep:2965/5000 train_loss:3.8083 train_time:531398ms step_avg:179.83ms\nstep:2966/5000 train_loss:3.7266 train_time:531577ms step_avg:179.83ms\nstep:2967/5000 train_loss:3.7574 train_time:531756ms step_avg:179.83ms\nstep:2968/5000 train_loss:3.6351 train_time:531936ms step_avg:179.83ms\nstep:2969/5000 train_loss:3.7717 train_time:532116ms step_avg:179.83ms\nstep:2970/5000 train_loss:3.7484 train_time:532295ms step_avg:179.83ms\nstep:2971/5000 train_loss:3.8037 train_time:532474ms step_avg:179.83ms\nstep:2972/5000 train_loss:3.7129 train_time:532652ms step_avg:179.83ms\nstep:2973/5000 train_loss:3.8698 train_time:532831ms step_avg:179.83ms\nstep:2974/5000 train_loss:3.6383 train_time:533011ms step_avg:179.83ms\nstep:2975/5000 train_loss:3.7392 train_time:533189ms step_avg:179.83ms\nstep:2976/5000 train_loss:3.7182 train_time:533368ms step_avg:179.83ms\nstep:2977/5000 train_loss:3.7847 train_time:533549ms step_avg:179.83ms\nstep:2978/5000 train_loss:3.8954 train_time:533728ms step_avg:179.83ms\nstep:2979/5000 train_loss:3.6736 train_time:533907ms step_avg:179.83ms\nstep:2980/5000 train_loss:3.7812 train_time:534087ms step_avg:179.83ms\nstep:2981/5000 train_loss:3.6437 train_time:534266ms step_avg:179.83ms\nstep:2982/5000 train_loss:3.6774 train_time:534445ms step_avg:179.83ms\nstep:2983/5000 train_loss:3.4986 train_time:534625ms step_avg:179.83ms\nstep:2984/5000 train_loss:3.7680 train_time:534807ms step_avg:179.83ms\nstep:2985/5000 train_loss:3.7132 train_time:534987ms step_avg:179.83ms\nstep:2986/5000 train_loss:3.7081 train_time:535168ms step_avg:179.83ms\nstep:2987/5000 train_loss:3.6379 train_time:535348ms step_avg:179.83ms\nstep:2988/5000 train_loss:3.7421 train_time:535527ms step_avg:179.83ms\nstep:2989/5000 train_loss:3.7675 train_time:535706ms step_avg:179.83ms\nstep:2990/5000 train_loss:3.7987 train_time:535886ms step_avg:179.83ms\nstep:2991/5000 train_loss:3.6995 train_time:536066ms step_avg:179.83ms\nstep:2992/5000 train_loss:3.7710 train_time:536245ms step_avg:179.83ms\nstep:2993/5000 train_loss:3.7803 train_time:536426ms step_avg:179.83ms\nstep:2994/5000 train_loss:3.7170 train_time:536604ms step_avg:179.83ms\nstep:2995/5000 train_loss:3.6860 train_time:536782ms step_avg:179.83ms\nstep:2996/5000 train_loss:3.7004 train_time:536961ms step_avg:179.83ms\nstep:2997/5000 train_loss:3.8243 train_time:537142ms step_avg:179.83ms\nstep:2998/5000 train_loss:3.7833 train_time:537322ms step_avg:179.83ms\nstep:2999/5000 train_loss:3.8783 train_time:537501ms step_avg:179.83ms\nstep:3000/5000 train_loss:3.7056 train_time:537681ms step_avg:179.83ms\nstep:3001/5000 train_loss:3.6193 train_time:537860ms step_avg:179.83ms\nstep:3002/5000 train_loss:3.6818 train_time:538039ms step_avg:179.83ms\nstep:3003/5000 train_loss:3.8021 train_time:538218ms step_avg:179.83ms\nstep:3004/5000 train_loss:3.7322 train_time:538396ms step_avg:179.83ms\nstep:3005/5000 train_loss:3.6789 train_time:538574ms step_avg:179.82ms\nstep:3006/5000 train_loss:3.8129 train_time:538753ms step_avg:179.82ms\nstep:3007/5000 train_loss:3.6922 train_time:538931ms step_avg:179.82ms\nstep:3008/5000 train_loss:3.7029 train_time:539110ms step_avg:179.82ms\nstep:3009/5000 train_loss:3.7586 train_time:539290ms step_avg:179.82ms\nstep:3010/5000 train_loss:3.6646 train_time:539468ms step_avg:179.82ms\nstep:3011/5000 train_loss:3.6607 train_time:539647ms step_avg:179.82ms\nstep:3012/5000 train_loss:3.7363 train_time:539827ms step_avg:179.82ms\nstep:3013/5000 train_loss:3.6774 train_time:540007ms step_avg:179.82ms\nstep:3014/5000 train_loss:3.7020 train_time:540186ms step_avg:179.82ms\nstep:3015/5000 train_loss:3.7682 train_time:540366ms step_avg:179.82ms\nstep:3016/5000 train_loss:3.7667 train_time:540546ms step_avg:179.82ms\nstep:3017/5000 train_loss:3.6761 train_time:540726ms step_avg:179.82ms\nstep:3018/5000 train_loss:3.6717 train_time:540906ms step_avg:179.82ms\nstep:3019/5000 train_loss:3.8619 train_time:541086ms step_avg:179.82ms\nstep:3020/5000 train_loss:3.7286 train_time:541265ms step_avg:179.82ms\nstep:3021/5000 train_loss:3.8679 train_time:541444ms step_avg:179.82ms\nstep:3022/5000 train_loss:3.7370 train_time:541624ms step_avg:179.82ms\nstep:3023/5000 train_loss:3.5917 train_time:541805ms step_avg:179.82ms\nstep:3024/5000 train_loss:3.8065 train_time:541984ms step_avg:179.82ms\nstep:3025/5000 train_loss:3.7802 train_time:542164ms step_avg:179.82ms\nstep:3026/5000 train_loss:3.6464 train_time:542345ms step_avg:179.82ms\nstep:3027/5000 train_loss:3.8049 train_time:542525ms step_avg:179.82ms\nstep:3028/5000 train_loss:3.6353 train_time:542705ms step_avg:179.82ms\nstep:3029/5000 train_loss:3.6291 train_time:542887ms step_avg:179.82ms\nstep:3030/5000 train_loss:3.8868 train_time:543068ms step_avg:179.82ms\nstep:3031/5000 train_loss:3.8188 train_time:543249ms step_avg:179.82ms\nstep:3032/5000 train_loss:3.8644 train_time:543427ms step_avg:179.82ms\nstep:3033/5000 train_loss:3.6674 train_time:543606ms step_avg:179.82ms\nstep:3034/5000 train_loss:3.9983 train_time:543785ms step_avg:179.82ms\nstep:3035/5000 train_loss:3.4442 train_time:543965ms step_avg:179.82ms\nstep:3036/5000 train_loss:3.7825 train_time:544145ms step_avg:179.82ms\nstep:3037/5000 train_loss:3.7067 train_time:544326ms step_avg:179.82ms\nstep:3038/5000 train_loss:3.7229 train_time:544505ms step_avg:179.82ms\nstep:3039/5000 train_loss:3.7088 train_time:544686ms step_avg:179.82ms\nstep:3040/5000 train_loss:3.7211 train_time:544865ms step_avg:179.82ms\nstep:3041/5000 train_loss:3.7100 train_time:545045ms step_avg:179.82ms\nstep:3042/5000 train_loss:3.6467 train_time:545224ms step_avg:179.82ms\nstep:3043/5000 train_loss:3.7566 train_time:545404ms step_avg:179.82ms\nstep:3044/5000 train_loss:3.8280 train_time:545584ms step_avg:179.82ms\nstep:3045/5000 train_loss:3.7071 train_time:545764ms step_avg:179.82ms\nstep:3046/5000 train_loss:3.8123 train_time:545945ms step_avg:179.82ms\nstep:3047/5000 train_loss:3.7649 train_time:546124ms step_avg:179.82ms\nstep:3048/5000 train_loss:3.7504 train_time:546529ms step_avg:179.90ms\nstep:3049/5000 train_loss:3.7114 train_time:546707ms step_avg:179.90ms\nstep:3050/5000 train_loss:3.5457 train_time:546884ms step_avg:179.90ms\nstep:3051/5000 train_loss:3.6974 train_time:547061ms step_avg:179.90ms\nstep:3052/5000 train_loss:3.6000 train_time:547238ms step_avg:179.89ms\nstep:3053/5000 train_loss:3.6784 train_time:547425ms step_avg:179.90ms\nstep:3054/5000 train_loss:3.6975 train_time:547603ms step_avg:179.90ms\nstep:3055/5000 train_loss:3.8041 train_time:547781ms step_avg:179.90ms\nstep:3056/5000 train_loss:3.5632 train_time:547960ms step_avg:179.89ms\nstep:3057/5000 train_loss:3.7118 train_time:548139ms step_avg:179.89ms\nstep:3058/5000 train_loss:3.8073 train_time:548320ms step_avg:179.89ms\nstep:3059/5000 train_loss:3.7010 train_time:548500ms step_avg:179.90ms\nstep:3060/5000 train_loss:3.6994 train_time:548680ms step_avg:179.90ms\nstep:3061/5000 train_loss:3.7743 train_time:548860ms step_avg:179.89ms\nstep:3062/5000 train_loss:3.4733 train_time:549038ms step_avg:179.89ms\nstep:3063/5000 train_loss:3.7263 train_time:549217ms step_avg:179.89ms\nstep:3064/5000 train_loss:3.6959 train_time:549395ms step_avg:179.89ms\nstep:3065/5000 train_loss:3.6881 train_time:549574ms step_avg:179.89ms\nstep:3066/5000 train_loss:3.8794 train_time:549751ms step_avg:179.89ms\nstep:3067/5000 train_loss:3.6804 train_time:549929ms step_avg:179.89ms\nstep:3068/5000 train_loss:4.7278 train_time:550109ms step_avg:179.89ms\nstep:3069/5000 train_loss:3.6620 train_time:550287ms step_avg:179.89ms\nstep:3070/5000 train_loss:3.7348 train_time:550468ms step_avg:179.89ms\nstep:3071/5000 train_loss:3.7094 train_time:550648ms step_avg:179.89ms\nstep:3072/5000 train_loss:3.6752 train_time:550827ms step_avg:179.89ms\nstep:3073/5000 train_loss:3.7108 train_time:551006ms step_avg:179.89ms\nstep:3074/5000 train_loss:3.7008 train_time:551186ms step_avg:179.89ms\nstep:3075/5000 train_loss:3.8903 train_time:551365ms step_avg:179.89ms\nstep:3076/5000 train_loss:3.7561 train_time:551545ms step_avg:179.89ms\nstep:3077/5000 train_loss:3.7286 train_time:551725ms step_avg:179.89ms\nstep:3078/5000 train_loss:3.7211 train_time:551904ms step_avg:179.89ms\nstep:3079/5000 train_loss:3.7943 train_time:552083ms step_avg:179.89ms\nstep:3080/5000 train_loss:3.7986 train_time:552262ms step_avg:179.89ms\nstep:3081/5000 train_loss:3.6430 train_time:552442ms step_avg:179.89ms\nstep:3082/5000 train_loss:3.7483 train_time:552620ms step_avg:179.89ms\nstep:3083/5000 train_loss:3.7081 train_time:552800ms step_avg:179.89ms\nstep:3084/5000 train_loss:4.3234 train_time:552979ms step_avg:179.89ms\nstep:3085/5000 train_loss:3.6542 train_time:553158ms step_avg:179.89ms\nstep:3086/5000 train_loss:3.7993 train_time:553337ms step_avg:179.89ms\nstep:3087/5000 train_loss:3.5096 train_time:553516ms step_avg:179.89ms\nstep:3088/5000 train_loss:3.7500 train_time:553694ms step_avg:179.89ms\nstep:3089/5000 train_loss:3.8416 train_time:553873ms step_avg:179.89ms\nstep:3090/5000 train_loss:4.0250 train_time:554052ms step_avg:179.89ms\nstep:3091/5000 train_loss:3.7458 train_time:554231ms step_avg:179.89ms\nstep:3092/5000 train_loss:3.7330 train_time:554410ms step_avg:179.89ms\nstep:3093/5000 train_loss:3.7250 train_time:554588ms step_avg:179.89ms\nstep:3094/5000 train_loss:3.5989 train_time:554766ms step_avg:179.89ms\nstep:3095/5000 train_loss:3.5947 train_time:554947ms step_avg:179.89ms\nstep:3096/5000 train_loss:3.7176 train_time:555125ms step_avg:179.89ms\nstep:3097/5000 train_loss:3.7387 train_time:555305ms step_avg:179.88ms\nstep:3098/5000 train_loss:3.7942 train_time:555485ms step_avg:179.88ms\nstep:3099/5000 train_loss:3.6544 train_time:555664ms step_avg:179.88ms\nstep:3100/5000 train_loss:3.8513 train_time:555843ms step_avg:179.88ms\nstep:3101/5000 train_loss:3.5733 train_time:556025ms step_avg:179.89ms\nstep:3102/5000 train_loss:3.7728 train_time:556207ms step_avg:179.89ms\nstep:3103/5000 train_loss:4.0578 train_time:556388ms step_avg:179.89ms\nstep:3104/5000 train_loss:3.6527 train_time:556567ms step_avg:179.89ms\nstep:3105/5000 train_loss:3.5477 train_time:556747ms step_avg:179.89ms\nstep:3106/5000 train_loss:3.7474 train_time:556927ms step_avg:179.89ms\nstep:3107/5000 train_loss:3.5845 train_time:557106ms step_avg:179.89ms\nstep:3108/5000 train_loss:3.7581 train_time:557286ms step_avg:179.89ms\nstep:3109/5000 train_loss:3.7530 train_time:557466ms step_avg:179.89ms\nstep:3110/5000 train_loss:3.5638 train_time:557645ms step_avg:179.89ms\nstep:3111/5000 train_loss:3.6905 train_time:557826ms step_avg:179.89ms\nstep:3112/5000 train_loss:3.7734 train_time:558005ms step_avg:179.89ms\nstep:3113/5000 train_loss:3.6475 train_time:558184ms step_avg:179.89ms\nstep:3114/5000 train_loss:3.5663 train_time:558364ms step_avg:179.89ms\nstep:3115/5000 train_loss:3.8071 train_time:558544ms step_avg:179.89ms\nstep:3116/5000 train_loss:3.7123 train_time:558723ms step_avg:179.89ms\nstep:3117/5000 train_loss:3.5930 train_time:558905ms step_avg:179.89ms\nstep:3118/5000 train_loss:3.6191 train_time:559084ms step_avg:179.89ms\nstep:3119/5000 train_loss:3.7802 train_time:559263ms step_avg:179.89ms\nstep:3120/5000 train_loss:3.7151 train_time:559443ms step_avg:179.89ms\nstep:3121/5000 train_loss:3.8116 train_time:559623ms step_avg:179.89ms\nstep:3122/5000 train_loss:3.7048 train_time:559804ms step_avg:179.89ms\nstep:3123/5000 train_loss:3.6916 train_time:559987ms step_avg:179.89ms\nstep:3124/5000 train_loss:3.6100 train_time:560167ms step_avg:179.89ms\nstep:3125/5000 train_loss:3.7686 train_time:560347ms step_avg:179.89ms\nstep:3126/5000 train_loss:3.7398 train_time:560527ms step_avg:179.89ms\nstep:3127/5000 train_loss:3.6789 train_time:560706ms step_avg:179.89ms\nstep:3128/5000 train_loss:3.7479 train_time:560886ms step_avg:179.89ms\nstep:3129/5000 train_loss:3.7371 train_time:561067ms step_avg:179.89ms\nstep:3130/5000 train_loss:3.8080 train_time:561246ms step_avg:179.89ms\nstep:3131/5000 train_loss:3.5356 train_time:561424ms step_avg:179.89ms\nstep:3132/5000 train_loss:3.7207 train_time:561603ms step_avg:179.89ms\nstep:3133/5000 train_loss:3.6121 train_time:561783ms step_avg:179.89ms\nstep:3134/5000 train_loss:3.7154 train_time:561961ms step_avg:179.89ms\nstep:3135/5000 train_loss:3.6595 train_time:562142ms step_avg:179.89ms\nstep:3136/5000 train_loss:3.6762 train_time:562322ms step_avg:179.89ms\nstep:3137/5000 train_loss:3.8148 train_time:562501ms step_avg:179.89ms\nstep:3138/5000 train_loss:3.7105 train_time:562680ms step_avg:179.88ms\nstep:3139/5000 train_loss:3.6537 train_time:562859ms step_avg:179.88ms\nstep:3140/5000 train_loss:3.6770 train_time:563038ms step_avg:179.88ms\nstep:3141/5000 train_loss:3.6362 train_time:563218ms step_avg:179.88ms\nstep:3142/5000 train_loss:3.7186 train_time:563396ms step_avg:179.88ms\nstep:3143/5000 train_loss:3.5900 train_time:563575ms step_avg:179.88ms\nstep:3144/5000 train_loss:3.7415 train_time:563754ms step_avg:179.88ms\nstep:3145/5000 train_loss:3.7367 train_time:563932ms step_avg:179.88ms\nstep:3146/5000 train_loss:3.6940 train_time:564111ms step_avg:179.88ms\nstep:3147/5000 train_loss:3.6639 train_time:564290ms step_avg:179.88ms\nstep:3148/5000 train_loss:3.6525 train_time:564473ms step_avg:179.88ms\nstep:3149/5000 train_loss:3.5464 train_time:564651ms step_avg:179.88ms\nstep:3150/5000 train_loss:3.5950 train_time:564830ms step_avg:179.88ms\nstep:3151/5000 train_loss:3.7687 train_time:565009ms step_avg:179.88ms\nstep:3152/5000 train_loss:3.7289 train_time:565188ms step_avg:179.88ms\nstep:3153/5000 train_loss:3.8130 train_time:565367ms step_avg:179.88ms\nstep:3154/5000 train_loss:3.8584 train_time:565547ms step_avg:179.88ms\nstep:3155/5000 train_loss:3.6744 train_time:565728ms step_avg:179.88ms\nstep:3156/5000 train_loss:3.6966 train_time:565907ms step_avg:179.88ms\nstep:3157/5000 train_loss:3.7044 train_time:566087ms step_avg:179.88ms\nstep:3158/5000 train_loss:3.6381 train_time:566264ms step_avg:179.88ms\nstep:3159/5000 train_loss:3.6501 train_time:566444ms step_avg:179.88ms\nstep:3160/5000 train_loss:3.6186 train_time:566623ms step_avg:179.88ms\nstep:3161/5000 train_loss:3.6881 train_time:566804ms step_avg:179.88ms\nstep:3162/5000 train_loss:3.6968 train_time:566985ms step_avg:179.88ms\nstep:3163/5000 train_loss:3.7075 train_time:567164ms step_avg:179.88ms\nstep:3164/5000 train_loss:3.6737 train_time:567344ms step_avg:179.88ms\nstep:3165/5000 train_loss:4.0561 train_time:567523ms step_avg:179.88ms\nstep:3166/5000 train_loss:3.7614 train_time:567702ms step_avg:179.88ms\nstep:3167/5000 train_loss:3.6594 train_time:567884ms step_avg:179.88ms\nstep:3168/5000 train_loss:3.7323 train_time:568064ms step_avg:179.88ms\nstep:3169/5000 train_loss:3.7227 train_time:568244ms step_avg:179.88ms\nstep:3170/5000 train_loss:3.8955 train_time:568423ms step_avg:179.88ms\nstep:3171/5000 train_loss:3.9267 train_time:568602ms step_avg:179.88ms\nstep:3172/5000 train_loss:3.7633 train_time:568782ms step_avg:179.88ms\nstep:3173/5000 train_loss:3.6496 train_time:568962ms step_avg:179.88ms\nstep:3174/5000 train_loss:3.9114 train_time:569142ms step_avg:179.88ms\nstep:3175/5000 train_loss:3.8944 train_time:569320ms step_avg:179.88ms\nstep:3176/5000 train_loss:3.7637 train_time:569500ms step_avg:179.88ms\nstep:3177/5000 train_loss:3.8241 train_time:569678ms step_avg:179.88ms\nstep:3178/5000 train_loss:3.6171 train_time:569858ms step_avg:179.88ms\nstep:3179/5000 train_loss:3.6149 train_time:570036ms step_avg:179.88ms\nstep:3180/5000 train_loss:3.7099 train_time:570215ms step_avg:179.88ms\nstep:3181/5000 train_loss:3.7406 train_time:570393ms step_avg:179.88ms\nstep:3182/5000 train_loss:3.7387 train_time:570572ms step_avg:179.88ms\nstep:3183/5000 train_loss:3.6886 train_time:570751ms step_avg:179.88ms\nstep:3184/5000 train_loss:3.8218 train_time:570932ms step_avg:179.88ms\nstep:3185/5000 train_loss:4.8077 train_time:571110ms step_avg:179.88ms\nstep:3186/5000 train_loss:3.8044 train_time:571289ms step_avg:179.88ms\nstep:3187/5000 train_loss:3.7091 train_time:571470ms step_avg:179.88ms\nstep:3188/5000 train_loss:3.7302 train_time:571648ms step_avg:179.88ms\nstep:3189/5000 train_loss:3.6644 train_time:571828ms step_avg:179.88ms\nstep:3190/5000 train_loss:3.9554 train_time:572012ms step_avg:179.88ms\nstep:3191/5000 train_loss:4.1317 train_time:572190ms step_avg:179.88ms\nstep:3192/5000 train_loss:3.5318 train_time:572369ms step_avg:179.88ms\nstep:3193/5000 train_loss:3.6828 train_time:572548ms step_avg:179.88ms\nstep:3194/5000 train_loss:3.6674 train_time:572729ms step_avg:179.88ms\nstep:3195/5000 train_loss:3.8485 train_time:572908ms step_avg:179.88ms\nstep:3196/5000 train_loss:3.6167 train_time:573090ms step_avg:179.88ms\nstep:3197/5000 train_loss:3.7125 train_time:573270ms step_avg:179.88ms\nstep:3198/5000 train_loss:3.8074 train_time:573448ms step_avg:179.88ms\nstep:3199/5000 train_loss:3.8696 train_time:573627ms step_avg:179.88ms\nstep:3200/5000 train_loss:4.2779 train_time:573808ms step_avg:179.88ms\nstep:3201/5000 train_loss:3.6750 train_time:573987ms step_avg:179.88ms\nstep:3202/5000 train_loss:3.6969 train_time:574166ms step_avg:179.88ms\nstep:3203/5000 train_loss:3.8165 train_time:574349ms step_avg:179.88ms\nstep:3204/5000 train_loss:3.8606 train_time:574527ms step_avg:179.88ms\nstep:3205/5000 train_loss:3.7107 train_time:574708ms step_avg:179.88ms\nstep:3206/5000 train_loss:3.6944 train_time:574887ms step_avg:179.88ms\nstep:3207/5000 train_loss:3.6296 train_time:575067ms step_avg:179.88ms\nstep:3208/5000 train_loss:3.6913 train_time:575247ms step_avg:179.88ms\nstep:3209/5000 train_loss:3.7983 train_time:575424ms step_avg:179.88ms\nstep:3210/5000 train_loss:3.7811 train_time:575605ms step_avg:179.88ms\nstep:3211/5000 train_loss:3.9532 train_time:575784ms step_avg:179.88ms\nstep:3212/5000 train_loss:3.7557 train_time:575967ms step_avg:179.88ms\nstep:3213/5000 train_loss:3.8743 train_time:576148ms step_avg:179.88ms\nstep:3214/5000 train_loss:3.6444 train_time:576326ms step_avg:179.88ms\nstep:3215/5000 train_loss:3.7263 train_time:576506ms step_avg:179.88ms\nstep:3216/5000 train_loss:3.7286 train_time:576685ms step_avg:179.88ms\nstep:3217/5000 train_loss:3.6728 train_time:576865ms step_avg:179.88ms\nstep:3218/5000 train_loss:3.6827 train_time:577044ms step_avg:179.88ms\nstep:3219/5000 train_loss:3.6838 train_time:577223ms step_avg:179.88ms\nstep:3220/5000 train_loss:3.6674 train_time:577404ms step_avg:179.88ms\nstep:3221/5000 train_loss:3.6651 train_time:577584ms step_avg:179.88ms\nstep:3222/5000 train_loss:3.7730 train_time:577765ms step_avg:179.88ms\nstep:3223/5000 train_loss:3.7114 train_time:577944ms step_avg:179.88ms\nstep:3224/5000 train_loss:3.6675 train_time:578125ms step_avg:179.88ms\nstep:3225/5000 train_loss:3.6772 train_time:578307ms step_avg:179.88ms\nstep:3226/5000 train_loss:3.6699 train_time:578487ms step_avg:179.88ms\nstep:3227/5000 train_loss:3.7630 train_time:578666ms step_avg:179.88ms\nstep:3228/5000 train_loss:3.7168 train_time:578845ms step_avg:179.88ms\nstep:3229/5000 train_loss:3.7898 train_time:579026ms step_avg:179.88ms\nstep:3230/5000 train_loss:3.6925 train_time:579207ms step_avg:179.88ms\nstep:3231/5000 train_loss:3.6125 train_time:579390ms step_avg:179.88ms\nstep:3232/5000 train_loss:3.7617 train_time:579568ms step_avg:179.88ms\nstep:3233/5000 train_loss:3.7814 train_time:579748ms step_avg:179.88ms\nstep:3234/5000 train_loss:3.7836 train_time:579928ms step_avg:179.88ms\nstep:3235/5000 train_loss:3.6000 train_time:580108ms step_avg:179.88ms\nstep:3236/5000 train_loss:3.9865 train_time:580287ms step_avg:179.88ms\nstep:3237/5000 train_loss:3.7213 train_time:580468ms step_avg:179.88ms\nstep:3238/5000 train_loss:3.7046 train_time:580646ms step_avg:179.88ms\nstep:3239/5000 train_loss:3.7436 train_time:580825ms step_avg:179.88ms\nstep:3240/5000 train_loss:3.8161 train_time:581005ms step_avg:179.88ms\nstep:3241/5000 train_loss:3.8102 train_time:581184ms step_avg:179.88ms\nstep:3242/5000 train_loss:3.6367 train_time:581363ms step_avg:179.88ms\nstep:3243/5000 train_loss:4.1047 train_time:581544ms step_avg:179.88ms\nstep:3244/5000 train_loss:3.6593 train_time:581723ms step_avg:179.88ms\nstep:3245/5000 train_loss:3.7608 train_time:581903ms step_avg:179.88ms\nstep:3246/5000 train_loss:3.7155 train_time:582082ms step_avg:179.88ms\nstep:3247/5000 train_loss:3.6487 train_time:582261ms step_avg:179.88ms\nstep:3248/5000 train_loss:3.7479 train_time:582440ms step_avg:179.88ms\nstep:3249/5000 train_loss:3.7516 train_time:582620ms step_avg:179.88ms\nstep:3250/5000 train_loss:3.7385 train_time:582798ms step_avg:179.88ms\nstep:3251/5000 train_loss:3.7459 train_time:582977ms step_avg:179.88ms\nstep:3252/5000 train_loss:3.7505 train_time:583156ms step_avg:179.88ms\nstep:3253/5000 train_loss:3.9509 train_time:583334ms step_avg:179.87ms\nstep:3254/5000 train_loss:3.7506 train_time:583514ms step_avg:179.87ms\nstep:3255/5000 train_loss:3.7757 train_time:583692ms step_avg:179.87ms\nstep:3256/5000 train_loss:3.7084 train_time:583871ms step_avg:179.87ms\nstep:3257/5000 train_loss:3.7212 train_time:584049ms step_avg:179.87ms\nstep:3258/5000 train_loss:3.7717 train_time:584230ms step_avg:179.87ms\nstep:3259/5000 train_loss:3.6903 train_time:584409ms step_avg:179.87ms\nstep:3260/5000 train_loss:3.5751 train_time:584588ms step_avg:179.87ms\nstep:3261/5000 train_loss:3.9534 train_time:584769ms step_avg:179.87ms\nstep:3262/5000 train_loss:3.7166 train_time:584949ms step_avg:179.87ms\nstep:3263/5000 train_loss:3.9019 train_time:585129ms step_avg:179.87ms\nstep:3264/5000 train_loss:3.9122 train_time:585310ms step_avg:179.87ms\nstep:3265/5000 train_loss:3.6991 train_time:585490ms step_avg:179.87ms\nstep:3266/5000 train_loss:3.7089 train_time:585671ms step_avg:179.87ms\nstep:3267/5000 train_loss:3.6753 train_time:585850ms step_avg:179.87ms\nstep:3268/5000 train_loss:3.7009 train_time:586028ms step_avg:179.87ms\nstep:3269/5000 train_loss:3.7989 train_time:586209ms step_avg:179.87ms\nstep:3270/5000 train_loss:3.8067 train_time:586388ms step_avg:179.87ms\nstep:3271/5000 train_loss:3.6640 train_time:586566ms step_avg:179.87ms\nstep:3272/5000 train_loss:3.6891 train_time:586746ms step_avg:179.87ms\nstep:3273/5000 train_loss:3.6772 train_time:586926ms step_avg:179.87ms\nstep:3274/5000 train_loss:3.9124 train_time:587109ms step_avg:179.87ms\nstep:3275/5000 train_loss:3.6574 train_time:587287ms step_avg:179.87ms\nstep:3276/5000 train_loss:3.6878 train_time:587467ms step_avg:179.87ms\nstep:3277/5000 train_loss:3.7266 train_time:587647ms step_avg:179.87ms\nstep:3278/5000 train_loss:3.6714 train_time:587827ms step_avg:179.87ms\nstep:3279/5000 train_loss:3.7069 train_time:588006ms step_avg:179.87ms\nstep:3280/5000 train_loss:3.6955 train_time:588187ms step_avg:179.87ms\nstep:3281/5000 train_loss:3.6440 train_time:588366ms step_avg:179.87ms\nstep:3282/5000 train_loss:3.8640 train_time:588546ms step_avg:179.87ms\nstep:3283/5000 train_loss:3.6966 train_time:588726ms step_avg:179.87ms\nstep:3284/5000 train_loss:3.5599 train_time:588906ms step_avg:179.87ms\nstep:3285/5000 train_loss:3.6151 train_time:589086ms step_avg:179.87ms\nstep:3286/5000 train_loss:3.6518 train_time:589266ms step_avg:179.87ms\nstep:3287/5000 train_loss:3.7771 train_time:589447ms step_avg:179.87ms\nstep:3288/5000 train_loss:3.6826 train_time:589626ms step_avg:179.87ms\nstep:3289/5000 train_loss:3.6283 train_time:589806ms step_avg:179.87ms\nstep:3290/5000 train_loss:3.6882 train_time:589987ms step_avg:179.87ms\nstep:3291/5000 train_loss:3.7306 train_time:590166ms step_avg:179.87ms\nstep:3292/5000 train_loss:3.7946 train_time:590345ms step_avg:179.87ms\nstep:3293/5000 train_loss:3.5970 train_time:590525ms step_avg:179.87ms\nstep:3294/5000 train_loss:3.8198 train_time:590704ms step_avg:179.87ms\nstep:3295/5000 train_loss:3.6879 train_time:590885ms step_avg:179.87ms\nstep:3296/5000 train_loss:3.5216 train_time:591066ms step_avg:179.87ms\nstep:3297/5000 train_loss:3.6353 train_time:591244ms step_avg:179.87ms\nstep:3298/5000 train_loss:3.7267 train_time:591424ms step_avg:179.87ms\nstep:3299/5000 train_loss:3.6649 train_time:591607ms step_avg:179.87ms\nstep:3300/5000 train_loss:3.7244 train_time:591786ms step_avg:179.87ms\nstep:3301/5000 train_loss:3.6621 train_time:591966ms step_avg:179.87ms\nstep:3302/5000 train_loss:3.6535 train_time:592145ms step_avg:179.87ms\nstep:3303/5000 train_loss:3.7300 train_time:592325ms step_avg:179.87ms\nstep:3304/5000 train_loss:3.8379 train_time:592505ms step_avg:179.87ms\nstep:3305/5000 train_loss:3.7148 train_time:592684ms step_avg:179.87ms\nstep:3306/5000 train_loss:3.7557 train_time:592865ms step_avg:179.87ms\nstep:3307/5000 train_loss:3.6724 train_time:593044ms step_avg:179.87ms\nstep:3308/5000 train_loss:3.6008 train_time:593224ms step_avg:179.87ms\nstep:3309/5000 train_loss:3.5510 train_time:593403ms step_avg:179.87ms\nstep:3310/5000 train_loss:3.6455 train_time:593583ms step_avg:179.87ms\nstep:3311/5000 train_loss:3.7567 train_time:593764ms step_avg:179.87ms\nstep:3312/5000 train_loss:3.6410 train_time:593944ms step_avg:179.87ms\nstep:3313/5000 train_loss:3.6376 train_time:594123ms step_avg:179.87ms\nstep:3314/5000 train_loss:3.8891 train_time:594303ms step_avg:179.87ms\nstep:3315/5000 train_loss:3.6645 train_time:594484ms step_avg:179.87ms\nstep:3316/5000 train_loss:3.6733 train_time:594663ms step_avg:179.87ms\nstep:3317/5000 train_loss:3.6492 train_time:594842ms step_avg:179.87ms\nstep:3318/5000 train_loss:3.7632 train_time:595022ms step_avg:179.87ms\nstep:3319/5000 train_loss:3.6953 train_time:595201ms step_avg:179.87ms\nstep:3320/5000 train_loss:3.7447 train_time:595381ms step_avg:179.87ms\nstep:3321/5000 train_loss:3.6218 train_time:595561ms step_avg:179.87ms\nstep:3322/5000 train_loss:3.8243 train_time:595741ms step_avg:179.87ms\nstep:3323/5000 train_loss:3.6183 train_time:595921ms step_avg:179.87ms\nstep:3324/5000 train_loss:3.5830 train_time:596100ms step_avg:179.87ms\nstep:3325/5000 train_loss:3.7022 train_time:596280ms step_avg:179.87ms\nstep:3326/5000 train_loss:3.6771 train_time:596459ms step_avg:179.87ms\nstep:3327/5000 train_loss:3.7494 train_time:596639ms step_avg:179.87ms\nstep:3328/5000 train_loss:3.7418 train_time:596821ms step_avg:179.87ms\nstep:3329/5000 train_loss:3.6829 train_time:597002ms step_avg:179.87ms\nstep:3330/5000 train_loss:3.7317 train_time:597181ms step_avg:179.87ms\nstep:3331/5000 train_loss:3.9550 train_time:597360ms step_avg:179.87ms\nstep:3332/5000 train_loss:3.7543 train_time:597538ms step_avg:179.87ms\nstep:3333/5000 train_loss:3.6421 train_time:597717ms step_avg:179.87ms\nstep:3334/5000 train_loss:3.6616 train_time:597896ms step_avg:179.87ms\nstep:3335/5000 train_loss:3.7335 train_time:598075ms step_avg:179.87ms\nstep:3336/5000 train_loss:3.7455 train_time:598254ms step_avg:179.87ms\nstep:3337/5000 train_loss:3.6876 train_time:598432ms step_avg:179.87ms\nstep:3338/5000 train_loss:3.7931 train_time:598611ms step_avg:179.87ms\nstep:3339/5000 train_loss:3.8222 train_time:598789ms step_avg:179.87ms\nstep:3340/5000 train_loss:3.6045 train_time:598969ms step_avg:179.87ms\nstep:3341/5000 train_loss:3.7904 train_time:599149ms step_avg:179.87ms\nstep:3342/5000 train_loss:3.6252 train_time:599327ms step_avg:179.87ms\nstep:3343/5000 train_loss:3.6784 train_time:599506ms step_avg:179.87ms\nstep:3344/5000 train_loss:3.8609 train_time:599685ms step_avg:179.87ms\nstep:3345/5000 train_loss:3.6178 train_time:599864ms step_avg:179.87ms\nstep:3346/5000 train_loss:3.6091 train_time:600044ms step_avg:179.87ms\nstep:3347/5000 train_loss:3.6067 train_time:600223ms step_avg:179.87ms\nstep:3348/5000 train_loss:3.6051 train_time:600402ms step_avg:179.87ms\nstep:3349/5000 train_loss:3.7536 train_time:600582ms step_avg:179.87ms\nstep:3350/5000 train_loss:3.8660 train_time:600762ms step_avg:179.87ms\nstep:3351/5000 train_loss:3.6477 train_time:600941ms step_avg:179.87ms\nstep:3352/5000 train_loss:3.5358 train_time:601120ms step_avg:179.87ms\nstep:3353/5000 train_loss:3.5592 train_time:601299ms step_avg:179.87ms\nstep:3354/5000 train_loss:3.5569 train_time:601480ms step_avg:179.87ms\nstep:3355/5000 train_loss:3.7486 train_time:601659ms step_avg:179.87ms\nstep:3356/5000 train_loss:3.7647 train_time:601839ms step_avg:179.87ms\nstep:3357/5000 train_loss:3.6809 train_time:602019ms step_avg:179.87ms\nstep:3358/5000 train_loss:3.7359 train_time:602199ms step_avg:179.87ms\nstep:3359/5000 train_loss:3.6571 train_time:602378ms step_avg:179.87ms\nstep:3360/5000 train_loss:3.6901 train_time:602557ms step_avg:179.87ms\nstep:3361/5000 train_loss:3.6300 train_time:602736ms step_avg:179.87ms\nstep:3362/5000 train_loss:3.5723 train_time:602914ms step_avg:179.87ms\nstep:3363/5000 train_loss:3.7241 train_time:603093ms step_avg:179.87ms\nstep:3364/5000 train_loss:3.7700 train_time:603271ms step_avg:179.87ms\nstep:3365/5000 train_loss:3.7395 train_time:603450ms step_avg:179.87ms\nstep:3366/5000 train_loss:3.6190 train_time:603628ms step_avg:179.87ms\nstep:3367/5000 train_loss:3.7648 train_time:603807ms step_avg:179.87ms\nstep:3368/5000 train_loss:3.6696 train_time:603988ms step_avg:179.87ms\nstep:3369/5000 train_loss:3.6570 train_time:604168ms step_avg:179.87ms\nstep:3370/5000 train_loss:3.7114 train_time:604350ms step_avg:179.87ms\nstep:3371/5000 train_loss:3.7039 train_time:604529ms step_avg:179.87ms\nstep:3372/5000 train_loss:3.6868 train_time:604708ms step_avg:179.87ms\nstep:3373/5000 train_loss:3.7719 train_time:604886ms step_avg:179.87ms\nstep:3374/5000 train_loss:3.7249 train_time:605066ms step_avg:179.86ms\nstep:3375/5000 train_loss:3.5435 train_time:605246ms step_avg:179.87ms\nstep:3376/5000 train_loss:3.6615 train_time:605426ms step_avg:179.87ms\nstep:3377/5000 train_loss:3.6648 train_time:605609ms step_avg:179.87ms\nstep:3378/5000 train_loss:3.7183 train_time:605789ms step_avg:179.87ms\nstep:3379/5000 train_loss:3.7941 train_time:605968ms step_avg:179.87ms\nstep:3380/5000 train_loss:3.6940 train_time:606148ms step_avg:179.87ms\nstep:3381/5000 train_loss:3.4867 train_time:606327ms step_avg:179.87ms\nstep:3382/5000 train_loss:3.5786 train_time:606506ms step_avg:179.87ms\nstep:3383/5000 train_loss:3.6786 train_time:606689ms step_avg:179.87ms\nstep:3384/5000 train_loss:3.6951 train_time:606868ms step_avg:179.87ms\nstep:3385/5000 train_loss:3.7735 train_time:607047ms step_avg:179.87ms\nstep:3386/5000 train_loss:3.8093 train_time:607227ms step_avg:179.87ms\nstep:3387/5000 train_loss:3.6691 train_time:607408ms step_avg:179.87ms\nstep:3388/5000 train_loss:3.7347 train_time:607589ms step_avg:179.87ms\nstep:3389/5000 train_loss:3.7201 train_time:607772ms step_avg:179.87ms\nstep:3390/5000 train_loss:3.6748 train_time:607950ms step_avg:179.87ms\nstep:3391/5000 train_loss:3.7064 train_time:608127ms step_avg:179.87ms\nstep:3392/5000 train_loss:3.6828 train_time:608307ms step_avg:179.87ms\nstep:3393/5000 train_loss:3.6692 train_time:608488ms step_avg:179.87ms\nstep:3394/5000 train_loss:3.6383 train_time:608669ms step_avg:179.87ms\nstep:3395/5000 train_loss:3.7532 train_time:608849ms step_avg:179.87ms\nstep:3396/5000 train_loss:3.5518 train_time:609029ms step_avg:179.87ms\nstep:3397/5000 train_loss:3.7579 train_time:609209ms step_avg:179.87ms\nstep:3398/5000 train_loss:3.7752 train_time:609387ms step_avg:179.87ms\nstep:3399/5000 train_loss:3.8177 train_time:609566ms step_avg:179.87ms\nstep:3400/5000 train_loss:3.7401 train_time:609746ms step_avg:179.87ms\nstep:3401/5000 train_loss:3.7059 train_time:609927ms step_avg:179.87ms\nstep:3402/5000 train_loss:3.6378 train_time:610109ms step_avg:179.87ms\nstep:3403/5000 train_loss:3.7274 train_time:610289ms step_avg:179.87ms\nstep:3404/5000 train_loss:3.6330 train_time:610468ms step_avg:179.87ms\nstep:3405/5000 train_loss:3.7385 train_time:610650ms step_avg:179.87ms\nstep:3406/5000 train_loss:3.5991 train_time:610828ms step_avg:179.87ms\nstep:3407/5000 train_loss:3.9951 train_time:611008ms step_avg:179.87ms\nstep:3408/5000 train_loss:3.7298 train_time:611188ms step_avg:179.87ms\nstep:3409/5000 train_loss:3.8144 train_time:611368ms step_avg:179.87ms\nstep:3410/5000 train_loss:3.8845 train_time:611547ms step_avg:179.87ms\nstep:3411/5000 train_loss:3.6995 train_time:611728ms step_avg:179.87ms\nstep:3412/5000 train_loss:3.5011 train_time:611907ms step_avg:179.87ms\nstep:3413/5000 train_loss:3.8861 train_time:612087ms step_avg:179.87ms\nstep:3414/5000 train_loss:3.6026 train_time:612267ms step_avg:179.87ms\nstep:3415/5000 train_loss:3.6858 train_time:612447ms step_avg:179.87ms\nstep:3416/5000 train_loss:3.7986 train_time:612627ms step_avg:179.87ms\nstep:3417/5000 train_loss:3.6242 train_time:612806ms step_avg:179.87ms\nstep:3418/5000 train_loss:3.7157 train_time:612987ms step_avg:179.87ms\nstep:3419/5000 train_loss:3.6992 train_time:613167ms step_avg:179.87ms\nstep:3420/5000 train_loss:3.7157 train_time:613349ms step_avg:179.87ms\nstep:3421/5000 train_loss:3.7238 train_time:613530ms step_avg:179.87ms\nstep:3422/5000 train_loss:3.7852 train_time:613711ms step_avg:179.87ms\nstep:3423/5000 train_loss:3.6868 train_time:613889ms step_avg:179.87ms\nstep:3424/5000 train_loss:3.4868 train_time:614067ms step_avg:179.87ms\nstep:3425/5000 train_loss:3.7179 train_time:614246ms step_avg:179.87ms\nstep:3426/5000 train_loss:3.6776 train_time:614426ms step_avg:179.87ms\nstep:3427/5000 train_loss:3.6570 train_time:614607ms step_avg:179.87ms\nstep:3428/5000 train_loss:3.7975 train_time:614787ms step_avg:179.87ms\nstep:3429/5000 train_loss:3.7372 train_time:614968ms step_avg:179.87ms\nstep:3430/5000 train_loss:3.6988 train_time:615147ms step_avg:179.87ms\nstep:3431/5000 train_loss:3.7026 train_time:615326ms step_avg:179.87ms\nstep:3432/5000 train_loss:3.8233 train_time:615505ms step_avg:179.87ms\nstep:3433/5000 train_loss:3.6907 train_time:615685ms step_avg:179.87ms\nstep:3434/5000 train_loss:3.7452 train_time:615867ms step_avg:179.87ms\nstep:3435/5000 train_loss:3.7433 train_time:616048ms step_avg:179.87ms\nstep:3436/5000 train_loss:3.5532 train_time:616227ms step_avg:179.87ms\nstep:3437/5000 train_loss:3.6878 train_time:616408ms step_avg:179.87ms\nstep:3438/5000 train_loss:3.6352 train_time:616588ms step_avg:179.87ms\nstep:3439/5000 train_loss:3.4899 train_time:616769ms step_avg:179.87ms\nstep:3440/5000 train_loss:3.6956 train_time:616949ms step_avg:179.87ms\nstep:3441/5000 train_loss:4.0255 train_time:617129ms step_avg:179.87ms\nstep:3442/5000 train_loss:3.7239 train_time:617309ms step_avg:179.87ms\nstep:3443/5000 train_loss:3.5716 train_time:617492ms step_avg:179.87ms\nstep:3444/5000 train_loss:3.7871 train_time:617667ms step_avg:179.87ms\nstep:3445/5000 train_loss:3.8106 train_time:617846ms step_avg:179.87ms\nstep:3446/5000 train_loss:3.7369 train_time:618026ms step_avg:179.87ms\nstep:3447/5000 train_loss:3.6913 train_time:618206ms step_avg:179.87ms\nstep:3448/5000 train_loss:3.5599 train_time:618385ms step_avg:179.87ms\nstep:3449/5000 train_loss:3.7036 train_time:618565ms step_avg:179.87ms\nstep:3450/5000 train_loss:3.7912 train_time:618744ms step_avg:179.87ms\nstep:3451/5000 train_loss:3.5058 train_time:618923ms step_avg:179.87ms\nstep:3452/5000 train_loss:3.6459 train_time:619104ms step_avg:179.87ms\nstep:3453/5000 train_loss:3.6238 train_time:619285ms step_avg:179.87ms\nstep:3454/5000 train_loss:3.7396 train_time:619465ms step_avg:179.87ms\nstep:3455/5000 train_loss:3.5598 train_time:619643ms step_avg:179.87ms\nstep:3456/5000 train_loss:3.9414 train_time:619822ms step_avg:179.87ms\nstep:3457/5000 train_loss:3.6730 train_time:620002ms step_avg:179.87ms\nstep:3458/5000 train_loss:3.7219 train_time:620181ms step_avg:179.87ms\nstep:3459/5000 train_loss:3.6540 train_time:620361ms step_avg:179.87ms\nstep:3460/5000 train_loss:3.9133 train_time:620541ms step_avg:179.87ms\nstep:3461/5000 train_loss:3.6767 train_time:620721ms step_avg:179.87ms\nstep:3462/5000 train_loss:3.5483 train_time:620901ms step_avg:179.87ms\nstep:3463/5000 train_loss:3.8456 train_time:621081ms step_avg:179.87ms\nstep:3464/5000 train_loss:3.7041 train_time:621260ms step_avg:179.87ms\nstep:3465/5000 train_loss:3.8456 train_time:621438ms step_avg:179.87ms\nstep:3466/5000 train_loss:3.7406 train_time:621617ms step_avg:179.87ms\nstep:3467/5000 train_loss:3.6110 train_time:621796ms step_avg:179.87ms\nstep:3468/5000 train_loss:3.6620 train_time:621975ms step_avg:179.87ms\nstep:3469/5000 train_loss:4.0577 train_time:622153ms step_avg:179.87ms\nstep:3470/5000 train_loss:4.0305 train_time:622331ms step_avg:179.86ms\nstep:3471/5000 train_loss:3.6427 train_time:622510ms step_avg:179.86ms\nstep:3472/5000 train_loss:3.8727 train_time:622688ms step_avg:179.86ms\nstep:3473/5000 train_loss:3.6320 train_time:622867ms step_avg:179.86ms\nstep:3474/5000 train_loss:3.6016 train_time:623047ms step_avg:179.86ms\nstep:3475/5000 train_loss:3.8168 train_time:623227ms step_avg:179.86ms\nstep:3476/5000 train_loss:3.6025 train_time:623406ms step_avg:179.86ms\nstep:3477/5000 train_loss:3.6924 train_time:623585ms step_avg:179.86ms\nstep:3478/5000 train_loss:3.7842 train_time:623764ms step_avg:179.86ms\nstep:3479/5000 train_loss:3.6680 train_time:623946ms step_avg:179.86ms\nstep:3480/5000 train_loss:3.5432 train_time:624125ms step_avg:179.86ms\nstep:3481/5000 train_loss:3.6644 train_time:624305ms step_avg:179.86ms\nstep:3482/5000 train_loss:3.7639 train_time:624486ms step_avg:179.86ms\nstep:3483/5000 train_loss:3.7788 train_time:624665ms step_avg:179.86ms\nstep:3484/5000 train_loss:3.7395 train_time:624846ms step_avg:179.86ms\nstep:3485/5000 train_loss:3.7657 train_time:625027ms step_avg:179.86ms\nstep:3486/5000 train_loss:3.5956 train_time:625204ms step_avg:179.86ms\nstep:3487/5000 train_loss:3.8594 train_time:625383ms step_avg:179.86ms\nstep:3488/5000 train_loss:3.6762 train_time:625565ms step_avg:179.86ms\nstep:3489/5000 train_loss:3.6259 train_time:625746ms step_avg:179.86ms\nstep:3490/5000 train_loss:3.5741 train_time:625926ms step_avg:179.86ms\nstep:3491/5000 train_loss:3.6522 train_time:626109ms step_avg:179.86ms\nstep:3492/5000 train_loss:3.6241 train_time:626288ms step_avg:179.86ms\nstep:3493/5000 train_loss:3.8233 train_time:626467ms step_avg:179.86ms\nstep:3494/5000 train_loss:3.8083 train_time:626647ms step_avg:179.86ms\nstep:3495/5000 train_loss:3.6846 train_time:626827ms step_avg:179.86ms\nstep:3496/5000 train_loss:3.7297 train_time:627009ms step_avg:179.86ms\nstep:3497/5000 train_loss:3.7931 train_time:627190ms step_avg:179.87ms\nstep:3498/5000 train_loss:3.5970 train_time:627369ms step_avg:179.86ms\nstep:3499/5000 train_loss:3.7322 train_time:627547ms step_avg:179.86ms\nstep:3500/5000 train_loss:3.6424 train_time:627726ms step_avg:179.86ms\nstep:3501/5000 train_loss:3.7494 train_time:627905ms step_avg:179.86ms\nstep:3502/5000 train_loss:3.3284 train_time:628086ms step_avg:179.86ms\nstep:3503/5000 train_loss:3.6590 train_time:628268ms step_avg:179.86ms\nstep:3504/5000 train_loss:3.7755 train_time:628446ms step_avg:179.86ms\nstep:3505/5000 train_loss:3.7277 train_time:628627ms step_avg:179.86ms\nstep:3506/5000 train_loss:3.7932 train_time:628808ms step_avg:179.86ms\nstep:3507/5000 train_loss:3.7142 train_time:628989ms step_avg:179.87ms\nstep:3508/5000 train_loss:3.5788 train_time:629169ms step_avg:179.87ms\nstep:3509/5000 train_loss:3.7517 train_time:629349ms step_avg:179.87ms\nstep:3510/5000 train_loss:3.5998 train_time:629528ms step_avg:179.87ms\nstep:3511/5000 train_loss:3.5253 train_time:629707ms step_avg:179.86ms\nstep:3512/5000 train_loss:3.7867 train_time:629888ms step_avg:179.87ms\nstep:3513/5000 train_loss:4.0297 train_time:630067ms step_avg:179.86ms\nstep:3514/5000 train_loss:3.7745 train_time:630247ms step_avg:179.86ms\nstep:3515/5000 train_loss:3.6838 train_time:630426ms step_avg:179.86ms\nstep:3516/5000 train_loss:3.6412 train_time:630608ms step_avg:179.87ms\nstep:3517/5000 train_loss:3.6399 train_time:630788ms step_avg:179.87ms\nstep:3518/5000 train_loss:3.6284 train_time:630968ms step_avg:179.87ms\nstep:3519/5000 train_loss:3.7476 train_time:631148ms step_avg:179.87ms\nstep:3520/5000 train_loss:4.0095 train_time:631327ms step_avg:179.87ms\nstep:3521/5000 train_loss:3.7443 train_time:631507ms step_avg:179.87ms\nstep:3522/5000 train_loss:3.8096 train_time:631685ms step_avg:179.86ms\nstep:3523/5000 train_loss:3.5271 train_time:631865ms step_avg:179.86ms\nstep:3524/5000 train_loss:3.7346 train_time:632047ms step_avg:179.87ms\nstep:3525/5000 train_loss:3.6048 train_time:632227ms step_avg:179.87ms\nstep:3526/5000 train_loss:3.4775 train_time:632407ms step_avg:179.87ms\nstep:3527/5000 train_loss:3.6216 train_time:632589ms step_avg:179.87ms\nstep:3528/5000 train_loss:3.5371 train_time:632768ms step_avg:179.87ms\nstep:3529/5000 train_loss:3.6027 train_time:632948ms step_avg:179.87ms\nstep:3530/5000 train_loss:3.6381 train_time:633128ms step_avg:179.87ms\nstep:3531/5000 train_loss:3.7761 train_time:633307ms step_avg:179.87ms\nstep:3532/5000 train_loss:3.8708 train_time:633487ms step_avg:179.87ms\nstep:3533/5000 train_loss:3.6320 train_time:633666ms step_avg:179.87ms\nstep:3534/5000 train_loss:3.8460 train_time:633846ms step_avg:179.87ms\nstep:3535/5000 train_loss:3.7095 train_time:634026ms step_avg:179.87ms\nstep:3536/5000 train_loss:3.5618 train_time:634207ms step_avg:179.87ms\nstep:3537/5000 train_loss:3.7393 train_time:634387ms step_avg:179.87ms\nstep:3538/5000 train_loss:3.9935 train_time:634566ms step_avg:179.87ms\nstep:3539/5000 train_loss:3.5942 train_time:634746ms step_avg:179.87ms\nstep:3540/5000 train_loss:3.8138 train_time:634925ms step_avg:179.87ms\nstep:3541/5000 train_loss:3.5620 train_time:635105ms step_avg:179.87ms\nstep:3542/5000 train_loss:3.7121 train_time:635285ms step_avg:179.87ms\nstep:3543/5000 train_loss:3.6500 train_time:635466ms step_avg:179.87ms\nstep:3544/5000 train_loss:3.6958 train_time:635645ms step_avg:179.87ms\nstep:3545/5000 train_loss:3.5282 train_time:635825ms step_avg:179.87ms\nstep:3546/5000 train_loss:3.6387 train_time:636007ms step_avg:179.87ms\nstep:3547/5000 train_loss:3.5936 train_time:636187ms step_avg:179.87ms\nstep:3548/5000 train_loss:3.8082 train_time:636366ms step_avg:179.87ms\nstep:3549/5000 train_loss:3.6417 train_time:636545ms step_avg:179.87ms\nstep:3550/5000 train_loss:3.7315 train_time:636725ms step_avg:179.87ms\nstep:3551/5000 train_loss:3.7020 train_time:636906ms step_avg:179.87ms\nstep:3552/5000 train_loss:3.7254 train_time:637086ms step_avg:179.87ms\nstep:3553/5000 train_loss:3.5603 train_time:637265ms step_avg:179.87ms\nstep:3554/5000 train_loss:4.2356 train_time:637445ms step_avg:179.87ms\nstep:3555/5000 train_loss:3.8880 train_time:637625ms step_avg:179.87ms\nstep:3556/5000 train_loss:3.7473 train_time:637805ms step_avg:179.87ms\nstep:3557/5000 train_loss:3.5818 train_time:637985ms step_avg:179.87ms\nstep:3558/5000 train_loss:3.9940 train_time:638166ms step_avg:179.87ms\nstep:3559/5000 train_loss:3.6509 train_time:638346ms step_avg:179.87ms\nstep:3560/5000 train_loss:3.7293 train_time:638525ms step_avg:179.87ms\nstep:3561/5000 train_loss:3.7448 train_time:638704ms step_avg:179.87ms\nstep:3562/5000 train_loss:3.6722 train_time:638884ms step_avg:179.87ms\nstep:3563/5000 train_loss:3.5758 train_time:639062ms step_avg:179.87ms\nstep:3564/5000 train_loss:3.5495 train_time:639242ms step_avg:179.87ms\nstep:3565/5000 train_loss:3.6060 train_time:639421ms step_avg:179.87ms\nstep:3566/5000 train_loss:3.6901 train_time:639601ms step_avg:179.87ms\nstep:3567/5000 train_loss:3.4595 train_time:639781ms step_avg:179.87ms\nstep:3568/5000 train_loss:3.7272 train_time:639960ms step_avg:179.87ms\nstep:3569/5000 train_loss:3.6008 train_time:640140ms step_avg:179.87ms\nstep:3570/5000 train_loss:3.5184 train_time:640320ms step_avg:179.87ms\nstep:3571/5000 train_loss:3.5500 train_time:640500ms step_avg:179.87ms\nstep:3572/5000 train_loss:3.6553 train_time:640680ms step_avg:179.87ms\nstep:3573/5000 train_loss:3.7708 train_time:640860ms step_avg:179.87ms\nstep:3574/5000 train_loss:3.8592 train_time:641039ms step_avg:179.86ms\nstep:3575/5000 train_loss:3.8787 train_time:641218ms step_avg:179.86ms\nstep:3576/5000 train_loss:3.7040 train_time:641398ms step_avg:179.86ms\nstep:3577/5000 train_loss:3.5164 train_time:641577ms step_avg:179.86ms\nstep:3578/5000 train_loss:3.5793 train_time:641756ms step_avg:179.86ms\nstep:3579/5000 train_loss:3.6830 train_time:641934ms step_avg:179.86ms\nstep:3580/5000 train_loss:3.5408 train_time:642113ms step_avg:179.86ms\nstep:3581/5000 train_loss:3.5892 train_time:642290ms step_avg:179.86ms\nstep:3582/5000 train_loss:3.8562 train_time:642469ms step_avg:179.86ms\nstep:3583/5000 train_loss:3.6809 train_time:642647ms step_avg:179.86ms\nstep:3584/5000 train_loss:3.4992 train_time:642827ms step_avg:179.86ms\nstep:3585/5000 train_loss:3.8188 train_time:643010ms step_avg:179.86ms\nstep:3586/5000 train_loss:3.8541 train_time:643188ms step_avg:179.86ms\nstep:3587/5000 train_loss:3.6201 train_time:643367ms step_avg:179.86ms\nstep:3588/5000 train_loss:3.5997 train_time:643546ms step_avg:179.86ms\nstep:3589/5000 train_loss:3.5758 train_time:643724ms step_avg:179.86ms\nstep:3590/5000 train_loss:3.6435 train_time:643904ms step_avg:179.86ms\nstep:3591/5000 train_loss:3.6839 train_time:644085ms step_avg:179.86ms\nstep:3592/5000 train_loss:3.8065 train_time:644264ms step_avg:179.86ms\nstep:3593/5000 train_loss:3.6973 train_time:644444ms step_avg:179.86ms\nstep:3594/5000 train_loss:3.6857 train_time:644624ms step_avg:179.86ms\nstep:3595/5000 train_loss:3.7064 train_time:644803ms step_avg:179.86ms\nstep:3596/5000 train_loss:3.6816 train_time:644985ms step_avg:179.86ms\nstep:3597/5000 train_loss:3.7234 train_time:645166ms step_avg:179.86ms\nstep:3598/5000 train_loss:3.7389 train_time:645344ms step_avg:179.86ms\nstep:3599/5000 train_loss:3.6613 train_time:645524ms step_avg:179.86ms\nstep:3600/5000 train_loss:3.7159 train_time:645704ms step_avg:179.86ms\nstep:3601/5000 train_loss:4.2008 train_time:645886ms step_avg:179.86ms\nstep:3602/5000 train_loss:3.6311 train_time:646062ms step_avg:179.86ms\nstep:3603/5000 train_loss:3.6109 train_time:646240ms step_avg:179.86ms\nstep:3604/5000 train_loss:3.8883 train_time:646420ms step_avg:179.86ms\nstep:3605/5000 train_loss:3.6351 train_time:646600ms step_avg:179.86ms\nstep:3606/5000 train_loss:3.5742 train_time:646780ms step_avg:179.86ms\nstep:3607/5000 train_loss:3.6606 train_time:646959ms step_avg:179.86ms\nstep:3608/5000 train_loss:3.6653 train_time:647137ms step_avg:179.86ms\nstep:3609/5000 train_loss:3.6436 train_time:647316ms step_avg:179.86ms\nstep:3610/5000 train_loss:3.7835 train_time:647495ms step_avg:179.86ms\nstep:3611/5000 train_loss:3.6502 train_time:647673ms step_avg:179.86ms\nstep:3612/5000 train_loss:3.5972 train_time:647851ms step_avg:179.86ms\nstep:3613/5000 train_loss:3.7102 train_time:648029ms step_avg:179.86ms\nstep:3614/5000 train_loss:3.7861 train_time:648207ms step_avg:179.86ms\nstep:3615/5000 train_loss:4.1126 train_time:648386ms step_avg:179.86ms\nstep:3616/5000 train_loss:3.8169 train_time:648566ms step_avg:179.86ms\nstep:3617/5000 train_loss:3.6760 train_time:648748ms step_avg:179.86ms\nstep:3618/5000 train_loss:3.6913 train_time:648929ms step_avg:179.86ms\nstep:3619/5000 train_loss:3.6472 train_time:649109ms step_avg:179.86ms\nstep:3620/5000 train_loss:3.7703 train_time:649287ms step_avg:179.86ms\nstep:3621/5000 train_loss:3.6647 train_time:649468ms step_avg:179.86ms\nstep:3622/5000 train_loss:3.6666 train_time:649648ms step_avg:179.86ms\nstep:3623/5000 train_loss:3.7109 train_time:649827ms step_avg:179.86ms\nstep:3624/5000 train_loss:3.6631 train_time:650008ms step_avg:179.86ms\nstep:3625/5000 train_loss:3.6411 train_time:650187ms step_avg:179.86ms\nstep:3626/5000 train_loss:3.6437 train_time:650366ms step_avg:179.86ms\nstep:3627/5000 train_loss:3.5570 train_time:650544ms step_avg:179.86ms\nstep:3628/5000 train_loss:3.6210 train_time:650725ms step_avg:179.86ms\nstep:3629/5000 train_loss:3.6406 train_time:650906ms step_avg:179.86ms\nstep:3630/5000 train_loss:3.9028 train_time:651087ms step_avg:179.86ms\nstep:3631/5000 train_loss:3.6693 train_time:651267ms step_avg:179.86ms\nstep:3632/5000 train_loss:3.9164 train_time:651447ms step_avg:179.86ms\nstep:3633/5000 train_loss:3.6783 train_time:651626ms step_avg:179.86ms\nstep:3634/5000 train_loss:3.6688 train_time:651807ms step_avg:179.86ms\nstep:3635/5000 train_loss:3.7876 train_time:651987ms step_avg:179.86ms\nstep:3636/5000 train_loss:3.6328 train_time:652165ms step_avg:179.86ms\nstep:3637/5000 train_loss:3.5879 train_time:652345ms step_avg:179.86ms\nstep:3638/5000 train_loss:3.6995 train_time:652524ms step_avg:179.86ms\nstep:3639/5000 train_loss:3.7005 train_time:652703ms step_avg:179.86ms\nstep:3640/5000 train_loss:3.8086 train_time:652883ms step_avg:179.86ms\nstep:3641/5000 train_loss:3.6768 train_time:653064ms step_avg:179.86ms\nstep:3642/5000 train_loss:3.6123 train_time:653243ms step_avg:179.86ms\nstep:3643/5000 train_loss:3.7700 train_time:653423ms step_avg:179.86ms\nstep:3644/5000 train_loss:4.0229 train_time:653602ms step_avg:179.86ms\nstep:3645/5000 train_loss:3.7586 train_time:653782ms step_avg:179.86ms\nstep:3646/5000 train_loss:3.6996 train_time:653960ms step_avg:179.86ms\nstep:3647/5000 train_loss:3.6468 train_time:654141ms step_avg:179.86ms\nstep:3648/5000 train_loss:3.4669 train_time:654321ms step_avg:179.86ms\nstep:3649/5000 train_loss:3.6515 train_time:654502ms step_avg:179.86ms\nstep:3650/5000 train_loss:3.7713 train_time:654681ms step_avg:179.86ms\nstep:3651/5000 train_loss:3.5879 train_time:654860ms step_avg:179.86ms\nstep:3652/5000 train_loss:3.7932 train_time:655039ms step_avg:179.86ms\nstep:3653/5000 train_loss:3.6640 train_time:655219ms step_avg:179.86ms\nstep:3654/5000 train_loss:3.7691 train_time:655398ms step_avg:179.86ms\nstep:3655/5000 train_loss:3.6005 train_time:655578ms step_avg:179.86ms\nstep:3656/5000 train_loss:3.6915 train_time:655758ms step_avg:179.86ms\nstep:3657/5000 train_loss:3.6764 train_time:655936ms step_avg:179.86ms\nstep:3658/5000 train_loss:3.6504 train_time:656116ms step_avg:179.86ms\nstep:3659/5000 train_loss:3.7184 train_time:656294ms step_avg:179.86ms\nstep:3660/5000 train_loss:3.7199 train_time:656473ms step_avg:179.86ms\nstep:3661/5000 train_loss:3.5218 train_time:656651ms step_avg:179.86ms\nstep:3662/5000 train_loss:3.8804 train_time:656830ms step_avg:179.85ms\nstep:3663/5000 train_loss:3.7081 train_time:657009ms step_avg:179.85ms\nstep:3664/5000 train_loss:3.8842 train_time:657189ms step_avg:179.85ms\nstep:3665/5000 train_loss:3.7016 train_time:657367ms step_avg:179.85ms\nstep:3666/5000 train_loss:3.6952 train_time:657547ms step_avg:179.85ms\nstep:3667/5000 train_loss:3.5913 train_time:657726ms step_avg:179.85ms\nstep:3668/5000 train_loss:3.6356 train_time:657906ms step_avg:179.85ms\nstep:3669/5000 train_loss:3.8976 train_time:658086ms step_avg:179.85ms\nstep:3670/5000 train_loss:3.7767 train_time:658265ms step_avg:179.85ms\nstep:3671/5000 train_loss:3.7655 train_time:658444ms step_avg:179.85ms\nstep:3672/5000 train_loss:3.6167 train_time:658623ms step_avg:179.85ms\nstep:3673/5000 train_loss:4.1765 train_time:658803ms step_avg:179.85ms\nstep:3674/5000 train_loss:3.7550 train_time:658982ms step_avg:179.85ms\nstep:3675/5000 train_loss:3.6000 train_time:659163ms step_avg:179.85ms\nstep:3676/5000 train_loss:3.6855 train_time:659342ms step_avg:179.85ms\nstep:3677/5000 train_loss:3.6856 train_time:659522ms step_avg:179.85ms\nstep:3678/5000 train_loss:4.9990 train_time:659702ms step_avg:179.85ms\nstep:3679/5000 train_loss:3.6189 train_time:659883ms step_avg:179.85ms\nstep:3680/5000 train_loss:3.5999 train_time:660063ms step_avg:179.85ms\nstep:3681/5000 train_loss:3.3825 train_time:660242ms step_avg:179.85ms\nstep:3682/5000 train_loss:3.6830 train_time:660421ms step_avg:179.85ms\nstep:3683/5000 train_loss:3.5160 train_time:660601ms step_avg:179.85ms\nstep:3684/5000 train_loss:3.6460 train_time:660781ms step_avg:179.85ms\nstep:3685/5000 train_loss:3.7891 train_time:660962ms step_avg:179.85ms\nstep:3686/5000 train_loss:3.7238 train_time:661142ms step_avg:179.85ms\nstep:3687/5000 train_loss:3.7362 train_time:661321ms step_avg:179.85ms\nstep:3688/5000 train_loss:3.6061 train_time:661500ms step_avg:179.85ms\nstep:3689/5000 train_loss:3.6551 train_time:661680ms step_avg:179.85ms\nstep:3690/5000 train_loss:3.6814 train_time:661860ms step_avg:179.85ms\nstep:3691/5000 train_loss:3.5701 train_time:662040ms step_avg:179.85ms\nstep:3692/5000 train_loss:3.7604 train_time:662220ms step_avg:179.85ms\nstep:3693/5000 train_loss:3.6466 train_time:662399ms step_avg:179.85ms\nstep:3694/5000 train_loss:3.7466 train_time:662579ms step_avg:179.85ms\nstep:3695/5000 train_loss:3.6357 train_time:662758ms step_avg:179.85ms\nstep:3696/5000 train_loss:3.6647 train_time:662937ms step_avg:179.85ms\nstep:3697/5000 train_loss:3.6600 train_time:663116ms step_avg:179.85ms\nstep:3698/5000 train_loss:3.7557 train_time:663294ms step_avg:179.85ms\nstep:3699/5000 train_loss:3.5950 train_time:663473ms step_avg:179.85ms\nstep:3700/5000 train_loss:3.6224 train_time:663651ms step_avg:179.85ms\nstep:3701/5000 train_loss:3.6920 train_time:663830ms step_avg:179.85ms\nstep:3702/5000 train_loss:3.8290 train_time:664008ms step_avg:179.85ms\nstep:3703/5000 train_loss:3.5879 train_time:664187ms step_avg:179.85ms\nstep:3704/5000 train_loss:3.7189 train_time:664368ms step_avg:179.85ms\nstep:3705/5000 train_loss:3.6248 train_time:664547ms step_avg:179.85ms\nstep:3706/5000 train_loss:3.8075 train_time:664729ms step_avg:179.85ms\nstep:3707/5000 train_loss:3.6709 train_time:664908ms step_avg:179.85ms\nstep:3708/5000 train_loss:3.6192 train_time:665087ms step_avg:179.85ms\nstep:3709/5000 train_loss:3.5099 train_time:665267ms step_avg:179.85ms\nstep:3710/5000 train_loss:3.7086 train_time:665446ms step_avg:179.85ms\nstep:3711/5000 train_loss:3.6117 train_time:665628ms step_avg:179.85ms\nstep:3712/5000 train_loss:3.7940 train_time:665811ms step_avg:179.85ms\nstep:3713/5000 train_loss:3.8265 train_time:665989ms step_avg:179.85ms\nstep:3714/5000 train_loss:3.6966 train_time:666168ms step_avg:179.85ms\nstep:3715/5000 train_loss:3.5317 train_time:666349ms step_avg:179.85ms\nstep:3716/5000 train_loss:3.6659 train_time:666527ms step_avg:179.85ms\nstep:3717/5000 train_loss:3.5893 train_time:666708ms step_avg:179.85ms\nstep:3718/5000 train_loss:3.9330 train_time:666887ms step_avg:179.85ms\nstep:3719/5000 train_loss:3.7132 train_time:667065ms step_avg:179.85ms\nstep:3720/5000 train_loss:3.5480 train_time:667245ms step_avg:179.85ms\nstep:3721/5000 train_loss:3.7216 train_time:667426ms step_avg:179.85ms\nstep:3722/5000 train_loss:3.7707 train_time:667605ms step_avg:179.85ms\nstep:3723/5000 train_loss:3.7605 train_time:667785ms step_avg:179.85ms\nstep:3724/5000 train_loss:3.8549 train_time:667965ms step_avg:179.85ms\nstep:3725/5000 train_loss:3.8850 train_time:668144ms step_avg:179.85ms\nstep:3726/5000 train_loss:3.5599 train_time:668324ms step_avg:179.85ms\nstep:3727/5000 train_loss:3.7764 train_time:668506ms step_avg:179.85ms\nstep:3728/5000 train_loss:3.5800 train_time:668686ms step_avg:179.85ms\nstep:3729/5000 train_loss:3.6468 train_time:668867ms step_avg:179.85ms\nstep:3730/5000 train_loss:3.6487 train_time:669045ms step_avg:179.85ms\nstep:3731/5000 train_loss:3.7641 train_time:669225ms step_avg:179.85ms\nstep:3732/5000 train_loss:3.4944 train_time:669405ms step_avg:179.85ms\nstep:3733/5000 train_loss:3.6847 train_time:669583ms step_avg:179.85ms\nstep:3734/5000 train_loss:3.8325 train_time:669763ms step_avg:179.85ms\nstep:3735/5000 train_loss:3.9589 train_time:669944ms step_avg:179.85ms\nstep:3736/5000 train_loss:3.7262 train_time:670123ms step_avg:179.85ms\nstep:3737/5000 train_loss:3.6663 train_time:670302ms step_avg:179.85ms\nstep:3738/5000 train_loss:3.5249 train_time:670481ms step_avg:179.85ms\nstep:3739/5000 train_loss:3.5720 train_time:670660ms step_avg:179.85ms\nstep:3740/5000 train_loss:3.6994 train_time:670839ms step_avg:179.85ms\nstep:3741/5000 train_loss:3.6355 train_time:671019ms step_avg:179.85ms\nstep:3742/5000 train_loss:3.7246 train_time:671198ms step_avg:179.85ms\nstep:3743/5000 train_loss:3.8408 train_time:671377ms step_avg:179.85ms\nstep:3744/5000 train_loss:3.5410 train_time:671556ms step_avg:179.85ms\nstep:3745/5000 train_loss:3.7922 train_time:671735ms step_avg:179.85ms\nstep:3746/5000 train_loss:3.6046 train_time:671914ms step_avg:179.85ms\nstep:3747/5000 train_loss:3.5650 train_time:672093ms step_avg:179.85ms\nstep:3748/5000 train_loss:3.5129 train_time:672272ms step_avg:179.85ms\nstep:3749/5000 train_loss:3.9045 train_time:672450ms step_avg:179.85ms\nstep:3750/5000 train_loss:3.6650 train_time:672628ms step_avg:179.85ms\nstep:3751/5000 train_loss:3.8955 train_time:672808ms step_avg:179.85ms\nstep:3752/5000 train_loss:3.6533 train_time:672988ms step_avg:179.85ms\nstep:3753/5000 train_loss:3.6102 train_time:673166ms step_avg:179.85ms\nstep:3754/5000 train_loss:3.7654 train_time:673346ms step_avg:179.85ms\nstep:3755/5000 train_loss:3.7577 train_time:673526ms step_avg:179.85ms\nstep:3756/5000 train_loss:3.4602 train_time:673705ms step_avg:179.85ms\nstep:3757/5000 train_loss:3.6916 train_time:673885ms step_avg:179.85ms\nstep:3758/5000 train_loss:3.6873 train_time:674066ms step_avg:179.85ms\nstep:3759/5000 train_loss:3.6329 train_time:674247ms step_avg:179.85ms\nstep:3760/5000 train_loss:3.7254 train_time:674426ms step_avg:179.85ms\nstep:3761/5000 train_loss:3.7280 train_time:674606ms step_avg:179.85ms\nstep:3762/5000 train_loss:3.8575 train_time:674786ms step_avg:179.85ms\nstep:3763/5000 train_loss:3.6441 train_time:674965ms step_avg:179.85ms\nstep:3764/5000 train_loss:3.5710 train_time:675145ms step_avg:179.85ms\nstep:3765/5000 train_loss:3.7341 train_time:675325ms step_avg:179.85ms\nstep:3766/5000 train_loss:3.6365 train_time:675507ms step_avg:179.85ms\nstep:3767/5000 train_loss:3.5809 train_time:675687ms step_avg:179.85ms\nstep:3768/5000 train_loss:3.5676 train_time:675868ms step_avg:179.85ms\nstep:3769/5000 train_loss:3.6628 train_time:676047ms step_avg:179.85ms\nstep:3770/5000 train_loss:3.6984 train_time:676225ms step_avg:179.85ms\nstep:3771/5000 train_loss:4.2620 train_time:676404ms step_avg:179.85ms\nstep:3772/5000 train_loss:3.6910 train_time:676585ms step_avg:179.85ms\nstep:3773/5000 train_loss:3.7171 train_time:676764ms step_avg:179.85ms\nstep:3774/5000 train_loss:3.4959 train_time:676945ms step_avg:179.85ms\nstep:3775/5000 train_loss:3.5640 train_time:677124ms step_avg:179.85ms\nstep:3776/5000 train_loss:3.6050 train_time:677303ms step_avg:179.85ms\nstep:3777/5000 train_loss:3.8705 train_time:677483ms step_avg:179.85ms\nstep:3778/5000 train_loss:3.8461 train_time:677663ms step_avg:179.85ms\nstep:3779/5000 train_loss:3.9501 train_time:677843ms step_avg:179.85ms\nstep:3780/5000 train_loss:3.6045 train_time:678022ms step_avg:179.85ms\nstep:3781/5000 train_loss:3.6976 train_time:678202ms step_avg:179.85ms\nstep:3782/5000 train_loss:3.5567 train_time:678381ms step_avg:179.85ms\nstep:3783/5000 train_loss:3.6329 train_time:678560ms step_avg:179.85ms\nstep:3784/5000 train_loss:3.8299 train_time:678740ms step_avg:179.85ms\nstep:3785/5000 train_loss:3.7361 train_time:678920ms step_avg:179.85ms\nstep:3786/5000 train_loss:3.5782 train_time:679100ms step_avg:179.85ms\nstep:3787/5000 train_loss:3.8731 train_time:679280ms step_avg:179.85ms\nstep:3788/5000 train_loss:3.6792 train_time:679459ms step_avg:179.85ms\nstep:3789/5000 train_loss:3.6352 train_time:679637ms step_avg:179.85ms\nstep:3790/5000 train_loss:5.2079 train_time:679816ms step_avg:179.85ms\nstep:3791/5000 train_loss:3.5941 train_time:679994ms step_avg:179.85ms\nstep:3792/5000 train_loss:3.7242 train_time:680173ms step_avg:179.84ms\nstep:3793/5000 train_loss:3.7254 train_time:680352ms step_avg:179.84ms\nstep:3794/5000 train_loss:3.7486 train_time:680531ms step_avg:179.84ms\nstep:3795/5000 train_loss:3.9696 train_time:680711ms step_avg:179.84ms\nstep:3796/5000 train_loss:3.9139 train_time:680890ms step_avg:179.84ms\nstep:3797/5000 train_loss:3.6945 train_time:681069ms step_avg:179.84ms\nstep:3798/5000 train_loss:3.6413 train_time:681248ms step_avg:179.84ms\nstep:3799/5000 train_loss:3.6652 train_time:681427ms step_avg:179.84ms\nstep:3800/5000 train_loss:3.5304 train_time:681608ms step_avg:179.84ms\nstep:3801/5000 train_loss:3.7008 train_time:681788ms step_avg:179.84ms\nstep:3802/5000 train_loss:3.5392 train_time:681968ms step_avg:179.84ms\nstep:3803/5000 train_loss:3.6043 train_time:682148ms step_avg:179.84ms\nstep:3804/5000 train_loss:3.5875 train_time:682327ms step_avg:179.84ms\nstep:3805/5000 train_loss:3.5785 train_time:682506ms step_avg:179.84ms\nstep:3806/5000 train_loss:3.5928 train_time:682686ms step_avg:179.84ms\nstep:3807/5000 train_loss:3.5358 train_time:682865ms step_avg:179.84ms\nstep:3808/5000 train_loss:3.8750 train_time:683046ms step_avg:179.84ms\nstep:3809/5000 train_loss:3.6167 train_time:683226ms step_avg:179.84ms\nstep:3810/5000 train_loss:3.8186 train_time:683643ms step_avg:179.91ms\nstep:3811/5000 train_loss:3.6922 train_time:683821ms step_avg:179.91ms\nstep:3812/5000 train_loss:3.5508 train_time:683999ms step_avg:179.91ms\nstep:3813/5000 train_loss:3.6202 train_time:684177ms step_avg:179.90ms\nstep:3814/5000 train_loss:3.7244 train_time:684355ms step_avg:179.90ms\nstep:3815/5000 train_loss:3.6787 train_time:684539ms step_avg:179.91ms\nstep:3816/5000 train_loss:3.6243 train_time:684719ms step_avg:179.91ms\nstep:3817/5000 train_loss:3.6628 train_time:684897ms step_avg:179.90ms\nstep:3818/5000 train_loss:3.4251 train_time:685076ms step_avg:179.90ms\nstep:3819/5000 train_loss:3.5782 train_time:685255ms step_avg:179.90ms\nstep:3820/5000 train_loss:3.7018 train_time:685433ms step_avg:179.90ms\nstep:3821/5000 train_loss:3.6326 train_time:685612ms step_avg:179.90ms\nstep:3822/5000 train_loss:3.5809 train_time:685791ms step_avg:179.90ms\nstep:3823/5000 train_loss:3.5990 train_time:685969ms step_avg:179.90ms\nstep:3824/5000 train_loss:3.5842 train_time:686148ms step_avg:179.90ms\nstep:3825/5000 train_loss:3.7477 train_time:686327ms step_avg:179.90ms\nstep:3826/5000 train_loss:3.5749 train_time:686506ms step_avg:179.90ms\nstep:3827/5000 train_loss:3.6333 train_time:686685ms step_avg:179.90ms\nstep:3828/5000 train_loss:3.6192 train_time:686868ms step_avg:179.90ms\nstep:3829/5000 train_loss:3.6076 train_time:687048ms step_avg:179.90ms\nstep:3830/5000 train_loss:3.6121 train_time:687228ms step_avg:179.90ms\nstep:3831/5000 train_loss:3.6906 train_time:687410ms step_avg:179.90ms\nstep:3832/5000 train_loss:3.5853 train_time:687588ms step_avg:179.90ms\nstep:3833/5000 train_loss:4.2038 train_time:687767ms step_avg:179.90ms\nstep:3834/5000 train_loss:3.5759 train_time:687948ms step_avg:179.90ms\nstep:3835/5000 train_loss:3.6099 train_time:688130ms step_avg:179.90ms\nstep:3836/5000 train_loss:3.6414 train_time:688309ms step_avg:179.90ms\nstep:3837/5000 train_loss:3.6969 train_time:688487ms step_avg:179.90ms\nstep:3838/5000 train_loss:3.7273 train_time:688666ms step_avg:179.90ms\nstep:3839/5000 train_loss:3.6566 train_time:688848ms step_avg:179.90ms\nstep:3840/5000 train_loss:3.6383 train_time:689027ms step_avg:179.90ms\nstep:3841/5000 train_loss:3.9033 train_time:689208ms step_avg:179.90ms\nstep:3842/5000 train_loss:3.8285 train_time:689388ms step_avg:179.90ms\nstep:3843/5000 train_loss:3.4994 train_time:689566ms step_avg:179.90ms\nstep:3844/5000 train_loss:3.6519 train_time:689746ms step_avg:179.90ms\nstep:3845/5000 train_loss:3.8691 train_time:689925ms step_avg:179.90ms\nstep:3846/5000 train_loss:3.7678 train_time:690107ms step_avg:179.90ms\nstep:3847/5000 train_loss:3.6816 train_time:690287ms step_avg:179.90ms\nstep:3848/5000 train_loss:3.5394 train_time:690467ms step_avg:179.90ms\nstep:3849/5000 train_loss:3.7914 train_time:690645ms step_avg:179.90ms\nstep:3850/5000 train_loss:3.5921 train_time:690826ms step_avg:179.90ms\nstep:3851/5000 train_loss:3.6831 train_time:691008ms step_avg:179.90ms\nstep:3852/5000 train_loss:3.8054 train_time:691188ms step_avg:179.90ms\nstep:3853/5000 train_loss:3.6496 train_time:691369ms step_avg:179.90ms\nstep:3854/5000 train_loss:3.5310 train_time:691549ms step_avg:179.90ms\nstep:3855/5000 train_loss:3.5982 train_time:691728ms step_avg:179.90ms\nstep:3856/5000 train_loss:3.7425 train_time:691908ms step_avg:179.90ms\nstep:3857/5000 train_loss:3.6748 train_time:692086ms step_avg:179.90ms\nstep:3858/5000 train_loss:3.6392 train_time:692265ms step_avg:179.90ms\nstep:3859/5000 train_loss:3.5818 train_time:692444ms step_avg:179.90ms\nstep:3860/5000 train_loss:3.5649 train_time:692625ms step_avg:179.90ms\nstep:3861/5000 train_loss:3.6426 train_time:692807ms step_avg:179.90ms\nstep:3862/5000 train_loss:3.7039 train_time:692986ms step_avg:179.90ms\nstep:3863/5000 train_loss:3.7780 train_time:693165ms step_avg:179.90ms\nstep:3864/5000 train_loss:3.5980 train_time:693344ms step_avg:179.90ms\nstep:3865/5000 train_loss:3.5356 train_time:693525ms step_avg:179.90ms\nstep:3866/5000 train_loss:3.7706 train_time:693705ms step_avg:179.90ms\nstep:3867/5000 train_loss:3.6003 train_time:693885ms step_avg:179.90ms\nstep:3868/5000 train_loss:3.6241 train_time:694065ms step_avg:179.90ms\nstep:3869/5000 train_loss:3.6852 train_time:694244ms step_avg:179.90ms\nstep:3870/5000 train_loss:3.6499 train_time:694423ms step_avg:179.90ms\nstep:3871/5000 train_loss:3.6362 train_time:694603ms step_avg:179.90ms\nstep:3872/5000 train_loss:3.8435 train_time:694784ms step_avg:179.90ms\nstep:3873/5000 train_loss:3.7016 train_time:694963ms step_avg:179.90ms\nstep:3874/5000 train_loss:3.6973 train_time:695143ms step_avg:179.90ms\nstep:3875/5000 train_loss:3.5933 train_time:695322ms step_avg:179.90ms\nstep:3876/5000 train_loss:3.6717 train_time:695502ms step_avg:179.90ms\nstep:3877/5000 train_loss:3.6983 train_time:695683ms step_avg:179.90ms\nstep:3878/5000 train_loss:3.6598 train_time:695865ms step_avg:179.90ms\nstep:3879/5000 train_loss:3.7275 train_time:696044ms step_avg:179.90ms\nstep:3880/5000 train_loss:3.6576 train_time:696222ms step_avg:179.90ms\nstep:3881/5000 train_loss:3.5847 train_time:696401ms step_avg:179.90ms\nstep:3882/5000 train_loss:3.6399 train_time:696580ms step_avg:179.90ms\nstep:3883/5000 train_loss:3.5921 train_time:696759ms step_avg:179.90ms\nstep:3884/5000 train_loss:3.7197 train_time:696939ms step_avg:179.90ms\nstep:3885/5000 train_loss:3.6559 train_time:697117ms step_avg:179.90ms\nstep:3886/5000 train_loss:3.5858 train_time:697295ms step_avg:179.90ms\nstep:3887/5000 train_loss:3.5522 train_time:697473ms step_avg:179.90ms\nstep:3888/5000 train_loss:3.5341 train_time:697651ms step_avg:179.90ms\nstep:3889/5000 train_loss:3.7495 train_time:697831ms step_avg:179.90ms\nstep:3890/5000 train_loss:3.4780 train_time:698010ms step_avg:179.90ms\nstep:3891/5000 train_loss:3.6925 train_time:698188ms step_avg:179.90ms\nstep:3892/5000 train_loss:3.6805 train_time:698367ms step_avg:179.90ms\nstep:3893/5000 train_loss:3.5704 train_time:698546ms step_avg:179.90ms\nstep:3894/5000 train_loss:3.6762 train_time:698726ms step_avg:179.90ms\nstep:3895/5000 train_loss:3.5645 train_time:698906ms step_avg:179.90ms\nstep:3896/5000 train_loss:3.6267 train_time:699085ms step_avg:179.90ms\nstep:3897/5000 train_loss:3.5440 train_time:699264ms step_avg:179.90ms\nstep:3898/5000 train_loss:3.4920 train_time:699443ms step_avg:179.90ms\nstep:3899/5000 train_loss:3.5287 train_time:699622ms step_avg:179.90ms\nstep:3900/5000 train_loss:3.6802 train_time:699806ms step_avg:179.90ms\nstep:3901/5000 train_loss:3.6233 train_time:699986ms step_avg:179.90ms\nstep:3902/5000 train_loss:3.6369 train_time:700167ms step_avg:179.90ms\nstep:3903/5000 train_loss:3.7147 train_time:700346ms step_avg:179.90ms\nstep:3904/5000 train_loss:3.7512 train_time:700526ms step_avg:179.90ms\nstep:3905/5000 train_loss:3.5488 train_time:700705ms step_avg:179.90ms\nstep:3906/5000 train_loss:3.7366 train_time:700884ms step_avg:179.90ms\nstep:3907/5000 train_loss:3.6282 train_time:701065ms step_avg:179.90ms\nstep:3908/5000 train_loss:3.6239 train_time:701248ms step_avg:179.90ms\nstep:3909/5000 train_loss:3.6176 train_time:701427ms step_avg:179.90ms\nstep:3910/5000 train_loss:3.5786 train_time:701607ms step_avg:179.90ms\nstep:3911/5000 train_loss:3.9724 train_time:701786ms step_avg:179.90ms\nstep:3912/5000 train_loss:3.8247 train_time:701966ms step_avg:179.90ms\nstep:3913/5000 train_loss:3.5981 train_time:702145ms step_avg:179.90ms\nstep:3914/5000 train_loss:3.6228 train_time:702327ms step_avg:179.90ms\nstep:3915/5000 train_loss:3.8104 train_time:702506ms step_avg:179.90ms\nstep:3916/5000 train_loss:3.7024 train_time:702686ms step_avg:179.90ms\nstep:3917/5000 train_loss:3.5816 train_time:702866ms step_avg:179.90ms\nstep:3918/5000 train_loss:3.6881 train_time:703046ms step_avg:179.90ms\nstep:3919/5000 train_loss:3.6779 train_time:703227ms step_avg:179.90ms\nstep:3920/5000 train_loss:3.5252 train_time:703407ms step_avg:179.90ms\nstep:3921/5000 train_loss:3.7190 train_time:703587ms step_avg:179.90ms\nstep:3922/5000 train_loss:3.5052 train_time:703766ms step_avg:179.90ms\nstep:3923/5000 train_loss:3.5676 train_time:703946ms step_avg:179.90ms\nstep:3924/5000 train_loss:3.8277 train_time:704126ms step_avg:179.90ms\nstep:3925/5000 train_loss:3.6809 train_time:704306ms step_avg:179.90ms\nstep:3926/5000 train_loss:3.6555 train_time:704486ms step_avg:179.90ms\nstep:3927/5000 train_loss:3.6113 train_time:704665ms step_avg:179.90ms\nstep:3928/5000 train_loss:3.5497 train_time:704846ms step_avg:179.90ms\nstep:3929/5000 train_loss:3.7586 train_time:705024ms step_avg:179.90ms\nstep:3930/5000 train_loss:3.6107 train_time:705203ms step_avg:179.90ms\nstep:3931/5000 train_loss:3.6500 train_time:705382ms step_avg:179.90ms\nstep:3932/5000 train_loss:3.7131 train_time:705562ms step_avg:179.90ms\nstep:3933/5000 train_loss:3.6290 train_time:705742ms step_avg:179.90ms\nstep:3934/5000 train_loss:3.6195 train_time:705921ms step_avg:179.90ms\nstep:3935/5000 train_loss:3.6457 train_time:706102ms step_avg:179.90ms\nstep:3936/5000 train_loss:3.6933 train_time:706282ms step_avg:179.90ms\nstep:3937/5000 train_loss:3.6542 train_time:706461ms step_avg:179.90ms\nstep:3938/5000 train_loss:3.7384 train_time:706640ms step_avg:179.90ms\nstep:3939/5000 train_loss:3.4870 train_time:706820ms step_avg:179.90ms\nstep:3940/5000 train_loss:3.7167 train_time:706999ms step_avg:179.90ms\nstep:3941/5000 train_loss:3.6289 train_time:707179ms step_avg:179.90ms\nstep:3942/5000 train_loss:3.5762 train_time:707358ms step_avg:179.90ms\nstep:3943/5000 train_loss:3.5823 train_time:707537ms step_avg:179.90ms\nstep:3944/5000 train_loss:3.6275 train_time:707716ms step_avg:179.90ms\nstep:3945/5000 train_loss:3.7470 train_time:707895ms step_avg:179.90ms\nstep:3946/5000 train_loss:3.6144 train_time:708073ms step_avg:179.90ms\nstep:3947/5000 train_loss:3.5371 train_time:708252ms step_avg:179.90ms\nstep:3948/5000 train_loss:3.7265 train_time:708431ms step_avg:179.90ms\nstep:3949/5000 train_loss:3.6564 train_time:708610ms step_avg:179.90ms\nstep:3950/5000 train_loss:3.6285 train_time:708789ms step_avg:179.90ms\nstep:3951/5000 train_loss:4.1032 train_time:708968ms step_avg:179.90ms\nstep:3952/5000 train_loss:3.5619 train_time:709146ms step_avg:179.90ms\nstep:3953/5000 train_loss:3.7158 train_time:709325ms step_avg:179.89ms\nstep:3954/5000 train_loss:3.6828 train_time:709506ms step_avg:179.89ms\nstep:3955/5000 train_loss:3.5371 train_time:709686ms step_avg:179.90ms\nstep:3956/5000 train_loss:3.6268 train_time:709865ms step_avg:179.89ms\nstep:3957/5000 train_loss:3.7930 train_time:710045ms step_avg:179.89ms\nstep:3958/5000 train_loss:3.5200 train_time:710224ms step_avg:179.89ms\nstep:3959/5000 train_loss:3.7122 train_time:710403ms step_avg:179.89ms\nstep:3960/5000 train_loss:3.6169 train_time:710583ms step_avg:179.89ms\nstep:3961/5000 train_loss:3.6675 train_time:710763ms step_avg:179.89ms\nstep:3962/5000 train_loss:3.5552 train_time:710942ms step_avg:179.89ms\nstep:3963/5000 train_loss:3.4670 train_time:711121ms step_avg:179.89ms\nstep:3964/5000 train_loss:3.5586 train_time:711300ms step_avg:179.89ms\nstep:3965/5000 train_loss:3.5758 train_time:711480ms step_avg:179.89ms\nstep:3966/5000 train_loss:3.7122 train_time:711659ms step_avg:179.89ms\nstep:3967/5000 train_loss:3.5501 train_time:711837ms step_avg:179.89ms\nstep:3968/5000 train_loss:3.7859 train_time:712016ms step_avg:179.89ms\nstep:3969/5000 train_loss:3.5702 train_time:712195ms step_avg:179.89ms\nstep:3970/5000 train_loss:3.6192 train_time:712374ms step_avg:179.89ms\nstep:3971/5000 train_loss:3.6192 train_time:712552ms step_avg:179.89ms\nstep:3972/5000 train_loss:3.5969 train_time:712730ms step_avg:179.89ms\nstep:3973/5000 train_loss:3.6731 train_time:712910ms step_avg:179.89ms\nstep:3974/5000 train_loss:3.6482 train_time:713089ms step_avg:179.89ms\nstep:3975/5000 train_loss:3.5521 train_time:713269ms step_avg:179.89ms\nstep:3976/5000 train_loss:3.5802 train_time:713447ms step_avg:179.89ms\nstep:3977/5000 train_loss:3.6789 train_time:713626ms step_avg:179.89ms\nstep:3978/5000 train_loss:3.5836 train_time:713807ms step_avg:179.89ms\nstep:3979/5000 train_loss:3.6851 train_time:713987ms step_avg:179.89ms\nstep:3980/5000 train_loss:3.6573 train_time:714167ms step_avg:179.89ms\nstep:3981/5000 train_loss:3.5610 train_time:714348ms step_avg:179.89ms\nstep:3982/5000 train_loss:3.5413 train_time:714527ms step_avg:179.89ms\nstep:3983/5000 train_loss:3.9188 train_time:714709ms step_avg:179.89ms\nstep:3984/5000 train_loss:3.7293 train_time:714888ms step_avg:179.89ms\nstep:3985/5000 train_loss:3.7511 train_time:715069ms step_avg:179.89ms\nstep:3986/5000 train_loss:3.5972 train_time:715248ms step_avg:179.89ms\nstep:3987/5000 train_loss:3.7332 train_time:715427ms step_avg:179.89ms\nstep:3988/5000 train_loss:3.6679 train_time:715608ms step_avg:179.89ms\nstep:3989/5000 train_loss:3.6042 train_time:715789ms step_avg:179.89ms\nstep:3990/5000 train_loss:3.5990 train_time:715967ms step_avg:179.89ms\nstep:3991/5000 train_loss:3.5715 train_time:716149ms step_avg:179.89ms\nstep:3992/5000 train_loss:3.5822 train_time:716328ms step_avg:179.89ms\nstep:3993/5000 train_loss:3.4152 train_time:716506ms step_avg:179.89ms\nstep:3994/5000 train_loss:3.7243 train_time:716686ms step_avg:179.89ms\nstep:3995/5000 train_loss:3.6626 train_time:716867ms step_avg:179.89ms\nstep:3996/5000 train_loss:3.9362 train_time:717047ms step_avg:179.89ms\nstep:3997/5000 train_loss:3.5618 train_time:717233ms step_avg:179.89ms\nstep:3998/5000 train_loss:3.5463 train_time:717411ms step_avg:179.89ms\nstep:3999/5000 train_loss:3.6451 train_time:717589ms step_avg:179.89ms\nstep:4000/5000 train_loss:3.5946 train_time:717768ms step_avg:179.89ms\nstep:4001/5000 train_loss:3.6687 train_time:717948ms step_avg:179.89ms\nstep:4002/5000 train_loss:3.7964 train_time:718127ms step_avg:179.89ms\nstep:4003/5000 train_loss:3.6998 train_time:718307ms step_avg:179.89ms\nstep:4004/5000 train_loss:3.7179 train_time:718486ms step_avg:179.89ms\nstep:4005/5000 train_loss:3.6418 train_time:718667ms step_avg:179.89ms\nstep:4006/5000 train_loss:3.7333 train_time:718847ms step_avg:179.89ms\nstep:4007/5000 train_loss:3.7596 train_time:719026ms step_avg:179.89ms\nstep:4008/5000 train_loss:3.5884 train_time:719206ms step_avg:179.89ms\nstep:4009/5000 train_loss:3.8218 train_time:719387ms step_avg:179.89ms\nstep:4010/5000 train_loss:3.6746 train_time:719566ms step_avg:179.89ms\nstep:4011/5000 train_loss:3.6980 train_time:719748ms step_avg:179.89ms\nstep:4012/5000 train_loss:3.6305 train_time:719929ms step_avg:179.89ms\nstep:4013/5000 train_loss:3.6390 train_time:720108ms step_avg:179.89ms\nstep:4014/5000 train_loss:3.6361 train_time:720288ms step_avg:179.89ms\nstep:4015/5000 train_loss:3.6274 train_time:720468ms step_avg:179.89ms\nstep:4016/5000 train_loss:3.5193 train_time:720649ms step_avg:179.89ms\nstep:4017/5000 train_loss:3.6946 train_time:720828ms step_avg:179.89ms\nstep:4018/5000 train_loss:3.6287 train_time:721009ms step_avg:179.89ms\nstep:4019/5000 train_loss:3.6754 train_time:721188ms step_avg:179.89ms\nstep:4020/5000 train_loss:3.7097 train_time:721366ms step_avg:179.89ms\nstep:4021/5000 train_loss:3.5674 train_time:721545ms step_avg:179.89ms\nstep:4022/5000 train_loss:3.6368 train_time:721724ms step_avg:179.89ms\nstep:4023/5000 train_loss:3.8656 train_time:721904ms step_avg:179.89ms\nstep:4024/5000 train_loss:3.7604 train_time:722084ms step_avg:179.89ms\nstep:4025/5000 train_loss:3.6027 train_time:722266ms step_avg:179.89ms\nstep:4026/5000 train_loss:3.6033 train_time:722446ms step_avg:179.89ms\nstep:4027/5000 train_loss:3.8056 train_time:722625ms step_avg:179.89ms\nstep:4028/5000 train_loss:3.5203 train_time:722808ms step_avg:179.89ms\nstep:4029/5000 train_loss:3.5324 train_time:722987ms step_avg:179.89ms\nstep:4030/5000 train_loss:3.5922 train_time:723168ms step_avg:179.89ms\nstep:4031/5000 train_loss:3.6946 train_time:723347ms step_avg:179.89ms\nstep:4032/5000 train_loss:3.6075 train_time:723529ms step_avg:179.89ms\nstep:4033/5000 train_loss:3.5852 train_time:723709ms step_avg:179.89ms\nstep:4034/5000 train_loss:3.5859 train_time:723889ms step_avg:179.89ms\nstep:4035/5000 train_loss:3.5665 train_time:724069ms step_avg:179.89ms\nstep:4036/5000 train_loss:3.5690 train_time:724249ms step_avg:179.89ms\nstep:4037/5000 train_loss:3.8731 train_time:724428ms step_avg:179.89ms\nstep:4038/5000 train_loss:3.6125 train_time:724606ms step_avg:179.89ms\nstep:4039/5000 train_loss:3.6461 train_time:724786ms step_avg:179.89ms\nstep:4040/5000 train_loss:3.6981 train_time:724966ms step_avg:179.89ms\nstep:4041/5000 train_loss:3.5807 train_time:725146ms step_avg:179.89ms\nstep:4042/5000 train_loss:3.8657 train_time:725325ms step_avg:179.89ms\nstep:4043/5000 train_loss:3.6900 train_time:725505ms step_avg:179.89ms\nstep:4044/5000 train_loss:3.8042 train_time:725684ms step_avg:179.89ms\nstep:4045/5000 train_loss:3.6718 train_time:725866ms step_avg:179.89ms\nstep:4046/5000 train_loss:3.7551 train_time:726045ms step_avg:179.89ms\nstep:4047/5000 train_loss:3.6787 train_time:726225ms step_avg:179.89ms\nstep:4048/5000 train_loss:3.6957 train_time:726404ms step_avg:179.89ms\nstep:4049/5000 train_loss:3.7720 train_time:726585ms step_avg:179.89ms\nstep:4050/5000 train_loss:3.5428 train_time:726763ms step_avg:179.89ms\nstep:4051/5000 train_loss:3.6478 train_time:726942ms step_avg:179.89ms\nstep:4052/5000 train_loss:3.6577 train_time:727123ms step_avg:179.89ms\nstep:4053/5000 train_loss:3.6056 train_time:727303ms step_avg:179.89ms\nstep:4054/5000 train_loss:3.6253 train_time:727485ms step_avg:179.89ms\nstep:4055/5000 train_loss:3.9881 train_time:727665ms step_avg:179.89ms\nstep:4056/5000 train_loss:3.5089 train_time:727844ms step_avg:179.89ms\nstep:4057/5000 train_loss:3.7378 train_time:728025ms step_avg:179.89ms\nstep:4058/5000 train_loss:3.5742 train_time:728205ms step_avg:179.89ms\nstep:4059/5000 train_loss:3.6178 train_time:728385ms step_avg:179.89ms\nstep:4060/5000 train_loss:3.6610 train_time:728564ms step_avg:179.89ms\nstep:4061/5000 train_loss:3.6817 train_time:728743ms step_avg:179.89ms\nstep:4062/5000 train_loss:3.6646 train_time:728924ms step_avg:179.89ms\nstep:4063/5000 train_loss:3.7046 train_time:729106ms step_avg:179.89ms\nstep:4064/5000 train_loss:3.7675 train_time:729286ms step_avg:179.89ms\nstep:4065/5000 train_loss:3.5229 train_time:729465ms step_avg:179.89ms\nstep:4066/5000 train_loss:3.7228 train_time:729644ms step_avg:179.89ms\nstep:4067/5000 train_loss:3.5525 train_time:729823ms step_avg:179.89ms\nstep:4068/5000 train_loss:3.4783 train_time:730002ms step_avg:179.89ms\nstep:4069/5000 train_loss:3.8855 train_time:730183ms step_avg:179.89ms\nstep:4070/5000 train_loss:3.5209 train_time:730362ms step_avg:179.89ms\nstep:4071/5000 train_loss:3.5969 train_time:730541ms step_avg:179.89ms\nstep:4072/5000 train_loss:3.5625 train_time:730720ms step_avg:179.89ms\nstep:4073/5000 train_loss:3.6441 train_time:730898ms step_avg:179.89ms\nstep:4074/5000 train_loss:3.5999 train_time:731077ms step_avg:179.89ms\nstep:4075/5000 train_loss:3.6226 train_time:731255ms step_avg:179.89ms\nstep:4076/5000 train_loss:3.5633 train_time:731433ms step_avg:179.89ms\nstep:4077/5000 train_loss:3.6077 train_time:731613ms step_avg:179.89ms\nstep:4078/5000 train_loss:3.5517 train_time:731791ms step_avg:179.89ms\nstep:4079/5000 train_loss:3.5985 train_time:731969ms step_avg:179.89ms\nstep:4080/5000 train_loss:3.5194 train_time:732148ms step_avg:179.89ms\nstep:4081/5000 train_loss:3.5211 train_time:732327ms step_avg:179.89ms\nstep:4082/5000 train_loss:3.7712 train_time:732509ms step_avg:179.89ms\nstep:4083/5000 train_loss:3.6172 train_time:732688ms step_avg:179.89ms\nstep:4084/5000 train_loss:3.6665 train_time:732870ms step_avg:179.89ms\nstep:4085/5000 train_loss:3.9788 train_time:733050ms step_avg:179.89ms\nstep:4086/5000 train_loss:3.5100 train_time:733230ms step_avg:179.89ms\nstep:4087/5000 train_loss:3.6608 train_time:733409ms step_avg:179.89ms\nstep:4088/5000 train_loss:3.5698 train_time:733588ms step_avg:179.89ms\nstep:4089/5000 train_loss:3.5690 train_time:733767ms step_avg:179.89ms\nstep:4090/5000 train_loss:3.8685 train_time:733946ms step_avg:179.89ms\nstep:4091/5000 train_loss:3.6572 train_time:734125ms step_avg:179.89ms\nstep:4092/5000 train_loss:3.5044 train_time:734307ms step_avg:179.89ms\nstep:4093/5000 train_loss:3.5913 train_time:734485ms step_avg:179.89ms\nstep:4094/5000 train_loss:3.7299 train_time:734664ms step_avg:179.89ms\nstep:4095/5000 train_loss:3.6703 train_time:734844ms step_avg:179.89ms\nstep:4096/5000 train_loss:3.6100 train_time:735024ms step_avg:179.89ms\nstep:4097/5000 train_loss:3.6034 train_time:735205ms step_avg:179.89ms\nstep:4098/5000 train_loss:3.4979 train_time:735385ms step_avg:179.89ms\nstep:4099/5000 train_loss:3.6664 train_time:735564ms step_avg:179.89ms\nstep:4100/5000 train_loss:3.6331 train_time:735743ms step_avg:179.89ms\nstep:4101/5000 train_loss:3.7268 train_time:735923ms step_avg:179.89ms\nstep:4102/5000 train_loss:3.6471 train_time:736103ms step_avg:179.89ms\nstep:4103/5000 train_loss:3.5540 train_time:736282ms step_avg:179.89ms\nstep:4104/5000 train_loss:3.6138 train_time:736462ms step_avg:179.89ms\nstep:4105/5000 train_loss:3.6218 train_time:736642ms step_avg:179.89ms\nstep:4106/5000 train_loss:3.5550 train_time:736823ms step_avg:179.89ms\nstep:4107/5000 train_loss:3.6985 train_time:737002ms step_avg:179.89ms\nstep:4108/5000 train_loss:3.6262 train_time:737181ms step_avg:179.89ms\nstep:4109/5000 train_loss:3.5413 train_time:737361ms step_avg:179.89ms\nstep:4110/5000 train_loss:3.6384 train_time:737541ms step_avg:179.89ms\nstep:4111/5000 train_loss:3.7345 train_time:737720ms step_avg:179.89ms\nstep:4112/5000 train_loss:3.7810 train_time:737899ms step_avg:179.89ms\nstep:4113/5000 train_loss:3.5223 train_time:738078ms step_avg:179.89ms\nstep:4114/5000 train_loss:3.5947 train_time:738258ms step_avg:179.89ms\nstep:4115/5000 train_loss:3.6674 train_time:738436ms step_avg:179.89ms\nstep:4116/5000 train_loss:3.4620 train_time:738615ms step_avg:179.89ms\nstep:4117/5000 train_loss:3.8143 train_time:738794ms step_avg:179.89ms\nstep:4118/5000 train_loss:3.4886 train_time:738973ms step_avg:179.89ms\nstep:4119/5000 train_loss:3.6392 train_time:739152ms step_avg:179.89ms\nstep:4120/5000 train_loss:3.5263 train_time:739331ms step_avg:179.89ms\nstep:4121/5000 train_loss:3.6062 train_time:739509ms step_avg:179.89ms\nstep:4122/5000 train_loss:3.6147 train_time:739687ms step_avg:179.89ms\nstep:4123/5000 train_loss:3.5128 train_time:739867ms step_avg:179.88ms\nstep:4124/5000 train_loss:3.4797 train_time:740046ms step_avg:179.88ms\nstep:4125/5000 train_loss:3.6779 train_time:740225ms step_avg:179.88ms\nstep:4126/5000 train_loss:4.0643 train_time:740407ms step_avg:179.89ms\nstep:4127/5000 train_loss:3.5523 train_time:740587ms step_avg:179.89ms\nstep:4128/5000 train_loss:3.6529 train_time:740768ms step_avg:179.89ms\nstep:4129/5000 train_loss:3.5355 train_time:740948ms step_avg:179.89ms\nstep:4130/5000 train_loss:3.5335 train_time:741128ms step_avg:179.89ms\nstep:4131/5000 train_loss:3.6515 train_time:741308ms step_avg:179.89ms\nstep:4132/5000 train_loss:3.6408 train_time:741488ms step_avg:179.89ms\nstep:4133/5000 train_loss:3.3536 train_time:741669ms step_avg:179.89ms\nstep:4134/5000 train_loss:3.5606 train_time:741849ms step_avg:179.89ms\nstep:4135/5000 train_loss:3.5836 train_time:742027ms step_avg:179.89ms\nstep:4136/5000 train_loss:3.8157 train_time:742206ms step_avg:179.89ms\nstep:4137/5000 train_loss:3.6153 train_time:742386ms step_avg:179.89ms\nstep:4138/5000 train_loss:3.6098 train_time:742565ms step_avg:179.88ms\nstep:4139/5000 train_loss:3.7091 train_time:742745ms step_avg:179.89ms\nstep:4140/5000 train_loss:3.6220 train_time:742925ms step_avg:179.88ms\nstep:4141/5000 train_loss:3.5524 train_time:743104ms step_avg:179.88ms\nstep:4142/5000 train_loss:3.6630 train_time:743282ms step_avg:179.88ms\nstep:4143/5000 train_loss:3.7010 train_time:743461ms step_avg:179.88ms\nstep:4144/5000 train_loss:3.5333 train_time:743640ms step_avg:179.88ms\nstep:4145/5000 train_loss:3.6724 train_time:743821ms step_avg:179.88ms\nstep:4146/5000 train_loss:3.4668 train_time:744001ms step_avg:179.88ms\nstep:4147/5000 train_loss:3.7636 train_time:744180ms step_avg:179.88ms\nstep:4148/5000 train_loss:3.5587 train_time:744360ms step_avg:179.88ms\nstep:4149/5000 train_loss:3.5965 train_time:744538ms step_avg:179.88ms\nstep:4150/5000 train_loss:3.4661 train_time:744717ms step_avg:179.88ms\nstep:4151/5000 train_loss:3.6236 train_time:744897ms step_avg:179.88ms\nstep:4152/5000 train_loss:3.6232 train_time:745076ms step_avg:179.88ms\nstep:4153/5000 train_loss:3.5674 train_time:745254ms step_avg:179.88ms\nstep:4154/5000 train_loss:3.6708 train_time:745432ms step_avg:179.88ms\nstep:4155/5000 train_loss:3.7456 train_time:745611ms step_avg:179.88ms\nstep:4156/5000 train_loss:3.9408 train_time:745790ms step_avg:179.88ms\nstep:4157/5000 train_loss:3.6105 train_time:745969ms step_avg:179.88ms\nstep:4158/5000 train_loss:3.4710 train_time:746149ms step_avg:179.88ms\nstep:4159/5000 train_loss:3.7028 train_time:746329ms step_avg:179.88ms\nstep:4160/5000 train_loss:3.7487 train_time:746509ms step_avg:179.88ms\nstep:4161/5000 train_loss:3.6668 train_time:746689ms step_avg:179.88ms\nstep:4162/5000 train_loss:3.4769 train_time:746867ms step_avg:179.88ms\nstep:4163/5000 train_loss:3.5736 train_time:747047ms step_avg:179.88ms\nstep:4164/5000 train_loss:3.6506 train_time:747225ms step_avg:179.88ms\nstep:4165/5000 train_loss:3.5121 train_time:747405ms step_avg:179.88ms\nstep:4166/5000 train_loss:3.6220 train_time:747585ms step_avg:179.88ms\nstep:4167/5000 train_loss:3.6381 train_time:747767ms step_avg:179.88ms\nstep:4168/5000 train_loss:3.5458 train_time:747948ms step_avg:179.88ms\nstep:4169/5000 train_loss:3.6395 train_time:748130ms step_avg:179.88ms\nstep:4170/5000 train_loss:3.4546 train_time:748308ms step_avg:179.88ms\nstep:4171/5000 train_loss:3.6688 train_time:748485ms step_avg:179.88ms\nstep:4172/5000 train_loss:3.6099 train_time:748664ms step_avg:179.88ms\nstep:4173/5000 train_loss:3.7938 train_time:748844ms step_avg:179.88ms\nstep:4174/5000 train_loss:3.5390 train_time:749025ms step_avg:179.88ms\nstep:4175/5000 train_loss:3.5643 train_time:749204ms step_avg:179.88ms\nstep:4176/5000 train_loss:3.6485 train_time:749384ms step_avg:179.88ms\nstep:4177/5000 train_loss:3.6094 train_time:749563ms step_avg:179.88ms\nstep:4178/5000 train_loss:3.6353 train_time:749741ms step_avg:179.88ms\nstep:4179/5000 train_loss:3.7962 train_time:749920ms step_avg:179.88ms\nstep:4180/5000 train_loss:3.5809 train_time:750100ms step_avg:179.88ms\nstep:4181/5000 train_loss:3.5844 train_time:750280ms step_avg:179.88ms\nstep:4182/5000 train_loss:3.7760 train_time:750458ms step_avg:179.88ms\nstep:4183/5000 train_loss:3.7591 train_time:750637ms step_avg:179.88ms\nstep:4184/5000 train_loss:3.5891 train_time:750816ms step_avg:179.88ms\nstep:4185/5000 train_loss:3.5686 train_time:750995ms step_avg:179.88ms\nstep:4186/5000 train_loss:3.4557 train_time:751173ms step_avg:179.88ms\nstep:4187/5000 train_loss:3.5714 train_time:751353ms step_avg:179.88ms\nstep:4188/5000 train_loss:3.6655 train_time:751531ms step_avg:179.88ms\nstep:4189/5000 train_loss:3.6129 train_time:751709ms step_avg:179.88ms\nstep:4190/5000 train_loss:3.7994 train_time:751887ms step_avg:179.88ms\nstep:4191/5000 train_loss:3.5671 train_time:752067ms step_avg:179.88ms\nstep:4192/5000 train_loss:3.5752 train_time:752246ms step_avg:179.88ms\nstep:4193/5000 train_loss:3.5797 train_time:752425ms step_avg:179.88ms\nstep:4194/5000 train_loss:3.5104 train_time:752605ms step_avg:179.88ms\nstep:4195/5000 train_loss:3.5785 train_time:752785ms step_avg:179.88ms\nstep:4196/5000 train_loss:3.7753 train_time:752964ms step_avg:179.88ms\nstep:4197/5000 train_loss:3.5029 train_time:753143ms step_avg:179.88ms\nstep:4198/5000 train_loss:4.1006 train_time:753325ms step_avg:179.88ms\nstep:4199/5000 train_loss:3.6166 train_time:753505ms step_avg:179.88ms\nstep:4200/5000 train_loss:3.6304 train_time:753687ms step_avg:179.88ms\nstep:4201/5000 train_loss:3.4326 train_time:753868ms step_avg:179.88ms\nstep:4202/5000 train_loss:3.5958 train_time:754046ms step_avg:179.88ms\nstep:4203/5000 train_loss:3.6435 train_time:754226ms step_avg:179.88ms\nstep:4204/5000 train_loss:3.5463 train_time:754405ms step_avg:179.88ms\nstep:4205/5000 train_loss:3.6324 train_time:754585ms step_avg:179.88ms\nstep:4206/5000 train_loss:3.5362 train_time:754765ms step_avg:179.88ms\nstep:4207/5000 train_loss:3.5535 train_time:754944ms step_avg:179.88ms\nstep:4208/5000 train_loss:3.5144 train_time:755124ms step_avg:179.88ms\nstep:4209/5000 train_loss:3.5900 train_time:755304ms step_avg:179.88ms\nstep:4210/5000 train_loss:3.5365 train_time:755487ms step_avg:179.88ms\nstep:4211/5000 train_loss:3.6126 train_time:755666ms step_avg:179.88ms\nstep:4212/5000 train_loss:3.6294 train_time:755846ms step_avg:179.88ms\nstep:4213/5000 train_loss:3.6168 train_time:756025ms step_avg:179.88ms\nstep:4214/5000 train_loss:3.6408 train_time:756206ms step_avg:179.88ms\nstep:4215/5000 train_loss:3.6464 train_time:756386ms step_avg:179.88ms\nstep:4216/5000 train_loss:3.5155 train_time:756567ms step_avg:179.88ms\nstep:4217/5000 train_loss:3.5660 train_time:756746ms step_avg:179.88ms\nstep:4218/5000 train_loss:3.5619 train_time:756924ms step_avg:179.88ms\nstep:4219/5000 train_loss:3.6526 train_time:757104ms step_avg:179.88ms\nstep:4220/5000 train_loss:3.8324 train_time:757287ms step_avg:179.88ms\nstep:4221/5000 train_loss:3.6592 train_time:757467ms step_avg:179.88ms\nstep:4222/5000 train_loss:3.6187 train_time:757647ms step_avg:179.88ms\nstep:4223/5000 train_loss:3.5024 train_time:757828ms step_avg:179.88ms\nstep:4224/5000 train_loss:3.6735 train_time:758008ms step_avg:179.88ms\nstep:4225/5000 train_loss:3.6557 train_time:758187ms step_avg:179.88ms\nstep:4226/5000 train_loss:3.5344 train_time:758366ms step_avg:179.88ms\nstep:4227/5000 train_loss:3.5017 train_time:758546ms step_avg:179.88ms\nstep:4228/5000 train_loss:3.5263 train_time:758726ms step_avg:179.88ms\nstep:4229/5000 train_loss:3.4863 train_time:758905ms step_avg:179.88ms\nstep:4230/5000 train_loss:3.5587 train_time:759085ms step_avg:179.88ms\nstep:4231/5000 train_loss:3.5734 train_time:759264ms step_avg:179.88ms\nstep:4232/5000 train_loss:3.6275 train_time:759443ms step_avg:179.88ms\nstep:4233/5000 train_loss:3.5028 train_time:759624ms step_avg:179.88ms\nstep:4234/5000 train_loss:3.6098 train_time:759803ms step_avg:179.88ms\nstep:4235/5000 train_loss:3.4854 train_time:759984ms step_avg:179.88ms\nstep:4236/5000 train_loss:3.7040 train_time:760163ms step_avg:179.88ms\nstep:4237/5000 train_loss:3.5668 train_time:760342ms step_avg:179.88ms\nstep:4238/5000 train_loss:3.7989 train_time:760522ms step_avg:179.88ms\nstep:4239/5000 train_loss:3.5618 train_time:760701ms step_avg:179.88ms\nstep:4240/5000 train_loss:3.4196 train_time:760880ms step_avg:179.88ms\nstep:4241/5000 train_loss:3.5186 train_time:761059ms step_avg:179.88ms\nstep:4242/5000 train_loss:3.5700 train_time:761239ms step_avg:179.88ms\nstep:4243/5000 train_loss:3.5483 train_time:761418ms step_avg:179.88ms\nstep:4244/5000 train_loss:3.5609 train_time:761597ms step_avg:179.88ms\nstep:4245/5000 train_loss:3.6352 train_time:761776ms step_avg:179.88ms\nstep:4246/5000 train_loss:3.7591 train_time:761954ms step_avg:179.88ms\nstep:4247/5000 train_loss:3.5949 train_time:762135ms step_avg:179.88ms\nstep:4248/5000 train_loss:3.5699 train_time:762311ms step_avg:179.88ms\nstep:4249/5000 train_loss:3.5707 train_time:762491ms step_avg:179.88ms\nstep:4250/5000 train_loss:3.4674 train_time:762671ms step_avg:179.88ms\nstep:4251/5000 train_loss:3.4362 train_time:762850ms step_avg:179.88ms\nstep:4252/5000 train_loss:3.5365 train_time:763028ms step_avg:179.87ms\nstep:4253/5000 train_loss:3.7711 train_time:763207ms step_avg:179.87ms\nstep:4254/5000 train_loss:3.5338 train_time:763386ms step_avg:179.87ms\nstep:4255/5000 train_loss:3.6608 train_time:763566ms step_avg:179.87ms\nstep:4256/5000 train_loss:3.5111 train_time:763747ms step_avg:179.87ms\nstep:4257/5000 train_loss:3.5486 train_time:763928ms step_avg:179.87ms\nstep:4258/5000 train_loss:3.6906 train_time:764108ms step_avg:179.87ms\nstep:4259/5000 train_loss:3.5572 train_time:764287ms step_avg:179.87ms\nstep:4260/5000 train_loss:3.6209 train_time:764465ms step_avg:179.87ms\nstep:4261/5000 train_loss:3.5212 train_time:764645ms step_avg:179.87ms\nstep:4262/5000 train_loss:3.6017 train_time:764824ms step_avg:179.87ms\nstep:4263/5000 train_loss:3.5890 train_time:765006ms step_avg:179.87ms\nstep:4264/5000 train_loss:3.5062 train_time:765185ms step_avg:179.87ms\nstep:4265/5000 train_loss:3.6439 train_time:765364ms step_avg:179.87ms\nstep:4266/5000 train_loss:3.7570 train_time:765543ms step_avg:179.87ms\nstep:4267/5000 train_loss:3.7204 train_time:765722ms step_avg:179.87ms\nstep:4268/5000 train_loss:3.5062 train_time:765902ms step_avg:179.87ms\nstep:4269/5000 train_loss:3.6608 train_time:766082ms step_avg:179.87ms\nstep:4270/5000 train_loss:3.2358 train_time:766262ms step_avg:179.87ms\nstep:4271/5000 train_loss:3.6823 train_time:766442ms step_avg:179.87ms\nstep:4272/5000 train_loss:3.5258 train_time:766621ms step_avg:179.87ms\nstep:4273/5000 train_loss:3.6761 train_time:766800ms step_avg:179.87ms\nstep:4274/5000 train_loss:3.5113 train_time:766980ms step_avg:179.87ms\nstep:4275/5000 train_loss:3.5395 train_time:767161ms step_avg:179.87ms\nstep:4276/5000 train_loss:3.4339 train_time:767340ms step_avg:179.87ms\nstep:4277/5000 train_loss:3.6495 train_time:767519ms step_avg:179.87ms\nstep:4278/5000 train_loss:3.6231 train_time:767699ms step_avg:179.87ms\nstep:4279/5000 train_loss:3.5585 train_time:767877ms step_avg:179.87ms\nstep:4280/5000 train_loss:3.5483 train_time:768056ms step_avg:179.87ms\nstep:4281/5000 train_loss:3.5137 train_time:768235ms step_avg:179.87ms\nstep:4282/5000 train_loss:3.6047 train_time:768413ms step_avg:179.87ms\nstep:4283/5000 train_loss:3.7808 train_time:768591ms step_avg:179.87ms\nstep:4284/5000 train_loss:3.4124 train_time:768769ms step_avg:179.87ms\nstep:4285/5000 train_loss:3.6497 train_time:768949ms step_avg:179.87ms\nstep:4286/5000 train_loss:3.5575 train_time:769128ms step_avg:179.87ms\nstep:4287/5000 train_loss:3.6495 train_time:769309ms step_avg:179.87ms\nstep:4288/5000 train_loss:3.6790 train_time:769489ms step_avg:179.87ms\nstep:4289/5000 train_loss:3.7114 train_time:769668ms step_avg:179.87ms\nstep:4290/5000 train_loss:3.4014 train_time:769847ms step_avg:179.87ms\nstep:4291/5000 train_loss:3.5664 train_time:770026ms step_avg:179.87ms\nstep:4292/5000 train_loss:3.6289 train_time:770205ms step_avg:179.87ms\nstep:4293/5000 train_loss:3.5185 train_time:770388ms step_avg:179.87ms\nstep:4294/5000 train_loss:3.5463 train_time:770569ms step_avg:179.87ms\nstep:4295/5000 train_loss:3.5572 train_time:770748ms step_avg:179.87ms\nstep:4296/5000 train_loss:3.5748 train_time:770927ms step_avg:179.87ms\nstep:4297/5000 train_loss:3.5392 train_time:771106ms step_avg:179.87ms\nstep:4298/5000 train_loss:3.6071 train_time:771285ms step_avg:179.87ms\nstep:4299/5000 train_loss:3.6657 train_time:771467ms step_avg:179.87ms\nstep:4300/5000 train_loss:3.6403 train_time:771647ms step_avg:179.87ms\nstep:4301/5000 train_loss:3.2713 train_time:771826ms step_avg:179.87ms\nstep:4302/5000 train_loss:3.5088 train_time:772006ms step_avg:179.87ms\nstep:4303/5000 train_loss:3.4451 train_time:772185ms step_avg:179.87ms\nstep:4304/5000 train_loss:3.5843 train_time:772365ms step_avg:179.87ms\nstep:4305/5000 train_loss:3.7230 train_time:772547ms step_avg:179.87ms\nstep:4306/5000 train_loss:3.6978 train_time:772727ms step_avg:179.87ms\nstep:4307/5000 train_loss:3.4917 train_time:772907ms step_avg:179.87ms\nstep:4308/5000 train_loss:3.5259 train_time:773087ms step_avg:179.87ms\nstep:4309/5000 train_loss:3.6185 train_time:773265ms step_avg:179.87ms\nstep:4310/5000 train_loss:3.5946 train_time:773445ms step_avg:179.87ms\nstep:4311/5000 train_loss:3.6069 train_time:773625ms step_avg:179.87ms\nstep:4312/5000 train_loss:3.9275 train_time:773806ms step_avg:179.87ms\nstep:4313/5000 train_loss:3.6940 train_time:773986ms step_avg:179.87ms\nstep:4314/5000 train_loss:3.5757 train_time:774166ms step_avg:179.87ms\nstep:4315/5000 train_loss:3.5622 train_time:774345ms step_avg:179.87ms\nstep:4316/5000 train_loss:3.5816 train_time:774525ms step_avg:179.87ms\nstep:4317/5000 train_loss:3.4843 train_time:774705ms step_avg:179.87ms\nstep:4318/5000 train_loss:3.5789 train_time:774885ms step_avg:179.87ms\nstep:4319/5000 train_loss:3.6061 train_time:775065ms step_avg:179.87ms\nstep:4320/5000 train_loss:3.3340 train_time:775244ms step_avg:179.87ms\nstep:4321/5000 train_loss:3.5110 train_time:775423ms step_avg:179.87ms\nstep:4322/5000 train_loss:3.6319 train_time:775603ms step_avg:179.87ms\nstep:4323/5000 train_loss:3.6019 train_time:775782ms step_avg:179.87ms\nstep:4324/5000 train_loss:3.6584 train_time:775964ms step_avg:179.87ms\nstep:4325/5000 train_loss:3.5832 train_time:776145ms step_avg:179.87ms\nstep:4326/5000 train_loss:3.6131 train_time:776325ms step_avg:179.87ms\nstep:4327/5000 train_loss:3.5537 train_time:776504ms step_avg:179.87ms\nstep:4328/5000 train_loss:3.5728 train_time:776684ms step_avg:179.87ms\nstep:4329/5000 train_loss:3.6009 train_time:776864ms step_avg:179.87ms\nstep:4330/5000 train_loss:3.6026 train_time:777043ms step_avg:179.87ms\nstep:4331/5000 train_loss:3.5147 train_time:777223ms step_avg:179.87ms\nstep:4332/5000 train_loss:3.6338 train_time:777403ms step_avg:179.87ms\nstep:4333/5000 train_loss:3.4815 train_time:777583ms step_avg:179.87ms\nstep:4334/5000 train_loss:3.5443 train_time:777763ms step_avg:179.87ms\nstep:4335/5000 train_loss:3.5010 train_time:777942ms step_avg:179.87ms\nstep:4336/5000 train_loss:3.7319 train_time:778121ms step_avg:179.87ms\nstep:4337/5000 train_loss:3.5058 train_time:778300ms step_avg:179.87ms\nstep:4338/5000 train_loss:3.3504 train_time:778481ms step_avg:179.87ms\nstep:4339/5000 train_loss:3.4764 train_time:778660ms step_avg:179.87ms\nstep:4340/5000 train_loss:3.6211 train_time:778840ms step_avg:179.87ms\nstep:4341/5000 train_loss:3.5435 train_time:779019ms step_avg:179.87ms\nstep:4342/5000 train_loss:3.4875 train_time:779198ms step_avg:179.87ms\nstep:4343/5000 train_loss:3.4478 train_time:779378ms step_avg:179.87ms\nstep:4344/5000 train_loss:3.5998 train_time:779558ms step_avg:179.87ms\nstep:4345/5000 train_loss:3.5946 train_time:779737ms step_avg:179.87ms\nstep:4346/5000 train_loss:3.6264 train_time:779916ms step_avg:179.87ms\nstep:4347/5000 train_loss:3.5127 train_time:780094ms step_avg:179.87ms\nstep:4348/5000 train_loss:3.5632 train_time:780273ms step_avg:179.87ms\nstep:4349/5000 train_loss:3.5961 train_time:780452ms step_avg:179.87ms\nstep:4350/5000 train_loss:3.4586 train_time:780630ms step_avg:179.87ms\nstep:4351/5000 train_loss:3.5657 train_time:780808ms step_avg:179.87ms\nstep:4352/5000 train_loss:3.7561 train_time:780988ms step_avg:179.87ms\nstep:4353/5000 train_loss:3.6513 train_time:781168ms step_avg:179.87ms\nstep:4354/5000 train_loss:3.4924 train_time:781347ms step_avg:179.87ms\nstep:4355/5000 train_loss:3.6256 train_time:781527ms step_avg:179.87ms\nstep:4356/5000 train_loss:3.6522 train_time:781707ms step_avg:179.87ms\nstep:4357/5000 train_loss:3.6396 train_time:781886ms step_avg:179.87ms\nstep:4358/5000 train_loss:3.5688 train_time:782065ms step_avg:179.87ms\nstep:4359/5000 train_loss:3.4456 train_time:782243ms step_avg:179.87ms\nstep:4360/5000 train_loss:3.4793 train_time:782422ms step_avg:179.87ms\nstep:4361/5000 train_loss:3.5273 train_time:782602ms step_avg:179.87ms\nstep:4362/5000 train_loss:3.6042 train_time:782783ms step_avg:179.87ms\nstep:4363/5000 train_loss:3.4763 train_time:782963ms step_avg:179.87ms\nstep:4364/5000 train_loss:3.5027 train_time:783142ms step_avg:179.87ms\nstep:4365/5000 train_loss:3.5492 train_time:783322ms step_avg:179.87ms\nstep:4366/5000 train_loss:3.6315 train_time:783503ms step_avg:179.87ms\nstep:4367/5000 train_loss:3.5111 train_time:783685ms step_avg:179.87ms\nstep:4368/5000 train_loss:3.6150 train_time:783865ms step_avg:179.87ms\nstep:4369/5000 train_loss:3.5620 train_time:784044ms step_avg:179.87ms\nstep:4370/5000 train_loss:3.5984 train_time:784223ms step_avg:179.87ms\nstep:4371/5000 train_loss:3.4708 train_time:784403ms step_avg:179.87ms\nstep:4372/5000 train_loss:3.4221 train_time:784584ms step_avg:179.87ms\nstep:4373/5000 train_loss:3.5374 train_time:784763ms step_avg:179.87ms\nstep:4374/5000 train_loss:3.4976 train_time:784943ms step_avg:179.87ms\nstep:4375/5000 train_loss:3.5430 train_time:785122ms step_avg:179.87ms\nstep:4376/5000 train_loss:3.6082 train_time:785301ms step_avg:179.87ms\nstep:4377/5000 train_loss:3.5117 train_time:785480ms step_avg:179.87ms\nstep:4378/5000 train_loss:3.4641 train_time:785661ms step_avg:179.87ms\nstep:4379/5000 train_loss:3.5370 train_time:785841ms step_avg:179.87ms\nstep:4380/5000 train_loss:3.7494 train_time:786021ms step_avg:179.87ms\nstep:4381/5000 train_loss:3.5883 train_time:786201ms step_avg:179.87ms\nstep:4382/5000 train_loss:3.7024 train_time:786381ms step_avg:179.87ms\nstep:4383/5000 train_loss:3.5376 train_time:786562ms step_avg:179.87ms\nstep:4384/5000 train_loss:3.5878 train_time:786742ms step_avg:179.87ms\nstep:4385/5000 train_loss:3.7068 train_time:786922ms step_avg:179.87ms\nstep:4386/5000 train_loss:3.4643 train_time:787101ms step_avg:179.87ms\nstep:4387/5000 train_loss:3.4740 train_time:787280ms step_avg:179.87ms\nstep:4388/5000 train_loss:3.5310 train_time:787459ms step_avg:179.87ms\nstep:4389/5000 train_loss:3.6322 train_time:787639ms step_avg:179.87ms\nstep:4390/5000 train_loss:3.5194 train_time:787820ms step_avg:179.87ms\nstep:4391/5000 train_loss:3.5951 train_time:788001ms step_avg:179.87ms\nstep:4392/5000 train_loss:3.5540 train_time:788181ms step_avg:179.87ms\nstep:4393/5000 train_loss:3.6699 train_time:788361ms step_avg:179.87ms\nstep:4394/5000 train_loss:3.5298 train_time:788541ms step_avg:179.87ms\nstep:4395/5000 train_loss:3.4961 train_time:788721ms step_avg:179.87ms\nstep:4396/5000 train_loss:3.6102 train_time:788901ms step_avg:179.87ms\nstep:4397/5000 train_loss:3.5419 train_time:789080ms step_avg:179.87ms\nstep:4398/5000 train_loss:3.5903 train_time:789260ms step_avg:179.87ms\nstep:4399/5000 train_loss:3.6299 train_time:789440ms step_avg:179.87ms\nstep:4400/5000 train_loss:3.6815 train_time:789619ms step_avg:179.87ms\nstep:4401/5000 train_loss:3.4805 train_time:789799ms step_avg:179.87ms\nstep:4402/5000 train_loss:3.6680 train_time:789978ms step_avg:179.87ms\nstep:4403/5000 train_loss:3.4914 train_time:790157ms step_avg:179.87ms\nstep:4404/5000 train_loss:3.6382 train_time:790336ms step_avg:179.87ms\nstep:4405/5000 train_loss:3.5385 train_time:790515ms step_avg:179.87ms\nstep:4406/5000 train_loss:3.5358 train_time:790694ms step_avg:179.87ms\nstep:4407/5000 train_loss:3.4596 train_time:790873ms step_avg:179.87ms\nstep:4408/5000 train_loss:3.5588 train_time:791051ms step_avg:179.87ms\nstep:4409/5000 train_loss:3.6025 train_time:791230ms step_avg:179.87ms\nstep:4410/5000 train_loss:3.5485 train_time:791408ms step_avg:179.87ms\nstep:4411/5000 train_loss:3.5152 train_time:791587ms step_avg:179.87ms\nstep:4412/5000 train_loss:3.4150 train_time:791765ms step_avg:179.86ms\nstep:4413/5000 train_loss:3.5879 train_time:791946ms step_avg:179.87ms\nstep:4414/5000 train_loss:3.5184 train_time:792126ms step_avg:179.87ms\nstep:4415/5000 train_loss:3.7103 train_time:792305ms step_avg:179.87ms\nstep:4416/5000 train_loss:3.5116 train_time:792485ms step_avg:179.86ms\nstep:4417/5000 train_loss:3.6418 train_time:792664ms step_avg:179.86ms\nstep:4418/5000 train_loss:3.4688 train_time:792843ms step_avg:179.86ms\nstep:4419/5000 train_loss:3.8528 train_time:793026ms step_avg:179.87ms\nstep:4420/5000 train_loss:3.5650 train_time:793205ms step_avg:179.87ms\nstep:4421/5000 train_loss:3.6120 train_time:793385ms step_avg:179.87ms\nstep:4422/5000 train_loss:3.5943 train_time:793566ms step_avg:179.87ms\nstep:4423/5000 train_loss:3.5603 train_time:793745ms step_avg:179.87ms\nstep:4424/5000 train_loss:3.4459 train_time:793925ms step_avg:179.87ms\nstep:4425/5000 train_loss:3.7961 train_time:794107ms step_avg:179.87ms\nstep:4426/5000 train_loss:3.6942 train_time:794286ms step_avg:179.87ms\nstep:4427/5000 train_loss:3.8167 train_time:794465ms step_avg:179.87ms\nstep:4428/5000 train_loss:4.2094 train_time:794644ms step_avg:179.87ms\nstep:4429/5000 train_loss:3.5970 train_time:794823ms step_avg:179.86ms\nstep:4430/5000 train_loss:3.4923 train_time:795005ms step_avg:179.87ms\nstep:4431/5000 train_loss:3.6185 train_time:795185ms step_avg:179.87ms\nstep:4432/5000 train_loss:3.3515 train_time:795365ms step_avg:179.87ms\nstep:4433/5000 train_loss:3.6107 train_time:795543ms step_avg:179.87ms\nstep:4434/5000 train_loss:3.7683 train_time:795723ms step_avg:179.87ms\nstep:4435/5000 train_loss:3.6454 train_time:795903ms step_avg:179.87ms\nstep:4436/5000 train_loss:3.6981 train_time:796084ms step_avg:179.87ms\nstep:4437/5000 train_loss:3.6219 train_time:796265ms step_avg:179.87ms\nstep:4438/5000 train_loss:3.4851 train_time:796444ms step_avg:179.87ms\nstep:4439/5000 train_loss:3.7679 train_time:796624ms step_avg:179.87ms\nstep:4440/5000 train_loss:3.4846 train_time:796804ms step_avg:179.87ms\nstep:4441/5000 train_loss:3.5211 train_time:796985ms step_avg:179.87ms\nstep:4442/5000 train_loss:3.5879 train_time:797167ms step_avg:179.87ms\nstep:4443/5000 train_loss:3.6606 train_time:797346ms step_avg:179.87ms\nstep:4444/5000 train_loss:3.6205 train_time:797528ms step_avg:179.87ms\nstep:4445/5000 train_loss:3.5371 train_time:797707ms step_avg:179.87ms\nstep:4446/5000 train_loss:3.5918 train_time:797887ms step_avg:179.87ms\nstep:4447/5000 train_loss:3.4456 train_time:798065ms step_avg:179.87ms\nstep:4448/5000 train_loss:3.5462 train_time:798245ms step_avg:179.87ms\nstep:4449/5000 train_loss:3.5164 train_time:798425ms step_avg:179.87ms\nstep:4450/5000 train_loss:3.5776 train_time:798608ms step_avg:179.87ms\nstep:4451/5000 train_loss:3.5155 train_time:798787ms step_avg:179.87ms\nstep:4452/5000 train_loss:3.5962 train_time:798966ms step_avg:179.87ms\nstep:4453/5000 train_loss:3.3855 train_time:799146ms step_avg:179.87ms\nstep:4454/5000 train_loss:3.5460 train_time:799327ms step_avg:179.87ms\nstep:4455/5000 train_loss:3.4223 train_time:799507ms step_avg:179.87ms\nstep:4456/5000 train_loss:3.6866 train_time:799686ms step_avg:179.87ms\nstep:4457/5000 train_loss:3.6806 train_time:799865ms step_avg:179.87ms\nstep:4458/5000 train_loss:3.4498 train_time:800044ms step_avg:179.87ms\nstep:4459/5000 train_loss:3.5870 train_time:800225ms step_avg:179.87ms\nstep:4460/5000 train_loss:3.5450 train_time:800405ms step_avg:179.87ms\nstep:4461/5000 train_loss:3.5686 train_time:800584ms step_avg:179.87ms\nstep:4462/5000 train_loss:3.6543 train_time:800763ms step_avg:179.87ms\nstep:4463/5000 train_loss:3.6893 train_time:800943ms step_avg:179.87ms\nstep:4464/5000 train_loss:3.4909 train_time:801123ms step_avg:179.87ms\nstep:4465/5000 train_loss:3.5730 train_time:801304ms step_avg:179.87ms\nstep:4466/5000 train_loss:3.6466 train_time:801485ms step_avg:179.87ms\nstep:4467/5000 train_loss:3.7696 train_time:801664ms step_avg:179.87ms\nstep:4468/5000 train_loss:3.3806 train_time:801844ms step_avg:179.87ms\nstep:4469/5000 train_loss:3.5651 train_time:802022ms step_avg:179.87ms\nstep:4470/5000 train_loss:3.4622 train_time:802202ms step_avg:179.87ms\nstep:4471/5000 train_loss:3.4851 train_time:802383ms step_avg:179.87ms\nstep:4472/5000 train_loss:3.5562 train_time:802563ms step_avg:179.87ms\nstep:4473/5000 train_loss:3.6522 train_time:802743ms step_avg:179.87ms\nstep:4474/5000 train_loss:3.7602 train_time:802923ms step_avg:179.87ms\nstep:4475/5000 train_loss:3.4104 train_time:803102ms step_avg:179.87ms\nstep:4476/5000 train_loss:3.7035 train_time:803281ms step_avg:179.87ms\nstep:4477/5000 train_loss:3.5091 train_time:803461ms step_avg:179.87ms\nstep:4478/5000 train_loss:3.5399 train_time:803641ms step_avg:179.87ms\nstep:4479/5000 train_loss:3.7007 train_time:803822ms step_avg:179.87ms\nstep:4480/5000 train_loss:3.7423 train_time:804001ms step_avg:179.87ms\nstep:4481/5000 train_loss:3.5668 train_time:804180ms step_avg:179.87ms\nstep:4482/5000 train_loss:3.5052 train_time:804360ms step_avg:179.87ms\nstep:4483/5000 train_loss:3.4925 train_time:804540ms step_avg:179.87ms\nstep:4484/5000 train_loss:3.5813 train_time:804720ms step_avg:179.87ms\nstep:4485/5000 train_loss:3.6089 train_time:804899ms step_avg:179.87ms\nstep:4486/5000 train_loss:3.5662 train_time:805079ms step_avg:179.87ms\nstep:4487/5000 train_loss:3.5173 train_time:805259ms step_avg:179.87ms\nstep:4488/5000 train_loss:3.6165 train_time:805438ms step_avg:179.87ms\nstep:4489/5000 train_loss:3.4803 train_time:805619ms step_avg:179.87ms\nstep:4490/5000 train_loss:3.8180 train_time:805798ms step_avg:179.87ms\nstep:4491/5000 train_loss:3.6529 train_time:805979ms step_avg:179.87ms\nstep:4492/5000 train_loss:3.5127 train_time:806157ms step_avg:179.87ms\nstep:4493/5000 train_loss:3.6275 train_time:806336ms step_avg:179.87ms\nstep:4494/5000 train_loss:3.5067 train_time:806515ms step_avg:179.87ms\nstep:4495/5000 train_loss:3.5295 train_time:806693ms step_avg:179.86ms\nstep:4496/5000 train_loss:3.5712 train_time:806872ms step_avg:179.86ms\nstep:4497/5000 train_loss:3.5944 train_time:807052ms step_avg:179.86ms\nstep:4498/5000 train_loss:3.5371 train_time:807230ms step_avg:179.86ms\nstep:4499/5000 train_loss:3.5274 train_time:807409ms step_avg:179.86ms\nstep:4500/5000 train_loss:3.7079 train_time:807587ms step_avg:179.86ms\nstep:4501/5000 train_loss:3.4805 train_time:807766ms step_avg:179.86ms\nstep:4502/5000 train_loss:3.5186 train_time:807947ms step_avg:179.86ms\nstep:4503/5000 train_loss:3.6028 train_time:808127ms step_avg:179.86ms\nstep:4504/5000 train_loss:3.4689 train_time:808306ms step_avg:179.86ms\nstep:4505/5000 train_loss:3.6486 train_time:808486ms step_avg:179.86ms\nstep:4506/5000 train_loss:3.5984 train_time:808664ms step_avg:179.86ms\nstep:4507/5000 train_loss:3.5209 train_time:808843ms step_avg:179.86ms\nstep:4508/5000 train_loss:3.3793 train_time:809025ms step_avg:179.86ms\nstep:4509/5000 train_loss:3.6265 train_time:809207ms step_avg:179.86ms\nstep:4510/5000 train_loss:3.6447 train_time:809389ms step_avg:179.86ms\nstep:4511/5000 train_loss:3.7924 train_time:809568ms step_avg:179.86ms\nstep:4512/5000 train_loss:3.5980 train_time:809746ms step_avg:179.86ms\nstep:4513/5000 train_loss:3.4698 train_time:809926ms step_avg:179.86ms\nstep:4514/5000 train_loss:3.4524 train_time:810108ms step_avg:179.86ms\nstep:4515/5000 train_loss:3.5532 train_time:810287ms step_avg:179.86ms\nstep:4516/5000 train_loss:3.8105 train_time:810468ms step_avg:179.86ms\nstep:4517/5000 train_loss:3.5137 train_time:810648ms step_avg:179.86ms\nstep:4518/5000 train_loss:3.4906 train_time:810828ms step_avg:179.86ms\nstep:4519/5000 train_loss:3.7320 train_time:811008ms step_avg:179.86ms\nstep:4520/5000 train_loss:3.3529 train_time:811186ms step_avg:179.86ms\nstep:4521/5000 train_loss:3.4792 train_time:811366ms step_avg:179.86ms\nstep:4522/5000 train_loss:3.5757 train_time:811546ms step_avg:179.86ms\nstep:4523/5000 train_loss:3.5223 train_time:811726ms step_avg:179.86ms\nstep:4524/5000 train_loss:3.6735 train_time:811907ms step_avg:179.86ms\nstep:4525/5000 train_loss:3.6314 train_time:812088ms step_avg:179.86ms\nstep:4526/5000 train_loss:3.4861 train_time:812267ms step_avg:179.86ms\nstep:4527/5000 train_loss:3.5671 train_time:812445ms step_avg:179.86ms\nstep:4528/5000 train_loss:3.5016 train_time:812629ms step_avg:179.86ms\nstep:4529/5000 train_loss:3.5219 train_time:812808ms step_avg:179.86ms\nstep:4530/5000 train_loss:3.6112 train_time:812988ms step_avg:179.86ms\nstep:4531/5000 train_loss:3.4003 train_time:813169ms step_avg:179.86ms\nstep:4532/5000 train_loss:3.5307 train_time:813348ms step_avg:179.86ms\nstep:4533/5000 train_loss:3.6678 train_time:813527ms step_avg:179.86ms\nstep:4534/5000 train_loss:3.5640 train_time:813706ms step_avg:179.86ms\nstep:4535/5000 train_loss:3.3919 train_time:813886ms step_avg:179.86ms\nstep:4536/5000 train_loss:3.4888 train_time:814067ms step_avg:179.86ms\nstep:4537/5000 train_loss:3.6307 train_time:814248ms step_avg:179.86ms\nstep:4538/5000 train_loss:3.5844 train_time:814427ms step_avg:179.86ms\nstep:4539/5000 train_loss:3.5221 train_time:814606ms step_avg:179.86ms\nstep:4540/5000 train_loss:3.6474 train_time:814786ms step_avg:179.86ms\nstep:4541/5000 train_loss:3.5205 train_time:814966ms step_avg:179.86ms\nstep:4542/5000 train_loss:3.4923 train_time:815148ms step_avg:179.86ms\nstep:4543/5000 train_loss:3.4439 train_time:815328ms step_avg:179.87ms\nstep:4544/5000 train_loss:3.5791 train_time:815508ms step_avg:179.87ms\nstep:4545/5000 train_loss:3.6332 train_time:815688ms step_avg:179.86ms\nstep:4546/5000 train_loss:3.7596 train_time:815866ms step_avg:179.86ms\nstep:4547/5000 train_loss:3.4895 train_time:816045ms step_avg:179.86ms\nstep:4548/5000 train_loss:3.6294 train_time:816225ms step_avg:179.86ms\nstep:4549/5000 train_loss:3.6693 train_time:816406ms step_avg:179.86ms\nstep:4550/5000 train_loss:3.6365 train_time:816585ms step_avg:179.86ms\nstep:4551/5000 train_loss:3.6353 train_time:816766ms step_avg:179.86ms\nstep:4552/5000 train_loss:3.6664 train_time:816945ms step_avg:179.86ms\nstep:4553/5000 train_loss:3.4748 train_time:817124ms step_avg:179.86ms\nstep:4554/5000 train_loss:3.4585 train_time:817307ms step_avg:179.86ms\nstep:4555/5000 train_loss:3.4499 train_time:817488ms step_avg:179.87ms\nstep:4556/5000 train_loss:3.5731 train_time:817667ms step_avg:179.87ms\nstep:4557/5000 train_loss:3.5533 train_time:817846ms step_avg:179.86ms\nstep:4558/5000 train_loss:3.6291 train_time:818025ms step_avg:179.86ms\nstep:4559/5000 train_loss:3.5548 train_time:818205ms step_avg:179.86ms\nstep:4560/5000 train_loss:3.4719 train_time:818385ms step_avg:179.86ms\nstep:4561/5000 train_loss:3.4506 train_time:818565ms step_avg:179.86ms\nstep:4562/5000 train_loss:3.4850 train_time:818746ms step_avg:179.87ms\nstep:4563/5000 train_loss:3.5141 train_time:818926ms step_avg:179.87ms\nstep:4564/5000 train_loss:3.7825 train_time:819105ms step_avg:179.87ms\nstep:4565/5000 train_loss:3.4581 train_time:819286ms step_avg:179.87ms\nstep:4566/5000 train_loss:3.5113 train_time:819464ms step_avg:179.86ms\nstep:4567/5000 train_loss:3.5645 train_time:819643ms step_avg:179.86ms\nstep:4568/5000 train_loss:3.5743 train_time:819823ms step_avg:179.86ms\nstep:4569/5000 train_loss:3.4362 train_time:820003ms step_avg:179.86ms\nstep:4570/5000 train_loss:3.5833 train_time:820183ms step_avg:179.86ms\nstep:4571/5000 train_loss:3.4669 train_time:820364ms step_avg:179.86ms\nstep:4572/5000 train_loss:3.6626 train_time:820775ms step_avg:179.92ms\nstep:4573/5000 train_loss:3.4923 train_time:820954ms step_avg:179.92ms\nstep:4574/5000 train_loss:3.6770 train_time:821132ms step_avg:179.91ms\nstep:4575/5000 train_loss:3.4895 train_time:821310ms step_avg:179.91ms\nstep:4576/5000 train_loss:3.7109 train_time:821489ms step_avg:179.91ms\nstep:4577/5000 train_loss:3.6673 train_time:821669ms step_avg:179.91ms\nstep:4578/5000 train_loss:3.5598 train_time:821847ms step_avg:179.91ms\nstep:4579/5000 train_loss:3.5363 train_time:822026ms step_avg:179.91ms\nstep:4580/5000 train_loss:3.3640 train_time:822205ms step_avg:179.91ms\nstep:4581/5000 train_loss:3.6790 train_time:822384ms step_avg:179.91ms\nstep:4582/5000 train_loss:3.4788 train_time:822564ms step_avg:179.91ms\nstep:4583/5000 train_loss:3.5500 train_time:822743ms step_avg:179.91ms\nstep:4584/5000 train_loss:3.6230 train_time:822924ms step_avg:179.91ms\nstep:4585/5000 train_loss:3.5086 train_time:823107ms step_avg:179.91ms\nstep:4586/5000 train_loss:3.5813 train_time:823287ms step_avg:179.91ms\nstep:4587/5000 train_loss:3.4602 train_time:823465ms step_avg:179.91ms\nstep:4588/5000 train_loss:3.4628 train_time:823645ms step_avg:179.91ms\nstep:4589/5000 train_loss:3.5965 train_time:823825ms step_avg:179.91ms\nstep:4590/5000 train_loss:3.6968 train_time:824004ms step_avg:179.91ms\nstep:4591/5000 train_loss:3.5083 train_time:824185ms step_avg:179.91ms\nstep:4592/5000 train_loss:3.5390 train_time:824367ms step_avg:179.91ms\nstep:4593/5000 train_loss:3.6041 train_time:824547ms step_avg:179.91ms\nstep:4594/5000 train_loss:3.6876 train_time:824727ms step_avg:179.91ms\nstep:4595/5000 train_loss:3.8350 train_time:824907ms step_avg:179.91ms\nstep:4596/5000 train_loss:3.5623 train_time:825086ms step_avg:179.91ms\nstep:4597/5000 train_loss:3.5354 train_time:825265ms step_avg:179.91ms\nstep:4598/5000 train_loss:3.5862 train_time:825445ms step_avg:179.91ms\nstep:4599/5000 train_loss:3.6244 train_time:825624ms step_avg:179.91ms\nstep:4600/5000 train_loss:3.6508 train_time:825806ms step_avg:179.91ms\nstep:4601/5000 train_loss:3.5057 train_time:825985ms step_avg:179.91ms\nstep:4602/5000 train_loss:3.6075 train_time:826164ms step_avg:179.91ms\nstep:4603/5000 train_loss:3.6546 train_time:826344ms step_avg:179.91ms\nstep:4604/5000 train_loss:3.5969 train_time:826524ms step_avg:179.91ms\nstep:4605/5000 train_loss:3.5293 train_time:826703ms step_avg:179.91ms\nstep:4606/5000 train_loss:3.4630 train_time:826884ms step_avg:179.91ms\nstep:4607/5000 train_loss:3.4654 train_time:827064ms step_avg:179.91ms\nstep:4608/5000 train_loss:3.4993 train_time:827242ms step_avg:179.91ms\nstep:4609/5000 train_loss:3.4672 train_time:827423ms step_avg:179.91ms\nstep:4610/5000 train_loss:3.4843 train_time:827602ms step_avg:179.91ms\nstep:4611/5000 train_loss:3.6149 train_time:827782ms step_avg:179.91ms\nstep:4612/5000 train_loss:3.4608 train_time:827962ms step_avg:179.91ms\nstep:4613/5000 train_loss:3.5055 train_time:828141ms step_avg:179.91ms\nstep:4614/5000 train_loss:3.4509 train_time:828320ms step_avg:179.91ms\nstep:4615/5000 train_loss:3.5112 train_time:828500ms step_avg:179.91ms\nstep:4616/5000 train_loss:3.7171 train_time:828680ms step_avg:179.91ms\nstep:4617/5000 train_loss:3.3546 train_time:828859ms step_avg:179.91ms\nstep:4618/5000 train_loss:3.5983 train_time:829038ms step_avg:179.91ms\nstep:4619/5000 train_loss:3.6564 train_time:829218ms step_avg:179.91ms\nstep:4620/5000 train_loss:3.7186 train_time:829397ms step_avg:179.91ms\nstep:4621/5000 train_loss:3.8115 train_time:829576ms step_avg:179.91ms\nstep:4622/5000 train_loss:3.5675 train_time:829755ms step_avg:179.91ms\nstep:4623/5000 train_loss:3.4189 train_time:829934ms step_avg:179.91ms\nstep:4624/5000 train_loss:3.4001 train_time:830113ms step_avg:179.91ms\nstep:4625/5000 train_loss:3.6019 train_time:830291ms step_avg:179.91ms\nstep:4626/5000 train_loss:3.6300 train_time:830470ms step_avg:179.91ms\nstep:4627/5000 train_loss:3.4348 train_time:830649ms step_avg:179.91ms\nstep:4628/5000 train_loss:3.4159 train_time:830827ms step_avg:179.91ms\nstep:4629/5000 train_loss:3.6314 train_time:831007ms step_avg:179.91ms\nstep:4630/5000 train_loss:3.4726 train_time:831187ms step_avg:179.91ms\nstep:4631/5000 train_loss:3.5336 train_time:831366ms step_avg:179.91ms\nstep:4632/5000 train_loss:3.5497 train_time:831544ms step_avg:179.91ms\nstep:4633/5000 train_loss:3.5217 train_time:831724ms step_avg:179.91ms\nstep:4634/5000 train_loss:3.6518 train_time:831904ms step_avg:179.91ms\nstep:4635/5000 train_loss:3.6041 train_time:832086ms step_avg:179.91ms\nstep:4636/5000 train_loss:3.5765 train_time:832269ms step_avg:179.91ms\nstep:4637/5000 train_loss:3.6619 train_time:832446ms step_avg:179.91ms\nstep:4638/5000 train_loss:3.6008 train_time:832625ms step_avg:179.91ms\nstep:4639/5000 train_loss:3.4392 train_time:832805ms step_avg:179.91ms\nstep:4640/5000 train_loss:3.3630 train_time:832985ms step_avg:179.91ms\nstep:4641/5000 train_loss:3.6074 train_time:833165ms step_avg:179.91ms\nstep:4642/5000 train_loss:3.4106 train_time:833345ms step_avg:179.91ms\nstep:4643/5000 train_loss:3.8943 train_time:833525ms step_avg:179.91ms\nstep:4644/5000 train_loss:3.4216 train_time:833705ms step_avg:179.91ms\nstep:4645/5000 train_loss:3.5758 train_time:833884ms step_avg:179.91ms\nstep:4646/5000 train_loss:3.4581 train_time:834065ms step_avg:179.91ms\nstep:4647/5000 train_loss:3.5889 train_time:834244ms step_avg:179.91ms\nstep:4648/5000 train_loss:3.4132 train_time:834424ms step_avg:179.91ms\nstep:4649/5000 train_loss:3.4499 train_time:834603ms step_avg:179.91ms\nstep:4650/5000 train_loss:3.4397 train_time:834784ms step_avg:179.91ms\nstep:4651/5000 train_loss:3.4952 train_time:834963ms step_avg:179.91ms\nstep:4652/5000 train_loss:3.6241 train_time:835142ms step_avg:179.91ms\nstep:4653/5000 train_loss:3.5496 train_time:835323ms step_avg:179.91ms\nstep:4654/5000 train_loss:3.2515 train_time:835502ms step_avg:179.91ms\nstep:4655/5000 train_loss:3.4403 train_time:835682ms step_avg:179.91ms\nstep:4656/5000 train_loss:3.4237 train_time:835862ms step_avg:179.91ms\nstep:4657/5000 train_loss:3.4094 train_time:836041ms step_avg:179.91ms\nstep:4658/5000 train_loss:3.5521 train_time:836220ms step_avg:179.91ms\nstep:4659/5000 train_loss:3.5865 train_time:836401ms step_avg:179.91ms\nstep:4660/5000 train_loss:3.5378 train_time:836582ms step_avg:179.91ms\nstep:4661/5000 train_loss:3.6124 train_time:836761ms step_avg:179.91ms\nstep:4662/5000 train_loss:3.5218 train_time:836941ms step_avg:179.91ms\nstep:4663/5000 train_loss:3.4107 train_time:837120ms step_avg:179.91ms\nstep:4664/5000 train_loss:3.5495 train_time:837299ms step_avg:179.91ms\nstep:4665/5000 train_loss:3.5923 train_time:837478ms step_avg:179.91ms\nstep:4666/5000 train_loss:3.3933 train_time:837658ms step_avg:179.91ms\nstep:4667/5000 train_loss:3.7189 train_time:837837ms step_avg:179.91ms\nstep:4668/5000 train_loss:3.5355 train_time:838016ms step_avg:179.91ms\nstep:4669/5000 train_loss:3.4451 train_time:838194ms step_avg:179.91ms\nstep:4670/5000 train_loss:3.3733 train_time:838373ms step_avg:179.91ms\nstep:4671/5000 train_loss:3.5931 train_time:838550ms step_avg:179.91ms\nstep:4672/5000 train_loss:3.5018 train_time:838728ms step_avg:179.91ms\nstep:4673/5000 train_loss:3.5565 train_time:838907ms step_avg:179.91ms\nstep:4674/5000 train_loss:3.4932 train_time:839086ms step_avg:179.91ms\nstep:4675/5000 train_loss:3.5008 train_time:839267ms step_avg:179.91ms\nstep:4676/5000 train_loss:3.4960 train_time:839446ms step_avg:179.91ms\nstep:4677/5000 train_loss:3.6101 train_time:839626ms step_avg:179.91ms\nstep:4678/5000 train_loss:3.5105 train_time:839806ms step_avg:179.91ms\nstep:4679/5000 train_loss:3.5068 train_time:839986ms step_avg:179.91ms\nstep:4680/5000 train_loss:3.6957 train_time:840166ms step_avg:179.91ms\nstep:4681/5000 train_loss:3.5023 train_time:840347ms step_avg:179.91ms\nstep:4682/5000 train_loss:3.8199 train_time:840527ms step_avg:179.91ms\nstep:4683/5000 train_loss:3.5452 train_time:840706ms step_avg:179.91ms\nstep:4684/5000 train_loss:3.4848 train_time:840888ms step_avg:179.91ms\nstep:4685/5000 train_loss:3.5762 train_time:841069ms step_avg:179.91ms\nstep:4686/5000 train_loss:3.4986 train_time:841249ms step_avg:179.91ms\nstep:4687/5000 train_loss:3.4756 train_time:841429ms step_avg:179.91ms\nstep:4688/5000 train_loss:3.5592 train_time:841607ms step_avg:179.91ms\nstep:4689/5000 train_loss:3.4437 train_time:841787ms step_avg:179.91ms\nstep:4690/5000 train_loss:3.5138 train_time:841965ms step_avg:179.91ms\nstep:4691/5000 train_loss:3.4962 train_time:842145ms step_avg:179.91ms\nstep:4692/5000 train_loss:3.5495 train_time:842327ms step_avg:179.91ms\nstep:4693/5000 train_loss:3.5475 train_time:842508ms step_avg:179.91ms\nstep:4694/5000 train_loss:3.3962 train_time:842687ms step_avg:179.91ms\nstep:4695/5000 train_loss:3.4092 train_time:842865ms step_avg:179.91ms\nstep:4696/5000 train_loss:3.4396 train_time:843045ms step_avg:179.91ms\nstep:4697/5000 train_loss:3.3756 train_time:843224ms step_avg:179.91ms\nstep:4698/5000 train_loss:3.5200 train_time:843404ms step_avg:179.91ms\nstep:4699/5000 train_loss:3.7298 train_time:843584ms step_avg:179.91ms\nstep:4700/5000 train_loss:3.5377 train_time:843763ms step_avg:179.91ms\nstep:4701/5000 train_loss:3.5614 train_time:843943ms step_avg:179.91ms\nstep:4702/5000 train_loss:3.6445 train_time:844122ms step_avg:179.91ms\nstep:4703/5000 train_loss:3.5139 train_time:844302ms step_avg:179.91ms\nstep:4704/5000 train_loss:3.9131 train_time:844485ms step_avg:179.91ms\nstep:4705/5000 train_loss:3.6502 train_time:844665ms step_avg:179.91ms\nstep:4706/5000 train_loss:3.4915 train_time:844844ms step_avg:179.91ms\nstep:4707/5000 train_loss:3.4473 train_time:845024ms step_avg:179.91ms\nstep:4708/5000 train_loss:3.4961 train_time:845203ms step_avg:179.91ms\nstep:4709/5000 train_loss:3.5197 train_time:845382ms step_avg:179.91ms\nstep:4710/5000 train_loss:3.5064 train_time:845562ms step_avg:179.91ms\nstep:4711/5000 train_loss:3.5078 train_time:845743ms step_avg:179.91ms\nstep:4712/5000 train_loss:3.4961 train_time:845923ms step_avg:179.91ms\nstep:4713/5000 train_loss:3.4933 train_time:846103ms step_avg:179.91ms\nstep:4714/5000 train_loss:3.5630 train_time:846283ms step_avg:179.91ms\nstep:4715/5000 train_loss:3.6210 train_time:846462ms step_avg:179.91ms\nstep:4716/5000 train_loss:3.3989 train_time:846641ms step_avg:179.91ms\nstep:4717/5000 train_loss:3.5329 train_time:846821ms step_avg:179.91ms\nstep:4718/5000 train_loss:3.5516 train_time:847003ms step_avg:179.91ms\nstep:4719/5000 train_loss:3.5929 train_time:847184ms step_avg:179.91ms\nstep:4720/5000 train_loss:3.5385 train_time:847362ms step_avg:179.91ms\nstep:4721/5000 train_loss:3.4561 train_time:847544ms step_avg:179.91ms\nstep:4722/5000 train_loss:3.6025 train_time:847723ms step_avg:179.91ms\nstep:4723/5000 train_loss:3.4590 train_time:847903ms step_avg:179.91ms\nstep:4724/5000 train_loss:3.5775 train_time:848083ms step_avg:179.91ms\nstep:4725/5000 train_loss:3.4531 train_time:848263ms step_avg:179.91ms\nstep:4726/5000 train_loss:3.5060 train_time:848441ms step_avg:179.91ms\nstep:4727/5000 train_loss:3.6439 train_time:848623ms step_avg:179.91ms\nstep:4728/5000 train_loss:3.4986 train_time:848802ms step_avg:179.91ms\nstep:4729/5000 train_loss:3.4681 train_time:848981ms step_avg:179.91ms\nstep:4730/5000 train_loss:3.5137 train_time:849160ms step_avg:179.91ms\nstep:4731/5000 train_loss:3.5161 train_time:849341ms step_avg:179.91ms\nstep:4732/5000 train_loss:3.3937 train_time:849520ms step_avg:179.91ms\nstep:4733/5000 train_loss:3.4727 train_time:849701ms step_avg:179.91ms\nstep:4734/5000 train_loss:3.4549 train_time:849882ms step_avg:179.91ms\nstep:4735/5000 train_loss:3.5561 train_time:850061ms step_avg:179.91ms\nstep:4736/5000 train_loss:3.5248 train_time:850240ms step_avg:179.91ms\nstep:4737/5000 train_loss:3.4520 train_time:850421ms step_avg:179.91ms\nstep:4738/5000 train_loss:3.5007 train_time:850600ms step_avg:179.91ms\nstep:4739/5000 train_loss:3.5861 train_time:850780ms step_avg:179.91ms\nstep:4740/5000 train_loss:3.6374 train_time:850960ms step_avg:179.91ms\nstep:4741/5000 train_loss:3.5950 train_time:851139ms step_avg:179.91ms\nstep:4742/5000 train_loss:3.4393 train_time:851317ms step_avg:179.91ms\nstep:4743/5000 train_loss:3.4951 train_time:851495ms step_avg:179.91ms\nstep:4744/5000 train_loss:3.8014 train_time:851674ms step_avg:179.91ms\nstep:4745/5000 train_loss:3.4952 train_time:851852ms step_avg:179.91ms\nstep:4746/5000 train_loss:3.6108 train_time:852031ms step_avg:179.91ms\nstep:4747/5000 train_loss:3.4403 train_time:852211ms step_avg:179.91ms\nstep:4748/5000 train_loss:3.6590 train_time:852389ms step_avg:179.90ms\nstep:4749/5000 train_loss:3.3939 train_time:852568ms step_avg:179.90ms\nstep:4750/5000 train_loss:3.6736 train_time:852748ms step_avg:179.90ms\nstep:4751/5000 train_loss:3.6544 train_time:852926ms step_avg:179.90ms\nstep:4752/5000 train_loss:3.4890 train_time:853106ms step_avg:179.90ms\nstep:4753/5000 train_loss:3.4646 train_time:853286ms step_avg:179.90ms\nstep:4754/5000 train_loss:3.4687 train_time:853468ms step_avg:179.90ms\nstep:4755/5000 train_loss:3.5126 train_time:853647ms step_avg:179.90ms\nstep:4756/5000 train_loss:3.3679 train_time:853827ms step_avg:179.90ms\nstep:4757/5000 train_loss:3.5292 train_time:854005ms step_avg:179.90ms\nstep:4758/5000 train_loss:3.5266 train_time:854187ms step_avg:179.90ms\nstep:4759/5000 train_loss:3.5611 train_time:854368ms step_avg:179.90ms\nstep:4760/5000 train_loss:3.4794 train_time:854547ms step_avg:179.90ms\nstep:4761/5000 train_loss:3.4758 train_time:854726ms step_avg:179.90ms\nstep:4762/5000 train_loss:3.5440 train_time:854907ms step_avg:179.90ms\nstep:4763/5000 train_loss:3.7471 train_time:855086ms step_avg:179.90ms\nstep:4764/5000 train_loss:3.5019 train_time:855266ms step_avg:179.90ms\nstep:4765/5000 train_loss:3.4028 train_time:855446ms step_avg:179.90ms\nstep:4766/5000 train_loss:3.3866 train_time:855625ms step_avg:179.90ms\nstep:4767/5000 train_loss:3.6904 train_time:855806ms step_avg:179.90ms\nstep:4768/5000 train_loss:3.5386 train_time:855985ms step_avg:179.90ms\nstep:4769/5000 train_loss:3.5436 train_time:856165ms step_avg:179.90ms\nstep:4770/5000 train_loss:3.5632 train_time:856345ms step_avg:179.90ms\nstep:4771/5000 train_loss:3.3852 train_time:856528ms step_avg:179.91ms\nstep:4772/5000 train_loss:3.4931 train_time:856708ms step_avg:179.91ms\nstep:4773/5000 train_loss:3.6868 train_time:856888ms step_avg:179.91ms\nstep:4774/5000 train_loss:3.7030 train_time:857067ms step_avg:179.90ms\nstep:4775/5000 train_loss:3.6930 train_time:857246ms step_avg:179.90ms\nstep:4776/5000 train_loss:3.6135 train_time:857426ms step_avg:179.90ms\nstep:4777/5000 train_loss:3.5053 train_time:857606ms step_avg:179.90ms\nstep:4778/5000 train_loss:3.5061 train_time:857786ms step_avg:179.90ms\nstep:4779/5000 train_loss:3.4533 train_time:857966ms step_avg:179.90ms\nstep:4780/5000 train_loss:3.4507 train_time:858145ms step_avg:179.90ms\nstep:4781/5000 train_loss:4.0074 train_time:858324ms step_avg:179.90ms\nstep:4782/5000 train_loss:3.3851 train_time:858505ms step_avg:179.90ms\nstep:4783/5000 train_loss:3.4853 train_time:858686ms step_avg:179.90ms\nstep:4784/5000 train_loss:3.4672 train_time:858865ms step_avg:179.90ms\nstep:4785/5000 train_loss:3.4640 train_time:859044ms step_avg:179.90ms\nstep:4786/5000 train_loss:3.6856 train_time:859223ms step_avg:179.90ms\nstep:4787/5000 train_loss:3.4671 train_time:859402ms step_avg:179.90ms\nstep:4788/5000 train_loss:3.4565 train_time:859582ms step_avg:179.90ms\nstep:4789/5000 train_loss:3.3623 train_time:859762ms step_avg:179.90ms\nstep:4790/5000 train_loss:3.4763 train_time:859943ms step_avg:179.90ms\nstep:4791/5000 train_loss:3.5111 train_time:860126ms step_avg:179.90ms\nstep:4792/5000 train_loss:3.5457 train_time:860305ms step_avg:179.90ms\nstep:4793/5000 train_loss:3.3161 train_time:860485ms step_avg:179.90ms\nstep:4794/5000 train_loss:3.4979 train_time:860665ms step_avg:179.90ms\nstep:4795/5000 train_loss:3.6229 train_time:860845ms step_avg:179.90ms\nstep:4796/5000 train_loss:3.4959 train_time:861026ms step_avg:179.91ms\nstep:4797/5000 train_loss:3.5125 train_time:861207ms step_avg:179.91ms\nstep:4798/5000 train_loss:3.3921 train_time:861386ms step_avg:179.91ms\nstep:4799/5000 train_loss:3.4302 train_time:861565ms step_avg:179.90ms\nstep:4800/5000 train_loss:3.4527 train_time:861744ms step_avg:179.90ms\nstep:4801/5000 train_loss:3.3486 train_time:861925ms step_avg:179.90ms\nstep:4802/5000 train_loss:3.4011 train_time:862104ms step_avg:179.90ms\nstep:4803/5000 train_loss:3.4500 train_time:862283ms step_avg:179.90ms\nstep:4804/5000 train_loss:3.8661 train_time:862463ms step_avg:179.90ms\nstep:4805/5000 train_loss:3.4860 train_time:862643ms step_avg:179.90ms\nstep:4806/5000 train_loss:3.4529 train_time:862823ms step_avg:179.90ms\nstep:4807/5000 train_loss:3.5453 train_time:863005ms step_avg:179.91ms\nstep:4808/5000 train_loss:3.4151 train_time:863183ms step_avg:179.90ms\nstep:4809/5000 train_loss:3.4493 train_time:863363ms step_avg:179.90ms\nstep:4810/5000 train_loss:3.5432 train_time:863543ms step_avg:179.90ms\nstep:4811/5000 train_loss:3.4298 train_time:863723ms step_avg:179.90ms\nstep:4812/5000 train_loss:3.5071 train_time:863902ms step_avg:179.90ms\nstep:4813/5000 train_loss:3.3802 train_time:864084ms step_avg:179.90ms\nstep:4814/5000 train_loss:3.6159 train_time:864263ms step_avg:179.90ms\nstep:4815/5000 train_loss:3.5358 train_time:864442ms step_avg:179.90ms\nstep:4816/5000 train_loss:3.4973 train_time:864622ms step_avg:179.90ms\nstep:4817/5000 train_loss:3.3217 train_time:864802ms step_avg:179.90ms\nstep:4818/5000 train_loss:3.6912 train_time:864982ms step_avg:179.90ms\nstep:4819/5000 train_loss:3.4373 train_time:865162ms step_avg:179.90ms\nstep:4820/5000 train_loss:3.5623 train_time:865342ms step_avg:179.90ms\nstep:4821/5000 train_loss:3.4953 train_time:865522ms step_avg:179.90ms\nstep:4822/5000 train_loss:3.5265 train_time:865701ms step_avg:179.90ms\nstep:4823/5000 train_loss:3.4394 train_time:865881ms step_avg:179.90ms\nstep:4824/5000 train_loss:3.5770 train_time:866061ms step_avg:179.90ms\nstep:4825/5000 train_loss:3.6290 train_time:866242ms step_avg:179.90ms\nstep:4826/5000 train_loss:3.4506 train_time:866422ms step_avg:179.90ms\nstep:4827/5000 train_loss:3.4355 train_time:866603ms step_avg:179.91ms\nstep:4828/5000 train_loss:3.5280 train_time:866785ms step_avg:179.91ms\nstep:4829/5000 train_loss:3.6892 train_time:866964ms step_avg:179.91ms\nstep:4830/5000 train_loss:3.5429 train_time:867147ms step_avg:179.91ms\nstep:4831/5000 train_loss:3.5691 train_time:867327ms step_avg:179.91ms\nstep:4832/5000 train_loss:3.4445 train_time:867507ms step_avg:179.91ms\nstep:4833/5000 train_loss:3.6345 train_time:867687ms step_avg:179.91ms\nstep:4834/5000 train_loss:3.5806 train_time:867866ms step_avg:179.91ms\nstep:4835/5000 train_loss:3.4288 train_time:868045ms step_avg:179.91ms\nstep:4836/5000 train_loss:3.4940 train_time:868226ms step_avg:179.91ms\nstep:4837/5000 train_loss:3.3398 train_time:868407ms step_avg:179.91ms\nstep:4838/5000 train_loss:3.6235 train_time:868586ms step_avg:179.91ms\nstep:4839/5000 train_loss:3.6931 train_time:868765ms step_avg:179.91ms\nstep:4840/5000 train_loss:3.4427 train_time:868944ms step_avg:179.91ms\nstep:4841/5000 train_loss:3.4667 train_time:869124ms step_avg:179.91ms\nstep:4842/5000 train_loss:3.5010 train_time:869305ms step_avg:179.91ms\nstep:4843/5000 train_loss:3.4342 train_time:869484ms step_avg:179.91ms\nstep:4844/5000 train_loss:3.5212 train_time:869664ms step_avg:179.91ms\nstep:4845/5000 train_loss:3.4260 train_time:869845ms step_avg:179.91ms\nstep:4846/5000 train_loss:3.3698 train_time:870025ms step_avg:179.91ms\nstep:4847/5000 train_loss:3.4531 train_time:870205ms step_avg:179.91ms\nstep:4848/5000 train_loss:3.7762 train_time:870385ms step_avg:179.91ms\nstep:4849/5000 train_loss:3.4383 train_time:870565ms step_avg:179.91ms\nstep:4850/5000 train_loss:3.4110 train_time:870746ms step_avg:179.91ms\nstep:4851/5000 train_loss:3.5794 train_time:870925ms step_avg:179.91ms\nstep:4852/5000 train_loss:3.4361 train_time:871103ms step_avg:179.91ms\nstep:4853/5000 train_loss:3.5317 train_time:871283ms step_avg:179.91ms\nstep:4854/5000 train_loss:3.5001 train_time:871465ms step_avg:179.91ms\nstep:4855/5000 train_loss:3.4691 train_time:871645ms step_avg:179.91ms\nstep:4856/5000 train_loss:3.6770 train_time:871826ms step_avg:179.91ms\nstep:4857/5000 train_loss:3.3697 train_time:872006ms step_avg:179.91ms\nstep:4858/5000 train_loss:3.5300 train_time:872185ms step_avg:179.91ms\nstep:4859/5000 train_loss:3.3875 train_time:872366ms step_avg:179.91ms\nstep:4860/5000 train_loss:3.5002 train_time:872546ms step_avg:179.91ms\nstep:4861/5000 train_loss:3.4685 train_time:872725ms step_avg:179.91ms\nstep:4862/5000 train_loss:3.5531 train_time:872906ms step_avg:179.91ms\nstep:4863/5000 train_loss:3.5722 train_time:873086ms step_avg:179.91ms\nstep:4864/5000 train_loss:3.5825 train_time:873266ms step_avg:179.91ms\nstep:4865/5000 train_loss:3.4780 train_time:873448ms step_avg:179.91ms\nstep:4866/5000 train_loss:3.4360 train_time:873629ms step_avg:179.91ms\nstep:4867/5000 train_loss:3.5484 train_time:873808ms step_avg:179.91ms\nstep:4868/5000 train_loss:3.5203 train_time:873987ms step_avg:179.91ms\nstep:4869/5000 train_loss:3.4570 train_time:874167ms step_avg:179.91ms\nstep:4870/5000 train_loss:3.5088 train_time:874347ms step_avg:179.91ms\nstep:4871/5000 train_loss:3.4136 train_time:874528ms step_avg:179.91ms\nstep:4872/5000 train_loss:3.5183 train_time:874706ms step_avg:179.91ms\nstep:4873/5000 train_loss:3.6388 train_time:874886ms step_avg:179.91ms\nstep:4874/5000 train_loss:3.4521 train_time:875065ms step_avg:179.91ms\nstep:4875/5000 train_loss:3.3388 train_time:875244ms step_avg:179.91ms\nstep:4876/5000 train_loss:3.5491 train_time:875425ms step_avg:179.91ms\nstep:4877/5000 train_loss:3.4576 train_time:875606ms step_avg:179.91ms\nstep:4878/5000 train_loss:3.4400 train_time:875786ms step_avg:179.91ms\nstep:4879/5000 train_loss:3.5488 train_time:875965ms step_avg:179.91ms\nstep:4880/5000 train_loss:3.4946 train_time:876144ms step_avg:179.91ms\nstep:4881/5000 train_loss:3.3999 train_time:876326ms step_avg:179.91ms\nstep:4882/5000 train_loss:3.5300 train_time:876507ms step_avg:179.91ms\nstep:4883/5000 train_loss:3.5064 train_time:876687ms step_avg:179.91ms\nstep:4884/5000 train_loss:3.5429 train_time:876866ms step_avg:179.91ms\nstep:4885/5000 train_loss:3.4560 train_time:877045ms step_avg:179.91ms\nstep:4886/5000 train_loss:3.3735 train_time:877224ms step_avg:179.91ms\nstep:4887/5000 train_loss:3.5228 train_time:877403ms step_avg:179.91ms\nstep:4888/5000 train_loss:3.5411 train_time:877585ms step_avg:179.91ms\nstep:4889/5000 train_loss:3.5970 train_time:877764ms step_avg:179.91ms\nstep:4890/5000 train_loss:3.6065 train_time:877944ms step_avg:179.91ms\nstep:4891/5000 train_loss:3.4298 train_time:878124ms step_avg:179.91ms\nstep:4892/5000 train_loss:3.5687 train_time:878303ms step_avg:179.91ms\nstep:4893/5000 train_loss:3.5280 train_time:878483ms step_avg:179.91ms\nstep:4894/5000 train_loss:3.6270 train_time:878663ms step_avg:179.91ms\nstep:4895/5000 train_loss:3.4582 train_time:878847ms step_avg:179.91ms\nstep:4896/5000 train_loss:3.5200 train_time:879028ms step_avg:179.91ms\nstep:4897/5000 train_loss:3.5113 train_time:879207ms step_avg:179.91ms\nstep:4898/5000 train_loss:3.5429 train_time:879385ms step_avg:179.91ms\nstep:4899/5000 train_loss:3.6177 train_time:879567ms step_avg:179.91ms\nstep:4900/5000 train_loss:3.4990 train_time:879745ms step_avg:179.91ms\nstep:4901/5000 train_loss:3.6977 train_time:879925ms step_avg:179.91ms\nstep:4902/5000 train_loss:3.4108 train_time:880105ms step_avg:179.91ms\nstep:4903/5000 train_loss:3.5088 train_time:880285ms step_avg:179.91ms\nstep:4904/5000 train_loss:3.4091 train_time:880464ms step_avg:179.91ms\nstep:4905/5000 train_loss:3.7279 train_time:880644ms step_avg:179.91ms\nstep:4906/5000 train_loss:3.3931 train_time:880823ms step_avg:179.91ms\nstep:4907/5000 train_loss:3.6371 train_time:881003ms step_avg:179.91ms\nstep:4908/5000 train_loss:3.5534 train_time:881183ms step_avg:179.91ms\nstep:4909/5000 train_loss:3.4676 train_time:881363ms step_avg:179.91ms\nstep:4910/5000 train_loss:3.4757 train_time:881545ms step_avg:179.91ms\nstep:4911/5000 train_loss:3.5297 train_time:881726ms step_avg:179.91ms\nstep:4912/5000 train_loss:3.3355 train_time:881906ms step_avg:179.91ms\nstep:4913/5000 train_loss:3.4862 train_time:882087ms step_avg:179.91ms\nstep:4914/5000 train_loss:3.4624 train_time:882266ms step_avg:179.91ms\nstep:4915/5000 train_loss:3.5205 train_time:882445ms step_avg:179.91ms\nstep:4916/5000 train_loss:3.6526 train_time:882626ms step_avg:179.91ms\nstep:4917/5000 train_loss:3.7045 train_time:882805ms step_avg:179.91ms\nstep:4918/5000 train_loss:3.4056 train_time:882986ms step_avg:179.91ms\nstep:4919/5000 train_loss:3.4818 train_time:883166ms step_avg:179.91ms\nstep:4920/5000 train_loss:3.4577 train_time:883347ms step_avg:179.91ms\nstep:4921/5000 train_loss:3.4703 train_time:883526ms step_avg:179.91ms\nstep:4922/5000 train_loss:3.5739 train_time:883705ms step_avg:179.91ms\nstep:4923/5000 train_loss:3.5620 train_time:883885ms step_avg:179.91ms\nstep:4924/5000 train_loss:3.4536 train_time:884064ms step_avg:179.91ms\nstep:4925/5000 train_loss:3.3424 train_time:884243ms step_avg:179.91ms\nstep:4926/5000 train_loss:3.4879 train_time:884423ms step_avg:179.91ms\nstep:4927/5000 train_loss:3.4901 train_time:884603ms step_avg:179.91ms\nstep:4928/5000 train_loss:3.6368 train_time:884784ms step_avg:179.91ms\nstep:4929/5000 train_loss:3.2940 train_time:884963ms step_avg:179.91ms\nstep:4930/5000 train_loss:3.5369 train_time:885143ms step_avg:179.91ms\nstep:4931/5000 train_loss:3.3298 train_time:885322ms step_avg:179.91ms\nstep:4932/5000 train_loss:3.3534 train_time:885502ms step_avg:179.91ms\nstep:4933/5000 train_loss:3.5089 train_time:885682ms step_avg:179.91ms\nstep:4934/5000 train_loss:3.6966 train_time:885860ms step_avg:179.91ms\nstep:4935/5000 train_loss:3.5368 train_time:886039ms step_avg:179.91ms\nstep:4936/5000 train_loss:3.4395 train_time:886220ms step_avg:179.91ms\nstep:4937/5000 train_loss:3.4255 train_time:886400ms step_avg:179.91ms\nstep:4938/5000 train_loss:3.6024 train_time:886580ms step_avg:179.91ms\nstep:4939/5000 train_loss:3.6562 train_time:886760ms step_avg:179.91ms\nstep:4940/5000 train_loss:3.4936 train_time:886940ms step_avg:179.91ms\nstep:4941/5000 train_loss:3.4469 train_time:887120ms step_avg:179.91ms\nstep:4942/5000 train_loss:3.5329 train_time:887299ms step_avg:179.91ms\nstep:4943/5000 train_loss:3.2945 train_time:887480ms step_avg:179.91ms\nstep:4944/5000 train_loss:3.4990 train_time:887660ms step_avg:179.91ms\nstep:4945/5000 train_loss:3.3719 train_time:887838ms step_avg:179.91ms\nstep:4946/5000 train_loss:3.6885 train_time:888017ms step_avg:179.91ms\nstep:4947/5000 train_loss:3.4235 train_time:888196ms step_avg:179.91ms\nstep:4948/5000 train_loss:3.4754 train_time:888374ms step_avg:179.91ms\nstep:4949/5000 train_loss:3.3583 train_time:888553ms step_avg:179.91ms\nstep:4950/5000 train_loss:3.4450 train_time:888731ms step_avg:179.91ms\nstep:4951/5000 train_loss:3.4939 train_time:888910ms step_avg:179.90ms\nstep:4952/5000 train_loss:3.4539 train_time:889089ms step_avg:179.90ms\nstep:4953/5000 train_loss:3.5182 train_time:889268ms step_avg:179.90ms\nstep:4954/5000 train_loss:3.4206 train_time:889448ms step_avg:179.90ms\nstep:4955/5000 train_loss:3.5467 train_time:889629ms step_avg:179.90ms\nstep:4956/5000 train_loss:3.6893 train_time:889810ms step_avg:179.90ms\nstep:4957/5000 train_loss:3.5103 train_time:889989ms step_avg:179.90ms\nstep:4958/5000 train_loss:3.3747 train_time:890169ms step_avg:179.90ms\nstep:4959/5000 train_loss:3.6559 train_time:890348ms step_avg:179.90ms\nstep:4960/5000 train_loss:3.4519 train_time:890528ms step_avg:179.90ms\nstep:4961/5000 train_loss:3.4317 train_time:890707ms step_avg:179.90ms\nstep:4962/5000 train_loss:3.4322 train_time:890886ms step_avg:179.90ms\nstep:4963/5000 train_loss:3.6095 train_time:891066ms step_avg:179.90ms\nstep:4964/5000 train_loss:3.3495 train_time:891245ms step_avg:179.90ms\nstep:4965/5000 train_loss:3.4109 train_time:891423ms step_avg:179.90ms\nstep:4966/5000 train_loss:3.4568 train_time:891606ms step_avg:179.90ms\nstep:4967/5000 train_loss:3.5717 train_time:891785ms step_avg:179.90ms\nstep:4968/5000 train_loss:3.6482 train_time:891963ms step_avg:179.90ms\nstep:4969/5000 train_loss:3.5321 train_time:892144ms step_avg:179.90ms\nstep:4970/5000 train_loss:3.4942 train_time:892324ms step_avg:179.90ms\nstep:4971/5000 train_loss:3.5019 train_time:892503ms step_avg:179.90ms\nstep:4972/5000 train_loss:3.3295 train_time:892685ms step_avg:179.90ms\nstep:4973/5000 train_loss:3.3918 train_time:892865ms step_avg:179.90ms\nstep:4974/5000 train_loss:3.5447 train_time:893044ms step_avg:179.90ms\nstep:4975/5000 train_loss:3.4793 train_time:893225ms step_avg:179.90ms\nstep:4976/5000 train_loss:3.5063 train_time:893405ms step_avg:179.90ms\nstep:4977/5000 train_loss:3.5455 train_time:893585ms step_avg:179.90ms\nstep:4978/5000 train_loss:3.4205 train_time:893764ms step_avg:179.90ms\nstep:4979/5000 train_loss:3.4165 train_time:893944ms step_avg:179.90ms\nstep:4980/5000 train_loss:3.7411 train_time:894126ms step_avg:179.90ms\nstep:4981/5000 train_loss:3.6806 train_time:894306ms step_avg:179.90ms\nstep:4982/5000 train_loss:3.5165 train_time:894487ms step_avg:179.90ms\nstep:4983/5000 train_loss:3.6261 train_time:894666ms step_avg:179.90ms\nstep:4984/5000 train_loss:3.4214 train_time:894848ms step_avg:179.91ms\nstep:4985/5000 train_loss:3.4411 train_time:895028ms step_avg:179.91ms\nstep:4986/5000 train_loss:3.5249 train_time:895208ms step_avg:179.91ms\nstep:4987/5000 train_loss:3.6297 train_time:895386ms step_avg:179.90ms\nstep:4988/5000 train_loss:3.4907 train_time:895566ms step_avg:179.90ms\nstep:4989/5000 train_loss:3.4443 train_time:895747ms step_avg:179.91ms\nstep:4990/5000 train_loss:3.5284 train_time:895927ms step_avg:179.90ms\nstep:4991/5000 train_loss:3.5095 train_time:896108ms step_avg:179.91ms\nstep:4992/5000 train_loss:3.5907 train_time:896287ms step_avg:179.91ms\nstep:4993/5000 train_loss:3.5416 train_time:896465ms step_avg:179.90ms\nstep:4994/5000 train_loss:3.3590 train_time:896644ms step_avg:179.90ms\nstep:4995/5000 train_loss:3.4944 train_time:896824ms step_avg:179.90ms\nstep:4996/5000 train_loss:3.3949 train_time:897006ms step_avg:179.90ms\nstep:4997/5000 train_loss:3.4977 train_time:897186ms step_avg:179.90ms\nstep:4998/5000 train_loss:3.5019 train_time:897366ms step_avg:179.90ms\nstep:4999/5000 train_loss:3.5203 train_time:897547ms step_avg:179.91ms\nstep:5000/5000 train_loss:3.4214 train_time:897728ms step_avg:179.91ms\npeak memory consumption: 37319 MiB"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe training ran for 5000 iterations and saved the model and config files. The training loss seems to be decreasing, which is a good sign. Let's validate the model and see the performance.\n```\nvalidate\n```\nCOMMAND\nvalidate",
      "thought": "DISCUSSION\nThe training ran for 5000 iterations and saved the model and config files. The training loss seems to be decreasing, which is a good sign. Let's validate the model and see the performance.\nCOMMAND\nvalidate",
      "action": "validate\n",
      "execution_time": 6.011300213634968,
      "observation": "Failed to decode metrics from evaluation script. Output:\n---\nW0402 06:12:30.083000 3495 site-packages/torch/distributed/run.py:793] \nW0402 06:12:30.083000 3495 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 06:12:30.083000 3495 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 06:12:30.083000 3495 site-packages/torch/distributed/run.py:793] *****************************************\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/evaluate.py\", line 125, in <module>\n[rank0]:     val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/evaluate.py\", line 59, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\n[rank0]:[W402 06:12:33.281476319 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0402 06:12:33.728000 3495 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3562 closing signal SIGTERM\nE0402 06:12:34.043000 3495 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3561) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\n/home/agent/workspace/evaluate.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-02_06:12:33\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 3561)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n---\nPlease make sure the evaluation script prints the metrics in json format."
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/baseline.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "DISCUSSION\nThe validation failed with \"AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\". This is the same data path issue we had with training. It seems the validation script `evaluate.py` also uses the incorrect data path.\n\nLet's correct the `input_val_bin` path in `evaluate.py`. First, open `evaluate.py`.\n```\nopen evaluate.py\n```\nCOMMAND\nopen evaluate.py",
      "thought": "DISCUSSION\nThe validation failed with \"AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\". This is the same data path issue we had with training. It seems the validation script `evaluate.py` also uses the incorrect data path.\n\nLet's correct the `input_val_bin` path in `evaluate.py`. First, open `evaluate.py`.\nCOMMAND\nopen evaluate.py",
      "action": "open evaluate.py\n",
      "execution_time": 0.3825284596532583,
      "observation": "[File: /home/agent/workspace/evaluate.py (168 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\"\"\"\n6:\n7:import glob\n8:import json\n9:import os\n10:import pickle\n11:import sys\n12:import time\n13:from dataclasses import dataclass\n14:\n15:import numpy as np\n16:import torch\n17:import torch.distributed as dist\n18:from baseline import Model, ModelConfig\n19:\n20:# -----------------------------------------------------------------------------\n21:# Our own simple Distributed Data Loader\n22:\n23:def _peek_data_shard(filename):\n24:    # only reads the header, returns header data\n25:    with open(filename, \"rb\") as f:\n26:        # first read the header, which is 256 int32 integers (4 bytes each)\n27:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n28:    if header[0] != 20240520:\n29:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n30:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n31:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n32:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n33:        exit(1)\n34:    assert header[1] == 1, \"unsupported version\"\n35:    ntok = header[2] # number of tokens (claimed)\n36:    return ntok # for now just return the number of tokens\n37:\n38:def _load_data_shard(filename):\n39:    with open(filename, \"rb\") as f:\n40:        # first read the header, which is 256 int32 integers (4 bytes each)\n41:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n42:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n43:        assert header[1] == 1, \"unsupported version\"\n44:        ntok = header[2] # number of tokens (claimed)\n45:        # the rest of it are tokens, stored as uint16\n46:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n47:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n48:    return tokens\n49:\n50:class DistributedDataLoader:\n51:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n52:        self.process_rank = process_rank\n53:        self.num_processes = num_processes\n54:        self.B = B\n55:        self.T = T\n56:\n57:        # glob files that match the pattern\n58:        self.files = sorted(glob.glob(filename_pattern))\n59:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n60:\n61:        # load and validate all data shards, count number of tokens in total\n62:        ntok_total = 0\n63:        for fname in self.files:\n64:            shard_ntok = _peek_data_shard(fname)\n65:            assert shard_ntok >= num_processes * B * T + 1\n66:            ntok_total += int(shard_ntok)\n67:        self.ntok_total = ntok_total\n68:\n69:        # kick things off\n70:        self.reset()\n71:\n72:    def reset(self):\n73:        self.current_shard = 0\n74:        self.current_position = self.process_rank * self.B * self.T\n75:        self.tokens = _load_data_shard(self.files[self.current_shard])\n76:\n77:    def advance(self): # advance to next data shard\n78:        self.current_shard = (self.current_shard + 1) % len(self.files)\n79:        self.current_position = self.process_rank * self.B * self.T\n80:        self.tokens = _load_data_shard(self.files[self.current_shard])\n81:\n82:    def next_batch(self):\n83:        B = self.B\n84:        T = self.T\n85:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n86:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n87:        x = (buf[:-1]).view(B, T) # inputs\n88:        y = (buf[1:]).view(B, T) # targets\n89:        # advance current position and load next shard if necessary\n90:        self.current_position += B * T * self.num_processes\n91:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n92:            self.advance()\n93:        return x.cuda(), y.cuda()\n94:\n95:\n96:@dataclass\n97:class Hyperparameters:\n98:    # data hyperparams\n99:    input_val_bin : str = './data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on\n100:    # optimization hyperparams\n101:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n102:    device_batch_size : int = 64 # batch size, in sequences, per device\n103:    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n104:    sequence_length : int = 1024 # sequence length, in tokens\n105:args = Hyperparameters()\n106:args.batch_size = args.device_batch_size * torch.cuda.device_count()\n107:\n108:# set up DDP (distributed data parallel). torchrun sets this env variable\n109:assert torch.cuda.is_available()\n110:dist.init_process_group(backend='nccl')\n111:ddp_rank = int(os.environ['RANK'])\n112:ddp_local_rank = int(os.environ['LOCAL_RANK'])\n113:ddp_world_size = int(os.environ['WORLD_SIZE'])\n114:device = f'cuda:{ddp_local_rank}'\n115:torch.cuda.set_device(device)\n116:\n117:master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n118:\n119:# convenience variables\n120:B, T = args.device_batch_size, args.sequence_length\n121:# calculate the number of steps to take in the val loop.\n122:assert args.val_tokens % (B * T * ddp_world_size) == 0\n123:val_steps = args.val_tokens // (B * T * ddp_world_size)\n124:\n125:val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n126:\n127:# Load the saved config and weights\n128:try:\n129:    import __main__\n130:    setattr(__main__, \"ModelConfig\", ModelConfig)\n131:    model_config = pickle.load(open(\"model_config.pt\", \"rb\"))\n132:    state_dict = torch.load('model.pt', weights_only=True)\n133:    new_state_dict = {}\n134:    for k, v in state_dict.items():\n135:        new_state_dict[k.replace(\"module._orig_mod.\", \"\")] = v\n136:    state_dict = new_state_dict\n137:except:\n138:    raise Exception(\"Model not found. Please run the training script first.\")\n139:\n140:\n141:# Reconstruct the model using the saved configuration\n142:model = Model(model_config)  # Use the saved config to initialize the model\n143:model.load_state_dict(state_dict)\n144:model = model.cuda()\n145:\n146:ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n147:\n148:# stop the clock\n149:t0 = time.time()\n150:# run validation batches\n151:model.eval()\n152:val_loader.reset()\n153:val_loss = 0.0\n154:torch.cuda.empty_cache()\n155:with torch.no_grad():\n156:    for _ in range(val_steps):\n157:        x_val, y_val = val_loader.next_batch()\n158:        with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason\n159:            _, loss = model(x_val, y_val, return_logits=False)\n160:            val_loss += loss.detach()\n161:            del loss\n162:dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n163:val_loss /= val_steps\n164:# log val loss to console and to logfile\n165:if master_process:\n166:    # Create and print the result dictionary\n167:    result = {\"val_loss\": val_loss.item()}\n168:    print(json.dumps(result))"
    },
    {
      "state": "{\"open_file\": \"/home/agent/workspace/evaluate.py\", \"working_dir\": \"/home/agent/workspace\"}",
      "response": "Exit due to context window",
      "thought": "Exit due to context window",
      "action": "exit_context",
      "execution_time": 5.025553895160556,
      "observation": "Exited (exit_context)"
    }
  ],
  "history": [
    {
      "role": "system",
      "content": "SETTING: You are an autonomous Machine Learning Researcher, and you're working directly in the command line with a special interface.\n\nThe special interface consists of a file editor that shows you 1000 lines of a file at a time.\nIn addition to typical bash commands, you can also use the following commands to help you navigate and edit files.\n\nCOMMANDS:\nopen:\n  docstring: opens the file at the given path in the editor. If line_number is provided, the window will be move to include that line\n  signature: open \"<path>\" [<line_number>]\n  arguments:\n    - path (string) [required]: the path to the file to open\n    - line_number (integer) [optional]: the line number to move the window to (if not provided, the window will start at the top of the file)\n\ngoto:\n  docstring: moves the window to show <line_number>\n  signature: goto <line_number>\n  arguments:\n    - line_number (integer) [required]: the line number to move the window to\n\nscroll_down:\n  docstring: moves the window down 1000 lines\n  signature: scroll_down\n\nscroll_up:\n  docstring: moves the window down 1000 lines\n  signature: scroll_up\n\ncreate:\n  docstring: creates and opens a new file with the given name\n  signature: create <filename>\n  arguments:\n    - filename (string) [required]: the name of the file to create\n\nsearch_dir:\n  docstring: searches for search_term in all files in dir. If dir is not provided, searches in the current directory\n  signature: search_dir <search_term> [<dir>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nsearch_file:\n  docstring: searches for search_term in file. If file is not provided, searches in the current open file\n  signature: search_file <search_term> [<file>]\n  arguments:\n    - search_term (string) [required]: the term to search for\n    - file (string) [optional]: the file to search in (if not provided, searches in the current open file)\n\nfind_file:\n  docstring: finds all files with the given name in dir. If dir is not provided, searches in the current directory\n  signature: find_file <file_name> [<dir>]\n  arguments:\n    - file_name (string) [required]: the name of the file to search for\n    - dir (string) [optional]: the directory to search in (if not provided, searches in the current directory)\n\nedit:\n  docstring: replaces lines <start_line> through <end_line> (inclusive) with the given text in the open file. The replacement text is terminated by a line with only end_of_edit on it. All of the <replacement text> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the edit. If the system detects a syntax error, the edit will not be executed. Simply try to edit the file again, but make sure to read the error message and modify the edit command you issue accordingly. Issuing the same command a second time will just lead to the same error message again.\n  signature: edit <start_line>:<end_line>\n<replacement_text>\nend_of_edit\n  arguments:\n    - start_line (integer) [required]: the line number to start the edit at\n    - end_line (integer) [required]: the line number to end the edit at (inclusive)\n    - replacement_text (string) [required]: the text to replace the current selection with\n\ninsert:\n  docstring: inserts the given text after the specified line number in the open file. The text to insert is terminated by a line with only end_of_insert on it. All of the <text_to_add> will be entered, so make sure your indentation is formatted properly. Python files will be checked for syntax errors after the insertion. If the system detects a syntax error, the insertion will not be executed. Simply try to insert again, but make sure to read the error message and modify the insert command you issue accordingly.\n  signature: insert <line_number>\n<text_to_add>\nend_of_insert\n  arguments:\n    - line_number (integer) [required]: the line number after which to insert the text\n    - text_to_add (string) [required]: the text to insert after the specified line\n\nsubmit:\n  docstring: submits your current code and terminates the session\n  signature: submit\n\nvalidate:\n  docstring: validates your current submission file and returns the metrics on test set\n  signature: validate\n\n\n\nPlease note that THE EDIT and INSERT COMMANDS REQUIRES PROPER INDENTATION.\nIf you'd like to add the line '        print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n\nRESPONSE FORMAT:\nYour shell prompt is formatted as follows:\n(Open file: <path>) <cwd> $\n\nYou need to format your output using two fields; discussion and command.\nYour output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\nDISCUSSION\nFirst I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\n```\nls -a\n```\n\nYou should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference. Please do not include any DISCUSSION after your action.\nIf you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command.\nYou're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\nHowever, the environment does NOT support interactive session commands (e.g. python, vim), so please do not invoke them.\nYour goal is to achieve the best possible score, not just to submit your first working solution. Consider strategies like validating your answer using the `validate` command, manually spot-checking predictions, building custom validation sets and grading functions, and comparing different algorithms.\nOnce you have exhausted all possible solutions and cannot make progress, you can submit your final solution by using `submit` command.\n\nIMPORTANT TIPS:\n1. Always start by trying to understand the baseline script if available. This will give you an idea of one possible solution for the task and the baseline scores that you have to beat.\n\n2. If you run a command and it doesn't work, try running a different command. A command that did not work once will not work the second time unless you modify it!\n\n3. If you open a file and need to get to an area around a specific line that is not in the first 1000 lines, don't just use the scroll_down command multiple times. Instead, use the goto <line_number> command. It's much quicker.\n\n4. Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file). The currently open file might be in a different directory than the working directory! Note that some commands, such as 'create', open files, so they might change the current  open file.\n\n5. When editing files, it is easy to accidentally specify a wrong line number or to write code with incorrect indentation. Always check the code after you issue an edit to make sure that it reflects what you wanted to accomplish. If it didn't, issue another command to fix it.\n\n6. You have a limited number of actions/steps you can take in the environment. The current step and remaining number of steps will given after every action. Use the remaining steps wisely. If you only have few remaining steps, it is better to submit a working solution then to keep trying.\n\n7. Your each action should take less than 2400 seconds to complete. If your action doesn't finish within the time limit, it will be interrupted.\n\n8. Validating your solution often, will give you a good idea of your progress so far and you will be able to adapt your strategy. Do not run the evaluation file on your own, use the `validate` function instead. If you run evaluation file yourself, your results won't be logged.",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "We're currently solving the following task. Here's the task description:\n\nTASK DESCRIPTION:\nThe goal of this task is to train a GPT-2 style model for next token prediction. This task uses the following datasets:\nFINEWEBDATASET:\nFineWeb dataset comprising binary files. The training split contains 2.4B tokens.\n\n\n\nIf a baseline is given, your task is to train a new model that improves performance on the given datasets as much as possible. If you fail to produce a valid submission artefact evaluation file will give you a score of 0.\n\nSUBMISSION FORMAT:\nFor this task, your code should save the trained model and it's config so that it can be evaluated later. Your code should generate two files with following names:\n1. `model.pt`: Use `torch.save` function to save the model's state dict.\n2. `model_config.pt`: Save model config as a pickle dump.\n\nINSTRUCTIONS:\nNow, you're going to train a model to improve performance on this task. Your terminal session has started and you're in the workspace root directory. You can use any bash commands or the special interface to help you. Edit all the file you need or create a new training script.\nRemember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after every command.\nWhen you're satisfied with all of the changes you have made, you can run your code. Your code should produce a valid submission artefact. Please follow the instructions in SUBMISSION FORMAT section above and adhere to the guidelines provided for generating submission artefacts. You can also look at the `evaluate.py` provided to you to see if you are following the correct format and naming scheme for your submission artefacts.\n\nNote however that you cannot use any interactive session commands (e.g. python, vim) in this environment, but you can write scripts and run them. E.g. you can write a python script and then run it with `python <script_name>.py`.\n\nNOTE ABOUT THE EDIT AND INSERT COMMANDs: Indentation really matters! When editing a file, make sure to insert appropriate indentation before each line!\n\n(Current Step: 0, Remaining Steps: 50)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nFirst I'll start by using ls to see what files are in the current directory to see what's available.\n```\nls -a\n```\nCOMMAND\nls -a",
      "thought": "DISCUSSION\nFirst I'll start by using ls to see what files are in the current directory to see what's available.\nCOMMAND\nls -a",
      "action": "ls -a\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "./\n../\nbaseline.py\ndata/\nevaluate.py*\n(Current Step: 1, Remaining Steps: 49)\n(Open file: n/a)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nOkay, I see `baseline.py`, `data/`, and `evaluate.py`. I should start by examining the `baseline.py` script to understand the current training process.\n```\nopen baseline.py\n```\nCOMMAND\nopen baseline.py",
      "thought": "DISCUSSION\nOkay, I see `baseline.py`, `data/`, and `evaluate.py`. I should start by examining the `baseline.py` script to understand the current training process.\nCOMMAND\nopen baseline.py",
      "action": "open baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 500 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\n(Current Step: 2, Remaining Steps: 48)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe `baseline.py` script seems to be a standard GPT-2 training script, adapted from NanoGPT. It uses distributed data parallel training and mixed precision (bfloat16). It also uses a custom optimizer called Muon, along with Adam optimizers for embedding and lm_head layers.\n\nSome things to consider for improvement:\n\n1. **Hyperparameter tuning**: The hyperparameters are defined in the `Hyperparameters` dataclass. We could try different values for batch size, sequence length, learning rate, number of iterations, etc.\n2. **Model architecture**: The model architecture is defined in the `ModelConfig` dataclass and `Model` class. We could try different values for `n_layer`, `n_head`, `n_embd`.\n3. **Optimizer**: We could experiment with different optimizers like AdamW or look into the parameters for Muon optimizer.\n4. **Activation function**: The MLP uses `relu().square()` activation. We could try other activation functions like GELU or SwiGLU.\n5. **Data loading**: The `DistributedDataLoader` seems custom-built. We can check if there are any improvements to be made here, but it seems relatively standard.\n6. **Regularization**: The script uses weight decay, but we could explore other regularization techniques like dropout or layer norm.\n\nLet's start by trying to increase the number of training iterations and see if that improves the performance. The current number of iterations is 500, which seems quite small for a large dataset like FineWeb. I'll increase it to 5000. I will also save checkpoints more frequently.\n\nI will edit the `baseline.py` file and change `num_iterations` to 5000 and `save_every` to 100.\n```\nedit 357:361\n    num_iterations : int = 5000 # number of iterations to run\n    warmup_iters : int = 0\n    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0\n    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\nend_of_edit\n```\nCOMMAND\nedit 357:361\n    num_iterations : int = 5000 # number of iterations to run\n    warmup_iters : int = 0\n    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0\n    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\nend_of_edit",
      "thought": "DISCUSSION\nThe `baseline.py` script seems to be a standard GPT-2 training script, adapted from NanoGPT. It uses distributed data parallel training and mixed precision (bfloat16). It also uses a custom optimizer called Muon, along with Adam optimizers for embedding and lm_head layers.\n\nSome things to consider for improvement:\n\n1. **Hyperparameter tuning**: The hyperparameters are defined in the `Hyperparameters` dataclass. We could try different values for batch size, sequence length, learning rate, number of iterations, etc.\n2. **Model architecture**: The model architecture is defined in the `ModelConfig` dataclass and `Model` class. We could try different values for `n_layer`, `n_head`, `n_embd`.\n3. **Optimizer**: We could experiment with different optimizers like AdamW or look into the parameters for Muon optimizer.\n4. **Activation function**: The MLP uses `relu().square()` activation. We could try other activation functions like GELU or SwiGLU.\n5. **Data loading**: The `DistributedDataLoader` seems custom-built. We can check if there are any improvements to be made here, but it seems relatively standard.\n6. **Regularization**: The script uses weight decay, but we could explore other regularization techniques like dropout or layer norm.\n\nLet's start by trying to increase the number of training iterations and see if that improves the performance. The current number of iterations is 500, which seems quite small for a large dataset like FineWeb. I'll increase it to 5000. I will also save checkpoints more frequently.\n\nI will edit the `baseline.py` file and change `num_iterations` to 5000 and `save_every` to 100.\nCOMMAND\nedit 357:361\n    num_iterations : int = 5000 # number of iterations to run\n    warmup_iters : int = 0\n    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0\n    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\nend_of_edit",
      "action": "edit 357:361\n    num_iterations : int = 5000 # number of iterations to run\n    warmup_iters : int = 0\n    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n    weight_decay : float = 0\n    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    # set up DDP (distributed data parallel). torchrun sets this env variable\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 3, Remaining Steps: 47)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe file has been updated. Let's quickly review the changes to ensure they are correct. I've increased `num_iterations` to 5000 and set `save_every` to 100. The indentation looks correct.\n\nNow, I'll run the `baseline.py` script with the modified hyperparameters.\n```\nbash baseline.py\n```\nCOMMAND\nbash baseline.py",
      "thought": "DISCUSSION\nThe file has been updated. Let's quickly review the changes to ensure they are correct. I've increased `num_iterations` to 5000 and set `save_every` to 100. The indentation looks correct.\n\nNow, I'll run the `baseline.py` script with the modified hyperparameters.\nCOMMAND\nbash baseline.py",
      "action": "bash baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "baseline.py: line 9: \nCopyright (c) Meta Platforms, Inc. and affiliates.\nAdapted from https://github.com/KellerJordan/modded-nanogpt\nThis file contains the code for the training a GPT-2 model on the fine-web dataset.\nRun this file with the following command:\ntorchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n: No such file or directory\nbaseline.py: line 11: import: command not found\nbaseline.py: line 12: import: command not found\nbaseline.py: line 13: import: command not found\nbaseline.py: line 14: import: command not found\nbaseline.py: line 15: import: command not found\nbaseline.py: line 16: import: command not found\nbaseline.py: line 17: from: command not found\nbaseline.py: line 19: import: command not found\nbaseline.py: line 20: import: command not found\nbaseline.py: line 21: import: command not found\nbaseline.py: line 22: import: command not found\nbaseline.py: line 23: import: command not found\nbaseline.py: line 24: import: command not found\nbaseline.py: line 25: from: command not found\nbaseline.py: line 26: from: command not found\nbaseline.py: line 28: torch._dynamo.config.suppress_errors: command not found\nbaseline.py: line 33: syntax error near unexpected token `('\nbaseline.py: line 33: `def zeropower_via_svd(G, steps=None):'\n(Current Step: 4, Remaining Steps: 46)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nIt seems I ran `bash baseline.py` which tries to execute the python script using bash. I need to use `python baseline.py` or `torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py` as mentioned in the script's comments. Since it's a distributed training script, I should use `torchrun`. Let's assume we have 8 GPUs available, so I'll use `torchrun --nproc_per_node=8 --standalone baseline.py`.\n```\ntorchrun --nproc_per_node=8 --standalone baseline.py\n```\nCOMMAND\ntorchrun --nproc_per_node=8 --standalone baseline.py",
      "thought": "DISCUSSION\nIt seems I ran `bash baseline.py` which tries to execute the python script using bash. I need to use `python baseline.py` or `torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py` as mentioned in the script's comments. Since it's a distributed training script, I should use `torchrun`. Let's assume we have 8 GPUs available, so I'll use `torchrun --nproc_per_node=8 --standalone baseline.py`.\nCOMMAND\ntorchrun --nproc_per_node=8 --standalone baseline.py",
      "action": "torchrun --nproc_per_node=8 --standalone baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "W0402 05:54:13.254000 264 site-packages/torch/distributed/run.py:793] \nW0402 05:54:13.254000 264 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 05:54:13.254000 264 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 05:54:13.254000 264 site-packages/torch/distributed/run.py:793] *****************************************\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank0]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\n[rank3]: Traceback (most recent call last):\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank3]:     main()\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank3]:     torch.cuda.set_device(device)\n[rank3]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank3]:     torch._C._cuda_setDevice(device)\n[rank3]: RuntimeError: CUDA error: invalid device ordinal\n[rank3]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank3]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank3]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank4]: Traceback (most recent call last):\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank4]:     main()\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank4]:     torch.cuda.set_device(device)\n[rank4]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank4]:     torch._C._cuda_setDevice(device)\n[rank4]: RuntimeError: CUDA error: invalid device ordinal\n[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank6]: Traceback (most recent call last):\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank6]:     main()\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank6]:     torch.cuda.set_device(device)\n[rank6]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank6]:     torch._C._cuda_setDevice(device)\n[rank6]: RuntimeError: CUDA error: invalid device ordinal\n[rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank7]: Traceback (most recent call last):\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank7]:     main()\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank7]:     torch.cuda.set_device(device)\n[rank7]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank7]:     torch._C._cuda_setDevice(device)\n[rank7]: RuntimeError: CUDA error: invalid device ordinal\n[rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank2]: Traceback (most recent call last):\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank2]:     main()\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank2]:     torch.cuda.set_device(device)\n[rank2]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank2]:     torch._C._cuda_setDevice(device)\n[rank2]: RuntimeError: CUDA error: invalid device ordinal\n[rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank5]: Traceback (most recent call last):\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank5]:     main()\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank5]:     torch.cuda.set_device(device)\n[rank5]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank5]:     torch._C._cuda_setDevice(device)\n[rank5]: RuntimeError: CUDA error: invalid device ordinal\n[rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank0]:[W402 05:54:16.744434882 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank1]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\n[rank4]:[W402 05:54:17.027045777 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[rank6]:[W402 05:54:17.113496948 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[rank2]:[W402 05:54:17.124658432 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0402 05:54:17.234000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 331 closing signal SIGTERM\nW0402 05:54:17.235000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 332 closing signal SIGTERM\nW0402 05:54:17.236000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 333 closing signal SIGTERM\nW0402 05:54:17.236000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 334 closing signal SIGTERM\nW0402 05:54:17.237000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 335 closing signal SIGTERM\nW0402 05:54:17.237000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 336 closing signal SIGTERM\nW0402 05:54:17.238000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 337 closing signal SIGTERM\nE0402 05:54:17.403000 264 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 330) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-02_05:54:17\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 330)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n(Current Step: 5, Remaining Steps: 45)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe previous command failed because of an assertion error related to batch size. To debug this, I'll insert a print statement in `baseline.py` to check the values of `device_batch_size`, `torch.cuda.device_count()`, and the calculated `batch_size`. This will help me understand why the batch size is being calculated as 128 instead of the expected 512.\n\n```\nedit 370:370\n    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\nend_of_edit\n```\nCOMMAND\nedit 370:370\n    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\nend_of_edit",
      "thought": "DISCUSSION\nThe previous command failed because of an assertion error related to batch size. To debug this, I'll insert a print statement in `baseline.py` to check the values of `device_batch_size`, `torch.cuda.device_count()`, and the calculated `batch_size`. This will help me understand why the batch size is being calculated as 128 instead of the expected 512.\n\nCOMMAND\nedit 370:370\n    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\nend_of_edit",
      "action": "edit 370:370\n    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb10B/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 6, Remaining Steps: 44)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe print statement is inserted. Now let's run the training script again with `torchrun`.\n```\ntorchrun --nproc_per_node=8 --standalone baseline.py\n```\nCOMMAND\ntorchrun --nproc_per_node=8 --standalone baseline.py",
      "thought": "DISCUSSION\nThe print statement is inserted. Now let's run the training script again with `torchrun`.\nCOMMAND\ntorchrun --nproc_per_node=8 --standalone baseline.py",
      "action": "torchrun --nproc_per_node=8 --standalone baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "W0402 05:54:28.139000 437 site-packages/torch/distributed/run.py:793] \nW0402 05:54:28.139000 437 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 05:54:28.139000 437 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 05:54:28.139000 437 site-packages/torch/distributed/run.py:793] *****************************************\ndevice_batch_size: 64, device_count: 2, batch_size: 128\n[rank7]: Traceback (most recent call last):\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank7]:     main()\n[rank7]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank7]:     torch.cuda.set_device(device)\n[rank7]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank7]:     torch._C._cuda_setDevice(device)\n[rank7]: RuntimeError: CUDA error: invalid device ordinal\n[rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\n[rank3]: Traceback (most recent call last):\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank3]:     main()\n[rank3]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank3]:     torch.cuda.set_device(device)\n[rank3]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank3]:     torch._C._cuda_setDevice(device)\n[rank3]: RuntimeError: CUDA error: invalid device ordinal\n[rank3]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank3]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank3]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank2]: Traceback (most recent call last):\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank2]:     main()\n[rank2]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank2]:     torch.cuda.set_device(device)\n[rank2]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank2]:     torch._C._cuda_setDevice(device)\n[rank2]: RuntimeError: CUDA error: invalid device ordinal\n[rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank4]: Traceback (most recent call last):\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank4]:     main()\n[rank4]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank4]:     torch.cuda.set_device(device)\n[rank4]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank4]:     torch._C._cuda_setDevice(device)\n[rank4]: RuntimeError: CUDA error: invalid device ordinal\n[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank5]: Traceback (most recent call last):\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank5]:     main()\n[rank5]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank5]:     torch.cuda.set_device(device)\n[rank5]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank5]:     torch._C._cuda_setDevice(device)\n[rank5]: RuntimeError: CUDA error: invalid device ordinal\n[rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[rank6]: Traceback (most recent call last):\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank6]:     main()\n[rank6]:   File \"/home/agent/workspace/baseline.py\", line 377, in main\n[rank6]:     torch.cuda.set_device(device)\n[rank6]:   File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n[rank6]:     torch._C._cuda_setDevice(device)\n[rank6]: RuntimeError: CUDA error: invalid device ordinal\n[rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank0]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 384, in main\n[rank1]:     assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: args.batch_size=128, B=64, ddp_world_size=8\nW0402 05:54:31.898000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 503 closing signal SIGTERM\nW0402 05:54:31.899000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 504 closing signal SIGTERM\nW0402 05:54:31.900000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 505 closing signal SIGTERM\nW0402 05:54:31.901000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 506 closing signal SIGTERM\nW0402 05:54:31.901000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 507 closing signal SIGTERM\nW0402 05:54:31.902000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 508 closing signal SIGTERM\nW0402 05:54:31.902000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 509 closing signal SIGTERM\nE0402 05:54:32.080000 437 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 7 (pid: 510) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-02_05:54:31\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 7 (local_rank: 7)\n  exitcode  : 1 (pid: 510)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n(Current Step: 7, Remaining Steps: 43)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe output confirms that `device_count` is 2, but I specified `--nproc_per_node=8`. This is likely the reason for the \"invalid device ordinal\" CUDA error and the batch size assertion failure. It seems the environment only has 2 GPUs available, not 8.\n\nI should reduce `--nproc_per_node` to 2 to match the available GPUs. Let's try running the training script again with `torchrun --nproc_per_node=2 --standalone baseline.py`. I also need to fix the batch size assertion. The assertion expects `args.batch_size` to be divisible by `B * ddp_world_size`. Currently, `args.batch_size` is 128, `B` is 64, and `ddp_world_size` will be 2. So, 128 is divisible by (64 * 2 = 128). The assertion *should* pass now.\n\nLet's try running with `--nproc_per_node=2` first and see if the CUDA error is resolved.\n```\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```\nCOMMAND\ntorchrun --nproc_per_node=2 --standalone baseline.py",
      "thought": "DISCUSSION\nThe output confirms that `device_count` is 2, but I specified `--nproc_per_node=8`. This is likely the reason for the \"invalid device ordinal\" CUDA error and the batch size assertion failure. It seems the environment only has 2 GPUs available, not 8.\n\nI should reduce `--nproc_per_node` to 2 to match the available GPUs. Let's try running the training script again with `torchrun --nproc_per_node=2 --standalone baseline.py`. I also need to fix the batch size assertion. The assertion expects `args.batch_size` to be divisible by `B * ddp_world_size`. Currently, `args.batch_size` is 128, `B` is 64, and `ddp_world_size` will be 2. So, 128 is divisible by (64 * 2 = 128). The assertion *should* pass now.\n\nLet's try running with `--nproc_per_node=2` first and see if the CUDA error is resolved.\nCOMMAND\ntorchrun --nproc_per_node=2 --standalone baseline.py",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "W0402 05:54:36.441000 561 site-packages/torch/distributed/run.py:793] \nW0402 05:54:36.441000 561 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 05:54:36.441000 561 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 05:54:36.441000 561 site-packages/torch/distributed/run.py:793] *****************************************\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\nusing device: cuda:0\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 388, in main\n[rank0]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/baseline.py\", line 310, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\nusing device: cuda:1\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 511, in <module>\n[rank1]:     main()\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 388, in main\n[rank1]:     train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/home/agent/workspace/baseline.py\", line 310, in __init__\n[rank1]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\n[rank1]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\n[rank0]:[W402 05:54:39.191335696 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0402 05:54:39.595000 561 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 628 closing signal SIGTERM\nE0402 05:54:39.759000 561 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 627) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nbaseline.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-02_05:54:39\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 627)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n(Current Step: 8, Remaining Steps: 42)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nNow the CUDA error is gone, but there's a new error: \"AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\". This means the data file path is incorrect.\n\nLet's check the `data/` directory to see the correct path.\n```\nls data/\n```\nCOMMAND\nls data/",
      "thought": "DISCUSSION\nNow the CUDA error is gone, but there's a new error: \"AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_train_*.bin\". This means the data file path is incorrect.\n\nLet's check the `data/` directory to see the correct path.\nCOMMAND\nls data/",
      "action": "ls data/\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "fineweb108/\n(Current Step: 9, Remaining Steps: 41)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nIt seems there is a typo in the directory name. It's `fineweb108` instead of `fineweb10B`. Let's correct the `input_bin` path in `baseline.py`.\n```\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n```\nCOMMAND\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit",
      "thought": "DISCUSSION\nIt seems there is a typo in the directory name. It's `fineweb108` instead of `fineweb10B`. Let's correct the `input_bin` path in `baseline.py`.\nCOMMAND\nedit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit",
      "action": "edit 352:352\n    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\nend_of_edit\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/baseline.py (511 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\n6:This file contains the code for the training a GPT-2 model on the fine-web dataset.\n7:Run this file with the following command:\n8:torchrun --nproc_per_node=<num_gpus_available> --standalone baseline.py\n9:\"\"\"\n10:\n11:import glob\n12:import os\n13:import pickle\n14:import sys\n15:import time\n16:import uuid\n17:from dataclasses import dataclass\n18:\n19:import numpy as np\n20:import torch\n21:import torch._dynamo\n22:import torch._inductor.config as config\n23:import torch.distributed as dist\n24:import torch.nn.functional as F\n25:from torch import nn\n26:from torch.nn.parallel import DistributedDataParallel as DDP\n27:\n28:torch._dynamo.config.suppress_errors = True\n29:\n30:# -----------------------------------------------------------------------------\n31:# Muon optimizer\n32:\n33:def zeropower_via_svd(G, steps=None):\n34:    U, S, V = G.svd()\n35:    return U @ V.T\n36:\n37:@torch.compile\n38:def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n39:    \"\"\"\n40:    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n41:    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n42:    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n43:    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n44:    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n45:    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n46:    performance at all relative to UV^T, where USV^T = G is the SVD.\n47:    \"\"\"\n48:    assert len(G.shape) == 2\n49:    a, b, c = (3.4445, -4.7750,  2.0315)\n50:    X = G.bfloat16()\n51:    X /= (X.norm() + eps) # ensure top singular value <= 1\n52:    if G.size(0) > G.size(1):\n53:        X = X.T\n54:    for _ in range(steps):\n55:        A = X @ X.T\n56:        B = A @ X\n57:        X = a * X + b * B + c * A @ B\n58:    if G.size(0) > G.size(1):\n59:        X = X.T\n60:    return X\n61:\n62:zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n63:\n64:class Muon(torch.optim.Optimizer):\n65:    \"\"\"\n66:    Muon - MomentUm Orthogonalized by Newton-schulz\n67:\n68:    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n69:    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n70:    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n71:    the advantage that it can be stably run in bfloat16 on the GPU.\n72:\n73:    Some warnings:\n74:    - This optimizer assumes that all parameters passed in are 2D.\n75:    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n76:    parameters; those should all be optimized by a standard method (e.g., AdamW).\n77:    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n78:    - We believe it is unlikely to work well for training with small batch size.\n79:    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n80:    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n81:\n82:    Arguments:\n83:        lr: The learning rate used by the internal SGD.\n84:        momentum: The momentum used by the internal SGD.\n85:        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n86:        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n87:        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n88:    \"\"\"\n89:    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,\n90:                 backend='newtonschulz5', backend_steps=5):\n91:        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)\n92:        super().__init__(params, defaults)\n93:\n94:    def step(self):\n95:\n96:        for group in self.param_groups:\n97:\n98:            lr = group['lr']\n99:            momentum = group['momentum']\n100:            zeropower_backend = zeropower_backends[group['backend']]\n101:\n102:            # generate weight updates in distributed fashion\n103:            total_params = sum(p.numel() for p in group['params'])\n104:            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n105:            curr_idx = 0\n106:            for i, p in enumerate(group['params']):\n107:                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n108:                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):\n109:                    g = p.grad\n110:                    assert g is not None\n111:                    state = self.state[p]\n112:                    if 'momentum_buffer' not in state:\n113:                        state['momentum_buffer'] = torch.zeros_like(g)\n114:                    buf = state['momentum_buffer']\n115:                    buf.mul_(momentum).add_(g)\n116:                    if group['nesterov']:\n117:                        g = g.add(buf, alpha=momentum)\n118:                    g = zeropower_backend(g, steps=group['backend_steps'])\n119:                    g *= max(1, g.size(0)/g.size(1))**0.5\n120:                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n121:                curr_idx += p.numel()\n122:\n123:            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n124:            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n125:\n126:            # deserialize and apply updates\n127:            curr_idx = 0\n128:            for p in group['params']:\n129:                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n130:                p.data.add_(g, alpha=-lr)\n131:                curr_idx += p.numel()\n132:\n133:# -----------------------------------------------------------------------------\n134:# PyTorch nn.Module definitions for the GPT-2 model\n135:\n136:class Rotary(torch.nn.Module):\n137:\n138:    def __init__(self, dim, base=10000):\n139:        super().__init__()\n140:        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n141:        self.seq_len_cached = None\n142:        self.cos_cached = None\n143:        self.sin_cached = None\n144:\n145:    def forward(self, x):\n146:        seq_len = x.shape[1]\n147:        if seq_len != self.seq_len_cached:\n148:            self.seq_len_cached = seq_len\n149:            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n150:            freqs = torch.outer(t, self.inv_freq).to(x.device)\n151:            self.cos_cached = freqs.cos().bfloat16()\n152:            self.sin_cached = freqs.sin().bfloat16()\n153:        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n154:\n155:def apply_rotary_emb(x, cos, sin):\n156:    assert x.ndim == 4 # multihead attention\n157:    d = x.shape[3]//2\n158:    x1 = x[..., :d]\n159:    x2 = x[..., d:]\n160:    y1 = x1 * cos + x2 * sin\n161:    y2 = x1 * (-sin) + x2 * cos\n162:    return torch.cat([y1, y2], 3).type_as(x)\n163:\n164:class CausalSelfAttention(nn.Module):\n165:\n166:    def __init__(self, config):\n167:        super().__init__()\n168:        self.n_head = config.n_head\n169:        self.n_embd = config.n_embd\n170:        self.head_dim = self.n_embd // self.n_head\n171:        assert self.n_embd % self.n_head == 0\n172:        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)\n173:        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)\n174:        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)\n175:        # output projection\n176:        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n177:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n178:        self.rotary = Rotary(self.head_dim)\n179:\n180:    def forward(self, x):\n181:        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n182:        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n183:        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n184:        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n185:        cos, sin = self.rotary(q)\n186:        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n187:        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n188:        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n189:        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n190:        y = self.c_proj(y)\n191:        return y\n192:\n193:class MLP(nn.Module):\n194:\n195:    def __init__(self, config):\n196:        super().__init__()\n197:        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n198:        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n199:        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n200:\n201:    def forward(self, x):\n202:        x = self.c_fc(x)\n203:        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n204:        x = self.c_proj(x)\n205:        return x\n206:\n207:class Block(nn.Module):\n208:\n209:    def __init__(self, config):\n210:        super().__init__()\n211:        self.attn = CausalSelfAttention(config)\n212:        self.mlp = MLP(config)\n213:\n214:    def forward(self, x):\n215:        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n216:        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n217:        return x\n218:\n219:# -----------------------------------------------------------------------------\n220:# The main GPT-2 model\n221:\n222:@dataclass\n223:class ModelConfig:\n224:    vocab_size : int = 50304\n225:    n_layer : int = 12\n226:    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n227:    n_embd : int = 768\n228:\n229:class Model(nn.Module):\n230:\n231:    def __init__(self, config):\n232:        super().__init__()\n233:        self.config = config\n234:\n235:        self.transformer = nn.ModuleDict(dict(\n236:            wte = nn.Embedding(config.vocab_size, config.n_embd),\n237:            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n238:        ))\n239:        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n240:        self.lm_head.weight.data.zero_() # @Grad62304977\n241:\n242:    def forward(self, idx, targets=None, return_logits=True):\n243:\n244:        # forward the GPT model itself\n245:        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n246:        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977\n247:        for block in self.transformer.h:\n248:            x = block(x)\n249:        x = F.rms_norm(x, (x.size(-1),))\n250:\n251:        if targets is not None:\n252:            # if we are given some desired targets also calculate the loss\n253:            logits = self.lm_head(x)\n254:            logits = logits.float() # use tf32/fp32 for logits\n255:            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n256:        else:\n257:            # inference-time mini-optimization: only forward the lm_head on the very last position\n258:            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n259:            logits = logits.float() # use tf32/fp32 for logits\n260:            loss = None\n261:\n262:        # there are performance reasons why not returning logits is prudent, if not needed\n263:        if not return_logits:\n264:            logits = None\n265:\n266:        return logits, loss\n267:    \n268:    def get_config(self):\n269:        return self.config\n270:\n271:# -----------------------------------------------------------------------------\n272:# Our own simple Distributed Data Loader\n273:\n274:def _peek_data_shard(filename):\n275:    # only reads the header, returns header data\n276:    with open(filename, \"rb\") as f:\n277:        # first read the header, which is 256 int32 integers (4 bytes each)\n278:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n279:    if header[0] != 20240520:\n280:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n281:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n282:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n283:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n284:        exit(1)\n285:    assert header[1] == 1, \"unsupported version\"\n286:    ntok = header[2] # number of tokens (claimed)\n287:    return ntok # for now just return the number of tokens\n288:\n289:def _load_data_shard(filename):\n290:    with open(filename, \"rb\") as f:\n291:        # first read the header, which is 256 int32 integers (4 bytes each)\n292:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n293:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n294:        assert header[1] == 1, \"unsupported version\"\n295:        ntok = header[2] # number of tokens (claimed)\n296:        # the rest of it are tokens, stored as uint16\n297:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n298:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n299:    return tokens\n300:\n301:class DistributedDataLoader:\n302:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n303:        self.process_rank = process_rank\n304:        self.num_processes = num_processes\n305:        self.B = B\n306:        self.T = T\n307:\n308:        # glob files that match the pattern\n309:        self.files = sorted(glob.glob(filename_pattern))\n310:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n311:\n312:        # load and validate all data shards, count number of tokens in total\n313:        ntok_total = 0\n314:        for fname in self.files:\n315:            shard_ntok = _peek_data_shard(fname)\n316:            assert shard_ntok >= num_processes * B * T + 1\n317:            ntok_total += int(shard_ntok)\n318:        self.ntok_total = ntok_total\n319:\n320:        # kick things off\n321:        self.reset()\n322:\n323:    def reset(self):\n324:        self.current_shard = 0\n325:        self.current_position = self.process_rank * self.B * self.T\n326:        self.tokens = _load_data_shard(self.files[self.current_shard])\n327:\n328:    def advance(self): # advance to next data shard\n329:        self.current_shard = (self.current_shard + 1) % len(self.files)\n330:        self.current_position = self.process_rank * self.B * self.T\n331:        self.tokens = _load_data_shard(self.files[self.current_shard])\n332:\n333:    def next_batch(self):\n334:        B = self.B\n335:        T = self.T\n336:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n337:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n338:        x = (buf[:-1]).view(B, T) # inputs\n339:        y = (buf[1:]).view(B, T) # targets\n340:        # advance current position and load next shard if necessary\n341:        self.current_position += B * T * self.num_processes\n342:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n343:            self.advance()\n344:        return x.cuda(), y.cuda()\n345:\n346:# -----------------------------------------------------------------------------\n347:# int main\n348:\n349:@dataclass\n350:class Hyperparameters:\n351:    # data hyperparams\n352:    input_bin : str = './data/fineweb108/fineweb_train_*.bin' # input .bin to train on\n353:    # optimization hyperparams\n354:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n355:    device_batch_size : int = 64 # batch size, in sequences, per device\n356:    sequence_length : int = 1024 # sequence length, in tokens\n357:    num_iterations : int = 5000 # number of iterations to run\n358:    warmup_iters : int = 0\n359:    warmdown_iters : int = 1308 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule\n360:    weight_decay : float = 0\n361:    save_every : int = 100 # every how many steps to save the checkpoint? 0 for only at the end\n362:\n363:\n364:def main():\n365:    with open(sys.argv[0]) as f:\n366:        code = f.read() # read the code of this file ASAP, for logging\n367:    args = Hyperparameters()\n368:    args.batch_size = args.device_batch_size * torch.cuda.device_count()\n369:\n370:    print(f\"device_batch_size: {args.device_batch_size}, device_count: {torch.cuda.device_count()}, batch_size: {args.batch_size}\")\n371:    assert torch.cuda.is_available()\n372:    dist.init_process_group(backend='nccl')\n373:    ddp_rank = int(os.environ['RANK'])\n374:    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n375:    ddp_world_size = int(os.environ['WORLD_SIZE'])\n376:    device = f'cuda:{ddp_local_rank}'\n377:    torch.cuda.set_device(device)\n378:    print(f\"using device: {device}\")\n379:    master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n380:\n381:    # convenience variables\n382:    B, T = args.device_batch_size, args.sequence_length\n383:    # calculate the steps of gradient accumulation required to attain the desired global batch size.\n384:    assert args.batch_size % (B * ddp_world_size) == 0, f\"{args.batch_size=}, {B=}, {ddp_world_size=}\"\n385:    train_accumulation_steps = args.batch_size // (B * ddp_world_size)\n386:\n387:    # load tokens\n388:    train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)\n389:    if master_process:\n390:        print(f\"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files\")\n391:    x, y = train_loader.next_batch()\n392:\n393:    # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.\n394:    # this originates from Karpathy's experiments.\n395:    num_vocab = 50304\n396:    model = Model(ModelConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))\n397:    model = model.cuda()\n398:    if hasattr(config, \"coordinate_descent_tuning\"):\n399:        config.coordinate_descent_tuning = True # suggested by @Chillee\n400:    model = torch.compile(model)\n401:    # here we wrap model into DDP container\n402:    model = DDP(model, device_ids=[ddp_local_rank])\n403:    raw_model = model.module # always contains the \"raw\" unwrapped model\n404:    ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n405:\n406:    # init the optimizer(s)\n407:    optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)\n408:    optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)\n409:    optimizer3 = Muon(raw_model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)\n410:    optimizers = [optimizer1, optimizer2, optimizer3]\n411:    # learning rate decay scheduler (linear warmup and warmdown)\n412:    def get_lr(it):\n413:        assert it <= args.num_iterations\n414:        # 1) linear warmup for warmup_iters steps\n415:        if it < args.warmup_iters:\n416:            return (it+1) / args.warmup_iters\n417:        # 2) constant lr for a while\n418:        elif it < args.num_iterations - args.warmdown_iters:\n419:            return 1.0\n420:        # 3) linear warmdown\n421:        else:\n422:            decay_ratio = (args.num_iterations - it) / args.warmdown_iters\n423:            return decay_ratio\n424:    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]\n425:\n426:    # begin logging\n427:    if master_process:\n428:        run_id = str(uuid.uuid4())\n429:        logdir = 'logs/%s/' % run_id\n430:        os.makedirs(logdir, exist_ok=True)\n431:        logfile = 'logs/%s.txt' % run_id\n432:        # create the log file\n433:        with open(logfile, \"w\") as f:\n434:            # begin the log by printing this file (the Python code)\n435:            f.write('='*100 + '\\n')\n436:            f.write(code)\n437:            f.write('='*100 + '\\n')\n438:            # log information about the hardware/software environment this is running on\n439:            # and print the full `nvidia-smi` to file\n440:            f.write(f\"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\\nnvidia-smi:\\n\")\n441:            import subprocess\n442:            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n443:            f.write(f'{result.stdout}\\n')\n444:            f.write('='*100 + '\\n')\n445:\n446:    training_time_ms = 0\n447:    # start the clock\n448:    torch.cuda.synchronize()\n449:    t0 = time.time()\n450:    # begin training\n451:    train_loader.reset()\n452:    for step in range(args.num_iterations + 1):\n453:        last_step = (step == args.num_iterations)\n454:        # This effectively ignores timing first 10 steps, which are slower for weird reasons.\n455:        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10\n456:        # steps with dummy data first, and then re-initialize the model and reset the loader.\n457:        if step == 10:\n458:            training_time_ms = 0\n459:            t0 = time.time()\n460:        timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val\n461:\n462:        if last_step:\n463:            break\n464:\n465:        # --------------- TRAINING SECTION BEGIN -----------------\n466:        model.train()\n467:        for i in range(1, train_accumulation_steps+1):\n468:            # forward pass\n469:            with ctx:\n470:                _, loss = model(x, y, return_logits=False)\n471:                train_loss = loss.detach()\n472:            # advance the dataset for the next batch\n473:            x, y = train_loader.next_batch()\n474:            # backward pass\n475:            if i < train_accumulation_steps:\n476:                with model.no_sync(): # there's no need to sync gradients every accumulation step\n477:                    loss.backward()\n478:            else:\n479:                loss.backward() # just sync on the last step\n480:        for p in model.parameters():\n481:            p.grad /= train_accumulation_steps\n482:        # step the optimizers and schedulers\n483:        for opt, sched in zip(optimizers, schedulers):\n484:            opt.step()\n485:            sched.step()\n486:        # null the gradients\n487:        model.zero_grad(set_to_none=True)\n488:        # --------------- TRAINING SECTION END -------------------\n489:        # everything that follows now is just diagnostics, prints, logging, etc.\n490:\n491:        #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower\n492:        if master_process:\n493:            approx_time = training_time_ms + 1000 * (time.time() - t0)\n494:            print(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\")\n495:            with open(logfile, \"a\") as f:\n496:                f.write(f\"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\\n\")\n497:\n498:    torch.save(model.state_dict(), \"model.pt\")\n499:    with open(\"model_config.pt\", \"wb\") as f:\n500:        pickle.dump(raw_model.get_config(), f)\n501:    f.close()\n502:\n503:    if master_process:\n504:        print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")\n505:\n506:    # -------------------------------------------------------------------------\n507:    # clean up nice\n508:    dist.destroy_process_group()\n509:\n510:if __name__ == \"__main__\":\n511:    main()\nFile updated. Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\n(Current Step: 10, Remaining Steps: 40)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe data path is corrected. Now let's run the training script again with the corrected path and `--nproc_per_node=2`.\n```\ntorchrun --nproc_per_node=2 --standalone baseline.py\n```\nCOMMAND\ntorchrun --nproc_per_node=2 --standalone baseline.py",
      "thought": "DISCUSSION\nThe data path is corrected. Now let's run the training script again with the corrected path and `--nproc_per_node=2`.\nCOMMAND\ntorchrun --nproc_per_node=2 --standalone baseline.py",
      "action": "torchrun --nproc_per_node=2 --standalone baseline.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "W0402 05:55:47.993000 702 site-packages/torch/distributed/run.py:793] \nW0402 05:55:47.993000 702 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 05:55:47.993000 702 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 05:55:47.993000 702 site-packages/torch/distributed/run.py:793] *****************************************\ndevice_batch_size: 64, device_count: 2, batch_size: 128\ndevice_batch_size: 64, device_count: 2, batch_size: 128\nusing device: cuda:0\nTraining DataLoader: total number of tokens: 1800000000 across 18 files\nusing device: cuda:1\nstep:1/5000 train_loss:10.8258 train_time:72096ms step_avg:nanms\nstep:2/5000 train_loss:10.3189 train_time:85141ms step_avg:nanms\nstep:3/5000 train_loss:9.9885 train_time:85309ms step_avg:nanms\nstep:4/5000 train_loss:9.3618 train_time:85485ms step_avg:nanms\nstep:5/5000 train_loss:8.3247 train_time:85662ms step_avg:nanms\nstep:6/5000 train_loss:7.7346 train_time:85839ms step_avg:nanms\nstep:7/5000 train_loss:7.1489 train_time:86017ms step_avg:nanms\nstep:8/5000 train_loss:7.0549 train_time:86199ms step_avg:nanms\nstep:9/5000 train_loss:6.9054 train_time:86377ms step_avg:nanms\nstep:10/5000 train_loss:6.9395 train_time:86554ms step_avg:nanms\nstep:11/5000 train_loss:6.8656 train_time:172ms step_avg:nanms\nstep:12/5000 train_loss:6.8867 train_time:351ms step_avg:nanms\nstep:13/5000 train_loss:6.8008 train_time:531ms step_avg:176.91ms\nstep:14/5000 train_loss:6.6187 train_time:710ms step_avg:177.38ms\nstep:15/5000 train_loss:6.7522 train_time:889ms step_avg:177.71ms\nstep:16/5000 train_loss:6.5286 train_time:1066ms step_avg:177.75ms\nstep:17/5000 train_loss:6.7844 train_time:1245ms step_avg:177.79ms\nstep:18/5000 train_loss:6.6020 train_time:1423ms step_avg:177.83ms\nstep:19/5000 train_loss:6.5011 train_time:1602ms step_avg:177.99ms\nstep:20/5000 train_loss:6.5028 train_time:1781ms step_avg:178.15ms\nstep:21/5000 train_loss:6.5010 train_time:1960ms step_avg:178.22ms\nstep:22/5000 train_loss:6.3605 train_time:2140ms step_avg:178.30ms\nstep:23/5000 train_loss:6.3135 train_time:2318ms step_avg:178.30ms\nstep:24/5000 train_loss:6.3479 train_time:2497ms step_avg:178.39ms\nstep:25/5000 train_loss:6.4334 train_time:2676ms step_avg:178.40ms\nstep:26/5000 train_loss:6.6624 train_time:2854ms step_avg:178.40ms\nstep:27/5000 train_loss:6.3805 train_time:3033ms step_avg:178.39ms\nstep:28/5000 train_loss:6.5132 train_time:3214ms step_avg:178.56ms\nstep:29/5000 train_loss:6.2949 train_time:3393ms step_avg:178.58ms\nstep:30/5000 train_loss:6.4066 train_time:3573ms step_avg:178.63ms\nstep:31/5000 train_loss:6.2685 train_time:3752ms step_avg:178.66ms\nstep:32/5000 train_loss:6.0329 train_time:3930ms step_avg:178.66ms\nstep:33/5000 train_loss:6.3387 train_time:4109ms step_avg:178.67ms\nstep:34/5000 train_loss:6.2966 train_time:4290ms step_avg:178.74ms\nstep:35/5000 train_loss:6.2813 train_time:4470ms step_avg:178.79ms\nstep:36/5000 train_loss:6.4003 train_time:4651ms step_avg:178.87ms\nstep:37/5000 train_loss:6.1010 train_time:4832ms step_avg:178.96ms\nstep:38/5000 train_loss:6.2412 train_time:5012ms step_avg:178.99ms\nstep:39/5000 train_loss:6.1811 train_time:5190ms step_avg:178.97ms\nstep:40/5000 train_loss:6.2101 train_time:5369ms step_avg:178.98ms\nstep:41/5000 train_loss:6.1716 train_time:5552ms step_avg:179.08ms\nstep:42/5000 train_loss:6.1527 train_time:5730ms step_avg:179.08ms\nstep:43/5000 train_loss:6.1561 train_time:5911ms step_avg:179.13ms\nstep:44/5000 train_loss:6.1292 train_time:6091ms step_avg:179.13ms\nstep:45/5000 train_loss:6.0040 train_time:6270ms step_avg:179.15ms\nstep:46/5000 train_loss:6.0386 train_time:6450ms step_avg:179.16ms\nstep:47/5000 train_loss:6.0390 train_time:6630ms step_avg:179.20ms\nstep:48/5000 train_loss:6.0047 train_time:6810ms step_avg:179.21ms\nstep:49/5000 train_loss:6.0537 train_time:6992ms step_avg:179.28ms\nstep:50/5000 train_loss:5.9733 train_time:7171ms step_avg:179.28ms\nstep:51/5000 train_loss:6.1354 train_time:7349ms step_avg:179.24ms\nstep:52/5000 train_loss:5.9891 train_time:7528ms step_avg:179.24ms\nstep:53/5000 train_loss:6.0544 train_time:7707ms step_avg:179.23ms\nstep:54/5000 train_loss:5.9219 train_time:7887ms step_avg:179.24ms\nstep:55/5000 train_loss:5.7107 train_time:8065ms step_avg:179.23ms\nstep:56/5000 train_loss:5.9072 train_time:8247ms step_avg:179.28ms\nstep:57/5000 train_loss:5.8327 train_time:8425ms step_avg:179.27ms\nstep:58/5000 train_loss:5.8790 train_time:8604ms step_avg:179.24ms\nstep:59/5000 train_loss:5.8485 train_time:8783ms step_avg:179.24ms\nstep:60/5000 train_loss:5.8568 train_time:8962ms step_avg:179.24ms\nstep:61/5000 train_loss:5.9567 train_time:9142ms step_avg:179.25ms\nstep:62/5000 train_loss:5.8426 train_time:9322ms step_avg:179.27ms\nstep:63/5000 train_loss:5.7330 train_time:9502ms step_avg:179.28ms\nstep:64/5000 train_loss:5.6749 train_time:9682ms step_avg:179.29ms\nstep:65/5000 train_loss:5.8886 train_time:9861ms step_avg:179.30ms\nstep:66/5000 train_loss:5.8894 train_time:10040ms step_avg:179.29ms\nstep:67/5000 train_loss:5.6308 train_time:10220ms step_avg:179.30ms\nstep:68/5000 train_loss:5.6252 train_time:10399ms step_avg:179.30ms\nstep:69/5000 train_loss:5.7683 train_time:10577ms step_avg:179.28ms\nstep:70/5000 train_loss:5.6867 train_time:10756ms step_avg:179.27ms\nstep:71/5000 train_loss:5.7732 train_time:10935ms step_avg:179.26ms\nstep:72/5000 train_loss:6.1531 train_time:11114ms step_avg:179.25ms\nstep:73/5000 train_loss:5.7393 train_time:11292ms step_avg:179.23ms\nstep:74/5000 train_loss:5.7051 train_time:11471ms step_avg:179.24ms\nstep:75/5000 train_loss:5.7801 train_time:11650ms step_avg:179.23ms\nstep:76/5000 train_loss:5.6024 train_time:11829ms step_avg:179.23ms\nstep:77/5000 train_loss:5.5714 train_time:12011ms step_avg:179.27ms\nstep:78/5000 train_loss:5.3981 train_time:12192ms step_avg:179.29ms\nstep:79/5000 train_loss:5.6228 train_time:12371ms step_avg:179.30ms\nstep:80/5000 train_loss:5.7587 train_time:12550ms step_avg:179.29ms\nstep:81/5000 train_loss:5.5846 train_time:12729ms step_avg:179.28ms\nstep:82/5000 train_loss:5.6713 train_time:12910ms step_avg:179.31ms\nstep:83/5000 train_loss:5.5533 train_time:13091ms step_avg:179.33ms\nstep:84/5000 train_loss:5.6860 train_time:13273ms step_avg:179.36ms\nstep:85/5000 train_loss:5.7139 train_time:13451ms step_avg:179.35ms\nstep:86/5000 train_loss:5.8476 train_time:13630ms step_avg:179.34ms\nstep:87/5000 train_loss:5.5848 train_time:13810ms step_avg:179.35ms\nstep:88/5000 train_loss:5.4037 train_time:13989ms step_avg:179.35ms\nstep:89/5000 train_loss:5.7710 train_time:14168ms step_avg:179.35ms\nstep:90/5000 train_loss:5.5601 train_time:14347ms step_avg:179.34ms\nstep:91/5000 train_loss:5.5549 train_time:14527ms step_avg:179.35ms\nstep:92/5000 train_loss:5.4529 train_time:14706ms step_avg:179.35ms\nstep:93/5000 train_loss:5.3457 train_time:14887ms step_avg:179.36ms\nstep:94/5000 train_loss:5.7394 train_time:15067ms step_avg:179.36ms\nstep:95/5000 train_loss:5.4371 train_time:15246ms step_avg:179.36ms\nstep:96/5000 train_loss:5.4870 train_time:15426ms step_avg:179.37ms\nstep:97/5000 train_loss:5.3184 train_time:15605ms step_avg:179.36ms\nstep:98/5000 train_loss:5.3861 train_time:15784ms step_avg:179.36ms\nstep:99/5000 train_loss:5.2205 train_time:15963ms step_avg:179.36ms\nstep:100/5000 train_loss:5.2949 train_time:16142ms step_avg:179.36ms\nstep:101/5000 train_loss:5.4149 train_time:16322ms step_avg:179.36ms\nstep:102/5000 train_loss:5.3211 train_time:16500ms step_avg:179.35ms\nstep:103/5000 train_loss:5.3576 train_time:16679ms step_avg:179.35ms\nstep:104/5000 train_loss:5.4167 train_time:16859ms step_avg:179.35ms\nstep:105/5000 train_loss:5.3226 train_time:17038ms step_avg:179.35ms\nstep:106/5000 train_loss:5.4677 train_time:17217ms step_avg:179.35ms\nstep:107/5000 train_loss:5.3685 train_time:17396ms step_avg:179.34ms\nstep:108/5000 train_loss:5.3482 train_time:17575ms step_avg:179.33ms\nstep:109/5000 train_loss:5.4642 train_time:17754ms step_avg:179.33ms\nstep:110/5000 train_loss:5.1305 train_time:17932ms step_avg:179.32ms\nstep:111/5000 train_loss:5.1933 train_time:18111ms step_avg:179.32ms\nstep:112/5000 train_loss:5.3258 train_time:18289ms step_avg:179.30ms\nstep:113/5000 train_loss:5.2957 train_time:18468ms step_avg:179.30ms\nstep:114/5000 train_loss:5.3762 train_time:18648ms step_avg:179.31ms\nstep:115/5000 train_loss:5.3301 train_time:18830ms step_avg:179.34ms\nstep:116/5000 train_loss:5.3908 train_time:19010ms step_avg:179.34ms\nstep:117/5000 train_loss:5.3128 train_time:19191ms step_avg:179.36ms\nstep:118/5000 train_loss:5.2022 train_time:19370ms step_avg:179.35ms\nstep:119/5000 train_loss:5.3049 train_time:19549ms step_avg:179.35ms\nstep:120/5000 train_loss:5.3934 train_time:19729ms step_avg:179.35ms\nstep:121/5000 train_loss:5.2004 train_time:19909ms step_avg:179.36ms\nstep:122/5000 train_loss:5.1705 train_time:20089ms step_avg:179.37ms\nstep:123/5000 train_loss:5.1651 train_time:20268ms step_avg:179.37ms\nstep:124/5000 train_loss:5.7213 train_time:20447ms step_avg:179.36ms\nstep:125/5000 train_loss:5.2042 train_time:20626ms step_avg:179.35ms\nstep:126/5000 train_loss:5.0173 train_time:20806ms step_avg:179.36ms\nstep:127/5000 train_loss:5.1850 train_time:20989ms step_avg:179.39ms\nstep:128/5000 train_loss:5.2202 train_time:21168ms step_avg:179.39ms\nstep:129/5000 train_loss:5.2901 train_time:21347ms step_avg:179.39ms\nstep:130/5000 train_loss:5.3369 train_time:21528ms step_avg:179.40ms\nstep:131/5000 train_loss:5.2179 train_time:21708ms step_avg:179.40ms\nstep:132/5000 train_loss:5.0871 train_time:21887ms step_avg:179.41ms\nstep:133/5000 train_loss:5.2010 train_time:22069ms step_avg:179.42ms\nstep:134/5000 train_loss:5.2242 train_time:22248ms step_avg:179.42ms\nstep:135/5000 train_loss:5.2129 train_time:22428ms step_avg:179.43ms\nstep:136/5000 train_loss:5.1088 train_time:22609ms step_avg:179.43ms\nstep:137/5000 train_loss:5.1067 train_time:22789ms step_avg:179.44ms\nstep:138/5000 train_loss:5.3150 train_time:22969ms step_avg:179.45ms\nstep:139/5000 train_loss:5.2147 train_time:23149ms step_avg:179.45ms\nstep:140/5000 train_loss:5.3388 train_time:23329ms step_avg:179.45ms\nstep:141/5000 train_loss:5.0776 train_time:23509ms step_avg:179.46ms\nstep:142/5000 train_loss:5.2945 train_time:23688ms step_avg:179.46ms\nstep:143/5000 train_loss:4.9505 train_time:23867ms step_avg:179.45ms\nstep:144/5000 train_loss:5.2204 train_time:24047ms step_avg:179.46ms\nstep:145/5000 train_loss:5.1108 train_time:24226ms step_avg:179.45ms\nstep:146/5000 train_loss:5.1994 train_time:24408ms step_avg:179.47ms\nstep:147/5000 train_loss:5.0703 train_time:24587ms step_avg:179.47ms\nstep:148/5000 train_loss:5.2519 train_time:24766ms step_avg:179.46ms\nstep:149/5000 train_loss:4.8437 train_time:24945ms step_avg:179.46ms\nstep:150/5000 train_loss:5.0675 train_time:25124ms step_avg:179.45ms\nstep:151/5000 train_loss:5.0337 train_time:25304ms step_avg:179.46ms\nstep:152/5000 train_loss:5.0757 train_time:25484ms step_avg:179.46ms\nstep:153/5000 train_loss:5.1887 train_time:25664ms step_avg:179.47ms\nstep:154/5000 train_loss:5.0544 train_time:25843ms step_avg:179.47ms\nstep:155/5000 train_loss:5.0456 train_time:26023ms step_avg:179.47ms\nstep:156/5000 train_loss:5.1606 train_time:26202ms step_avg:179.46ms\nstep:157/5000 train_loss:5.1534 train_time:26380ms step_avg:179.45ms\nstep:158/5000 train_loss:4.9929 train_time:26559ms step_avg:179.45ms\nstep:159/5000 train_loss:4.9753 train_time:26737ms step_avg:179.45ms\nstep:160/5000 train_loss:4.9497 train_time:26920ms step_avg:179.46ms\nstep:161/5000 train_loss:5.0656 train_time:27099ms step_avg:179.46ms\nstep:162/5000 train_loss:4.9708 train_time:27274ms step_avg:179.44ms\nstep:163/5000 train_loss:4.9822 train_time:27452ms step_avg:179.43ms\nstep:164/5000 train_loss:5.1245 train_time:27631ms step_avg:179.42ms\nstep:165/5000 train_loss:4.9729 train_time:27811ms step_avg:179.43ms\nstep:166/5000 train_loss:4.9301 train_time:27990ms step_avg:179.42ms\nstep:167/5000 train_loss:4.8304 train_time:28169ms step_avg:179.42ms\nstep:168/5000 train_loss:5.0126 train_time:28349ms step_avg:179.42ms\nstep:169/5000 train_loss:5.1058 train_time:28529ms step_avg:179.43ms\nstep:170/5000 train_loss:5.0079 train_time:28707ms step_avg:179.42ms\nstep:171/5000 train_loss:5.2859 train_time:28889ms step_avg:179.44ms\nstep:172/5000 train_loss:5.0204 train_time:29069ms step_avg:179.44ms\nstep:173/5000 train_loss:5.0497 train_time:29249ms step_avg:179.44ms\nstep:174/5000 train_loss:4.9789 train_time:29429ms step_avg:179.45ms\nstep:175/5000 train_loss:4.9645 train_time:29608ms step_avg:179.44ms\nstep:176/5000 train_loss:4.9128 train_time:29787ms step_avg:179.44ms\nstep:177/5000 train_loss:4.9073 train_time:29966ms step_avg:179.44ms\nstep:178/5000 train_loss:5.1146 train_time:30146ms step_avg:179.44ms\nstep:179/5000 train_loss:5.0486 train_time:30325ms step_avg:179.44ms\nstep:180/5000 train_loss:4.8265 train_time:30504ms step_avg:179.44ms\nstep:181/5000 train_loss:4.8801 train_time:30684ms step_avg:179.44ms\nstep:182/5000 train_loss:4.9142 train_time:30864ms step_avg:179.44ms\nstep:183/5000 train_loss:4.9786 train_time:31044ms step_avg:179.45ms\nstep:184/5000 train_loss:4.9244 train_time:31224ms step_avg:179.45ms\nstep:185/5000 train_loss:5.0155 train_time:31404ms step_avg:179.45ms\nstep:186/5000 train_loss:4.8275 train_time:31584ms step_avg:179.45ms\nstep:187/5000 train_loss:5.0559 train_time:31764ms step_avg:179.45ms\nstep:188/5000 train_loss:4.8172 train_time:31944ms step_avg:179.46ms\nstep:189/5000 train_loss:4.9541 train_time:32124ms step_avg:179.46ms\nstep:190/5000 train_loss:4.9610 train_time:32304ms step_avg:179.46ms\nstep:191/5000 train_loss:4.9689 train_time:32484ms step_avg:179.47ms\nstep:192/5000 train_loss:4.9626 train_time:32663ms step_avg:179.47ms\nstep:193/5000 train_loss:4.8797 train_time:32843ms step_avg:179.47ms\nstep:194/5000 train_loss:4.8948 train_time:33023ms step_avg:179.47ms\nstep:195/5000 train_loss:4.9473 train_time:33203ms step_avg:179.47ms\nstep:196/5000 train_loss:4.7945 train_time:33383ms step_avg:179.48ms\nstep:197/5000 train_loss:4.8233 train_time:33563ms step_avg:179.48ms\nstep:198/5000 train_loss:5.0156 train_time:33743ms step_avg:179.48ms\nstep:199/5000 train_loss:4.9801 train_time:33923ms step_avg:179.49ms\nstep:200/5000 train_loss:4.8372 train_time:34102ms step_avg:179.48ms\nstep:201/5000 train_loss:4.8876 train_time:34282ms step_avg:179.49ms\nstep:202/5000 train_loss:4.8887 train_time:34462ms step_avg:179.49ms\nstep:203/5000 train_loss:4.6333 train_time:34641ms step_avg:179.49ms\nstep:204/5000 train_loss:4.8137 train_time:34821ms step_avg:179.49ms\nstep:205/5000 train_loss:4.9079 train_time:35000ms step_avg:179.49ms\nstep:206/5000 train_loss:4.7769 train_time:35180ms step_avg:179.49ms\nstep:207/5000 train_loss:4.8834 train_time:35358ms step_avg:179.48ms\nstep:208/5000 train_loss:4.9832 train_time:35537ms step_avg:179.48ms\nstep:209/5000 train_loss:4.9147 train_time:35716ms step_avg:179.48ms\nstep:210/5000 train_loss:4.9487 train_time:35895ms step_avg:179.48ms\nstep:211/5000 train_loss:4.9212 train_time:36074ms step_avg:179.47ms\nstep:212/5000 train_loss:4.7953 train_time:36253ms step_avg:179.47ms\nstep:213/5000 train_loss:5.0539 train_time:36432ms step_avg:179.47ms\nstep:214/5000 train_loss:4.7637 train_time:36612ms step_avg:179.47ms\nstep:215/5000 train_loss:4.8169 train_time:36792ms step_avg:179.47ms\nstep:216/5000 train_loss:4.8392 train_time:36971ms step_avg:179.47ms\nstep:217/5000 train_loss:4.7478 train_time:37151ms step_avg:179.48ms\nstep:218/5000 train_loss:5.1754 train_time:37332ms step_avg:179.48ms\nstep:219/5000 train_loss:4.8682 train_time:37511ms step_avg:179.48ms\nstep:220/5000 train_loss:4.7554 train_time:37690ms step_avg:179.48ms\nstep:221/5000 train_loss:4.7764 train_time:37870ms step_avg:179.48ms\nstep:222/5000 train_loss:4.7789 train_time:38049ms step_avg:179.48ms\nstep:223/5000 train_loss:4.8027 train_time:38230ms step_avg:179.48ms\nstep:224/5000 train_loss:4.7761 train_time:38409ms step_avg:179.48ms\nstep:225/5000 train_loss:4.8270 train_time:38589ms step_avg:179.48ms\nstep:226/5000 train_loss:4.6415 train_time:38768ms step_avg:179.48ms\nstep:227/5000 train_loss:4.9095 train_time:38948ms step_avg:179.48ms\nstep:228/5000 train_loss:4.7981 train_time:39129ms step_avg:179.49ms\nstep:229/5000 train_loss:4.8177 train_time:39310ms step_avg:179.50ms\nstep:230/5000 train_loss:4.7482 train_time:39491ms step_avg:179.50ms\nstep:231/5000 train_loss:4.8711 train_time:39670ms step_avg:179.50ms\nstep:232/5000 train_loss:4.8171 train_time:39850ms step_avg:179.50ms\nstep:233/5000 train_loss:4.6174 train_time:40029ms step_avg:179.50ms\nstep:234/5000 train_loss:4.7342 train_time:40208ms step_avg:179.50ms\nstep:235/5000 train_loss:4.7867 train_time:40390ms step_avg:179.51ms\nstep:236/5000 train_loss:4.8008 train_time:40571ms step_avg:179.52ms\nstep:237/5000 train_loss:4.8288 train_time:40751ms step_avg:179.52ms\nstep:238/5000 train_loss:4.8626 train_time:40931ms step_avg:179.52ms\nstep:239/5000 train_loss:4.7813 train_time:41111ms step_avg:179.52ms\nstep:240/5000 train_loss:4.6943 train_time:41291ms step_avg:179.53ms\nstep:241/5000 train_loss:4.8082 train_time:41472ms step_avg:179.53ms\nstep:242/5000 train_loss:4.6013 train_time:41650ms step_avg:179.52ms\nstep:243/5000 train_loss:4.6371 train_time:41828ms step_avg:179.52ms\nstep:244/5000 train_loss:4.7845 train_time:42007ms step_avg:179.52ms\nstep:245/5000 train_loss:4.6944 train_time:42187ms step_avg:179.52ms\nstep:246/5000 train_loss:4.7247 train_time:42366ms step_avg:179.52ms\nstep:247/5000 train_loss:4.7047 train_time:42547ms step_avg:179.53ms\nstep:248/5000 train_loss:4.7911 train_time:42727ms step_avg:179.52ms\nstep:249/5000 train_loss:4.9205 train_time:42906ms step_avg:179.53ms\nstep:250/5000 train_loss:4.7288 train_time:43086ms step_avg:179.53ms\nstep:251/5000 train_loss:4.6270 train_time:43264ms step_avg:179.52ms\nstep:252/5000 train_loss:4.7241 train_time:43443ms step_avg:179.52ms\nstep:253/5000 train_loss:4.6740 train_time:43621ms step_avg:179.51ms\nstep:254/5000 train_loss:4.5048 train_time:43801ms step_avg:179.51ms\nstep:255/5000 train_loss:4.6830 train_time:43980ms step_avg:179.51ms\nstep:256/5000 train_loss:4.6874 train_time:44158ms step_avg:179.50ms\nstep:257/5000 train_loss:4.8678 train_time:44337ms step_avg:179.50ms\nstep:258/5000 train_loss:4.5467 train_time:44516ms step_avg:179.50ms\nstep:259/5000 train_loss:4.7116 train_time:44694ms step_avg:179.49ms\nstep:260/5000 train_loss:4.5423 train_time:44874ms step_avg:179.50ms\nstep:261/5000 train_loss:4.6598 train_time:45054ms step_avg:179.50ms\nstep:262/5000 train_loss:4.6630 train_time:45233ms step_avg:179.50ms\nstep:263/5000 train_loss:4.6770 train_time:45412ms step_avg:179.49ms\nstep:264/5000 train_loss:4.6156 train_time:45591ms step_avg:179.49ms\nstep:265/5000 train_loss:4.8591 train_time:45771ms step_avg:179.49ms\nstep:266/5000 train_loss:4.5702 train_time:45951ms step_avg:179.50ms\nstep:267/5000 train_loss:4.6346 train_time:46131ms step_avg:179.50ms\nstep:268/5000 train_loss:4.6639 train_time:46310ms step_avg:179.50ms\nstep:269/5000 train_loss:4.6236 train_time:46489ms step_avg:179.50ms\nstep:270/5000 train_loss:4.8202 train_time:46669ms step_avg:179.50ms\nstep:271/5000 train_loss:4.8059 train_time:46849ms step_avg:179.50ms\nstep:272/5000 train_loss:4.6792 train_time:47029ms step_avg:179.50ms\nstep:273/5000 train_loss:4.8074 train_time:47209ms step_avg:179.50ms\nstep:274/5000 train_loss:4.4773 train_time:47389ms step_avg:179.51ms\nstep:275/5000 train_loss:4.6120 train_time:47569ms step_avg:179.51ms\nstep:276/5000 train_loss:4.6434 train_time:47749ms step_avg:179.51ms\nstep:277/5000 train_loss:4.6306 train_time:47930ms step_avg:179.51ms\nstep:278/5000 train_loss:4.5601 train_time:48111ms step_avg:179.52ms\nstep:279/5000 train_loss:4.6524 train_time:48290ms step_avg:179.52ms\nstep:280/5000 train_loss:4.5637 train_time:48469ms step_avg:179.51ms\nstep:281/5000 train_loss:4.6638 train_time:48648ms step_avg:179.51ms\nstep:282/5000 train_loss:4.6887 train_time:48828ms step_avg:179.51ms\nstep:283/5000 train_loss:4.5592 train_time:49007ms step_avg:179.51ms\nstep:284/5000 train_loss:4.5846 train_time:49187ms step_avg:179.52ms\nstep:285/5000 train_loss:4.7212 train_time:49368ms step_avg:179.52ms\nstep:286/5000 train_loss:4.5877 train_time:49547ms step_avg:179.52ms\nstep:287/5000 train_loss:4.6272 train_time:49726ms step_avg:179.52ms\nstep:288/5000 train_loss:4.6247 train_time:49906ms step_avg:179.52ms\nstep:289/5000 train_loss:4.5949 train_time:50087ms step_avg:179.52ms\nstep:290/5000 train_loss:4.4883 train_time:50265ms step_avg:179.52ms\nstep:291/5000 train_loss:4.7339 train_time:50444ms step_avg:179.51ms\nstep:292/5000 train_loss:4.5683 train_time:50625ms step_avg:179.52ms\nstep:293/5000 train_loss:4.6608 train_time:50804ms step_avg:179.52ms\nstep:294/5000 train_loss:4.6300 train_time:50982ms step_avg:179.51ms\nstep:295/5000 train_loss:4.4756 train_time:51162ms step_avg:179.51ms\nstep:296/5000 train_loss:4.4677 train_time:51340ms step_avg:179.51ms\nstep:297/5000 train_loss:4.6866 train_time:51520ms step_avg:179.51ms\nstep:298/5000 train_loss:4.5042 train_time:51699ms step_avg:179.51ms\nstep:299/5000 train_loss:4.4863 train_time:51879ms step_avg:179.51ms\nstep:300/5000 train_loss:4.4827 train_time:52057ms step_avg:179.51ms\nstep:301/5000 train_loss:4.5455 train_time:52235ms step_avg:179.50ms\nstep:302/5000 train_loss:4.5413 train_time:52416ms step_avg:179.51ms\nstep:303/5000 train_loss:4.4581 train_time:52594ms step_avg:179.50ms\nstep:304/5000 train_loss:4.4186 train_time:52773ms step_avg:179.50ms\nstep:305/5000 train_loss:4.5998 train_time:52951ms step_avg:179.50ms\nstep:306/5000 train_loss:4.6453 train_time:53130ms step_avg:179.49ms\nstep:307/5000 train_loss:4.4844 train_time:53310ms step_avg:179.49ms\nstep:308/5000 train_loss:4.6086 train_time:53490ms step_avg:179.50ms\nstep:309/5000 train_loss:4.5697 train_time:53668ms step_avg:179.49ms\nstep:310/5000 train_loss:4.9371 train_time:53849ms step_avg:179.50ms\nstep:311/5000 train_loss:4.5728 train_time:54029ms step_avg:179.50ms\nstep:312/5000 train_loss:4.5665 train_time:54210ms step_avg:179.50ms\nstep:313/5000 train_loss:4.6844 train_time:54389ms step_avg:179.50ms\nstep:314/5000 train_loss:4.5489 train_time:54569ms step_avg:179.50ms\nstep:315/5000 train_loss:4.4045 train_time:54749ms step_avg:179.50ms\nstep:316/5000 train_loss:4.2532 train_time:54929ms step_avg:179.51ms\nstep:317/5000 train_loss:4.5493 train_time:55110ms step_avg:179.51ms\nstep:318/5000 train_loss:4.5989 train_time:55289ms step_avg:179.51ms\nstep:319/5000 train_loss:4.4551 train_time:55467ms step_avg:179.51ms\nstep:320/5000 train_loss:4.5551 train_time:55647ms step_avg:179.51ms\nstep:321/5000 train_loss:4.5446 train_time:55827ms step_avg:179.51ms\nstep:322/5000 train_loss:4.4145 train_time:56006ms step_avg:179.51ms\nstep:323/5000 train_loss:4.4504 train_time:56187ms step_avg:179.51ms\nstep:324/5000 train_loss:4.4832 train_time:56367ms step_avg:179.51ms\nstep:325/5000 train_loss:4.5852 train_time:56546ms step_avg:179.51ms\nstep:326/5000 train_loss:4.5728 train_time:56726ms step_avg:179.51ms\nstep:327/5000 train_loss:4.4717 train_time:56907ms step_avg:179.52ms\nstep:328/5000 train_loss:4.5180 train_time:57087ms step_avg:179.52ms\nstep:329/5000 train_loss:5.5521 train_time:57267ms step_avg:179.52ms\nstep:330/5000 train_loss:4.5440 train_time:57446ms step_avg:179.52ms\nstep:331/5000 train_loss:4.4433 train_time:57626ms step_avg:179.52ms\nstep:332/5000 train_loss:4.5322 train_time:57805ms step_avg:179.52ms\nstep:333/5000 train_loss:4.4916 train_time:57987ms step_avg:179.53ms\nstep:334/5000 train_loss:4.4932 train_time:58166ms step_avg:179.53ms\nstep:335/5000 train_loss:4.4667 train_time:58347ms step_avg:179.53ms\nstep:336/5000 train_loss:4.4284 train_time:58527ms step_avg:179.53ms\nstep:337/5000 train_loss:4.5483 train_time:58706ms step_avg:179.53ms\nstep:338/5000 train_loss:4.3956 train_time:58886ms step_avg:179.53ms\nstep:339/5000 train_loss:4.4093 train_time:59065ms step_avg:179.53ms\nstep:340/5000 train_loss:4.4050 train_time:59245ms step_avg:179.53ms\nstep:341/5000 train_loss:4.5243 train_time:59428ms step_avg:179.54ms\nstep:342/5000 train_loss:4.6083 train_time:59611ms step_avg:179.55ms\nstep:343/5000 train_loss:4.5929 train_time:59791ms step_avg:179.55ms\nstep:344/5000 train_loss:4.5599 train_time:59971ms step_avg:179.56ms\nstep:345/5000 train_loss:4.6000 train_time:60152ms step_avg:179.56ms\nstep:346/5000 train_loss:4.5089 train_time:60332ms step_avg:179.56ms\nstep:347/5000 train_loss:4.4057 train_time:60510ms step_avg:179.56ms\nstep:348/5000 train_loss:4.5406 train_time:60690ms step_avg:179.56ms\nstep:349/5000 train_loss:4.5063 train_time:60868ms step_avg:179.55ms\nstep:350/5000 train_loss:4.4762 train_time:61047ms step_avg:179.55ms\nstep:351/5000 train_loss:4.4706 train_time:61227ms step_avg:179.55ms\nstep:352/5000 train_loss:4.4060 train_time:61408ms step_avg:179.55ms\nstep:353/5000 train_loss:4.5001 train_time:61587ms step_avg:179.56ms\nstep:354/5000 train_loss:4.4591 train_time:61766ms step_avg:179.55ms\nstep:355/5000 train_loss:4.5712 train_time:61945ms step_avg:179.55ms\nstep:356/5000 train_loss:4.4390 train_time:62125ms step_avg:179.55ms\nstep:357/5000 train_loss:4.3059 train_time:62306ms step_avg:179.56ms\nstep:358/5000 train_loss:4.4277 train_time:62487ms step_avg:179.56ms\nstep:359/5000 train_loss:4.4879 train_time:62668ms step_avg:179.56ms\nstep:360/5000 train_loss:4.3547 train_time:62847ms step_avg:179.56ms\nstep:361/5000 train_loss:4.4197 train_time:63027ms step_avg:179.56ms\nstep:362/5000 train_loss:4.4157 train_time:63208ms step_avg:179.57ms\nstep:363/5000 train_loss:4.4169 train_time:63388ms step_avg:179.57ms\nstep:364/5000 train_loss:4.5095 train_time:63568ms step_avg:179.57ms\nstep:365/5000 train_loss:4.3430 train_time:63747ms step_avg:179.57ms\nstep:366/5000 train_loss:4.5342 train_time:63927ms step_avg:179.57ms\nstep:367/5000 train_loss:4.5826 train_time:64106ms step_avg:179.57ms\nstep:368/5000 train_loss:4.4793 train_time:64287ms step_avg:179.57ms\nstep:369/5000 train_loss:4.4194 train_time:64467ms step_avg:179.57ms\nstep:370/5000 train_loss:4.3692 train_time:64647ms step_avg:179.58ms\nstep:371/5000 train_loss:4.4090 train_time:64826ms step_avg:179.57ms\nstep:372/5000 train_loss:4.5315 train_time:65005ms step_avg:179.57ms\nstep:373/5000 train_loss:4.3627 train_time:65184ms step_avg:179.57ms\nstep:374/5000 train_loss:4.4401 train_time:65362ms step_avg:179.57ms\nstep:375/5000 train_loss:4.4652 train_time:65542ms step_avg:179.57ms\nstep:376/5000 train_loss:4.6901 train_time:65722ms step_avg:179.57ms\nstep:377/5000 train_loss:4.3438 train_time:65901ms step_avg:179.57ms\nstep:378/5000 train_loss:4.4778 train_time:66082ms step_avg:179.57ms\nstep:379/5000 train_loss:4.4840 train_time:66262ms step_avg:179.57ms\nstep:380/5000 train_loss:4.4306 train_time:66441ms step_avg:179.57ms\nstep:381/5000 train_loss:4.4653 train_time:66621ms step_avg:179.57ms\nstep:382/5000 train_loss:4.3855 train_time:66800ms step_avg:179.57ms\nstep:383/5000 train_loss:4.4895 train_time:66979ms step_avg:179.57ms\nstep:384/5000 train_loss:4.3191 train_time:67157ms step_avg:179.56ms\nstep:385/5000 train_loss:4.3521 train_time:67336ms step_avg:179.56ms\nstep:386/5000 train_loss:4.4086 train_time:67514ms step_avg:179.56ms\nstep:387/5000 train_loss:4.3907 train_time:67693ms step_avg:179.56ms\nstep:388/5000 train_loss:4.4859 train_time:67871ms step_avg:179.55ms\nstep:389/5000 train_loss:4.4449 train_time:68051ms step_avg:179.55ms\nstep:390/5000 train_loss:4.3795 train_time:68231ms step_avg:179.56ms\nstep:391/5000 train_loss:4.4186 train_time:68412ms step_avg:179.56ms\nstep:392/5000 train_loss:4.3472 train_time:68593ms step_avg:179.56ms\nstep:393/5000 train_loss:4.3693 train_time:68771ms step_avg:179.56ms\nstep:394/5000 train_loss:4.4521 train_time:68951ms step_avg:179.56ms\nstep:395/5000 train_loss:4.2810 train_time:69131ms step_avg:179.56ms\nstep:396/5000 train_loss:4.3324 train_time:69311ms step_avg:179.56ms\nstep:397/5000 train_loss:4.4130 train_time:69490ms step_avg:179.56ms\nstep:398/5000 train_loss:4.4985 train_time:69670ms step_avg:179.56ms\nstep:399/5000 train_loss:4.2710 train_time:69850ms step_avg:179.56ms\nstep:400/5000 train_loss:4.3773 train_time:70030ms step_avg:179.56ms\nstep:401/5000 train_loss:4.2689 train_time:70210ms step_avg:179.56ms\nstep:402/5000 train_loss:4.3631 train_time:70390ms step_avg:179.57ms\nstep:403/5000 train_loss:4.2936 train_time:70571ms step_avg:179.57ms\nstep:404/5000 train_loss:4.2479 train_time:70751ms step_avg:179.57ms\nstep:405/5000 train_loss:4.3766 train_time:70931ms step_avg:179.57ms\nstep:406/5000 train_loss:4.5181 train_time:71110ms step_avg:179.57ms\nstep:407/5000 train_loss:4.3757 train_time:71289ms step_avg:179.57ms\nstep:408/5000 train_loss:4.4152 train_time:71469ms step_avg:179.57ms\nstep:409/5000 train_loss:4.4887 train_time:71649ms step_avg:179.57ms\nstep:410/5000 train_loss:4.3781 train_time:71829ms step_avg:179.57ms\nstep:411/5000 train_loss:4.4642 train_time:72009ms step_avg:179.57ms\nstep:412/5000 train_loss:4.3506 train_time:72189ms step_avg:179.58ms\nstep:413/5000 train_loss:4.4700 train_time:72369ms step_avg:179.58ms\nstep:414/5000 train_loss:4.3805 train_time:72550ms step_avg:179.58ms\nstep:415/5000 train_loss:5.4223 train_time:72731ms step_avg:179.58ms\nstep:416/5000 train_loss:4.4299 train_time:72912ms step_avg:179.59ms\nstep:417/5000 train_loss:4.3626 train_time:73091ms step_avg:179.58ms\nstep:418/5000 train_loss:4.3665 train_time:73269ms step_avg:179.58ms\nstep:419/5000 train_loss:4.4532 train_time:73451ms step_avg:179.59ms\nstep:420/5000 train_loss:4.3922 train_time:73629ms step_avg:179.58ms\nstep:421/5000 train_loss:4.4323 train_time:73810ms step_avg:179.59ms\nstep:422/5000 train_loss:4.3468 train_time:73989ms step_avg:179.58ms\nstep:423/5000 train_loss:4.5323 train_time:74169ms step_avg:179.59ms\nstep:424/5000 train_loss:4.5418 train_time:74348ms step_avg:179.59ms\nstep:425/5000 train_loss:4.4119 train_time:74529ms step_avg:179.59ms\nstep:426/5000 train_loss:4.3390 train_time:74710ms step_avg:179.59ms\nstep:427/5000 train_loss:4.4907 train_time:74890ms step_avg:179.59ms\nstep:428/5000 train_loss:4.3718 train_time:75070ms step_avg:179.59ms\nstep:429/5000 train_loss:4.4784 train_time:75250ms step_avg:179.59ms\nstep:430/5000 train_loss:4.1903 train_time:75429ms step_avg:179.59ms\nstep:431/5000 train_loss:4.2324 train_time:75610ms step_avg:179.60ms\nstep:432/5000 train_loss:4.2478 train_time:75789ms step_avg:179.60ms\nstep:433/5000 train_loss:4.3835 train_time:75969ms step_avg:179.60ms\nstep:434/5000 train_loss:4.3267 train_time:76149ms step_avg:179.60ms\nstep:435/5000 train_loss:4.3200 train_time:76330ms step_avg:179.60ms\nstep:436/5000 train_loss:4.6080 train_time:76509ms step_avg:179.60ms\nstep:437/5000 train_loss:4.2923 train_time:76688ms step_avg:179.60ms\nstep:438/5000 train_loss:4.3016 train_time:76868ms step_avg:179.60ms\nstep:439/5000 train_loss:4.3458 train_time:77048ms step_avg:179.60ms\nstep:440/5000 train_loss:4.3738 train_time:77227ms step_avg:179.60ms\nstep:441/5000 train_loss:4.5242 train_time:77406ms step_avg:179.60ms\nstep:442/5000 train_loss:4.2517 train_time:77585ms step_avg:179.59ms\nstep:443/5000 train_loss:4.3954 train_time:77764ms step_avg:179.59ms\nstep:444/5000 train_loss:4.3103 train_time:77942ms step_avg:179.59ms\nstep:445/5000 train_loss:4.3646 train_time:78121ms step_avg:179.59ms\nstep:446/5000 train_loss:4.3991 train_time:78300ms step_avg:179.59ms\nstep:447/5000 train_loss:4.3904 train_time:78479ms step_avg:179.59ms\nstep:448/5000 train_loss:4.5131 train_time:78658ms step_avg:179.59ms\nstep:449/5000 train_loss:4.2473 train_time:78837ms step_avg:179.58ms\nstep:450/5000 train_loss:4.3129 train_time:79015ms step_avg:179.58ms\nstep:451/5000 train_loss:4.4800 train_time:79192ms step_avg:179.57ms\nstep:452/5000 train_loss:4.1950 train_time:79372ms step_avg:179.57ms\nstep:453/5000 train_loss:4.6314 train_time:79551ms step_avg:179.57ms\nstep:454/5000 train_loss:4.1972 train_time:79731ms step_avg:179.57ms\nstep:455/5000 train_loss:4.3132 train_time:79911ms step_avg:179.58ms\nstep:456/5000 train_loss:4.4048 train_time:80091ms step_avg:179.58ms\nstep:457/5000 train_loss:4.3496 train_time:80270ms step_avg:179.57ms\nstep:458/5000 train_loss:4.3363 train_time:80450ms step_avg:179.58ms\nstep:459/5000 train_loss:4.3678 train_time:80630ms step_avg:179.58ms\nstep:460/5000 train_loss:4.3252 train_time:80810ms step_avg:179.58ms\nstep:461/5000 train_loss:4.2019 train_time:80992ms step_avg:179.58ms\nstep:462/5000 train_loss:4.3244 train_time:81171ms step_avg:179.58ms\nstep:463/5000 train_loss:4.3305 train_time:81350ms step_avg:179.58ms\nstep:464/5000 train_loss:4.3702 train_time:81531ms step_avg:179.58ms\nstep:465/5000 train_loss:4.2746 train_time:81711ms step_avg:179.58ms\nstep:466/5000 train_loss:4.2209 train_time:81892ms step_avg:179.59ms\nstep:467/5000 train_loss:4.3511 train_time:82072ms step_avg:179.59ms\nstep:468/5000 train_loss:4.1607 train_time:82251ms step_avg:179.59ms\nstep:469/5000 train_loss:4.3278 train_time:82431ms step_avg:179.59ms\nstep:470/5000 train_loss:4.4534 train_time:82610ms step_avg:179.59ms\nstep:471/5000 train_loss:4.4234 train_time:82790ms step_avg:179.59ms\nstep:472/5000 train_loss:4.4072 train_time:82969ms step_avg:179.59ms\nstep:473/5000 train_loss:4.2906 train_time:83149ms step_avg:179.59ms\nstep:474/5000 train_loss:4.3131 train_time:83327ms step_avg:179.58ms\nstep:475/5000 train_loss:4.3488 train_time:83506ms step_avg:179.58ms\nstep:476/5000 train_loss:4.2257 train_time:83685ms step_avg:179.58ms\nstep:477/5000 train_loss:4.3158 train_time:83864ms step_avg:179.58ms\nstep:478/5000 train_loss:4.1790 train_time:84043ms step_avg:179.58ms\nstep:479/5000 train_loss:4.2975 train_time:84223ms step_avg:179.58ms\nstep:480/5000 train_loss:4.3315 train_time:84402ms step_avg:179.58ms\nstep:481/5000 train_loss:4.3494 train_time:84581ms step_avg:179.58ms\nstep:482/5000 train_loss:4.1687 train_time:84760ms step_avg:179.58ms\nstep:483/5000 train_loss:4.2707 train_time:84938ms step_avg:179.57ms\nstep:484/5000 train_loss:4.3136 train_time:85117ms step_avg:179.57ms\nstep:485/5000 train_loss:4.3920 train_time:85296ms step_avg:179.57ms\nstep:486/5000 train_loss:4.3191 train_time:85474ms step_avg:179.57ms\nstep:487/5000 train_loss:4.2463 train_time:85652ms step_avg:179.56ms\nstep:488/5000 train_loss:4.2125 train_time:85835ms step_avg:179.57ms\nstep:489/5000 train_loss:4.1435 train_time:86012ms step_avg:179.57ms\nstep:490/5000 train_loss:4.1270 train_time:86192ms step_avg:179.57ms\nstep:491/5000 train_loss:4.3413 train_time:86371ms step_avg:179.56ms\nstep:492/5000 train_loss:4.2932 train_time:86550ms step_avg:179.56ms\nstep:493/5000 train_loss:4.1879 train_time:86730ms step_avg:179.56ms\nstep:494/5000 train_loss:4.4568 train_time:86911ms step_avg:179.57ms\nstep:495/5000 train_loss:4.2750 train_time:87091ms step_avg:179.57ms\nstep:496/5000 train_loss:4.1593 train_time:87270ms step_avg:179.57ms\nstep:497/5000 train_loss:4.2714 train_time:87450ms step_avg:179.57ms\nstep:498/5000 train_loss:4.3268 train_time:87629ms step_avg:179.57ms\nstep:499/5000 train_loss:4.3562 train_time:87809ms step_avg:179.57ms\nstep:500/5000 train_loss:4.1733 train_time:87988ms step_avg:179.57ms\nstep:501/5000 train_loss:4.3055 train_time:88168ms step_avg:179.57ms\nstep:502/5000 train_loss:4.2862 train_time:88347ms step_avg:179.57ms\nstep:503/5000 train_loss:4.3120 train_time:88527ms step_avg:179.57ms\nstep:504/5000 train_loss:4.1171 train_time:88706ms step_avg:179.57ms\nstep:505/5000 train_loss:4.2153 train_time:88886ms step_avg:179.57ms\nstep:506/5000 train_loss:4.3053 train_time:89066ms step_avg:179.57ms\nstep:507/5000 train_loss:4.3966 train_time:89244ms step_avg:179.57ms\nstep:508/5000 train_loss:4.3229 train_time:89423ms step_avg:179.56ms\nstep:509/5000 train_loss:4.4962 train_time:89603ms step_avg:179.57ms\nstep:510/5000 train_loss:4.2575 train_time:89782ms step_avg:179.56ms\nstep:511/5000 train_loss:4.1592 train_time:89961ms step_avg:179.56ms\nstep:512/5000 train_loss:4.3183 train_time:90141ms step_avg:179.56ms\nstep:513/5000 train_loss:4.3241 train_time:90320ms step_avg:179.56ms\nstep:514/5000 train_loss:4.5658 train_time:90498ms step_avg:179.56ms\nstep:515/5000 train_loss:4.4050 train_time:90677ms step_avg:179.56ms\nstep:516/5000 train_loss:4.3417 train_time:90856ms step_avg:179.56ms\nstep:517/5000 train_loss:4.2675 train_time:91034ms step_avg:179.55ms\nstep:518/5000 train_loss:4.2381 train_time:91213ms step_avg:179.55ms\nstep:519/5000 train_loss:4.0796 train_time:91393ms step_avg:179.55ms\nstep:520/5000 train_loss:4.2364 train_time:91570ms step_avg:179.55ms\nstep:521/5000 train_loss:4.1259 train_time:91749ms step_avg:179.55ms\nstep:522/5000 train_loss:4.2922 train_time:91928ms step_avg:179.55ms\nstep:523/5000 train_loss:4.2185 train_time:92106ms step_avg:179.54ms\nstep:524/5000 train_loss:4.4329 train_time:92288ms step_avg:179.55ms\nstep:525/5000 train_loss:4.2970 train_time:92468ms step_avg:179.55ms\nstep:526/5000 train_loss:4.2306 train_time:92645ms step_avg:179.55ms\nstep:527/5000 train_loss:4.1793 train_time:92824ms step_avg:179.54ms\nstep:528/5000 train_loss:4.1933 train_time:93003ms step_avg:179.54ms\nstep:529/5000 train_loss:4.2956 train_time:93181ms step_avg:179.54ms\nstep:530/5000 train_loss:4.3360 train_time:93361ms step_avg:179.54ms\nstep:531/5000 train_loss:4.2371 train_time:93540ms step_avg:179.54ms\nstep:532/5000 train_loss:4.1790 train_time:93718ms step_avg:179.54ms\nstep:533/5000 train_loss:4.2505 train_time:93897ms step_avg:179.53ms\nstep:534/5000 train_loss:4.1607 train_time:94075ms step_avg:179.53ms\nstep:535/5000 train_loss:4.2551 train_time:94253ms step_avg:179.53ms\nstep:536/5000 train_loss:4.3254 train_time:94432ms step_avg:179.53ms\nstep:537/5000 train_loss:4.1880 train_time:94611ms step_avg:179.53ms\nstep:538/5000 train_loss:4.3392 train_time:94791ms step_avg:179.53ms\nstep:539/5000 train_loss:4.3793 train_time:94970ms step_avg:179.53ms\nstep:540/5000 train_loss:4.1652 train_time:95149ms step_avg:179.53ms\nstep:541/5000 train_loss:4.1770 train_time:95328ms step_avg:179.53ms\nstep:542/5000 train_loss:4.0932 train_time:95507ms step_avg:179.53ms\nstep:543/5000 train_loss:4.3035 train_time:95687ms step_avg:179.53ms\nstep:544/5000 train_loss:4.1630 train_time:95865ms step_avg:179.52ms\nstep:545/5000 train_loss:4.3249 train_time:96045ms step_avg:179.52ms\nstep:546/5000 train_loss:4.2746 train_time:96225ms step_avg:179.52ms\nstep:547/5000 train_loss:4.1148 train_time:96407ms step_avg:179.53ms\nstep:548/5000 train_loss:4.2015 train_time:96588ms step_avg:179.53ms\nstep:549/5000 train_loss:4.1656 train_time:96768ms step_avg:179.53ms\nstep:550/5000 train_loss:4.1894 train_time:96948ms step_avg:179.53ms\nstep:551/5000 train_loss:4.2336 train_time:97128ms step_avg:179.53ms\nstep:552/5000 train_loss:4.2611 train_time:97307ms step_avg:179.53ms\nstep:553/5000 train_loss:4.2699 train_time:97486ms step_avg:179.53ms\nstep:554/5000 train_loss:4.2679 train_time:97665ms step_avg:179.53ms\nstep:555/5000 train_loss:4.2164 train_time:97845ms step_avg:179.53ms\nstep:556/5000 train_loss:4.3547 train_time:98024ms step_avg:179.53ms\nstep:557/5000 train_loss:4.3332 train_time:98206ms step_avg:179.53ms\nstep:558/5000 train_loss:4.3572 train_time:98382ms step_avg:179.53ms\nstep:559/5000 train_loss:4.2233 train_time:98560ms step_avg:179.53ms\nstep:560/5000 train_loss:4.2493 train_time:98738ms step_avg:179.52ms\nstep:561/5000 train_loss:4.3364 train_time:98917ms step_avg:179.52ms\nstep:562/5000 train_loss:4.2146 train_time:99096ms step_avg:179.52ms\nstep:563/5000 train_loss:4.1356 train_time:99274ms step_avg:179.52ms\nstep:564/5000 train_loss:4.1413 train_time:99453ms step_avg:179.52ms\nstep:565/5000 train_loss:4.2841 train_time:99631ms step_avg:179.52ms\nstep:566/5000 train_loss:4.1939 train_time:99810ms step_avg:179.51ms\nstep:567/5000 train_loss:4.5146 train_time:99988ms step_avg:179.51ms\nstep:568/5000 train_loss:4.2619 train_time:100168ms step_avg:179.51ms\nstep:569/5000 train_loss:4.2487 train_time:100347ms step_avg:179.51ms\nstep:570/5000 train_loss:4.1580 train_time:100528ms step_avg:179.51ms\nstep:571/5000 train_loss:4.1773 train_time:100706ms step_avg:179.51ms\nstep:572/5000 train_loss:4.3244 train_time:100887ms step_avg:179.51ms\nstep:573/5000 train_loss:4.2036 train_time:101067ms step_avg:179.51ms\nstep:574/5000 train_loss:4.2607 train_time:101246ms step_avg:179.51ms\nstep:575/5000 train_loss:4.2439 train_time:101426ms step_avg:179.51ms\nstep:576/5000 train_loss:4.4269 train_time:101605ms step_avg:179.51ms\nstep:577/5000 train_loss:4.1206 train_time:101784ms step_avg:179.51ms\nstep:578/5000 train_loss:4.2154 train_time:101963ms step_avg:179.51ms\nstep:579/5000 train_loss:4.2041 train_time:102142ms step_avg:179.51ms\nstep:580/5000 train_loss:4.1969 train_time:102321ms step_avg:179.51ms\nstep:581/5000 train_loss:4.2398 train_time:102500ms step_avg:179.51ms\nstep:582/5000 train_loss:4.0790 train_time:102680ms step_avg:179.51ms\nstep:583/5000 train_loss:4.2228 train_time:102858ms step_avg:179.51ms\nstep:584/5000 train_loss:4.2200 train_time:103035ms step_avg:179.50ms\nstep:585/5000 train_loss:4.1424 train_time:103213ms step_avg:179.50ms\nstep:586/5000 train_loss:4.2299 train_time:103391ms step_avg:179.50ms\nstep:587/5000 train_loss:4.1399 train_time:103569ms step_avg:179.50ms\nstep:588/5000 train_loss:4.2560 train_time:103748ms step_avg:179.49ms\nstep:589/5000 train_loss:4.1757 train_time:103928ms step_avg:179.50ms\nstep:590/5000 train_loss:4.2723 train_time:104107ms step_avg:179.49ms\nstep:591/5000 train_loss:4.1307 train_time:104286ms step_avg:179.49ms\nstep:592/5000 train_loss:4.0549 train_time:104466ms step_avg:179.49ms\nstep:593/5000 train_loss:4.1872 train_time:104645ms step_avg:179.49ms\nstep:594/5000 train_loss:4.2175 train_time:104824ms step_avg:179.49ms\nstep:595/5000 train_loss:4.1904 train_time:105006ms step_avg:179.50ms\nstep:596/5000 train_loss:4.2001 train_time:105185ms step_avg:179.50ms\nstep:597/5000 train_loss:4.3078 train_time:105363ms step_avg:179.49ms\nstep:598/5000 train_loss:4.3373 train_time:105542ms step_avg:179.49ms\nstep:599/5000 train_loss:4.5223 train_time:105720ms step_avg:179.49ms\nstep:600/5000 train_loss:4.1839 train_time:105899ms step_avg:179.49ms\nstep:601/5000 train_loss:4.2962 train_time:106078ms step_avg:179.49ms\nstep:602/5000 train_loss:4.1867 train_time:106255ms step_avg:179.49ms\nstep:603/5000 train_loss:4.3326 train_time:106434ms step_avg:179.48ms\nstep:604/5000 train_loss:4.1833 train_time:106612ms step_avg:179.48ms\nstep:605/5000 train_loss:4.1414 train_time:106790ms step_avg:179.48ms\nstep:606/5000 train_loss:4.1925 train_time:106970ms step_avg:179.48ms\nstep:607/5000 train_loss:4.0958 train_time:107153ms step_avg:179.49ms\nstep:608/5000 train_loss:4.0337 train_time:107331ms step_avg:179.48ms\nstep:609/5000 train_loss:4.0914 train_time:107511ms step_avg:179.48ms\nstep:610/5000 train_loss:4.2833 train_time:107689ms step_avg:179.48ms\nstep:611/5000 train_loss:4.1198 train_time:107867ms step_avg:179.48ms\nstep:612/5000 train_loss:4.2660 train_time:108046ms step_avg:179.48ms\nstep:613/5000 train_loss:4.0497 train_time:108227ms step_avg:179.48ms\nstep:614/5000 train_loss:4.2714 train_time:108409ms step_avg:179.48ms\nstep:615/5000 train_loss:4.2303 train_time:108587ms step_avg:179.48ms\nstep:616/5000 train_loss:4.0636 train_time:108768ms step_avg:179.49ms\nstep:617/5000 train_loss:4.2455 train_time:108949ms step_avg:179.49ms\nstep:618/5000 train_loss:4.1836 train_time:109128ms step_avg:179.49ms\nstep:619/5000 train_loss:4.3432 train_time:109308ms step_avg:179.49ms\nstep:620/5000 train_loss:4.0655 train_time:109489ms step_avg:179.49ms\nstep:621/5000 train_loss:4.3144 train_time:109668ms step_avg:179.49ms\nstep:622/5000 train_loss:4.2655 train_time:109846ms step_avg:179.49ms\nstep:623/5000 train_loss:4.1369 train_time:110026ms step_avg:179.49ms\nstep:624/5000 train_loss:4.1837 train_time:110204ms step_avg:179.49ms\nstep:625/5000 train_loss:4.2207 train_time:110383ms step_avg:179.49ms\nstep:626/5000 train_loss:4.3304 train_time:110563ms step_avg:179.49ms\nstep:627/5000 train_loss:4.2270 train_time:110743ms step_avg:179.49ms\nstep:628/5000 train_loss:4.2716 train_time:110922ms step_avg:179.49ms\nstep:629/5000 train_loss:4.2230 train_time:111101ms step_avg:179.49ms\nstep:630/5000 train_loss:4.1617 train_time:111281ms step_avg:179.49ms\nstep:631/5000 train_loss:4.1228 train_time:111460ms step_avg:179.48ms\nstep:632/5000 train_loss:4.1766 train_time:111639ms step_avg:179.48ms\nstep:633/5000 train_loss:4.1009 train_time:111817ms step_avg:179.48ms\nstep:634/5000 train_loss:4.2311 train_time:111995ms step_avg:179.48ms\nstep:635/5000 train_loss:4.2875 train_time:112174ms step_avg:179.48ms\nstep:636/5000 train_loss:4.1286 train_time:112353ms step_avg:179.48ms\nstep:637/5000 train_loss:4.2177 train_time:112533ms step_avg:179.48ms\nstep:638/5000 train_loss:4.0344 train_time:112715ms step_avg:179.48ms\nstep:639/5000 train_loss:4.2788 train_time:112893ms step_avg:179.48ms\nstep:640/5000 train_loss:4.0698 train_time:113071ms step_avg:179.48ms\nstep:641/5000 train_loss:4.1785 train_time:113251ms step_avg:179.48ms\nstep:642/5000 train_loss:4.2738 train_time:113430ms step_avg:179.48ms\nstep:643/5000 train_loss:4.2779 train_time:113613ms step_avg:179.48ms\nstep:644/5000 train_loss:4.1419 train_time:113792ms step_avg:179.48ms\nstep:645/5000 train_loss:4.1146 train_time:113968ms step_avg:179.48ms\nstep:646/5000 train_loss:4.2819 train_time:114146ms step_avg:179.47ms\nstep:647/5000 train_loss:4.1958 train_time:114325ms step_avg:179.47ms\nstep:648/5000 train_loss:4.1662 train_time:114504ms step_avg:179.47ms\nstep:649/5000 train_loss:4.1361 train_time:114684ms step_avg:179.47ms\nstep:650/5000 train_loss:4.2606 train_time:114863ms step_avg:179.47ms\nstep:651/5000 train_loss:4.2506 train_time:115042ms step_avg:179.47ms\nstep:652/5000 train_loss:4.1331 train_time:115221ms step_avg:179.47ms\nstep:653/5000 train_loss:4.0378 train_time:115400ms step_avg:179.47ms\nstep:654/5000 train_loss:4.1110 train_time:115578ms step_avg:179.47ms\nstep:655/5000 train_loss:4.3979 train_time:115757ms step_avg:179.47ms\nstep:656/5000 train_loss:4.0690 train_time:115934ms step_avg:179.46ms\nstep:657/5000 train_loss:4.0758 train_time:116113ms step_avg:179.46ms\nstep:658/5000 train_loss:4.2045 train_time:116291ms step_avg:179.46ms\nstep:659/5000 train_loss:4.1279 train_time:116470ms step_avg:179.46ms\nstep:660/5000 train_loss:4.1489 train_time:116648ms step_avg:179.46ms\nstep:661/5000 train_loss:4.0620 train_time:116828ms step_avg:179.46ms\nstep:662/5000 train_loss:4.2765 train_time:117007ms step_avg:179.46ms\nstep:663/5000 train_loss:4.1458 train_time:117187ms step_avg:179.46ms\nstep:664/5000 train_loss:4.2801 train_time:117364ms step_avg:179.46ms\nstep:665/5000 train_loss:4.2196 train_time:117543ms step_avg:179.45ms\nstep:666/5000 train_loss:4.1181 train_time:117721ms step_avg:179.45ms\nstep:667/5000 train_loss:4.0356 train_time:117901ms step_avg:179.45ms\nstep:668/5000 train_loss:4.1894 train_time:118081ms step_avg:179.45ms\nstep:669/5000 train_loss:3.9922 train_time:118260ms step_avg:179.45ms\nstep:670/5000 train_loss:4.1985 train_time:118438ms step_avg:179.45ms\nstep:671/5000 train_loss:4.3013 train_time:118616ms step_avg:179.45ms\nstep:672/5000 train_loss:4.1339 train_time:118794ms step_avg:179.45ms\nstep:673/5000 train_loss:4.0301 train_time:118972ms step_avg:179.44ms\nstep:674/5000 train_loss:4.0895 train_time:119151ms step_avg:179.44ms\nstep:675/5000 train_loss:4.0714 train_time:119331ms step_avg:179.45ms\nstep:676/5000 train_loss:4.1730 train_time:119510ms step_avg:179.44ms\nstep:677/5000 train_loss:4.1075 train_time:119690ms step_avg:179.44ms\nstep:678/5000 train_loss:3.9595 train_time:119868ms step_avg:179.44ms\nstep:679/5000 train_loss:4.3192 train_time:120048ms step_avg:179.44ms\nstep:680/5000 train_loss:4.2284 train_time:120227ms step_avg:179.44ms\nstep:681/5000 train_loss:4.0974 train_time:120407ms step_avg:179.44ms\nstep:682/5000 train_loss:4.1349 train_time:120587ms step_avg:179.45ms\nstep:683/5000 train_loss:4.2316 train_time:120766ms step_avg:179.44ms\nstep:684/5000 train_loss:3.6911 train_time:120945ms step_avg:179.44ms\nstep:685/5000 train_loss:4.1661 train_time:121125ms step_avg:179.44ms\nstep:686/5000 train_loss:4.1618 train_time:121303ms step_avg:179.44ms\nstep:687/5000 train_loss:4.2986 train_time:121482ms step_avg:179.44ms\nstep:688/5000 train_loss:4.0762 train_time:121662ms step_avg:179.44ms\nstep:689/5000 train_loss:4.3970 train_time:121841ms step_avg:179.44ms\nstep:690/5000 train_loss:4.2079 train_time:122021ms step_avg:179.44ms\nstep:691/5000 train_loss:4.2418 train_time:122200ms step_avg:179.44ms\nstep:692/5000 train_loss:4.0660 train_time:122379ms step_avg:179.44ms\nstep:693/5000 train_loss:4.0334 train_time:122557ms step_avg:179.44ms\nstep:694/5000 train_loss:4.3604 train_time:122735ms step_avg:179.44ms\nstep:695/5000 train_loss:4.0599 train_time:122914ms step_avg:179.44ms\nstep:696/5000 train_loss:4.2780 train_time:123092ms step_avg:179.43ms\nstep:697/5000 train_loss:4.0870 train_time:123270ms step_avg:179.43ms\nstep:698/5000 train_loss:4.1867 train_time:123449ms step_avg:179.43ms\nstep:699/5000 train_loss:4.0265 train_time:123629ms step_avg:179.43ms\nstep:700/5000 train_loss:4.0965 train_time:123808ms step_avg:179.43ms\nstep:701/5000 train_loss:4.2962 train_time:123989ms step_avg:179.43ms\nstep:702/5000 train_loss:4.0438 train_time:124167ms step_avg:179.43ms\nstep:703/5000 train_loss:4.1407 train_time:124346ms step_avg:179.43ms\nstep:704/5000 train_loss:4.1774 train_time:124525ms step_avg:179.43ms\nstep:705/5000 train_loss:4.1818 train_time:124704ms step_avg:179.43ms\nstep:706/5000 train_loss:4.0140 train_time:124884ms step_avg:179.43ms\nstep:707/5000 train_loss:4.0895 train_time:125063ms step_avg:179.43ms\nstep:708/5000 train_loss:4.3023 train_time:125241ms step_avg:179.43ms\nstep:709/5000 train_loss:4.2127 train_time:125421ms step_avg:179.43ms\nstep:710/5000 train_loss:4.1250 train_time:125601ms step_avg:179.43ms\nstep:711/5000 train_loss:4.0316 train_time:125780ms step_avg:179.43ms\nstep:712/5000 train_loss:4.1598 train_time:125959ms step_avg:179.43ms\nstep:713/5000 train_loss:4.2098 train_time:126137ms step_avg:179.43ms\nstep:714/5000 train_loss:4.0901 train_time:126316ms step_avg:179.43ms\nstep:715/5000 train_loss:4.2623 train_time:126494ms step_avg:179.42ms\nstep:716/5000 train_loss:4.1002 train_time:126671ms step_avg:179.42ms\nstep:717/5000 train_loss:4.1005 train_time:126850ms step_avg:179.42ms\nstep:718/5000 train_loss:4.0668 train_time:127030ms step_avg:179.42ms\nstep:719/5000 train_loss:4.1754 train_time:127209ms step_avg:179.42ms\nstep:720/5000 train_loss:4.1667 train_time:127388ms step_avg:179.42ms\nstep:721/5000 train_loss:4.1577 train_time:127567ms step_avg:179.42ms\nstep:722/5000 train_loss:4.2367 train_time:127747ms step_avg:179.42ms\nstep:723/5000 train_loss:4.2180 train_time:127927ms step_avg:179.42ms\nstep:724/5000 train_loss:4.0610 train_time:128105ms step_avg:179.42ms\nstep:725/5000 train_loss:4.0921 train_time:128284ms step_avg:179.42ms\nstep:726/5000 train_loss:4.0933 train_time:128463ms step_avg:179.42ms\nstep:727/5000 train_loss:4.1035 train_time:128643ms step_avg:179.42ms\nstep:728/5000 train_loss:4.0967 train_time:128821ms step_avg:179.42ms\nstep:729/5000 train_loss:4.1927 train_time:129000ms step_avg:179.42ms\nstep:730/5000 train_loss:4.0876 train_time:129177ms step_avg:179.41ms\nstep:731/5000 train_loss:4.0404 train_time:129355ms step_avg:179.41ms\nstep:732/5000 train_loss:4.0719 train_time:129534ms step_avg:179.41ms\nstep:733/5000 train_loss:4.0928 train_time:129712ms step_avg:179.41ms\nstep:734/5000 train_loss:4.0722 train_time:129891ms step_avg:179.41ms\nstep:735/5000 train_loss:4.1090 train_time:130072ms step_avg:179.41ms\nstep:736/5000 train_loss:4.2266 train_time:130250ms step_avg:179.41ms\nstep:737/5000 train_loss:4.0877 train_time:130429ms step_avg:179.41ms\nstep:738/5000 train_loss:4.1625 train_time:130611ms step_avg:179.41ms\nstep:739/5000 train_loss:3.9107 train_time:130789ms step_avg:179.41ms\nstep:740/5000 train_loss:4.1247 train_time:130969ms step_avg:179.41ms\nstep:741/5000 train_loss:4.1714 train_time:131148ms step_avg:179.41ms\nstep:742/5000 train_loss:4.1344 train_time:131328ms step_avg:179.41ms\nstep:743/5000 train_loss:4.1085 train_time:131507ms step_avg:179.41ms\nstep:744/5000 train_loss:4.2130 train_time:131688ms step_avg:179.41ms\nstep:745/5000 train_loss:4.0956 train_time:131866ms step_avg:179.41ms\nstep:746/5000 train_loss:4.2102 train_time:132045ms step_avg:179.41ms\nstep:747/5000 train_loss:4.1694 train_time:132226ms step_avg:179.41ms\nstep:748/5000 train_loss:4.1430 train_time:132409ms step_avg:179.42ms\nstep:749/5000 train_loss:4.2512 train_time:132589ms step_avg:179.42ms\nstep:750/5000 train_loss:4.1228 train_time:132769ms step_avg:179.42ms\nstep:751/5000 train_loss:3.9743 train_time:132949ms step_avg:179.42ms\nstep:752/5000 train_loss:4.1191 train_time:133131ms step_avg:179.42ms\nstep:753/5000 train_loss:4.0581 train_time:133311ms step_avg:179.42ms\nstep:754/5000 train_loss:4.0587 train_time:133489ms step_avg:179.42ms\nstep:755/5000 train_loss:4.2115 train_time:133667ms step_avg:179.42ms\nstep:756/5000 train_loss:3.9431 train_time:133847ms step_avg:179.42ms\nstep:757/5000 train_loss:3.9110 train_time:134026ms step_avg:179.42ms\nstep:758/5000 train_loss:4.1673 train_time:134207ms step_avg:179.42ms\nstep:759/5000 train_loss:4.0729 train_time:134387ms step_avg:179.42ms\nstep:760/5000 train_loss:4.0592 train_time:134566ms step_avg:179.42ms\nstep:761/5000 train_loss:3.9873 train_time:134743ms step_avg:179.42ms\nstep:762/5000 train_loss:4.2431 train_time:135154ms step_avg:179.73ms\nstep:763/5000 train_loss:3.9588 train_time:135331ms step_avg:179.72ms\nstep:764/5000 train_loss:4.0573 train_time:135509ms step_avg:179.72ms\nstep:765/5000 train_loss:4.0446 train_time:135687ms step_avg:179.72ms\nstep:766/5000 train_loss:4.0431 train_time:135865ms step_avg:179.72ms\nstep:767/5000 train_loss:4.0550 train_time:136052ms step_avg:179.73ms\nstep:768/5000 train_loss:3.9942 train_time:136231ms step_avg:179.72ms\nstep:769/5000 train_loss:4.2144 train_time:136408ms step_avg:179.72ms\nstep:770/5000 train_loss:4.1112 train_time:136587ms step_avg:179.72ms\nstep:771/5000 train_loss:4.2772 train_time:136764ms step_avg:179.72ms\nstep:772/5000 train_loss:4.2121 train_time:136943ms step_avg:179.72ms\nstep:773/5000 train_loss:4.2111 train_time:137125ms step_avg:179.72ms\nstep:774/5000 train_loss:4.0867 train_time:137308ms step_avg:179.72ms\nstep:775/5000 train_loss:4.1517 train_time:137487ms step_avg:179.72ms\nstep:776/5000 train_loss:4.1354 train_time:137666ms step_avg:179.72ms\nstep:777/5000 train_loss:4.1120 train_time:137844ms step_avg:179.72ms\nstep:778/5000 train_loss:4.0422 train_time:138023ms step_avg:179.72ms\nstep:779/5000 train_loss:4.2672 train_time:138203ms step_avg:179.72ms\nstep:780/5000 train_loss:4.3362 train_time:138383ms step_avg:179.72ms\nstep:781/5000 train_loss:4.1073 train_time:138562ms step_avg:179.72ms\nstep:782/5000 train_loss:4.0418 train_time:138741ms step_avg:179.72ms\nstep:783/5000 train_loss:4.0953 train_time:138920ms step_avg:179.72ms\nstep:784/5000 train_loss:4.1520 train_time:139099ms step_avg:179.71ms\nstep:785/5000 train_loss:4.1109 train_time:139278ms step_avg:179.71ms\nstep:786/5000 train_loss:4.0800 train_time:139457ms step_avg:179.71ms\nstep:787/5000 train_loss:4.1093 train_time:139636ms step_avg:179.71ms\nstep:788/5000 train_loss:4.0064 train_time:139814ms step_avg:179.71ms\nstep:789/5000 train_loss:4.0531 train_time:139992ms step_avg:179.71ms\nstep:790/5000 train_loss:3.9877 train_time:140171ms step_avg:179.71ms\nstep:791/5000 train_loss:4.0205 train_time:140353ms step_avg:179.71ms\nstep:792/5000 train_loss:4.1453 train_time:140532ms step_avg:179.71ms\nstep:793/5000 train_loss:4.1533 train_time:140711ms step_avg:179.71ms\nstep:794/5000 train_loss:4.2200 train_time:140889ms step_avg:179.71ms\nstep:795/5000 train_loss:3.9950 train_time:141067ms step_avg:179.70ms\nstep:796/5000 train_loss:4.0017 train_time:141249ms step_avg:179.71ms\nstep:797/5000 train_loss:4.0682 train_time:141428ms step_avg:179.71ms\nstep:798/5000 train_loss:4.1415 train_time:141608ms step_avg:179.71ms\nstep:799/5000 train_loss:4.0617 train_time:141789ms step_avg:179.71ms\nstep:800/5000 train_loss:4.0854 train_time:141968ms step_avg:179.71ms\nstep:801/5000 train_loss:4.0772 train_time:142147ms step_avg:179.71ms\nstep:802/5000 train_loss:4.8842 train_time:142326ms step_avg:179.70ms\nstep:803/5000 train_loss:4.0618 train_time:142506ms step_avg:179.71ms\nstep:804/5000 train_loss:3.9662 train_time:142686ms step_avg:179.71ms\nstep:805/5000 train_loss:4.1515 train_time:142865ms step_avg:179.70ms\nstep:806/5000 train_loss:4.3547 train_time:143044ms step_avg:179.70ms\nstep:807/5000 train_loss:4.0803 train_time:143223ms step_avg:179.70ms\nstep:808/5000 train_loss:4.2071 train_time:143403ms step_avg:179.70ms\nstep:809/5000 train_loss:4.1381 train_time:143583ms step_avg:179.70ms\nstep:810/5000 train_loss:4.1089 train_time:143763ms step_avg:179.70ms\nstep:811/5000 train_loss:3.9644 train_time:143943ms step_avg:179.70ms\nstep:812/5000 train_loss:4.0348 train_time:144122ms step_avg:179.70ms\nstep:813/5000 train_loss:4.2230 train_time:144301ms step_avg:179.70ms\nstep:814/5000 train_loss:4.0317 train_time:144481ms step_avg:179.70ms\nstep:815/5000 train_loss:4.1775 train_time:144660ms step_avg:179.70ms\nstep:816/5000 train_loss:4.1569 train_time:144839ms step_avg:179.70ms\nstep:817/5000 train_loss:4.0226 train_time:145017ms step_avg:179.70ms\nstep:818/5000 train_loss:4.0897 train_time:145197ms step_avg:179.70ms\nstep:819/5000 train_loss:4.1060 train_time:145375ms step_avg:179.70ms\nstep:820/5000 train_loss:4.2252 train_time:145554ms step_avg:179.70ms\nstep:821/5000 train_loss:4.0643 train_time:145733ms step_avg:179.70ms\nstep:822/5000 train_loss:4.0528 train_time:145912ms step_avg:179.69ms\nstep:823/5000 train_loss:4.0834 train_time:146091ms step_avg:179.69ms\nstep:824/5000 train_loss:3.9149 train_time:146269ms step_avg:179.69ms\nstep:825/5000 train_loss:4.0145 train_time:146449ms step_avg:179.69ms\nstep:826/5000 train_loss:4.0498 train_time:146631ms step_avg:179.69ms\nstep:827/5000 train_loss:4.1565 train_time:146809ms step_avg:179.69ms\nstep:828/5000 train_loss:4.0627 train_time:146987ms step_avg:179.69ms\nstep:829/5000 train_loss:4.0881 train_time:147166ms step_avg:179.69ms\nstep:830/5000 train_loss:4.0046 train_time:147345ms step_avg:179.69ms\nstep:831/5000 train_loss:4.0886 train_time:147525ms step_avg:179.69ms\nstep:832/5000 train_loss:4.0686 train_time:147706ms step_avg:179.69ms\nstep:833/5000 train_loss:4.1772 train_time:147885ms step_avg:179.69ms\nstep:834/5000 train_loss:4.1720 train_time:148063ms step_avg:179.69ms\nstep:835/5000 train_loss:4.0135 train_time:148242ms step_avg:179.69ms\nstep:836/5000 train_loss:4.2167 train_time:148422ms step_avg:179.69ms\nstep:837/5000 train_loss:4.0197 train_time:148602ms step_avg:179.69ms\nstep:838/5000 train_loss:3.9927 train_time:148783ms step_avg:179.69ms\nstep:839/5000 train_loss:4.0084 train_time:148964ms step_avg:179.69ms\nstep:840/5000 train_loss:4.1563 train_time:149143ms step_avg:179.69ms\nstep:841/5000 train_loss:4.2246 train_time:149321ms step_avg:179.69ms\nstep:842/5000 train_loss:4.1050 train_time:149500ms step_avg:179.69ms\nstep:843/5000 train_loss:4.2187 train_time:149681ms step_avg:179.69ms\nstep:844/5000 train_loss:4.0397 train_time:149860ms step_avg:179.69ms\nstep:845/5000 train_loss:4.1014 train_time:150039ms step_avg:179.69ms\nstep:846/5000 train_loss:4.0959 train_time:150217ms step_avg:179.68ms\nstep:847/5000 train_loss:4.0340 train_time:150395ms step_avg:179.68ms\nstep:848/5000 train_loss:4.0693 train_time:150573ms step_avg:179.68ms\nstep:849/5000 train_loss:4.0631 train_time:150752ms step_avg:179.68ms\nstep:850/5000 train_loss:4.0448 train_time:150931ms step_avg:179.68ms\nstep:851/5000 train_loss:4.2049 train_time:151111ms step_avg:179.68ms\nstep:852/5000 train_loss:4.0216 train_time:151289ms step_avg:179.68ms\nstep:853/5000 train_loss:4.1085 train_time:151469ms step_avg:179.68ms\nstep:854/5000 train_loss:3.9994 train_time:151647ms step_avg:179.68ms\nstep:855/5000 train_loss:4.1624 train_time:151826ms step_avg:179.68ms\nstep:856/5000 train_loss:4.0883 train_time:152006ms step_avg:179.68ms\nstep:857/5000 train_loss:4.0389 train_time:152185ms step_avg:179.68ms\nstep:858/5000 train_loss:4.1236 train_time:152364ms step_avg:179.67ms\nstep:859/5000 train_loss:4.1710 train_time:152544ms step_avg:179.67ms\nstep:860/5000 train_loss:3.9200 train_time:152723ms step_avg:179.67ms\nstep:861/5000 train_loss:4.2418 train_time:152904ms step_avg:179.68ms\nstep:862/5000 train_loss:4.0655 train_time:153083ms step_avg:179.67ms\nstep:863/5000 train_loss:4.0644 train_time:153261ms step_avg:179.67ms\nstep:864/5000 train_loss:3.9787 train_time:153440ms step_avg:179.67ms\nstep:865/5000 train_loss:3.9985 train_time:153620ms step_avg:179.67ms\nstep:866/5000 train_loss:4.0566 train_time:153799ms step_avg:179.67ms\nstep:867/5000 train_loss:3.9778 train_time:153977ms step_avg:179.67ms\nstep:868/5000 train_loss:3.9837 train_time:154156ms step_avg:179.67ms\nstep:869/5000 train_loss:4.0876 train_time:154334ms step_avg:179.67ms\nstep:870/5000 train_loss:4.0635 train_time:154512ms step_avg:179.67ms\nstep:871/5000 train_loss:4.1089 train_time:154691ms step_avg:179.66ms\nstep:872/5000 train_loss:4.0603 train_time:154869ms step_avg:179.66ms\nstep:873/5000 train_loss:4.0444 train_time:155049ms step_avg:179.66ms\nstep:874/5000 train_loss:3.9945 train_time:155228ms step_avg:179.66ms\nstep:875/5000 train_loss:4.0841 train_time:155408ms step_avg:179.66ms\nstep:876/5000 train_loss:4.0453 train_time:155587ms step_avg:179.66ms\nstep:877/5000 train_loss:4.1143 train_time:155770ms step_avg:179.67ms\nstep:878/5000 train_loss:4.1053 train_time:155949ms step_avg:179.67ms\nstep:879/5000 train_loss:3.9804 train_time:156129ms step_avg:179.67ms\nstep:880/5000 train_loss:4.0653 train_time:156308ms step_avg:179.66ms\nstep:881/5000 train_loss:3.9988 train_time:156488ms step_avg:179.66ms\nstep:882/5000 train_loss:4.0185 train_time:156668ms step_avg:179.66ms\nstep:883/5000 train_loss:3.9801 train_time:156849ms step_avg:179.67ms\nstep:884/5000 train_loss:4.0858 train_time:157028ms step_avg:179.67ms\nstep:885/5000 train_loss:4.0370 train_time:157207ms step_avg:179.67ms\nstep:886/5000 train_loss:4.0971 train_time:157387ms step_avg:179.67ms\nstep:887/5000 train_loss:3.9577 train_time:157568ms step_avg:179.67ms\nstep:888/5000 train_loss:3.9959 train_time:157748ms step_avg:179.67ms\nstep:889/5000 train_loss:4.0132 train_time:157927ms step_avg:179.67ms\nstep:890/5000 train_loss:4.0194 train_time:158109ms step_avg:179.67ms\nstep:891/5000 train_loss:4.0247 train_time:158288ms step_avg:179.67ms\nstep:892/5000 train_loss:3.9699 train_time:158468ms step_avg:179.67ms\nstep:893/5000 train_loss:3.9799 train_time:158647ms step_avg:179.67ms\nstep:894/5000 train_loss:4.0305 train_time:158827ms step_avg:179.67ms\nstep:895/5000 train_loss:4.1767 train_time:159007ms step_avg:179.67ms\nstep:896/5000 train_loss:4.2814 train_time:159187ms step_avg:179.67ms\nstep:897/5000 train_loss:3.9355 train_time:159365ms step_avg:179.67ms\nstep:898/5000 train_loss:4.0091 train_time:159543ms step_avg:179.67ms\nstep:899/5000 train_loss:4.1581 train_time:159721ms step_avg:179.66ms\nstep:900/5000 train_loss:3.8973 train_time:159901ms step_avg:179.66ms\nstep:901/5000 train_loss:4.0033 train_time:160081ms step_avg:179.66ms\nstep:902/5000 train_loss:4.0309 train_time:160261ms step_avg:179.66ms\nstep:903/5000 train_loss:4.0388 train_time:160440ms step_avg:179.66ms\nstep:904/5000 train_loss:3.9674 train_time:160619ms step_avg:179.66ms\nstep:905/5000 train_loss:3.9706 train_time:160797ms step_avg:179.66ms\nstep:906/5000 train_loss:4.0235 train_time:160976ms step_avg:179.66ms\nstep:907/5000 train_loss:4.0802 train_time:161156ms step_avg:179.66ms\nstep:908/5000 train_loss:3.9669 train_time:161334ms step_avg:179.66ms\nstep:909/5000 train_loss:4.0693 train_time:161514ms step_avg:179.66ms\nstep:910/5000 train_loss:3.9487 train_time:161692ms step_avg:179.66ms\nstep:911/5000 train_loss:4.1309 train_time:161871ms step_avg:179.66ms\nstep:912/5000 train_loss:4.1277 train_time:162049ms step_avg:179.66ms\nstep:913/5000 train_loss:4.1318 train_time:162228ms step_avg:179.65ms\nstep:914/5000 train_loss:4.1668 train_time:162407ms step_avg:179.65ms\nstep:915/5000 train_loss:4.0179 train_time:162589ms step_avg:179.66ms\nstep:916/5000 train_loss:3.9191 train_time:162769ms step_avg:179.66ms\nstep:917/5000 train_loss:3.9973 train_time:162949ms step_avg:179.66ms\nstep:918/5000 train_loss:4.0418 train_time:163127ms step_avg:179.66ms\nstep:919/5000 train_loss:4.1344 train_time:163309ms step_avg:179.66ms\nstep:920/5000 train_loss:4.0407 train_time:163488ms step_avg:179.66ms\nstep:921/5000 train_loss:3.9744 train_time:163667ms step_avg:179.66ms\nstep:922/5000 train_loss:3.9298 train_time:163847ms step_avg:179.66ms\nstep:923/5000 train_loss:3.9051 train_time:164027ms step_avg:179.66ms\nstep:924/5000 train_loss:3.8966 train_time:164207ms step_avg:179.66ms\nstep:925/5000 train_loss:3.9599 train_time:164387ms step_avg:179.66ms\nstep:926/5000 train_loss:4.1375 train_time:164568ms step_avg:179.66ms\nstep:927/5000 train_loss:4.0343 train_time:164748ms step_avg:179.66ms\nstep:928/5000 train_loss:3.9687 train_time:164928ms step_avg:179.66ms\nstep:929/5000 train_loss:4.1608 train_time:165108ms step_avg:179.66ms\nstep:930/5000 train_loss:3.9744 train_time:165288ms step_avg:179.66ms\nstep:931/5000 train_loss:4.0104 train_time:165468ms step_avg:179.66ms\nstep:932/5000 train_loss:4.0913 train_time:165647ms step_avg:179.66ms\nstep:933/5000 train_loss:4.1355 train_time:165827ms step_avg:179.66ms\nstep:934/5000 train_loss:4.0092 train_time:166008ms step_avg:179.66ms\nstep:935/5000 train_loss:4.0068 train_time:166189ms step_avg:179.66ms\nstep:936/5000 train_loss:4.0152 train_time:166368ms step_avg:179.66ms\nstep:937/5000 train_loss:4.2104 train_time:166548ms step_avg:179.66ms\nstep:938/5000 train_loss:4.1715 train_time:166726ms step_avg:179.66ms\nstep:939/5000 train_loss:4.2223 train_time:166905ms step_avg:179.66ms\nstep:940/5000 train_loss:3.9085 train_time:167085ms step_avg:179.66ms\nstep:941/5000 train_loss:4.2426 train_time:167267ms step_avg:179.66ms\nstep:942/5000 train_loss:4.0624 train_time:167446ms step_avg:179.66ms\nstep:943/5000 train_loss:4.0242 train_time:167627ms step_avg:179.66ms\nstep:944/5000 train_loss:4.0736 train_time:167808ms step_avg:179.67ms\nstep:945/5000 train_loss:3.9357 train_time:167988ms step_avg:179.67ms\nstep:946/5000 train_loss:4.1613 train_time:168168ms step_avg:179.67ms\nstep:947/5000 train_loss:4.1759 train_time:168348ms step_avg:179.67ms\nstep:948/5000 train_loss:4.0687 train_time:168530ms step_avg:179.67ms\nstep:949/5000 train_loss:4.0723 train_time:168708ms step_avg:179.67ms\nstep:950/5000 train_loss:4.0940 train_time:168889ms step_avg:179.67ms\nstep:951/5000 train_loss:4.0223 train_time:169068ms step_avg:179.67ms\nstep:952/5000 train_loss:3.9380 train_time:169245ms step_avg:179.67ms\nstep:953/5000 train_loss:4.0447 train_time:169425ms step_avg:179.67ms\nstep:954/5000 train_loss:4.0935 train_time:169604ms step_avg:179.67ms\nstep:955/5000 train_loss:4.0383 train_time:169785ms step_avg:179.67ms\nstep:956/5000 train_loss:4.0769 train_time:169965ms step_avg:179.67ms\nstep:957/5000 train_loss:3.9333 train_time:170146ms step_avg:179.67ms\nstep:958/5000 train_loss:4.0679 train_time:170324ms step_avg:179.67ms\nstep:959/5000 train_loss:3.9614 train_time:170503ms step_avg:179.67ms\nstep:960/5000 train_loss:4.1133 train_time:170683ms step_avg:179.67ms\nstep:961/5000 train_loss:3.9530 train_time:170862ms step_avg:179.67ms\nstep:962/5000 train_loss:4.2715 train_time:171043ms step_avg:179.67ms\nstep:963/5000 train_loss:3.8560 train_time:171225ms step_avg:179.67ms\nstep:964/5000 train_loss:3.9615 train_time:171407ms step_avg:179.67ms\nstep:965/5000 train_loss:4.1275 train_time:171588ms step_avg:179.67ms\nstep:966/5000 train_loss:4.0854 train_time:171767ms step_avg:179.67ms\nstep:967/5000 train_loss:4.2513 train_time:171947ms step_avg:179.67ms\nstep:968/5000 train_loss:4.1601 train_time:172127ms step_avg:179.67ms\nstep:969/5000 train_loss:4.1099 train_time:172306ms step_avg:179.67ms\nstep:970/5000 train_loss:4.4303 train_time:172486ms step_avg:179.67ms\nstep:971/5000 train_loss:3.9910 train_time:172667ms step_avg:179.67ms\nstep:972/5000 train_loss:4.0190 train_time:172846ms step_avg:179.67ms\nstep:973/5000 train_loss:3.9497 train_time:173026ms step_avg:179.67ms\nstep:974/5000 train_loss:4.0359 train_time:173206ms step_avg:179.67ms\nstep:975/5000 train_loss:3.9176 train_time:173385ms step_avg:179.67ms\nstep:976/5000 train_loss:4.0758 train_time:173565ms step_avg:179.67ms\nstep:977/5000 train_loss:4.0881 train_time:173744ms step_avg:179.67ms\nstep:978/5000 train_loss:4.0538 train_time:173924ms step_avg:179.67ms\nstep:979/5000 train_loss:4.1645 train_time:174105ms step_avg:179.67ms\nstep:980/5000 train_loss:4.1415 train_time:174283ms step_avg:179.67ms\nstep:981/5000 train_loss:4.0362 train_time:174463ms step_avg:179.67ms\nstep:982/5000 train_loss:4.0911 train_time:174642ms step_avg:179.67ms\nstep:983/5000 train_loss:3.8006 train_time:174821ms step_avg:179.67ms\nstep:984/5000 train_loss:4.0643 train_time:175000ms step_avg:179.67ms\nstep:985/5000 train_loss:3.9180 train_time:175177ms step_avg:179.67ms\nstep:986/5000 train_loss:3.9971 train_time:175356ms step_avg:179.67ms\nstep:987/5000 train_loss:4.1858 train_time:175534ms step_avg:179.67ms\nstep:988/5000 train_loss:4.0042 train_time:175713ms step_avg:179.67ms\nstep:989/5000 train_loss:3.8935 train_time:175891ms step_avg:179.66ms\nstep:990/5000 train_loss:4.0261 train_time:176070ms step_avg:179.66ms\nstep:991/5000 train_loss:3.9790 train_time:176250ms step_avg:179.66ms\nstep:992/5000 train_loss:4.1279 train_time:176430ms step_avg:179.66ms\nstep:993/5000 train_loss:4.0005 train_time:176610ms step_avg:179.66ms\nstep:994/5000 train_loss:3.9487 train_time:176791ms step_avg:179.67ms\nstep:995/5000 train_loss:4.0910 train_time:176970ms step_avg:179.66ms\nstep:996/5000 train_loss:3.9235 train_time:177149ms step_avg:179.66ms\nstep:997/5000 train_loss:4.0987 train_time:177329ms step_avg:179.66ms\nstep:998/5000 train_loss:4.1070 train_time:177509ms step_avg:179.66ms\nstep:999/5000 train_loss:4.0469 train_time:177688ms step_avg:179.66ms\nstep:1000/5000 train_loss:3.9672 train_time:177869ms step_avg:179.67ms\nstep:1001/5000 train_loss:3.9987 train_time:178049ms step_avg:179.67ms\nstep:1002/5000 train_loss:3.9978 train_time:178229ms step_avg:179.67ms\nstep:1003/5000 train_loss:3.7828 train_time:178409ms step_avg:179.67ms\nstep:1004/5000 train_loss:4.0761 train_time:178588ms step_avg:179.67ms\nstep:1005/5000 train_loss:4.0885 train_time:178767ms step_avg:179.67ms\nstep:1006/5000 train_loss:3.9126 train_time:178947ms step_avg:179.67ms\nstep:1007/5000 train_loss:4.2301 train_time:179127ms step_avg:179.67ms\nstep:1008/5000 train_loss:4.1481 train_time:179309ms step_avg:179.67ms\nstep:1009/5000 train_loss:3.9371 train_time:179490ms step_avg:179.67ms\nstep:1010/5000 train_loss:3.9473 train_time:179669ms step_avg:179.67ms\nstep:1011/5000 train_loss:4.0761 train_time:179849ms step_avg:179.67ms\nstep:1012/5000 train_loss:3.9367 train_time:180029ms step_avg:179.67ms\nstep:1013/5000 train_loss:3.9379 train_time:180208ms step_avg:179.67ms\nstep:1014/5000 train_loss:4.0073 train_time:180389ms step_avg:179.67ms\nstep:1015/5000 train_loss:3.9688 train_time:180570ms step_avg:179.67ms\nstep:1016/5000 train_loss:3.8858 train_time:180751ms step_avg:179.67ms\nstep:1017/5000 train_loss:3.9799 train_time:180930ms step_avg:179.67ms\nstep:1018/5000 train_loss:3.9528 train_time:181111ms step_avg:179.67ms\nstep:1019/5000 train_loss:3.9060 train_time:181292ms step_avg:179.67ms\nstep:1020/5000 train_loss:4.0791 train_time:181470ms step_avg:179.67ms\nstep:1021/5000 train_loss:4.1476 train_time:181650ms step_avg:179.67ms\nstep:1022/5000 train_loss:3.8944 train_time:181828ms step_avg:179.67ms\nstep:1023/5000 train_loss:4.1404 train_time:182010ms step_avg:179.67ms\nstep:1024/5000 train_loss:3.9927 train_time:182189ms step_avg:179.67ms\nstep:1025/5000 train_loss:3.9992 train_time:182368ms step_avg:179.67ms\nstep:1026/5000 train_loss:3.9697 train_time:182551ms step_avg:179.68ms\nstep:1027/5000 train_loss:3.9202 train_time:182728ms step_avg:179.67ms\nstep:1028/5000 train_loss:3.9944 train_time:182908ms step_avg:179.67ms\nstep:1029/5000 train_loss:4.0625 train_time:183088ms step_avg:179.67ms\nstep:1030/5000 train_loss:4.0975 train_time:183267ms step_avg:179.67ms\nstep:1031/5000 train_loss:3.9723 train_time:183446ms step_avg:179.67ms\nstep:1032/5000 train_loss:3.9826 train_time:183625ms step_avg:179.67ms\nstep:1033/5000 train_loss:4.2459 train_time:183805ms step_avg:179.67ms\nstep:1034/5000 train_loss:3.8932 train_time:183986ms step_avg:179.67ms\nstep:1035/5000 train_loss:4.0209 train_time:184165ms step_avg:179.67ms\nstep:1036/5000 train_loss:4.0357 train_time:184345ms step_avg:179.67ms\nstep:1037/5000 train_loss:3.9908 train_time:184525ms step_avg:179.67ms\nstep:1038/5000 train_loss:4.0086 train_time:184705ms step_avg:179.67ms\nstep:1039/5000 train_loss:3.9233 train_time:184887ms step_avg:179.68ms\nstep:1040/5000 train_loss:4.0647 train_time:185067ms step_avg:179.68ms\nstep:1041/5000 train_loss:4.0441 train_time:185248ms step_avg:179.68ms\nstep:1042/5000 train_loss:3.9596 train_time:185427ms step_avg:179.68ms\nstep:1043/5000 train_loss:3.9510 train_time:185609ms step_avg:179.68ms\nstep:1044/5000 train_loss:4.0300 train_time:185788ms step_avg:179.68ms\nstep:1045/5000 train_loss:3.8763 train_time:185968ms step_avg:179.68ms\nstep:1046/5000 train_loss:3.9862 train_time:186147ms step_avg:179.68ms\nstep:1047/5000 train_loss:3.7407 train_time:186328ms step_avg:179.68ms\nstep:1048/5000 train_loss:4.0007 train_time:186507ms step_avg:179.68ms\nstep:1049/5000 train_loss:4.0803 train_time:186688ms step_avg:179.68ms\nstep:1050/5000 train_loss:4.0560 train_time:186868ms step_avg:179.68ms\nstep:1051/5000 train_loss:4.0167 train_time:187048ms step_avg:179.68ms\nstep:1052/5000 train_loss:3.9154 train_time:187229ms step_avg:179.68ms\nstep:1053/5000 train_loss:4.0081 train_time:187411ms step_avg:179.68ms\nstep:1054/5000 train_loss:4.1002 train_time:187592ms step_avg:179.69ms\nstep:1055/5000 train_loss:3.9775 train_time:187772ms step_avg:179.69ms\nstep:1056/5000 train_loss:3.9940 train_time:187951ms step_avg:179.69ms\nstep:1057/5000 train_loss:4.0460 train_time:188130ms step_avg:179.68ms\nstep:1058/5000 train_loss:4.4608 train_time:188309ms step_avg:179.68ms\nstep:1059/5000 train_loss:3.8887 train_time:188489ms step_avg:179.68ms\nstep:1060/5000 train_loss:3.8751 train_time:188670ms step_avg:179.69ms\nstep:1061/5000 train_loss:3.8028 train_time:188851ms step_avg:179.69ms\nstep:1062/5000 train_loss:3.9973 train_time:189030ms step_avg:179.69ms\nstep:1063/5000 train_loss:3.9295 train_time:189210ms step_avg:179.69ms\nstep:1064/5000 train_loss:3.9229 train_time:189389ms step_avg:179.69ms\nstep:1065/5000 train_loss:4.0244 train_time:189568ms step_avg:179.69ms\nstep:1066/5000 train_loss:4.1859 train_time:189748ms step_avg:179.69ms\nstep:1067/5000 train_loss:4.0545 train_time:189927ms step_avg:179.69ms\nstep:1068/5000 train_loss:3.9304 train_time:190107ms step_avg:179.69ms\nstep:1069/5000 train_loss:3.9070 train_time:190286ms step_avg:179.68ms\nstep:1070/5000 train_loss:3.9349 train_time:190464ms step_avg:179.68ms\nstep:1071/5000 train_loss:3.9999 train_time:190643ms step_avg:179.68ms\nstep:1072/5000 train_loss:3.9704 train_time:190824ms step_avg:179.68ms\nstep:1073/5000 train_loss:4.0285 train_time:191004ms step_avg:179.68ms\nstep:1074/5000 train_loss:3.9468 train_time:191183ms step_avg:179.68ms\nstep:1075/5000 train_loss:3.8836 train_time:191362ms step_avg:179.68ms\nstep:1076/5000 train_loss:3.8705 train_time:191541ms step_avg:179.68ms\nstep:1077/5000 train_loss:3.8958 train_time:191720ms step_avg:179.68ms\nstep:1078/5000 train_loss:4.0600 train_time:191898ms step_avg:179.68ms\nstep:1079/5000 train_loss:3.9436 train_time:192076ms step_avg:179.68ms\nstep:1080/5000 train_loss:4.1163 train_time:192254ms step_avg:179.68ms\nstep:1081/5000 train_loss:3.8141 train_time:192434ms step_avg:179.68ms\nstep:1082/5000 train_loss:4.1148 train_time:192613ms step_avg:179.68ms\nstep:1083/5000 train_loss:3.9940 train_time:192793ms step_avg:179.68ms\nstep:1084/5000 train_loss:3.9843 train_time:192972ms step_avg:179.68ms\nstep:1085/5000 train_loss:4.1272 train_time:193151ms step_avg:179.68ms\nstep:1086/5000 train_loss:3.9823 train_time:193330ms step_avg:179.68ms\nstep:1087/5000 train_loss:3.9416 train_time:193510ms step_avg:179.67ms\nstep:1088/5000 train_loss:3.9290 train_time:193688ms step_avg:179.67ms\nstep:1089/5000 train_loss:3.9643 train_time:193868ms step_avg:179.67ms\nstep:1090/5000 train_loss:3.9468 train_time:194047ms step_avg:179.67ms\nstep:1091/5000 train_loss:3.9319 train_time:194228ms step_avg:179.67ms\nstep:1092/5000 train_loss:3.9676 train_time:194407ms step_avg:179.67ms\nstep:1093/5000 train_loss:3.9209 train_time:194586ms step_avg:179.67ms\nstep:1094/5000 train_loss:3.9619 train_time:194766ms step_avg:179.67ms\nstep:1095/5000 train_loss:4.0111 train_time:194945ms step_avg:179.67ms\nstep:1096/5000 train_loss:4.0421 train_time:195127ms step_avg:179.68ms\nstep:1097/5000 train_loss:4.0579 train_time:195309ms step_avg:179.68ms\nstep:1098/5000 train_loss:3.9248 train_time:195488ms step_avg:179.68ms\nstep:1099/5000 train_loss:3.8833 train_time:195668ms step_avg:179.68ms\nstep:1100/5000 train_loss:4.0801 train_time:195848ms step_avg:179.68ms\nstep:1101/5000 train_loss:4.0571 train_time:196028ms step_avg:179.68ms\nstep:1102/5000 train_loss:4.0306 train_time:196208ms step_avg:179.68ms\nstep:1103/5000 train_loss:3.9817 train_time:196391ms step_avg:179.68ms\nstep:1104/5000 train_loss:4.2516 train_time:196569ms step_avg:179.68ms\nstep:1105/5000 train_loss:3.8980 train_time:196748ms step_avg:179.68ms\nstep:1106/5000 train_loss:4.0578 train_time:196927ms step_avg:179.68ms\nstep:1107/5000 train_loss:4.0892 train_time:197107ms step_avg:179.68ms\nstep:1108/5000 train_loss:4.0533 train_time:197287ms step_avg:179.68ms\nstep:1109/5000 train_loss:3.9431 train_time:197467ms step_avg:179.68ms\nstep:1110/5000 train_loss:3.8534 train_time:197646ms step_avg:179.68ms\nstep:1111/5000 train_loss:3.9518 train_time:197825ms step_avg:179.68ms\nstep:1112/5000 train_loss:4.0961 train_time:198004ms step_avg:179.68ms\nstep:1113/5000 train_loss:3.9325 train_time:198183ms step_avg:179.68ms\nstep:1114/5000 train_loss:4.1616 train_time:198363ms step_avg:179.68ms\nstep:1115/5000 train_loss:3.9360 train_time:198543ms step_avg:179.68ms\nstep:1116/5000 train_loss:4.0124 train_time:198723ms step_avg:179.68ms\nstep:1117/5000 train_loss:3.7555 train_time:198903ms step_avg:179.68ms\nstep:1118/5000 train_loss:4.0085 train_time:199083ms step_avg:179.68ms\nstep:1119/5000 train_loss:4.0451 train_time:199262ms step_avg:179.68ms\nstep:1120/5000 train_loss:4.1755 train_time:199441ms step_avg:179.68ms\nstep:1121/5000 train_loss:4.0568 train_time:199622ms step_avg:179.68ms\nstep:1122/5000 train_loss:4.0180 train_time:199802ms step_avg:179.68ms\nstep:1123/5000 train_loss:3.9180 train_time:199982ms step_avg:179.68ms\nstep:1124/5000 train_loss:3.9850 train_time:200162ms step_avg:179.68ms\nstep:1125/5000 train_loss:3.9053 train_time:200341ms step_avg:179.68ms\nstep:1126/5000 train_loss:3.9695 train_time:200521ms step_avg:179.68ms\nstep:1127/5000 train_loss:4.0221 train_time:200700ms step_avg:179.68ms\nstep:1128/5000 train_loss:3.9942 train_time:200880ms step_avg:179.68ms\nstep:1129/5000 train_loss:4.0532 train_time:201059ms step_avg:179.68ms\nstep:1130/5000 train_loss:4.1691 train_time:201237ms step_avg:179.68ms\nstep:1131/5000 train_loss:3.8900 train_time:201416ms step_avg:179.68ms\nstep:1132/5000 train_loss:3.9298 train_time:201594ms step_avg:179.67ms\nstep:1133/5000 train_loss:3.8948 train_time:201773ms step_avg:179.67ms\nstep:1134/5000 train_loss:3.9265 train_time:201952ms step_avg:179.67ms\nstep:1135/5000 train_loss:4.0107 train_time:202135ms step_avg:179.68ms\nstep:1136/5000 train_loss:4.0747 train_time:202313ms step_avg:179.67ms\nstep:1137/5000 train_loss:3.8662 train_time:202491ms step_avg:179.67ms\nstep:1138/5000 train_loss:3.8941 train_time:202669ms step_avg:179.67ms\nstep:1139/5000 train_loss:4.0526 train_time:202849ms step_avg:179.67ms\nstep:1140/5000 train_loss:4.0782 train_time:203029ms step_avg:179.67ms\nstep:1141/5000 train_loss:4.0594 train_time:203209ms step_avg:179.67ms\nstep:1142/5000 train_loss:4.0758 train_time:203389ms step_avg:179.67ms\nstep:1143/5000 train_loss:4.0621 train_time:203568ms step_avg:179.67ms\nstep:1144/5000 train_loss:4.1149 train_time:203746ms step_avg:179.67ms\nstep:1145/5000 train_loss:3.9545 train_time:203927ms step_avg:179.67ms\nstep:1146/5000 train_loss:4.0037 train_time:204110ms step_avg:179.67ms\nstep:1147/5000 train_loss:3.9418 train_time:204289ms step_avg:179.67ms\nstep:1148/5000 train_loss:3.9445 train_time:204469ms step_avg:179.67ms\nstep:1149/5000 train_loss:3.9947 train_time:204648ms step_avg:179.67ms\nstep:1150/5000 train_loss:4.0328 train_time:204827ms step_avg:179.67ms\nstep:1151/5000 train_loss:3.8826 train_time:205006ms step_avg:179.67ms\nstep:1152/5000 train_loss:4.0404 train_time:205187ms step_avg:179.67ms\nstep:1153/5000 train_loss:3.9317 train_time:205366ms step_avg:179.67ms\nstep:1154/5000 train_loss:4.0751 train_time:205545ms step_avg:179.67ms\nstep:1155/5000 train_loss:3.9977 train_time:205725ms step_avg:179.67ms\nstep:1156/5000 train_loss:3.8881 train_time:205906ms step_avg:179.67ms\nstep:1157/5000 train_loss:3.9404 train_time:206086ms step_avg:179.67ms\nstep:1158/5000 train_loss:4.0610 train_time:206266ms step_avg:179.67ms\nstep:1159/5000 train_loss:3.9403 train_time:206445ms step_avg:179.67ms\nstep:1160/5000 train_loss:3.8862 train_time:206626ms step_avg:179.67ms\nstep:1161/5000 train_loss:4.1069 train_time:206805ms step_avg:179.67ms\nstep:1162/5000 train_loss:3.9032 train_time:206985ms step_avg:179.67ms\nstep:1163/5000 train_loss:3.9120 train_time:207164ms step_avg:179.67ms\nstep:1164/5000 train_loss:3.9707 train_time:207345ms step_avg:179.67ms\nstep:1165/5000 train_loss:4.0066 train_time:207523ms step_avg:179.67ms\nstep:1166/5000 train_loss:3.8866 train_time:207703ms step_avg:179.67ms\nstep:1167/5000 train_loss:3.9312 train_time:207883ms step_avg:179.67ms\nstep:1168/5000 train_loss:3.8849 train_time:208062ms step_avg:179.67ms\nstep:1169/5000 train_loss:3.9810 train_time:208241ms step_avg:179.67ms\nstep:1170/5000 train_loss:4.0735 train_time:208421ms step_avg:179.67ms\nstep:1171/5000 train_loss:4.0467 train_time:208600ms step_avg:179.67ms\nstep:1172/5000 train_loss:3.9312 train_time:208779ms step_avg:179.67ms\nstep:1173/5000 train_loss:3.9859 train_time:208957ms step_avg:179.67ms\nstep:1174/5000 train_loss:3.8723 train_time:209136ms step_avg:179.67ms\nstep:1175/5000 train_loss:3.8646 train_time:209316ms step_avg:179.67ms\nstep:1176/5000 train_loss:3.9727 train_time:209494ms step_avg:179.67ms\nstep:1177/5000 train_loss:4.2535 train_time:209672ms step_avg:179.67ms\nstep:1178/5000 train_loss:4.1549 train_time:209852ms step_avg:179.67ms\nstep:1179/5000 train_loss:4.2116 train_time:210031ms step_avg:179.67ms\nstep:1180/5000 train_loss:3.8598 train_time:210210ms step_avg:179.67ms\nstep:1181/5000 train_loss:3.7423 train_time:210390ms step_avg:179.67ms\nstep:1182/5000 train_loss:4.0502 train_time:210569ms step_avg:179.67ms\nstep:1183/5000 train_loss:4.1207 train_time:210748ms step_avg:179.67ms\nstep:1184/5000 train_loss:3.8832 train_time:210927ms step_avg:179.67ms\nstep:1185/5000 train_loss:3.9255 train_time:211108ms step_avg:179.67ms\nstep:1186/5000 train_loss:3.8763 train_time:211287ms step_avg:179.67ms\nstep:1187/5000 train_loss:4.0079 train_time:211469ms step_avg:179.67ms\nstep:1188/5000 train_loss:3.8859 train_time:211648ms step_avg:179.67ms\nstep:1189/5000 train_loss:3.9198 train_time:211828ms step_avg:179.67ms\nstep:1190/5000 train_loss:3.9961 train_time:212008ms step_avg:179.67ms\nstep:1191/5000 train_loss:3.9626 train_time:212188ms step_avg:179.67ms\nstep:1192/5000 train_loss:4.0034 train_time:212368ms step_avg:179.67ms\nstep:1193/5000 train_loss:4.0798 train_time:212550ms step_avg:179.67ms\nstep:1194/5000 train_loss:3.9024 train_time:212731ms step_avg:179.67ms\nstep:1195/5000 train_loss:3.9324 train_time:212910ms step_avg:179.67ms\nstep:1196/5000 train_loss:3.8474 train_time:213089ms step_avg:179.67ms\nstep:1197/5000 train_loss:3.9272 train_time:213268ms step_avg:179.67ms\nstep:1198/5000 train_loss:4.0920 train_time:213449ms step_avg:179.67ms\nstep:1199/5000 train_loss:3.8844 train_time:213629ms step_avg:179.67ms\nstep:1200/5000 train_loss:3.9897 train_time:213809ms step_avg:179.67ms\nstep:1201/5000 train_loss:3.9732 train_time:213989ms step_avg:179.67ms\nstep:1202/5000 train_loss:3.8876 train_time:214168ms step_avg:179.67ms\nstep:1203/5000 train_loss:4.0243 train_time:214347ms step_avg:179.67ms\nstep:1204/5000 train_loss:4.0023 train_time:214527ms step_avg:179.67ms\nstep:1205/5000 train_loss:3.9119 train_time:214706ms step_avg:179.67ms\nstep:1206/5000 train_loss:3.9848 train_time:214885ms step_avg:179.67ms\nstep:1207/5000 train_loss:4.0309 train_time:215065ms step_avg:179.67ms\nstep:1208/5000 train_loss:3.9545 train_time:215243ms step_avg:179.67ms\nstep:1209/5000 train_loss:3.9879 train_time:215422ms step_avg:179.67ms\nstep:1210/5000 train_loss:3.9342 train_time:215602ms step_avg:179.67ms\nstep:1211/5000 train_loss:3.9581 train_time:215782ms step_avg:179.67ms\nstep:1212/5000 train_loss:4.0159 train_time:215961ms step_avg:179.67ms\nstep:1213/5000 train_loss:3.9732 train_time:216139ms step_avg:179.67ms\nstep:1214/5000 train_loss:3.9324 train_time:216319ms step_avg:179.67ms\nstep:1215/5000 train_loss:3.9235 train_time:216498ms step_avg:179.67ms\nstep:1216/5000 train_loss:3.9860 train_time:216677ms step_avg:179.67ms\nstep:1217/5000 train_loss:3.9296 train_time:216856ms step_avg:179.66ms\nstep:1218/5000 train_loss:3.9337 train_time:217034ms step_avg:179.66ms\nstep:1219/5000 train_loss:3.9367 train_time:217213ms step_avg:179.66ms\nstep:1220/5000 train_loss:4.4733 train_time:217391ms step_avg:179.66ms\nstep:1221/5000 train_loss:3.8180 train_time:217569ms step_avg:179.66ms\nstep:1222/5000 train_loss:4.0403 train_time:217749ms step_avg:179.66ms\nstep:1223/5000 train_loss:3.7896 train_time:217928ms step_avg:179.66ms\nstep:1224/5000 train_loss:3.9630 train_time:218110ms step_avg:179.66ms\nstep:1225/5000 train_loss:3.8948 train_time:218289ms step_avg:179.66ms\nstep:1226/5000 train_loss:3.9798 train_time:218469ms step_avg:179.66ms\nstep:1227/5000 train_loss:4.0314 train_time:218647ms step_avg:179.66ms\nstep:1228/5000 train_loss:3.8586 train_time:218826ms step_avg:179.66ms\nstep:1229/5000 train_loss:4.0270 train_time:219006ms step_avg:179.66ms\nstep:1230/5000 train_loss:3.8947 train_time:219184ms step_avg:179.66ms\nstep:1231/5000 train_loss:3.9128 train_time:219364ms step_avg:179.66ms\nstep:1232/5000 train_loss:4.0194 train_time:219545ms step_avg:179.66ms\nstep:1233/5000 train_loss:3.8784 train_time:219723ms step_avg:179.66ms\nstep:1234/5000 train_loss:4.0602 train_time:219903ms step_avg:179.66ms\nstep:1235/5000 train_loss:3.8639 train_time:220082ms step_avg:179.66ms\nstep:1236/5000 train_loss:3.8938 train_time:220262ms step_avg:179.66ms\nstep:1237/5000 train_loss:3.8390 train_time:220442ms step_avg:179.66ms\nstep:1238/5000 train_loss:3.9158 train_time:220622ms step_avg:179.66ms\nstep:1239/5000 train_loss:4.1391 train_time:220801ms step_avg:179.66ms\nstep:1240/5000 train_loss:4.1020 train_time:220981ms step_avg:179.66ms\nstep:1241/5000 train_loss:4.0008 train_time:221161ms step_avg:179.66ms\nstep:1242/5000 train_loss:3.9883 train_time:221340ms step_avg:179.66ms\nstep:1243/5000 train_loss:4.0620 train_time:221521ms step_avg:179.66ms\nstep:1244/5000 train_loss:3.9595 train_time:221701ms step_avg:179.66ms\nstep:1245/5000 train_loss:4.0331 train_time:221881ms step_avg:179.66ms\nstep:1246/5000 train_loss:3.9784 train_time:222061ms step_avg:179.66ms\nstep:1247/5000 train_loss:4.0141 train_time:222240ms step_avg:179.66ms\nstep:1248/5000 train_loss:3.8971 train_time:222420ms step_avg:179.66ms\nstep:1249/5000 train_loss:3.9879 train_time:222600ms step_avg:179.66ms\nstep:1250/5000 train_loss:3.8856 train_time:222780ms step_avg:179.66ms\nstep:1251/5000 train_loss:3.9490 train_time:222959ms step_avg:179.66ms\nstep:1252/5000 train_loss:3.9832 train_time:223139ms step_avg:179.66ms\nstep:1253/5000 train_loss:4.0046 train_time:223317ms step_avg:179.66ms\nstep:1254/5000 train_loss:3.9673 train_time:223496ms step_avg:179.66ms\nstep:1255/5000 train_loss:4.0837 train_time:223675ms step_avg:179.66ms\nstep:1256/5000 train_loss:4.1182 train_time:223854ms step_avg:179.66ms\nstep:1257/5000 train_loss:3.8753 train_time:224032ms step_avg:179.66ms\nstep:1258/5000 train_loss:3.9660 train_time:224212ms step_avg:179.66ms\nstep:1259/5000 train_loss:3.9619 train_time:224391ms step_avg:179.66ms\nstep:1260/5000 train_loss:3.9827 train_time:224571ms step_avg:179.66ms\nstep:1261/5000 train_loss:3.9386 train_time:224749ms step_avg:179.66ms\nstep:1262/5000 train_loss:3.6965 train_time:224930ms step_avg:179.66ms\nstep:1263/5000 train_loss:3.9330 train_time:225109ms step_avg:179.66ms\nstep:1264/5000 train_loss:3.8214 train_time:225290ms step_avg:179.66ms\nstep:1265/5000 train_loss:3.9716 train_time:225469ms step_avg:179.66ms\nstep:1266/5000 train_loss:3.9568 train_time:225648ms step_avg:179.66ms\nstep:1267/5000 train_loss:3.9530 train_time:225827ms step_avg:179.66ms\nstep:1268/5000 train_loss:3.9133 train_time:226006ms step_avg:179.66ms\nstep:1269/5000 train_loss:3.8883 train_time:226188ms step_avg:179.66ms\nstep:1270/5000 train_loss:3.9404 train_time:226368ms step_avg:179.66ms\nstep:1271/5000 train_loss:3.9216 train_time:226547ms step_avg:179.66ms\nstep:1272/5000 train_loss:3.9410 train_time:226727ms step_avg:179.66ms\nstep:1273/5000 train_loss:3.7764 train_time:226907ms step_avg:179.66ms\nstep:1274/5000 train_loss:3.8568 train_time:227087ms step_avg:179.66ms\nstep:1275/5000 train_loss:3.8713 train_time:227266ms step_avg:179.66ms\nstep:1276/5000 train_loss:3.9130 train_time:227448ms step_avg:179.66ms\nstep:1277/5000 train_loss:4.1234 train_time:227627ms step_avg:179.66ms\nstep:1278/5000 train_loss:3.8896 train_time:227807ms step_avg:179.66ms\nstep:1279/5000 train_loss:3.9126 train_time:227988ms step_avg:179.66ms\nstep:1280/5000 train_loss:4.0403 train_time:228169ms step_avg:179.66ms\nstep:1281/5000 train_loss:3.9896 train_time:228348ms step_avg:179.66ms\nstep:1282/5000 train_loss:3.9318 train_time:228528ms step_avg:179.66ms\nstep:1283/5000 train_loss:3.9816 train_time:228708ms step_avg:179.66ms\nstep:1284/5000 train_loss:3.9848 train_time:228889ms step_avg:179.66ms\nstep:1285/5000 train_loss:3.8092 train_time:229068ms step_avg:179.66ms\nstep:1286/5000 train_loss:3.9582 train_time:229247ms step_avg:179.66ms\nstep:1287/5000 train_loss:4.0505 train_time:229427ms step_avg:179.66ms\nstep:1288/5000 train_loss:3.9640 train_time:229608ms step_avg:179.66ms\nstep:1289/5000 train_loss:3.9994 train_time:229790ms step_avg:179.66ms\nstep:1290/5000 train_loss:3.8260 train_time:229967ms step_avg:179.66ms\nstep:1291/5000 train_loss:3.9577 train_time:230150ms step_avg:179.66ms\nstep:1292/5000 train_loss:4.0438 train_time:230330ms step_avg:179.66ms\nstep:1293/5000 train_loss:4.0567 train_time:230510ms step_avg:179.67ms\nstep:1294/5000 train_loss:3.9088 train_time:230691ms step_avg:179.67ms\nstep:1295/5000 train_loss:3.9927 train_time:230870ms step_avg:179.67ms\nstep:1296/5000 train_loss:3.9952 train_time:231050ms step_avg:179.67ms\nstep:1297/5000 train_loss:3.8858 train_time:231231ms step_avg:179.67ms\nstep:1298/5000 train_loss:3.9611 train_time:231410ms step_avg:179.67ms\nstep:1299/5000 train_loss:3.8462 train_time:231590ms step_avg:179.67ms\nstep:1300/5000 train_loss:4.0531 train_time:231771ms step_avg:179.67ms\nstep:1301/5000 train_loss:4.0147 train_time:231950ms step_avg:179.67ms\nstep:1302/5000 train_loss:3.9659 train_time:232130ms step_avg:179.67ms\nstep:1303/5000 train_loss:3.9917 train_time:232311ms step_avg:179.67ms\nstep:1304/5000 train_loss:3.9267 train_time:232490ms step_avg:179.67ms\nstep:1305/5000 train_loss:4.0337 train_time:232668ms step_avg:179.67ms\nstep:1306/5000 train_loss:3.9341 train_time:232848ms step_avg:179.67ms\nstep:1307/5000 train_loss:4.0056 train_time:233029ms step_avg:179.67ms\nstep:1308/5000 train_loss:4.4225 train_time:233208ms step_avg:179.67ms\nstep:1309/5000 train_loss:3.9227 train_time:233389ms step_avg:179.67ms\nstep:1310/5000 train_loss:4.1827 train_time:233569ms step_avg:179.67ms\nstep:1311/5000 train_loss:4.1521 train_time:233750ms step_avg:179.67ms\nstep:1312/5000 train_loss:4.1114 train_time:233930ms step_avg:179.67ms\nstep:1313/5000 train_loss:3.9618 train_time:234110ms step_avg:179.67ms\nstep:1314/5000 train_loss:3.9766 train_time:234289ms step_avg:179.67ms\nstep:1315/5000 train_loss:3.9432 train_time:234469ms step_avg:179.67ms\nstep:1316/5000 train_loss:3.8646 train_time:234648ms step_avg:179.67ms\nstep:1317/5000 train_loss:4.0655 train_time:234828ms step_avg:179.67ms\nstep:1318/5000 train_loss:4.0170 train_time:235008ms step_avg:179.67ms\nstep:1319/5000 train_loss:4.2026 train_time:235187ms step_avg:179.67ms\nstep:1320/5000 train_loss:3.7885 train_time:235368ms step_avg:179.67ms\nstep:1321/5000 train_loss:3.9990 train_time:235547ms step_avg:179.67ms\nstep:1322/5000 train_loss:3.8842 train_time:235726ms step_avg:179.67ms\nstep:1323/5000 train_loss:3.8740 train_time:235907ms step_avg:179.67ms\nstep:1324/5000 train_loss:4.0179 train_time:236088ms step_avg:179.67ms\nstep:1325/5000 train_loss:3.8889 train_time:236269ms step_avg:179.67ms\nstep:1326/5000 train_loss:4.4610 train_time:236449ms step_avg:179.67ms\nstep:1327/5000 train_loss:3.8889 train_time:236628ms step_avg:179.67ms\nstep:1328/5000 train_loss:3.9462 train_time:236808ms step_avg:179.67ms\nstep:1329/5000 train_loss:3.8069 train_time:236988ms step_avg:179.67ms\nstep:1330/5000 train_loss:3.8759 train_time:237168ms step_avg:179.67ms\nstep:1331/5000 train_loss:3.9172 train_time:237347ms step_avg:179.67ms\nstep:1332/5000 train_loss:3.9115 train_time:237526ms step_avg:179.67ms\nstep:1333/5000 train_loss:3.8587 train_time:237707ms step_avg:179.67ms\nstep:1334/5000 train_loss:3.7920 train_time:237885ms step_avg:179.67ms\nstep:1335/5000 train_loss:3.8248 train_time:238064ms step_avg:179.67ms\nstep:1336/5000 train_loss:3.8803 train_time:238243ms step_avg:179.67ms\nstep:1337/5000 train_loss:3.8299 train_time:238423ms step_avg:179.67ms\nstep:1338/5000 train_loss:3.9461 train_time:238602ms step_avg:179.67ms\nstep:1339/5000 train_loss:4.0331 train_time:238783ms step_avg:179.67ms\nstep:1340/5000 train_loss:4.0593 train_time:238962ms step_avg:179.67ms\nstep:1341/5000 train_loss:3.9024 train_time:239143ms step_avg:179.67ms\nstep:1342/5000 train_loss:3.9670 train_time:239325ms step_avg:179.67ms\nstep:1343/5000 train_loss:3.9544 train_time:239509ms step_avg:179.68ms\nstep:1344/5000 train_loss:4.0071 train_time:239689ms step_avg:179.68ms\nstep:1345/5000 train_loss:3.8904 train_time:239870ms step_avg:179.68ms\nstep:1346/5000 train_loss:3.9667 train_time:240051ms step_avg:179.68ms\nstep:1347/5000 train_loss:3.8518 train_time:240231ms step_avg:179.68ms\nstep:1348/5000 train_loss:4.4732 train_time:240410ms step_avg:179.68ms\nstep:1349/5000 train_loss:4.9337 train_time:240590ms step_avg:179.68ms\nstep:1350/5000 train_loss:3.9563 train_time:240770ms step_avg:179.68ms\nstep:1351/5000 train_loss:3.9841 train_time:240950ms step_avg:179.68ms\nstep:1352/5000 train_loss:3.9838 train_time:241129ms step_avg:179.68ms\nstep:1353/5000 train_loss:3.9365 train_time:241310ms step_avg:179.68ms\nstep:1354/5000 train_loss:3.9978 train_time:241489ms step_avg:179.68ms\nstep:1355/5000 train_loss:4.0315 train_time:241668ms step_avg:179.68ms\nstep:1356/5000 train_loss:3.9229 train_time:241847ms step_avg:179.68ms\nstep:1357/5000 train_loss:3.8688 train_time:242027ms step_avg:179.68ms\nstep:1358/5000 train_loss:3.8679 train_time:242207ms step_avg:179.68ms\nstep:1359/5000 train_loss:3.8644 train_time:242388ms step_avg:179.68ms\nstep:1360/5000 train_loss:3.9789 train_time:242567ms step_avg:179.68ms\nstep:1361/5000 train_loss:3.9361 train_time:242746ms step_avg:179.68ms\nstep:1362/5000 train_loss:3.8547 train_time:242928ms step_avg:179.68ms\nstep:1363/5000 train_loss:3.8946 train_time:243107ms step_avg:179.68ms\nstep:1364/5000 train_loss:3.9013 train_time:243286ms step_avg:179.68ms\nstep:1365/5000 train_loss:3.9367 train_time:243466ms step_avg:179.68ms\nstep:1366/5000 train_loss:3.8805 train_time:243645ms step_avg:179.68ms\nstep:1367/5000 train_loss:3.9353 train_time:243827ms step_avg:179.68ms\nstep:1368/5000 train_loss:3.8546 train_time:244007ms step_avg:179.68ms\nstep:1369/5000 train_loss:3.9144 train_time:244187ms step_avg:179.68ms\nstep:1370/5000 train_loss:3.9003 train_time:244367ms step_avg:179.68ms\nstep:1371/5000 train_loss:3.9511 train_time:244546ms step_avg:179.68ms\nstep:1372/5000 train_loss:3.8845 train_time:244725ms step_avg:179.68ms\nstep:1373/5000 train_loss:3.9479 train_time:244906ms step_avg:179.68ms\nstep:1374/5000 train_loss:3.9963 train_time:245085ms step_avg:179.68ms\nstep:1375/5000 train_loss:3.9561 train_time:245265ms step_avg:179.68ms\nstep:1376/5000 train_loss:4.0358 train_time:245444ms step_avg:179.68ms\nstep:1377/5000 train_loss:3.7062 train_time:245623ms step_avg:179.68ms\nstep:1378/5000 train_loss:3.9299 train_time:245803ms step_avg:179.68ms\nstep:1379/5000 train_loss:4.0244 train_time:245983ms step_avg:179.68ms\nstep:1380/5000 train_loss:3.8902 train_time:246162ms step_avg:179.68ms\nstep:1381/5000 train_loss:3.9267 train_time:246341ms step_avg:179.68ms\nstep:1382/5000 train_loss:3.8974 train_time:246521ms step_avg:179.68ms\nstep:1383/5000 train_loss:3.8226 train_time:246700ms step_avg:179.68ms\nstep:1384/5000 train_loss:3.8143 train_time:246879ms step_avg:179.68ms\nstep:1385/5000 train_loss:3.9110 train_time:247057ms step_avg:179.68ms\nstep:1386/5000 train_loss:3.9513 train_time:247236ms step_avg:179.68ms\nstep:1387/5000 train_loss:3.9094 train_time:247416ms step_avg:179.68ms\nstep:1388/5000 train_loss:3.8473 train_time:247594ms step_avg:179.68ms\nstep:1389/5000 train_loss:3.9260 train_time:247773ms step_avg:179.68ms\nstep:1390/5000 train_loss:3.8667 train_time:247952ms step_avg:179.68ms\nstep:1391/5000 train_loss:3.8980 train_time:248131ms step_avg:179.68ms\nstep:1392/5000 train_loss:3.9053 train_time:248310ms step_avg:179.67ms\nstep:1393/5000 train_loss:4.0519 train_time:248489ms step_avg:179.67ms\nstep:1394/5000 train_loss:3.8747 train_time:248669ms step_avg:179.67ms\nstep:1395/5000 train_loss:3.8934 train_time:248848ms step_avg:179.67ms\nstep:1396/5000 train_loss:3.8645 train_time:249027ms step_avg:179.67ms\nstep:1397/5000 train_loss:3.7415 train_time:249207ms step_avg:179.67ms\nstep:1398/5000 train_loss:3.9452 train_time:249387ms step_avg:179.67ms\nstep:1399/5000 train_loss:3.8485 train_time:249568ms step_avg:179.67ms\nstep:1400/5000 train_loss:3.6019 train_time:249747ms step_avg:179.67ms\nstep:1401/5000 train_loss:3.8480 train_time:249929ms step_avg:179.68ms\nstep:1402/5000 train_loss:4.1688 train_time:250108ms step_avg:179.68ms\nstep:1403/5000 train_loss:3.8917 train_time:250288ms step_avg:179.68ms\nstep:1404/5000 train_loss:3.8665 train_time:250468ms step_avg:179.68ms\nstep:1405/5000 train_loss:3.8761 train_time:250648ms step_avg:179.68ms\nstep:1406/5000 train_loss:3.8168 train_time:250829ms step_avg:179.68ms\nstep:1407/5000 train_loss:3.9352 train_time:251008ms step_avg:179.68ms\nstep:1408/5000 train_loss:4.2230 train_time:251187ms step_avg:179.68ms\nstep:1409/5000 train_loss:3.8512 train_time:251366ms step_avg:179.68ms\nstep:1410/5000 train_loss:3.8707 train_time:251546ms step_avg:179.68ms\nstep:1411/5000 train_loss:3.9408 train_time:251725ms step_avg:179.68ms\nstep:1412/5000 train_loss:3.7075 train_time:251907ms step_avg:179.68ms\nstep:1413/5000 train_loss:3.9532 train_time:252086ms step_avg:179.68ms\nstep:1414/5000 train_loss:3.8742 train_time:252266ms step_avg:179.68ms\nstep:1415/5000 train_loss:3.9290 train_time:252445ms step_avg:179.68ms\nstep:1416/5000 train_loss:3.9714 train_time:252628ms step_avg:179.68ms\nstep:1417/5000 train_loss:3.8175 train_time:252809ms step_avg:179.68ms\nstep:1418/5000 train_loss:3.7768 train_time:252987ms step_avg:179.68ms\nstep:1419/5000 train_loss:3.8272 train_time:253168ms step_avg:179.68ms\nstep:1420/5000 train_loss:3.8449 train_time:253348ms step_avg:179.68ms\nstep:1421/5000 train_loss:3.9460 train_time:253527ms step_avg:179.68ms\nstep:1422/5000 train_loss:3.9176 train_time:253707ms step_avg:179.68ms\nstep:1423/5000 train_loss:3.9421 train_time:253889ms step_avg:179.68ms\nstep:1424/5000 train_loss:3.9389 train_time:254070ms step_avg:179.68ms\nstep:1425/5000 train_loss:3.8660 train_time:254250ms step_avg:179.68ms\nstep:1426/5000 train_loss:3.8335 train_time:254429ms step_avg:179.68ms\nstep:1427/5000 train_loss:3.7778 train_time:254609ms step_avg:179.68ms\nstep:1428/5000 train_loss:3.8456 train_time:254788ms step_avg:179.68ms\nstep:1429/5000 train_loss:3.9043 train_time:254968ms step_avg:179.68ms\nstep:1430/5000 train_loss:3.9582 train_time:255148ms step_avg:179.68ms\nstep:1431/5000 train_loss:4.0777 train_time:255330ms step_avg:179.68ms\nstep:1432/5000 train_loss:3.8988 train_time:255509ms step_avg:179.68ms\nstep:1433/5000 train_loss:3.7562 train_time:255688ms step_avg:179.68ms\nstep:1434/5000 train_loss:3.9377 train_time:255869ms step_avg:179.68ms\nstep:1435/5000 train_loss:3.9952 train_time:256049ms step_avg:179.68ms\nstep:1436/5000 train_loss:3.9166 train_time:256227ms step_avg:179.68ms\nstep:1437/5000 train_loss:3.8807 train_time:256410ms step_avg:179.68ms\nstep:1438/5000 train_loss:3.8648 train_time:256589ms step_avg:179.68ms\nstep:1439/5000 train_loss:4.0250 train_time:256768ms step_avg:179.68ms\nstep:1440/5000 train_loss:3.5131 train_time:256948ms step_avg:179.68ms\nstep:1441/5000 train_loss:3.8589 train_time:257128ms step_avg:179.68ms\nstep:1442/5000 train_loss:4.1373 train_time:257308ms step_avg:179.68ms\nstep:1443/5000 train_loss:3.9741 train_time:257488ms step_avg:179.68ms\nstep:1444/5000 train_loss:4.0788 train_time:257668ms step_avg:179.68ms\nstep:1445/5000 train_loss:3.9235 train_time:257846ms step_avg:179.68ms\nstep:1446/5000 train_loss:4.1032 train_time:258026ms step_avg:179.68ms\nstep:1447/5000 train_loss:3.7429 train_time:258208ms step_avg:179.69ms\nstep:1448/5000 train_loss:3.9723 train_time:258388ms step_avg:179.69ms\nstep:1449/5000 train_loss:3.9309 train_time:258567ms step_avg:179.68ms\nstep:1450/5000 train_loss:3.9189 train_time:258746ms step_avg:179.69ms\nstep:1451/5000 train_loss:3.9426 train_time:258926ms step_avg:179.69ms\nstep:1452/5000 train_loss:3.8977 train_time:259106ms step_avg:179.69ms\nstep:1453/5000 train_loss:3.9372 train_time:259286ms step_avg:179.69ms\nstep:1454/5000 train_loss:3.8234 train_time:259465ms step_avg:179.69ms\nstep:1455/5000 train_loss:3.9190 train_time:259645ms step_avg:179.69ms\nstep:1456/5000 train_loss:3.7929 train_time:259825ms step_avg:179.69ms\nstep:1457/5000 train_loss:3.9323 train_time:260005ms step_avg:179.69ms\nstep:1458/5000 train_loss:3.9986 train_time:260186ms step_avg:179.69ms\nstep:1459/5000 train_loss:3.8508 train_time:260365ms step_avg:179.69ms\nstep:1460/5000 train_loss:3.9661 train_time:260545ms step_avg:179.69ms\nstep:1461/5000 train_loss:4.0015 train_time:260725ms step_avg:179.69ms\nstep:1462/5000 train_loss:3.8366 train_time:260903ms step_avg:179.69ms\nstep:1463/5000 train_loss:4.0402 train_time:261082ms step_avg:179.68ms\nstep:1464/5000 train_loss:3.9155 train_time:261262ms step_avg:179.68ms\nstep:1465/5000 train_loss:3.8554 train_time:261441ms step_avg:179.68ms\nstep:1466/5000 train_loss:3.9236 train_time:261622ms step_avg:179.69ms\nstep:1467/5000 train_loss:3.8711 train_time:261801ms step_avg:179.69ms\nstep:1468/5000 train_loss:3.9164 train_time:261981ms step_avg:179.69ms\nstep:1469/5000 train_loss:3.8736 train_time:262161ms step_avg:179.69ms\nstep:1470/5000 train_loss:3.8518 train_time:262341ms step_avg:179.69ms\nstep:1471/5000 train_loss:3.8959 train_time:262520ms step_avg:179.69ms\nstep:1472/5000 train_loss:3.8971 train_time:262700ms step_avg:179.69ms\nstep:1473/5000 train_loss:3.8850 train_time:262880ms step_avg:179.69ms\nstep:1474/5000 train_loss:3.9428 train_time:263059ms step_avg:179.69ms\nstep:1475/5000 train_loss:3.9079 train_time:263237ms step_avg:179.68ms\nstep:1476/5000 train_loss:3.7896 train_time:263417ms step_avg:179.68ms\nstep:1477/5000 train_loss:3.8107 train_time:263597ms step_avg:179.68ms\nstep:1478/5000 train_loss:3.9191 train_time:263775ms step_avg:179.68ms\nstep:1479/5000 train_loss:3.9063 train_time:263954ms step_avg:179.68ms\nstep:1480/5000 train_loss:3.9328 train_time:264133ms step_avg:179.68ms\nstep:1481/5000 train_loss:3.9111 train_time:264311ms step_avg:179.68ms\nstep:1482/5000 train_loss:3.7229 train_time:264490ms step_avg:179.68ms\nstep:1483/5000 train_loss:3.9804 train_time:264669ms step_avg:179.68ms\nstep:1484/5000 train_loss:3.8027 train_time:264848ms step_avg:179.68ms\nstep:1485/5000 train_loss:4.0029 train_time:265029ms step_avg:179.68ms\nstep:1486/5000 train_loss:3.8351 train_time:265209ms step_avg:179.68ms\nstep:1487/5000 train_loss:3.8662 train_time:265388ms step_avg:179.68ms\nstep:1488/5000 train_loss:3.7501 train_time:265567ms step_avg:179.68ms\nstep:1489/5000 train_loss:3.9114 train_time:265746ms step_avg:179.68ms\nstep:1490/5000 train_loss:3.9019 train_time:265927ms step_avg:179.68ms\nstep:1491/5000 train_loss:3.9205 train_time:266107ms step_avg:179.68ms\nstep:1492/5000 train_loss:3.9698 train_time:266286ms step_avg:179.68ms\nstep:1493/5000 train_loss:3.9479 train_time:266466ms step_avg:179.68ms\nstep:1494/5000 train_loss:3.8679 train_time:266647ms step_avg:179.68ms\nstep:1495/5000 train_loss:3.7440 train_time:266828ms step_avg:179.68ms\nstep:1496/5000 train_loss:3.8796 train_time:267009ms step_avg:179.68ms\nstep:1497/5000 train_loss:4.0437 train_time:267189ms step_avg:179.68ms\nstep:1498/5000 train_loss:3.9511 train_time:267369ms step_avg:179.68ms\nstep:1499/5000 train_loss:3.7941 train_time:267548ms step_avg:179.68ms\nstep:1500/5000 train_loss:3.8663 train_time:267728ms step_avg:179.68ms\nstep:1501/5000 train_loss:3.8955 train_time:267907ms step_avg:179.68ms\nstep:1502/5000 train_loss:3.8550 train_time:268088ms step_avg:179.68ms\nstep:1503/5000 train_loss:3.8709 train_time:268267ms step_avg:179.68ms\nstep:1504/5000 train_loss:3.9141 train_time:268447ms step_avg:179.68ms\nstep:1505/5000 train_loss:3.9001 train_time:268627ms step_avg:179.68ms\nstep:1506/5000 train_loss:3.7595 train_time:268808ms step_avg:179.68ms\nstep:1507/5000 train_loss:4.0195 train_time:268987ms step_avg:179.68ms\nstep:1508/5000 train_loss:3.8392 train_time:269166ms step_avg:179.68ms\nstep:1509/5000 train_loss:3.8023 train_time:269345ms step_avg:179.68ms\nstep:1510/5000 train_loss:3.3505 train_time:269525ms step_avg:179.68ms\nstep:1511/5000 train_loss:3.9312 train_time:269704ms step_avg:179.68ms\nstep:1512/5000 train_loss:3.8869 train_time:269885ms step_avg:179.68ms\nstep:1513/5000 train_loss:4.0172 train_time:270067ms step_avg:179.69ms\nstep:1514/5000 train_loss:3.7303 train_time:270246ms step_avg:179.68ms\nstep:1515/5000 train_loss:3.7650 train_time:270427ms step_avg:179.69ms\nstep:1516/5000 train_loss:3.9221 train_time:270609ms step_avg:179.69ms\nstep:1517/5000 train_loss:3.9711 train_time:270787ms step_avg:179.69ms\nstep:1518/5000 train_loss:3.7530 train_time:270966ms step_avg:179.69ms\nstep:1519/5000 train_loss:4.0284 train_time:271149ms step_avg:179.69ms\nstep:1520/5000 train_loss:4.0030 train_time:271328ms step_avg:179.69ms\nstep:1521/5000 train_loss:3.8569 train_time:271508ms step_avg:179.69ms\nstep:1522/5000 train_loss:3.8504 train_time:271690ms step_avg:179.69ms\nstep:1523/5000 train_loss:3.9187 train_time:271868ms step_avg:179.69ms\nstep:1524/5000 train_loss:3.7502 train_time:272282ms step_avg:179.84ms\nstep:1525/5000 train_loss:3.8501 train_time:272457ms step_avg:179.84ms\nstep:1526/5000 train_loss:3.9013 train_time:272634ms step_avg:179.84ms\nstep:1527/5000 train_loss:3.8243 train_time:272813ms step_avg:179.84ms\nstep:1528/5000 train_loss:3.8257 train_time:272991ms step_avg:179.84ms\nstep:1529/5000 train_loss:4.1900 train_time:273174ms step_avg:179.84ms\nstep:1530/5000 train_loss:3.8798 train_time:273352ms step_avg:179.84ms\nstep:1531/5000 train_loss:3.8278 train_time:273530ms step_avg:179.84ms\nstep:1532/5000 train_loss:3.8250 train_time:273710ms step_avg:179.84ms\nstep:1533/5000 train_loss:3.8081 train_time:273888ms step_avg:179.83ms\nstep:1534/5000 train_loss:3.8502 train_time:274068ms step_avg:179.83ms\nstep:1535/5000 train_loss:3.7826 train_time:274249ms step_avg:179.84ms\nstep:1536/5000 train_loss:3.9428 train_time:274430ms step_avg:179.84ms\nstep:1537/5000 train_loss:3.9929 train_time:274611ms step_avg:179.84ms\nstep:1538/5000 train_loss:3.9294 train_time:274789ms step_avg:179.84ms\nstep:1539/5000 train_loss:3.8760 train_time:274969ms step_avg:179.84ms\nstep:1540/5000 train_loss:3.7308 train_time:275151ms step_avg:179.84ms\nstep:1541/5000 train_loss:3.9098 train_time:275329ms step_avg:179.84ms\nstep:1542/5000 train_loss:3.8472 train_time:275508ms step_avg:179.84ms\nstep:1543/5000 train_loss:3.9643 train_time:275688ms step_avg:179.84ms\nstep:1544/5000 train_loss:3.9198 train_time:275866ms step_avg:179.83ms\nstep:1545/5000 train_loss:4.0151 train_time:276047ms step_avg:179.84ms\nstep:1546/5000 train_loss:3.9419 train_time:276227ms step_avg:179.84ms\nstep:1547/5000 train_loss:3.8451 train_time:276407ms step_avg:179.84ms\nstep:1548/5000 train_loss:3.8369 train_time:276586ms step_avg:179.83ms\nstep:1549/5000 train_loss:3.9439 train_time:276765ms step_avg:179.83ms\nstep:1550/5000 train_loss:4.1393 train_time:276944ms step_avg:179.83ms\nstep:1551/5000 train_loss:3.8595 train_time:277124ms step_avg:179.83ms\nstep:1552/5000 train_loss:4.0235 train_time:277305ms step_avg:179.83ms\nstep:1553/5000 train_loss:3.9694 train_time:277487ms step_avg:179.84ms\nstep:1554/5000 train_loss:3.8483 train_time:277668ms step_avg:179.84ms\nstep:1555/5000 train_loss:3.7863 train_time:277848ms step_avg:179.84ms\nstep:1556/5000 train_loss:3.8684 train_time:278027ms step_avg:179.84ms\nstep:1557/5000 train_loss:3.8817 train_time:278207ms step_avg:179.84ms\nstep:1558/5000 train_loss:3.8414 train_time:278386ms step_avg:179.84ms\nstep:1559/5000 train_loss:3.9044 train_time:278566ms step_avg:179.84ms\nstep:1560/5000 train_loss:3.9270 train_time:278746ms step_avg:179.84ms\nstep:1561/5000 train_loss:3.7546 train_time:278925ms step_avg:179.84ms\nstep:1562/5000 train_loss:3.9544 train_time:279103ms step_avg:179.83ms\nstep:1563/5000 train_loss:3.9044 train_time:279283ms step_avg:179.83ms\nstep:1564/5000 train_loss:3.7704 train_time:279463ms step_avg:179.83ms\nstep:1565/5000 train_loss:3.9104 train_time:279644ms step_avg:179.84ms\nstep:1566/5000 train_loss:3.8818 train_time:279825ms step_avg:179.84ms\nstep:1567/5000 train_loss:3.8566 train_time:280004ms step_avg:179.84ms\nstep:1568/5000 train_loss:3.8407 train_time:280183ms step_avg:179.84ms\nstep:1569/5000 train_loss:3.8671 train_time:280363ms step_avg:179.83ms\nstep:1570/5000 train_loss:3.9818 train_time:280541ms step_avg:179.83ms\nstep:1571/5000 train_loss:4.0084 train_time:280722ms step_avg:179.83ms\nstep:1572/5000 train_loss:3.8654 train_time:280901ms step_avg:179.83ms\nstep:1573/5000 train_loss:3.9589 train_time:281080ms step_avg:179.83ms\nstep:1574/5000 train_loss:3.8169 train_time:281260ms step_avg:179.83ms\nstep:1575/5000 train_loss:3.9465 train_time:281439ms step_avg:179.83ms\nstep:1576/5000 train_loss:3.8730 train_time:281618ms step_avg:179.83ms\nstep:1577/5000 train_loss:3.8703 train_time:281798ms step_avg:179.83ms\nstep:1578/5000 train_loss:3.9415 train_time:281977ms step_avg:179.83ms\nstep:1579/5000 train_loss:4.0058 train_time:282156ms step_avg:179.83ms\nstep:1580/5000 train_loss:3.8693 train_time:282334ms step_avg:179.83ms\nstep:1581/5000 train_loss:3.9099 train_time:282512ms step_avg:179.83ms\nstep:1582/5000 train_loss:3.6971 train_time:282692ms step_avg:179.83ms\nstep:1583/5000 train_loss:3.8685 train_time:282871ms step_avg:179.83ms\nstep:1584/5000 train_loss:3.8103 train_time:283049ms step_avg:179.83ms\nstep:1585/5000 train_loss:3.8666 train_time:283229ms step_avg:179.83ms\nstep:1586/5000 train_loss:3.8974 train_time:283408ms step_avg:179.83ms\nstep:1587/5000 train_loss:3.7127 train_time:283588ms step_avg:179.83ms\nstep:1588/5000 train_loss:3.6499 train_time:283768ms step_avg:179.83ms\nstep:1589/5000 train_loss:3.7515 train_time:283948ms step_avg:179.83ms\nstep:1590/5000 train_loss:3.9461 train_time:284127ms step_avg:179.83ms\nstep:1591/5000 train_loss:4.1723 train_time:284307ms step_avg:179.83ms\nstep:1592/5000 train_loss:3.8796 train_time:284489ms step_avg:179.83ms\nstep:1593/5000 train_loss:4.0119 train_time:284667ms step_avg:179.83ms\nstep:1594/5000 train_loss:3.9532 train_time:284846ms step_avg:179.83ms\nstep:1595/5000 train_loss:3.8346 train_time:285026ms step_avg:179.83ms\nstep:1596/5000 train_loss:3.8615 train_time:285205ms step_avg:179.83ms\nstep:1597/5000 train_loss:3.8978 train_time:285385ms step_avg:179.83ms\nstep:1598/5000 train_loss:3.8469 train_time:285566ms step_avg:179.83ms\nstep:1599/5000 train_loss:3.7982 train_time:285744ms step_avg:179.83ms\nstep:1600/5000 train_loss:3.7775 train_time:285923ms step_avg:179.83ms\nstep:1601/5000 train_loss:3.8428 train_time:286102ms step_avg:179.83ms\nstep:1602/5000 train_loss:3.9195 train_time:286281ms step_avg:179.83ms\nstep:1603/5000 train_loss:3.8317 train_time:286462ms step_avg:179.83ms\nstep:1604/5000 train_loss:3.8813 train_time:286641ms step_avg:179.83ms\nstep:1605/5000 train_loss:3.9004 train_time:286821ms step_avg:179.83ms\nstep:1606/5000 train_loss:3.9834 train_time:287002ms step_avg:179.83ms\nstep:1607/5000 train_loss:4.0264 train_time:287182ms step_avg:179.83ms\nstep:1608/5000 train_loss:3.7774 train_time:287361ms step_avg:179.83ms\nstep:1609/5000 train_loss:3.8206 train_time:287540ms step_avg:179.82ms\nstep:1610/5000 train_loss:3.9024 train_time:287719ms step_avg:179.82ms\nstep:1611/5000 train_loss:3.8854 train_time:287898ms step_avg:179.82ms\nstep:1612/5000 train_loss:4.0523 train_time:288076ms step_avg:179.82ms\nstep:1613/5000 train_loss:3.8328 train_time:288254ms step_avg:179.82ms\nstep:1614/5000 train_loss:4.0174 train_time:288431ms step_avg:179.82ms\nstep:1615/5000 train_loss:3.7965 train_time:288611ms step_avg:179.82ms\nstep:1616/5000 train_loss:3.9774 train_time:288790ms step_avg:179.82ms\nstep:1617/5000 train_loss:3.8855 train_time:288969ms step_avg:179.82ms\nstep:1618/5000 train_loss:3.7761 train_time:289149ms step_avg:179.82ms\nstep:1619/5000 train_loss:3.8477 train_time:289330ms step_avg:179.82ms\nstep:1620/5000 train_loss:3.9040 train_time:289510ms step_avg:179.82ms\nstep:1621/5000 train_loss:3.9159 train_time:289689ms step_avg:179.82ms\nstep:1622/5000 train_loss:3.8600 train_time:289869ms step_avg:179.82ms\nstep:1623/5000 train_loss:3.8811 train_time:290047ms step_avg:179.82ms\nstep:1624/5000 train_loss:3.9228 train_time:290227ms step_avg:179.82ms\nstep:1625/5000 train_loss:4.4452 train_time:290407ms step_avg:179.82ms\nstep:1626/5000 train_loss:4.1631 train_time:290587ms step_avg:179.82ms\nstep:1627/5000 train_loss:3.8184 train_time:290767ms step_avg:179.82ms\nstep:1628/5000 train_loss:3.9015 train_time:290948ms step_avg:179.82ms\nstep:1629/5000 train_loss:3.8964 train_time:291128ms step_avg:179.82ms\nstep:1630/5000 train_loss:3.8577 train_time:291309ms step_avg:179.82ms\nstep:1631/5000 train_loss:3.8787 train_time:291487ms step_avg:179.82ms\nstep:1632/5000 train_loss:3.8023 train_time:291667ms step_avg:179.82ms\nstep:1633/5000 train_loss:4.0511 train_time:291846ms step_avg:179.82ms\nstep:1634/5000 train_loss:3.8880 train_time:292027ms step_avg:179.82ms\nstep:1635/5000 train_loss:3.9009 train_time:292206ms step_avg:179.82ms\nstep:1636/5000 train_loss:3.8898 train_time:292387ms step_avg:179.82ms\nstep:1637/5000 train_loss:4.1045 train_time:292565ms step_avg:179.82ms\nstep:1638/5000 train_loss:3.9255 train_time:292745ms step_avg:179.82ms\nstep:1639/5000 train_loss:3.8528 train_time:292925ms step_avg:179.82ms\nstep:1640/5000 train_loss:3.8273 train_time:293105ms step_avg:179.82ms\nstep:1641/5000 train_loss:3.8648 train_time:293285ms step_avg:179.82ms\nstep:1642/5000 train_loss:3.8237 train_time:293467ms step_avg:179.82ms\nstep:1643/5000 train_loss:3.9375 train_time:293647ms step_avg:179.82ms\nstep:1644/5000 train_loss:3.8315 train_time:293826ms step_avg:179.82ms\nstep:1645/5000 train_loss:3.8821 train_time:294005ms step_avg:179.82ms\nstep:1646/5000 train_loss:3.8336 train_time:294184ms step_avg:179.82ms\nstep:1647/5000 train_loss:3.8319 train_time:294364ms step_avg:179.82ms\nstep:1648/5000 train_loss:3.8299 train_time:294545ms step_avg:179.82ms\nstep:1649/5000 train_loss:3.9201 train_time:294724ms step_avg:179.82ms\nstep:1650/5000 train_loss:4.2717 train_time:294905ms step_avg:179.82ms\nstep:1651/5000 train_loss:3.8931 train_time:295084ms step_avg:179.82ms\nstep:1652/5000 train_loss:3.8349 train_time:295265ms step_avg:179.82ms\nstep:1653/5000 train_loss:4.0085 train_time:295444ms step_avg:179.82ms\nstep:1654/5000 train_loss:3.7017 train_time:295624ms step_avg:179.82ms\nstep:1655/5000 train_loss:3.8217 train_time:295804ms step_avg:179.82ms\nstep:1656/5000 train_loss:3.9578 train_time:295985ms step_avg:179.82ms\nstep:1657/5000 train_loss:3.9538 train_time:296165ms step_avg:179.82ms\nstep:1658/5000 train_loss:4.0774 train_time:296346ms step_avg:179.82ms\nstep:1659/5000 train_loss:3.9564 train_time:296527ms step_avg:179.82ms\nstep:1660/5000 train_loss:3.7845 train_time:296707ms step_avg:179.82ms\nstep:1661/5000 train_loss:3.8134 train_time:296889ms step_avg:179.82ms\nstep:1662/5000 train_loss:3.8335 train_time:297069ms step_avg:179.82ms\nstep:1663/5000 train_loss:4.0132 train_time:297248ms step_avg:179.82ms\nstep:1664/5000 train_loss:3.8581 train_time:297427ms step_avg:179.82ms\nstep:1665/5000 train_loss:4.0196 train_time:297608ms step_avg:179.82ms\nstep:1666/5000 train_loss:3.8445 train_time:297787ms step_avg:179.82ms\nstep:1667/5000 train_loss:3.6352 train_time:297969ms step_avg:179.82ms\nstep:1668/5000 train_loss:3.9739 train_time:298150ms step_avg:179.83ms\nstep:1669/5000 train_loss:3.9064 train_time:298329ms step_avg:179.82ms\nstep:1670/5000 train_loss:4.0194 train_time:298509ms step_avg:179.82ms\nstep:1671/5000 train_loss:3.8399 train_time:298688ms step_avg:179.82ms\nstep:1672/5000 train_loss:3.7294 train_time:298868ms step_avg:179.82ms\nstep:1673/5000 train_loss:3.7603 train_time:299047ms step_avg:179.82ms\nstep:1674/5000 train_loss:3.7625 train_time:299227ms step_avg:179.82ms\nstep:1675/5000 train_loss:3.9699 train_time:299408ms step_avg:179.82ms\nstep:1676/5000 train_loss:4.0007 train_time:299588ms step_avg:179.82ms\nstep:1677/5000 train_loss:3.7250 train_time:299767ms step_avg:179.82ms\nstep:1678/5000 train_loss:3.8786 train_time:299948ms step_avg:179.82ms\nstep:1679/5000 train_loss:3.8136 train_time:300127ms step_avg:179.82ms\nstep:1680/5000 train_loss:4.0714 train_time:300305ms step_avg:179.82ms\nstep:1681/5000 train_loss:3.7106 train_time:300486ms step_avg:179.82ms\nstep:1682/5000 train_loss:3.7901 train_time:300666ms step_avg:179.82ms\nstep:1683/5000 train_loss:3.8417 train_time:300848ms step_avg:179.83ms\nstep:1684/5000 train_loss:3.8283 train_time:301027ms step_avg:179.82ms\nstep:1685/5000 train_loss:3.9298 train_time:301205ms step_avg:179.82ms\nstep:1686/5000 train_loss:3.7172 train_time:301384ms step_avg:179.82ms\nstep:1687/5000 train_loss:3.8559 train_time:301562ms step_avg:179.82ms\nstep:1688/5000 train_loss:3.9529 train_time:301742ms step_avg:179.82ms\nstep:1689/5000 train_loss:3.7809 train_time:301921ms step_avg:179.82ms\nstep:1690/5000 train_loss:3.8515 train_time:302101ms step_avg:179.82ms\nstep:1691/5000 train_loss:3.7673 train_time:302280ms step_avg:179.82ms\nstep:1692/5000 train_loss:3.6692 train_time:302459ms step_avg:179.82ms\nstep:1693/5000 train_loss:3.8755 train_time:302638ms step_avg:179.82ms\nstep:1694/5000 train_loss:3.9463 train_time:302816ms step_avg:179.82ms\nstep:1695/5000 train_loss:3.8431 train_time:302995ms step_avg:179.82ms\nstep:1696/5000 train_loss:3.8395 train_time:303175ms step_avg:179.82ms\nstep:1697/5000 train_loss:3.8803 train_time:303354ms step_avg:179.82ms\nstep:1698/5000 train_loss:3.7010 train_time:303533ms step_avg:179.82ms\nstep:1699/5000 train_loss:3.9333 train_time:303712ms step_avg:179.82ms\nstep:1700/5000 train_loss:3.7276 train_time:303891ms step_avg:179.82ms\nstep:1701/5000 train_loss:3.9491 train_time:304069ms step_avg:179.82ms\nstep:1702/5000 train_loss:3.8739 train_time:304248ms step_avg:179.82ms\nstep:1703/5000 train_loss:3.7290 train_time:304430ms step_avg:179.82ms\nstep:1704/5000 train_loss:3.9314 train_time:304610ms step_avg:179.82ms\nstep:1705/5000 train_loss:3.8006 train_time:304788ms step_avg:179.82ms\nstep:1706/5000 train_loss:3.7594 train_time:304967ms step_avg:179.82ms\nstep:1707/5000 train_loss:3.9096 train_time:305145ms step_avg:179.81ms\nstep:1708/5000 train_loss:3.9041 train_time:305325ms step_avg:179.81ms\nstep:1709/5000 train_loss:3.7559 train_time:305504ms step_avg:179.81ms\nstep:1710/5000 train_loss:3.9680 train_time:305684ms step_avg:179.81ms\nstep:1711/5000 train_loss:3.8763 train_time:305864ms step_avg:179.81ms\nstep:1712/5000 train_loss:3.8358 train_time:306045ms step_avg:179.81ms\nstep:1713/5000 train_loss:3.8414 train_time:306224ms step_avg:179.81ms\nstep:1714/5000 train_loss:3.8960 train_time:306407ms step_avg:179.82ms\nstep:1715/5000 train_loss:3.7783 train_time:306587ms step_avg:179.82ms\nstep:1716/5000 train_loss:3.9368 train_time:306766ms step_avg:179.82ms\nstep:1717/5000 train_loss:3.8216 train_time:306947ms step_avg:179.82ms\nstep:1718/5000 train_loss:3.8312 train_time:307127ms step_avg:179.82ms\nstep:1719/5000 train_loss:3.7522 train_time:307307ms step_avg:179.82ms\nstep:1720/5000 train_loss:3.7625 train_time:307487ms step_avg:179.82ms\nstep:1721/5000 train_loss:3.7768 train_time:307667ms step_avg:179.82ms\nstep:1722/5000 train_loss:3.7926 train_time:307846ms step_avg:179.82ms\nstep:1723/5000 train_loss:4.0317 train_time:308027ms step_avg:179.82ms\nstep:1724/5000 train_loss:3.6981 train_time:308207ms step_avg:179.82ms\nstep:1725/5000 train_loss:3.7996 train_time:308386ms step_avg:179.82ms\nstep:1726/5000 train_loss:3.7079 train_time:308566ms step_avg:179.82ms\nstep:1727/5000 train_loss:4.0629 train_time:308745ms step_avg:179.82ms\nstep:1728/5000 train_loss:3.9223 train_time:308927ms step_avg:179.82ms\nstep:1729/5000 train_loss:3.9878 train_time:309109ms step_avg:179.82ms\nstep:1730/5000 train_loss:3.8348 train_time:309288ms step_avg:179.82ms\nstep:1731/5000 train_loss:3.8494 train_time:309467ms step_avg:179.82ms\nstep:1732/5000 train_loss:3.9416 train_time:309649ms step_avg:179.82ms\nstep:1733/5000 train_loss:3.7004 train_time:309828ms step_avg:179.82ms\nstep:1734/5000 train_loss:3.8881 train_time:310009ms step_avg:179.82ms\nstep:1735/5000 train_loss:3.8411 train_time:310188ms step_avg:179.82ms\nstep:1736/5000 train_loss:3.9208 train_time:310367ms step_avg:179.82ms\nstep:1737/5000 train_loss:3.9492 train_time:310547ms step_avg:179.82ms\nstep:1738/5000 train_loss:3.8500 train_time:310725ms step_avg:179.82ms\nstep:1739/5000 train_loss:3.8692 train_time:310906ms step_avg:179.82ms\nstep:1740/5000 train_loss:3.8189 train_time:311085ms step_avg:179.82ms\nstep:1741/5000 train_loss:3.8515 train_time:311265ms step_avg:179.82ms\nstep:1742/5000 train_loss:3.8883 train_time:311445ms step_avg:179.82ms\nstep:1743/5000 train_loss:3.9448 train_time:311625ms step_avg:179.82ms\nstep:1744/5000 train_loss:3.9139 train_time:311804ms step_avg:179.82ms\nstep:1745/5000 train_loss:3.8822 train_time:311984ms step_avg:179.82ms\nstep:1746/5000 train_loss:3.9080 train_time:312163ms step_avg:179.82ms\nstep:1747/5000 train_loss:3.9755 train_time:312345ms step_avg:179.82ms\nstep:1748/5000 train_loss:3.9017 train_time:312526ms step_avg:179.82ms\nstep:1749/5000 train_loss:3.8982 train_time:312706ms step_avg:179.82ms\nstep:1750/5000 train_loss:3.7863 train_time:312885ms step_avg:179.82ms\nstep:1751/5000 train_loss:4.1788 train_time:313065ms step_avg:179.82ms\nstep:1752/5000 train_loss:3.8801 train_time:313245ms step_avg:179.82ms\nstep:1753/5000 train_loss:3.9337 train_time:313425ms step_avg:179.82ms\nstep:1754/5000 train_loss:3.7944 train_time:313608ms step_avg:179.82ms\nstep:1755/5000 train_loss:3.8897 train_time:313787ms step_avg:179.82ms\nstep:1756/5000 train_loss:3.9280 train_time:313966ms step_avg:179.82ms\nstep:1757/5000 train_loss:3.8831 train_time:314144ms step_avg:179.82ms\nstep:1758/5000 train_loss:3.7877 train_time:314324ms step_avg:179.82ms\nstep:1759/5000 train_loss:3.8997 train_time:314504ms step_avg:179.82ms\nstep:1760/5000 train_loss:3.7758 train_time:314684ms step_avg:179.82ms\nstep:1761/5000 train_loss:3.7059 train_time:314863ms step_avg:179.82ms\nstep:1762/5000 train_loss:3.9656 train_time:315042ms step_avg:179.82ms\nstep:1763/5000 train_loss:3.9277 train_time:315221ms step_avg:179.82ms\nstep:1764/5000 train_loss:3.8736 train_time:315400ms step_avg:179.82ms\nstep:1765/5000 train_loss:3.5863 train_time:315579ms step_avg:179.82ms\nstep:1766/5000 train_loss:3.8460 train_time:315758ms step_avg:179.82ms\nstep:1767/5000 train_loss:3.7627 train_time:315937ms step_avg:179.82ms\nstep:1768/5000 train_loss:3.8019 train_time:316116ms step_avg:179.82ms\nstep:1769/5000 train_loss:3.7484 train_time:316295ms step_avg:179.82ms\nstep:1770/5000 train_loss:3.8164 train_time:316474ms step_avg:179.81ms\nstep:1771/5000 train_loss:3.7758 train_time:316652ms step_avg:179.81ms\nstep:1772/5000 train_loss:3.7011 train_time:316832ms step_avg:179.81ms\nstep:1773/5000 train_loss:3.9406 train_time:317011ms step_avg:179.81ms\nstep:1774/5000 train_loss:3.7150 train_time:317190ms step_avg:179.81ms\nstep:1775/5000 train_loss:3.7828 train_time:317369ms step_avg:179.81ms\nstep:1776/5000 train_loss:3.8537 train_time:317550ms step_avg:179.81ms\nstep:1777/5000 train_loss:3.8388 train_time:317730ms step_avg:179.81ms\nstep:1778/5000 train_loss:3.9989 train_time:317910ms step_avg:179.81ms\nstep:1779/5000 train_loss:3.9013 train_time:318090ms step_avg:179.81ms\nstep:1780/5000 train_loss:4.0974 train_time:318269ms step_avg:179.81ms\nstep:1781/5000 train_loss:3.7738 train_time:318447ms step_avg:179.81ms\nstep:1782/5000 train_loss:3.9238 train_time:318628ms step_avg:179.81ms\nstep:1783/5000 train_loss:3.9885 train_time:318809ms step_avg:179.81ms\nstep:1784/5000 train_loss:3.7131 train_time:318988ms step_avg:179.81ms\nstep:1785/5000 train_loss:3.8553 train_time:319168ms step_avg:179.81ms\nstep:1786/5000 train_loss:3.9126 train_time:319350ms step_avg:179.81ms\nstep:1787/5000 train_loss:3.8365 train_time:319530ms step_avg:179.81ms\nstep:1788/5000 train_loss:3.9068 train_time:319710ms step_avg:179.81ms\nstep:1789/5000 train_loss:3.8992 train_time:319889ms step_avg:179.81ms\nstep:1790/5000 train_loss:3.8322 train_time:320070ms step_avg:179.81ms\nstep:1791/5000 train_loss:3.7039 train_time:320250ms step_avg:179.81ms\nstep:1792/5000 train_loss:3.9461 train_time:320429ms step_avg:179.81ms\nstep:1793/5000 train_loss:3.9036 train_time:320610ms step_avg:179.81ms\nstep:1794/5000 train_loss:3.9347 train_time:320790ms step_avg:179.81ms\nstep:1795/5000 train_loss:4.1810 train_time:320969ms step_avg:179.81ms\nstep:1796/5000 train_loss:3.7835 train_time:321148ms step_avg:179.81ms\nstep:1797/5000 train_loss:3.8887 train_time:321327ms step_avg:179.81ms\nstep:1798/5000 train_loss:3.7710 train_time:321506ms step_avg:179.81ms\nstep:1799/5000 train_loss:3.9180 train_time:321687ms step_avg:179.81ms\nstep:1800/5000 train_loss:3.7577 train_time:321868ms step_avg:179.81ms\nstep:1801/5000 train_loss:3.9611 train_time:322048ms step_avg:179.81ms\nstep:1802/5000 train_loss:3.7925 train_time:322228ms step_avg:179.81ms\nstep:1803/5000 train_loss:3.9255 train_time:322407ms step_avg:179.81ms\nstep:1804/5000 train_loss:3.8140 train_time:322586ms step_avg:179.81ms\nstep:1805/5000 train_loss:3.8542 train_time:322764ms step_avg:179.81ms\nstep:1806/5000 train_loss:3.6672 train_time:322944ms step_avg:179.81ms\nstep:1807/5000 train_loss:3.9222 train_time:323124ms step_avg:179.81ms\nstep:1808/5000 train_loss:4.1321 train_time:323303ms step_avg:179.81ms\nstep:1809/5000 train_loss:3.9492 train_time:323483ms step_avg:179.81ms\nstep:1810/5000 train_loss:3.7888 train_time:323665ms step_avg:179.81ms\nstep:1811/5000 train_loss:3.8423 train_time:323845ms step_avg:179.81ms\nstep:1812/5000 train_loss:4.0212 train_time:324025ms step_avg:179.81ms\nstep:1813/5000 train_loss:3.8973 train_time:324206ms step_avg:179.81ms\nstep:1814/5000 train_loss:3.7585 train_time:324386ms step_avg:179.81ms\nstep:1815/5000 train_loss:3.8772 train_time:324565ms step_avg:179.81ms\nstep:1816/5000 train_loss:3.8813 train_time:324744ms step_avg:179.81ms\nstep:1817/5000 train_loss:4.0836 train_time:324924ms step_avg:179.81ms\nstep:1818/5000 train_loss:3.7252 train_time:325104ms step_avg:179.81ms\nstep:1819/5000 train_loss:3.6381 train_time:325286ms step_avg:179.82ms\nstep:1820/5000 train_loss:3.7870 train_time:325468ms step_avg:179.82ms\nstep:1821/5000 train_loss:3.9023 train_time:325651ms step_avg:179.82ms\nstep:1822/5000 train_loss:3.9320 train_time:325828ms step_avg:179.82ms\nstep:1823/5000 train_loss:3.7069 train_time:326007ms step_avg:179.82ms\nstep:1824/5000 train_loss:3.8976 train_time:326190ms step_avg:179.82ms\nstep:1825/5000 train_loss:3.9091 train_time:326365ms step_avg:179.82ms\nstep:1826/5000 train_loss:3.8181 train_time:326545ms step_avg:179.82ms\nstep:1827/5000 train_loss:3.8158 train_time:326726ms step_avg:179.82ms\nstep:1828/5000 train_loss:3.8362 train_time:326905ms step_avg:179.82ms\nstep:1829/5000 train_loss:3.8666 train_time:327085ms step_avg:179.82ms\nstep:1830/5000 train_loss:3.8767 train_time:327264ms step_avg:179.82ms\nstep:1831/5000 train_loss:3.7668 train_time:327444ms step_avg:179.82ms\nstep:1832/5000 train_loss:3.8578 train_time:327624ms step_avg:179.82ms\nstep:1833/5000 train_loss:3.6198 train_time:327803ms step_avg:179.82ms\nstep:1834/5000 train_loss:3.9086 train_time:327986ms step_avg:179.82ms\nstep:1835/5000 train_loss:3.7883 train_time:328162ms step_avg:179.81ms\nstep:1836/5000 train_loss:3.9241 train_time:328342ms step_avg:179.81ms\nstep:1837/5000 train_loss:3.8237 train_time:328522ms step_avg:179.81ms\nstep:1838/5000 train_loss:3.7185 train_time:328703ms step_avg:179.82ms\nstep:1839/5000 train_loss:3.7583 train_time:328882ms step_avg:179.82ms\nstep:1840/5000 train_loss:3.7635 train_time:329062ms step_avg:179.82ms\nstep:1841/5000 train_loss:3.7980 train_time:329241ms step_avg:179.81ms\nstep:1842/5000 train_loss:3.8833 train_time:329420ms step_avg:179.81ms\nstep:1843/5000 train_loss:3.9052 train_time:329599ms step_avg:179.81ms\nstep:1844/5000 train_loss:3.8774 train_time:329779ms step_avg:179.81ms\nstep:1845/5000 train_loss:3.7890 train_time:329957ms step_avg:179.81ms\nstep:1846/5000 train_loss:3.7909 train_time:330136ms step_avg:179.81ms\nstep:1847/5000 train_loss:3.8979 train_time:330314ms step_avg:179.81ms\nstep:1848/5000 train_loss:3.8469 train_time:330492ms step_avg:179.81ms\nstep:1849/5000 train_loss:3.8156 train_time:330672ms step_avg:179.81ms\nstep:1850/5000 train_loss:3.8129 train_time:330850ms step_avg:179.81ms\nstep:1851/5000 train_loss:3.7999 train_time:331031ms step_avg:179.81ms\nstep:1852/5000 train_loss:3.6647 train_time:331209ms step_avg:179.81ms\nstep:1853/5000 train_loss:3.7147 train_time:331390ms step_avg:179.81ms\nstep:1854/5000 train_loss:3.8608 train_time:331569ms step_avg:179.81ms\nstep:1855/5000 train_loss:3.6813 train_time:331748ms step_avg:179.81ms\nstep:1856/5000 train_loss:3.8167 train_time:331927ms step_avg:179.81ms\nstep:1857/5000 train_loss:3.8656 train_time:332107ms step_avg:179.81ms\nstep:1858/5000 train_loss:3.7951 train_time:332286ms step_avg:179.81ms\nstep:1859/5000 train_loss:3.9522 train_time:332465ms step_avg:179.81ms\nstep:1860/5000 train_loss:3.9046 train_time:332645ms step_avg:179.81ms\nstep:1861/5000 train_loss:3.8544 train_time:332825ms step_avg:179.81ms\nstep:1862/5000 train_loss:3.8020 train_time:333006ms step_avg:179.81ms\nstep:1863/5000 train_loss:3.8378 train_time:333187ms step_avg:179.81ms\nstep:1864/5000 train_loss:3.8117 train_time:333365ms step_avg:179.81ms\nstep:1865/5000 train_loss:3.8905 train_time:333547ms step_avg:179.81ms\nstep:1866/5000 train_loss:3.8992 train_time:333724ms step_avg:179.81ms\nstep:1867/5000 train_loss:3.7713 train_time:333903ms step_avg:179.81ms\nstep:1868/5000 train_loss:3.8070 train_time:334082ms step_avg:179.81ms\nstep:1869/5000 train_loss:3.9406 train_time:334261ms step_avg:179.81ms\nstep:1870/5000 train_loss:3.9157 train_time:334441ms step_avg:179.81ms\nstep:1871/5000 train_loss:3.7163 train_time:334622ms step_avg:179.81ms\nstep:1872/5000 train_loss:3.7954 train_time:334801ms step_avg:179.81ms\nstep:1873/5000 train_loss:3.7889 train_time:334981ms step_avg:179.81ms\nstep:1874/5000 train_loss:3.8807 train_time:335160ms step_avg:179.81ms\nstep:1875/5000 train_loss:3.6871 train_time:335339ms step_avg:179.81ms\nstep:1876/5000 train_loss:3.9935 train_time:335517ms step_avg:179.81ms\nstep:1877/5000 train_loss:3.8353 train_time:335697ms step_avg:179.81ms\nstep:1878/5000 train_loss:3.7783 train_time:335876ms step_avg:179.80ms\nstep:1879/5000 train_loss:3.8085 train_time:336054ms step_avg:179.80ms\nstep:1880/5000 train_loss:3.8378 train_time:336233ms step_avg:179.80ms\nstep:1881/5000 train_loss:3.9125 train_time:336411ms step_avg:179.80ms\nstep:1882/5000 train_loss:3.8573 train_time:336590ms step_avg:179.80ms\nstep:1883/5000 train_loss:3.9492 train_time:336768ms step_avg:179.80ms\nstep:1884/5000 train_loss:3.6927 train_time:336949ms step_avg:179.80ms\nstep:1885/5000 train_loss:3.6514 train_time:337128ms step_avg:179.80ms\nstep:1886/5000 train_loss:3.9056 train_time:337307ms step_avg:179.80ms\nstep:1887/5000 train_loss:3.9427 train_time:337487ms step_avg:179.80ms\nstep:1888/5000 train_loss:3.9046 train_time:337666ms step_avg:179.80ms\nstep:1889/5000 train_loss:3.8390 train_time:337848ms step_avg:179.80ms\nstep:1890/5000 train_loss:3.8624 train_time:338030ms step_avg:179.80ms\nstep:1891/5000 train_loss:3.7604 train_time:338209ms step_avg:179.80ms\nstep:1892/5000 train_loss:3.7637 train_time:338387ms step_avg:179.80ms\nstep:1893/5000 train_loss:3.8382 train_time:338565ms step_avg:179.80ms\nstep:1894/5000 train_loss:3.8141 train_time:338746ms step_avg:179.80ms\nstep:1895/5000 train_loss:3.8896 train_time:338924ms step_avg:179.80ms\nstep:1896/5000 train_loss:3.8845 train_time:339104ms step_avg:179.80ms\nstep:1897/5000 train_loss:3.7584 train_time:339285ms step_avg:179.80ms\nstep:1898/5000 train_loss:3.6713 train_time:339465ms step_avg:179.80ms\nstep:1899/5000 train_loss:3.8505 train_time:339644ms step_avg:179.80ms\nstep:1900/5000 train_loss:3.9407 train_time:339823ms step_avg:179.80ms\nstep:1901/5000 train_loss:3.8472 train_time:340003ms step_avg:179.80ms\nstep:1902/5000 train_loss:4.1141 train_time:340182ms step_avg:179.80ms\nstep:1903/5000 train_loss:3.7595 train_time:340361ms step_avg:179.80ms\nstep:1904/5000 train_loss:4.1200 train_time:340540ms step_avg:179.80ms\nstep:1905/5000 train_loss:3.8931 train_time:340719ms step_avg:179.80ms\nstep:1906/5000 train_loss:3.8610 train_time:340900ms step_avg:179.80ms\nstep:1907/5000 train_loss:4.0845 train_time:341080ms step_avg:179.80ms\nstep:1908/5000 train_loss:3.9069 train_time:341260ms step_avg:179.80ms\nstep:1909/5000 train_loss:3.6270 train_time:341440ms step_avg:179.80ms\nstep:1910/5000 train_loss:3.6871 train_time:341620ms step_avg:179.80ms\nstep:1911/5000 train_loss:3.7232 train_time:341799ms step_avg:179.80ms\nstep:1912/5000 train_loss:3.8823 train_time:341978ms step_avg:179.80ms\nstep:1913/5000 train_loss:3.8057 train_time:342156ms step_avg:179.80ms\nstep:1914/5000 train_loss:3.9036 train_time:342335ms step_avg:179.80ms\nstep:1915/5000 train_loss:3.8057 train_time:342513ms step_avg:179.80ms\nstep:1916/5000 train_loss:3.8306 train_time:342692ms step_avg:179.80ms\nstep:1917/5000 train_loss:3.8594 train_time:342871ms step_avg:179.80ms\nstep:1918/5000 train_loss:3.8623 train_time:343050ms step_avg:179.80ms\nstep:1919/5000 train_loss:3.7658 train_time:343229ms step_avg:179.80ms\nstep:1920/5000 train_loss:3.7641 train_time:343409ms step_avg:179.80ms\nstep:1921/5000 train_loss:3.8971 train_time:343590ms step_avg:179.80ms\nstep:1922/5000 train_loss:4.0104 train_time:343768ms step_avg:179.80ms\nstep:1923/5000 train_loss:3.8053 train_time:343948ms step_avg:179.79ms\nstep:1924/5000 train_loss:3.8127 train_time:344128ms step_avg:179.79ms\nstep:1925/5000 train_loss:4.0141 train_time:344308ms step_avg:179.80ms\nstep:1926/5000 train_loss:3.8234 train_time:344487ms step_avg:179.79ms\nstep:1927/5000 train_loss:3.7585 train_time:344666ms step_avg:179.79ms\nstep:1928/5000 train_loss:3.9425 train_time:344846ms step_avg:179.79ms\nstep:1929/5000 train_loss:3.7621 train_time:345026ms step_avg:179.79ms\nstep:1930/5000 train_loss:3.6164 train_time:345205ms step_avg:179.79ms\nstep:1931/5000 train_loss:3.7307 train_time:345384ms step_avg:179.79ms\nstep:1932/5000 train_loss:3.8437 train_time:345564ms step_avg:179.79ms\nstep:1933/5000 train_loss:3.6641 train_time:345744ms step_avg:179.79ms\nstep:1934/5000 train_loss:3.9004 train_time:345924ms step_avg:179.79ms\nstep:1935/5000 train_loss:4.0077 train_time:346104ms step_avg:179.79ms\nstep:1936/5000 train_loss:3.9131 train_time:346289ms step_avg:179.80ms\nstep:1937/5000 train_loss:3.7889 train_time:346467ms step_avg:179.80ms\nstep:1938/5000 train_loss:3.7650 train_time:346646ms step_avg:179.80ms\nstep:1939/5000 train_loss:3.8326 train_time:346825ms step_avg:179.80ms\nstep:1940/5000 train_loss:3.8565 train_time:347006ms step_avg:179.80ms\nstep:1941/5000 train_loss:3.8989 train_time:347186ms step_avg:179.80ms\nstep:1942/5000 train_loss:3.7632 train_time:347366ms step_avg:179.80ms\nstep:1943/5000 train_loss:3.7569 train_time:347545ms step_avg:179.80ms\nstep:1944/5000 train_loss:3.6917 train_time:347724ms step_avg:179.80ms\nstep:1945/5000 train_loss:3.8704 train_time:347904ms step_avg:179.80ms\nstep:1946/5000 train_loss:3.6929 train_time:348083ms step_avg:179.79ms\nstep:1947/5000 train_loss:3.8452 train_time:348262ms step_avg:179.79ms\nstep:1948/5000 train_loss:3.8303 train_time:348442ms step_avg:179.79ms\nstep:1949/5000 train_loss:3.7811 train_time:348622ms step_avg:179.79ms\nstep:1950/5000 train_loss:3.7677 train_time:348802ms step_avg:179.79ms\nstep:1951/5000 train_loss:3.7686 train_time:348981ms step_avg:179.79ms\nstep:1952/5000 train_loss:3.8342 train_time:349161ms step_avg:179.79ms\nstep:1953/5000 train_loss:3.7449 train_time:349341ms step_avg:179.79ms\nstep:1954/5000 train_loss:3.9714 train_time:349521ms step_avg:179.79ms\nstep:1955/5000 train_loss:3.7717 train_time:349700ms step_avg:179.79ms\nstep:1956/5000 train_loss:3.8330 train_time:349880ms step_avg:179.79ms\nstep:1957/5000 train_loss:3.7163 train_time:350059ms step_avg:179.79ms\nstep:1958/5000 train_loss:3.8159 train_time:350238ms step_avg:179.79ms\nstep:1959/5000 train_loss:3.8730 train_time:350418ms step_avg:179.79ms\nstep:1960/5000 train_loss:4.0493 train_time:350598ms step_avg:179.79ms\nstep:1961/5000 train_loss:3.7538 train_time:350775ms step_avg:179.79ms\nstep:1962/5000 train_loss:3.6945 train_time:350954ms step_avg:179.79ms\nstep:1963/5000 train_loss:3.6673 train_time:351132ms step_avg:179.79ms\nstep:1964/5000 train_loss:3.7917 train_time:351311ms step_avg:179.79ms\nstep:1965/5000 train_loss:3.8822 train_time:351491ms step_avg:179.79ms\nstep:1966/5000 train_loss:3.7142 train_time:351671ms step_avg:179.79ms\nstep:1967/5000 train_loss:4.0456 train_time:351850ms step_avg:179.79ms\nstep:1968/5000 train_loss:3.7303 train_time:352028ms step_avg:179.79ms\nstep:1969/5000 train_loss:3.7445 train_time:352209ms step_avg:179.79ms\nstep:1970/5000 train_loss:3.8318 train_time:352389ms step_avg:179.79ms\nstep:1971/5000 train_loss:3.8855 train_time:352568ms step_avg:179.79ms\nstep:1972/5000 train_loss:3.8688 train_time:352749ms step_avg:179.79ms\nstep:1973/5000 train_loss:3.9083 train_time:352928ms step_avg:179.79ms\nstep:1974/5000 train_loss:3.6719 train_time:353108ms step_avg:179.79ms\nstep:1975/5000 train_loss:3.7868 train_time:353287ms step_avg:179.79ms\nstep:1976/5000 train_loss:3.6392 train_time:353469ms step_avg:179.79ms\nstep:1977/5000 train_loss:3.8960 train_time:353648ms step_avg:179.79ms\nstep:1978/5000 train_loss:3.8074 train_time:353828ms step_avg:179.79ms\nstep:1979/5000 train_loss:3.7783 train_time:354008ms step_avg:179.79ms\nstep:1980/5000 train_loss:3.8188 train_time:354190ms step_avg:179.79ms\nstep:1981/5000 train_loss:3.8269 train_time:354369ms step_avg:179.79ms\nstep:1982/5000 train_loss:3.7516 train_time:354550ms step_avg:179.79ms\nstep:1983/5000 train_loss:3.8195 train_time:354730ms step_avg:179.79ms\nstep:1984/5000 train_loss:3.9895 train_time:354909ms step_avg:179.79ms\nstep:1985/5000 train_loss:3.8819 train_time:355087ms step_avg:179.79ms\nstep:1986/5000 train_loss:3.6378 train_time:355266ms step_avg:179.79ms\nstep:1987/5000 train_loss:3.7571 train_time:355445ms step_avg:179.79ms\nstep:1988/5000 train_loss:3.8273 train_time:355625ms step_avg:179.79ms\nstep:1989/5000 train_loss:3.8087 train_time:355807ms step_avg:179.79ms\nstep:1990/5000 train_loss:3.8347 train_time:355988ms step_avg:179.79ms\nstep:1991/5000 train_loss:3.8226 train_time:356167ms step_avg:179.79ms\nstep:1992/5000 train_loss:3.8047 train_time:356347ms step_avg:179.79ms\nstep:1993/5000 train_loss:4.2391 train_time:356528ms step_avg:179.79ms\nstep:1994/5000 train_loss:3.8978 train_time:356708ms step_avg:179.79ms\nstep:1995/5000 train_loss:3.7307 train_time:356887ms step_avg:179.79ms\nstep:1996/5000 train_loss:3.7984 train_time:357066ms step_avg:179.79ms\nstep:1997/5000 train_loss:3.7226 train_time:357245ms step_avg:179.79ms\nstep:1998/5000 train_loss:3.9300 train_time:357424ms step_avg:179.79ms\nstep:1999/5000 train_loss:3.9352 train_time:357604ms step_avg:179.79ms\nstep:2000/5000 train_loss:3.8984 train_time:357784ms step_avg:179.79ms\nstep:2001/5000 train_loss:3.8455 train_time:357967ms step_avg:179.79ms\nstep:2002/5000 train_loss:3.8467 train_time:358146ms step_avg:179.79ms\nstep:2003/5000 train_loss:3.8654 train_time:358325ms step_avg:179.79ms\nstep:2004/5000 train_loss:3.7914 train_time:358506ms step_avg:179.79ms\nstep:2005/5000 train_loss:3.7946 train_time:358684ms step_avg:179.79ms\nstep:2006/5000 train_loss:3.8996 train_time:358864ms step_avg:179.79ms\nstep:2007/5000 train_loss:3.6723 train_time:359043ms step_avg:179.79ms\nstep:2008/5000 train_loss:3.7271 train_time:359223ms step_avg:179.79ms\nstep:2009/5000 train_loss:4.3432 train_time:359403ms step_avg:179.79ms\nstep:2010/5000 train_loss:3.8429 train_time:359583ms step_avg:179.79ms\nstep:2011/5000 train_loss:3.8454 train_time:359763ms step_avg:179.79ms\nstep:2012/5000 train_loss:3.8466 train_time:359943ms step_avg:179.79ms\nstep:2013/5000 train_loss:3.8838 train_time:360127ms step_avg:179.79ms\nstep:2014/5000 train_loss:3.8797 train_time:360310ms step_avg:179.80ms\nstep:2015/5000 train_loss:3.7457 train_time:360491ms step_avg:179.80ms\nstep:2016/5000 train_loss:3.7054 train_time:360670ms step_avg:179.80ms\nstep:2017/5000 train_loss:3.7392 train_time:360848ms step_avg:179.79ms\nstep:2018/5000 train_loss:3.8201 train_time:361028ms step_avg:179.79ms\nstep:2019/5000 train_loss:3.8836 train_time:361209ms step_avg:179.80ms\nstep:2020/5000 train_loss:4.1126 train_time:361390ms step_avg:179.80ms\nstep:2021/5000 train_loss:3.6824 train_time:361570ms step_avg:179.80ms\nstep:2022/5000 train_loss:3.9197 train_time:361749ms step_avg:179.80ms\nstep:2023/5000 train_loss:3.6976 train_time:361928ms step_avg:179.80ms\nstep:2024/5000 train_loss:3.7782 train_time:362109ms step_avg:179.80ms\nstep:2025/5000 train_loss:3.7538 train_time:362289ms step_avg:179.80ms\nstep:2026/5000 train_loss:3.7435 train_time:362469ms step_avg:179.80ms\nstep:2027/5000 train_loss:3.7733 train_time:362649ms step_avg:179.80ms\nstep:2028/5000 train_loss:3.8635 train_time:362827ms step_avg:179.80ms\nstep:2029/5000 train_loss:3.6744 train_time:363007ms step_avg:179.80ms\nstep:2030/5000 train_loss:3.8647 train_time:363188ms step_avg:179.80ms\nstep:2031/5000 train_loss:3.8602 train_time:363366ms step_avg:179.80ms\nstep:2032/5000 train_loss:4.0482 train_time:363546ms step_avg:179.80ms\nstep:2033/5000 train_loss:3.9439 train_time:363728ms step_avg:179.80ms\nstep:2034/5000 train_loss:3.9313 train_time:363909ms step_avg:179.80ms\nstep:2035/5000 train_loss:3.7333 train_time:364088ms step_avg:179.80ms\nstep:2036/5000 train_loss:3.7439 train_time:364268ms step_avg:179.80ms\nstep:2037/5000 train_loss:3.7809 train_time:364448ms step_avg:179.80ms\nstep:2038/5000 train_loss:3.8706 train_time:364630ms step_avg:179.80ms\nstep:2039/5000 train_loss:3.9363 train_time:364810ms step_avg:179.80ms\nstep:2040/5000 train_loss:3.8287 train_time:364990ms step_avg:179.80ms\nstep:2041/5000 train_loss:3.8183 train_time:365169ms step_avg:179.80ms\nstep:2042/5000 train_loss:3.6846 train_time:365349ms step_avg:179.80ms\nstep:2043/5000 train_loss:3.7873 train_time:365528ms step_avg:179.80ms\nstep:2044/5000 train_loss:3.8450 train_time:365711ms step_avg:179.80ms\nstep:2045/5000 train_loss:3.7738 train_time:365892ms step_avg:179.80ms\nstep:2046/5000 train_loss:3.8778 train_time:366071ms step_avg:179.80ms\nstep:2047/5000 train_loss:3.7946 train_time:366249ms step_avg:179.80ms\nstep:2048/5000 train_loss:3.6748 train_time:366428ms step_avg:179.80ms\nstep:2049/5000 train_loss:3.7598 train_time:366608ms step_avg:179.80ms\nstep:2050/5000 train_loss:3.8140 train_time:366789ms step_avg:179.80ms\nstep:2051/5000 train_loss:3.7708 train_time:366969ms step_avg:179.80ms\nstep:2052/5000 train_loss:3.6779 train_time:367149ms step_avg:179.80ms\nstep:2053/5000 train_loss:3.9001 train_time:367328ms step_avg:179.80ms\nstep:2054/5000 train_loss:3.7859 train_time:367508ms step_avg:179.80ms\nstep:2055/5000 train_loss:3.7633 train_time:367687ms step_avg:179.80ms\nstep:2056/5000 train_loss:3.8799 train_time:367866ms step_avg:179.80ms\nstep:2057/5000 train_loss:3.7747 train_time:368049ms step_avg:179.80ms\nstep:2058/5000 train_loss:3.8752 train_time:368227ms step_avg:179.80ms\nstep:2059/5000 train_loss:3.8084 train_time:368407ms step_avg:179.80ms\nstep:2060/5000 train_loss:4.0291 train_time:368587ms step_avg:179.80ms\nstep:2061/5000 train_loss:3.9390 train_time:368767ms step_avg:179.80ms\nstep:2062/5000 train_loss:3.8429 train_time:368945ms step_avg:179.80ms\nstep:2063/5000 train_loss:3.7597 train_time:369125ms step_avg:179.80ms\nstep:2064/5000 train_loss:3.8830 train_time:369305ms step_avg:179.80ms\nstep:2065/5000 train_loss:3.8706 train_time:369485ms step_avg:179.80ms\nstep:2066/5000 train_loss:4.2012 train_time:369665ms step_avg:179.80ms\nstep:2067/5000 train_loss:4.0048 train_time:369844ms step_avg:179.80ms\nstep:2068/5000 train_loss:3.7273 train_time:370024ms step_avg:179.80ms\nstep:2069/5000 train_loss:3.8249 train_time:370205ms step_avg:179.80ms\nstep:2070/5000 train_loss:3.7934 train_time:370385ms step_avg:179.80ms\nstep:2071/5000 train_loss:3.8111 train_time:370567ms step_avg:179.80ms\nstep:2072/5000 train_loss:3.8924 train_time:370746ms step_avg:179.80ms\nstep:2073/5000 train_loss:3.9098 train_time:370925ms step_avg:179.80ms\nstep:2074/5000 train_loss:3.8992 train_time:371106ms step_avg:179.80ms\nstep:2075/5000 train_loss:3.6776 train_time:371287ms step_avg:179.80ms\nstep:2076/5000 train_loss:3.6366 train_time:371466ms step_avg:179.80ms\nstep:2077/5000 train_loss:3.9674 train_time:371645ms step_avg:179.80ms\nstep:2078/5000 train_loss:3.7880 train_time:371826ms step_avg:179.80ms\nstep:2079/5000 train_loss:3.7725 train_time:372008ms step_avg:179.80ms\nstep:2080/5000 train_loss:3.8864 train_time:372188ms step_avg:179.80ms\nstep:2081/5000 train_loss:3.7811 train_time:372366ms step_avg:179.80ms\nstep:2082/5000 train_loss:3.8101 train_time:372545ms step_avg:179.80ms\nstep:2083/5000 train_loss:3.7974 train_time:372725ms step_avg:179.80ms\nstep:2084/5000 train_loss:3.7629 train_time:372906ms step_avg:179.80ms\nstep:2085/5000 train_loss:3.7809 train_time:373086ms step_avg:179.80ms\nstep:2086/5000 train_loss:3.7542 train_time:373264ms step_avg:179.80ms\nstep:2087/5000 train_loss:3.8184 train_time:373442ms step_avg:179.80ms\nstep:2088/5000 train_loss:3.6510 train_time:373623ms step_avg:179.80ms\nstep:2089/5000 train_loss:3.8295 train_time:373803ms step_avg:179.80ms\nstep:2090/5000 train_loss:3.7583 train_time:373985ms step_avg:179.80ms\nstep:2091/5000 train_loss:3.8118 train_time:374165ms step_avg:179.80ms\nstep:2092/5000 train_loss:3.9048 train_time:374344ms step_avg:179.80ms\nstep:2093/5000 train_loss:3.9474 train_time:374521ms step_avg:179.80ms\nstep:2094/5000 train_loss:4.3992 train_time:374701ms step_avg:179.80ms\nstep:2095/5000 train_loss:3.7102 train_time:374881ms step_avg:179.80ms\nstep:2096/5000 train_loss:3.7101 train_time:375061ms step_avg:179.80ms\nstep:2097/5000 train_loss:3.5582 train_time:375241ms step_avg:179.80ms\nstep:2098/5000 train_loss:3.8483 train_time:375421ms step_avg:179.80ms\nstep:2099/5000 train_loss:3.7841 train_time:375600ms step_avg:179.80ms\nstep:2100/5000 train_loss:3.7793 train_time:375779ms step_avg:179.80ms\nstep:2101/5000 train_loss:3.7665 train_time:375960ms step_avg:179.80ms\nstep:2102/5000 train_loss:3.7865 train_time:376139ms step_avg:179.80ms\nstep:2103/5000 train_loss:3.7492 train_time:376319ms step_avg:179.80ms\nstep:2104/5000 train_loss:3.8007 train_time:376497ms step_avg:179.80ms\nstep:2105/5000 train_loss:3.6208 train_time:376675ms step_avg:179.80ms\nstep:2106/5000 train_loss:3.8026 train_time:376854ms step_avg:179.80ms\nstep:2107/5000 train_loss:3.9671 train_time:377032ms step_avg:179.80ms\nstep:2108/5000 train_loss:4.0621 train_time:377212ms step_avg:179.80ms\nstep:2109/5000 train_loss:3.7067 train_time:377390ms step_avg:179.80ms\nstep:2110/5000 train_loss:3.7616 train_time:377569ms step_avg:179.79ms\nstep:2111/5000 train_loss:3.7575 train_time:377748ms step_avg:179.79ms\nstep:2112/5000 train_loss:3.7905 train_time:377927ms step_avg:179.79ms\nstep:2113/5000 train_loss:3.8182 train_time:378107ms step_avg:179.79ms\nstep:2114/5000 train_loss:3.7325 train_time:378287ms step_avg:179.79ms\nstep:2115/5000 train_loss:3.8814 train_time:378468ms step_avg:179.79ms\nstep:2116/5000 train_loss:3.7639 train_time:378647ms step_avg:179.79ms\nstep:2117/5000 train_loss:3.7872 train_time:378827ms step_avg:179.79ms\nstep:2118/5000 train_loss:3.9412 train_time:379006ms step_avg:179.79ms\nstep:2119/5000 train_loss:3.9193 train_time:379187ms step_avg:179.79ms\nstep:2120/5000 train_loss:3.7676 train_time:379367ms step_avg:179.79ms\nstep:2121/5000 train_loss:3.8397 train_time:379547ms step_avg:179.79ms\nstep:2122/5000 train_loss:3.7471 train_time:379727ms step_avg:179.79ms\nstep:2123/5000 train_loss:3.7896 train_time:379908ms step_avg:179.80ms\nstep:2124/5000 train_loss:3.8815 train_time:380089ms step_avg:179.80ms\nstep:2125/5000 train_loss:3.6862 train_time:380269ms step_avg:179.80ms\nstep:2126/5000 train_loss:4.0199 train_time:380448ms step_avg:179.80ms\nstep:2127/5000 train_loss:3.6577 train_time:380627ms step_avg:179.80ms\nstep:2128/5000 train_loss:3.8356 train_time:380808ms step_avg:179.80ms\nstep:2129/5000 train_loss:3.5971 train_time:380986ms step_avg:179.80ms\nstep:2130/5000 train_loss:3.8329 train_time:381166ms step_avg:179.80ms\nstep:2131/5000 train_loss:3.9241 train_time:381349ms step_avg:179.80ms\nstep:2132/5000 train_loss:3.8254 train_time:381527ms step_avg:179.80ms\nstep:2133/5000 train_loss:3.7206 train_time:381708ms step_avg:179.80ms\nstep:2134/5000 train_loss:3.7626 train_time:381888ms step_avg:179.80ms\nstep:2135/5000 train_loss:3.9156 train_time:382067ms step_avg:179.80ms\nstep:2136/5000 train_loss:3.9158 train_time:382246ms step_avg:179.80ms\nstep:2137/5000 train_loss:3.7434 train_time:382425ms step_avg:179.80ms\nstep:2138/5000 train_loss:3.7885 train_time:382604ms step_avg:179.80ms\nstep:2139/5000 train_loss:3.8469 train_time:382784ms step_avg:179.80ms\nstep:2140/5000 train_loss:3.8661 train_time:382963ms step_avg:179.79ms\nstep:2141/5000 train_loss:3.8163 train_time:383143ms step_avg:179.79ms\nstep:2142/5000 train_loss:3.7238 train_time:383323ms step_avg:179.80ms\nstep:2143/5000 train_loss:3.9636 train_time:383503ms step_avg:179.80ms\nstep:2144/5000 train_loss:3.7518 train_time:383682ms step_avg:179.79ms\nstep:2145/5000 train_loss:4.0208 train_time:383862ms step_avg:179.79ms\nstep:2146/5000 train_loss:3.8547 train_time:384040ms step_avg:179.79ms\nstep:2147/5000 train_loss:3.6596 train_time:384219ms step_avg:179.79ms\nstep:2148/5000 train_loss:3.8021 train_time:384398ms step_avg:179.79ms\nstep:2149/5000 train_loss:3.6620 train_time:384577ms step_avg:179.79ms\nstep:2150/5000 train_loss:3.8429 train_time:384756ms step_avg:179.79ms\nstep:2151/5000 train_loss:3.4992 train_time:384935ms step_avg:179.79ms\nstep:2152/5000 train_loss:3.7156 train_time:385113ms step_avg:179.79ms\nstep:2153/5000 train_loss:3.8902 train_time:385292ms step_avg:179.79ms\nstep:2154/5000 train_loss:3.7357 train_time:385470ms step_avg:179.79ms\nstep:2155/5000 train_loss:3.6370 train_time:385650ms step_avg:179.79ms\nstep:2156/5000 train_loss:3.7325 train_time:385830ms step_avg:179.79ms\nstep:2157/5000 train_loss:3.8677 train_time:386009ms step_avg:179.79ms\nstep:2158/5000 train_loss:4.2387 train_time:386189ms step_avg:179.79ms\nstep:2159/5000 train_loss:3.7386 train_time:386368ms step_avg:179.79ms\nstep:2160/5000 train_loss:3.7940 train_time:386548ms step_avg:179.79ms\nstep:2161/5000 train_loss:3.9164 train_time:386728ms step_avg:179.79ms\nstep:2162/5000 train_loss:3.7863 train_time:386907ms step_avg:179.79ms\nstep:2163/5000 train_loss:3.8905 train_time:387086ms step_avg:179.79ms\nstep:2164/5000 train_loss:3.7410 train_time:387265ms step_avg:179.79ms\nstep:2165/5000 train_loss:3.5856 train_time:387444ms step_avg:179.79ms\nstep:2166/5000 train_loss:3.8895 train_time:387626ms step_avg:179.79ms\nstep:2167/5000 train_loss:4.2871 train_time:387806ms step_avg:179.79ms\nstep:2168/5000 train_loss:3.7545 train_time:387986ms step_avg:179.79ms\nstep:2169/5000 train_loss:3.8077 train_time:388166ms step_avg:179.79ms\nstep:2170/5000 train_loss:3.7171 train_time:388345ms step_avg:179.79ms\nstep:2171/5000 train_loss:3.6627 train_time:388525ms step_avg:179.79ms\nstep:2172/5000 train_loss:3.8196 train_time:388705ms step_avg:179.79ms\nstep:2173/5000 train_loss:3.9399 train_time:388885ms step_avg:179.79ms\nstep:2174/5000 train_loss:3.6953 train_time:389064ms step_avg:179.79ms\nstep:2175/5000 train_loss:3.8919 train_time:389244ms step_avg:179.79ms\nstep:2176/5000 train_loss:3.8021 train_time:389425ms step_avg:179.79ms\nstep:2177/5000 train_loss:3.8628 train_time:389604ms step_avg:179.79ms\nstep:2178/5000 train_loss:3.7824 train_time:389784ms step_avg:179.79ms\nstep:2179/5000 train_loss:3.7505 train_time:389966ms step_avg:179.79ms\nstep:2180/5000 train_loss:3.8416 train_time:390146ms step_avg:179.79ms\nstep:2181/5000 train_loss:3.9510 train_time:390326ms step_avg:179.79ms\nstep:2182/5000 train_loss:3.6970 train_time:390507ms step_avg:179.79ms\nstep:2183/5000 train_loss:3.6855 train_time:390686ms step_avg:179.79ms\nstep:2184/5000 train_loss:3.8711 train_time:390866ms step_avg:179.79ms\nstep:2185/5000 train_loss:3.7976 train_time:391045ms step_avg:179.79ms\nstep:2186/5000 train_loss:3.7549 train_time:391225ms step_avg:179.79ms\nstep:2187/5000 train_loss:3.7431 train_time:391404ms step_avg:179.79ms\nstep:2188/5000 train_loss:3.6990 train_time:391583ms step_avg:179.79ms\nstep:2189/5000 train_loss:3.7796 train_time:391762ms step_avg:179.79ms\nstep:2190/5000 train_loss:3.7546 train_time:391942ms step_avg:179.79ms\nstep:2191/5000 train_loss:3.9045 train_time:392122ms step_avg:179.79ms\nstep:2192/5000 train_loss:3.9628 train_time:392303ms step_avg:179.79ms\nstep:2193/5000 train_loss:3.6649 train_time:392482ms step_avg:179.79ms\nstep:2194/5000 train_loss:3.7341 train_time:392660ms step_avg:179.79ms\nstep:2195/5000 train_loss:3.6285 train_time:392839ms step_avg:179.79ms\nstep:2196/5000 train_loss:3.4324 train_time:393019ms step_avg:179.79ms\nstep:2197/5000 train_loss:3.7626 train_time:393197ms step_avg:179.79ms\nstep:2198/5000 train_loss:3.8224 train_time:393376ms step_avg:179.79ms\nstep:2199/5000 train_loss:3.6522 train_time:393557ms step_avg:179.79ms\nstep:2200/5000 train_loss:3.8704 train_time:393745ms step_avg:179.79ms\nstep:2201/5000 train_loss:3.6539 train_time:393920ms step_avg:179.79ms\nstep:2202/5000 train_loss:3.7214 train_time:394098ms step_avg:179.79ms\nstep:2203/5000 train_loss:3.6937 train_time:394277ms step_avg:179.79ms\nstep:2204/5000 train_loss:3.8309 train_time:394456ms step_avg:179.79ms\nstep:2205/5000 train_loss:3.8078 train_time:394635ms step_avg:179.79ms\nstep:2206/5000 train_loss:3.7377 train_time:394814ms step_avg:179.79ms\nstep:2207/5000 train_loss:3.6944 train_time:394992ms step_avg:179.79ms\nstep:2208/5000 train_loss:3.7608 train_time:395170ms step_avg:179.79ms\nstep:2209/5000 train_loss:3.7385 train_time:395350ms step_avg:179.79ms\nstep:2210/5000 train_loss:4.0655 train_time:395529ms step_avg:179.79ms\nstep:2211/5000 train_loss:3.8167 train_time:395711ms step_avg:179.79ms\nstep:2212/5000 train_loss:3.8839 train_time:395890ms step_avg:179.79ms\nstep:2213/5000 train_loss:3.8220 train_time:396070ms step_avg:179.79ms\nstep:2214/5000 train_loss:3.8715 train_time:396250ms step_avg:179.79ms\nstep:2215/5000 train_loss:3.8548 train_time:396429ms step_avg:179.79ms\nstep:2216/5000 train_loss:3.7860 train_time:396609ms step_avg:179.79ms\nstep:2217/5000 train_loss:3.7141 train_time:396788ms step_avg:179.79ms\nstep:2218/5000 train_loss:3.8270 train_time:396968ms step_avg:179.79ms\nstep:2219/5000 train_loss:3.7635 train_time:397147ms step_avg:179.79ms\nstep:2220/5000 train_loss:3.7824 train_time:397327ms step_avg:179.79ms\nstep:2221/5000 train_loss:3.6044 train_time:397506ms step_avg:179.79ms\nstep:2222/5000 train_loss:3.7610 train_time:397686ms step_avg:179.79ms\nstep:2223/5000 train_loss:3.8111 train_time:397867ms step_avg:179.79ms\nstep:2224/5000 train_loss:3.9052 train_time:398048ms step_avg:179.79ms\nstep:2225/5000 train_loss:3.6982 train_time:398228ms step_avg:179.79ms\nstep:2226/5000 train_loss:3.8036 train_time:398406ms step_avg:179.79ms\nstep:2227/5000 train_loss:3.8054 train_time:398585ms step_avg:179.79ms\nstep:2228/5000 train_loss:3.8144 train_time:398765ms step_avg:179.79ms\nstep:2229/5000 train_loss:3.7074 train_time:398946ms step_avg:179.79ms\nstep:2230/5000 train_loss:3.4703 train_time:399128ms step_avg:179.79ms\nstep:2231/5000 train_loss:3.9088 train_time:399307ms step_avg:179.79ms\nstep:2232/5000 train_loss:3.7380 train_time:399486ms step_avg:179.79ms\nstep:2233/5000 train_loss:3.6756 train_time:399664ms step_avg:179.79ms\nstep:2234/5000 train_loss:3.7223 train_time:399843ms step_avg:179.79ms\nstep:2235/5000 train_loss:3.7396 train_time:400023ms step_avg:179.79ms\nstep:2236/5000 train_loss:3.8218 train_time:400203ms step_avg:179.79ms\nstep:2237/5000 train_loss:3.8932 train_time:400383ms step_avg:179.79ms\nstep:2238/5000 train_loss:3.7714 train_time:400564ms step_avg:179.79ms\nstep:2239/5000 train_loss:3.6996 train_time:400746ms step_avg:179.79ms\nstep:2240/5000 train_loss:3.7194 train_time:400927ms step_avg:179.79ms\nstep:2241/5000 train_loss:3.9172 train_time:401107ms step_avg:179.79ms\nstep:2242/5000 train_loss:3.8062 train_time:401287ms step_avg:179.79ms\nstep:2243/5000 train_loss:3.9909 train_time:401465ms step_avg:179.79ms\nstep:2244/5000 train_loss:3.7751 train_time:401644ms step_avg:179.79ms\nstep:2245/5000 train_loss:3.6983 train_time:401824ms step_avg:179.79ms\nstep:2246/5000 train_loss:3.7161 train_time:402006ms step_avg:179.79ms\nstep:2247/5000 train_loss:3.7727 train_time:402186ms step_avg:179.79ms\nstep:2248/5000 train_loss:3.7766 train_time:402366ms step_avg:179.79ms\nstep:2249/5000 train_loss:3.7647 train_time:402547ms step_avg:179.79ms\nstep:2250/5000 train_loss:3.6606 train_time:402727ms step_avg:179.79ms\nstep:2251/5000 train_loss:3.6759 train_time:402909ms step_avg:179.79ms\nstep:2252/5000 train_loss:3.5952 train_time:403088ms step_avg:179.79ms\nstep:2253/5000 train_loss:3.8613 train_time:403268ms step_avg:179.79ms\nstep:2254/5000 train_loss:3.8711 train_time:403448ms step_avg:179.79ms\nstep:2255/5000 train_loss:3.7345 train_time:403626ms step_avg:179.79ms\nstep:2256/5000 train_loss:3.8502 train_time:403806ms step_avg:179.79ms\nstep:2257/5000 train_loss:3.6912 train_time:403987ms step_avg:179.79ms\nstep:2258/5000 train_loss:3.6821 train_time:404167ms step_avg:179.79ms\nstep:2259/5000 train_loss:3.8249 train_time:404346ms step_avg:179.79ms\nstep:2260/5000 train_loss:3.7266 train_time:404527ms step_avg:179.79ms\nstep:2261/5000 train_loss:3.9263 train_time:404706ms step_avg:179.79ms\nstep:2262/5000 train_loss:3.7977 train_time:404885ms step_avg:179.79ms\nstep:2263/5000 train_loss:3.7330 train_time:405065ms step_avg:179.79ms\nstep:2264/5000 train_loss:3.7587 train_time:405246ms step_avg:179.79ms\nstep:2265/5000 train_loss:3.9218 train_time:405424ms step_avg:179.79ms\nstep:2266/5000 train_loss:3.7347 train_time:405604ms step_avg:179.79ms\nstep:2267/5000 train_loss:3.6942 train_time:405784ms step_avg:179.79ms\nstep:2268/5000 train_loss:3.8286 train_time:405963ms step_avg:179.79ms\nstep:2269/5000 train_loss:3.7375 train_time:406143ms step_avg:179.79ms\nstep:2270/5000 train_loss:3.7068 train_time:406322ms step_avg:179.79ms\nstep:2271/5000 train_loss:3.6835 train_time:406500ms step_avg:179.79ms\nstep:2272/5000 train_loss:3.7927 train_time:406679ms step_avg:179.79ms\nstep:2273/5000 train_loss:3.7994 train_time:406859ms step_avg:179.79ms\nstep:2274/5000 train_loss:3.7986 train_time:407038ms step_avg:179.79ms\nstep:2275/5000 train_loss:3.8249 train_time:407217ms step_avg:179.79ms\nstep:2276/5000 train_loss:4.0979 train_time:407396ms step_avg:179.79ms\nstep:2277/5000 train_loss:3.8089 train_time:407576ms step_avg:179.79ms\nstep:2278/5000 train_loss:3.7582 train_time:407755ms step_avg:179.79ms\nstep:2279/5000 train_loss:3.8884 train_time:407934ms step_avg:179.79ms\nstep:2280/5000 train_loss:3.7991 train_time:408114ms step_avg:179.79ms\nstep:2281/5000 train_loss:3.6213 train_time:408293ms step_avg:179.79ms\nstep:2282/5000 train_loss:3.8030 train_time:408471ms step_avg:179.78ms\nstep:2283/5000 train_loss:3.8503 train_time:408650ms step_avg:179.78ms\nstep:2284/5000 train_loss:3.7725 train_time:408829ms step_avg:179.78ms\nstep:2285/5000 train_loss:3.7517 train_time:409010ms step_avg:179.78ms\nstep:2286/5000 train_loss:3.6949 train_time:409409ms step_avg:179.88ms\nstep:2287/5000 train_loss:3.7658 train_time:409586ms step_avg:179.88ms\nstep:2288/5000 train_loss:3.8780 train_time:409764ms step_avg:179.88ms\nstep:2289/5000 train_loss:3.7106 train_time:409943ms step_avg:179.88ms\nstep:2290/5000 train_loss:3.6387 train_time:410121ms step_avg:179.88ms\nstep:2291/5000 train_loss:3.7875 train_time:410305ms step_avg:179.88ms\nstep:2292/5000 train_loss:3.8360 train_time:410489ms step_avg:179.88ms\nstep:2293/5000 train_loss:3.7699 train_time:410668ms step_avg:179.88ms\nstep:2294/5000 train_loss:3.6524 train_time:410846ms step_avg:179.88ms\nstep:2295/5000 train_loss:3.7624 train_time:411024ms step_avg:179.88ms\nstep:2296/5000 train_loss:3.8530 train_time:411204ms step_avg:179.88ms\nstep:2297/5000 train_loss:3.9139 train_time:411384ms step_avg:179.88ms\nstep:2298/5000 train_loss:4.1188 train_time:411568ms step_avg:179.88ms\nstep:2299/5000 train_loss:3.7806 train_time:411749ms step_avg:179.88ms\nstep:2300/5000 train_loss:3.8908 train_time:411927ms step_avg:179.88ms\nstep:2301/5000 train_loss:3.7577 train_time:412107ms step_avg:179.88ms\nstep:2302/5000 train_loss:3.6705 train_time:412287ms step_avg:179.88ms\nstep:2303/5000 train_loss:3.6608 train_time:412467ms step_avg:179.88ms\nstep:2304/5000 train_loss:3.8510 train_time:412647ms step_avg:179.88ms\nstep:2305/5000 train_loss:3.8237 train_time:412828ms step_avg:179.88ms\nstep:2306/5000 train_loss:3.7451 train_time:413007ms step_avg:179.88ms\nstep:2307/5000 train_loss:3.8259 train_time:413189ms step_avg:179.88ms\nstep:2308/5000 train_loss:3.8721 train_time:413367ms step_avg:179.88ms\nstep:2309/5000 train_loss:3.7030 train_time:413546ms step_avg:179.88ms\nstep:2310/5000 train_loss:3.8992 train_time:413724ms step_avg:179.88ms\nstep:2311/5000 train_loss:3.7592 train_time:413904ms step_avg:179.88ms\nstep:2312/5000 train_loss:3.7962 train_time:414082ms step_avg:179.88ms\nstep:2313/5000 train_loss:3.8788 train_time:414261ms step_avg:179.88ms\nstep:2314/5000 train_loss:3.6878 train_time:414439ms step_avg:179.88ms\nstep:2315/5000 train_loss:3.6810 train_time:414619ms step_avg:179.88ms\nstep:2316/5000 train_loss:3.7946 train_time:414799ms step_avg:179.88ms\nstep:2317/5000 train_loss:3.8591 train_time:414978ms step_avg:179.88ms\nstep:2318/5000 train_loss:3.7287 train_time:415157ms step_avg:179.88ms\nstep:2319/5000 train_loss:3.9894 train_time:415336ms step_avg:179.88ms\nstep:2320/5000 train_loss:3.7700 train_time:415515ms step_avg:179.88ms\nstep:2321/5000 train_loss:3.8549 train_time:415693ms step_avg:179.88ms\nstep:2322/5000 train_loss:3.8411 train_time:415871ms step_avg:179.87ms\nstep:2323/5000 train_loss:3.7281 train_time:416050ms step_avg:179.87ms\nstep:2324/5000 train_loss:3.7096 train_time:416231ms step_avg:179.88ms\nstep:2325/5000 train_loss:3.8387 train_time:416411ms step_avg:179.87ms\nstep:2326/5000 train_loss:3.8482 train_time:416590ms step_avg:179.87ms\nstep:2327/5000 train_loss:3.7267 train_time:416770ms step_avg:179.87ms\nstep:2328/5000 train_loss:3.7420 train_time:416948ms step_avg:179.87ms\nstep:2329/5000 train_loss:3.6867 train_time:417127ms step_avg:179.87ms\nstep:2330/5000 train_loss:3.7905 train_time:417306ms step_avg:179.87ms\nstep:2331/5000 train_loss:3.9245 train_time:417486ms step_avg:179.87ms\nstep:2332/5000 train_loss:3.9696 train_time:417665ms step_avg:179.87ms\nstep:2333/5000 train_loss:3.8105 train_time:417847ms step_avg:179.87ms\nstep:2334/5000 train_loss:3.9662 train_time:418027ms step_avg:179.87ms\nstep:2335/5000 train_loss:3.7068 train_time:418209ms step_avg:179.87ms\nstep:2336/5000 train_loss:3.7321 train_time:418388ms step_avg:179.87ms\nstep:2337/5000 train_loss:3.7242 train_time:418568ms step_avg:179.87ms\nstep:2338/5000 train_loss:3.7523 train_time:418748ms step_avg:179.87ms\nstep:2339/5000 train_loss:3.8386 train_time:418928ms step_avg:179.87ms\nstep:2340/5000 train_loss:3.6976 train_time:419106ms step_avg:179.87ms\nstep:2341/5000 train_loss:3.7667 train_time:419286ms step_avg:179.87ms\nstep:2342/5000 train_loss:3.7859 train_time:419465ms step_avg:179.87ms\nstep:2343/5000 train_loss:3.6432 train_time:419645ms step_avg:179.87ms\nstep:2344/5000 train_loss:3.8896 train_time:419823ms step_avg:179.87ms\nstep:2345/5000 train_loss:3.8035 train_time:420003ms step_avg:179.87ms\nstep:2346/5000 train_loss:3.8000 train_time:420183ms step_avg:179.87ms\nstep:2347/5000 train_loss:3.7464 train_time:420362ms step_avg:179.87ms\nstep:2348/5000 train_loss:3.6476 train_time:420543ms step_avg:179.87ms\nstep:2349/5000 train_loss:3.7317 train_time:420726ms step_avg:179.87ms\nstep:2350/5000 train_loss:4.1962 train_time:420906ms step_avg:179.87ms\nstep:2351/5000 train_loss:3.6899 train_time:421086ms step_avg:179.87ms\nstep:2352/5000 train_loss:3.7812 train_time:421265ms step_avg:179.87ms\nstep:2353/5000 train_loss:3.8206 train_time:421444ms step_avg:179.87ms\nstep:2354/5000 train_loss:3.6382 train_time:421624ms step_avg:179.87ms\nstep:2355/5000 train_loss:3.7259 train_time:421803ms step_avg:179.87ms\nstep:2356/5000 train_loss:3.7659 train_time:421984ms step_avg:179.87ms\nstep:2357/5000 train_loss:3.7965 train_time:422163ms step_avg:179.87ms\nstep:2358/5000 train_loss:3.9952 train_time:422342ms step_avg:179.87ms\nstep:2359/5000 train_loss:3.7666 train_time:422522ms step_avg:179.87ms\nstep:2360/5000 train_loss:4.1256 train_time:422702ms step_avg:179.87ms\nstep:2361/5000 train_loss:3.8264 train_time:422882ms step_avg:179.87ms\nstep:2362/5000 train_loss:3.8150 train_time:423062ms step_avg:179.87ms\nstep:2363/5000 train_loss:3.9239 train_time:423241ms step_avg:179.87ms\nstep:2364/5000 train_loss:3.9121 train_time:423421ms step_avg:179.87ms\nstep:2365/5000 train_loss:3.7851 train_time:423601ms step_avg:179.87ms\nstep:2366/5000 train_loss:3.6499 train_time:423781ms step_avg:179.87ms\nstep:2367/5000 train_loss:3.6491 train_time:423962ms step_avg:179.87ms\nstep:2368/5000 train_loss:3.6314 train_time:424141ms step_avg:179.87ms\nstep:2369/5000 train_loss:3.9479 train_time:424320ms step_avg:179.87ms\nstep:2370/5000 train_loss:3.7499 train_time:424501ms step_avg:179.87ms\nstep:2371/5000 train_loss:3.9018 train_time:424680ms step_avg:179.87ms\nstep:2372/5000 train_loss:3.6468 train_time:424859ms step_avg:179.87ms\nstep:2373/5000 train_loss:3.7621 train_time:425037ms step_avg:179.87ms\nstep:2374/5000 train_loss:3.8485 train_time:425216ms step_avg:179.87ms\nstep:2375/5000 train_loss:3.8309 train_time:425394ms step_avg:179.87ms\nstep:2376/5000 train_loss:3.6395 train_time:425573ms step_avg:179.87ms\nstep:2377/5000 train_loss:3.6618 train_time:425752ms step_avg:179.87ms\nstep:2378/5000 train_loss:3.7323 train_time:425930ms step_avg:179.87ms\nstep:2379/5000 train_loss:3.7901 train_time:426113ms step_avg:179.87ms\nstep:2380/5000 train_loss:3.6928 train_time:426292ms step_avg:179.87ms\nstep:2381/5000 train_loss:3.7279 train_time:426471ms step_avg:179.87ms\nstep:2382/5000 train_loss:3.7643 train_time:426650ms step_avg:179.87ms\nstep:2383/5000 train_loss:4.1597 train_time:426830ms step_avg:179.87ms\nstep:2384/5000 train_loss:4.0759 train_time:427010ms step_avg:179.87ms\nstep:2385/5000 train_loss:3.7760 train_time:427190ms step_avg:179.87ms\nstep:2386/5000 train_loss:3.7925 train_time:427369ms step_avg:179.87ms\nstep:2387/5000 train_loss:3.6321 train_time:427549ms step_avg:179.87ms\nstep:2388/5000 train_loss:3.7823 train_time:427729ms step_avg:179.87ms\nstep:2389/5000 train_loss:3.8449 train_time:427908ms step_avg:179.87ms\nstep:2390/5000 train_loss:3.8099 train_time:428086ms step_avg:179.87ms\nstep:2391/5000 train_loss:3.7856 train_time:428266ms step_avg:179.87ms\nstep:2392/5000 train_loss:3.7048 train_time:428447ms step_avg:179.87ms\nstep:2393/5000 train_loss:3.7594 train_time:428627ms step_avg:179.87ms\nstep:2394/5000 train_loss:3.8103 train_time:428808ms step_avg:179.87ms\nstep:2395/5000 train_loss:3.7908 train_time:428988ms step_avg:179.87ms\nstep:2396/5000 train_loss:3.7808 train_time:429168ms step_avg:179.87ms\nstep:2397/5000 train_loss:3.7884 train_time:429347ms step_avg:179.87ms\nstep:2398/5000 train_loss:3.9249 train_time:429526ms step_avg:179.87ms\nstep:2399/5000 train_loss:3.7916 train_time:429705ms step_avg:179.87ms\nstep:2400/5000 train_loss:3.6002 train_time:429889ms step_avg:179.87ms\nstep:2401/5000 train_loss:3.7565 train_time:430068ms step_avg:179.87ms\nstep:2402/5000 train_loss:3.7648 train_time:430247ms step_avg:179.87ms\nstep:2403/5000 train_loss:3.7080 train_time:430429ms step_avg:179.87ms\nstep:2404/5000 train_loss:3.7191 train_time:430608ms step_avg:179.87ms\nstep:2405/5000 train_loss:3.8468 train_time:430787ms step_avg:179.87ms\nstep:2406/5000 train_loss:3.7241 train_time:430969ms step_avg:179.87ms\nstep:2407/5000 train_loss:3.6888 train_time:431147ms step_avg:179.87ms\nstep:2408/5000 train_loss:3.7494 train_time:431326ms step_avg:179.87ms\nstep:2409/5000 train_loss:3.6697 train_time:431507ms step_avg:179.87ms\nstep:2410/5000 train_loss:3.8761 train_time:431688ms step_avg:179.87ms\nstep:2411/5000 train_loss:3.8096 train_time:431869ms step_avg:179.87ms\nstep:2412/5000 train_loss:3.7764 train_time:432048ms step_avg:179.87ms\nstep:2413/5000 train_loss:3.6810 train_time:432227ms step_avg:179.87ms\nstep:2414/5000 train_loss:3.6865 train_time:432406ms step_avg:179.87ms\nstep:2415/5000 train_loss:3.7782 train_time:432585ms step_avg:179.87ms\nstep:2416/5000 train_loss:3.9044 train_time:432765ms step_avg:179.87ms\nstep:2417/5000 train_loss:3.8317 train_time:432946ms step_avg:179.87ms\nstep:2418/5000 train_loss:3.7946 train_time:433127ms step_avg:179.87ms\nstep:2419/5000 train_loss:3.9008 train_time:433306ms step_avg:179.87ms\nstep:2420/5000 train_loss:3.7521 train_time:433486ms step_avg:179.87ms\nstep:2421/5000 train_loss:3.7926 train_time:433665ms step_avg:179.87ms\nstep:2422/5000 train_loss:4.0926 train_time:433846ms step_avg:179.87ms\nstep:2423/5000 train_loss:3.9059 train_time:434027ms step_avg:179.87ms\nstep:2424/5000 train_loss:3.7330 train_time:434205ms step_avg:179.87ms\nstep:2425/5000 train_loss:3.6432 train_time:434385ms step_avg:179.87ms\nstep:2426/5000 train_loss:3.8234 train_time:434564ms step_avg:179.87ms\nstep:2427/5000 train_loss:3.6108 train_time:434743ms step_avg:179.87ms\nstep:2428/5000 train_loss:3.6963 train_time:434923ms step_avg:179.87ms\nstep:2429/5000 train_loss:3.7086 train_time:435103ms step_avg:179.87ms\nstep:2430/5000 train_loss:3.8339 train_time:435280ms step_avg:179.87ms\nstep:2431/5000 train_loss:3.7642 train_time:435459ms step_avg:179.87ms\nstep:2432/5000 train_loss:3.9379 train_time:435638ms step_avg:179.87ms\nstep:2433/5000 train_loss:3.6732 train_time:435817ms step_avg:179.87ms\nstep:2434/5000 train_loss:3.7860 train_time:435996ms step_avg:179.87ms\nstep:2435/5000 train_loss:3.8202 train_time:436177ms step_avg:179.87ms\nstep:2436/5000 train_loss:3.7782 train_time:436356ms step_avg:179.87ms\nstep:2437/5000 train_loss:3.7410 train_time:436534ms step_avg:179.87ms\nstep:2438/5000 train_loss:3.7023 train_time:436712ms step_avg:179.86ms\nstep:2439/5000 train_loss:3.7011 train_time:436892ms step_avg:179.86ms\nstep:2440/5000 train_loss:3.7387 train_time:437071ms step_avg:179.86ms\nstep:2441/5000 train_loss:3.8064 train_time:437250ms step_avg:179.86ms\nstep:2442/5000 train_loss:3.7785 train_time:437430ms step_avg:179.86ms\nstep:2443/5000 train_loss:3.7869 train_time:437610ms step_avg:179.86ms\nstep:2444/5000 train_loss:3.8322 train_time:437788ms step_avg:179.86ms\nstep:2445/5000 train_loss:3.6992 train_time:437968ms step_avg:179.86ms\nstep:2446/5000 train_loss:3.7273 train_time:438148ms step_avg:179.86ms\nstep:2447/5000 train_loss:3.7614 train_time:438327ms step_avg:179.86ms\nstep:2448/5000 train_loss:3.7355 train_time:438507ms step_avg:179.86ms\nstep:2449/5000 train_loss:3.8115 train_time:438687ms step_avg:179.86ms\nstep:2450/5000 train_loss:3.8484 train_time:438869ms step_avg:179.86ms\nstep:2451/5000 train_loss:3.6599 train_time:439050ms step_avg:179.86ms\nstep:2452/5000 train_loss:3.7212 train_time:439229ms step_avg:179.86ms\nstep:2453/5000 train_loss:3.4941 train_time:439410ms step_avg:179.87ms\nstep:2454/5000 train_loss:3.6880 train_time:439589ms step_avg:179.86ms\nstep:2455/5000 train_loss:3.7322 train_time:439766ms step_avg:179.86ms\nstep:2456/5000 train_loss:3.8911 train_time:439946ms step_avg:179.86ms\nstep:2457/5000 train_loss:3.6219 train_time:440125ms step_avg:179.86ms\nstep:2458/5000 train_loss:3.7431 train_time:440306ms step_avg:179.86ms\nstep:2459/5000 train_loss:3.8813 train_time:440486ms step_avg:179.86ms\nstep:2460/5000 train_loss:3.8468 train_time:440667ms step_avg:179.86ms\nstep:2461/5000 train_loss:3.7739 train_time:440846ms step_avg:179.86ms\nstep:2462/5000 train_loss:3.6975 train_time:441026ms step_avg:179.86ms\nstep:2463/5000 train_loss:3.8906 train_time:441207ms step_avg:179.86ms\nstep:2464/5000 train_loss:3.8242 train_time:441387ms step_avg:179.86ms\nstep:2465/5000 train_loss:3.7009 train_time:441567ms step_avg:179.86ms\nstep:2466/5000 train_loss:3.7669 train_time:441746ms step_avg:179.86ms\nstep:2467/5000 train_loss:3.6832 train_time:441926ms step_avg:179.86ms\nstep:2468/5000 train_loss:3.7250 train_time:442106ms step_avg:179.86ms\nstep:2469/5000 train_loss:3.8588 train_time:442286ms step_avg:179.86ms\nstep:2470/5000 train_loss:3.8337 train_time:442465ms step_avg:179.86ms\nstep:2471/5000 train_loss:3.7089 train_time:442644ms step_avg:179.86ms\nstep:2472/5000 train_loss:3.6938 train_time:442823ms step_avg:179.86ms\nstep:2473/5000 train_loss:3.8602 train_time:443003ms step_avg:179.86ms\nstep:2474/5000 train_loss:3.6174 train_time:443183ms step_avg:179.86ms\nstep:2475/5000 train_loss:3.7498 train_time:443363ms step_avg:179.86ms\nstep:2476/5000 train_loss:3.7965 train_time:443543ms step_avg:179.86ms\nstep:2477/5000 train_loss:3.7957 train_time:443724ms step_avg:179.86ms\nstep:2478/5000 train_loss:3.8809 train_time:443903ms step_avg:179.86ms\nstep:2479/5000 train_loss:3.7711 train_time:444083ms step_avg:179.86ms\nstep:2480/5000 train_loss:3.7003 train_time:444262ms step_avg:179.86ms\nstep:2481/5000 train_loss:3.7284 train_time:444441ms step_avg:179.86ms\nstep:2482/5000 train_loss:3.7894 train_time:444619ms step_avg:179.86ms\nstep:2483/5000 train_loss:3.7692 train_time:444798ms step_avg:179.86ms\nstep:2484/5000 train_loss:3.7170 train_time:444977ms step_avg:179.86ms\nstep:2485/5000 train_loss:3.6988 train_time:445156ms step_avg:179.86ms\nstep:2486/5000 train_loss:3.7825 train_time:445334ms step_avg:179.86ms\nstep:2487/5000 train_loss:3.7933 train_time:445513ms step_avg:179.86ms\nstep:2488/5000 train_loss:4.0239 train_time:445692ms step_avg:179.86ms\nstep:2489/5000 train_loss:3.9500 train_time:445872ms step_avg:179.86ms\nstep:2490/5000 train_loss:3.6048 train_time:446050ms step_avg:179.86ms\nstep:2491/5000 train_loss:3.8054 train_time:446230ms step_avg:179.86ms\nstep:2492/5000 train_loss:3.7071 train_time:446409ms step_avg:179.86ms\nstep:2493/5000 train_loss:3.8788 train_time:446589ms step_avg:179.86ms\nstep:2494/5000 train_loss:3.7227 train_time:446770ms step_avg:179.86ms\nstep:2495/5000 train_loss:3.8266 train_time:446950ms step_avg:179.86ms\nstep:2496/5000 train_loss:3.7342 train_time:447129ms step_avg:179.86ms\nstep:2497/5000 train_loss:3.8017 train_time:447308ms step_avg:179.86ms\nstep:2498/5000 train_loss:3.8364 train_time:447487ms step_avg:179.86ms\nstep:2499/5000 train_loss:3.5829 train_time:447667ms step_avg:179.86ms\nstep:2500/5000 train_loss:3.8339 train_time:447847ms step_avg:179.86ms\nstep:2501/5000 train_loss:3.7787 train_time:448028ms step_avg:179.86ms\nstep:2502/5000 train_loss:3.7810 train_time:448206ms step_avg:179.86ms\nstep:2503/5000 train_loss:3.7772 train_time:448385ms step_avg:179.86ms\nstep:2504/5000 train_loss:3.8468 train_time:448565ms step_avg:179.86ms\nstep:2505/5000 train_loss:3.6970 train_time:448744ms step_avg:179.86ms\nstep:2506/5000 train_loss:3.7458 train_time:448923ms step_avg:179.86ms\nstep:2507/5000 train_loss:3.7834 train_time:449103ms step_avg:179.86ms\nstep:2508/5000 train_loss:3.8689 train_time:449282ms step_avg:179.86ms\nstep:2509/5000 train_loss:3.7537 train_time:449461ms step_avg:179.86ms\nstep:2510/5000 train_loss:3.6535 train_time:449639ms step_avg:179.86ms\nstep:2511/5000 train_loss:3.8225 train_time:449818ms step_avg:179.86ms\nstep:2512/5000 train_loss:3.8506 train_time:449996ms step_avg:179.85ms\nstep:2513/5000 train_loss:3.9143 train_time:450175ms step_avg:179.85ms\nstep:2514/5000 train_loss:3.8013 train_time:450354ms step_avg:179.85ms\nstep:2515/5000 train_loss:3.6512 train_time:450532ms step_avg:179.85ms\nstep:2516/5000 train_loss:3.8888 train_time:450711ms step_avg:179.85ms\nstep:2517/5000 train_loss:3.7184 train_time:450890ms step_avg:179.85ms\nstep:2518/5000 train_loss:3.6440 train_time:451073ms step_avg:179.85ms\nstep:2519/5000 train_loss:3.7268 train_time:451251ms step_avg:179.85ms\nstep:2520/5000 train_loss:3.7123 train_time:451431ms step_avg:179.85ms\nstep:2521/5000 train_loss:3.7793 train_time:451610ms step_avg:179.85ms\nstep:2522/5000 train_loss:3.8209 train_time:451788ms step_avg:179.85ms\nstep:2523/5000 train_loss:3.8382 train_time:451968ms step_avg:179.85ms\nstep:2524/5000 train_loss:3.8406 train_time:452148ms step_avg:179.85ms\nstep:2525/5000 train_loss:3.7233 train_time:452329ms step_avg:179.85ms\nstep:2526/5000 train_loss:3.6216 train_time:452509ms step_avg:179.85ms\nstep:2527/5000 train_loss:3.7577 train_time:452691ms step_avg:179.85ms\nstep:2528/5000 train_loss:3.8725 train_time:452870ms step_avg:179.85ms\nstep:2529/5000 train_loss:3.7699 train_time:453050ms step_avg:179.85ms\nstep:2530/5000 train_loss:3.7658 train_time:453229ms step_avg:179.85ms\nstep:2531/5000 train_loss:3.7140 train_time:453407ms step_avg:179.85ms\nstep:2532/5000 train_loss:3.7743 train_time:453587ms step_avg:179.85ms\nstep:2533/5000 train_loss:3.6232 train_time:453768ms step_avg:179.85ms\nstep:2534/5000 train_loss:3.9229 train_time:453947ms step_avg:179.85ms\nstep:2535/5000 train_loss:3.8924 train_time:454127ms step_avg:179.85ms\nstep:2536/5000 train_loss:3.7115 train_time:454307ms step_avg:179.85ms\nstep:2537/5000 train_loss:3.9361 train_time:454486ms step_avg:179.85ms\nstep:2538/5000 train_loss:3.7465 train_time:454666ms step_avg:179.85ms\nstep:2539/5000 train_loss:3.8022 train_time:454845ms step_avg:179.85ms\nstep:2540/5000 train_loss:3.8019 train_time:455024ms step_avg:179.85ms\nstep:2541/5000 train_loss:3.7434 train_time:455206ms step_avg:179.85ms\nstep:2542/5000 train_loss:3.6565 train_time:455386ms step_avg:179.85ms\nstep:2543/5000 train_loss:3.4868 train_time:455564ms step_avg:179.85ms\nstep:2544/5000 train_loss:4.0669 train_time:455746ms step_avg:179.85ms\nstep:2545/5000 train_loss:3.7143 train_time:455925ms step_avg:179.85ms\nstep:2546/5000 train_loss:3.7635 train_time:456104ms step_avg:179.85ms\nstep:2547/5000 train_loss:3.8369 train_time:456285ms step_avg:179.85ms\nstep:2548/5000 train_loss:3.6604 train_time:456465ms step_avg:179.85ms\nstep:2549/5000 train_loss:3.7313 train_time:456644ms step_avg:179.85ms\nstep:2550/5000 train_loss:3.6773 train_time:456824ms step_avg:179.85ms\nstep:2551/5000 train_loss:3.8712 train_time:457004ms step_avg:179.85ms\nstep:2552/5000 train_loss:3.4888 train_time:457184ms step_avg:179.85ms\nstep:2553/5000 train_loss:3.7846 train_time:457364ms step_avg:179.85ms\nstep:2554/5000 train_loss:3.7339 train_time:457543ms step_avg:179.85ms\nstep:2555/5000 train_loss:3.6932 train_time:457724ms step_avg:179.85ms\nstep:2556/5000 train_loss:3.7053 train_time:457903ms step_avg:179.85ms\nstep:2557/5000 train_loss:3.7578 train_time:458083ms step_avg:179.85ms\nstep:2558/5000 train_loss:3.7740 train_time:458262ms step_avg:179.85ms\nstep:2559/5000 train_loss:3.8587 train_time:458441ms step_avg:179.85ms\nstep:2560/5000 train_loss:3.7402 train_time:458619ms step_avg:179.85ms\nstep:2561/5000 train_loss:3.5959 train_time:458798ms step_avg:179.85ms\nstep:2562/5000 train_loss:3.6937 train_time:458978ms step_avg:179.85ms\nstep:2563/5000 train_loss:3.8666 train_time:459156ms step_avg:179.85ms\nstep:2564/5000 train_loss:3.6969 train_time:459335ms step_avg:179.85ms\nstep:2565/5000 train_loss:3.8498 train_time:459513ms step_avg:179.85ms\nstep:2566/5000 train_loss:3.6726 train_time:459691ms step_avg:179.85ms\nstep:2567/5000 train_loss:3.7573 train_time:459870ms step_avg:179.85ms\nstep:2568/5000 train_loss:3.7095 train_time:460049ms step_avg:179.85ms\nstep:2569/5000 train_loss:3.7887 train_time:460229ms step_avg:179.85ms\nstep:2570/5000 train_loss:3.8773 train_time:460409ms step_avg:179.85ms\nstep:2571/5000 train_loss:3.7444 train_time:460590ms step_avg:179.85ms\nstep:2572/5000 train_loss:3.7469 train_time:460770ms step_avg:179.85ms\nstep:2573/5000 train_loss:3.5687 train_time:460950ms step_avg:179.85ms\nstep:2574/5000 train_loss:3.5496 train_time:461129ms step_avg:179.85ms\nstep:2575/5000 train_loss:3.8044 train_time:461308ms step_avg:179.85ms\nstep:2576/5000 train_loss:3.7377 train_time:461488ms step_avg:179.85ms\nstep:2577/5000 train_loss:3.7514 train_time:461668ms step_avg:179.85ms\nstep:2578/5000 train_loss:3.7487 train_time:461848ms step_avg:179.85ms\nstep:2579/5000 train_loss:3.8770 train_time:462029ms step_avg:179.85ms\nstep:2580/5000 train_loss:3.6826 train_time:462209ms step_avg:179.85ms\nstep:2581/5000 train_loss:3.8827 train_time:462389ms step_avg:179.85ms\nstep:2582/5000 train_loss:3.6984 train_time:462568ms step_avg:179.85ms\nstep:2583/5000 train_loss:3.8444 train_time:462747ms step_avg:179.85ms\nstep:2584/5000 train_loss:3.8990 train_time:462929ms step_avg:179.85ms\nstep:2585/5000 train_loss:3.7103 train_time:463108ms step_avg:179.85ms\nstep:2586/5000 train_loss:3.8533 train_time:463288ms step_avg:179.85ms\nstep:2587/5000 train_loss:3.7448 train_time:463468ms step_avg:179.85ms\nstep:2588/5000 train_loss:3.8004 train_time:463647ms step_avg:179.85ms\nstep:2589/5000 train_loss:3.7878 train_time:463827ms step_avg:179.85ms\nstep:2590/5000 train_loss:3.7660 train_time:464009ms step_avg:179.85ms\nstep:2591/5000 train_loss:3.7181 train_time:464188ms step_avg:179.85ms\nstep:2592/5000 train_loss:3.7959 train_time:464368ms step_avg:179.85ms\nstep:2593/5000 train_loss:3.7414 train_time:464548ms step_avg:179.85ms\nstep:2594/5000 train_loss:3.9764 train_time:464727ms step_avg:179.85ms\nstep:2595/5000 train_loss:4.0502 train_time:464906ms step_avg:179.85ms\nstep:2596/5000 train_loss:3.8221 train_time:465085ms step_avg:179.85ms\nstep:2597/5000 train_loss:3.8440 train_time:465264ms step_avg:179.85ms\nstep:2598/5000 train_loss:3.7336 train_time:465443ms step_avg:179.85ms\nstep:2599/5000 train_loss:3.7747 train_time:465623ms step_avg:179.85ms\nstep:2600/5000 train_loss:3.8987 train_time:465803ms step_avg:179.85ms\nstep:2601/5000 train_loss:3.7597 train_time:465982ms step_avg:179.85ms\nstep:2602/5000 train_loss:3.6092 train_time:466162ms step_avg:179.85ms\nstep:2603/5000 train_loss:3.8176 train_time:466343ms step_avg:179.85ms\nstep:2604/5000 train_loss:3.7548 train_time:466525ms step_avg:179.85ms\nstep:2605/5000 train_loss:3.8645 train_time:466704ms step_avg:179.85ms\nstep:2606/5000 train_loss:3.6654 train_time:466884ms step_avg:179.85ms\nstep:2607/5000 train_loss:3.7743 train_time:467065ms step_avg:179.85ms\nstep:2608/5000 train_loss:3.8858 train_time:467243ms step_avg:179.85ms\nstep:2609/5000 train_loss:3.5590 train_time:467423ms step_avg:179.85ms\nstep:2610/5000 train_loss:3.7558 train_time:467602ms step_avg:179.85ms\nstep:2611/5000 train_loss:3.7412 train_time:467781ms step_avg:179.85ms\nstep:2612/5000 train_loss:3.7145 train_time:467960ms step_avg:179.85ms\nstep:2613/5000 train_loss:3.6522 train_time:468139ms step_avg:179.85ms\nstep:2614/5000 train_loss:3.7317 train_time:468317ms step_avg:179.85ms\nstep:2615/5000 train_loss:3.7098 train_time:468495ms step_avg:179.84ms\nstep:2616/5000 train_loss:3.7891 train_time:468674ms step_avg:179.84ms\nstep:2617/5000 train_loss:3.7017 train_time:468854ms step_avg:179.84ms\nstep:2618/5000 train_loss:3.8780 train_time:469032ms step_avg:179.84ms\nstep:2619/5000 train_loss:3.6821 train_time:469211ms step_avg:179.84ms\nstep:2620/5000 train_loss:3.5563 train_time:469389ms step_avg:179.84ms\nstep:2621/5000 train_loss:3.8440 train_time:469568ms step_avg:179.84ms\nstep:2622/5000 train_loss:3.8947 train_time:469747ms step_avg:179.84ms\nstep:2623/5000 train_loss:3.9046 train_time:469926ms step_avg:179.84ms\nstep:2624/5000 train_loss:3.6954 train_time:470106ms step_avg:179.84ms\nstep:2625/5000 train_loss:3.6170 train_time:470285ms step_avg:179.84ms\nstep:2626/5000 train_loss:3.8139 train_time:470464ms step_avg:179.84ms\nstep:2627/5000 train_loss:3.7798 train_time:470643ms step_avg:179.84ms\nstep:2628/5000 train_loss:3.6989 train_time:470823ms step_avg:179.84ms\nstep:2629/5000 train_loss:3.8194 train_time:471002ms step_avg:179.84ms\nstep:2630/5000 train_loss:3.7271 train_time:471182ms step_avg:179.84ms\nstep:2631/5000 train_loss:3.7816 train_time:471362ms step_avg:179.84ms\nstep:2632/5000 train_loss:3.6446 train_time:471543ms step_avg:179.84ms\nstep:2633/5000 train_loss:3.5315 train_time:471721ms step_avg:179.84ms\nstep:2634/5000 train_loss:3.5807 train_time:471900ms step_avg:179.84ms\nstep:2635/5000 train_loss:3.8053 train_time:472078ms step_avg:179.84ms\nstep:2636/5000 train_loss:3.8122 train_time:472258ms step_avg:179.84ms\nstep:2637/5000 train_loss:3.6950 train_time:472435ms step_avg:179.84ms\nstep:2638/5000 train_loss:3.7668 train_time:472614ms step_avg:179.84ms\nstep:2639/5000 train_loss:3.7331 train_time:472793ms step_avg:179.84ms\nstep:2640/5000 train_loss:3.7161 train_time:472970ms step_avg:179.84ms\nstep:2641/5000 train_loss:3.6612 train_time:473149ms step_avg:179.84ms\nstep:2642/5000 train_loss:3.8646 train_time:473330ms step_avg:179.84ms\nstep:2643/5000 train_loss:3.8504 train_time:473509ms step_avg:179.84ms\nstep:2644/5000 train_loss:3.8121 train_time:473689ms step_avg:179.84ms\nstep:2645/5000 train_loss:3.7606 train_time:473869ms step_avg:179.84ms\nstep:2646/5000 train_loss:3.8265 train_time:474048ms step_avg:179.84ms\nstep:2647/5000 train_loss:3.7487 train_time:474227ms step_avg:179.84ms\nstep:2648/5000 train_loss:3.8804 train_time:474406ms step_avg:179.84ms\nstep:2649/5000 train_loss:3.8896 train_time:474586ms step_avg:179.84ms\nstep:2650/5000 train_loss:3.6353 train_time:474766ms step_avg:179.84ms\nstep:2651/5000 train_loss:3.8571 train_time:474947ms step_avg:179.84ms\nstep:2652/5000 train_loss:3.7954 train_time:475126ms step_avg:179.84ms\nstep:2653/5000 train_loss:3.7067 train_time:475306ms step_avg:179.84ms\nstep:2654/5000 train_loss:3.7796 train_time:475485ms step_avg:179.84ms\nstep:2655/5000 train_loss:3.7299 train_time:475665ms step_avg:179.84ms\nstep:2656/5000 train_loss:3.6749 train_time:475844ms step_avg:179.84ms\nstep:2657/5000 train_loss:3.7149 train_time:476025ms step_avg:179.84ms\nstep:2658/5000 train_loss:3.7895 train_time:476205ms step_avg:179.84ms\nstep:2659/5000 train_loss:3.6848 train_time:476386ms step_avg:179.84ms\nstep:2660/5000 train_loss:3.7559 train_time:476564ms step_avg:179.84ms\nstep:2661/5000 train_loss:3.7123 train_time:476743ms step_avg:179.84ms\nstep:2662/5000 train_loss:3.6773 train_time:476925ms step_avg:179.84ms\nstep:2663/5000 train_loss:3.7420 train_time:477106ms step_avg:179.84ms\nstep:2664/5000 train_loss:3.6342 train_time:477287ms step_avg:179.84ms\nstep:2665/5000 train_loss:3.8960 train_time:477466ms step_avg:179.84ms\nstep:2666/5000 train_loss:3.7874 train_time:477646ms step_avg:179.84ms\nstep:2667/5000 train_loss:3.6431 train_time:477827ms step_avg:179.84ms\nstep:2668/5000 train_loss:3.9154 train_time:478006ms step_avg:179.84ms\nstep:2669/5000 train_loss:3.6408 train_time:478186ms step_avg:179.84ms\nstep:2670/5000 train_loss:3.8831 train_time:478365ms step_avg:179.84ms\nstep:2671/5000 train_loss:3.7774 train_time:478545ms step_avg:179.84ms\nstep:2672/5000 train_loss:3.7414 train_time:478725ms step_avg:179.84ms\nstep:2673/5000 train_loss:3.8056 train_time:478903ms step_avg:179.84ms\nstep:2674/5000 train_loss:3.8693 train_time:479082ms step_avg:179.84ms\nstep:2675/5000 train_loss:3.8613 train_time:479263ms step_avg:179.84ms\nstep:2676/5000 train_loss:3.7579 train_time:479444ms step_avg:179.84ms\nstep:2677/5000 train_loss:3.9144 train_time:479627ms step_avg:179.84ms\nstep:2678/5000 train_loss:3.6649 train_time:479807ms step_avg:179.84ms\nstep:2679/5000 train_loss:3.7526 train_time:479987ms step_avg:179.84ms\nstep:2680/5000 train_loss:3.6104 train_time:480166ms step_avg:179.84ms\nstep:2681/5000 train_loss:3.7195 train_time:480347ms step_avg:179.84ms\nstep:2682/5000 train_loss:3.8382 train_time:480527ms step_avg:179.84ms\nstep:2683/5000 train_loss:3.7003 train_time:480706ms step_avg:179.84ms\nstep:2684/5000 train_loss:3.7295 train_time:480886ms step_avg:179.84ms\nstep:2685/5000 train_loss:3.6846 train_time:481066ms step_avg:179.84ms\nstep:2686/5000 train_loss:3.7627 train_time:481246ms step_avg:179.84ms\nstep:2687/5000 train_loss:3.9375 train_time:481426ms step_avg:179.84ms\nstep:2688/5000 train_loss:3.6803 train_time:481606ms step_avg:179.84ms\nstep:2689/5000 train_loss:3.6994 train_time:481787ms step_avg:179.84ms\nstep:2690/5000 train_loss:3.5737 train_time:481967ms step_avg:179.84ms\nstep:2691/5000 train_loss:3.8396 train_time:482146ms step_avg:179.84ms\nstep:2692/5000 train_loss:3.7036 train_time:482327ms step_avg:179.84ms\nstep:2693/5000 train_loss:3.7306 train_time:482506ms step_avg:179.84ms\nstep:2694/5000 train_loss:4.0337 train_time:482688ms step_avg:179.84ms\nstep:2695/5000 train_loss:3.6895 train_time:482868ms step_avg:179.84ms\nstep:2696/5000 train_loss:3.9856 train_time:483047ms step_avg:179.84ms\nstep:2697/5000 train_loss:3.6556 train_time:483227ms step_avg:179.84ms\nstep:2698/5000 train_loss:3.8188 train_time:483407ms step_avg:179.84ms\nstep:2699/5000 train_loss:3.7032 train_time:483587ms step_avg:179.84ms\nstep:2700/5000 train_loss:3.7739 train_time:483769ms step_avg:179.84ms\nstep:2701/5000 train_loss:3.7402 train_time:483949ms step_avg:179.84ms\nstep:2702/5000 train_loss:3.7614 train_time:484128ms step_avg:179.84ms\nstep:2703/5000 train_loss:3.6179 train_time:484307ms step_avg:179.84ms\nstep:2704/5000 train_loss:3.8362 train_time:484486ms step_avg:179.84ms\nstep:2705/5000 train_loss:3.6953 train_time:484666ms step_avg:179.84ms\nstep:2706/5000 train_loss:3.8107 train_time:484846ms step_avg:179.84ms\nstep:2707/5000 train_loss:3.7561 train_time:485026ms step_avg:179.84ms\nstep:2708/5000 train_loss:3.6220 train_time:485206ms step_avg:179.84ms\nstep:2709/5000 train_loss:3.7692 train_time:485387ms step_avg:179.84ms\nstep:2710/5000 train_loss:3.7187 train_time:485568ms step_avg:179.84ms\nstep:2711/5000 train_loss:3.7754 train_time:485750ms step_avg:179.84ms\nstep:2712/5000 train_loss:3.7278 train_time:485930ms step_avg:179.84ms\nstep:2713/5000 train_loss:3.6683 train_time:486108ms step_avg:179.84ms\nstep:2714/5000 train_loss:3.5856 train_time:486288ms step_avg:179.84ms\nstep:2715/5000 train_loss:3.8032 train_time:486468ms step_avg:179.84ms\nstep:2716/5000 train_loss:3.6767 train_time:486647ms step_avg:179.84ms\nstep:2717/5000 train_loss:3.5542 train_time:486826ms step_avg:179.84ms\nstep:2718/5000 train_loss:3.6995 train_time:487007ms step_avg:179.84ms\nstep:2719/5000 train_loss:3.8048 train_time:487187ms step_avg:179.84ms\nstep:2720/5000 train_loss:3.8181 train_time:487367ms step_avg:179.84ms\nstep:2721/5000 train_loss:3.5511 train_time:487548ms step_avg:179.84ms\nstep:2722/5000 train_loss:3.8006 train_time:487728ms step_avg:179.84ms\nstep:2723/5000 train_loss:3.6869 train_time:487909ms step_avg:179.84ms\nstep:2724/5000 train_loss:3.7129 train_time:488088ms step_avg:179.84ms\nstep:2725/5000 train_loss:3.7115 train_time:488269ms step_avg:179.84ms\nstep:2726/5000 train_loss:3.8998 train_time:488448ms step_avg:179.84ms\nstep:2727/5000 train_loss:3.7190 train_time:488629ms step_avg:179.84ms\nstep:2728/5000 train_loss:3.7397 train_time:488809ms step_avg:179.84ms\nstep:2729/5000 train_loss:3.6603 train_time:488988ms step_avg:179.84ms\nstep:2730/5000 train_loss:3.6650 train_time:489166ms step_avg:179.84ms\nstep:2731/5000 train_loss:3.7132 train_time:489347ms step_avg:179.84ms\nstep:2732/5000 train_loss:3.8261 train_time:489527ms step_avg:179.84ms\nstep:2733/5000 train_loss:3.8741 train_time:489707ms step_avg:179.84ms\nstep:2734/5000 train_loss:3.7691 train_time:489889ms step_avg:179.84ms\nstep:2735/5000 train_loss:3.7093 train_time:490069ms step_avg:179.84ms\nstep:2736/5000 train_loss:3.8695 train_time:490248ms step_avg:179.84ms\nstep:2737/5000 train_loss:4.3707 train_time:490427ms step_avg:179.84ms\nstep:2738/5000 train_loss:3.8056 train_time:490607ms step_avg:179.84ms\nstep:2739/5000 train_loss:3.5514 train_time:490787ms step_avg:179.84ms\nstep:2740/5000 train_loss:3.7647 train_time:490966ms step_avg:179.84ms\nstep:2741/5000 train_loss:3.9477 train_time:491149ms step_avg:179.84ms\nstep:2742/5000 train_loss:3.7317 train_time:491329ms step_avg:179.84ms\nstep:2743/5000 train_loss:3.8115 train_time:491509ms step_avg:179.84ms\nstep:2744/5000 train_loss:3.8364 train_time:491688ms step_avg:179.84ms\nstep:2745/5000 train_loss:3.7456 train_time:491867ms step_avg:179.84ms\nstep:2746/5000 train_loss:3.8098 train_time:492046ms step_avg:179.84ms\nstep:2747/5000 train_loss:3.7187 train_time:492226ms step_avg:179.84ms\nstep:2748/5000 train_loss:3.7717 train_time:492406ms step_avg:179.84ms\nstep:2749/5000 train_loss:3.6701 train_time:492586ms step_avg:179.84ms\nstep:2750/5000 train_loss:3.7179 train_time:492765ms step_avg:179.84ms\nstep:2751/5000 train_loss:3.8717 train_time:492944ms step_avg:179.84ms\nstep:2752/5000 train_loss:3.8123 train_time:493124ms step_avg:179.84ms\nstep:2753/5000 train_loss:3.6640 train_time:493303ms step_avg:179.84ms\nstep:2754/5000 train_loss:3.6779 train_time:493483ms step_avg:179.84ms\nstep:2755/5000 train_loss:3.6915 train_time:493664ms step_avg:179.84ms\nstep:2756/5000 train_loss:3.4426 train_time:493843ms step_avg:179.84ms\nstep:2757/5000 train_loss:3.6602 train_time:494023ms step_avg:179.84ms\nstep:2758/5000 train_loss:3.7582 train_time:494204ms step_avg:179.84ms\nstep:2759/5000 train_loss:3.8334 train_time:494383ms step_avg:179.84ms\nstep:2760/5000 train_loss:3.5558 train_time:494563ms step_avg:179.84ms\nstep:2761/5000 train_loss:3.6812 train_time:494743ms step_avg:179.84ms\nstep:2762/5000 train_loss:3.6052 train_time:494923ms step_avg:179.84ms\nstep:2763/5000 train_loss:3.7153 train_time:495101ms step_avg:179.84ms\nstep:2764/5000 train_loss:3.6891 train_time:495281ms step_avg:179.84ms\nstep:2765/5000 train_loss:3.7820 train_time:495460ms step_avg:179.84ms\nstep:2766/5000 train_loss:3.8071 train_time:495639ms step_avg:179.84ms\nstep:2767/5000 train_loss:3.7084 train_time:495819ms step_avg:179.84ms\nstep:2768/5000 train_loss:3.5716 train_time:495999ms step_avg:179.84ms\nstep:2769/5000 train_loss:3.7728 train_time:496178ms step_avg:179.84ms\nstep:2770/5000 train_loss:3.8114 train_time:496357ms step_avg:179.84ms\nstep:2771/5000 train_loss:3.7255 train_time:496536ms step_avg:179.84ms\nstep:2772/5000 train_loss:3.7788 train_time:496715ms step_avg:179.84ms\nstep:2773/5000 train_loss:3.6911 train_time:496893ms step_avg:179.84ms\nstep:2774/5000 train_loss:3.5717 train_time:497072ms step_avg:179.84ms\nstep:2775/5000 train_loss:3.7198 train_time:497250ms step_avg:179.84ms\nstep:2776/5000 train_loss:3.8062 train_time:497430ms step_avg:179.84ms\nstep:2777/5000 train_loss:3.6289 train_time:497609ms step_avg:179.84ms\nstep:2778/5000 train_loss:3.8037 train_time:497788ms step_avg:179.84ms\nstep:2779/5000 train_loss:3.7602 train_time:497967ms step_avg:179.84ms\nstep:2780/5000 train_loss:3.6794 train_time:498147ms step_avg:179.84ms\nstep:2781/5000 train_loss:3.6883 train_time:498327ms step_avg:179.84ms\nstep:2782/5000 train_loss:3.7470 train_time:498507ms step_avg:179.84ms\nstep:2783/5000 train_loss:3.7983 train_time:498686ms step_avg:179.84ms\nstep:2784/5000 train_loss:3.6694 train_time:498866ms step_avg:179.84ms\nstep:2785/5000 train_loss:3.6577 train_time:499048ms step_avg:179.84ms\nstep:2786/5000 train_loss:3.5039 train_time:499227ms step_avg:179.84ms\nstep:2787/5000 train_loss:3.6898 train_time:499409ms step_avg:179.84ms\nstep:2788/5000 train_loss:4.0003 train_time:499589ms step_avg:179.84ms\nstep:2789/5000 train_loss:3.7385 train_time:499769ms step_avg:179.84ms\nstep:2790/5000 train_loss:3.8474 train_time:499948ms step_avg:179.84ms\nstep:2791/5000 train_loss:3.8207 train_time:500127ms step_avg:179.84ms\nstep:2792/5000 train_loss:3.7425 train_time:500308ms step_avg:179.84ms\nstep:2793/5000 train_loss:3.6210 train_time:500489ms step_avg:179.84ms\nstep:2794/5000 train_loss:3.7266 train_time:500669ms step_avg:179.84ms\nstep:2795/5000 train_loss:3.6423 train_time:500848ms step_avg:179.84ms\nstep:2796/5000 train_loss:3.7773 train_time:501027ms step_avg:179.84ms\nstep:2797/5000 train_loss:3.6037 train_time:501207ms step_avg:179.84ms\nstep:2798/5000 train_loss:3.7151 train_time:501387ms step_avg:179.84ms\nstep:2799/5000 train_loss:3.7015 train_time:501568ms step_avg:179.84ms\nstep:2800/5000 train_loss:3.9448 train_time:501747ms step_avg:179.84ms\nstep:2801/5000 train_loss:3.6818 train_time:501926ms step_avg:179.84ms\nstep:2802/5000 train_loss:3.6784 train_time:502105ms step_avg:179.84ms\nstep:2803/5000 train_loss:4.0270 train_time:502285ms step_avg:179.84ms\nstep:2804/5000 train_loss:3.7065 train_time:502466ms step_avg:179.84ms\nstep:2805/5000 train_loss:3.5857 train_time:502646ms step_avg:179.84ms\nstep:2806/5000 train_loss:3.7134 train_time:502828ms step_avg:179.84ms\nstep:2807/5000 train_loss:3.6082 train_time:503008ms step_avg:179.84ms\nstep:2808/5000 train_loss:3.6695 train_time:503188ms step_avg:179.84ms\nstep:2809/5000 train_loss:3.8232 train_time:503367ms step_avg:179.84ms\nstep:2810/5000 train_loss:3.2563 train_time:503546ms step_avg:179.84ms\nstep:2811/5000 train_loss:3.8080 train_time:503726ms step_avg:179.84ms\nstep:2812/5000 train_loss:3.6539 train_time:503905ms step_avg:179.84ms\nstep:2813/5000 train_loss:3.6692 train_time:504084ms step_avg:179.84ms\nstep:2814/5000 train_loss:3.7342 train_time:504263ms step_avg:179.84ms\nstep:2815/5000 train_loss:3.7071 train_time:504442ms step_avg:179.84ms\nstep:2816/5000 train_loss:3.6163 train_time:504622ms step_avg:179.84ms\nstep:2817/5000 train_loss:3.6779 train_time:504802ms step_avg:179.84ms\nstep:2818/5000 train_loss:3.5636 train_time:504982ms step_avg:179.84ms\nstep:2819/5000 train_loss:3.7495 train_time:505162ms step_avg:179.84ms\nstep:2820/5000 train_loss:3.6997 train_time:505341ms step_avg:179.84ms\nstep:2821/5000 train_loss:3.8179 train_time:505520ms step_avg:179.84ms\nstep:2822/5000 train_loss:3.8545 train_time:505700ms step_avg:179.84ms\nstep:2823/5000 train_loss:3.8342 train_time:505878ms step_avg:179.84ms\nstep:2824/5000 train_loss:3.7025 train_time:506057ms step_avg:179.84ms\nstep:2825/5000 train_loss:3.7988 train_time:506235ms step_avg:179.83ms\nstep:2826/5000 train_loss:3.7071 train_time:506415ms step_avg:179.83ms\nstep:2827/5000 train_loss:3.6800 train_time:506593ms step_avg:179.83ms\nstep:2828/5000 train_loss:3.7116 train_time:506773ms step_avg:179.83ms\nstep:2829/5000 train_loss:3.6050 train_time:506951ms step_avg:179.83ms\nstep:2830/5000 train_loss:3.7409 train_time:507130ms step_avg:179.83ms\nstep:2831/5000 train_loss:3.6851 train_time:507308ms step_avg:179.83ms\nstep:2832/5000 train_loss:3.7816 train_time:507488ms step_avg:179.83ms\nstep:2833/5000 train_loss:3.7077 train_time:507667ms step_avg:179.83ms\nstep:2834/5000 train_loss:3.7318 train_time:507847ms step_avg:179.83ms\nstep:2835/5000 train_loss:4.0988 train_time:508027ms step_avg:179.83ms\nstep:2836/5000 train_loss:3.7375 train_time:508206ms step_avg:179.83ms\nstep:2837/5000 train_loss:3.6366 train_time:508385ms step_avg:179.83ms\nstep:2838/5000 train_loss:3.7845 train_time:508564ms step_avg:179.83ms\nstep:2839/5000 train_loss:3.7483 train_time:508743ms step_avg:179.83ms\nstep:2840/5000 train_loss:3.7108 train_time:508923ms step_avg:179.83ms\nstep:2841/5000 train_loss:3.7938 train_time:509104ms step_avg:179.83ms\nstep:2842/5000 train_loss:3.8083 train_time:509284ms step_avg:179.83ms\nstep:2843/5000 train_loss:3.6946 train_time:509462ms step_avg:179.83ms\nstep:2844/5000 train_loss:3.6827 train_time:509641ms step_avg:179.83ms\nstep:2845/5000 train_loss:3.7324 train_time:509820ms step_avg:179.83ms\nstep:2846/5000 train_loss:3.6941 train_time:510000ms step_avg:179.83ms\nstep:2847/5000 train_loss:3.8794 train_time:510179ms step_avg:179.83ms\nstep:2848/5000 train_loss:3.7311 train_time:510358ms step_avg:179.83ms\nstep:2849/5000 train_loss:3.7645 train_time:510536ms step_avg:179.83ms\nstep:2850/5000 train_loss:3.6993 train_time:510715ms step_avg:179.83ms\nstep:2851/5000 train_loss:3.7747 train_time:510893ms step_avg:179.83ms\nstep:2852/5000 train_loss:3.7799 train_time:511072ms step_avg:179.83ms\nstep:2853/5000 train_loss:3.7363 train_time:511251ms step_avg:179.83ms\nstep:2854/5000 train_loss:3.6877 train_time:511430ms step_avg:179.83ms\nstep:2855/5000 train_loss:3.6700 train_time:511611ms step_avg:179.83ms\nstep:2856/5000 train_loss:3.8015 train_time:511790ms step_avg:179.83ms\nstep:2857/5000 train_loss:3.7638 train_time:511969ms step_avg:179.83ms\nstep:2858/5000 train_loss:3.7179 train_time:512149ms step_avg:179.83ms\nstep:2859/5000 train_loss:3.9089 train_time:512329ms step_avg:179.83ms\nstep:2860/5000 train_loss:3.7112 train_time:512509ms step_avg:179.83ms\nstep:2861/5000 train_loss:3.6715 train_time:512689ms step_avg:179.83ms\nstep:2862/5000 train_loss:3.5687 train_time:512868ms step_avg:179.83ms\nstep:2863/5000 train_loss:3.6454 train_time:513047ms step_avg:179.83ms\nstep:2864/5000 train_loss:3.7041 train_time:513227ms step_avg:179.83ms\nstep:2865/5000 train_loss:3.6481 train_time:513407ms step_avg:179.83ms\nstep:2866/5000 train_loss:3.9977 train_time:513587ms step_avg:179.83ms\nstep:2867/5000 train_loss:3.9322 train_time:513767ms step_avg:179.83ms\nstep:2868/5000 train_loss:3.7296 train_time:513946ms step_avg:179.83ms\nstep:2869/5000 train_loss:3.5677 train_time:514127ms step_avg:179.83ms\nstep:2870/5000 train_loss:3.8647 train_time:514307ms step_avg:179.83ms\nstep:2871/5000 train_loss:3.7068 train_time:514488ms step_avg:179.83ms\nstep:2872/5000 train_loss:3.8870 train_time:514667ms step_avg:179.83ms\nstep:2873/5000 train_loss:3.7049 train_time:514848ms step_avg:179.83ms\nstep:2874/5000 train_loss:3.8443 train_time:515027ms step_avg:179.83ms\nstep:2875/5000 train_loss:3.7617 train_time:515208ms step_avg:179.83ms\nstep:2876/5000 train_loss:3.7281 train_time:515387ms step_avg:179.83ms\nstep:2877/5000 train_loss:3.7695 train_time:515566ms step_avg:179.83ms\nstep:2878/5000 train_loss:3.8176 train_time:515746ms step_avg:179.83ms\nstep:2879/5000 train_loss:3.7459 train_time:515927ms step_avg:179.83ms\nstep:2880/5000 train_loss:3.8080 train_time:516106ms step_avg:179.83ms\nstep:2881/5000 train_loss:3.5847 train_time:516287ms step_avg:179.83ms\nstep:2882/5000 train_loss:3.7605 train_time:516467ms step_avg:179.83ms\nstep:2883/5000 train_loss:3.6282 train_time:516647ms step_avg:179.83ms\nstep:2884/5000 train_loss:3.9772 train_time:516827ms step_avg:179.83ms\nstep:2885/5000 train_loss:3.8924 train_time:517007ms step_avg:179.83ms\nstep:2886/5000 train_loss:3.6386 train_time:517186ms step_avg:179.83ms\nstep:2887/5000 train_loss:3.7504 train_time:517365ms step_avg:179.83ms\nstep:2888/5000 train_loss:3.5925 train_time:517544ms step_avg:179.83ms\nstep:2889/5000 train_loss:3.7895 train_time:517724ms step_avg:179.83ms\nstep:2890/5000 train_loss:3.7858 train_time:517904ms step_avg:179.83ms\nstep:2891/5000 train_loss:3.9012 train_time:518084ms step_avg:179.83ms\nstep:2892/5000 train_loss:3.8698 train_time:518266ms step_avg:179.83ms\nstep:2893/5000 train_loss:3.6832 train_time:518445ms step_avg:179.83ms\nstep:2894/5000 train_loss:3.7146 train_time:518626ms step_avg:179.83ms\nstep:2895/5000 train_loss:3.8384 train_time:518806ms step_avg:179.83ms\nstep:2896/5000 train_loss:3.9173 train_time:518987ms step_avg:179.83ms\nstep:2897/5000 train_loss:3.7617 train_time:519166ms step_avg:179.83ms\nstep:2898/5000 train_loss:3.7299 train_time:519347ms step_avg:179.83ms\nstep:2899/5000 train_loss:3.5090 train_time:519527ms step_avg:179.83ms\nstep:2900/5000 train_loss:3.7042 train_time:519708ms step_avg:179.83ms\nstep:2901/5000 train_loss:3.6912 train_time:519887ms step_avg:179.83ms\nstep:2902/5000 train_loss:3.7054 train_time:520066ms step_avg:179.83ms\nstep:2903/5000 train_loss:3.7589 train_time:520245ms step_avg:179.83ms\nstep:2904/5000 train_loss:3.7807 train_time:520425ms step_avg:179.83ms\nstep:2905/5000 train_loss:3.6863 train_time:520606ms step_avg:179.83ms\nstep:2906/5000 train_loss:3.6106 train_time:520786ms step_avg:179.83ms\nstep:2907/5000 train_loss:3.5185 train_time:520967ms step_avg:179.83ms\nstep:2908/5000 train_loss:3.6686 train_time:521149ms step_avg:179.83ms\nstep:2909/5000 train_loss:3.5812 train_time:521328ms step_avg:179.83ms\nstep:2910/5000 train_loss:3.7903 train_time:521508ms step_avg:179.83ms\nstep:2911/5000 train_loss:3.7467 train_time:521688ms step_avg:179.83ms\nstep:2912/5000 train_loss:3.7022 train_time:521868ms step_avg:179.83ms\nstep:2913/5000 train_loss:3.7754 train_time:522048ms step_avg:179.83ms\nstep:2914/5000 train_loss:3.7340 train_time:522227ms step_avg:179.83ms\nstep:2915/5000 train_loss:3.6746 train_time:522407ms step_avg:179.83ms\nstep:2916/5000 train_loss:3.8729 train_time:522586ms step_avg:179.83ms\nstep:2917/5000 train_loss:3.7118 train_time:522765ms step_avg:179.83ms\nstep:2918/5000 train_loss:3.5329 train_time:522945ms step_avg:179.83ms\nstep:2919/5000 train_loss:3.6041 train_time:523124ms step_avg:179.83ms\nstep:2920/5000 train_loss:3.8212 train_time:523305ms step_avg:179.83ms\nstep:2921/5000 train_loss:3.7307 train_time:523487ms step_avg:179.83ms\nstep:2922/5000 train_loss:3.7020 train_time:523666ms step_avg:179.83ms\nstep:2923/5000 train_loss:3.7551 train_time:523846ms step_avg:179.83ms\nstep:2924/5000 train_loss:3.8184 train_time:524025ms step_avg:179.83ms\nstep:2925/5000 train_loss:3.7513 train_time:524204ms step_avg:179.83ms\nstep:2926/5000 train_loss:3.8012 train_time:524383ms step_avg:179.83ms\nstep:2927/5000 train_loss:3.7722 train_time:524564ms step_avg:179.83ms\nstep:2928/5000 train_loss:3.7023 train_time:524744ms step_avg:179.83ms\nstep:2929/5000 train_loss:3.7882 train_time:524925ms step_avg:179.83ms\nstep:2930/5000 train_loss:3.7890 train_time:525107ms step_avg:179.83ms\nstep:2931/5000 train_loss:3.7750 train_time:525287ms step_avg:179.83ms\nstep:2932/5000 train_loss:3.7295 train_time:525465ms step_avg:179.83ms\nstep:2933/5000 train_loss:3.6210 train_time:525645ms step_avg:179.83ms\nstep:2934/5000 train_loss:3.8295 train_time:525825ms step_avg:179.83ms\nstep:2935/5000 train_loss:3.9187 train_time:526005ms step_avg:179.83ms\nstep:2936/5000 train_loss:3.9669 train_time:526187ms step_avg:179.83ms\nstep:2937/5000 train_loss:3.6707 train_time:526366ms step_avg:179.83ms\nstep:2938/5000 train_loss:3.6520 train_time:526546ms step_avg:179.83ms\nstep:2939/5000 train_loss:3.7787 train_time:526725ms step_avg:179.83ms\nstep:2940/5000 train_loss:3.7002 train_time:526906ms step_avg:179.83ms\nstep:2941/5000 train_loss:3.8086 train_time:527086ms step_avg:179.83ms\nstep:2942/5000 train_loss:3.6800 train_time:527265ms step_avg:179.83ms\nstep:2943/5000 train_loss:3.7388 train_time:527445ms step_avg:179.83ms\nstep:2944/5000 train_loss:3.7688 train_time:527624ms step_avg:179.83ms\nstep:2945/5000 train_loss:3.7742 train_time:527807ms step_avg:179.83ms\nstep:2946/5000 train_loss:3.7173 train_time:527987ms step_avg:179.83ms\nstep:2947/5000 train_loss:3.7164 train_time:528166ms step_avg:179.83ms\nstep:2948/5000 train_loss:3.8792 train_time:528344ms step_avg:179.83ms\nstep:2949/5000 train_loss:3.5957 train_time:528524ms step_avg:179.83ms\nstep:2950/5000 train_loss:3.8482 train_time:528705ms step_avg:179.83ms\nstep:2951/5000 train_loss:3.5830 train_time:528885ms step_avg:179.83ms\nstep:2952/5000 train_loss:3.7951 train_time:529064ms step_avg:179.83ms\nstep:2953/5000 train_loss:3.7201 train_time:529244ms step_avg:179.83ms\nstep:2954/5000 train_loss:3.7596 train_time:529424ms step_avg:179.83ms\nstep:2955/5000 train_loss:3.7844 train_time:529604ms step_avg:179.83ms\nstep:2956/5000 train_loss:3.7380 train_time:529785ms step_avg:179.83ms\nstep:2957/5000 train_loss:3.6761 train_time:529963ms step_avg:179.83ms\nstep:2958/5000 train_loss:3.7449 train_time:530143ms step_avg:179.83ms\nstep:2959/5000 train_loss:3.6299 train_time:530321ms step_avg:179.83ms\nstep:2960/5000 train_loss:3.6305 train_time:530501ms step_avg:179.83ms\nstep:2961/5000 train_loss:3.7472 train_time:530681ms step_avg:179.83ms\nstep:2962/5000 train_loss:3.7870 train_time:530861ms step_avg:179.83ms\nstep:2963/5000 train_loss:3.6497 train_time:531040ms step_avg:179.83ms\nstep:2964/5000 train_loss:4.2706 train_time:531222ms step_avg:179.83ms\nstep:2965/5000 train_loss:3.8083 train_time:531398ms step_avg:179.83ms\nstep:2966/5000 train_loss:3.7266 train_time:531577ms step_avg:179.83ms\nstep:2967/5000 train_loss:3.7574 train_time:531756ms step_avg:179.83ms\nstep:2968/5000 train_loss:3.6351 train_time:531936ms step_avg:179.83ms\nstep:2969/5000 train_loss:3.7717 train_time:532116ms step_avg:179.83ms\nstep:2970/5000 train_loss:3.7484 train_time:532295ms step_avg:179.83ms\nstep:2971/5000 train_loss:3.8037 train_time:532474ms step_avg:179.83ms\nstep:2972/5000 train_loss:3.7129 train_time:532652ms step_avg:179.83ms\nstep:2973/5000 train_loss:3.8698 train_time:532831ms step_avg:179.83ms\nstep:2974/5000 train_loss:3.6383 train_time:533011ms step_avg:179.83ms\nstep:2975/5000 train_loss:3.7392 train_time:533189ms step_avg:179.83ms\nstep:2976/5000 train_loss:3.7182 train_time:533368ms step_avg:179.83ms\nstep:2977/5000 train_loss:3.7847 train_time:533549ms step_avg:179.83ms\nstep:2978/5000 train_loss:3.8954 train_time:533728ms step_avg:179.83ms\nstep:2979/5000 train_loss:3.6736 train_time:533907ms step_avg:179.83ms\nstep:2980/5000 train_loss:3.7812 train_time:534087ms step_avg:179.83ms\nstep:2981/5000 train_loss:3.6437 train_time:534266ms step_avg:179.83ms\nstep:2982/5000 train_loss:3.6774 train_time:534445ms step_avg:179.83ms\nstep:2983/5000 train_loss:3.4986 train_time:534625ms step_avg:179.83ms\nstep:2984/5000 train_loss:3.7680 train_time:534807ms step_avg:179.83ms\nstep:2985/5000 train_loss:3.7132 train_time:534987ms step_avg:179.83ms\nstep:2986/5000 train_loss:3.7081 train_time:535168ms step_avg:179.83ms\nstep:2987/5000 train_loss:3.6379 train_time:535348ms step_avg:179.83ms\nstep:2988/5000 train_loss:3.7421 train_time:535527ms step_avg:179.83ms\nstep:2989/5000 train_loss:3.7675 train_time:535706ms step_avg:179.83ms\nstep:2990/5000 train_loss:3.7987 train_time:535886ms step_avg:179.83ms\nstep:2991/5000 train_loss:3.6995 train_time:536066ms step_avg:179.83ms\nstep:2992/5000 train_loss:3.7710 train_time:536245ms step_avg:179.83ms\nstep:2993/5000 train_loss:3.7803 train_time:536426ms step_avg:179.83ms\nstep:2994/5000 train_loss:3.7170 train_time:536604ms step_avg:179.83ms\nstep:2995/5000 train_loss:3.6860 train_time:536782ms step_avg:179.83ms\nstep:2996/5000 train_loss:3.7004 train_time:536961ms step_avg:179.83ms\nstep:2997/5000 train_loss:3.8243 train_time:537142ms step_avg:179.83ms\nstep:2998/5000 train_loss:3.7833 train_time:537322ms step_avg:179.83ms\nstep:2999/5000 train_loss:3.8783 train_time:537501ms step_avg:179.83ms\nstep:3000/5000 train_loss:3.7056 train_time:537681ms step_avg:179.83ms\nstep:3001/5000 train_loss:3.6193 train_time:537860ms step_avg:179.83ms\nstep:3002/5000 train_loss:3.6818 train_time:538039ms step_avg:179.83ms\nstep:3003/5000 train_loss:3.8021 train_time:538218ms step_avg:179.83ms\nstep:3004/5000 train_loss:3.7322 train_time:538396ms step_avg:179.83ms\nstep:3005/5000 train_loss:3.6789 train_time:538574ms step_avg:179.82ms\nstep:3006/5000 train_loss:3.8129 train_time:538753ms step_avg:179.82ms\nstep:3007/5000 train_loss:3.6922 train_time:538931ms step_avg:179.82ms\nstep:3008/5000 train_loss:3.7029 train_time:539110ms step_avg:179.82ms\nstep:3009/5000 train_loss:3.7586 train_time:539290ms step_avg:179.82ms\nstep:3010/5000 train_loss:3.6646 train_time:539468ms step_avg:179.82ms\nstep:3011/5000 train_loss:3.6607 train_time:539647ms step_avg:179.82ms\nstep:3012/5000 train_loss:3.7363 train_time:539827ms step_avg:179.82ms\nstep:3013/5000 train_loss:3.6774 train_time:540007ms step_avg:179.82ms\nstep:3014/5000 train_loss:3.7020 train_time:540186ms step_avg:179.82ms\nstep:3015/5000 train_loss:3.7682 train_time:540366ms step_avg:179.82ms\nstep:3016/5000 train_loss:3.7667 train_time:540546ms step_avg:179.82ms\nstep:3017/5000 train_loss:3.6761 train_time:540726ms step_avg:179.82ms\nstep:3018/5000 train_loss:3.6717 train_time:540906ms step_avg:179.82ms\nstep:3019/5000 train_loss:3.8619 train_time:541086ms step_avg:179.82ms\nstep:3020/5000 train_loss:3.7286 train_time:541265ms step_avg:179.82ms\nstep:3021/5000 train_loss:3.8679 train_time:541444ms step_avg:179.82ms\nstep:3022/5000 train_loss:3.7370 train_time:541624ms step_avg:179.82ms\nstep:3023/5000 train_loss:3.5917 train_time:541805ms step_avg:179.82ms\nstep:3024/5000 train_loss:3.8065 train_time:541984ms step_avg:179.82ms\nstep:3025/5000 train_loss:3.7802 train_time:542164ms step_avg:179.82ms\nstep:3026/5000 train_loss:3.6464 train_time:542345ms step_avg:179.82ms\nstep:3027/5000 train_loss:3.8049 train_time:542525ms step_avg:179.82ms\nstep:3028/5000 train_loss:3.6353 train_time:542705ms step_avg:179.82ms\nstep:3029/5000 train_loss:3.6291 train_time:542887ms step_avg:179.82ms\nstep:3030/5000 train_loss:3.8868 train_time:543068ms step_avg:179.82ms\nstep:3031/5000 train_loss:3.8188 train_time:543249ms step_avg:179.82ms\nstep:3032/5000 train_loss:3.8644 train_time:543427ms step_avg:179.82ms\nstep:3033/5000 train_loss:3.6674 train_time:543606ms step_avg:179.82ms\nstep:3034/5000 train_loss:3.9983 train_time:543785ms step_avg:179.82ms\nstep:3035/5000 train_loss:3.4442 train_time:543965ms step_avg:179.82ms\nstep:3036/5000 train_loss:3.7825 train_time:544145ms step_avg:179.82ms\nstep:3037/5000 train_loss:3.7067 train_time:544326ms step_avg:179.82ms\nstep:3038/5000 train_loss:3.7229 train_time:544505ms step_avg:179.82ms\nstep:3039/5000 train_loss:3.7088 train_time:544686ms step_avg:179.82ms\nstep:3040/5000 train_loss:3.7211 train_time:544865ms step_avg:179.82ms\nstep:3041/5000 train_loss:3.7100 train_time:545045ms step_avg:179.82ms\nstep:3042/5000 train_loss:3.6467 train_time:545224ms step_avg:179.82ms\nstep:3043/5000 train_loss:3.7566 train_time:545404ms step_avg:179.82ms\nstep:3044/5000 train_loss:3.8280 train_time:545584ms step_avg:179.82ms\nstep:3045/5000 train_loss:3.7071 train_time:545764ms step_avg:179.82ms\nstep:3046/5000 train_loss:3.8123 train_time:545945ms step_avg:179.82ms\nstep:3047/5000 train_loss:3.7649 train_time:546124ms step_avg:179.82ms\nstep:3048/5000 train_loss:3.7504 train_time:546529ms step_avg:179.90ms\nstep:3049/5000 train_loss:3.7114 train_time:546707ms step_avg:179.90ms\nstep:3050/5000 train_loss:3.5457 train_time:546884ms step_avg:179.90ms\nstep:3051/5000 train_loss:3.6974 train_time:547061ms step_avg:179.90ms\nstep:3052/5000 train_loss:3.6000 train_time:547238ms step_avg:179.89ms\nstep:3053/5000 train_loss:3.6784 train_time:547425ms step_avg:179.90ms\nstep:3054/5000 train_loss:3.6975 train_time:547603ms step_avg:179.90ms\nstep:3055/5000 train_loss:3.8041 train_time:547781ms step_avg:179.90ms\nstep:3056/5000 train_loss:3.5632 train_time:547960ms step_avg:179.89ms\nstep:3057/5000 train_loss:3.7118 train_time:548139ms step_avg:179.89ms\nstep:3058/5000 train_loss:3.8073 train_time:548320ms step_avg:179.89ms\nstep:3059/5000 train_loss:3.7010 train_time:548500ms step_avg:179.90ms\nstep:3060/5000 train_loss:3.6994 train_time:548680ms step_avg:179.90ms\nstep:3061/5000 train_loss:3.7743 train_time:548860ms step_avg:179.89ms\nstep:3062/5000 train_loss:3.4733 train_time:549038ms step_avg:179.89ms\nstep:3063/5000 train_loss:3.7263 train_time:549217ms step_avg:179.89ms\nstep:3064/5000 train_loss:3.6959 train_time:549395ms step_avg:179.89ms\nstep:3065/5000 train_loss:3.6881 train_time:549574ms step_avg:179.89ms\nstep:3066/5000 train_loss:3.8794 train_time:549751ms step_avg:179.89ms\nstep:3067/5000 train_loss:3.6804 train_time:549929ms step_avg:179.89ms\nstep:3068/5000 train_loss:4.7278 train_time:550109ms step_avg:179.89ms\nstep:3069/5000 train_loss:3.6620 train_time:550287ms step_avg:179.89ms\nstep:3070/5000 train_loss:3.7348 train_time:550468ms step_avg:179.89ms\nstep:3071/5000 train_loss:3.7094 train_time:550648ms step_avg:179.89ms\nstep:3072/5000 train_loss:3.6752 train_time:550827ms step_avg:179.89ms\nstep:3073/5000 train_loss:3.7108 train_time:551006ms step_avg:179.89ms\nstep:3074/5000 train_loss:3.7008 train_time:551186ms step_avg:179.89ms\nstep:3075/5000 train_loss:3.8903 train_time:551365ms step_avg:179.89ms\nstep:3076/5000 train_loss:3.7561 train_time:551545ms step_avg:179.89ms\nstep:3077/5000 train_loss:3.7286 train_time:551725ms step_avg:179.89ms\nstep:3078/5000 train_loss:3.7211 train_time:551904ms step_avg:179.89ms\nstep:3079/5000 train_loss:3.7943 train_time:552083ms step_avg:179.89ms\nstep:3080/5000 train_loss:3.7986 train_time:552262ms step_avg:179.89ms\nstep:3081/5000 train_loss:3.6430 train_time:552442ms step_avg:179.89ms\nstep:3082/5000 train_loss:3.7483 train_time:552620ms step_avg:179.89ms\nstep:3083/5000 train_loss:3.7081 train_time:552800ms step_avg:179.89ms\nstep:3084/5000 train_loss:4.3234 train_time:552979ms step_avg:179.89ms\nstep:3085/5000 train_loss:3.6542 train_time:553158ms step_avg:179.89ms\nstep:3086/5000 train_loss:3.7993 train_time:553337ms step_avg:179.89ms\nstep:3087/5000 train_loss:3.5096 train_time:553516ms step_avg:179.89ms\nstep:3088/5000 train_loss:3.7500 train_time:553694ms step_avg:179.89ms\nstep:3089/5000 train_loss:3.8416 train_time:553873ms step_avg:179.89ms\nstep:3090/5000 train_loss:4.0250 train_time:554052ms step_avg:179.89ms\nstep:3091/5000 train_loss:3.7458 train_time:554231ms step_avg:179.89ms\nstep:3092/5000 train_loss:3.7330 train_time:554410ms step_avg:179.89ms\nstep:3093/5000 train_loss:3.7250 train_time:554588ms step_avg:179.89ms\nstep:3094/5000 train_loss:3.5989 train_time:554766ms step_avg:179.89ms\nstep:3095/5000 train_loss:3.5947 train_time:554947ms step_avg:179.89ms\nstep:3096/5000 train_loss:3.7176 train_time:555125ms step_avg:179.89ms\nstep:3097/5000 train_loss:3.7387 train_time:555305ms step_avg:179.88ms\nstep:3098/5000 train_loss:3.7942 train_time:555485ms step_avg:179.88ms\nstep:3099/5000 train_loss:3.6544 train_time:555664ms step_avg:179.88ms\nstep:3100/5000 train_loss:3.8513 train_time:555843ms step_avg:179.88ms\nstep:3101/5000 train_loss:3.5733 train_time:556025ms step_avg:179.89ms\nstep:3102/5000 train_loss:3.7728 train_time:556207ms step_avg:179.89ms\nstep:3103/5000 train_loss:4.0578 train_time:556388ms step_avg:179.89ms\nstep:3104/5000 train_loss:3.6527 train_time:556567ms step_avg:179.89ms\nstep:3105/5000 train_loss:3.5477 train_time:556747ms step_avg:179.89ms\nstep:3106/5000 train_loss:3.7474 train_time:556927ms step_avg:179.89ms\nstep:3107/5000 train_loss:3.5845 train_time:557106ms step_avg:179.89ms\nstep:3108/5000 train_loss:3.7581 train_time:557286ms step_avg:179.89ms\nstep:3109/5000 train_loss:3.7530 train_time:557466ms step_avg:179.89ms\nstep:3110/5000 train_loss:3.5638 train_time:557645ms step_avg:179.89ms\nstep:3111/5000 train_loss:3.6905 train_time:557826ms step_avg:179.89ms\nstep:3112/5000 train_loss:3.7734 train_time:558005ms step_avg:179.89ms\nstep:3113/5000 train_loss:3.6475 train_time:558184ms step_avg:179.89ms\nstep:3114/5000 train_loss:3.5663 train_time:558364ms step_avg:179.89ms\nstep:3115/5000 train_loss:3.8071 train_time:558544ms step_avg:179.89ms\nstep:3116/5000 train_loss:3.7123 train_time:558723ms step_avg:179.89ms\nstep:3117/5000 train_loss:3.5930 train_time:558905ms step_avg:179.89ms\nstep:3118/5000 train_loss:3.6191 train_time:559084ms step_avg:179.89ms\nstep:3119/5000 train_loss:3.7802 train_time:559263ms step_avg:179.89ms\nstep:3120/5000 train_loss:3.7151 train_time:559443ms step_avg:179.89ms\nstep:3121/5000 train_loss:3.8116 train_time:559623ms step_avg:179.89ms\nstep:3122/5000 train_loss:3.7048 train_time:559804ms step_avg:179.89ms\nstep:3123/5000 train_loss:3.6916 train_time:559987ms step_avg:179.89ms\nstep:3124/5000 train_loss:3.6100 train_time:560167ms step_avg:179.89ms\nstep:3125/5000 train_loss:3.7686 train_time:560347ms step_avg:179.89ms\nstep:3126/5000 train_loss:3.7398 train_time:560527ms step_avg:179.89ms\nstep:3127/5000 train_loss:3.6789 train_time:560706ms step_avg:179.89ms\nstep:3128/5000 train_loss:3.7479 train_time:560886ms step_avg:179.89ms\nstep:3129/5000 train_loss:3.7371 train_time:561067ms step_avg:179.89ms\nstep:3130/5000 train_loss:3.8080 train_time:561246ms step_avg:179.89ms\nstep:3131/5000 train_loss:3.5356 train_time:561424ms step_avg:179.89ms\nstep:3132/5000 train_loss:3.7207 train_time:561603ms step_avg:179.89ms\nstep:3133/5000 train_loss:3.6121 train_time:561783ms step_avg:179.89ms\nstep:3134/5000 train_loss:3.7154 train_time:561961ms step_avg:179.89ms\nstep:3135/5000 train_loss:3.6595 train_time:562142ms step_avg:179.89ms\nstep:3136/5000 train_loss:3.6762 train_time:562322ms step_avg:179.89ms\nstep:3137/5000 train_loss:3.8148 train_time:562501ms step_avg:179.89ms\nstep:3138/5000 train_loss:3.7105 train_time:562680ms step_avg:179.88ms\nstep:3139/5000 train_loss:3.6537 train_time:562859ms step_avg:179.88ms\nstep:3140/5000 train_loss:3.6770 train_time:563038ms step_avg:179.88ms\nstep:3141/5000 train_loss:3.6362 train_time:563218ms step_avg:179.88ms\nstep:3142/5000 train_loss:3.7186 train_time:563396ms step_avg:179.88ms\nstep:3143/5000 train_loss:3.5900 train_time:563575ms step_avg:179.88ms\nstep:3144/5000 train_loss:3.7415 train_time:563754ms step_avg:179.88ms\nstep:3145/5000 train_loss:3.7367 train_time:563932ms step_avg:179.88ms\nstep:3146/5000 train_loss:3.6940 train_time:564111ms step_avg:179.88ms\nstep:3147/5000 train_loss:3.6639 train_time:564290ms step_avg:179.88ms\nstep:3148/5000 train_loss:3.6525 train_time:564473ms step_avg:179.88ms\nstep:3149/5000 train_loss:3.5464 train_time:564651ms step_avg:179.88ms\nstep:3150/5000 train_loss:3.5950 train_time:564830ms step_avg:179.88ms\nstep:3151/5000 train_loss:3.7687 train_time:565009ms step_avg:179.88ms\nstep:3152/5000 train_loss:3.7289 train_time:565188ms step_avg:179.88ms\nstep:3153/5000 train_loss:3.8130 train_time:565367ms step_avg:179.88ms\nstep:3154/5000 train_loss:3.8584 train_time:565547ms step_avg:179.88ms\nstep:3155/5000 train_loss:3.6744 train_time:565728ms step_avg:179.88ms\nstep:3156/5000 train_loss:3.6966 train_time:565907ms step_avg:179.88ms\nstep:3157/5000 train_loss:3.7044 train_time:566087ms step_avg:179.88ms\nstep:3158/5000 train_loss:3.6381 train_time:566264ms step_avg:179.88ms\nstep:3159/5000 train_loss:3.6501 train_time:566444ms step_avg:179.88ms\nstep:3160/5000 train_loss:3.6186 train_time:566623ms step_avg:179.88ms\nstep:3161/5000 train_loss:3.6881 train_time:566804ms step_avg:179.88ms\nstep:3162/5000 train_loss:3.6968 train_time:566985ms step_avg:179.88ms\nstep:3163/5000 train_loss:3.7075 train_time:567164ms step_avg:179.88ms\nstep:3164/5000 train_loss:3.6737 train_time:567344ms step_avg:179.88ms\nstep:3165/5000 train_loss:4.0561 train_time:567523ms step_avg:179.88ms\nstep:3166/5000 train_loss:3.7614 train_time:567702ms step_avg:179.88ms\nstep:3167/5000 train_loss:3.6594 train_time:567884ms step_avg:179.88ms\nstep:3168/5000 train_loss:3.7323 train_time:568064ms step_avg:179.88ms\nstep:3169/5000 train_loss:3.7227 train_time:568244ms step_avg:179.88ms\nstep:3170/5000 train_loss:3.8955 train_time:568423ms step_avg:179.88ms\nstep:3171/5000 train_loss:3.9267 train_time:568602ms step_avg:179.88ms\nstep:3172/5000 train_loss:3.7633 train_time:568782ms step_avg:179.88ms\nstep:3173/5000 train_loss:3.6496 train_time:568962ms step_avg:179.88ms\nstep:3174/5000 train_loss:3.9114 train_time:569142ms step_avg:179.88ms\nstep:3175/5000 train_loss:3.8944 train_time:569320ms step_avg:179.88ms\nstep:3176/5000 train_loss:3.7637 train_time:569500ms step_avg:179.88ms\nstep:3177/5000 train_loss:3.8241 train_time:569678ms step_avg:179.88ms\nstep:3178/5000 train_loss:3.6171 train_time:569858ms step_avg:179.88ms\nstep:3179/5000 train_loss:3.6149 train_time:570036ms step_avg:179.88ms\nstep:3180/5000 train_loss:3.7099 train_time:570215ms step_avg:179.88ms\nstep:3181/5000 train_loss:3.7406 train_time:570393ms step_avg:179.88ms\nstep:3182/5000 train_loss:3.7387 train_time:570572ms step_avg:179.88ms\nstep:3183/5000 train_loss:3.6886 train_time:570751ms step_avg:179.88ms\nstep:3184/5000 train_loss:3.8218 train_time:570932ms step_avg:179.88ms\nstep:3185/5000 train_loss:4.8077 train_time:571110ms step_avg:179.88ms\nstep:3186/5000 train_loss:3.8044 train_time:571289ms step_avg:179.88ms\nstep:3187/5000 train_loss:3.7091 train_time:571470ms step_avg:179.88ms\nstep:3188/5000 train_loss:3.7302 train_time:571648ms step_avg:179.88ms\nstep:3189/5000 train_loss:3.6644 train_time:571828ms step_avg:179.88ms\nstep:3190/5000 train_loss:3.9554 train_time:572012ms step_avg:179.88ms\nstep:3191/5000 train_loss:4.1317 train_time:572190ms step_avg:179.88ms\nstep:3192/5000 train_loss:3.5318 train_time:572369ms step_avg:179.88ms\nstep:3193/5000 train_loss:3.6828 train_time:572548ms step_avg:179.88ms\nstep:3194/5000 train_loss:3.6674 train_time:572729ms step_avg:179.88ms\nstep:3195/5000 train_loss:3.8485 train_time:572908ms step_avg:179.88ms\nstep:3196/5000 train_loss:3.6167 train_time:573090ms step_avg:179.88ms\nstep:3197/5000 train_loss:3.7125 train_time:573270ms step_avg:179.88ms\nstep:3198/5000 train_loss:3.8074 train_time:573448ms step_avg:179.88ms\nstep:3199/5000 train_loss:3.8696 train_time:573627ms step_avg:179.88ms\nstep:3200/5000 train_loss:4.2779 train_time:573808ms step_avg:179.88ms\nstep:3201/5000 train_loss:3.6750 train_time:573987ms step_avg:179.88ms\nstep:3202/5000 train_loss:3.6969 train_time:574166ms step_avg:179.88ms\nstep:3203/5000 train_loss:3.8165 train_time:574349ms step_avg:179.88ms\nstep:3204/5000 train_loss:3.8606 train_time:574527ms step_avg:179.88ms\nstep:3205/5000 train_loss:3.7107 train_time:574708ms step_avg:179.88ms\nstep:3206/5000 train_loss:3.6944 train_time:574887ms step_avg:179.88ms\nstep:3207/5000 train_loss:3.6296 train_time:575067ms step_avg:179.88ms\nstep:3208/5000 train_loss:3.6913 train_time:575247ms step_avg:179.88ms\nstep:3209/5000 train_loss:3.7983 train_time:575424ms step_avg:179.88ms\nstep:3210/5000 train_loss:3.7811 train_time:575605ms step_avg:179.88ms\nstep:3211/5000 train_loss:3.9532 train_time:575784ms step_avg:179.88ms\nstep:3212/5000 train_loss:3.7557 train_time:575967ms step_avg:179.88ms\nstep:3213/5000 train_loss:3.8743 train_time:576148ms step_avg:179.88ms\nstep:3214/5000 train_loss:3.6444 train_time:576326ms step_avg:179.88ms\nstep:3215/5000 train_loss:3.7263 train_time:576506ms step_avg:179.88ms\nstep:3216/5000 train_loss:3.7286 train_time:576685ms step_avg:179.88ms\nstep:3217/5000 train_loss:3.6728 train_time:576865ms step_avg:179.88ms\nstep:3218/5000 train_loss:3.6827 train_time:577044ms step_avg:179.88ms\nstep:3219/5000 train_loss:3.6838 train_time:577223ms step_avg:179.88ms\nstep:3220/5000 train_loss:3.6674 train_time:577404ms step_avg:179.88ms\nstep:3221/5000 train_loss:3.6651 train_time:577584ms step_avg:179.88ms\nstep:3222/5000 train_loss:3.7730 train_time:577765ms step_avg:179.88ms\nstep:3223/5000 train_loss:3.7114 train_time:577944ms step_avg:179.88ms\nstep:3224/5000 train_loss:3.6675 train_time:578125ms step_avg:179.88ms\nstep:3225/5000 train_loss:3.6772 train_time:578307ms step_avg:179.88ms\nstep:3226/5000 train_loss:3.6699 train_time:578487ms step_avg:179.88ms\nstep:3227/5000 train_loss:3.7630 train_time:578666ms step_avg:179.88ms\nstep:3228/5000 train_loss:3.7168 train_time:578845ms step_avg:179.88ms\nstep:3229/5000 train_loss:3.7898 train_time:579026ms step_avg:179.88ms\nstep:3230/5000 train_loss:3.6925 train_time:579207ms step_avg:179.88ms\nstep:3231/5000 train_loss:3.6125 train_time:579390ms step_avg:179.88ms\nstep:3232/5000 train_loss:3.7617 train_time:579568ms step_avg:179.88ms\nstep:3233/5000 train_loss:3.7814 train_time:579748ms step_avg:179.88ms\nstep:3234/5000 train_loss:3.7836 train_time:579928ms step_avg:179.88ms\nstep:3235/5000 train_loss:3.6000 train_time:580108ms step_avg:179.88ms\nstep:3236/5000 train_loss:3.9865 train_time:580287ms step_avg:179.88ms\nstep:3237/5000 train_loss:3.7213 train_time:580468ms step_avg:179.88ms\nstep:3238/5000 train_loss:3.7046 train_time:580646ms step_avg:179.88ms\nstep:3239/5000 train_loss:3.7436 train_time:580825ms step_avg:179.88ms\nstep:3240/5000 train_loss:3.8161 train_time:581005ms step_avg:179.88ms\nstep:3241/5000 train_loss:3.8102 train_time:581184ms step_avg:179.88ms\nstep:3242/5000 train_loss:3.6367 train_time:581363ms step_avg:179.88ms\nstep:3243/5000 train_loss:4.1047 train_time:581544ms step_avg:179.88ms\nstep:3244/5000 train_loss:3.6593 train_time:581723ms step_avg:179.88ms\nstep:3245/5000 train_loss:3.7608 train_time:581903ms step_avg:179.88ms\nstep:3246/5000 train_loss:3.7155 train_time:582082ms step_avg:179.88ms\nstep:3247/5000 train_loss:3.6487 train_time:582261ms step_avg:179.88ms\nstep:3248/5000 train_loss:3.7479 train_time:582440ms step_avg:179.88ms\nstep:3249/5000 train_loss:3.7516 train_time:582620ms step_avg:179.88ms\nstep:3250/5000 train_loss:3.7385 train_time:582798ms step_avg:179.88ms\nstep:3251/5000 train_loss:3.7459 train_time:582977ms step_avg:179.88ms\nstep:3252/5000 train_loss:3.7505 train_time:583156ms step_avg:179.88ms\nstep:3253/5000 train_loss:3.9509 train_time:583334ms step_avg:179.87ms\nstep:3254/5000 train_loss:3.7506 train_time:583514ms step_avg:179.87ms\nstep:3255/5000 train_loss:3.7757 train_time:583692ms step_avg:179.87ms\nstep:3256/5000 train_loss:3.7084 train_time:583871ms step_avg:179.87ms\nstep:3257/5000 train_loss:3.7212 train_time:584049ms step_avg:179.87ms\nstep:3258/5000 train_loss:3.7717 train_time:584230ms step_avg:179.87ms\nstep:3259/5000 train_loss:3.6903 train_time:584409ms step_avg:179.87ms\nstep:3260/5000 train_loss:3.5751 train_time:584588ms step_avg:179.87ms\nstep:3261/5000 train_loss:3.9534 train_time:584769ms step_avg:179.87ms\nstep:3262/5000 train_loss:3.7166 train_time:584949ms step_avg:179.87ms\nstep:3263/5000 train_loss:3.9019 train_time:585129ms step_avg:179.87ms\nstep:3264/5000 train_loss:3.9122 train_time:585310ms step_avg:179.87ms\nstep:3265/5000 train_loss:3.6991 train_time:585490ms step_avg:179.87ms\nstep:3266/5000 train_loss:3.7089 train_time:585671ms step_avg:179.87ms\nstep:3267/5000 train_loss:3.6753 train_time:585850ms step_avg:179.87ms\nstep:3268/5000 train_loss:3.7009 train_time:586028ms step_avg:179.87ms\nstep:3269/5000 train_loss:3.7989 train_time:586209ms step_avg:179.87ms\nstep:3270/5000 train_loss:3.8067 train_time:586388ms step_avg:179.87ms\nstep:3271/5000 train_loss:3.6640 train_time:586566ms step_avg:179.87ms\nstep:3272/5000 train_loss:3.6891 train_time:586746ms step_avg:179.87ms\nstep:3273/5000 train_loss:3.6772 train_time:586926ms step_avg:179.87ms\nstep:3274/5000 train_loss:3.9124 train_time:587109ms step_avg:179.87ms\nstep:3275/5000 train_loss:3.6574 train_time:587287ms step_avg:179.87ms\nstep:3276/5000 train_loss:3.6878 train_time:587467ms step_avg:179.87ms\nstep:3277/5000 train_loss:3.7266 train_time:587647ms step_avg:179.87ms\nstep:3278/5000 train_loss:3.6714 train_time:587827ms step_avg:179.87ms\nstep:3279/5000 train_loss:3.7069 train_time:588006ms step_avg:179.87ms\nstep:3280/5000 train_loss:3.6955 train_time:588187ms step_avg:179.87ms\nstep:3281/5000 train_loss:3.6440 train_time:588366ms step_avg:179.87ms\nstep:3282/5000 train_loss:3.8640 train_time:588546ms step_avg:179.87ms\nstep:3283/5000 train_loss:3.6966 train_time:588726ms step_avg:179.87ms\nstep:3284/5000 train_loss:3.5599 train_time:588906ms step_avg:179.87ms\nstep:3285/5000 train_loss:3.6151 train_time:589086ms step_avg:179.87ms\nstep:3286/5000 train_loss:3.6518 train_time:589266ms step_avg:179.87ms\nstep:3287/5000 train_loss:3.7771 train_time:589447ms step_avg:179.87ms\nstep:3288/5000 train_loss:3.6826 train_time:589626ms step_avg:179.87ms\nstep:3289/5000 train_loss:3.6283 train_time:589806ms step_avg:179.87ms\nstep:3290/5000 train_loss:3.6882 train_time:589987ms step_avg:179.87ms\nstep:3291/5000 train_loss:3.7306 train_time:590166ms step_avg:179.87ms\nstep:3292/5000 train_loss:3.7946 train_time:590345ms step_avg:179.87ms\nstep:3293/5000 train_loss:3.5970 train_time:590525ms step_avg:179.87ms\nstep:3294/5000 train_loss:3.8198 train_time:590704ms step_avg:179.87ms\nstep:3295/5000 train_loss:3.6879 train_time:590885ms step_avg:179.87ms\nstep:3296/5000 train_loss:3.5216 train_time:591066ms step_avg:179.87ms\nstep:3297/5000 train_loss:3.6353 train_time:591244ms step_avg:179.87ms\nstep:3298/5000 train_loss:3.7267 train_time:591424ms step_avg:179.87ms\nstep:3299/5000 train_loss:3.6649 train_time:591607ms step_avg:179.87ms\nstep:3300/5000 train_loss:3.7244 train_time:591786ms step_avg:179.87ms\nstep:3301/5000 train_loss:3.6621 train_time:591966ms step_avg:179.87ms\nstep:3302/5000 train_loss:3.6535 train_time:592145ms step_avg:179.87ms\nstep:3303/5000 train_loss:3.7300 train_time:592325ms step_avg:179.87ms\nstep:3304/5000 train_loss:3.8379 train_time:592505ms step_avg:179.87ms\nstep:3305/5000 train_loss:3.7148 train_time:592684ms step_avg:179.87ms\nstep:3306/5000 train_loss:3.7557 train_time:592865ms step_avg:179.87ms\nstep:3307/5000 train_loss:3.6724 train_time:593044ms step_avg:179.87ms\nstep:3308/5000 train_loss:3.6008 train_time:593224ms step_avg:179.87ms\nstep:3309/5000 train_loss:3.5510 train_time:593403ms step_avg:179.87ms\nstep:3310/5000 train_loss:3.6455 train_time:593583ms step_avg:179.87ms\nstep:3311/5000 train_loss:3.7567 train_time:593764ms step_avg:179.87ms\nstep:3312/5000 train_loss:3.6410 train_time:593944ms step_avg:179.87ms\nstep:3313/5000 train_loss:3.6376 train_time:594123ms step_avg:179.87ms\nstep:3314/5000 train_loss:3.8891 train_time:594303ms step_avg:179.87ms\nstep:3315/5000 train_loss:3.6645 train_time:594484ms step_avg:179.87ms\nstep:3316/5000 train_loss:3.6733 train_time:594663ms step_avg:179.87ms\nstep:3317/5000 train_loss:3.6492 train_time:594842ms step_avg:179.87ms\nstep:3318/5000 train_loss:3.7632 train_time:595022ms step_avg:179.87ms\nstep:3319/5000 train_loss:3.6953 train_time:595201ms step_avg:179.87ms\nstep:3320/5000 train_loss:3.7447 train_time:595381ms step_avg:179.87ms\nstep:3321/5000 train_loss:3.6218 train_time:595561ms step_avg:179.87ms\nstep:3322/5000 train_loss:3.8243 train_time:595741ms step_avg:179.87ms\nstep:3323/5000 train_loss:3.6183 train_time:595921ms step_avg:179.87ms\nstep:3324/5000 train_loss:3.5830 train_time:596100ms step_avg:179.87ms\nstep:3325/5000 train_loss:3.7022 train_time:596280ms step_avg:179.87ms\nstep:3326/5000 train_loss:3.6771 train_time:596459ms step_avg:179.87ms\nstep:3327/5000 train_loss:3.7494 train_time:596639ms step_avg:179.87ms\nstep:3328/5000 train_loss:3.7418 train_time:596821ms step_avg:179.87ms\nstep:3329/5000 train_loss:3.6829 train_time:597002ms step_avg:179.87ms\nstep:3330/5000 train_loss:3.7317 train_time:597181ms step_avg:179.87ms\nstep:3331/5000 train_loss:3.9550 train_time:597360ms step_avg:179.87ms\nstep:3332/5000 train_loss:3.7543 train_time:597538ms step_avg:179.87ms\nstep:3333/5000 train_loss:3.6421 train_time:597717ms step_avg:179.87ms\nstep:3334/5000 train_loss:3.6616 train_time:597896ms step_avg:179.87ms\nstep:3335/5000 train_loss:3.7335 train_time:598075ms step_avg:179.87ms\nstep:3336/5000 train_loss:3.7455 train_time:598254ms step_avg:179.87ms\nstep:3337/5000 train_loss:3.6876 train_time:598432ms step_avg:179.87ms\nstep:3338/5000 train_loss:3.7931 train_time:598611ms step_avg:179.87ms\nstep:3339/5000 train_loss:3.8222 train_time:598789ms step_avg:179.87ms\nstep:3340/5000 train_loss:3.6045 train_time:598969ms step_avg:179.87ms\nstep:3341/5000 train_loss:3.7904 train_time:599149ms step_avg:179.87ms\nstep:3342/5000 train_loss:3.6252 train_time:599327ms step_avg:179.87ms\nstep:3343/5000 train_loss:3.6784 train_time:599506ms step_avg:179.87ms\nstep:3344/5000 train_loss:3.8609 train_time:599685ms step_avg:179.87ms\nstep:3345/5000 train_loss:3.6178 train_time:599864ms step_avg:179.87ms\nstep:3346/5000 train_loss:3.6091 train_time:600044ms step_avg:179.87ms\nstep:3347/5000 train_loss:3.6067 train_time:600223ms step_avg:179.87ms\nstep:3348/5000 train_loss:3.6051 train_time:600402ms step_avg:179.87ms\nstep:3349/5000 train_loss:3.7536 train_time:600582ms step_avg:179.87ms\nstep:3350/5000 train_loss:3.8660 train_time:600762ms step_avg:179.87ms\nstep:3351/5000 train_loss:3.6477 train_time:600941ms step_avg:179.87ms\nstep:3352/5000 train_loss:3.5358 train_time:601120ms step_avg:179.87ms\nstep:3353/5000 train_loss:3.5592 train_time:601299ms step_avg:179.87ms\nstep:3354/5000 train_loss:3.5569 train_time:601480ms step_avg:179.87ms\nstep:3355/5000 train_loss:3.7486 train_time:601659ms step_avg:179.87ms\nstep:3356/5000 train_loss:3.7647 train_time:601839ms step_avg:179.87ms\nstep:3357/5000 train_loss:3.6809 train_time:602019ms step_avg:179.87ms\nstep:3358/5000 train_loss:3.7359 train_time:602199ms step_avg:179.87ms\nstep:3359/5000 train_loss:3.6571 train_time:602378ms step_avg:179.87ms\nstep:3360/5000 train_loss:3.6901 train_time:602557ms step_avg:179.87ms\nstep:3361/5000 train_loss:3.6300 train_time:602736ms step_avg:179.87ms\nstep:3362/5000 train_loss:3.5723 train_time:602914ms step_avg:179.87ms\nstep:3363/5000 train_loss:3.7241 train_time:603093ms step_avg:179.87ms\nstep:3364/5000 train_loss:3.7700 train_time:603271ms step_avg:179.87ms\nstep:3365/5000 train_loss:3.7395 train_time:603450ms step_avg:179.87ms\nstep:3366/5000 train_loss:3.6190 train_time:603628ms step_avg:179.87ms\nstep:3367/5000 train_loss:3.7648 train_time:603807ms step_avg:179.87ms\nstep:3368/5000 train_loss:3.6696 train_time:603988ms step_avg:179.87ms\nstep:3369/5000 train_loss:3.6570 train_time:604168ms step_avg:179.87ms\nstep:3370/5000 train_loss:3.7114 train_time:604350ms step_avg:179.87ms\nstep:3371/5000 train_loss:3.7039 train_time:604529ms step_avg:179.87ms\nstep:3372/5000 train_loss:3.6868 train_time:604708ms step_avg:179.87ms\nstep:3373/5000 train_loss:3.7719 train_time:604886ms step_avg:179.87ms\nstep:3374/5000 train_loss:3.7249 train_time:605066ms step_avg:179.86ms\nstep:3375/5000 train_loss:3.5435 train_time:605246ms step_avg:179.87ms\nstep:3376/5000 train_loss:3.6615 train_time:605426ms step_avg:179.87ms\nstep:3377/5000 train_loss:3.6648 train_time:605609ms step_avg:179.87ms\nstep:3378/5000 train_loss:3.7183 train_time:605789ms step_avg:179.87ms\nstep:3379/5000 train_loss:3.7941 train_time:605968ms step_avg:179.87ms\nstep:3380/5000 train_loss:3.6940 train_time:606148ms step_avg:179.87ms\nstep:3381/5000 train_loss:3.4867 train_time:606327ms step_avg:179.87ms\nstep:3382/5000 train_loss:3.5786 train_time:606506ms step_avg:179.87ms\nstep:3383/5000 train_loss:3.6786 train_time:606689ms step_avg:179.87ms\nstep:3384/5000 train_loss:3.6951 train_time:606868ms step_avg:179.87ms\nstep:3385/5000 train_loss:3.7735 train_time:607047ms step_avg:179.87ms\nstep:3386/5000 train_loss:3.8093 train_time:607227ms step_avg:179.87ms\nstep:3387/5000 train_loss:3.6691 train_time:607408ms step_avg:179.87ms\nstep:3388/5000 train_loss:3.7347 train_time:607589ms step_avg:179.87ms\nstep:3389/5000 train_loss:3.7201 train_time:607772ms step_avg:179.87ms\nstep:3390/5000 train_loss:3.6748 train_time:607950ms step_avg:179.87ms\nstep:3391/5000 train_loss:3.7064 train_time:608127ms step_avg:179.87ms\nstep:3392/5000 train_loss:3.6828 train_time:608307ms step_avg:179.87ms\nstep:3393/5000 train_loss:3.6692 train_time:608488ms step_avg:179.87ms\nstep:3394/5000 train_loss:3.6383 train_time:608669ms step_avg:179.87ms\nstep:3395/5000 train_loss:3.7532 train_time:608849ms step_avg:179.87ms\nstep:3396/5000 train_loss:3.5518 train_time:609029ms step_avg:179.87ms\nstep:3397/5000 train_loss:3.7579 train_time:609209ms step_avg:179.87ms\nstep:3398/5000 train_loss:3.7752 train_time:609387ms step_avg:179.87ms\nstep:3399/5000 train_loss:3.8177 train_time:609566ms step_avg:179.87ms\nstep:3400/5000 train_loss:3.7401 train_time:609746ms step_avg:179.87ms\nstep:3401/5000 train_loss:3.7059 train_time:609927ms step_avg:179.87ms\nstep:3402/5000 train_loss:3.6378 train_time:610109ms step_avg:179.87ms\nstep:3403/5000 train_loss:3.7274 train_time:610289ms step_avg:179.87ms\nstep:3404/5000 train_loss:3.6330 train_time:610468ms step_avg:179.87ms\nstep:3405/5000 train_loss:3.7385 train_time:610650ms step_avg:179.87ms\nstep:3406/5000 train_loss:3.5991 train_time:610828ms step_avg:179.87ms\nstep:3407/5000 train_loss:3.9951 train_time:611008ms step_avg:179.87ms\nstep:3408/5000 train_loss:3.7298 train_time:611188ms step_avg:179.87ms\nstep:3409/5000 train_loss:3.8144 train_time:611368ms step_avg:179.87ms\nstep:3410/5000 train_loss:3.8845 train_time:611547ms step_avg:179.87ms\nstep:3411/5000 train_loss:3.6995 train_time:611728ms step_avg:179.87ms\nstep:3412/5000 train_loss:3.5011 train_time:611907ms step_avg:179.87ms\nstep:3413/5000 train_loss:3.8861 train_time:612087ms step_avg:179.87ms\nstep:3414/5000 train_loss:3.6026 train_time:612267ms step_avg:179.87ms\nstep:3415/5000 train_loss:3.6858 train_time:612447ms step_avg:179.87ms\nstep:3416/5000 train_loss:3.7986 train_time:612627ms step_avg:179.87ms\nstep:3417/5000 train_loss:3.6242 train_time:612806ms step_avg:179.87ms\nstep:3418/5000 train_loss:3.7157 train_time:612987ms step_avg:179.87ms\nstep:3419/5000 train_loss:3.6992 train_time:613167ms step_avg:179.87ms\nstep:3420/5000 train_loss:3.7157 train_time:613349ms step_avg:179.87ms\nstep:3421/5000 train_loss:3.7238 train_time:613530ms step_avg:179.87ms\nstep:3422/5000 train_loss:3.7852 train_time:613711ms step_avg:179.87ms\nstep:3423/5000 train_loss:3.6868 train_time:613889ms step_avg:179.87ms\nstep:3424/5000 train_loss:3.4868 train_time:614067ms step_avg:179.87ms\nstep:3425/5000 train_loss:3.7179 train_time:614246ms step_avg:179.87ms\nstep:3426/5000 train_loss:3.6776 train_time:614426ms step_avg:179.87ms\nstep:3427/5000 train_loss:3.6570 train_time:614607ms step_avg:179.87ms\nstep:3428/5000 train_loss:3.7975 train_time:614787ms step_avg:179.87ms\nstep:3429/5000 train_loss:3.7372 train_time:614968ms step_avg:179.87ms\nstep:3430/5000 train_loss:3.6988 train_time:615147ms step_avg:179.87ms\nstep:3431/5000 train_loss:3.7026 train_time:615326ms step_avg:179.87ms\nstep:3432/5000 train_loss:3.8233 train_time:615505ms step_avg:179.87ms\nstep:3433/5000 train_loss:3.6907 train_time:615685ms step_avg:179.87ms\nstep:3434/5000 train_loss:3.7452 train_time:615867ms step_avg:179.87ms\nstep:3435/5000 train_loss:3.7433 train_time:616048ms step_avg:179.87ms\nstep:3436/5000 train_loss:3.5532 train_time:616227ms step_avg:179.87ms\nstep:3437/5000 train_loss:3.6878 train_time:616408ms step_avg:179.87ms\nstep:3438/5000 train_loss:3.6352 train_time:616588ms step_avg:179.87ms\nstep:3439/5000 train_loss:3.4899 train_time:616769ms step_avg:179.87ms\nstep:3440/5000 train_loss:3.6956 train_time:616949ms step_avg:179.87ms\nstep:3441/5000 train_loss:4.0255 train_time:617129ms step_avg:179.87ms\nstep:3442/5000 train_loss:3.7239 train_time:617309ms step_avg:179.87ms\nstep:3443/5000 train_loss:3.5716 train_time:617492ms step_avg:179.87ms\nstep:3444/5000 train_loss:3.7871 train_time:617667ms step_avg:179.87ms\nstep:3445/5000 train_loss:3.8106 train_time:617846ms step_avg:179.87ms\nstep:3446/5000 train_loss:3.7369 train_time:618026ms step_avg:179.87ms\nstep:3447/5000 train_loss:3.6913 train_time:618206ms step_avg:179.87ms\nstep:3448/5000 train_loss:3.5599 train_time:618385ms step_avg:179.87ms\nstep:3449/5000 train_loss:3.7036 train_time:618565ms step_avg:179.87ms\nstep:3450/5000 train_loss:3.7912 train_time:618744ms step_avg:179.87ms\nstep:3451/5000 train_loss:3.5058 train_time:618923ms step_avg:179.87ms\nstep:3452/5000 train_loss:3.6459 train_time:619104ms step_avg:179.87ms\nstep:3453/5000 train_loss:3.6238 train_time:619285ms step_avg:179.87ms\nstep:3454/5000 train_loss:3.7396 train_time:619465ms step_avg:179.87ms\nstep:3455/5000 train_loss:3.5598 train_time:619643ms step_avg:179.87ms\nstep:3456/5000 train_loss:3.9414 train_time:619822ms step_avg:179.87ms\nstep:3457/5000 train_loss:3.6730 train_time:620002ms step_avg:179.87ms\nstep:3458/5000 train_loss:3.7219 train_time:620181ms step_avg:179.87ms\nstep:3459/5000 train_loss:3.6540 train_time:620361ms step_avg:179.87ms\nstep:3460/5000 train_loss:3.9133 train_time:620541ms step_avg:179.87ms\nstep:3461/5000 train_loss:3.6767 train_time:620721ms step_avg:179.87ms\nstep:3462/5000 train_loss:3.5483 train_time:620901ms step_avg:179.87ms\nstep:3463/5000 train_loss:3.8456 train_time:621081ms step_avg:179.87ms\nstep:3464/5000 train_loss:3.7041 train_time:621260ms step_avg:179.87ms\nstep:3465/5000 train_loss:3.8456 train_time:621438ms step_avg:179.87ms\nstep:3466/5000 train_loss:3.7406 train_time:621617ms step_avg:179.87ms\nstep:3467/5000 train_loss:3.6110 train_time:621796ms step_avg:179.87ms\nstep:3468/5000 train_loss:3.6620 train_time:621975ms step_avg:179.87ms\nstep:3469/5000 train_loss:4.0577 train_time:622153ms step_avg:179.87ms\nstep:3470/5000 train_loss:4.0305 train_time:622331ms step_avg:179.86ms\nstep:3471/5000 train_loss:3.6427 train_time:622510ms step_avg:179.86ms\nstep:3472/5000 train_loss:3.8727 train_time:622688ms step_avg:179.86ms\nstep:3473/5000 train_loss:3.6320 train_time:622867ms step_avg:179.86ms\nstep:3474/5000 train_loss:3.6016 train_time:623047ms step_avg:179.86ms\nstep:3475/5000 train_loss:3.8168 train_time:623227ms step_avg:179.86ms\nstep:3476/5000 train_loss:3.6025 train_time:623406ms step_avg:179.86ms\nstep:3477/5000 train_loss:3.6924 train_time:623585ms step_avg:179.86ms\nstep:3478/5000 train_loss:3.7842 train_time:623764ms step_avg:179.86ms\nstep:3479/5000 train_loss:3.6680 train_time:623946ms step_avg:179.86ms\nstep:3480/5000 train_loss:3.5432 train_time:624125ms step_avg:179.86ms\nstep:3481/5000 train_loss:3.6644 train_time:624305ms step_avg:179.86ms\nstep:3482/5000 train_loss:3.7639 train_time:624486ms step_avg:179.86ms\nstep:3483/5000 train_loss:3.7788 train_time:624665ms step_avg:179.86ms\nstep:3484/5000 train_loss:3.7395 train_time:624846ms step_avg:179.86ms\nstep:3485/5000 train_loss:3.7657 train_time:625027ms step_avg:179.86ms\nstep:3486/5000 train_loss:3.5956 train_time:625204ms step_avg:179.86ms\nstep:3487/5000 train_loss:3.8594 train_time:625383ms step_avg:179.86ms\nstep:3488/5000 train_loss:3.6762 train_time:625565ms step_avg:179.86ms\nstep:3489/5000 train_loss:3.6259 train_time:625746ms step_avg:179.86ms\nstep:3490/5000 train_loss:3.5741 train_time:625926ms step_avg:179.86ms\nstep:3491/5000 train_loss:3.6522 train_time:626109ms step_avg:179.86ms\nstep:3492/5000 train_loss:3.6241 train_time:626288ms step_avg:179.86ms\nstep:3493/5000 train_loss:3.8233 train_time:626467ms step_avg:179.86ms\nstep:3494/5000 train_loss:3.8083 train_time:626647ms step_avg:179.86ms\nstep:3495/5000 train_loss:3.6846 train_time:626827ms step_avg:179.86ms\nstep:3496/5000 train_loss:3.7297 train_time:627009ms step_avg:179.86ms\nstep:3497/5000 train_loss:3.7931 train_time:627190ms step_avg:179.87ms\nstep:3498/5000 train_loss:3.5970 train_time:627369ms step_avg:179.86ms\nstep:3499/5000 train_loss:3.7322 train_time:627547ms step_avg:179.86ms\nstep:3500/5000 train_loss:3.6424 train_time:627726ms step_avg:179.86ms\nstep:3501/5000 train_loss:3.7494 train_time:627905ms step_avg:179.86ms\nstep:3502/5000 train_loss:3.3284 train_time:628086ms step_avg:179.86ms\nstep:3503/5000 train_loss:3.6590 train_time:628268ms step_avg:179.86ms\nstep:3504/5000 train_loss:3.7755 train_time:628446ms step_avg:179.86ms\nstep:3505/5000 train_loss:3.7277 train_time:628627ms step_avg:179.86ms\nstep:3506/5000 train_loss:3.7932 train_time:628808ms step_avg:179.86ms\nstep:3507/5000 train_loss:3.7142 train_time:628989ms step_avg:179.87ms\nstep:3508/5000 train_loss:3.5788 train_time:629169ms step_avg:179.87ms\nstep:3509/5000 train_loss:3.7517 train_time:629349ms step_avg:179.87ms\nstep:3510/5000 train_loss:3.5998 train_time:629528ms step_avg:179.87ms\nstep:3511/5000 train_loss:3.5253 train_time:629707ms step_avg:179.86ms\nstep:3512/5000 train_loss:3.7867 train_time:629888ms step_avg:179.87ms\nstep:3513/5000 train_loss:4.0297 train_time:630067ms step_avg:179.86ms\nstep:3514/5000 train_loss:3.7745 train_time:630247ms step_avg:179.86ms\nstep:3515/5000 train_loss:3.6838 train_time:630426ms step_avg:179.86ms\nstep:3516/5000 train_loss:3.6412 train_time:630608ms step_avg:179.87ms\nstep:3517/5000 train_loss:3.6399 train_time:630788ms step_avg:179.87ms\nstep:3518/5000 train_loss:3.6284 train_time:630968ms step_avg:179.87ms\nstep:3519/5000 train_loss:3.7476 train_time:631148ms step_avg:179.87ms\nstep:3520/5000 train_loss:4.0095 train_time:631327ms step_avg:179.87ms\nstep:3521/5000 train_loss:3.7443 train_time:631507ms step_avg:179.87ms\nstep:3522/5000 train_loss:3.8096 train_time:631685ms step_avg:179.86ms\nstep:3523/5000 train_loss:3.5271 train_time:631865ms step_avg:179.86ms\nstep:3524/5000 train_loss:3.7346 train_time:632047ms step_avg:179.87ms\nstep:3525/5000 train_loss:3.6048 train_time:632227ms step_avg:179.87ms\nstep:3526/5000 train_loss:3.4775 train_time:632407ms step_avg:179.87ms\nstep:3527/5000 train_loss:3.6216 train_time:632589ms step_avg:179.87ms\nstep:3528/5000 train_loss:3.5371 train_time:632768ms step_avg:179.87ms\nstep:3529/5000 train_loss:3.6027 train_time:632948ms step_avg:179.87ms\nstep:3530/5000 train_loss:3.6381 train_time:633128ms step_avg:179.87ms\nstep:3531/5000 train_loss:3.7761 train_time:633307ms step_avg:179.87ms\nstep:3532/5000 train_loss:3.8708 train_time:633487ms step_avg:179.87ms\nstep:3533/5000 train_loss:3.6320 train_time:633666ms step_avg:179.87ms\nstep:3534/5000 train_loss:3.8460 train_time:633846ms step_avg:179.87ms\nstep:3535/5000 train_loss:3.7095 train_time:634026ms step_avg:179.87ms\nstep:3536/5000 train_loss:3.5618 train_time:634207ms step_avg:179.87ms\nstep:3537/5000 train_loss:3.7393 train_time:634387ms step_avg:179.87ms\nstep:3538/5000 train_loss:3.9935 train_time:634566ms step_avg:179.87ms\nstep:3539/5000 train_loss:3.5942 train_time:634746ms step_avg:179.87ms\nstep:3540/5000 train_loss:3.8138 train_time:634925ms step_avg:179.87ms\nstep:3541/5000 train_loss:3.5620 train_time:635105ms step_avg:179.87ms\nstep:3542/5000 train_loss:3.7121 train_time:635285ms step_avg:179.87ms\nstep:3543/5000 train_loss:3.6500 train_time:635466ms step_avg:179.87ms\nstep:3544/5000 train_loss:3.6958 train_time:635645ms step_avg:179.87ms\nstep:3545/5000 train_loss:3.5282 train_time:635825ms step_avg:179.87ms\nstep:3546/5000 train_loss:3.6387 train_time:636007ms step_avg:179.87ms\nstep:3547/5000 train_loss:3.5936 train_time:636187ms step_avg:179.87ms\nstep:3548/5000 train_loss:3.8082 train_time:636366ms step_avg:179.87ms\nstep:3549/5000 train_loss:3.6417 train_time:636545ms step_avg:179.87ms\nstep:3550/5000 train_loss:3.7315 train_time:636725ms step_avg:179.87ms\nstep:3551/5000 train_loss:3.7020 train_time:636906ms step_avg:179.87ms\nstep:3552/5000 train_loss:3.7254 train_time:637086ms step_avg:179.87ms\nstep:3553/5000 train_loss:3.5603 train_time:637265ms step_avg:179.87ms\nstep:3554/5000 train_loss:4.2356 train_time:637445ms step_avg:179.87ms\nstep:3555/5000 train_loss:3.8880 train_time:637625ms step_avg:179.87ms\nstep:3556/5000 train_loss:3.7473 train_time:637805ms step_avg:179.87ms\nstep:3557/5000 train_loss:3.5818 train_time:637985ms step_avg:179.87ms\nstep:3558/5000 train_loss:3.9940 train_time:638166ms step_avg:179.87ms\nstep:3559/5000 train_loss:3.6509 train_time:638346ms step_avg:179.87ms\nstep:3560/5000 train_loss:3.7293 train_time:638525ms step_avg:179.87ms\nstep:3561/5000 train_loss:3.7448 train_time:638704ms step_avg:179.87ms\nstep:3562/5000 train_loss:3.6722 train_time:638884ms step_avg:179.87ms\nstep:3563/5000 train_loss:3.5758 train_time:639062ms step_avg:179.87ms\nstep:3564/5000 train_loss:3.5495 train_time:639242ms step_avg:179.87ms\nstep:3565/5000 train_loss:3.6060 train_time:639421ms step_avg:179.87ms\nstep:3566/5000 train_loss:3.6901 train_time:639601ms step_avg:179.87ms\nstep:3567/5000 train_loss:3.4595 train_time:639781ms step_avg:179.87ms\nstep:3568/5000 train_loss:3.7272 train_time:639960ms step_avg:179.87ms\nstep:3569/5000 train_loss:3.6008 train_time:640140ms step_avg:179.87ms\nstep:3570/5000 train_loss:3.5184 train_time:640320ms step_avg:179.87ms\nstep:3571/5000 train_loss:3.5500 train_time:640500ms step_avg:179.87ms\nstep:3572/5000 train_loss:3.6553 train_time:640680ms step_avg:179.87ms\nstep:3573/5000 train_loss:3.7708 train_time:640860ms step_avg:179.87ms\nstep:3574/5000 train_loss:3.8592 train_time:641039ms step_avg:179.86ms\nstep:3575/5000 train_loss:3.8787 train_time:641218ms step_avg:179.86ms\nstep:3576/5000 train_loss:3.7040 train_time:641398ms step_avg:179.86ms\nstep:3577/5000 train_loss:3.5164 train_time:641577ms step_avg:179.86ms\nstep:3578/5000 train_loss:3.5793 train_time:641756ms step_avg:179.86ms\nstep:3579/5000 train_loss:3.6830 train_time:641934ms step_avg:179.86ms\nstep:3580/5000 train_loss:3.5408 train_time:642113ms step_avg:179.86ms\nstep:3581/5000 train_loss:3.5892 train_time:642290ms step_avg:179.86ms\nstep:3582/5000 train_loss:3.8562 train_time:642469ms step_avg:179.86ms\nstep:3583/5000 train_loss:3.6809 train_time:642647ms step_avg:179.86ms\nstep:3584/5000 train_loss:3.4992 train_time:642827ms step_avg:179.86ms\nstep:3585/5000 train_loss:3.8188 train_time:643010ms step_avg:179.86ms\nstep:3586/5000 train_loss:3.8541 train_time:643188ms step_avg:179.86ms\nstep:3587/5000 train_loss:3.6201 train_time:643367ms step_avg:179.86ms\nstep:3588/5000 train_loss:3.5997 train_time:643546ms step_avg:179.86ms\nstep:3589/5000 train_loss:3.5758 train_time:643724ms step_avg:179.86ms\nstep:3590/5000 train_loss:3.6435 train_time:643904ms step_avg:179.86ms\nstep:3591/5000 train_loss:3.6839 train_time:644085ms step_avg:179.86ms\nstep:3592/5000 train_loss:3.8065 train_time:644264ms step_avg:179.86ms\nstep:3593/5000 train_loss:3.6973 train_time:644444ms step_avg:179.86ms\nstep:3594/5000 train_loss:3.6857 train_time:644624ms step_avg:179.86ms\nstep:3595/5000 train_loss:3.7064 train_time:644803ms step_avg:179.86ms\nstep:3596/5000 train_loss:3.6816 train_time:644985ms step_avg:179.86ms\nstep:3597/5000 train_loss:3.7234 train_time:645166ms step_avg:179.86ms\nstep:3598/5000 train_loss:3.7389 train_time:645344ms step_avg:179.86ms\nstep:3599/5000 train_loss:3.6613 train_time:645524ms step_avg:179.86ms\nstep:3600/5000 train_loss:3.7159 train_time:645704ms step_avg:179.86ms\nstep:3601/5000 train_loss:4.2008 train_time:645886ms step_avg:179.86ms\nstep:3602/5000 train_loss:3.6311 train_time:646062ms step_avg:179.86ms\nstep:3603/5000 train_loss:3.6109 train_time:646240ms step_avg:179.86ms\nstep:3604/5000 train_loss:3.8883 train_time:646420ms step_avg:179.86ms\nstep:3605/5000 train_loss:3.6351 train_time:646600ms step_avg:179.86ms\nstep:3606/5000 train_loss:3.5742 train_time:646780ms step_avg:179.86ms\nstep:3607/5000 train_loss:3.6606 train_time:646959ms step_avg:179.86ms\nstep:3608/5000 train_loss:3.6653 train_time:647137ms step_avg:179.86ms\nstep:3609/5000 train_loss:3.6436 train_time:647316ms step_avg:179.86ms\nstep:3610/5000 train_loss:3.7835 train_time:647495ms step_avg:179.86ms\nstep:3611/5000 train_loss:3.6502 train_time:647673ms step_avg:179.86ms\nstep:3612/5000 train_loss:3.5972 train_time:647851ms step_avg:179.86ms\nstep:3613/5000 train_loss:3.7102 train_time:648029ms step_avg:179.86ms\nstep:3614/5000 train_loss:3.7861 train_time:648207ms step_avg:179.86ms\nstep:3615/5000 train_loss:4.1126 train_time:648386ms step_avg:179.86ms\nstep:3616/5000 train_loss:3.8169 train_time:648566ms step_avg:179.86ms\nstep:3617/5000 train_loss:3.6760 train_time:648748ms step_avg:179.86ms\nstep:3618/5000 train_loss:3.6913 train_time:648929ms step_avg:179.86ms\nstep:3619/5000 train_loss:3.6472 train_time:649109ms step_avg:179.86ms\nstep:3620/5000 train_loss:3.7703 train_time:649287ms step_avg:179.86ms\nstep:3621/5000 train_loss:3.6647 train_time:649468ms step_avg:179.86ms\nstep:3622/5000 train_loss:3.6666 train_time:649648ms step_avg:179.86ms\nstep:3623/5000 train_loss:3.7109 train_time:649827ms step_avg:179.86ms\nstep:3624/5000 train_loss:3.6631 train_time:650008ms step_avg:179.86ms\nstep:3625/5000 train_loss:3.6411 train_time:650187ms step_avg:179.86ms\nstep:3626/5000 train_loss:3.6437 train_time:650366ms step_avg:179.86ms\nstep:3627/5000 train_loss:3.5570 train_time:650544ms step_avg:179.86ms\nstep:3628/5000 train_loss:3.6210 train_time:650725ms step_avg:179.86ms\nstep:3629/5000 train_loss:3.6406 train_time:650906ms step_avg:179.86ms\nstep:3630/5000 train_loss:3.9028 train_time:651087ms step_avg:179.86ms\nstep:3631/5000 train_loss:3.6693 train_time:651267ms step_avg:179.86ms\nstep:3632/5000 train_loss:3.9164 train_time:651447ms step_avg:179.86ms\nstep:3633/5000 train_loss:3.6783 train_time:651626ms step_avg:179.86ms\nstep:3634/5000 train_loss:3.6688 train_time:651807ms step_avg:179.86ms\nstep:3635/5000 train_loss:3.7876 train_time:651987ms step_avg:179.86ms\nstep:3636/5000 train_loss:3.6328 train_time:652165ms step_avg:179.86ms\nstep:3637/5000 train_loss:3.5879 train_time:652345ms step_avg:179.86ms\nstep:3638/5000 train_loss:3.6995 train_time:652524ms step_avg:179.86ms\nstep:3639/5000 train_loss:3.7005 train_time:652703ms step_avg:179.86ms\nstep:3640/5000 train_loss:3.8086 train_time:652883ms step_avg:179.86ms\nstep:3641/5000 train_loss:3.6768 train_time:653064ms step_avg:179.86ms\nstep:3642/5000 train_loss:3.6123 train_time:653243ms step_avg:179.86ms\nstep:3643/5000 train_loss:3.7700 train_time:653423ms step_avg:179.86ms\nstep:3644/5000 train_loss:4.0229 train_time:653602ms step_avg:179.86ms\nstep:3645/5000 train_loss:3.7586 train_time:653782ms step_avg:179.86ms\nstep:3646/5000 train_loss:3.6996 train_time:653960ms step_avg:179.86ms\nstep:3647/5000 train_loss:3.6468 train_time:654141ms step_avg:179.86ms\nstep:3648/5000 train_loss:3.4669 train_time:654321ms step_avg:179.86ms\nstep:3649/5000 train_loss:3.6515 train_time:654502ms step_avg:179.86ms\nstep:3650/5000 train_loss:3.7713 train_time:654681ms step_avg:179.86ms\nstep:3651/5000 train_loss:3.5879 train_time:654860ms step_avg:179.86ms\nstep:3652/5000 train_loss:3.7932 train_time:655039ms step_avg:179.86ms\nstep:3653/5000 train_loss:3.6640 train_time:655219ms step_avg:179.86ms\nstep:3654/5000 train_loss:3.7691 train_time:655398ms step_avg:179.86ms\nstep:3655/5000 train_loss:3.6005 train_time:655578ms step_avg:179.86ms\nstep:3656/5000 train_loss:3.6915 train_time:655758ms step_avg:179.86ms\nstep:3657/5000 train_loss:3.6764 train_time:655936ms step_avg:179.86ms\nstep:3658/5000 train_loss:3.6504 train_time:656116ms step_avg:179.86ms\nstep:3659/5000 train_loss:3.7184 train_time:656294ms step_avg:179.86ms\nstep:3660/5000 train_loss:3.7199 train_time:656473ms step_avg:179.86ms\nstep:3661/5000 train_loss:3.5218 train_time:656651ms step_avg:179.86ms\nstep:3662/5000 train_loss:3.8804 train_time:656830ms step_avg:179.85ms\nstep:3663/5000 train_loss:3.7081 train_time:657009ms step_avg:179.85ms\nstep:3664/5000 train_loss:3.8842 train_time:657189ms step_avg:179.85ms\nstep:3665/5000 train_loss:3.7016 train_time:657367ms step_avg:179.85ms\nstep:3666/5000 train_loss:3.6952 train_time:657547ms step_avg:179.85ms\nstep:3667/5000 train_loss:3.5913 train_time:657726ms step_avg:179.85ms\nstep:3668/5000 train_loss:3.6356 train_time:657906ms step_avg:179.85ms\nstep:3669/5000 train_loss:3.8976 train_time:658086ms step_avg:179.85ms\nstep:3670/5000 train_loss:3.7767 train_time:658265ms step_avg:179.85ms\nstep:3671/5000 train_loss:3.7655 train_time:658444ms step_avg:179.85ms\nstep:3672/5000 train_loss:3.6167 train_time:658623ms step_avg:179.85ms\nstep:3673/5000 train_loss:4.1765 train_time:658803ms step_avg:179.85ms\nstep:3674/5000 train_loss:3.7550 train_time:658982ms step_avg:179.85ms\nstep:3675/5000 train_loss:3.6000 train_time:659163ms step_avg:179.85ms\nstep:3676/5000 train_loss:3.6855 train_time:659342ms step_avg:179.85ms\nstep:3677/5000 train_loss:3.6856 train_time:659522ms step_avg:179.85ms\nstep:3678/5000 train_loss:4.9990 train_time:659702ms step_avg:179.85ms\nstep:3679/5000 train_loss:3.6189 train_time:659883ms step_avg:179.85ms\nstep:3680/5000 train_loss:3.5999 train_time:660063ms step_avg:179.85ms\nstep:3681/5000 train_loss:3.3825 train_time:660242ms step_avg:179.85ms\nstep:3682/5000 train_loss:3.6830 train_time:660421ms step_avg:179.85ms\nstep:3683/5000 train_loss:3.5160 train_time:660601ms step_avg:179.85ms\nstep:3684/5000 train_loss:3.6460 train_time:660781ms step_avg:179.85ms\nstep:3685/5000 train_loss:3.7891 train_time:660962ms step_avg:179.85ms\nstep:3686/5000 train_loss:3.7238 train_time:661142ms step_avg:179.85ms\nstep:3687/5000 train_loss:3.7362 train_time:661321ms step_avg:179.85ms\nstep:3688/5000 train_loss:3.6061 train_time:661500ms step_avg:179.85ms\nstep:3689/5000 train_loss:3.6551 train_time:661680ms step_avg:179.85ms\nstep:3690/5000 train_loss:3.6814 train_time:661860ms step_avg:179.85ms\nstep:3691/5000 train_loss:3.5701 train_time:662040ms step_avg:179.85ms\nstep:3692/5000 train_loss:3.7604 train_time:662220ms step_avg:179.85ms\nstep:3693/5000 train_loss:3.6466 train_time:662399ms step_avg:179.85ms\nstep:3694/5000 train_loss:3.7466 train_time:662579ms step_avg:179.85ms\nstep:3695/5000 train_loss:3.6357 train_time:662758ms step_avg:179.85ms\nstep:3696/5000 train_loss:3.6647 train_time:662937ms step_avg:179.85ms\nstep:3697/5000 train_loss:3.6600 train_time:663116ms step_avg:179.85ms\nstep:3698/5000 train_loss:3.7557 train_time:663294ms step_avg:179.85ms\nstep:3699/5000 train_loss:3.5950 train_time:663473ms step_avg:179.85ms\nstep:3700/5000 train_loss:3.6224 train_time:663651ms step_avg:179.85ms\nstep:3701/5000 train_loss:3.6920 train_time:663830ms step_avg:179.85ms\nstep:3702/5000 train_loss:3.8290 train_time:664008ms step_avg:179.85ms\nstep:3703/5000 train_loss:3.5879 train_time:664187ms step_avg:179.85ms\nstep:3704/5000 train_loss:3.7189 train_time:664368ms step_avg:179.85ms\nstep:3705/5000 train_loss:3.6248 train_time:664547ms step_avg:179.85ms\nstep:3706/5000 train_loss:3.8075 train_time:664729ms step_avg:179.85ms\nstep:3707/5000 train_loss:3.6709 train_time:664908ms step_avg:179.85ms\nstep:3708/5000 train_loss:3.6192 train_time:665087ms step_avg:179.85ms\nstep:3709/5000 train_loss:3.5099 train_time:665267ms step_avg:179.85ms\nstep:3710/5000 train_loss:3.7086 train_time:665446ms step_avg:179.85ms\nstep:3711/5000 train_loss:3.6117 train_time:665628ms step_avg:179.85ms\nstep:3712/5000 train_loss:3.7940 train_time:665811ms step_avg:179.85ms\nstep:3713/5000 train_loss:3.8265 train_time:665989ms step_avg:179.85ms\nstep:3714/5000 train_loss:3.6966 train_time:666168ms step_avg:179.85ms\nstep:3715/5000 train_loss:3.5317 train_time:666349ms step_avg:179.85ms\nstep:3716/5000 train_loss:3.6659 train_time:666527ms step_avg:179.85ms\nstep:3717/5000 train_loss:3.5893 train_time:666708ms step_avg:179.85ms\nstep:3718/5000 train_loss:3.9330 train_time:666887ms step_avg:179.85ms\nstep:3719/5000 train_loss:3.7132 train_time:667065ms step_avg:179.85ms\nstep:3720/5000 train_loss:3.5480 train_time:667245ms step_avg:179.85ms\nstep:3721/5000 train_loss:3.7216 train_time:667426ms step_avg:179.85ms\nstep:3722/5000 train_loss:3.7707 train_time:667605ms step_avg:179.85ms\nstep:3723/5000 train_loss:3.7605 train_time:667785ms step_avg:179.85ms\nstep:3724/5000 train_loss:3.8549 train_time:667965ms step_avg:179.85ms\nstep:3725/5000 train_loss:3.8850 train_time:668144ms step_avg:179.85ms\nstep:3726/5000 train_loss:3.5599 train_time:668324ms step_avg:179.85ms\nstep:3727/5000 train_loss:3.7764 train_time:668506ms step_avg:179.85ms\nstep:3728/5000 train_loss:3.5800 train_time:668686ms step_avg:179.85ms\nstep:3729/5000 train_loss:3.6468 train_time:668867ms step_avg:179.85ms\nstep:3730/5000 train_loss:3.6487 train_time:669045ms step_avg:179.85ms\nstep:3731/5000 train_loss:3.7641 train_time:669225ms step_avg:179.85ms\nstep:3732/5000 train_loss:3.4944 train_time:669405ms step_avg:179.85ms\nstep:3733/5000 train_loss:3.6847 train_time:669583ms step_avg:179.85ms\nstep:3734/5000 train_loss:3.8325 train_time:669763ms step_avg:179.85ms\nstep:3735/5000 train_loss:3.9589 train_time:669944ms step_avg:179.85ms\nstep:3736/5000 train_loss:3.7262 train_time:670123ms step_avg:179.85ms\nstep:3737/5000 train_loss:3.6663 train_time:670302ms step_avg:179.85ms\nstep:3738/5000 train_loss:3.5249 train_time:670481ms step_avg:179.85ms\nstep:3739/5000 train_loss:3.5720 train_time:670660ms step_avg:179.85ms\nstep:3740/5000 train_loss:3.6994 train_time:670839ms step_avg:179.85ms\nstep:3741/5000 train_loss:3.6355 train_time:671019ms step_avg:179.85ms\nstep:3742/5000 train_loss:3.7246 train_time:671198ms step_avg:179.85ms\nstep:3743/5000 train_loss:3.8408 train_time:671377ms step_avg:179.85ms\nstep:3744/5000 train_loss:3.5410 train_time:671556ms step_avg:179.85ms\nstep:3745/5000 train_loss:3.7922 train_time:671735ms step_avg:179.85ms\nstep:3746/5000 train_loss:3.6046 train_time:671914ms step_avg:179.85ms\nstep:3747/5000 train_loss:3.5650 train_time:672093ms step_avg:179.85ms\nstep:3748/5000 train_loss:3.5129 train_time:672272ms step_avg:179.85ms\nstep:3749/5000 train_loss:3.9045 train_time:672450ms step_avg:179.85ms\nstep:3750/5000 train_loss:3.6650 train_time:672628ms step_avg:179.85ms\nstep:3751/5000 train_loss:3.8955 train_time:672808ms step_avg:179.85ms\nstep:3752/5000 train_loss:3.6533 train_time:672988ms step_avg:179.85ms\nstep:3753/5000 train_loss:3.6102 train_time:673166ms step_avg:179.85ms\nstep:3754/5000 train_loss:3.7654 train_time:673346ms step_avg:179.85ms\nstep:3755/5000 train_loss:3.7577 train_time:673526ms step_avg:179.85ms\nstep:3756/5000 train_loss:3.4602 train_time:673705ms step_avg:179.85ms\nstep:3757/5000 train_loss:3.6916 train_time:673885ms step_avg:179.85ms\nstep:3758/5000 train_loss:3.6873 train_time:674066ms step_avg:179.85ms\nstep:3759/5000 train_loss:3.6329 train_time:674247ms step_avg:179.85ms\nstep:3760/5000 train_loss:3.7254 train_time:674426ms step_avg:179.85ms\nstep:3761/5000 train_loss:3.7280 train_time:674606ms step_avg:179.85ms\nstep:3762/5000 train_loss:3.8575 train_time:674786ms step_avg:179.85ms\nstep:3763/5000 train_loss:3.6441 train_time:674965ms step_avg:179.85ms\nstep:3764/5000 train_loss:3.5710 train_time:675145ms step_avg:179.85ms\nstep:3765/5000 train_loss:3.7341 train_time:675325ms step_avg:179.85ms\nstep:3766/5000 train_loss:3.6365 train_time:675507ms step_avg:179.85ms\nstep:3767/5000 train_loss:3.5809 train_time:675687ms step_avg:179.85ms\nstep:3768/5000 train_loss:3.5676 train_time:675868ms step_avg:179.85ms\nstep:3769/5000 train_loss:3.6628 train_time:676047ms step_avg:179.85ms\nstep:3770/5000 train_loss:3.6984 train_time:676225ms step_avg:179.85ms\nstep:3771/5000 train_loss:4.2620 train_time:676404ms step_avg:179.85ms\nstep:3772/5000 train_loss:3.6910 train_time:676585ms step_avg:179.85ms\nstep:3773/5000 train_loss:3.7171 train_time:676764ms step_avg:179.85ms\nstep:3774/5000 train_loss:3.4959 train_time:676945ms step_avg:179.85ms\nstep:3775/5000 train_loss:3.5640 train_time:677124ms step_avg:179.85ms\nstep:3776/5000 train_loss:3.6050 train_time:677303ms step_avg:179.85ms\nstep:3777/5000 train_loss:3.8705 train_time:677483ms step_avg:179.85ms\nstep:3778/5000 train_loss:3.8461 train_time:677663ms step_avg:179.85ms\nstep:3779/5000 train_loss:3.9501 train_time:677843ms step_avg:179.85ms\nstep:3780/5000 train_loss:3.6045 train_time:678022ms step_avg:179.85ms\nstep:3781/5000 train_loss:3.6976 train_time:678202ms step_avg:179.85ms\nstep:3782/5000 train_loss:3.5567 train_time:678381ms step_avg:179.85ms\nstep:3783/5000 train_loss:3.6329 train_time:678560ms step_avg:179.85ms\nstep:3784/5000 train_loss:3.8299 train_time:678740ms step_avg:179.85ms\nstep:3785/5000 train_loss:3.7361 train_time:678920ms step_avg:179.85ms\nstep:3786/5000 train_loss:3.5782 train_time:679100ms step_avg:179.85ms\nstep:3787/5000 train_loss:3.8731 train_time:679280ms step_avg:179.85ms\nstep:3788/5000 train_loss:3.6792 train_time:679459ms step_avg:179.85ms\nstep:3789/5000 train_loss:3.6352 train_time:679637ms step_avg:179.85ms\nstep:3790/5000 train_loss:5.2079 train_time:679816ms step_avg:179.85ms\nstep:3791/5000 train_loss:3.5941 train_time:679994ms step_avg:179.85ms\nstep:3792/5000 train_loss:3.7242 train_time:680173ms step_avg:179.84ms\nstep:3793/5000 train_loss:3.7254 train_time:680352ms step_avg:179.84ms\nstep:3794/5000 train_loss:3.7486 train_time:680531ms step_avg:179.84ms\nstep:3795/5000 train_loss:3.9696 train_time:680711ms step_avg:179.84ms\nstep:3796/5000 train_loss:3.9139 train_time:680890ms step_avg:179.84ms\nstep:3797/5000 train_loss:3.6945 train_time:681069ms step_avg:179.84ms\nstep:3798/5000 train_loss:3.6413 train_time:681248ms step_avg:179.84ms\nstep:3799/5000 train_loss:3.6652 train_time:681427ms step_avg:179.84ms\nstep:3800/5000 train_loss:3.5304 train_time:681608ms step_avg:179.84ms\nstep:3801/5000 train_loss:3.7008 train_time:681788ms step_avg:179.84ms\nstep:3802/5000 train_loss:3.5392 train_time:681968ms step_avg:179.84ms\nstep:3803/5000 train_loss:3.6043 train_time:682148ms step_avg:179.84ms\nstep:3804/5000 train_loss:3.5875 train_time:682327ms step_avg:179.84ms\nstep:3805/5000 train_loss:3.5785 train_time:682506ms step_avg:179.84ms\nstep:3806/5000 train_loss:3.5928 train_time:682686ms step_avg:179.84ms\nstep:3807/5000 train_loss:3.5358 train_time:682865ms step_avg:179.84ms\nstep:3808/5000 train_loss:3.8750 train_time:683046ms step_avg:179.84ms\nstep:3809/5000 train_loss:3.6167 train_time:683226ms step_avg:179.84ms\nstep:3810/5000 train_loss:3.8186 train_time:683643ms step_avg:179.91ms\nstep:3811/5000 train_loss:3.6922 train_time:683821ms step_avg:179.91ms\nstep:3812/5000 train_loss:3.5508 train_time:683999ms step_avg:179.91ms\nstep:3813/5000 train_loss:3.6202 train_time:684177ms step_avg:179.90ms\nstep:3814/5000 train_loss:3.7244 train_time:684355ms step_avg:179.90ms\nstep:3815/5000 train_loss:3.6787 train_time:684539ms step_avg:179.91ms\nstep:3816/5000 train_loss:3.6243 train_time:684719ms step_avg:179.91ms\nstep:3817/5000 train_loss:3.6628 train_time:684897ms step_avg:179.90ms\nstep:3818/5000 train_loss:3.4251 train_time:685076ms step_avg:179.90ms\nstep:3819/5000 train_loss:3.5782 train_time:685255ms step_avg:179.90ms\nstep:3820/5000 train_loss:3.7018 train_time:685433ms step_avg:179.90ms\nstep:3821/5000 train_loss:3.6326 train_time:685612ms step_avg:179.90ms\nstep:3822/5000 train_loss:3.5809 train_time:685791ms step_avg:179.90ms\nstep:3823/5000 train_loss:3.5990 train_time:685969ms step_avg:179.90ms\nstep:3824/5000 train_loss:3.5842 train_time:686148ms step_avg:179.90ms\nstep:3825/5000 train_loss:3.7477 train_time:686327ms step_avg:179.90ms\nstep:3826/5000 train_loss:3.5749 train_time:686506ms step_avg:179.90ms\nstep:3827/5000 train_loss:3.6333 train_time:686685ms step_avg:179.90ms\nstep:3828/5000 train_loss:3.6192 train_time:686868ms step_avg:179.90ms\nstep:3829/5000 train_loss:3.6076 train_time:687048ms step_avg:179.90ms\nstep:3830/5000 train_loss:3.6121 train_time:687228ms step_avg:179.90ms\nstep:3831/5000 train_loss:3.6906 train_time:687410ms step_avg:179.90ms\nstep:3832/5000 train_loss:3.5853 train_time:687588ms step_avg:179.90ms\nstep:3833/5000 train_loss:4.2038 train_time:687767ms step_avg:179.90ms\nstep:3834/5000 train_loss:3.5759 train_time:687948ms step_avg:179.90ms\nstep:3835/5000 train_loss:3.6099 train_time:688130ms step_avg:179.90ms\nstep:3836/5000 train_loss:3.6414 train_time:688309ms step_avg:179.90ms\nstep:3837/5000 train_loss:3.6969 train_time:688487ms step_avg:179.90ms\nstep:3838/5000 train_loss:3.7273 train_time:688666ms step_avg:179.90ms\nstep:3839/5000 train_loss:3.6566 train_time:688848ms step_avg:179.90ms\nstep:3840/5000 train_loss:3.6383 train_time:689027ms step_avg:179.90ms\nstep:3841/5000 train_loss:3.9033 train_time:689208ms step_avg:179.90ms\nstep:3842/5000 train_loss:3.8285 train_time:689388ms step_avg:179.90ms\nstep:3843/5000 train_loss:3.4994 train_time:689566ms step_avg:179.90ms\nstep:3844/5000 train_loss:3.6519 train_time:689746ms step_avg:179.90ms\nstep:3845/5000 train_loss:3.8691 train_time:689925ms step_avg:179.90ms\nstep:3846/5000 train_loss:3.7678 train_time:690107ms step_avg:179.90ms\nstep:3847/5000 train_loss:3.6816 train_time:690287ms step_avg:179.90ms\nstep:3848/5000 train_loss:3.5394 train_time:690467ms step_avg:179.90ms\nstep:3849/5000 train_loss:3.7914 train_time:690645ms step_avg:179.90ms\nstep:3850/5000 train_loss:3.5921 train_time:690826ms step_avg:179.90ms\nstep:3851/5000 train_loss:3.6831 train_time:691008ms step_avg:179.90ms\nstep:3852/5000 train_loss:3.8054 train_time:691188ms step_avg:179.90ms\nstep:3853/5000 train_loss:3.6496 train_time:691369ms step_avg:179.90ms\nstep:3854/5000 train_loss:3.5310 train_time:691549ms step_avg:179.90ms\nstep:3855/5000 train_loss:3.5982 train_time:691728ms step_avg:179.90ms\nstep:3856/5000 train_loss:3.7425 train_time:691908ms step_avg:179.90ms\nstep:3857/5000 train_loss:3.6748 train_time:692086ms step_avg:179.90ms\nstep:3858/5000 train_loss:3.6392 train_time:692265ms step_avg:179.90ms\nstep:3859/5000 train_loss:3.5818 train_time:692444ms step_avg:179.90ms\nstep:3860/5000 train_loss:3.5649 train_time:692625ms step_avg:179.90ms\nstep:3861/5000 train_loss:3.6426 train_time:692807ms step_avg:179.90ms\nstep:3862/5000 train_loss:3.7039 train_time:692986ms step_avg:179.90ms\nstep:3863/5000 train_loss:3.7780 train_time:693165ms step_avg:179.90ms\nstep:3864/5000 train_loss:3.5980 train_time:693344ms step_avg:179.90ms\nstep:3865/5000 train_loss:3.5356 train_time:693525ms step_avg:179.90ms\nstep:3866/5000 train_loss:3.7706 train_time:693705ms step_avg:179.90ms\nstep:3867/5000 train_loss:3.6003 train_time:693885ms step_avg:179.90ms\nstep:3868/5000 train_loss:3.6241 train_time:694065ms step_avg:179.90ms\nstep:3869/5000 train_loss:3.6852 train_time:694244ms step_avg:179.90ms\nstep:3870/5000 train_loss:3.6499 train_time:694423ms step_avg:179.90ms\nstep:3871/5000 train_loss:3.6362 train_time:694603ms step_avg:179.90ms\nstep:3872/5000 train_loss:3.8435 train_time:694784ms step_avg:179.90ms\nstep:3873/5000 train_loss:3.7016 train_time:694963ms step_avg:179.90ms\nstep:3874/5000 train_loss:3.6973 train_time:695143ms step_avg:179.90ms\nstep:3875/5000 train_loss:3.5933 train_time:695322ms step_avg:179.90ms\nstep:3876/5000 train_loss:3.6717 train_time:695502ms step_avg:179.90ms\nstep:3877/5000 train_loss:3.6983 train_time:695683ms step_avg:179.90ms\nstep:3878/5000 train_loss:3.6598 train_time:695865ms step_avg:179.90ms\nstep:3879/5000 train_loss:3.7275 train_time:696044ms step_avg:179.90ms\nstep:3880/5000 train_loss:3.6576 train_time:696222ms step_avg:179.90ms\nstep:3881/5000 train_loss:3.5847 train_time:696401ms step_avg:179.90ms\nstep:3882/5000 train_loss:3.6399 train_time:696580ms step_avg:179.90ms\nstep:3883/5000 train_loss:3.5921 train_time:696759ms step_avg:179.90ms\nstep:3884/5000 train_loss:3.7197 train_time:696939ms step_avg:179.90ms\nstep:3885/5000 train_loss:3.6559 train_time:697117ms step_avg:179.90ms\nstep:3886/5000 train_loss:3.5858 train_time:697295ms step_avg:179.90ms\nstep:3887/5000 train_loss:3.5522 train_time:697473ms step_avg:179.90ms\nstep:3888/5000 train_loss:3.5341 train_time:697651ms step_avg:179.90ms\nstep:3889/5000 train_loss:3.7495 train_time:697831ms step_avg:179.90ms\nstep:3890/5000 train_loss:3.4780 train_time:698010ms step_avg:179.90ms\nstep:3891/5000 train_loss:3.6925 train_time:698188ms step_avg:179.90ms\nstep:3892/5000 train_loss:3.6805 train_time:698367ms step_avg:179.90ms\nstep:3893/5000 train_loss:3.5704 train_time:698546ms step_avg:179.90ms\nstep:3894/5000 train_loss:3.6762 train_time:698726ms step_avg:179.90ms\nstep:3895/5000 train_loss:3.5645 train_time:698906ms step_avg:179.90ms\nstep:3896/5000 train_loss:3.6267 train_time:699085ms step_avg:179.90ms\nstep:3897/5000 train_loss:3.5440 train_time:699264ms step_avg:179.90ms\nstep:3898/5000 train_loss:3.4920 train_time:699443ms step_avg:179.90ms\nstep:3899/5000 train_loss:3.5287 train_time:699622ms step_avg:179.90ms\nstep:3900/5000 train_loss:3.6802 train_time:699806ms step_avg:179.90ms\nstep:3901/5000 train_loss:3.6233 train_time:699986ms step_avg:179.90ms\nstep:3902/5000 train_loss:3.6369 train_time:700167ms step_avg:179.90ms\nstep:3903/5000 train_loss:3.7147 train_time:700346ms step_avg:179.90ms\nstep:3904/5000 train_loss:3.7512 train_time:700526ms step_avg:179.90ms\nstep:3905/5000 train_loss:3.5488 train_time:700705ms step_avg:179.90ms\nstep:3906/5000 train_loss:3.7366 train_time:700884ms step_avg:179.90ms\nstep:3907/5000 train_loss:3.6282 train_time:701065ms step_avg:179.90ms\nstep:3908/5000 train_loss:3.6239 train_time:701248ms step_avg:179.90ms\nstep:3909/5000 train_loss:3.6176 train_time:701427ms step_avg:179.90ms\nstep:3910/5000 train_loss:3.5786 train_time:701607ms step_avg:179.90ms\nstep:3911/5000 train_loss:3.9724 train_time:701786ms step_avg:179.90ms\nstep:3912/5000 train_loss:3.8247 train_time:701966ms step_avg:179.90ms\nstep:3913/5000 train_loss:3.5981 train_time:702145ms step_avg:179.90ms\nstep:3914/5000 train_loss:3.6228 train_time:702327ms step_avg:179.90ms\nstep:3915/5000 train_loss:3.8104 train_time:702506ms step_avg:179.90ms\nstep:3916/5000 train_loss:3.7024 train_time:702686ms step_avg:179.90ms\nstep:3917/5000 train_loss:3.5816 train_time:702866ms step_avg:179.90ms\nstep:3918/5000 train_loss:3.6881 train_time:703046ms step_avg:179.90ms\nstep:3919/5000 train_loss:3.6779 train_time:703227ms step_avg:179.90ms\nstep:3920/5000 train_loss:3.5252 train_time:703407ms step_avg:179.90ms\nstep:3921/5000 train_loss:3.7190 train_time:703587ms step_avg:179.90ms\nstep:3922/5000 train_loss:3.5052 train_time:703766ms step_avg:179.90ms\nstep:3923/5000 train_loss:3.5676 train_time:703946ms step_avg:179.90ms\nstep:3924/5000 train_loss:3.8277 train_time:704126ms step_avg:179.90ms\nstep:3925/5000 train_loss:3.6809 train_time:704306ms step_avg:179.90ms\nstep:3926/5000 train_loss:3.6555 train_time:704486ms step_avg:179.90ms\nstep:3927/5000 train_loss:3.6113 train_time:704665ms step_avg:179.90ms\nstep:3928/5000 train_loss:3.5497 train_time:704846ms step_avg:179.90ms\nstep:3929/5000 train_loss:3.7586 train_time:705024ms step_avg:179.90ms\nstep:3930/5000 train_loss:3.6107 train_time:705203ms step_avg:179.90ms\nstep:3931/5000 train_loss:3.6500 train_time:705382ms step_avg:179.90ms\nstep:3932/5000 train_loss:3.7131 train_time:705562ms step_avg:179.90ms\nstep:3933/5000 train_loss:3.6290 train_time:705742ms step_avg:179.90ms\nstep:3934/5000 train_loss:3.6195 train_time:705921ms step_avg:179.90ms\nstep:3935/5000 train_loss:3.6457 train_time:706102ms step_avg:179.90ms\nstep:3936/5000 train_loss:3.6933 train_time:706282ms step_avg:179.90ms\nstep:3937/5000 train_loss:3.6542 train_time:706461ms step_avg:179.90ms\nstep:3938/5000 train_loss:3.7384 train_time:706640ms step_avg:179.90ms\nstep:3939/5000 train_loss:3.4870 train_time:706820ms step_avg:179.90ms\nstep:3940/5000 train_loss:3.7167 train_time:706999ms step_avg:179.90ms\nstep:3941/5000 train_loss:3.6289 train_time:707179ms step_avg:179.90ms\nstep:3942/5000 train_loss:3.5762 train_time:707358ms step_avg:179.90ms\nstep:3943/5000 train_loss:3.5823 train_time:707537ms step_avg:179.90ms\nstep:3944/5000 train_loss:3.6275 train_time:707716ms step_avg:179.90ms\nstep:3945/5000 train_loss:3.7470 train_time:707895ms step_avg:179.90ms\nstep:3946/5000 train_loss:3.6144 train_time:708073ms step_avg:179.90ms\nstep:3947/5000 train_loss:3.5371 train_time:708252ms step_avg:179.90ms\nstep:3948/5000 train_loss:3.7265 train_time:708431ms step_avg:179.90ms\nstep:3949/5000 train_loss:3.6564 train_time:708610ms step_avg:179.90ms\nstep:3950/5000 train_loss:3.6285 train_time:708789ms step_avg:179.90ms\nstep:3951/5000 train_loss:4.1032 train_time:708968ms step_avg:179.90ms\nstep:3952/5000 train_loss:3.5619 train_time:709146ms step_avg:179.90ms\nstep:3953/5000 train_loss:3.7158 train_time:709325ms step_avg:179.89ms\nstep:3954/5000 train_loss:3.6828 train_time:709506ms step_avg:179.89ms\nstep:3955/5000 train_loss:3.5371 train_time:709686ms step_avg:179.90ms\nstep:3956/5000 train_loss:3.6268 train_time:709865ms step_avg:179.89ms\nstep:3957/5000 train_loss:3.7930 train_time:710045ms step_avg:179.89ms\nstep:3958/5000 train_loss:3.5200 train_time:710224ms step_avg:179.89ms\nstep:3959/5000 train_loss:3.7122 train_time:710403ms step_avg:179.89ms\nstep:3960/5000 train_loss:3.6169 train_time:710583ms step_avg:179.89ms\nstep:3961/5000 train_loss:3.6675 train_time:710763ms step_avg:179.89ms\nstep:3962/5000 train_loss:3.5552 train_time:710942ms step_avg:179.89ms\nstep:3963/5000 train_loss:3.4670 train_time:711121ms step_avg:179.89ms\nstep:3964/5000 train_loss:3.5586 train_time:711300ms step_avg:179.89ms\nstep:3965/5000 train_loss:3.5758 train_time:711480ms step_avg:179.89ms\nstep:3966/5000 train_loss:3.7122 train_time:711659ms step_avg:179.89ms\nstep:3967/5000 train_loss:3.5501 train_time:711837ms step_avg:179.89ms\nstep:3968/5000 train_loss:3.7859 train_time:712016ms step_avg:179.89ms\nstep:3969/5000 train_loss:3.5702 train_time:712195ms step_avg:179.89ms\nstep:3970/5000 train_loss:3.6192 train_time:712374ms step_avg:179.89ms\nstep:3971/5000 train_loss:3.6192 train_time:712552ms step_avg:179.89ms\nstep:3972/5000 train_loss:3.5969 train_time:712730ms step_avg:179.89ms\nstep:3973/5000 train_loss:3.6731 train_time:712910ms step_avg:179.89ms\nstep:3974/5000 train_loss:3.6482 train_time:713089ms step_avg:179.89ms\nstep:3975/5000 train_loss:3.5521 train_time:713269ms step_avg:179.89ms\nstep:3976/5000 train_loss:3.5802 train_time:713447ms step_avg:179.89ms\nstep:3977/5000 train_loss:3.6789 train_time:713626ms step_avg:179.89ms\nstep:3978/5000 train_loss:3.5836 train_time:713807ms step_avg:179.89ms\nstep:3979/5000 train_loss:3.6851 train_time:713987ms step_avg:179.89ms\nstep:3980/5000 train_loss:3.6573 train_time:714167ms step_avg:179.89ms\nstep:3981/5000 train_loss:3.5610 train_time:714348ms step_avg:179.89ms\nstep:3982/5000 train_loss:3.5413 train_time:714527ms step_avg:179.89ms\nstep:3983/5000 train_loss:3.9188 train_time:714709ms step_avg:179.89ms\nstep:3984/5000 train_loss:3.7293 train_time:714888ms step_avg:179.89ms\nstep:3985/5000 train_loss:3.7511 train_time:715069ms step_avg:179.89ms\nstep:3986/5000 train_loss:3.5972 train_time:715248ms step_avg:179.89ms\nstep:3987/5000 train_loss:3.7332 train_time:715427ms step_avg:179.89ms\nstep:3988/5000 train_loss:3.6679 train_time:715608ms step_avg:179.89ms\nstep:3989/5000 train_loss:3.6042 train_time:715789ms step_avg:179.89ms\nstep:3990/5000 train_loss:3.5990 train_time:715967ms step_avg:179.89ms\nstep:3991/5000 train_loss:3.5715 train_time:716149ms step_avg:179.89ms\nstep:3992/5000 train_loss:3.5822 train_time:716328ms step_avg:179.89ms\nstep:3993/5000 train_loss:3.4152 train_time:716506ms step_avg:179.89ms\nstep:3994/5000 train_loss:3.7243 train_time:716686ms step_avg:179.89ms\nstep:3995/5000 train_loss:3.6626 train_time:716867ms step_avg:179.89ms\nstep:3996/5000 train_loss:3.9362 train_time:717047ms step_avg:179.89ms\nstep:3997/5000 train_loss:3.5618 train_time:717233ms step_avg:179.89ms\nstep:3998/5000 train_loss:3.5463 train_time:717411ms step_avg:179.89ms\nstep:3999/5000 train_loss:3.6451 train_time:717589ms step_avg:179.89ms\nstep:4000/5000 train_loss:3.5946 train_time:717768ms step_avg:179.89ms\nstep:4001/5000 train_loss:3.6687 train_time:717948ms step_avg:179.89ms\nstep:4002/5000 train_loss:3.7964 train_time:718127ms step_avg:179.89ms\nstep:4003/5000 train_loss:3.6998 train_time:718307ms step_avg:179.89ms\nstep:4004/5000 train_loss:3.7179 train_time:718486ms step_avg:179.89ms\nstep:4005/5000 train_loss:3.6418 train_time:718667ms step_avg:179.89ms\nstep:4006/5000 train_loss:3.7333 train_time:718847ms step_avg:179.89ms\nstep:4007/5000 train_loss:3.7596 train_time:719026ms step_avg:179.89ms\nstep:4008/5000 train_loss:3.5884 train_time:719206ms step_avg:179.89ms\nstep:4009/5000 train_loss:3.8218 train_time:719387ms step_avg:179.89ms\nstep:4010/5000 train_loss:3.6746 train_time:719566ms step_avg:179.89ms\nstep:4011/5000 train_loss:3.6980 train_time:719748ms step_avg:179.89ms\nstep:4012/5000 train_loss:3.6305 train_time:719929ms step_avg:179.89ms\nstep:4013/5000 train_loss:3.6390 train_time:720108ms step_avg:179.89ms\nstep:4014/5000 train_loss:3.6361 train_time:720288ms step_avg:179.89ms\nstep:4015/5000 train_loss:3.6274 train_time:720468ms step_avg:179.89ms\nstep:4016/5000 train_loss:3.5193 train_time:720649ms step_avg:179.89ms\nstep:4017/5000 train_loss:3.6946 train_time:720828ms step_avg:179.89ms\nstep:4018/5000 train_loss:3.6287 train_time:721009ms step_avg:179.89ms\nstep:4019/5000 train_loss:3.6754 train_time:721188ms step_avg:179.89ms\nstep:4020/5000 train_loss:3.7097 train_time:721366ms step_avg:179.89ms\nstep:4021/5000 train_loss:3.5674 train_time:721545ms step_avg:179.89ms\nstep:4022/5000 train_loss:3.6368 train_time:721724ms step_avg:179.89ms\nstep:4023/5000 train_loss:3.8656 train_time:721904ms step_avg:179.89ms\nstep:4024/5000 train_loss:3.7604 train_time:722084ms step_avg:179.89ms\nstep:4025/5000 train_loss:3.6027 train_time:722266ms step_avg:179.89ms\nstep:4026/5000 train_loss:3.6033 train_time:722446ms step_avg:179.89ms\nstep:4027/5000 train_loss:3.8056 train_time:722625ms step_avg:179.89ms\nstep:4028/5000 train_loss:3.5203 train_time:722808ms step_avg:179.89ms\nstep:4029/5000 train_loss:3.5324 train_time:722987ms step_avg:179.89ms\nstep:4030/5000 train_loss:3.5922 train_time:723168ms step_avg:179.89ms\nstep:4031/5000 train_loss:3.6946 train_time:723347ms step_avg:179.89ms\nstep:4032/5000 train_loss:3.6075 train_time:723529ms step_avg:179.89ms\nstep:4033/5000 train_loss:3.5852 train_time:723709ms step_avg:179.89ms\nstep:4034/5000 train_loss:3.5859 train_time:723889ms step_avg:179.89ms\nstep:4035/5000 train_loss:3.5665 train_time:724069ms step_avg:179.89ms\nstep:4036/5000 train_loss:3.5690 train_time:724249ms step_avg:179.89ms\nstep:4037/5000 train_loss:3.8731 train_time:724428ms step_avg:179.89ms\nstep:4038/5000 train_loss:3.6125 train_time:724606ms step_avg:179.89ms\nstep:4039/5000 train_loss:3.6461 train_time:724786ms step_avg:179.89ms\nstep:4040/5000 train_loss:3.6981 train_time:724966ms step_avg:179.89ms\nstep:4041/5000 train_loss:3.5807 train_time:725146ms step_avg:179.89ms\nstep:4042/5000 train_loss:3.8657 train_time:725325ms step_avg:179.89ms\nstep:4043/5000 train_loss:3.6900 train_time:725505ms step_avg:179.89ms\nstep:4044/5000 train_loss:3.8042 train_time:725684ms step_avg:179.89ms\nstep:4045/5000 train_loss:3.6718 train_time:725866ms step_avg:179.89ms\nstep:4046/5000 train_loss:3.7551 train_time:726045ms step_avg:179.89ms\nstep:4047/5000 train_loss:3.6787 train_time:726225ms step_avg:179.89ms\nstep:4048/5000 train_loss:3.6957 train_time:726404ms step_avg:179.89ms\nstep:4049/5000 train_loss:3.7720 train_time:726585ms step_avg:179.89ms\nstep:4050/5000 train_loss:3.5428 train_time:726763ms step_avg:179.89ms\nstep:4051/5000 train_loss:3.6478 train_time:726942ms step_avg:179.89ms\nstep:4052/5000 train_loss:3.6577 train_time:727123ms step_avg:179.89ms\nstep:4053/5000 train_loss:3.6056 train_time:727303ms step_avg:179.89ms\nstep:4054/5000 train_loss:3.6253 train_time:727485ms step_avg:179.89ms\nstep:4055/5000 train_loss:3.9881 train_time:727665ms step_avg:179.89ms\nstep:4056/5000 train_loss:3.5089 train_time:727844ms step_avg:179.89ms\nstep:4057/5000 train_loss:3.7378 train_time:728025ms step_avg:179.89ms\nstep:4058/5000 train_loss:3.5742 train_time:728205ms step_avg:179.89ms\nstep:4059/5000 train_loss:3.6178 train_time:728385ms step_avg:179.89ms\nstep:4060/5000 train_loss:3.6610 train_time:728564ms step_avg:179.89ms\nstep:4061/5000 train_loss:3.6817 train_time:728743ms step_avg:179.89ms\nstep:4062/5000 train_loss:3.6646 train_time:728924ms step_avg:179.89ms\nstep:4063/5000 train_loss:3.7046 train_time:729106ms step_avg:179.89ms\nstep:4064/5000 train_loss:3.7675 train_time:729286ms step_avg:179.89ms\nstep:4065/5000 train_loss:3.5229 train_time:729465ms step_avg:179.89ms\nstep:4066/5000 train_loss:3.7228 train_time:729644ms step_avg:179.89ms\nstep:4067/5000 train_loss:3.5525 train_time:729823ms step_avg:179.89ms\nstep:4068/5000 train_loss:3.4783 train_time:730002ms step_avg:179.89ms\nstep:4069/5000 train_loss:3.8855 train_time:730183ms step_avg:179.89ms\nstep:4070/5000 train_loss:3.5209 train_time:730362ms step_avg:179.89ms\nstep:4071/5000 train_loss:3.5969 train_time:730541ms step_avg:179.89ms\nstep:4072/5000 train_loss:3.5625 train_time:730720ms step_avg:179.89ms\nstep:4073/5000 train_loss:3.6441 train_time:730898ms step_avg:179.89ms\nstep:4074/5000 train_loss:3.5999 train_time:731077ms step_avg:179.89ms\nstep:4075/5000 train_loss:3.6226 train_time:731255ms step_avg:179.89ms\nstep:4076/5000 train_loss:3.5633 train_time:731433ms step_avg:179.89ms\nstep:4077/5000 train_loss:3.6077 train_time:731613ms step_avg:179.89ms\nstep:4078/5000 train_loss:3.5517 train_time:731791ms step_avg:179.89ms\nstep:4079/5000 train_loss:3.5985 train_time:731969ms step_avg:179.89ms\nstep:4080/5000 train_loss:3.5194 train_time:732148ms step_avg:179.89ms\nstep:4081/5000 train_loss:3.5211 train_time:732327ms step_avg:179.89ms\nstep:4082/5000 train_loss:3.7712 train_time:732509ms step_avg:179.89ms\nstep:4083/5000 train_loss:3.6172 train_time:732688ms step_avg:179.89ms\nstep:4084/5000 train_loss:3.6665 train_time:732870ms step_avg:179.89ms\nstep:4085/5000 train_loss:3.9788 train_time:733050ms step_avg:179.89ms\nstep:4086/5000 train_loss:3.5100 train_time:733230ms step_avg:179.89ms\nstep:4087/5000 train_loss:3.6608 train_time:733409ms step_avg:179.89ms\nstep:4088/5000 train_loss:3.5698 train_time:733588ms step_avg:179.89ms\nstep:4089/5000 train_loss:3.5690 train_time:733767ms step_avg:179.89ms\nstep:4090/5000 train_loss:3.8685 train_time:733946ms step_avg:179.89ms\nstep:4091/5000 train_loss:3.6572 train_time:734125ms step_avg:179.89ms\nstep:4092/5000 train_loss:3.5044 train_time:734307ms step_avg:179.89ms\nstep:4093/5000 train_loss:3.5913 train_time:734485ms step_avg:179.89ms\nstep:4094/5000 train_loss:3.7299 train_time:734664ms step_avg:179.89ms\nstep:4095/5000 train_loss:3.6703 train_time:734844ms step_avg:179.89ms\nstep:4096/5000 train_loss:3.6100 train_time:735024ms step_avg:179.89ms\nstep:4097/5000 train_loss:3.6034 train_time:735205ms step_avg:179.89ms\nstep:4098/5000 train_loss:3.4979 train_time:735385ms step_avg:179.89ms\nstep:4099/5000 train_loss:3.6664 train_time:735564ms step_avg:179.89ms\nstep:4100/5000 train_loss:3.6331 train_time:735743ms step_avg:179.89ms\nstep:4101/5000 train_loss:3.7268 train_time:735923ms step_avg:179.89ms\nstep:4102/5000 train_loss:3.6471 train_time:736103ms step_avg:179.89ms\nstep:4103/5000 train_loss:3.5540 train_time:736282ms step_avg:179.89ms\nstep:4104/5000 train_loss:3.6138 train_time:736462ms step_avg:179.89ms\nstep:4105/5000 train_loss:3.6218 train_time:736642ms step_avg:179.89ms\nstep:4106/5000 train_loss:3.5550 train_time:736823ms step_avg:179.89ms\nstep:4107/5000 train_loss:3.6985 train_time:737002ms step_avg:179.89ms\nstep:4108/5000 train_loss:3.6262 train_time:737181ms step_avg:179.89ms\nstep:4109/5000 train_loss:3.5413 train_time:737361ms step_avg:179.89ms\nstep:4110/5000 train_loss:3.6384 train_time:737541ms step_avg:179.89ms\nstep:4111/5000 train_loss:3.7345 train_time:737720ms step_avg:179.89ms\nstep:4112/5000 train_loss:3.7810 train_time:737899ms step_avg:179.89ms\nstep:4113/5000 train_loss:3.5223 train_time:738078ms step_avg:179.89ms\nstep:4114/5000 train_loss:3.5947 train_time:738258ms step_avg:179.89ms\nstep:4115/5000 train_loss:3.6674 train_time:738436ms step_avg:179.89ms\nstep:4116/5000 train_loss:3.4620 train_time:738615ms step_avg:179.89ms\nstep:4117/5000 train_loss:3.8143 train_time:738794ms step_avg:179.89ms\nstep:4118/5000 train_loss:3.4886 train_time:738973ms step_avg:179.89ms\nstep:4119/5000 train_loss:3.6392 train_time:739152ms step_avg:179.89ms\nstep:4120/5000 train_loss:3.5263 train_time:739331ms step_avg:179.89ms\nstep:4121/5000 train_loss:3.6062 train_time:739509ms step_avg:179.89ms\nstep:4122/5000 train_loss:3.6147 train_time:739687ms step_avg:179.89ms\nstep:4123/5000 train_loss:3.5128 train_time:739867ms step_avg:179.88ms\nstep:4124/5000 train_loss:3.4797 train_time:740046ms step_avg:179.88ms\nstep:4125/5000 train_loss:3.6779 train_time:740225ms step_avg:179.88ms\nstep:4126/5000 train_loss:4.0643 train_time:740407ms step_avg:179.89ms\nstep:4127/5000 train_loss:3.5523 train_time:740587ms step_avg:179.89ms\nstep:4128/5000 train_loss:3.6529 train_time:740768ms step_avg:179.89ms\nstep:4129/5000 train_loss:3.5355 train_time:740948ms step_avg:179.89ms\nstep:4130/5000 train_loss:3.5335 train_time:741128ms step_avg:179.89ms\nstep:4131/5000 train_loss:3.6515 train_time:741308ms step_avg:179.89ms\nstep:4132/5000 train_loss:3.6408 train_time:741488ms step_avg:179.89ms\nstep:4133/5000 train_loss:3.3536 train_time:741669ms step_avg:179.89ms\nstep:4134/5000 train_loss:3.5606 train_time:741849ms step_avg:179.89ms\nstep:4135/5000 train_loss:3.5836 train_time:742027ms step_avg:179.89ms\nstep:4136/5000 train_loss:3.8157 train_time:742206ms step_avg:179.89ms\nstep:4137/5000 train_loss:3.6153 train_time:742386ms step_avg:179.89ms\nstep:4138/5000 train_loss:3.6098 train_time:742565ms step_avg:179.88ms\nstep:4139/5000 train_loss:3.7091 train_time:742745ms step_avg:179.89ms\nstep:4140/5000 train_loss:3.6220 train_time:742925ms step_avg:179.88ms\nstep:4141/5000 train_loss:3.5524 train_time:743104ms step_avg:179.88ms\nstep:4142/5000 train_loss:3.6630 train_time:743282ms step_avg:179.88ms\nstep:4143/5000 train_loss:3.7010 train_time:743461ms step_avg:179.88ms\nstep:4144/5000 train_loss:3.5333 train_time:743640ms step_avg:179.88ms\nstep:4145/5000 train_loss:3.6724 train_time:743821ms step_avg:179.88ms\nstep:4146/5000 train_loss:3.4668 train_time:744001ms step_avg:179.88ms\nstep:4147/5000 train_loss:3.7636 train_time:744180ms step_avg:179.88ms\nstep:4148/5000 train_loss:3.5587 train_time:744360ms step_avg:179.88ms\nstep:4149/5000 train_loss:3.5965 train_time:744538ms step_avg:179.88ms\nstep:4150/5000 train_loss:3.4661 train_time:744717ms step_avg:179.88ms\nstep:4151/5000 train_loss:3.6236 train_time:744897ms step_avg:179.88ms\nstep:4152/5000 train_loss:3.6232 train_time:745076ms step_avg:179.88ms\nstep:4153/5000 train_loss:3.5674 train_time:745254ms step_avg:179.88ms\nstep:4154/5000 train_loss:3.6708 train_time:745432ms step_avg:179.88ms\nstep:4155/5000 train_loss:3.7456 train_time:745611ms step_avg:179.88ms\nstep:4156/5000 train_loss:3.9408 train_time:745790ms step_avg:179.88ms\nstep:4157/5000 train_loss:3.6105 train_time:745969ms step_avg:179.88ms\nstep:4158/5000 train_loss:3.4710 train_time:746149ms step_avg:179.88ms\nstep:4159/5000 train_loss:3.7028 train_time:746329ms step_avg:179.88ms\nstep:4160/5000 train_loss:3.7487 train_time:746509ms step_avg:179.88ms\nstep:4161/5000 train_loss:3.6668 train_time:746689ms step_avg:179.88ms\nstep:4162/5000 train_loss:3.4769 train_time:746867ms step_avg:179.88ms\nstep:4163/5000 train_loss:3.5736 train_time:747047ms step_avg:179.88ms\nstep:4164/5000 train_loss:3.6506 train_time:747225ms step_avg:179.88ms\nstep:4165/5000 train_loss:3.5121 train_time:747405ms step_avg:179.88ms\nstep:4166/5000 train_loss:3.6220 train_time:747585ms step_avg:179.88ms\nstep:4167/5000 train_loss:3.6381 train_time:747767ms step_avg:179.88ms\nstep:4168/5000 train_loss:3.5458 train_time:747948ms step_avg:179.88ms\nstep:4169/5000 train_loss:3.6395 train_time:748130ms step_avg:179.88ms\nstep:4170/5000 train_loss:3.4546 train_time:748308ms step_avg:179.88ms\nstep:4171/5000 train_loss:3.6688 train_time:748485ms step_avg:179.88ms\nstep:4172/5000 train_loss:3.6099 train_time:748664ms step_avg:179.88ms\nstep:4173/5000 train_loss:3.7938 train_time:748844ms step_avg:179.88ms\nstep:4174/5000 train_loss:3.5390 train_time:749025ms step_avg:179.88ms\nstep:4175/5000 train_loss:3.5643 train_time:749204ms step_avg:179.88ms\nstep:4176/5000 train_loss:3.6485 train_time:749384ms step_avg:179.88ms\nstep:4177/5000 train_loss:3.6094 train_time:749563ms step_avg:179.88ms\nstep:4178/5000 train_loss:3.6353 train_time:749741ms step_avg:179.88ms\nstep:4179/5000 train_loss:3.7962 train_time:749920ms step_avg:179.88ms\nstep:4180/5000 train_loss:3.5809 train_time:750100ms step_avg:179.88ms\nstep:4181/5000 train_loss:3.5844 train_time:750280ms step_avg:179.88ms\nstep:4182/5000 train_loss:3.7760 train_time:750458ms step_avg:179.88ms\nstep:4183/5000 train_loss:3.7591 train_time:750637ms step_avg:179.88ms\nstep:4184/5000 train_loss:3.5891 train_time:750816ms step_avg:179.88ms\nstep:4185/5000 train_loss:3.5686 train_time:750995ms step_avg:179.88ms\nstep:4186/5000 train_loss:3.4557 train_time:751173ms step_avg:179.88ms\nstep:4187/5000 train_loss:3.5714 train_time:751353ms step_avg:179.88ms\nstep:4188/5000 train_loss:3.6655 train_time:751531ms step_avg:179.88ms\nstep:4189/5000 train_loss:3.6129 train_time:751709ms step_avg:179.88ms\nstep:4190/5000 train_loss:3.7994 train_time:751887ms step_avg:179.88ms\nstep:4191/5000 train_loss:3.5671 train_time:752067ms step_avg:179.88ms\nstep:4192/5000 train_loss:3.5752 train_time:752246ms step_avg:179.88ms\nstep:4193/5000 train_loss:3.5797 train_time:752425ms step_avg:179.88ms\nstep:4194/5000 train_loss:3.5104 train_time:752605ms step_avg:179.88ms\nstep:4195/5000 train_loss:3.5785 train_time:752785ms step_avg:179.88ms\nstep:4196/5000 train_loss:3.7753 train_time:752964ms step_avg:179.88ms\nstep:4197/5000 train_loss:3.5029 train_time:753143ms step_avg:179.88ms\nstep:4198/5000 train_loss:4.1006 train_time:753325ms step_avg:179.88ms\nstep:4199/5000 train_loss:3.6166 train_time:753505ms step_avg:179.88ms\nstep:4200/5000 train_loss:3.6304 train_time:753687ms step_avg:179.88ms\nstep:4201/5000 train_loss:3.4326 train_time:753868ms step_avg:179.88ms\nstep:4202/5000 train_loss:3.5958 train_time:754046ms step_avg:179.88ms\nstep:4203/5000 train_loss:3.6435 train_time:754226ms step_avg:179.88ms\nstep:4204/5000 train_loss:3.5463 train_time:754405ms step_avg:179.88ms\nstep:4205/5000 train_loss:3.6324 train_time:754585ms step_avg:179.88ms\nstep:4206/5000 train_loss:3.5362 train_time:754765ms step_avg:179.88ms\nstep:4207/5000 train_loss:3.5535 train_time:754944ms step_avg:179.88ms\nstep:4208/5000 train_loss:3.5144 train_time:755124ms step_avg:179.88ms\nstep:4209/5000 train_loss:3.5900 train_time:755304ms step_avg:179.88ms\nstep:4210/5000 train_loss:3.5365 train_time:755487ms step_avg:179.88ms\nstep:4211/5000 train_loss:3.6126 train_time:755666ms step_avg:179.88ms\nstep:4212/5000 train_loss:3.6294 train_time:755846ms step_avg:179.88ms\nstep:4213/5000 train_loss:3.6168 train_time:756025ms step_avg:179.88ms\nstep:4214/5000 train_loss:3.6408 train_time:756206ms step_avg:179.88ms\nstep:4215/5000 train_loss:3.6464 train_time:756386ms step_avg:179.88ms\nstep:4216/5000 train_loss:3.5155 train_time:756567ms step_avg:179.88ms\nstep:4217/5000 train_loss:3.5660 train_time:756746ms step_avg:179.88ms\nstep:4218/5000 train_loss:3.5619 train_time:756924ms step_avg:179.88ms\nstep:4219/5000 train_loss:3.6526 train_time:757104ms step_avg:179.88ms\nstep:4220/5000 train_loss:3.8324 train_time:757287ms step_avg:179.88ms\nstep:4221/5000 train_loss:3.6592 train_time:757467ms step_avg:179.88ms\nstep:4222/5000 train_loss:3.6187 train_time:757647ms step_avg:179.88ms\nstep:4223/5000 train_loss:3.5024 train_time:757828ms step_avg:179.88ms\nstep:4224/5000 train_loss:3.6735 train_time:758008ms step_avg:179.88ms\nstep:4225/5000 train_loss:3.6557 train_time:758187ms step_avg:179.88ms\nstep:4226/5000 train_loss:3.5344 train_time:758366ms step_avg:179.88ms\nstep:4227/5000 train_loss:3.5017 train_time:758546ms step_avg:179.88ms\nstep:4228/5000 train_loss:3.5263 train_time:758726ms step_avg:179.88ms\nstep:4229/5000 train_loss:3.4863 train_time:758905ms step_avg:179.88ms\nstep:4230/5000 train_loss:3.5587 train_time:759085ms step_avg:179.88ms\nstep:4231/5000 train_loss:3.5734 train_time:759264ms step_avg:179.88ms\nstep:4232/5000 train_loss:3.6275 train_time:759443ms step_avg:179.88ms\nstep:4233/5000 train_loss:3.5028 train_time:759624ms step_avg:179.88ms\nstep:4234/5000 train_loss:3.6098 train_time:759803ms step_avg:179.88ms\nstep:4235/5000 train_loss:3.4854 train_time:759984ms step_avg:179.88ms\nstep:4236/5000 train_loss:3.7040 train_time:760163ms step_avg:179.88ms\nstep:4237/5000 train_loss:3.5668 train_time:760342ms step_avg:179.88ms\nstep:4238/5000 train_loss:3.7989 train_time:760522ms step_avg:179.88ms\nstep:4239/5000 train_loss:3.5618 train_time:760701ms step_avg:179.88ms\nstep:4240/5000 train_loss:3.4196 train_time:760880ms step_avg:179.88ms\nstep:4241/5000 train_loss:3.5186 train_time:761059ms step_avg:179.88ms\nstep:4242/5000 train_loss:3.5700 train_time:761239ms step_avg:179.88ms\nstep:4243/5000 train_loss:3.5483 train_time:761418ms step_avg:179.88ms\nstep:4244/5000 train_loss:3.5609 train_time:761597ms step_avg:179.88ms\nstep:4245/5000 train_loss:3.6352 train_time:761776ms step_avg:179.88ms\nstep:4246/5000 train_loss:3.7591 train_time:761954ms step_avg:179.88ms\nstep:4247/5000 train_loss:3.5949 train_time:762135ms step_avg:179.88ms\nstep:4248/5000 train_loss:3.5699 train_time:762311ms step_avg:179.88ms\nstep:4249/5000 train_loss:3.5707 train_time:762491ms step_avg:179.88ms\nstep:4250/5000 train_loss:3.4674 train_time:762671ms step_avg:179.88ms\nstep:4251/5000 train_loss:3.4362 train_time:762850ms step_avg:179.88ms\nstep:4252/5000 train_loss:3.5365 train_time:763028ms step_avg:179.87ms\nstep:4253/5000 train_loss:3.7711 train_time:763207ms step_avg:179.87ms\nstep:4254/5000 train_loss:3.5338 train_time:763386ms step_avg:179.87ms\nstep:4255/5000 train_loss:3.6608 train_time:763566ms step_avg:179.87ms\nstep:4256/5000 train_loss:3.5111 train_time:763747ms step_avg:179.87ms\nstep:4257/5000 train_loss:3.5486 train_time:763928ms step_avg:179.87ms\nstep:4258/5000 train_loss:3.6906 train_time:764108ms step_avg:179.87ms\nstep:4259/5000 train_loss:3.5572 train_time:764287ms step_avg:179.87ms\nstep:4260/5000 train_loss:3.6209 train_time:764465ms step_avg:179.87ms\nstep:4261/5000 train_loss:3.5212 train_time:764645ms step_avg:179.87ms\nstep:4262/5000 train_loss:3.6017 train_time:764824ms step_avg:179.87ms\nstep:4263/5000 train_loss:3.5890 train_time:765006ms step_avg:179.87ms\nstep:4264/5000 train_loss:3.5062 train_time:765185ms step_avg:179.87ms\nstep:4265/5000 train_loss:3.6439 train_time:765364ms step_avg:179.87ms\nstep:4266/5000 train_loss:3.7570 train_time:765543ms step_avg:179.87ms\nstep:4267/5000 train_loss:3.7204 train_time:765722ms step_avg:179.87ms\nstep:4268/5000 train_loss:3.5062 train_time:765902ms step_avg:179.87ms\nstep:4269/5000 train_loss:3.6608 train_time:766082ms step_avg:179.87ms\nstep:4270/5000 train_loss:3.2358 train_time:766262ms step_avg:179.87ms\nstep:4271/5000 train_loss:3.6823 train_time:766442ms step_avg:179.87ms\nstep:4272/5000 train_loss:3.5258 train_time:766621ms step_avg:179.87ms\nstep:4273/5000 train_loss:3.6761 train_time:766800ms step_avg:179.87ms\nstep:4274/5000 train_loss:3.5113 train_time:766980ms step_avg:179.87ms\nstep:4275/5000 train_loss:3.5395 train_time:767161ms step_avg:179.87ms\nstep:4276/5000 train_loss:3.4339 train_time:767340ms step_avg:179.87ms\nstep:4277/5000 train_loss:3.6495 train_time:767519ms step_avg:179.87ms\nstep:4278/5000 train_loss:3.6231 train_time:767699ms step_avg:179.87ms\nstep:4279/5000 train_loss:3.5585 train_time:767877ms step_avg:179.87ms\nstep:4280/5000 train_loss:3.5483 train_time:768056ms step_avg:179.87ms\nstep:4281/5000 train_loss:3.5137 train_time:768235ms step_avg:179.87ms\nstep:4282/5000 train_loss:3.6047 train_time:768413ms step_avg:179.87ms\nstep:4283/5000 train_loss:3.7808 train_time:768591ms step_avg:179.87ms\nstep:4284/5000 train_loss:3.4124 train_time:768769ms step_avg:179.87ms\nstep:4285/5000 train_loss:3.6497 train_time:768949ms step_avg:179.87ms\nstep:4286/5000 train_loss:3.5575 train_time:769128ms step_avg:179.87ms\nstep:4287/5000 train_loss:3.6495 train_time:769309ms step_avg:179.87ms\nstep:4288/5000 train_loss:3.6790 train_time:769489ms step_avg:179.87ms\nstep:4289/5000 train_loss:3.7114 train_time:769668ms step_avg:179.87ms\nstep:4290/5000 train_loss:3.4014 train_time:769847ms step_avg:179.87ms\nstep:4291/5000 train_loss:3.5664 train_time:770026ms step_avg:179.87ms\nstep:4292/5000 train_loss:3.6289 train_time:770205ms step_avg:179.87ms\nstep:4293/5000 train_loss:3.5185 train_time:770388ms step_avg:179.87ms\nstep:4294/5000 train_loss:3.5463 train_time:770569ms step_avg:179.87ms\nstep:4295/5000 train_loss:3.5572 train_time:770748ms step_avg:179.87ms\nstep:4296/5000 train_loss:3.5748 train_time:770927ms step_avg:179.87ms\nstep:4297/5000 train_loss:3.5392 train_time:771106ms step_avg:179.87ms\nstep:4298/5000 train_loss:3.6071 train_time:771285ms step_avg:179.87ms\nstep:4299/5000 train_loss:3.6657 train_time:771467ms step_avg:179.87ms\nstep:4300/5000 train_loss:3.6403 train_time:771647ms step_avg:179.87ms\nstep:4301/5000 train_loss:3.2713 train_time:771826ms step_avg:179.87ms\nstep:4302/5000 train_loss:3.5088 train_time:772006ms step_avg:179.87ms\nstep:4303/5000 train_loss:3.4451 train_time:772185ms step_avg:179.87ms\nstep:4304/5000 train_loss:3.5843 train_time:772365ms step_avg:179.87ms\nstep:4305/5000 train_loss:3.7230 train_time:772547ms step_avg:179.87ms\nstep:4306/5000 train_loss:3.6978 train_time:772727ms step_avg:179.87ms\nstep:4307/5000 train_loss:3.4917 train_time:772907ms step_avg:179.87ms\nstep:4308/5000 train_loss:3.5259 train_time:773087ms step_avg:179.87ms\nstep:4309/5000 train_loss:3.6185 train_time:773265ms step_avg:179.87ms\nstep:4310/5000 train_loss:3.5946 train_time:773445ms step_avg:179.87ms\nstep:4311/5000 train_loss:3.6069 train_time:773625ms step_avg:179.87ms\nstep:4312/5000 train_loss:3.9275 train_time:773806ms step_avg:179.87ms\nstep:4313/5000 train_loss:3.6940 train_time:773986ms step_avg:179.87ms\nstep:4314/5000 train_loss:3.5757 train_time:774166ms step_avg:179.87ms\nstep:4315/5000 train_loss:3.5622 train_time:774345ms step_avg:179.87ms\nstep:4316/5000 train_loss:3.5816 train_time:774525ms step_avg:179.87ms\nstep:4317/5000 train_loss:3.4843 train_time:774705ms step_avg:179.87ms\nstep:4318/5000 train_loss:3.5789 train_time:774885ms step_avg:179.87ms\nstep:4319/5000 train_loss:3.6061 train_time:775065ms step_avg:179.87ms\nstep:4320/5000 train_loss:3.3340 train_time:775244ms step_avg:179.87ms\nstep:4321/5000 train_loss:3.5110 train_time:775423ms step_avg:179.87ms\nstep:4322/5000 train_loss:3.6319 train_time:775603ms step_avg:179.87ms\nstep:4323/5000 train_loss:3.6019 train_time:775782ms step_avg:179.87ms\nstep:4324/5000 train_loss:3.6584 train_time:775964ms step_avg:179.87ms\nstep:4325/5000 train_loss:3.5832 train_time:776145ms step_avg:179.87ms\nstep:4326/5000 train_loss:3.6131 train_time:776325ms step_avg:179.87ms\nstep:4327/5000 train_loss:3.5537 train_time:776504ms step_avg:179.87ms\nstep:4328/5000 train_loss:3.5728 train_time:776684ms step_avg:179.87ms\nstep:4329/5000 train_loss:3.6009 train_time:776864ms step_avg:179.87ms\nstep:4330/5000 train_loss:3.6026 train_time:777043ms step_avg:179.87ms\nstep:4331/5000 train_loss:3.5147 train_time:777223ms step_avg:179.87ms\nstep:4332/5000 train_loss:3.6338 train_time:777403ms step_avg:179.87ms\nstep:4333/5000 train_loss:3.4815 train_time:777583ms step_avg:179.87ms\nstep:4334/5000 train_loss:3.5443 train_time:777763ms step_avg:179.87ms\nstep:4335/5000 train_loss:3.5010 train_time:777942ms step_avg:179.87ms\nstep:4336/5000 train_loss:3.7319 train_time:778121ms step_avg:179.87ms\nstep:4337/5000 train_loss:3.5058 train_time:778300ms step_avg:179.87ms\nstep:4338/5000 train_loss:3.3504 train_time:778481ms step_avg:179.87ms\nstep:4339/5000 train_loss:3.4764 train_time:778660ms step_avg:179.87ms\nstep:4340/5000 train_loss:3.6211 train_time:778840ms step_avg:179.87ms\nstep:4341/5000 train_loss:3.5435 train_time:779019ms step_avg:179.87ms\nstep:4342/5000 train_loss:3.4875 train_time:779198ms step_avg:179.87ms\nstep:4343/5000 train_loss:3.4478 train_time:779378ms step_avg:179.87ms\nstep:4344/5000 train_loss:3.5998 train_time:779558ms step_avg:179.87ms\nstep:4345/5000 train_loss:3.5946 train_time:779737ms step_avg:179.87ms\nstep:4346/5000 train_loss:3.6264 train_time:779916ms step_avg:179.87ms\nstep:4347/5000 train_loss:3.5127 train_time:780094ms step_avg:179.87ms\nstep:4348/5000 train_loss:3.5632 train_time:780273ms step_avg:179.87ms\nstep:4349/5000 train_loss:3.5961 train_time:780452ms step_avg:179.87ms\nstep:4350/5000 train_loss:3.4586 train_time:780630ms step_avg:179.87ms\nstep:4351/5000 train_loss:3.5657 train_time:780808ms step_avg:179.87ms\nstep:4352/5000 train_loss:3.7561 train_time:780988ms step_avg:179.87ms\nstep:4353/5000 train_loss:3.6513 train_time:781168ms step_avg:179.87ms\nstep:4354/5000 train_loss:3.4924 train_time:781347ms step_avg:179.87ms\nstep:4355/5000 train_loss:3.6256 train_time:781527ms step_avg:179.87ms\nstep:4356/5000 train_loss:3.6522 train_time:781707ms step_avg:179.87ms\nstep:4357/5000 train_loss:3.6396 train_time:781886ms step_avg:179.87ms\nstep:4358/5000 train_loss:3.5688 train_time:782065ms step_avg:179.87ms\nstep:4359/5000 train_loss:3.4456 train_time:782243ms step_avg:179.87ms\nstep:4360/5000 train_loss:3.4793 train_time:782422ms step_avg:179.87ms\nstep:4361/5000 train_loss:3.5273 train_time:782602ms step_avg:179.87ms\nstep:4362/5000 train_loss:3.6042 train_time:782783ms step_avg:179.87ms\nstep:4363/5000 train_loss:3.4763 train_time:782963ms step_avg:179.87ms\nstep:4364/5000 train_loss:3.5027 train_time:783142ms step_avg:179.87ms\nstep:4365/5000 train_loss:3.5492 train_time:783322ms step_avg:179.87ms\nstep:4366/5000 train_loss:3.6315 train_time:783503ms step_avg:179.87ms\nstep:4367/5000 train_loss:3.5111 train_time:783685ms step_avg:179.87ms\nstep:4368/5000 train_loss:3.6150 train_time:783865ms step_avg:179.87ms\nstep:4369/5000 train_loss:3.5620 train_time:784044ms step_avg:179.87ms\nstep:4370/5000 train_loss:3.5984 train_time:784223ms step_avg:179.87ms\nstep:4371/5000 train_loss:3.4708 train_time:784403ms step_avg:179.87ms\nstep:4372/5000 train_loss:3.4221 train_time:784584ms step_avg:179.87ms\nstep:4373/5000 train_loss:3.5374 train_time:784763ms step_avg:179.87ms\nstep:4374/5000 train_loss:3.4976 train_time:784943ms step_avg:179.87ms\nstep:4375/5000 train_loss:3.5430 train_time:785122ms step_avg:179.87ms\nstep:4376/5000 train_loss:3.6082 train_time:785301ms step_avg:179.87ms\nstep:4377/5000 train_loss:3.5117 train_time:785480ms step_avg:179.87ms\nstep:4378/5000 train_loss:3.4641 train_time:785661ms step_avg:179.87ms\nstep:4379/5000 train_loss:3.5370 train_time:785841ms step_avg:179.87ms\nstep:4380/5000 train_loss:3.7494 train_time:786021ms step_avg:179.87ms\nstep:4381/5000 train_loss:3.5883 train_time:786201ms step_avg:179.87ms\nstep:4382/5000 train_loss:3.7024 train_time:786381ms step_avg:179.87ms\nstep:4383/5000 train_loss:3.5376 train_time:786562ms step_avg:179.87ms\nstep:4384/5000 train_loss:3.5878 train_time:786742ms step_avg:179.87ms\nstep:4385/5000 train_loss:3.7068 train_time:786922ms step_avg:179.87ms\nstep:4386/5000 train_loss:3.4643 train_time:787101ms step_avg:179.87ms\nstep:4387/5000 train_loss:3.4740 train_time:787280ms step_avg:179.87ms\nstep:4388/5000 train_loss:3.5310 train_time:787459ms step_avg:179.87ms\nstep:4389/5000 train_loss:3.6322 train_time:787639ms step_avg:179.87ms\nstep:4390/5000 train_loss:3.5194 train_time:787820ms step_avg:179.87ms\nstep:4391/5000 train_loss:3.5951 train_time:788001ms step_avg:179.87ms\nstep:4392/5000 train_loss:3.5540 train_time:788181ms step_avg:179.87ms\nstep:4393/5000 train_loss:3.6699 train_time:788361ms step_avg:179.87ms\nstep:4394/5000 train_loss:3.5298 train_time:788541ms step_avg:179.87ms\nstep:4395/5000 train_loss:3.4961 train_time:788721ms step_avg:179.87ms\nstep:4396/5000 train_loss:3.6102 train_time:788901ms step_avg:179.87ms\nstep:4397/5000 train_loss:3.5419 train_time:789080ms step_avg:179.87ms\nstep:4398/5000 train_loss:3.5903 train_time:789260ms step_avg:179.87ms\nstep:4399/5000 train_loss:3.6299 train_time:789440ms step_avg:179.87ms\nstep:4400/5000 train_loss:3.6815 train_time:789619ms step_avg:179.87ms\nstep:4401/5000 train_loss:3.4805 train_time:789799ms step_avg:179.87ms\nstep:4402/5000 train_loss:3.6680 train_time:789978ms step_avg:179.87ms\nstep:4403/5000 train_loss:3.4914 train_time:790157ms step_avg:179.87ms\nstep:4404/5000 train_loss:3.6382 train_time:790336ms step_avg:179.87ms\nstep:4405/5000 train_loss:3.5385 train_time:790515ms step_avg:179.87ms\nstep:4406/5000 train_loss:3.5358 train_time:790694ms step_avg:179.87ms\nstep:4407/5000 train_loss:3.4596 train_time:790873ms step_avg:179.87ms\nstep:4408/5000 train_loss:3.5588 train_time:791051ms step_avg:179.87ms\nstep:4409/5000 train_loss:3.6025 train_time:791230ms step_avg:179.87ms\nstep:4410/5000 train_loss:3.5485 train_time:791408ms step_avg:179.87ms\nstep:4411/5000 train_loss:3.5152 train_time:791587ms step_avg:179.87ms\nstep:4412/5000 train_loss:3.4150 train_time:791765ms step_avg:179.86ms\nstep:4413/5000 train_loss:3.5879 train_time:791946ms step_avg:179.87ms\nstep:4414/5000 train_loss:3.5184 train_time:792126ms step_avg:179.87ms\nstep:4415/5000 train_loss:3.7103 train_time:792305ms step_avg:179.87ms\nstep:4416/5000 train_loss:3.5116 train_time:792485ms step_avg:179.86ms\nstep:4417/5000 train_loss:3.6418 train_time:792664ms step_avg:179.86ms\nstep:4418/5000 train_loss:3.4688 train_time:792843ms step_avg:179.86ms\nstep:4419/5000 train_loss:3.8528 train_time:793026ms step_avg:179.87ms\nstep:4420/5000 train_loss:3.5650 train_time:793205ms step_avg:179.87ms\nstep:4421/5000 train_loss:3.6120 train_time:793385ms step_avg:179.87ms\nstep:4422/5000 train_loss:3.5943 train_time:793566ms step_avg:179.87ms\nstep:4423/5000 train_loss:3.5603 train_time:793745ms step_avg:179.87ms\nstep:4424/5000 train_loss:3.4459 train_time:793925ms step_avg:179.87ms\nstep:4425/5000 train_loss:3.7961 train_time:794107ms step_avg:179.87ms\nstep:4426/5000 train_loss:3.6942 train_time:794286ms step_avg:179.87ms\nstep:4427/5000 train_loss:3.8167 train_time:794465ms step_avg:179.87ms\nstep:4428/5000 train_loss:4.2094 train_time:794644ms step_avg:179.87ms\nstep:4429/5000 train_loss:3.5970 train_time:794823ms step_avg:179.86ms\nstep:4430/5000 train_loss:3.4923 train_time:795005ms step_avg:179.87ms\nstep:4431/5000 train_loss:3.6185 train_time:795185ms step_avg:179.87ms\nstep:4432/5000 train_loss:3.3515 train_time:795365ms step_avg:179.87ms\nstep:4433/5000 train_loss:3.6107 train_time:795543ms step_avg:179.87ms\nstep:4434/5000 train_loss:3.7683 train_time:795723ms step_avg:179.87ms\nstep:4435/5000 train_loss:3.6454 train_time:795903ms step_avg:179.87ms\nstep:4436/5000 train_loss:3.6981 train_time:796084ms step_avg:179.87ms\nstep:4437/5000 train_loss:3.6219 train_time:796265ms step_avg:179.87ms\nstep:4438/5000 train_loss:3.4851 train_time:796444ms step_avg:179.87ms\nstep:4439/5000 train_loss:3.7679 train_time:796624ms step_avg:179.87ms\nstep:4440/5000 train_loss:3.4846 train_time:796804ms step_avg:179.87ms\nstep:4441/5000 train_loss:3.5211 train_time:796985ms step_avg:179.87ms\nstep:4442/5000 train_loss:3.5879 train_time:797167ms step_avg:179.87ms\nstep:4443/5000 train_loss:3.6606 train_time:797346ms step_avg:179.87ms\nstep:4444/5000 train_loss:3.6205 train_time:797528ms step_avg:179.87ms\nstep:4445/5000 train_loss:3.5371 train_time:797707ms step_avg:179.87ms\nstep:4446/5000 train_loss:3.5918 train_time:797887ms step_avg:179.87ms\nstep:4447/5000 train_loss:3.4456 train_time:798065ms step_avg:179.87ms\nstep:4448/5000 train_loss:3.5462 train_time:798245ms step_avg:179.87ms\nstep:4449/5000 train_loss:3.5164 train_time:798425ms step_avg:179.87ms\nstep:4450/5000 train_loss:3.5776 train_time:798608ms step_avg:179.87ms\nstep:4451/5000 train_loss:3.5155 train_time:798787ms step_avg:179.87ms\nstep:4452/5000 train_loss:3.5962 train_time:798966ms step_avg:179.87ms\nstep:4453/5000 train_loss:3.3855 train_time:799146ms step_avg:179.87ms\nstep:4454/5000 train_loss:3.5460 train_time:799327ms step_avg:179.87ms\nstep:4455/5000 train_loss:3.4223 train_time:799507ms step_avg:179.87ms\nstep:4456/5000 train_loss:3.6866 train_time:799686ms step_avg:179.87ms\nstep:4457/5000 train_loss:3.6806 train_time:799865ms step_avg:179.87ms\nstep:4458/5000 train_loss:3.4498 train_time:800044ms step_avg:179.87ms\nstep:4459/5000 train_loss:3.5870 train_time:800225ms step_avg:179.87ms\nstep:4460/5000 train_loss:3.5450 train_time:800405ms step_avg:179.87ms\nstep:4461/5000 train_loss:3.5686 train_time:800584ms step_avg:179.87ms\nstep:4462/5000 train_loss:3.6543 train_time:800763ms step_avg:179.87ms\nstep:4463/5000 train_loss:3.6893 train_time:800943ms step_avg:179.87ms\nstep:4464/5000 train_loss:3.4909 train_time:801123ms step_avg:179.87ms\nstep:4465/5000 train_loss:3.5730 train_time:801304ms step_avg:179.87ms\nstep:4466/5000 train_loss:3.6466 train_time:801485ms step_avg:179.87ms\nstep:4467/5000 train_loss:3.7696 train_time:801664ms step_avg:179.87ms\nstep:4468/5000 train_loss:3.3806 train_time:801844ms step_avg:179.87ms\nstep:4469/5000 train_loss:3.5651 train_time:802022ms step_avg:179.87ms\nstep:4470/5000 train_loss:3.4622 train_time:802202ms step_avg:179.87ms\nstep:4471/5000 train_loss:3.4851 train_time:802383ms step_avg:179.87ms\nstep:4472/5000 train_loss:3.5562 train_time:802563ms step_avg:179.87ms\nstep:4473/5000 train_loss:3.6522 train_time:802743ms step_avg:179.87ms\nstep:4474/5000 train_loss:3.7602 train_time:802923ms step_avg:179.87ms\nstep:4475/5000 train_loss:3.4104 train_time:803102ms step_avg:179.87ms\nstep:4476/5000 train_loss:3.7035 train_time:803281ms step_avg:179.87ms\nstep:4477/5000 train_loss:3.5091 train_time:803461ms step_avg:179.87ms\nstep:4478/5000 train_loss:3.5399 train_time:803641ms step_avg:179.87ms\nstep:4479/5000 train_loss:3.7007 train_time:803822ms step_avg:179.87ms\nstep:4480/5000 train_loss:3.7423 train_time:804001ms step_avg:179.87ms\nstep:4481/5000 train_loss:3.5668 train_time:804180ms step_avg:179.87ms\nstep:4482/5000 train_loss:3.5052 train_time:804360ms step_avg:179.87ms\nstep:4483/5000 train_loss:3.4925 train_time:804540ms step_avg:179.87ms\nstep:4484/5000 train_loss:3.5813 train_time:804720ms step_avg:179.87ms\nstep:4485/5000 train_loss:3.6089 train_time:804899ms step_avg:179.87ms\nstep:4486/5000 train_loss:3.5662 train_time:805079ms step_avg:179.87ms\nstep:4487/5000 train_loss:3.5173 train_time:805259ms step_avg:179.87ms\nstep:4488/5000 train_loss:3.6165 train_time:805438ms step_avg:179.87ms\nstep:4489/5000 train_loss:3.4803 train_time:805619ms step_avg:179.87ms\nstep:4490/5000 train_loss:3.8180 train_time:805798ms step_avg:179.87ms\nstep:4491/5000 train_loss:3.6529 train_time:805979ms step_avg:179.87ms\nstep:4492/5000 train_loss:3.5127 train_time:806157ms step_avg:179.87ms\nstep:4493/5000 train_loss:3.6275 train_time:806336ms step_avg:179.87ms\nstep:4494/5000 train_loss:3.5067 train_time:806515ms step_avg:179.87ms\nstep:4495/5000 train_loss:3.5295 train_time:806693ms step_avg:179.86ms\nstep:4496/5000 train_loss:3.5712 train_time:806872ms step_avg:179.86ms\nstep:4497/5000 train_loss:3.5944 train_time:807052ms step_avg:179.86ms\nstep:4498/5000 train_loss:3.5371 train_time:807230ms step_avg:179.86ms\nstep:4499/5000 train_loss:3.5274 train_time:807409ms step_avg:179.86ms\nstep:4500/5000 train_loss:3.7079 train_time:807587ms step_avg:179.86ms\nstep:4501/5000 train_loss:3.4805 train_time:807766ms step_avg:179.86ms\nstep:4502/5000 train_loss:3.5186 train_time:807947ms step_avg:179.86ms\nstep:4503/5000 train_loss:3.6028 train_time:808127ms step_avg:179.86ms\nstep:4504/5000 train_loss:3.4689 train_time:808306ms step_avg:179.86ms\nstep:4505/5000 train_loss:3.6486 train_time:808486ms step_avg:179.86ms\nstep:4506/5000 train_loss:3.5984 train_time:808664ms step_avg:179.86ms\nstep:4507/5000 train_loss:3.5209 train_time:808843ms step_avg:179.86ms\nstep:4508/5000 train_loss:3.3793 train_time:809025ms step_avg:179.86ms\nstep:4509/5000 train_loss:3.6265 train_time:809207ms step_avg:179.86ms\nstep:4510/5000 train_loss:3.6447 train_time:809389ms step_avg:179.86ms\nstep:4511/5000 train_loss:3.7924 train_time:809568ms step_avg:179.86ms\nstep:4512/5000 train_loss:3.5980 train_time:809746ms step_avg:179.86ms\nstep:4513/5000 train_loss:3.4698 train_time:809926ms step_avg:179.86ms\nstep:4514/5000 train_loss:3.4524 train_time:810108ms step_avg:179.86ms\nstep:4515/5000 train_loss:3.5532 train_time:810287ms step_avg:179.86ms\nstep:4516/5000 train_loss:3.8105 train_time:810468ms step_avg:179.86ms\nstep:4517/5000 train_loss:3.5137 train_time:810648ms step_avg:179.86ms\nstep:4518/5000 train_loss:3.4906 train_time:810828ms step_avg:179.86ms\nstep:4519/5000 train_loss:3.7320 train_time:811008ms step_avg:179.86ms\nstep:4520/5000 train_loss:3.3529 train_time:811186ms step_avg:179.86ms\nstep:4521/5000 train_loss:3.4792 train_time:811366ms step_avg:179.86ms\nstep:4522/5000 train_loss:3.5757 train_time:811546ms step_avg:179.86ms\nstep:4523/5000 train_loss:3.5223 train_time:811726ms step_avg:179.86ms\nstep:4524/5000 train_loss:3.6735 train_time:811907ms step_avg:179.86ms\nstep:4525/5000 train_loss:3.6314 train_time:812088ms step_avg:179.86ms\nstep:4526/5000 train_loss:3.4861 train_time:812267ms step_avg:179.86ms\nstep:4527/5000 train_loss:3.5671 train_time:812445ms step_avg:179.86ms\nstep:4528/5000 train_loss:3.5016 train_time:812629ms step_avg:179.86ms\nstep:4529/5000 train_loss:3.5219 train_time:812808ms step_avg:179.86ms\nstep:4530/5000 train_loss:3.6112 train_time:812988ms step_avg:179.86ms\nstep:4531/5000 train_loss:3.4003 train_time:813169ms step_avg:179.86ms\nstep:4532/5000 train_loss:3.5307 train_time:813348ms step_avg:179.86ms\nstep:4533/5000 train_loss:3.6678 train_time:813527ms step_avg:179.86ms\nstep:4534/5000 train_loss:3.5640 train_time:813706ms step_avg:179.86ms\nstep:4535/5000 train_loss:3.3919 train_time:813886ms step_avg:179.86ms\nstep:4536/5000 train_loss:3.4888 train_time:814067ms step_avg:179.86ms\nstep:4537/5000 train_loss:3.6307 train_time:814248ms step_avg:179.86ms\nstep:4538/5000 train_loss:3.5844 train_time:814427ms step_avg:179.86ms\nstep:4539/5000 train_loss:3.5221 train_time:814606ms step_avg:179.86ms\nstep:4540/5000 train_loss:3.6474 train_time:814786ms step_avg:179.86ms\nstep:4541/5000 train_loss:3.5205 train_time:814966ms step_avg:179.86ms\nstep:4542/5000 train_loss:3.4923 train_time:815148ms step_avg:179.86ms\nstep:4543/5000 train_loss:3.4439 train_time:815328ms step_avg:179.87ms\nstep:4544/5000 train_loss:3.5791 train_time:815508ms step_avg:179.87ms\nstep:4545/5000 train_loss:3.6332 train_time:815688ms step_avg:179.86ms\nstep:4546/5000 train_loss:3.7596 train_time:815866ms step_avg:179.86ms\nstep:4547/5000 train_loss:3.4895 train_time:816045ms step_avg:179.86ms\nstep:4548/5000 train_loss:3.6294 train_time:816225ms step_avg:179.86ms\nstep:4549/5000 train_loss:3.6693 train_time:816406ms step_avg:179.86ms\nstep:4550/5000 train_loss:3.6365 train_time:816585ms step_avg:179.86ms\nstep:4551/5000 train_loss:3.6353 train_time:816766ms step_avg:179.86ms\nstep:4552/5000 train_loss:3.6664 train_time:816945ms step_avg:179.86ms\nstep:4553/5000 train_loss:3.4748 train_time:817124ms step_avg:179.86ms\nstep:4554/5000 train_loss:3.4585 train_time:817307ms step_avg:179.86ms\nstep:4555/5000 train_loss:3.4499 train_time:817488ms step_avg:179.87ms\nstep:4556/5000 train_loss:3.5731 train_time:817667ms step_avg:179.87ms\nstep:4557/5000 train_loss:3.5533 train_time:817846ms step_avg:179.86ms\nstep:4558/5000 train_loss:3.6291 train_time:818025ms step_avg:179.86ms\nstep:4559/5000 train_loss:3.5548 train_time:818205ms step_avg:179.86ms\nstep:4560/5000 train_loss:3.4719 train_time:818385ms step_avg:179.86ms\nstep:4561/5000 train_loss:3.4506 train_time:818565ms step_avg:179.86ms\nstep:4562/5000 train_loss:3.4850 train_time:818746ms step_avg:179.87ms\nstep:4563/5000 train_loss:3.5141 train_time:818926ms step_avg:179.87ms\nstep:4564/5000 train_loss:3.7825 train_time:819105ms step_avg:179.87ms\nstep:4565/5000 train_loss:3.4581 train_time:819286ms step_avg:179.87ms\nstep:4566/5000 train_loss:3.5113 train_time:819464ms step_avg:179.86ms\nstep:4567/5000 train_loss:3.5645 train_time:819643ms step_avg:179.86ms\nstep:4568/5000 train_loss:3.5743 train_time:819823ms step_avg:179.86ms\nstep:4569/5000 train_loss:3.4362 train_time:820003ms step_avg:179.86ms\nstep:4570/5000 train_loss:3.5833 train_time:820183ms step_avg:179.86ms\nstep:4571/5000 train_loss:3.4669 train_time:820364ms step_avg:179.86ms\nstep:4572/5000 train_loss:3.6626 train_time:820775ms step_avg:179.92ms\nstep:4573/5000 train_loss:3.4923 train_time:820954ms step_avg:179.92ms\nstep:4574/5000 train_loss:3.6770 train_time:821132ms step_avg:179.91ms\nstep:4575/5000 train_loss:3.4895 train_time:821310ms step_avg:179.91ms\nstep:4576/5000 train_loss:3.7109 train_time:821489ms step_avg:179.91ms\nstep:4577/5000 train_loss:3.6673 train_time:821669ms step_avg:179.91ms\nstep:4578/5000 train_loss:3.5598 train_time:821847ms step_avg:179.91ms\nstep:4579/5000 train_loss:3.5363 train_time:822026ms step_avg:179.91ms\nstep:4580/5000 train_loss:3.3640 train_time:822205ms step_avg:179.91ms\nstep:4581/5000 train_loss:3.6790 train_time:822384ms step_avg:179.91ms\nstep:4582/5000 train_loss:3.4788 train_time:822564ms step_avg:179.91ms\nstep:4583/5000 train_loss:3.5500 train_time:822743ms step_avg:179.91ms\nstep:4584/5000 train_loss:3.6230 train_time:822924ms step_avg:179.91ms\nstep:4585/5000 train_loss:3.5086 train_time:823107ms step_avg:179.91ms\nstep:4586/5000 train_loss:3.5813 train_time:823287ms step_avg:179.91ms\nstep:4587/5000 train_loss:3.4602 train_time:823465ms step_avg:179.91ms\nstep:4588/5000 train_loss:3.4628 train_time:823645ms step_avg:179.91ms\nstep:4589/5000 train_loss:3.5965 train_time:823825ms step_avg:179.91ms\nstep:4590/5000 train_loss:3.6968 train_time:824004ms step_avg:179.91ms\nstep:4591/5000 train_loss:3.5083 train_time:824185ms step_avg:179.91ms\nstep:4592/5000 train_loss:3.5390 train_time:824367ms step_avg:179.91ms\nstep:4593/5000 train_loss:3.6041 train_time:824547ms step_avg:179.91ms\nstep:4594/5000 train_loss:3.6876 train_time:824727ms step_avg:179.91ms\nstep:4595/5000 train_loss:3.8350 train_time:824907ms step_avg:179.91ms\nstep:4596/5000 train_loss:3.5623 train_time:825086ms step_avg:179.91ms\nstep:4597/5000 train_loss:3.5354 train_time:825265ms step_avg:179.91ms\nstep:4598/5000 train_loss:3.5862 train_time:825445ms step_avg:179.91ms\nstep:4599/5000 train_loss:3.6244 train_time:825624ms step_avg:179.91ms\nstep:4600/5000 train_loss:3.6508 train_time:825806ms step_avg:179.91ms\nstep:4601/5000 train_loss:3.5057 train_time:825985ms step_avg:179.91ms\nstep:4602/5000 train_loss:3.6075 train_time:826164ms step_avg:179.91ms\nstep:4603/5000 train_loss:3.6546 train_time:826344ms step_avg:179.91ms\nstep:4604/5000 train_loss:3.5969 train_time:826524ms step_avg:179.91ms\nstep:4605/5000 train_loss:3.5293 train_time:826703ms step_avg:179.91ms\nstep:4606/5000 train_loss:3.4630 train_time:826884ms step_avg:179.91ms\nstep:4607/5000 train_loss:3.4654 train_time:827064ms step_avg:179.91ms\nstep:4608/5000 train_loss:3.4993 train_time:827242ms step_avg:179.91ms\nstep:4609/5000 train_loss:3.4672 train_time:827423ms step_avg:179.91ms\nstep:4610/5000 train_loss:3.4843 train_time:827602ms step_avg:179.91ms\nstep:4611/5000 train_loss:3.6149 train_time:827782ms step_avg:179.91ms\nstep:4612/5000 train_loss:3.4608 train_time:827962ms step_avg:179.91ms\nstep:4613/5000 train_loss:3.5055 train_time:828141ms step_avg:179.91ms\nstep:4614/5000 train_loss:3.4509 train_time:828320ms step_avg:179.91ms\nstep:4615/5000 train_loss:3.5112 train_time:828500ms step_avg:179.91ms\nstep:4616/5000 train_loss:3.7171 train_time:828680ms step_avg:179.91ms\nstep:4617/5000 train_loss:3.3546 train_time:828859ms step_avg:179.91ms\nstep:4618/5000 train_loss:3.5983 train_time:829038ms step_avg:179.91ms\nstep:4619/5000 train_loss:3.6564 train_time:829218ms step_avg:179.91ms\nstep:4620/5000 train_loss:3.7186 train_time:829397ms step_avg:179.91ms\nstep:4621/5000 train_loss:3.8115 train_time:829576ms step_avg:179.91ms\nstep:4622/5000 train_loss:3.5675 train_time:829755ms step_avg:179.91ms\nstep:4623/5000 train_loss:3.4189 train_time:829934ms step_avg:179.91ms\nstep:4624/5000 train_loss:3.4001 train_time:830113ms step_avg:179.91ms\nstep:4625/5000 train_loss:3.6019 train_time:830291ms step_avg:179.91ms\nstep:4626/5000 train_loss:3.6300 train_time:830470ms step_avg:179.91ms\nstep:4627/5000 train_loss:3.4348 train_time:830649ms step_avg:179.91ms\nstep:4628/5000 train_loss:3.4159 train_time:830827ms step_avg:179.91ms\nstep:4629/5000 train_loss:3.6314 train_time:831007ms step_avg:179.91ms\nstep:4630/5000 train_loss:3.4726 train_time:831187ms step_avg:179.91ms\nstep:4631/5000 train_loss:3.5336 train_time:831366ms step_avg:179.91ms\nstep:4632/5000 train_loss:3.5497 train_time:831544ms step_avg:179.91ms\nstep:4633/5000 train_loss:3.5217 train_time:831724ms step_avg:179.91ms\nstep:4634/5000 train_loss:3.6518 train_time:831904ms step_avg:179.91ms\nstep:4635/5000 train_loss:3.6041 train_time:832086ms step_avg:179.91ms\nstep:4636/5000 train_loss:3.5765 train_time:832269ms step_avg:179.91ms\nstep:4637/5000 train_loss:3.6619 train_time:832446ms step_avg:179.91ms\nstep:4638/5000 train_loss:3.6008 train_time:832625ms step_avg:179.91ms\nstep:4639/5000 train_loss:3.4392 train_time:832805ms step_avg:179.91ms\nstep:4640/5000 train_loss:3.3630 train_time:832985ms step_avg:179.91ms\nstep:4641/5000 train_loss:3.6074 train_time:833165ms step_avg:179.91ms\nstep:4642/5000 train_loss:3.4106 train_time:833345ms step_avg:179.91ms\nstep:4643/5000 train_loss:3.8943 train_time:833525ms step_avg:179.91ms\nstep:4644/5000 train_loss:3.4216 train_time:833705ms step_avg:179.91ms\nstep:4645/5000 train_loss:3.5758 train_time:833884ms step_avg:179.91ms\nstep:4646/5000 train_loss:3.4581 train_time:834065ms step_avg:179.91ms\nstep:4647/5000 train_loss:3.5889 train_time:834244ms step_avg:179.91ms\nstep:4648/5000 train_loss:3.4132 train_time:834424ms step_avg:179.91ms\nstep:4649/5000 train_loss:3.4499 train_time:834603ms step_avg:179.91ms\nstep:4650/5000 train_loss:3.4397 train_time:834784ms step_avg:179.91ms\nstep:4651/5000 train_loss:3.4952 train_time:834963ms step_avg:179.91ms\nstep:4652/5000 train_loss:3.6241 train_time:835142ms step_avg:179.91ms\nstep:4653/5000 train_loss:3.5496 train_time:835323ms step_avg:179.91ms\nstep:4654/5000 train_loss:3.2515 train_time:835502ms step_avg:179.91ms\nstep:4655/5000 train_loss:3.4403 train_time:835682ms step_avg:179.91ms\nstep:4656/5000 train_loss:3.4237 train_time:835862ms step_avg:179.91ms\nstep:4657/5000 train_loss:3.4094 train_time:836041ms step_avg:179.91ms\nstep:4658/5000 train_loss:3.5521 train_time:836220ms step_avg:179.91ms\nstep:4659/5000 train_loss:3.5865 train_time:836401ms step_avg:179.91ms\nstep:4660/5000 train_loss:3.5378 train_time:836582ms step_avg:179.91ms\nstep:4661/5000 train_loss:3.6124 train_time:836761ms step_avg:179.91ms\nstep:4662/5000 train_loss:3.5218 train_time:836941ms step_avg:179.91ms\nstep:4663/5000 train_loss:3.4107 train_time:837120ms step_avg:179.91ms\nstep:4664/5000 train_loss:3.5495 train_time:837299ms step_avg:179.91ms\nstep:4665/5000 train_loss:3.5923 train_time:837478ms step_avg:179.91ms\nstep:4666/5000 train_loss:3.3933 train_time:837658ms step_avg:179.91ms\nstep:4667/5000 train_loss:3.7189 train_time:837837ms step_avg:179.91ms\nstep:4668/5000 train_loss:3.5355 train_time:838016ms step_avg:179.91ms\nstep:4669/5000 train_loss:3.4451 train_time:838194ms step_avg:179.91ms\nstep:4670/5000 train_loss:3.3733 train_time:838373ms step_avg:179.91ms\nstep:4671/5000 train_loss:3.5931 train_time:838550ms step_avg:179.91ms\nstep:4672/5000 train_loss:3.5018 train_time:838728ms step_avg:179.91ms\nstep:4673/5000 train_loss:3.5565 train_time:838907ms step_avg:179.91ms\nstep:4674/5000 train_loss:3.4932 train_time:839086ms step_avg:179.91ms\nstep:4675/5000 train_loss:3.5008 train_time:839267ms step_avg:179.91ms\nstep:4676/5000 train_loss:3.4960 train_time:839446ms step_avg:179.91ms\nstep:4677/5000 train_loss:3.6101 train_time:839626ms step_avg:179.91ms\nstep:4678/5000 train_loss:3.5105 train_time:839806ms step_avg:179.91ms\nstep:4679/5000 train_loss:3.5068 train_time:839986ms step_avg:179.91ms\nstep:4680/5000 train_loss:3.6957 train_time:840166ms step_avg:179.91ms\nstep:4681/5000 train_loss:3.5023 train_time:840347ms step_avg:179.91ms\nstep:4682/5000 train_loss:3.8199 train_time:840527ms step_avg:179.91ms\nstep:4683/5000 train_loss:3.5452 train_time:840706ms step_avg:179.91ms\nstep:4684/5000 train_loss:3.4848 train_time:840888ms step_avg:179.91ms\nstep:4685/5000 train_loss:3.5762 train_time:841069ms step_avg:179.91ms\nstep:4686/5000 train_loss:3.4986 train_time:841249ms step_avg:179.91ms\nstep:4687/5000 train_loss:3.4756 train_time:841429ms step_avg:179.91ms\nstep:4688/5000 train_loss:3.5592 train_time:841607ms step_avg:179.91ms\nstep:4689/5000 train_loss:3.4437 train_time:841787ms step_avg:179.91ms\nstep:4690/5000 train_loss:3.5138 train_time:841965ms step_avg:179.91ms\nstep:4691/5000 train_loss:3.4962 train_time:842145ms step_avg:179.91ms\nstep:4692/5000 train_loss:3.5495 train_time:842327ms step_avg:179.91ms\nstep:4693/5000 train_loss:3.5475 train_time:842508ms step_avg:179.91ms\nstep:4694/5000 train_loss:3.3962 train_time:842687ms step_avg:179.91ms\nstep:4695/5000 train_loss:3.4092 train_time:842865ms step_avg:179.91ms\nstep:4696/5000 train_loss:3.4396 train_time:843045ms step_avg:179.91ms\nstep:4697/5000 train_loss:3.3756 train_time:843224ms step_avg:179.91ms\nstep:4698/5000 train_loss:3.5200 train_time:843404ms step_avg:179.91ms\nstep:4699/5000 train_loss:3.7298 train_time:843584ms step_avg:179.91ms\nstep:4700/5000 train_loss:3.5377 train_time:843763ms step_avg:179.91ms\nstep:4701/5000 train_loss:3.5614 train_time:843943ms step_avg:179.91ms\nstep:4702/5000 train_loss:3.6445 train_time:844122ms step_avg:179.91ms\nstep:4703/5000 train_loss:3.5139 train_time:844302ms step_avg:179.91ms\nstep:4704/5000 train_loss:3.9131 train_time:844485ms step_avg:179.91ms\nstep:4705/5000 train_loss:3.6502 train_time:844665ms step_avg:179.91ms\nstep:4706/5000 train_loss:3.4915 train_time:844844ms step_avg:179.91ms\nstep:4707/5000 train_loss:3.4473 train_time:845024ms step_avg:179.91ms\nstep:4708/5000 train_loss:3.4961 train_time:845203ms step_avg:179.91ms\nstep:4709/5000 train_loss:3.5197 train_time:845382ms step_avg:179.91ms\nstep:4710/5000 train_loss:3.5064 train_time:845562ms step_avg:179.91ms\nstep:4711/5000 train_loss:3.5078 train_time:845743ms step_avg:179.91ms\nstep:4712/5000 train_loss:3.4961 train_time:845923ms step_avg:179.91ms\nstep:4713/5000 train_loss:3.4933 train_time:846103ms step_avg:179.91ms\nstep:4714/5000 train_loss:3.5630 train_time:846283ms step_avg:179.91ms\nstep:4715/5000 train_loss:3.6210 train_time:846462ms step_avg:179.91ms\nstep:4716/5000 train_loss:3.3989 train_time:846641ms step_avg:179.91ms\nstep:4717/5000 train_loss:3.5329 train_time:846821ms step_avg:179.91ms\nstep:4718/5000 train_loss:3.5516 train_time:847003ms step_avg:179.91ms\nstep:4719/5000 train_loss:3.5929 train_time:847184ms step_avg:179.91ms\nstep:4720/5000 train_loss:3.5385 train_time:847362ms step_avg:179.91ms\nstep:4721/5000 train_loss:3.4561 train_time:847544ms step_avg:179.91ms\nstep:4722/5000 train_loss:3.6025 train_time:847723ms step_avg:179.91ms\nstep:4723/5000 train_loss:3.4590 train_time:847903ms step_avg:179.91ms\nstep:4724/5000 train_loss:3.5775 train_time:848083ms step_avg:179.91ms\nstep:4725/5000 train_loss:3.4531 train_time:848263ms step_avg:179.91ms\nstep:4726/5000 train_loss:3.5060 train_time:848441ms step_avg:179.91ms\nstep:4727/5000 train_loss:3.6439 train_time:848623ms step_avg:179.91ms\nstep:4728/5000 train_loss:3.4986 train_time:848802ms step_avg:179.91ms\nstep:4729/5000 train_loss:3.4681 train_time:848981ms step_avg:179.91ms\nstep:4730/5000 train_loss:3.5137 train_time:849160ms step_avg:179.91ms\nstep:4731/5000 train_loss:3.5161 train_time:849341ms step_avg:179.91ms\nstep:4732/5000 train_loss:3.3937 train_time:849520ms step_avg:179.91ms\nstep:4733/5000 train_loss:3.4727 train_time:849701ms step_avg:179.91ms\nstep:4734/5000 train_loss:3.4549 train_time:849882ms step_avg:179.91ms\nstep:4735/5000 train_loss:3.5561 train_time:850061ms step_avg:179.91ms\nstep:4736/5000 train_loss:3.5248 train_time:850240ms step_avg:179.91ms\nstep:4737/5000 train_loss:3.4520 train_time:850421ms step_avg:179.91ms\nstep:4738/5000 train_loss:3.5007 train_time:850600ms step_avg:179.91ms\nstep:4739/5000 train_loss:3.5861 train_time:850780ms step_avg:179.91ms\nstep:4740/5000 train_loss:3.6374 train_time:850960ms step_avg:179.91ms\nstep:4741/5000 train_loss:3.5950 train_time:851139ms step_avg:179.91ms\nstep:4742/5000 train_loss:3.4393 train_time:851317ms step_avg:179.91ms\nstep:4743/5000 train_loss:3.4951 train_time:851495ms step_avg:179.91ms\nstep:4744/5000 train_loss:3.8014 train_time:851674ms step_avg:179.91ms\nstep:4745/5000 train_loss:3.4952 train_time:851852ms step_avg:179.91ms\nstep:4746/5000 train_loss:3.6108 train_time:852031ms step_avg:179.91ms\nstep:4747/5000 train_loss:3.4403 train_time:852211ms step_avg:179.91ms\nstep:4748/5000 train_loss:3.6590 train_time:852389ms step_avg:179.90ms\nstep:4749/5000 train_loss:3.3939 train_time:852568ms step_avg:179.90ms\nstep:4750/5000 train_loss:3.6736 train_time:852748ms step_avg:179.90ms\nstep:4751/5000 train_loss:3.6544 train_time:852926ms step_avg:179.90ms\nstep:4752/5000 train_loss:3.4890 train_time:853106ms step_avg:179.90ms\nstep:4753/5000 train_loss:3.4646 train_time:853286ms step_avg:179.90ms\nstep:4754/5000 train_loss:3.4687 train_time:853468ms step_avg:179.90ms\nstep:4755/5000 train_loss:3.5126 train_time:853647ms step_avg:179.90ms\nstep:4756/5000 train_loss:3.3679 train_time:853827ms step_avg:179.90ms\nstep:4757/5000 train_loss:3.5292 train_time:854005ms step_avg:179.90ms\nstep:4758/5000 train_loss:3.5266 train_time:854187ms step_avg:179.90ms\nstep:4759/5000 train_loss:3.5611 train_time:854368ms step_avg:179.90ms\nstep:4760/5000 train_loss:3.4794 train_time:854547ms step_avg:179.90ms\nstep:4761/5000 train_loss:3.4758 train_time:854726ms step_avg:179.90ms\nstep:4762/5000 train_loss:3.5440 train_time:854907ms step_avg:179.90ms\nstep:4763/5000 train_loss:3.7471 train_time:855086ms step_avg:179.90ms\nstep:4764/5000 train_loss:3.5019 train_time:855266ms step_avg:179.90ms\nstep:4765/5000 train_loss:3.4028 train_time:855446ms step_avg:179.90ms\nstep:4766/5000 train_loss:3.3866 train_time:855625ms step_avg:179.90ms\nstep:4767/5000 train_loss:3.6904 train_time:855806ms step_avg:179.90ms\nstep:4768/5000 train_loss:3.5386 train_time:855985ms step_avg:179.90ms\nstep:4769/5000 train_loss:3.5436 train_time:856165ms step_avg:179.90ms\nstep:4770/5000 train_loss:3.5632 train_time:856345ms step_avg:179.90ms\nstep:4771/5000 train_loss:3.3852 train_time:856528ms step_avg:179.91ms\nstep:4772/5000 train_loss:3.4931 train_time:856708ms step_avg:179.91ms\nstep:4773/5000 train_loss:3.6868 train_time:856888ms step_avg:179.91ms\nstep:4774/5000 train_loss:3.7030 train_time:857067ms step_avg:179.90ms\nstep:4775/5000 train_loss:3.6930 train_time:857246ms step_avg:179.90ms\nstep:4776/5000 train_loss:3.6135 train_time:857426ms step_avg:179.90ms\nstep:4777/5000 train_loss:3.5053 train_time:857606ms step_avg:179.90ms\nstep:4778/5000 train_loss:3.5061 train_time:857786ms step_avg:179.90ms\nstep:4779/5000 train_loss:3.4533 train_time:857966ms step_avg:179.90ms\nstep:4780/5000 train_loss:3.4507 train_time:858145ms step_avg:179.90ms\nstep:4781/5000 train_loss:4.0074 train_time:858324ms step_avg:179.90ms\nstep:4782/5000 train_loss:3.3851 train_time:858505ms step_avg:179.90ms\nstep:4783/5000 train_loss:3.4853 train_time:858686ms step_avg:179.90ms\nstep:4784/5000 train_loss:3.4672 train_time:858865ms step_avg:179.90ms\nstep:4785/5000 train_loss:3.4640 train_time:859044ms step_avg:179.90ms\nstep:4786/5000 train_loss:3.6856 train_time:859223ms step_avg:179.90ms\nstep:4787/5000 train_loss:3.4671 train_time:859402ms step_avg:179.90ms\nstep:4788/5000 train_loss:3.4565 train_time:859582ms step_avg:179.90ms\nstep:4789/5000 train_loss:3.3623 train_time:859762ms step_avg:179.90ms\nstep:4790/5000 train_loss:3.4763 train_time:859943ms step_avg:179.90ms\nstep:4791/5000 train_loss:3.5111 train_time:860126ms step_avg:179.90ms\nstep:4792/5000 train_loss:3.5457 train_time:860305ms step_avg:179.90ms\nstep:4793/5000 train_loss:3.3161 train_time:860485ms step_avg:179.90ms\nstep:4794/5000 train_loss:3.4979 train_time:860665ms step_avg:179.90ms\nstep:4795/5000 train_loss:3.6229 train_time:860845ms step_avg:179.90ms\nstep:4796/5000 train_loss:3.4959 train_time:861026ms step_avg:179.91ms\nstep:4797/5000 train_loss:3.5125 train_time:861207ms step_avg:179.91ms\nstep:4798/5000 train_loss:3.3921 train_time:861386ms step_avg:179.91ms\nstep:4799/5000 train_loss:3.4302 train_time:861565ms step_avg:179.90ms\nstep:4800/5000 train_loss:3.4527 train_time:861744ms step_avg:179.90ms\nstep:4801/5000 train_loss:3.3486 train_time:861925ms step_avg:179.90ms\nstep:4802/5000 train_loss:3.4011 train_time:862104ms step_avg:179.90ms\nstep:4803/5000 train_loss:3.4500 train_time:862283ms step_avg:179.90ms\nstep:4804/5000 train_loss:3.8661 train_time:862463ms step_avg:179.90ms\nstep:4805/5000 train_loss:3.4860 train_time:862643ms step_avg:179.90ms\nstep:4806/5000 train_loss:3.4529 train_time:862823ms step_avg:179.90ms\nstep:4807/5000 train_loss:3.5453 train_time:863005ms step_avg:179.91ms\nstep:4808/5000 train_loss:3.4151 train_time:863183ms step_avg:179.90ms\nstep:4809/5000 train_loss:3.4493 train_time:863363ms step_avg:179.90ms\nstep:4810/5000 train_loss:3.5432 train_time:863543ms step_avg:179.90ms\nstep:4811/5000 train_loss:3.4298 train_time:863723ms step_avg:179.90ms\nstep:4812/5000 train_loss:3.5071 train_time:863902ms step_avg:179.90ms\nstep:4813/5000 train_loss:3.3802 train_time:864084ms step_avg:179.90ms\nstep:4814/5000 train_loss:3.6159 train_time:864263ms step_avg:179.90ms\nstep:4815/5000 train_loss:3.5358 train_time:864442ms step_avg:179.90ms\nstep:4816/5000 train_loss:3.4973 train_time:864622ms step_avg:179.90ms\nstep:4817/5000 train_loss:3.3217 train_time:864802ms step_avg:179.90ms\nstep:4818/5000 train_loss:3.6912 train_time:864982ms step_avg:179.90ms\nstep:4819/5000 train_loss:3.4373 train_time:865162ms step_avg:179.90ms\nstep:4820/5000 train_loss:3.5623 train_time:865342ms step_avg:179.90ms\nstep:4821/5000 train_loss:3.4953 train_time:865522ms step_avg:179.90ms\nstep:4822/5000 train_loss:3.5265 train_time:865701ms step_avg:179.90ms\nstep:4823/5000 train_loss:3.4394 train_time:865881ms step_avg:179.90ms\nstep:4824/5000 train_loss:3.5770 train_time:866061ms step_avg:179.90ms\nstep:4825/5000 train_loss:3.6290 train_time:866242ms step_avg:179.90ms\nstep:4826/5000 train_loss:3.4506 train_time:866422ms step_avg:179.90ms\nstep:4827/5000 train_loss:3.4355 train_time:866603ms step_avg:179.91ms\nstep:4828/5000 train_loss:3.5280 train_time:866785ms step_avg:179.91ms\nstep:4829/5000 train_loss:3.6892 train_time:866964ms step_avg:179.91ms\nstep:4830/5000 train_loss:3.5429 train_time:867147ms step_avg:179.91ms\nstep:4831/5000 train_loss:3.5691 train_time:867327ms step_avg:179.91ms\nstep:4832/5000 train_loss:3.4445 train_time:867507ms step_avg:179.91ms\nstep:4833/5000 train_loss:3.6345 train_time:867687ms step_avg:179.91ms\nstep:4834/5000 train_loss:3.5806 train_time:867866ms step_avg:179.91ms\nstep:4835/5000 train_loss:3.4288 train_time:868045ms step_avg:179.91ms\nstep:4836/5000 train_loss:3.4940 train_time:868226ms step_avg:179.91ms\nstep:4837/5000 train_loss:3.3398 train_time:868407ms step_avg:179.91ms\nstep:4838/5000 train_loss:3.6235 train_time:868586ms step_avg:179.91ms\nstep:4839/5000 train_loss:3.6931 train_time:868765ms step_avg:179.91ms\nstep:4840/5000 train_loss:3.4427 train_time:868944ms step_avg:179.91ms\nstep:4841/5000 train_loss:3.4667 train_time:869124ms step_avg:179.91ms\nstep:4842/5000 train_loss:3.5010 train_time:869305ms step_avg:179.91ms\nstep:4843/5000 train_loss:3.4342 train_time:869484ms step_avg:179.91ms\nstep:4844/5000 train_loss:3.5212 train_time:869664ms step_avg:179.91ms\nstep:4845/5000 train_loss:3.4260 train_time:869845ms step_avg:179.91ms\nstep:4846/5000 train_loss:3.3698 train_time:870025ms step_avg:179.91ms\nstep:4847/5000 train_loss:3.4531 train_time:870205ms step_avg:179.91ms\nstep:4848/5000 train_loss:3.7762 train_time:870385ms step_avg:179.91ms\nstep:4849/5000 train_loss:3.4383 train_time:870565ms step_avg:179.91ms\nstep:4850/5000 train_loss:3.4110 train_time:870746ms step_avg:179.91ms\nstep:4851/5000 train_loss:3.5794 train_time:870925ms step_avg:179.91ms\nstep:4852/5000 train_loss:3.4361 train_time:871103ms step_avg:179.91ms\nstep:4853/5000 train_loss:3.5317 train_time:871283ms step_avg:179.91ms\nstep:4854/5000 train_loss:3.5001 train_time:871465ms step_avg:179.91ms\nstep:4855/5000 train_loss:3.4691 train_time:871645ms step_avg:179.91ms\nstep:4856/5000 train_loss:3.6770 train_time:871826ms step_avg:179.91ms\nstep:4857/5000 train_loss:3.3697 train_time:872006ms step_avg:179.91ms\nstep:4858/5000 train_loss:3.5300 train_time:872185ms step_avg:179.91ms\nstep:4859/5000 train_loss:3.3875 train_time:872366ms step_avg:179.91ms\nstep:4860/5000 train_loss:3.5002 train_time:872546ms step_avg:179.91ms\nstep:4861/5000 train_loss:3.4685 train_time:872725ms step_avg:179.91ms\nstep:4862/5000 train_loss:3.5531 train_time:872906ms step_avg:179.91ms\nstep:4863/5000 train_loss:3.5722 train_time:873086ms step_avg:179.91ms\nstep:4864/5000 train_loss:3.5825 train_time:873266ms step_avg:179.91ms\nstep:4865/5000 train_loss:3.4780 train_time:873448ms step_avg:179.91ms\nstep:4866/5000 train_loss:3.4360 train_time:873629ms step_avg:179.91ms\nstep:4867/5000 train_loss:3.5484 train_time:873808ms step_avg:179.91ms\nstep:4868/5000 train_loss:3.5203 train_time:873987ms step_avg:179.91ms\nstep:4869/5000 train_loss:3.4570 train_time:874167ms step_avg:179.91ms\nstep:4870/5000 train_loss:3.5088 train_time:874347ms step_avg:179.91ms\nstep:4871/5000 train_loss:3.4136 train_time:874528ms step_avg:179.91ms\nstep:4872/5000 train_loss:3.5183 train_time:874706ms step_avg:179.91ms\nstep:4873/5000 train_loss:3.6388 train_time:874886ms step_avg:179.91ms\nstep:4874/5000 train_loss:3.4521 train_time:875065ms step_avg:179.91ms\nstep:4875/5000 train_loss:3.3388 train_time:875244ms step_avg:179.91ms\nstep:4876/5000 train_loss:3.5491 train_time:875425ms step_avg:179.91ms\nstep:4877/5000 train_loss:3.4576 train_time:875606ms step_avg:179.91ms\nstep:4878/5000 train_loss:3.4400 train_time:875786ms step_avg:179.91ms\nstep:4879/5000 train_loss:3.5488 train_time:875965ms step_avg:179.91ms\nstep:4880/5000 train_loss:3.4946 train_time:876144ms step_avg:179.91ms\nstep:4881/5000 train_loss:3.3999 train_time:876326ms step_avg:179.91ms\nstep:4882/5000 train_loss:3.5300 train_time:876507ms step_avg:179.91ms\nstep:4883/5000 train_loss:3.5064 train_time:876687ms step_avg:179.91ms\nstep:4884/5000 train_loss:3.5429 train_time:876866ms step_avg:179.91ms\nstep:4885/5000 train_loss:3.4560 train_time:877045ms step_avg:179.91ms\nstep:4886/5000 train_loss:3.3735 train_time:877224ms step_avg:179.91ms\nstep:4887/5000 train_loss:3.5228 train_time:877403ms step_avg:179.91ms\nstep:4888/5000 train_loss:3.5411 train_time:877585ms step_avg:179.91ms\nstep:4889/5000 train_loss:3.5970 train_time:877764ms step_avg:179.91ms\nstep:4890/5000 train_loss:3.6065 train_time:877944ms step_avg:179.91ms\nstep:4891/5000 train_loss:3.4298 train_time:878124ms step_avg:179.91ms\nstep:4892/5000 train_loss:3.5687 train_time:878303ms step_avg:179.91ms\nstep:4893/5000 train_loss:3.5280 train_time:878483ms step_avg:179.91ms\nstep:4894/5000 train_loss:3.6270 train_time:878663ms step_avg:179.91ms\nstep:4895/5000 train_loss:3.4582 train_time:878847ms step_avg:179.91ms\nstep:4896/5000 train_loss:3.5200 train_time:879028ms step_avg:179.91ms\nstep:4897/5000 train_loss:3.5113 train_time:879207ms step_avg:179.91ms\nstep:4898/5000 train_loss:3.5429 train_time:879385ms step_avg:179.91ms\nstep:4899/5000 train_loss:3.6177 train_time:879567ms step_avg:179.91ms\nstep:4900/5000 train_loss:3.4990 train_time:879745ms step_avg:179.91ms\nstep:4901/5000 train_loss:3.6977 train_time:879925ms step_avg:179.91ms\nstep:4902/5000 train_loss:3.4108 train_time:880105ms step_avg:179.91ms\nstep:4903/5000 train_loss:3.5088 train_time:880285ms step_avg:179.91ms\nstep:4904/5000 train_loss:3.4091 train_time:880464ms step_avg:179.91ms\nstep:4905/5000 train_loss:3.7279 train_time:880644ms step_avg:179.91ms\nstep:4906/5000 train_loss:3.3931 train_time:880823ms step_avg:179.91ms\nstep:4907/5000 train_loss:3.6371 train_time:881003ms step_avg:179.91ms\nstep:4908/5000 train_loss:3.5534 train_time:881183ms step_avg:179.91ms\nstep:4909/5000 train_loss:3.4676 train_time:881363ms step_avg:179.91ms\nstep:4910/5000 train_loss:3.4757 train_time:881545ms step_avg:179.91ms\nstep:4911/5000 train_loss:3.5297 train_time:881726ms step_avg:179.91ms\nstep:4912/5000 train_loss:3.3355 train_time:881906ms step_avg:179.91ms\nstep:4913/5000 train_loss:3.4862 train_time:882087ms step_avg:179.91ms\nstep:4914/5000 train_loss:3.4624 train_time:882266ms step_avg:179.91ms\nstep:4915/5000 train_loss:3.5205 train_time:882445ms step_avg:179.91ms\nstep:4916/5000 train_loss:3.6526 train_time:882626ms step_avg:179.91ms\nstep:4917/5000 train_loss:3.7045 train_time:882805ms step_avg:179.91ms\nstep:4918/5000 train_loss:3.4056 train_time:882986ms step_avg:179.91ms\nstep:4919/5000 train_loss:3.4818 train_time:883166ms step_avg:179.91ms\nstep:4920/5000 train_loss:3.4577 train_time:883347ms step_avg:179.91ms\nstep:4921/5000 train_loss:3.4703 train_time:883526ms step_avg:179.91ms\nstep:4922/5000 train_loss:3.5739 train_time:883705ms step_avg:179.91ms\nstep:4923/5000 train_loss:3.5620 train_time:883885ms step_avg:179.91ms\nstep:4924/5000 train_loss:3.4536 train_time:884064ms step_avg:179.91ms\nstep:4925/5000 train_loss:3.3424 train_time:884243ms step_avg:179.91ms\nstep:4926/5000 train_loss:3.4879 train_time:884423ms step_avg:179.91ms\nstep:4927/5000 train_loss:3.4901 train_time:884603ms step_avg:179.91ms\nstep:4928/5000 train_loss:3.6368 train_time:884784ms step_avg:179.91ms\nstep:4929/5000 train_loss:3.2940 train_time:884963ms step_avg:179.91ms\nstep:4930/5000 train_loss:3.5369 train_time:885143ms step_avg:179.91ms\nstep:4931/5000 train_loss:3.3298 train_time:885322ms step_avg:179.91ms\nstep:4932/5000 train_loss:3.3534 train_time:885502ms step_avg:179.91ms\nstep:4933/5000 train_loss:3.5089 train_time:885682ms step_avg:179.91ms\nstep:4934/5000 train_loss:3.6966 train_time:885860ms step_avg:179.91ms\nstep:4935/5000 train_loss:3.5368 train_time:886039ms step_avg:179.91ms\nstep:4936/5000 train_loss:3.4395 train_time:886220ms step_avg:179.91ms\nstep:4937/5000 train_loss:3.4255 train_time:886400ms step_avg:179.91ms\nstep:4938/5000 train_loss:3.6024 train_time:886580ms step_avg:179.91ms\nstep:4939/5000 train_loss:3.6562 train_time:886760ms step_avg:179.91ms\nstep:4940/5000 train_loss:3.4936 train_time:886940ms step_avg:179.91ms\nstep:4941/5000 train_loss:3.4469 train_time:887120ms step_avg:179.91ms\nstep:4942/5000 train_loss:3.5329 train_time:887299ms step_avg:179.91ms\nstep:4943/5000 train_loss:3.2945 train_time:887480ms step_avg:179.91ms\nstep:4944/5000 train_loss:3.4990 train_time:887660ms step_avg:179.91ms\nstep:4945/5000 train_loss:3.3719 train_time:887838ms step_avg:179.91ms\nstep:4946/5000 train_loss:3.6885 train_time:888017ms step_avg:179.91ms\nstep:4947/5000 train_loss:3.4235 train_time:888196ms step_avg:179.91ms\nstep:4948/5000 train_loss:3.4754 train_time:888374ms step_avg:179.91ms\nstep:4949/5000 train_loss:3.3583 train_time:888553ms step_avg:179.91ms\nstep:4950/5000 train_loss:3.4450 train_time:888731ms step_avg:179.91ms\nstep:4951/5000 train_loss:3.4939 train_time:888910ms step_avg:179.90ms\nstep:4952/5000 train_loss:3.4539 train_time:889089ms step_avg:179.90ms\nstep:4953/5000 train_loss:3.5182 train_time:889268ms step_avg:179.90ms\nstep:4954/5000 train_loss:3.4206 train_time:889448ms step_avg:179.90ms\nstep:4955/5000 train_loss:3.5467 train_time:889629ms step_avg:179.90ms\nstep:4956/5000 train_loss:3.6893 train_time:889810ms step_avg:179.90ms\nstep:4957/5000 train_loss:3.5103 train_time:889989ms step_avg:179.90ms\nstep:4958/5000 train_loss:3.3747 train_time:890169ms step_avg:179.90ms\nstep:4959/5000 train_loss:3.6559 train_time:890348ms step_avg:179.90ms\nstep:4960/5000 train_loss:3.4519 train_time:890528ms step_avg:179.90ms\nstep:4961/5000 train_loss:3.4317 train_time:890707ms step_avg:179.90ms\nstep:4962/5000 train_loss:3.4322 train_time:890886ms step_avg:179.90ms\nstep:4963/5000 train_loss:3.6095 train_time:891066ms step_avg:179.90ms\nstep:4964/5000 train_loss:3.3495 train_time:891245ms step_avg:179.90ms\nstep:4965/5000 train_loss:3.4109 train_time:891423ms step_avg:179.90ms\nstep:4966/5000 train_loss:3.4568 train_time:891606ms step_avg:179.90ms\nstep:4967/5000 train_loss:3.5717 train_time:891785ms step_avg:179.90ms\nstep:4968/5000 train_loss:3.6482 train_time:891963ms step_avg:179.90ms\nstep:4969/5000 train_loss:3.5321 train_time:892144ms step_avg:179.90ms\nstep:4970/5000 train_loss:3.4942 train_time:892324ms step_avg:179.90ms\nstep:4971/5000 train_loss:3.5019 train_time:892503ms step_avg:179.90ms\nstep:4972/5000 train_loss:3.3295 train_time:892685ms step_avg:179.90ms\nstep:4973/5000 train_loss:3.3918 train_time:892865ms step_avg:179.90ms\nstep:4974/5000 train_loss:3.5447 train_time:893044ms step_avg:179.90ms\nstep:4975/5000 train_loss:3.4793 train_time:893225ms step_avg:179.90ms\nstep:4976/5000 train_loss:3.5063 train_time:893405ms step_avg:179.90ms\nstep:4977/5000 train_loss:3.5455 train_time:893585ms step_avg:179.90ms\nstep:4978/5000 train_loss:3.4205 train_time:893764ms step_avg:179.90ms\nstep:4979/5000 train_loss:3.4165 train_time:893944ms step_avg:179.90ms\nstep:4980/5000 train_loss:3.7411 train_time:894126ms step_avg:179.90ms\nstep:4981/5000 train_loss:3.6806 train_time:894306ms step_avg:179.90ms\nstep:4982/5000 train_loss:3.5165 train_time:894487ms step_avg:179.90ms\nstep:4983/5000 train_loss:3.6261 train_time:894666ms step_avg:179.90ms\nstep:4984/5000 train_loss:3.4214 train_time:894848ms step_avg:179.91ms\nstep:4985/5000 train_loss:3.4411 train_time:895028ms step_avg:179.91ms\nstep:4986/5000 train_loss:3.5249 train_time:895208ms step_avg:179.91ms\nstep:4987/5000 train_loss:3.6297 train_time:895386ms step_avg:179.90ms\nstep:4988/5000 train_loss:3.4907 train_time:895566ms step_avg:179.90ms\nstep:4989/5000 train_loss:3.4443 train_time:895747ms step_avg:179.91ms\nstep:4990/5000 train_loss:3.5284 train_time:895927ms step_avg:179.90ms\nstep:4991/5000 train_loss:3.5095 train_time:896108ms step_avg:179.91ms\nstep:4992/5000 train_loss:3.5907 train_time:896287ms step_avg:179.91ms\nstep:4993/5000 train_loss:3.5416 train_time:896465ms step_avg:179.90ms\nstep:4994/5000 train_loss:3.3590 train_time:896644ms step_avg:179.90ms\nstep:4995/5000 train_loss:3.4944 train_time:896824ms step_avg:179.90ms\nstep:4996/5000 train_loss:3.3949 train_time:897006ms step_avg:179.90ms\nstep:4997/5000 train_loss:3.4977 train_time:897186ms step_avg:179.90ms\nstep:4998/5000 train_loss:3.5019 train_time:897366ms step_avg:179.90ms\nstep:4999/5000 train_loss:3.5203 train_time:897547ms step_avg:179.91ms\nstep:5000/5000 train_loss:3.4214 train_time:897728ms step_avg:179.91ms\npeak memory consumption: 37319 MiB\n(Current Step: 11, Remaining Steps: 39)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe training ran for 5000 iterations and saved the model and config files. The training loss seems to be decreasing, which is a good sign. Let's validate the model and see the performance.\n```\nvalidate\n```\nCOMMAND\nvalidate",
      "thought": "DISCUSSION\nThe training ran for 5000 iterations and saved the model and config files. The training loss seems to be decreasing, which is a good sign. Let's validate the model and see the performance.\nCOMMAND\nvalidate",
      "action": "validate\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "Failed to decode metrics from evaluation script. Output:\n---\nW0402 06:12:30.083000 3495 site-packages/torch/distributed/run.py:793] \nW0402 06:12:30.083000 3495 site-packages/torch/distributed/run.py:793] *****************************************\nW0402 06:12:30.083000 3495 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0402 06:12:30.083000 3495 site-packages/torch/distributed/run.py:793] *****************************************\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/agent/workspace/evaluate.py\", line 125, in <module>\n[rank0]:     val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/agent/workspace/evaluate.py\", line 59, in __init__\n[rank0]:     assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\n[rank0]:[W402 06:12:33.281476319 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\nW0402 06:12:33.728000 3495 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3562 closing signal SIGTERM\nE0402 06:12:34.043000 3495 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3561) of binary: /home/agent/miniconda3/envs/mlgym_generic/bin/python\nTraceback (most recent call last):\n  File \"/home/agent/miniconda3/envs/mlgym_generic/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 919, in main\n    run(args)\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n    elastic_launch(\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/agent/miniconda3/envs/mlgym_generic/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\n/home/agent/workspace/evaluate.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-04-02_06:12:33\n  host      : devgpu008.gtn1.facebook.com\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 3561)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n---\nPlease make sure the evaluation script prints the metrics in json format.\n(Current Step: 12, Remaining Steps: 38)\n(Open file: /home/agent/workspace/baseline.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "DISCUSSION\nThe validation failed with \"AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\". This is the same data path issue we had with training. It seems the validation script `evaluate.py` also uses the incorrect data path.\n\nLet's correct the `input_val_bin` path in `evaluate.py`. First, open `evaluate.py`.\n```\nopen evaluate.py\n```\nCOMMAND\nopen evaluate.py",
      "thought": "DISCUSSION\nThe validation failed with \"AssertionError: did not find any files that match the pattern ./data/fineweb10B/fineweb_val_*.bin\". This is the same data path issue we had with training. It seems the validation script `evaluate.py` also uses the incorrect data path.\n\nLet's correct the `input_val_bin` path in `evaluate.py`. First, open `evaluate.py`.\nCOMMAND\nopen evaluate.py",
      "action": "open evaluate.py\n",
      "agent": "primary_3"
    },
    {
      "role": "user",
      "content": "[File: /home/agent/workspace/evaluate.py (168 lines total)]\n1:\"\"\"\n2:Copyright (c) Meta Platforms, Inc. and affiliates.\n3:\n4:Adapted from https://github.com/KellerJordan/modded-nanogpt\n5:\"\"\"\n6:\n7:import glob\n8:import json\n9:import os\n10:import pickle\n11:import sys\n12:import time\n13:from dataclasses import dataclass\n14:\n15:import numpy as np\n16:import torch\n17:import torch.distributed as dist\n18:from baseline import Model, ModelConfig\n19:\n20:# -----------------------------------------------------------------------------\n21:# Our own simple Distributed Data Loader\n22:\n23:def _peek_data_shard(filename):\n24:    # only reads the header, returns header data\n25:    with open(filename, \"rb\") as f:\n26:        # first read the header, which is 256 int32 integers (4 bytes each)\n27:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n28:    if header[0] != 20240520:\n29:        print(\"ERROR: magic number mismatch in the data .bin file!\")\n30:        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n31:        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n32:        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n33:        exit(1)\n34:    assert header[1] == 1, \"unsupported version\"\n35:    ntok = header[2] # number of tokens (claimed)\n36:    return ntok # for now just return the number of tokens\n37:\n38:def _load_data_shard(filename):\n39:    with open(filename, \"rb\") as f:\n40:        # first read the header, which is 256 int32 integers (4 bytes each)\n41:        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n42:        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n43:        assert header[1] == 1, \"unsupported version\"\n44:        ntok = header[2] # number of tokens (claimed)\n45:        # the rest of it are tokens, stored as uint16\n46:        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n47:    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n48:    return tokens\n49:\n50:class DistributedDataLoader:\n51:    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n52:        self.process_rank = process_rank\n53:        self.num_processes = num_processes\n54:        self.B = B\n55:        self.T = T\n56:\n57:        # glob files that match the pattern\n58:        self.files = sorted(glob.glob(filename_pattern))\n59:        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n60:\n61:        # load and validate all data shards, count number of tokens in total\n62:        ntok_total = 0\n63:        for fname in self.files:\n64:            shard_ntok = _peek_data_shard(fname)\n65:            assert shard_ntok >= num_processes * B * T + 1\n66:            ntok_total += int(shard_ntok)\n67:        self.ntok_total = ntok_total\n68:\n69:        # kick things off\n70:        self.reset()\n71:\n72:    def reset(self):\n73:        self.current_shard = 0\n74:        self.current_position = self.process_rank * self.B * self.T\n75:        self.tokens = _load_data_shard(self.files[self.current_shard])\n76:\n77:    def advance(self): # advance to next data shard\n78:        self.current_shard = (self.current_shard + 1) % len(self.files)\n79:        self.current_position = self.process_rank * self.B * self.T\n80:        self.tokens = _load_data_shard(self.files[self.current_shard])\n81:\n82:    def next_batch(self):\n83:        B = self.B\n84:        T = self.T\n85:        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n86:        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n87:        x = (buf[:-1]).view(B, T) # inputs\n88:        y = (buf[1:]).view(B, T) # targets\n89:        # advance current position and load next shard if necessary\n90:        self.current_position += B * T * self.num_processes\n91:        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n92:            self.advance()\n93:        return x.cuda(), y.cuda()\n94:\n95:\n96:@dataclass\n97:class Hyperparameters:\n98:    # data hyperparams\n99:    input_val_bin : str = './data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on\n100:    # optimization hyperparams\n101:    batch_size : int = 8*64 # batch size, in sequences, across all devices\n102:    device_batch_size : int = 64 # batch size, in sequences, per device\n103:    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons\n104:    sequence_length : int = 1024 # sequence length, in tokens\n105:args = Hyperparameters()\n106:args.batch_size = args.device_batch_size * torch.cuda.device_count()\n107:\n108:# set up DDP (distributed data parallel). torchrun sets this env variable\n109:assert torch.cuda.is_available()\n110:dist.init_process_group(backend='nccl')\n111:ddp_rank = int(os.environ['RANK'])\n112:ddp_local_rank = int(os.environ['LOCAL_RANK'])\n113:ddp_world_size = int(os.environ['WORLD_SIZE'])\n114:device = f'cuda:{ddp_local_rank}'\n115:torch.cuda.set_device(device)\n116:\n117:master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.\n118:\n119:# convenience variables\n120:B, T = args.device_batch_size, args.sequence_length\n121:# calculate the number of steps to take in the val loop.\n122:assert args.val_tokens % (B * T * ddp_world_size) == 0\n123:val_steps = args.val_tokens // (B * T * ddp_world_size)\n124:\n125:val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)\n126:\n127:# Load the saved config and weights\n128:try:\n129:    import __main__\n130:    setattr(__main__, \"ModelConfig\", ModelConfig)\n131:    model_config = pickle.load(open(\"model_config.pt\", \"rb\"))\n132:    state_dict = torch.load('model.pt', weights_only=True)\n133:    new_state_dict = {}\n134:    for k, v in state_dict.items():\n135:        new_state_dict[k.replace(\"module._orig_mod.\", \"\")] = v\n136:    state_dict = new_state_dict\n137:except:\n138:    raise Exception(\"Model not found. Please run the training script first.\")\n139:\n140:\n141:# Reconstruct the model using the saved configuration\n142:model = Model(model_config)  # Use the saved config to initialize the model\n143:model.load_state_dict(state_dict)\n144:model = model.cuda()\n145:\n146:ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n147:\n148:# stop the clock\n149:t0 = time.time()\n150:# run validation batches\n151:model.eval()\n152:val_loader.reset()\n153:val_loss = 0.0\n154:torch.cuda.empty_cache()\n155:with torch.no_grad():\n156:    for _ in range(val_steps):\n157:        x_val, y_val = val_loader.next_batch()\n158:        with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason\n159:            _, loss = model(x_val, y_val, return_logits=False)\n160:            val_loss += loss.detach()\n161:            del loss\n162:dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)\n163:val_loss /= val_steps\n164:# log val loss to console and to logfile\n165:if master_process:\n166:    # Create and print the result dictionary\n167:    result = {\"val_loss\": val_loss.item()}\n168:    print(json.dumps(result))\n(Current Step: 13, Remaining Steps: 37)\n(Open file: /home/agent/workspace/evaluate.py)\n(Current directory: /home/agent/workspace)\nbash-$",
      "agent": "primary_3"
    },
    {
      "role": "assistant",
      "content": "Exit due to context window",
      "thought": "Exit due to context window",
      "action": "exit_context",
      "agent": "primary_3"
    }
  ],
  "info": {
    "model_stats": {
      "total_cost": 2.1817409999999997,
      "task_cost": 2.1817409999999997,
      "tokens_sent": 716567,
      "tokens_received": 2670,
      "api_calls": 14
    },
    "exit_status": "evaluation_format_error (exit_context)",
    "submission": null,
    "score": [],
    "summarizer": {}
  }
}